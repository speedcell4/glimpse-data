{
  "https://openreview.net/forum?id=-5k5tlOpUAq": {
    "title": "Can Large Multimodal Models Uncover Deep Semantics Behind Images?",
    "volume": "review",
    "abstract": "Understanding the deep semantics of images is essential in the era dominated by social media. However, current research works primarily on the superficial description of images, revealing a notable deficiency in the systematic investigation of the inherent deep semantics. In this work, we introduce DEEPEVAL, a comprehensive benchmark to assess Large Multimodal Models' (LMMs) capacities of visual deep semantics. DEEPEVAL includes human-annotated dataset and three progressive subtasks: fine-grained description selection, in-depth title matching, and deep semantics understanding. Utilizing DEEPEVAL, we evaluate 9 open-source LMMs and GPT-4V(ision). Our evaluation demonstrates a substantial gap between the deep semantic comprehension capabilities of existing LMMs and humans. For example, GPT-4V is 30% behind humans in understanding deep semantics, even though it achieves human-comparable performance in image description. Further analysis reveals that LMM performance on DEEPEVAL varies according to the specific facets of deep semantics explored, indicating the fundamental challenges remaining in developing LMMs",
    "checked": true,
    "id": "eaf0ee6c400d69a16fa8d5205b52a1a9b10e2646",
    "semantic_title": "can large multimodal models uncover deep semantics behind images?",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=PCicdEK7UIH": {
    "title": "Expanding Horizons in Short Text Analysis: Integrating LLMs and VAEs for Enhanced Topic Modeling",
    "volume": "review",
    "abstract": "Topic models are one of the compelling methods for discovering latent semantics in a document collection. However, it assumes that a document has sufficient co-occurrence information to be effective. However, in short texts, co-occurrence information is minimal, which results in feature sparsity in document representation. Therefore, existing topic models (probabilistic or neural) mostly fail to mine patterns from them to generate coherent topics. In this paper, we take a new approach to short-text topic modeling to address the data-sparsity issue by extending short text into longer sequences using large language models (LLMs) and decoding topics using a variational autoencoder (VAE). We observe that our model can substantially improve the performance of short-text topic modeling. Extensive experiments on multiple real-world datasets under extreme data sparsity scenarios show that our models can generate high-quality topics that outperform state-of-the-art models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=enZwbGgXeyT": {
    "title": "ETAS: Zero-Shot Transformer Architecture Search via Network Trainability and Expressivity",
    "volume": "review",
    "abstract": "Transformer Architecture Search (TAS) methods aim at automates searching the optimal Transformer architecture configurations for a given task. However, they are impeded by the prohibitive cost of evaluating Transformer architectures. Recently, several Zero-Shot TAS methods have been proposed to mitigate this problem by utilizing zero-cost proxies for evaluating Transformer architectures without training. Unfortunately, they are limited to specific tasks and lack theoretical guarantees. To solve this problem, we develop a new zero-cost proxy called NTSR that combines two theoretically-inspired indicators to measure the trainability and expressivity of Transformer networks separately. We then integrate it into an effective regularized evolution framework called ETAS demonstrate its efficacy on various tasks. The results show that our proposed NTSR proxy can consistently achieve a higher correlation with the true performance of Transformer networks on both computer vision and natural language processing tasks. Further, it can significantly accelerate the search process for finding the best-performing Transformer network architecture configurations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PEXHsm5Owwk": {
    "title": "Competence-Based Analysis of Language Models",
    "volume": "review",
    "abstract": "Despite the recent successes of large, pretrained neural language models (LLMs), little is known about the representations of linguistic structure they learn during pretraining, leading to unexpected behavior in response to small changes in inputs or application contexts. To better understand these models and behaviors, we propose a general analysis framework to move beyond traditional performance-based evaluation of LLMs and instead analyze them on the basis of their internal representations. Our framework, CALM (Competence-based Analysis of Language Models), is designed to study and measure the linguistic competence of LLMs in the context of specific tasks by intervening on models' internal representations of different linguistic properties using causal probing, and evaluating models' alignment under these interventions with a given ground-truth causal model of the task. We also develop a novel approach for performing causal probing interventions using gradient-based adversarial attacks, which can target a broader range of properties and representations than existing techniques. Finally, we carry out a case study of CALM using these interventions to analyze BERT and RoBERTa's competence across a variety of lexical inference tasks, showing that CALM can be used to explain and predict their behavior across these tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gf1IJoW7LiC": {
    "title": "STARLING: Self-supervised Training of Text-based Reinforcement Learning Agent with Large Language Models",
    "volume": "review",
    "abstract": "Interactive fiction games have emerged as an important application to improve the generalization capabilities of language-based reinforcement learning (RL) agents. Existing environments for interactive fiction games are domain-specific or time-consuming to generate and do not train the RL agents to master a specific set of skills. In this work, we introduce an interactive environment for self-supervised RL, \\textit{STARLING}, for text-based games that bootstraps the text-based RL agents with automatically generated games (based on the seed set of game ideas) to boost the performance and generalization capabilities to reach a goal of the target environment. These games let the agent hone their skills on a predefined set of tasks. We create and test an environment with $100$ games, generated using this automated framework that uses large language models (GPT3) and an interactive fiction game engine (based on Inform7) to provide the user with the ability to generate more games under minimal human supervision. Experimental results based on both the human participants and baseline text-based RL agents reveal that current state-of-the-art text-based RL agents cannot use previously learned skills in new situations at the level humans can. These results enforce STARLING's potential to serve as a sandbox environment for further research in self-supervised text-based RL",
    "checked": true,
    "id": "31b99032f20373a19043c85492bf717fc17c06f1",
    "semantic_title": "starling: self-supervised training of text-based reinforcement learning agent with large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vzswGwQdtxj": {
    "title": "Dissecting similarities in self-consistency: An analysis on impact of semantic consistency on language model reasoning",
    "volume": "review",
    "abstract": "While large language models (LLMs) have rapidly improved performance on a broad number of tasks, they still fall often fall short on reasoning tasks.\\citet{wang2023selfconsistency} propose \\textit{self-consistency}, finding that sampling multiple rationales before taking a majority vote stably improves performance across a wide variety of closed-answer reasoning tasks.Standard self-consistency aggregates the numerical outputs of these rationales; our work instead incorporates the content of the rationales to identify consensus responses, re-weighting solutions based on patterns found in their vector embeddings of sequence outputs. By doing so we analyze and evaluate the implied effect of consistent reasoning paths over the traditional focus on numerical outputs, while improving accuracy on common benchmarks by weighting based on semantically consistent answers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dULzm4PkCX": {
    "title": "EasyGen: Easing Multimodal Generation with a Bidirectional Conditional Diffusion Model and LLMs",
    "volume": "review",
    "abstract": "We present EasyGen, an efficient model designed to enhance multimodal understanding and generation by harnessing the capabilities of diffusion models and large language models (LLMs). Unlike existing multimodal models that predominately depend on encoders like CLIP or ImageBind and need ample amounts of training data to bridge modalities, EasyGen leverages BiDiffuser, a bidirectional conditional diffusion model, to foster more efficient modality interactions. EasyGen achieves text generation by training a projection layer linking BiDiffuser and an LLM, and facilities image generation by training an adapter to align the LLM's text space with the BiDiffuser's image space. Comprehensive quantitative and qualitative experiments show that EasyGen excels in data-efficient training, high-quality image generation, and extendibility, effectively addressing the challenges in multimodal generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_hDPYcbksu": {
    "title": "End-to-End Argument Mining as Augmented Natural Language Generation",
    "volume": "review",
    "abstract": "Argument Mining (AM) is a crucial aspect of computational argumentation, which deals with the identification and extraction of Argumentative Components (ACs) and their corresponding Argumentative Relations (ARs). This work proposes a unified end-to-end framework based on a generative paradigm, in which the argumentative structures are framed into label-augmented text, called Augmented Natural Language (ANL). Additionally, we explore the role of different types of markers in solving AM tasks. Through different marker-based fine-tuning strategies, we present an extensive study by integrating marker knowledge into our generative model. The proposed framework achieves competitive results to the state-of-the-art (SoTA) model and outperforms several baselines",
    "checked": true,
    "id": "afefc95b1dc066efd725aad75835dff465410f48",
    "semantic_title": "end-to-end argument mining as augmented natural language generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sz7j7MmcR6": {
    "title": "WatME: Towards Lossless Watermarking Through Lexical Redundancy",
    "volume": "review",
    "abstract": "Text watermarking has emerged as an important technique for detecting machine-generated text. However, existing methods generally use arbitrary vocabulary partitioning during decoding, which results in the absence of appropriate words during the response generation and disrupts the language model's expressiveness, thus severely degrading the quality of text response. To address these issues, we introduce a novel approach, Watermarking with Mutual Exclusion (WatME). Specifically, by leveraging linguistic prior knowledge of inherent lexical redundancy, WatME can dynamically optimize the use of available vocabulary during the decoding process of language models. It employs a mutually exclusive rule to manage this redundancy, avoiding situations where appropriate words are unavailable and maintaining the expressive power of large language models (LLMs). We present theoretical analysis and empirical evidence demonstrating that WatME substantially preserves the text generation ability of LLMs while maintaining watermark detectability. Specifically, we investigate watermarking's impact on the emergent abilities of LLMs, including knowledge recall and logical reasoning. Our comprehensive experiments confirm that WatME consistently outperforms existing methods in retaining these crucial capabilities of LLMs. Our code will be released to facilitate future research",
    "checked": true,
    "id": "0bee079faf3dc53380db55cd5fc1ba9267c4d5e7",
    "semantic_title": "watme: towards lossless watermarking through lexical redundancy",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ggEJrZ_Dn7": {
    "title": "Semantics or Syntax? Which is More Important in In-Context Learning for Sentence Classification",
    "volume": "review",
    "abstract": "In this study, we explore the impact of semantics and syntax in the construction of demonstration examples for in-context learning (ICL) with Large Language Models (LLMs). We identify the limitations of current methods that prioritize semantic similarity and underscore the importance of syntactic information, which has been underrepresented in sentence-level classification tasks. Through experiments measuring semantic and syntactic similarities, we reveal that ICL methods tend to favor syntactic congruence. Consequently, we propose a novel Semantics and Syntax-based Sentence Selection (SSSS) framework for selecting demonstration examples in ICL, integrating both semantic and syntactic dimensions. This approach addresses the challenges of constructing accurate semantic representations and quantifying syntactic structure similarities. The experimental results on three datasets suggest that the SSSS approach can facilitate more effective ICL by incorporating syntax into the demonstrative example selection, potentially leading to enhanced model performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J8xI94tulY": {
    "title": "Bridging Law and Data: Augmenting Reasoning via a Semi-Structured Dataset with IRAC methodology",
    "volume": "review",
    "abstract": "Despite the advancements of Large Language Models (LLMs), their effectiveness in legal reasoning is limited due to unique legal terminologies and the need for highly specialized knowledge. These limitations can be addressed with high-quality data for complex legal reasoning. To this end, this paper introduces a benchmark, LegalSemi, annotated with IRAC (Issue, Rule, Application, Conclusion) for legal scenario analysis, developed by legal experts. It includes 54 legal scenarios annotated with full IRAC analysis and an associated structured knowledge graph (SKG). Our analysis reveals that Mistral-7b, a state-of-the-art LLM, is particularly adept at identifying legal concepts, while GPT-3.5 shows superior performance in analysis and conclusion tasks. Notably, standard LLMs face challenges in rule retrieval, an issue significantly mitigated by integrating SKG, which enhances the accuracy by 48%. LegalSemi serves as an innovative and valuable benchmark for complex legal reasoning, with the potential for broader applications across various legal domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_8i0lxZjUo": {
    "title": "Arc Representation for Graph-based Dependency Parsing",
    "volume": "review",
    "abstract": "In this paper, we address the explicit representation of arcs in syntactic dependency parsing, diverging from conventional approaches where parsing algorithms directly manipulate dependency arc scores derived from input token representations. We propose augmenting the parser with an intermediate arc representation, arguing for two main advantages. Firstly, arc vectors encapsulate richer information, enhancing the capabilities of subsequent scoring functions. Secondly, by introducing refinement layers, we enable interactions among vector representations, facilitating the consideration of global long-range dependencies. We demonstrate the efficacy of this approach through empirical evaluations on PTB and UD dependency treebanks",
    "checked": false,
    "id": "b112a4a27c7e97229f9c12372efb7be047e10ff0",
    "semantic_title": "auxiliary tasks to boost biaffine semantic dependency parsing",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Fd2mmQKFKW": {
    "title": "On Fake News Detection with LLM Enhanced Semantics Mining",
    "volume": "review",
    "abstract": "Large language models (LLMs) have emerged as valuable tools for enhancing textual features in various text-related tasks. In this paper, we assess the effectiveness of news embeddings from ChatGPT for detecting fake news and showcase that despite their initial performance slightly surpassing the pre-trained BERT model, they still lag behind the state-of-the-arts. This shortfall is attributed to the reliance on tokenized training text, which misses the complex narratives and subtleties that are crucial for identifying fake news. To capture these nuances, we probe the high-level semantic relations among the news pieces, real entities, and topics, which are modeled as a heterogeneous graph with nodes denoting different items and the relations are represented as edges. We then propose a Generalized Page-Rank model and a consistent learning criteria for mining the local and global semantics centered on each news piece through the adaptive propagation of features across the graph. Our model shows new state-of-the-art performance on five benchmark datasets and the effectiveness of the key ingredients is supported by extensive analysis. Our code is available at \\url{https://github.com/LEG4FD/LEG4FD}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W2GDC0NiVou": {
    "title": "Graph Memory-based Editing for Large Language Models",
    "volume": "review",
    "abstract": "The information within Large Language Models (LLMs) quickly becomes outdated, prompting the development of various techniques to perform knowledge editing with new facts. However, existing knowledge editing methods often overlook the interconnected nature of facts, failing to account for the ripple effects caused by changing one piece of information. In our study, we present GMeLLo (Graph Memory-based Editing for Large Language Models), a simple yet effective memory-based method that transitions the Multi-hop Question Answering for Knowledge Editing (MQuAKE) task into a Knowledge-based Question Answering (KBQA) framework. GMeLLo stores all relevant facts externally in a Knowledge Graph (KG) and directs the language model to engage in semantic parsing. This involves translating natural language questions into formal queries to extract information from the KG. Notably, our method eliminates the need to fine-tune LLMs, ensuring that edited facts do not corrupt other information. In our experimental findings, we noted a noteworthy enhancement of GMeLLo in comparison to state-of-the-art model editors on the MQuAKE benchmarkâ€”a dataset tailored for multi-hop question answering, particularly evident when editing multiple facts simultaneously",
    "checked": false,
    "id": "b355b6a89ff412d61cabc83dd30fd42c16ed50e9",
    "semantic_title": "massive editing for large language models via meta learning",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=TamFHtCtRuq": {
    "title": "Promoting Structure-awareness of Large Language Model for Graph-to-text Generation",
    "volume": "review",
    "abstract": "Recent advancement of Large Language Models (LLMs) has remarkably pushed the boundaries towards artificial general intelligence (AGI), with their exceptional generation and reasoning abilities. Despite this progress, a critical gap remains in employing LLMs to proficiently understand graph data. In this paper, we propose a new framework, named StructLLM to enhance the graph capabilities of large language models. Our framework first uses a structure-aware pre-training stage to pre-train a graph model to capture the structural information. Subsequently, we introduce four structure-aware instruction tasks to train a graph-to-text projector which bridges the domain gap between graph and text. Finally, we fine-tune our system on the AMR-to-text and Kg-to-text generation tasks.Experimental results that our model obtains significantly better results compared to fine-tuned LLMs, surpassing state-of-the-art systems.Further analysis shows that our model can better process complex graphs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6g7dmrdjym": {
    "title": "A Curious Case of Searching for the Correlation between Training Data and Adversarial Robustness of Transformer Textual Models",
    "volume": "review",
    "abstract": "Existing works have shown that fine-tuned textual transformer models achieve state-of-the-art prediction performances but are also vulnerable to adversarial text perturbations. Traditional adversarial evaluation is often done \\textit{only after} fine-tuning the models and ignoring the training data. In this paper, we want to prove that there is also a strong correlation between training data and model robustness. To this end, we extract 13 different features representing a wide range of input fine-tuning corpora properties and use them to predict the adversarial robustness of the fine-tuned models. Focusing mostly on encoder-only transformer models BERT and RoBERTa with additional results for BART, ELECTRA and GPT2, we provide diverse evidence to support our argument. First, empirical analyses show that (a) extracted features can be used with a lightweight classifier such as Random Forest to effectively predict the attack success rate and (b) features with the most influence on the model robustness have a clear correlation with the robustness. Second, our framework can be used as a fast and effective additional tool for robustness evaluation since it (a) saves 30x-193x runtime compared to the traditional technique, (b) is transferable across models, (c) can be used under adversarial training, and (d) robust to statistical randomness. Our code will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3nNei7NFHM": {
    "title": "Conservative Knowledge Graph Completion Using Semantically Enriched Information",
    "volume": "review",
    "abstract": "In this paper, we present a novel conservative completion approach for Knowledge Graphs (KGs), designed to address the shortcomings of current knowledge completion methods, particularly their failure to guarantee the accuracy of completion results. Our method uniquely utilizes semantically enriched information inherent in KGs to construct a reasoner based on description logic. By integrating this reasoner with Link Prediction (LP) models, we ensure the correctness of the knowledge completion. Experimental findings show that a substantial proportion of predictions from diverse LP models can undergo conservative completion. Additionally, the volume of conservatively completable results escalates with the increase in semantically enriched information in the KGs",
    "checked": false,
    "id": "8535600e8e0e1477e7fe0aa0e6ee9eabcabeb31b",
    "semantic_title": "knowledge graph completion method combined with adaptive enhanced semantic information",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UFfwWqhy568": {
    "title": "Length Generalization of Causal Transformers without Position Encoding",
    "volume": "review",
    "abstract": "Generalizing to longer sentences is important for recent Transformer-based language models. Besides algorithms manipulating explicit position features, the success of Transformers without position encodings (NoPE) provides a new way to overcome the challenge. In this paper, we study the length generalization property of NoPE. We find that NoPE can extend to longer sequences than the commonly used explicit position encodings. Moreover, we propose a parameter-efficient tuning for searching attention heads' best temperature hyper-parameters, which further expands NoPE's context size. Experiments on long sequence language modeling and the synthetic passkey retrieval task show that NoPE can achieve competitive performances with state-of-the-art length generalization algorithms",
    "checked": true,
    "id": "37b1ce339678f63315c82841c6824dd739269636",
    "semantic_title": "length generalization of causal transformers without position encoding",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=75M475e-uM4": {
    "title": "BengaliLLama: An Instruction Following LLaMA Model for Bengali",
    "volume": "review",
    "abstract": "In the field of Large Language Models (LLMs), significant advancements have predominantly focused on a limited set of languages, raising concerns in linguistically diverse regions such as India, where a wide array of regional languages are spoken, and the majority of individuals communicate in native languages other than English. Addressing this limitation, our study introduces BengaliLlama, a model tailored for Bengali, the world's seventh most widely spoken language. This research leverages a dataset of 252K Bengali instructions, translated and manually validated from various open-source resources, and employs the LoRA architecture and LLaMA for fine-tuning. The resulting BengaliLlama model demonstrates enhanced proficiency in processing and responding to instruction-based queries in Bengali. The study discussed comprehensive evaluations that will motivate various Indic Model studies in the future. BengaliLlama will be made available for research and non-commercial use, contributing to the broader goal of creating more linguistically diverse and accessible AI technologies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2eUPjC6mVOU": {
    "title": "EIT: Enhanced Interactive Transformer",
    "volume": "review",
    "abstract": "Two principles: the \\textit{complementary principle} and the \\textit{consensus principle} are widely acknowledged in the literature of multi-view learning. However, the current design of Multi-head self-attention, an instance of multi-view learning, prioritizes the complementarity while ignoring the consensus. To address this problem, we propose an enhanced multi-head self-attention (EMHA). First, to satisfy the \\textit{complementary principle}, EMHA removes the one-to-one mapping constraint among queries and keys in multiple subspaces and allows each query to attend to multiple keys. On top of that, we develop a method to fully encourage consensus among heads by introducing two interaction models, namely Inner-Subspace Interaction and Cross-Subspace Interaction. Extensive experiments on a wide range of language tasks (e.g., machine translation, abstractive summarization and grammar correction, language modeling), show its superiority, with a very modest increase in model size",
    "checked": true,
    "id": "2e742c6ad6d71a2afea9a8f113b9d9c02bcdaf9a",
    "semantic_title": "eit: enhanced interactive transformer",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=odxU7UlrSzc": {
    "title": "Logical Reasoning over Natural Language as Knowledge Representation: A Survey",
    "volume": "review",
    "abstract": "Logical reasoning is central to human cognition and intelligence. Past research of logical reasoning within AI uses formal language as knowledge representation and symbolic reasoners. However, reasoning with formal language has proved challenging~(e.g., brittleness and knowledge-acquisition bottleneck). This paper provides a comprehensive overview on a new paradigm of logical reasoning, which uses natural language as knowledge representation and pretrained language models as reasoners, including philosophical definition and categorization of logical reasoning, advantages of the new paradigm, benchmarks and methods, challenges of the new paradigm, possible future directions, and relation to related NLP fields. This new paradigm is promising since it not only alleviates many challenges of formal representation but also has advantages over end-to-end neural methods",
    "checked": true,
    "id": "63d0e5a8f195b1453006781d4d8a4eb7262652d9",
    "semantic_title": "logical reasoning over natural language as knowledge representation: a survey",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=cF-wzBOy9u": {
    "title": "Debatrix: Advancing Automatic Debate Adjudication with Chronological and Multi-dimensional Analysis",
    "volume": "review",
    "abstract": "How can we construct an automated debate judge to assist with evaluating an extensive, fervent, multi-turn debate? This task is challenging, as judging a debate involves grappling with lengthy texts, intricate argument relationships, and multi-dimensional assessments, while current research mainly focuses on short dialogues, rarely touching upon the evaluation of an entire debate.In this paper, by leveraging Large Language Models (LLMs), we propose Debatrix, which makes the analysis and assessment of multi-turn debates more aligned with majority preferences. Specifically, Debatrix features a horizontal chronological workflow and a vertical multi-dimensional evaluation collaboration.To align with real-world debate scenarios, we introduced DebateArt and DebateCompetition benchmarks, comparing our system's performance to actual debate outcomes. The findings indicate a notable enhancement over directly using LLMs for debate evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8MM6Gkwrv7": {
    "title": "NewsEdits 2.0: Learning the Intentions Behind Updating News",
    "volume": "review",
    "abstract": "News articles are often published and republished. Their revision histories give us insights into the journalistic process and can assist in the development of computational journalism tools. They also make it challenging for large language models (LLMs) trained with news to reconcile conflicting, updating information. In this work, we release \\textit{NewsEdits 2.0}, based on \\newcite{spangher2022newsedits}'s large corpus of news article revision histories. \\textit{NewsEdits 2.0} introduces a taxonomy of edit-intention categories, including coarse categories: Fact Updates, Stylistic Updates, Contextual/Narrative Changes and XX finer-grained categories. In the first part of our work, we collect ZZ human-labeled annotations on 600 revision-pairs, and show that we can model these categories using small, scalable ensemble models with high F1 score (YY). In the second part of our work we seek to model, given old versions of news articles: \\textit{will this article have fact updates? Will it have a style updates?} We show that, while pretrained LLMs fail at this task, fine-tuning can boost performance to YY accuracy. Finally, we show via a novel use-case, \\textit{Question Answering with outdated references}, that \\textit{NewsEdits 2.0} should play an important role for users",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jk4zg8ET7e": {
    "title": "Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation",
    "volume": "review",
    "abstract": "In this work, we propose a novel approach called Distillation Contrastive Decoding to enhance the reasoning capabilities of Large Language Models (LLMs) during inference. Different from previous approaches that used smaller amateur models or analyzed differences in hidden states, DCD leverages contrastive chain-of-thought prompting and advanced distillation techniques, such as Dropout and Quantization, to address the limitations of Contrastive Decoding, which often require both an expert and an amateur model, thereby increasing computational demands. By integrating contrastive prompts with distillation, DCD obviates the need for an amateur model and reduces memory usage. Our evaluations show that DCD significantly improves LLM performance across various reasoning benchmarks, outperforming existing methods and achieving state-of-the-art results in both GSM8K and StrategyQA",
    "checked": true,
    "id": "02ca7c0a9938e7f5a6e2c8b4df8a92c5bdbc283c",
    "semantic_title": "distillation contrastive decoding: improving llms reasoning with contrastive decoding and distillation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=VupxYD9pqKX": {
    "title": "Branches Switching Based Pre-Training Strategy for Version Iteration of Large Language Models",
    "volume": "review",
    "abstract": "Due to the continuous emergence of online data, version iteration has become an indispensable requirement for Large Language Models (LLMs), which exacerbates the training cost of LLMs. Hence, one of the pivotal challenges for LLMs is how to reduce the total training cost across different versions. To achieve a better balance between the pre-training performance and training cost, we conduct a systematic investigation into the impact of various learning rate schedules. Extensive experiments on commonly used learning rate schedules show that these approaches primarily focus on the performance of LLMs of the current version, but overlook the mutual influence of training processes of LLMs across different versions. To address the above issue, we design a pre-training strategy called Branches Switching based Pre-Training for the training of LLMs across different versions. Compared with pre-training LLMs of different versions from scratch, our strategy reduces the total training cost to 58\\% while maintaining optimal pre-training performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Siz2-rPpmk": {
    "title": "Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models",
    "volume": "review",
    "abstract": "Though advanced in understanding visual information with human languages, Large Vision-Language Models (LVLMs) still suffer from multimodal hallucinations. A natural concern is that during multimodal interaction, the generated hallucinations could influence the LVLMs' subsequent generation. Thus, we raise a question: $\\textit{When presented with a query relevant to the previously generated hallucination, will LVLMs be misled and respond incorrectly, even though the ground visual information exists?}$ To answer this, we propose a framework to evaluate LVLMs' behaviors when encountering generated hallucinations, where LVLMs are required to answer specific visual questions with a curated hallucinatory conversation. Crucially, our experiment shows that the performance of LVLMs drops by $31\\%$ at least, indicating that LVLMs are prone to accept the generated hallucinations and make false claims that they would have not supported without distractions, which we term as $\\textit{Multimodal Hallucination Snowballing}$. To mitigate this issue, we further propose a training-free method called $\\textit{Residual Visual Decoding},$ where we revise the output distribution of LVLMs that are derived from the residual visual input, which provides models with direct access to the visual information. Experiments show that our method can mitigate more than $24%$ of the snowballed multimodal hallucination while maintaining capabilities",
    "checked": true,
    "id": "968bd4cf71c66bb153527778836e54c85ee6162c",
    "semantic_title": "investigating and mitigating the multimodal hallucination snowballing in large vision-language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9TK0eL6PIG": {
    "title": "Legal Judgment Reimagined: PredEx and the Rise of Intelligent AI Interpretation in Indian Courts",
    "volume": "review",
    "abstract": "This paper presents \\texttt{PredEx}, the largest annotated dataset for legal judgment prediction and explanation in the Indian context. This unique corpus enhances the training and evaluation of AI models in legal analysis. Our work innovates by applying instruction tuning to Large Language Models (LLMs), significantly improving their predictive accuracy and explanatory depth for legal judgments. We employed various transformer-based models, tailored for both general and Indian legal contexts. Through a combination of lexical, semantic, and expert assessments, we demonstrate the effectiveness of our approach. Despite challenges like handling extensive documents and reducing hallucinations, our results are promising, indicating a significant leap forward in AI-assisted legal judgment prediction and explanation. This study not only contributes a groundbreaking dataset but also paves the way for future advancements in AI-assisted legal judgment prediction and explanation",
    "checked": true,
    "id": "c8684d2b4da1fb818c0d2ca043a323e3c8c1e536",
    "semantic_title": "legal judgment reimagined: predex and the rise of intelligent ai interpretation in indian courts",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y2LXmPv5Dk": {
    "title": "Fill-in Language Models",
    "volume": "review",
    "abstract": "Language models have become the backbone of today's AI systems. However, their predominant left-to-right (L2R) generation limits the use of bidirectional context, which is essential for tasks that involve filling text in the middle. We propose the Fill-in Language Model (FiLM) that allows for flexible generation at any position without adhering to a specific generation order. Its training extends the masked language modeling objective by adopting varying mask probabilities sampled from the Beta distribution to enhance the generative capabilities.During inference, FiLM can seamlessly insert missing phrases, sentences, or paragraphs, ensuring that the outputs are fluent and are coherent with the surrounding context. In both automatic and human evaluations, FiLM outperforms existing infilling methods that rely on L2R language models trained on rearranged text segments. FiLM is easy to implement and can be either trained from scratchor fine-tuned from a L2R language model. Notably, as the model size grows, FiLM's perplexity approaches that of strong L2R language models of similar sizes, indicating FiLM's scalability and potential as a large language model",
    "checked": false,
    "id": "2d79d2ea57c87912bd46ae8e8bda1b26c4773855",
    "semantic_title": "film: fill-in language models for any-order generation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=d5GKe5XUpPt": {
    "title": "RAINPOEM: Retrieval-augmented In-context knowledge post editing on massive-editing memory in a transformer",
    "volume": "review",
    "abstract": "Although we have witnessed the powerful capabilities of large language models (LLMs) in the past, it is undeniable that in the current timeline, there are still unknown errors or information. %%Additionally, training a new LLM from scratch is a costly option. In response to these challenges, a series of methods injecting new facts into the LLM have been proposed. However, adjusting model weights to accommodate changes in information is likely to trigger a cascade of reactions, including adverse effects on the performance of knowledge outside the scope of editing. In this study, we introduce an \\textbf{RAINPOEM} -- \\textbf{R}etrieval-\\textbf{a}ugmented \\textbf{In}-context knowledge \\textbf{po}st \\textbf{e}diting on massive-editing \\textbf{m}emory -- that is designed for re-evaluating the results of massive-edited models by recalling their editing facts through retrieval and leveraging the in-context reasoning capabilities of LLMs to facilitate more robust responses. Our objective is based on the current state-of-the-art model editing methods to promote the edited model to further generate more accurate answers to provide more effective correction examples. In our experiments, the results affirm the key role of retrieval and in-context learning in post-edited LLM and suggest that combining parameter editing methods with retrieval augmented represents a future direction for model editing. Our code are publicly available at \\url{https://github.com/XXX/XXX}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d_py2gO1HM": {
    "title": "ALaRM: Align Language Models via Hierarchical Rewards Modeling",
    "volume": "review",
    "abstract": "We introduce \\textsc{ALaRM}, a novel framework modeling hierarchical rewards in reinforcement learning from human feedback (RLHF), which is designed to enhance the alignment of large language models (LLMs) with human preferences. The framework addresses the limitations of current alignment approaches, which often struggle with the inconsistency and sparsity of human supervision signals, by integrating holistic rewards with aspect-specific rewards. This integration enables more precise and consistent guidance of language models towards desired outcomes, particularly in complex and open text generation tasks. By employing a methodology that filters and combines multiple rewards based on their consistency, the framework provides a robust mechanism for improving model alignment. We validate our approach through applications in long-form question answering and machine translation tasks, employing \\code{gpt-3.5-turbo-1106} for pair-wise comparisons, and demonstrate significant improvements over existing baselines. Our work underscores the effectiveness of hierarchical rewards modeling in refining LLM training processes for better human preference alignment.We release our codes at \\url{https://ALaRM-anonymized.github.io}",
    "checked": true,
    "id": "4146b447187e1a09b736564854007c403f986c69",
    "semantic_title": "alarm: align language models via hierarchical rewards modeling",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=tfkLn_vvKPi": {
    "title": "Linear Representation of Numeric Attributes in Language Models",
    "volume": "review",
    "abstract": "Language models (LMs) can express factual knowledge involving numeric attributes such as \\emph{Karl Popper was born in 1902}.However, if and how such knowledge is encoded in the model's internal representations is poorly understood.Here, we introduce a simple method for finding and editing representations of numeric attributes such as an entity's birth year.Empirically, we find low-dimensional subspaces that encode numeric attributes in an interpretable and easily editable fashion.When editing representations along directions in these subspaces, LM output changes accordingly.For example, by patching activations along a \"birthyear\" direction we can make the LM express an increasingly late birthyear, e.g., \"Karl Popper was born in 1913\", \"Karl Popper was born in 1944\", \"Karl Popper was born in 1975\".Attribute-encoding directions exist across several numeric properties in all models under consideration, suggesting the intruiging possibility that linear representations of numeric attributes reliably emerge during LM pretraining",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s3iyNnOkg9": {
    "title": "Ask Again, Then Fail: Large Language Models' Vacillations in Judgement",
    "volume": "review",
    "abstract": "We observe that current conversational language models often waver in their judgements when faced with follow-up questions, even if the original judgement was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a \\textsc{Follow-up Questioning Mechanism} along with two metrics to quantify this inconsistency, confirming its widespread presence in current language models. To mitigate this issue, we explored various prompting strategies for closed-source models; moreover, we developed a training-based framework \\framework that teaches language models to maintain their judgements through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of models",
    "checked": true,
    "id": "ff3a22641d21e9725efb5e79f22094300b689ab7",
    "semantic_title": "ask again, then fail: large language models' vacillations in judgement",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=LCB5a8yPrb": {
    "title": "RECOST: External Knowledge Guided Data-efficient Instruction Tuning",
    "volume": "review",
    "abstract": "In the current landscape of large language models (LLMs), the process of instruction tuning serves as an essential step. Considering the high computing power overhead, data-efficient instruction tuning was proposed to reduce the training data size in this process, aiming at selecting high-quality instructional data. Nevertheless, we argue that most current data-efficient instruction-tuning methods are highly dependent on the quality of the original instruction-tuning dataset. When it comes to datasets synthesized by LLMs, a common scenario in this field, dirty samples will even be selected with a higher probability than other samples. To address these challenges, we utilized external knowledge (relevant examples or paragraphs) to evaluate those samples synthesized by LLMs with an in-context-based relative predictive entropy. Based on the new metric, we proposed a framework, dubbed as \\textbf{RECOST}, which integrates external-knowledge-base re-ranking and diversity-consistent sampling into a single pipeline. Through extensive experiments on several synthetic datasets (Alpaca and Alpaca-gpt4), we demonstrate the effectiveness of our method and achieve even better results with only \\textbf{1\\%} of the full dataset",
    "checked": true,
    "id": "62863fbcff6bc411bd83dc50d3e1bcbe72f651ef",
    "semantic_title": "recost: external knowledge guided data-efficient instruction tuning",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=EYTqRz83eY": {
    "title": "Word Embeddings Are Steers for Language Models",
    "volume": "review",
    "abstract": "Language models (LMs) automatically learn word embeddings during pre-training on language corpora. Although word embeddings are usually interpreted as feature vectors for individual words, their roles in language model generation remain underexplored. In this work, we theoretically and empirically revisit output word embeddings and find that their linear transformations are equivalent to steering language model generation styles. We name such steers LM-Steers and find them existing in LMs of all sizes. It requires learning parameters equal to 0.2\\% of the original LMs' size for steering each style. On tasks such as language model detoxification and sentiment control, LM-Steers can achieve comparable or superior performance compared with state-of-the-art controlled generation methods while maintaining a better balance with generation quality. The learned LM-Steer serves as a lens in text styles: it reveals that word embeddings are interpretable when associated with language model generations, and can highlight text spans that most indicate the style differences. A LM-Steer is transferrable between different language models by an explicit-form calculation. One can also continuously steer LMs simply by scaling the LM-Steer, or compose multiple LM-Steers by adding their transformations. We will make our code available to the research community following publication",
    "checked": true,
    "id": "0f900f07158d0abf9d7a419c392e33531f3eebae",
    "semantic_title": "word embeddings are steers for language models",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=BYPRDa1_9w": {
    "title": "MAVEN-ARG: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation",
    "volume": "review",
    "abstract": "Understanding events in texts is a core objective of natural language understanding, which requires detecting event occurrences, extracting event arguments, and analyzing inter-event relationships. However, due to the annotation challenges brought by task complexity, a large-scale dataset covering the full process of event understanding has long been absent. In this paper, we introduce MAVEN-Arg, which augments MAVEN datasets with event argument annotations, making the first all-in-one dataset supporting event detection, event argument extraction (EAE), and event relation extraction. As an EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive schema covering 162 event types and 612 argument roles, all with expert-written definitions and examples; (2) a large data scale, containing 98,591 events and 290,613 arguments obtained with laborious human annotation; (3) the exhaustive annotation supporting all task variants of EAE, which annotates both entity and non-entity event arguments in document level. Experiments indicate that MAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary large language models (LLMs). Furthermore, to demonstrate the benefits of an all-in-one dataset, we preliminarily explore a potential application, future event prediction, with LLMs. MAVEN-Arg and our baseline codes will be publicly released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=arZ7ceNze3y": {
    "title": "ANUBIS: For a Future where AI mirrors the best of us, free from the shadows of bias",
    "volume": "review",
    "abstract": "Bias identification and mitigation is an important research problem with far-reaching societal impact. Though there exist datasets for bias mitigation, they offer superficial debiased gold-standard. In the scope of the paper we present a high-quality dataset (ANUBIS) for evaluation of debiasing across bias types in conjunction with LLMs and human annotators. In addition, we leverage advanced Large Language Models (LLMs) for automatic and effective bias detection and mitigation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y7ZVLuwGpK": {
    "title": "Beyond Orthography: Automatic Recovery of Short Vowels and Dialectal Sounds in Arabic",
    "volume": "review",
    "abstract": "This paper presents a novel Dialectal Sound and Vowelization Recovery framework, designed to recognize borrowed and dialectal sounds within phonologically diverse and dialect-rich languages, that extends beyond its standard orthographic sound sets. The proposed framework utilized quantized sequence of input with(out) continuous pretrained self-supervised representation. We show the efficacy of the pipeline using limited data for Arabic, a dialect-rich language containing more than $22$ major dialects. Phonetically correct transcribed speech resources for dialectal Arabic is scare. Therefore, we introduce ArabVoice15, a first of its kind, curated test set featuring $5$ hours of dialectal speech across $15$ Arab countries, with phonetically accurate transcriptions, including borrowed and dialect-specific sounds. We described in detail the annotation guideline along with the analysis of the dialectal confusion pairs. Our extensive evaluation includes both subjective -- human perception tests and objective measures. Our empirical results, reported with three test sets, show that with only one and half hours of training data, our model improve character error rate by $\\approx 7\\%$ in ArabVoice15 compared to the baseline",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BVqrpSzQJB": {
    "title": "Self-Para-Consistency: Improving Reasoning Tasks at Low Cost for Large Language Models",
    "volume": "review",
    "abstract": "Recently, the self-consistency decoding strategy has shown the ability to improve performance for complex reasoning tasks with large language models (LLMs). However, the costs may be high because the sampling process of the strategy will generate some low-probability text resulting in low-quality reasoning paths. As a consequence, it requires a relatively large sampling number to obtain good aggregation performance. In this paper, we propose an alternative strategy, \\emph{self-para-consistency}. It first generates multiple paraphrases for each test question, then generates reasoning paths for the original and all the paraphrased questions based on greedy decoding, and finally selects the most consistent answer. Since all the candidate paths have relatively high probabilities, the sampling number could be much smaller than the self-consistency strategy. Extensive experiments on complex reasoning datasets demonstrate the effectiveness of our method in reducing the sampling number",
    "checked": false,
    "id": "f236d6f9c376dc453d7db11d15c8d9bc519adf65",
    "semantic_title": "teaching-assistant-in-the-loop: improving knowledge distillation from imperfect teacher models in low-budget scenarios",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=qP7QojL__D5": {
    "title": "LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin",
    "volume": "review",
    "abstract": "Supervised fine-tuning (SFT) is a crucial step for large language models (LLMs), enabling them to align with human instructions and enhance their capabilities in downstream tasks. Increasing substantially instruction data is a direct solution to align the model with a broader range of downstream tasks or notably improve its performance on a specific task. However, we find that large-scale increases in instruction data can damage the world knowledge previously stored in LLMs. To address this challenge, we propose LoRAMoE, a novelty framework that introduces several low-rank adapters (LoRA) and integrates them by using a router network, like a plugin version of Mixture of Experts (MoE). It freezes the backbone model and forces a portion of LoRAs to focus on leveraging world knowledge to solve downstream tasks, to alleviate world knowledge forgetting. Experimental results show that, as the instruction data increases, LoRAMoE can significantly improve the ability to process downstream tasks, while maintaining the world knowledge stored in the LLM. The code will be made available upon publication",
    "checked": true,
    "id": "9d793e542b757f234431d209e711c6ef88aa29de",
    "semantic_title": "loramoe: alleviate world knowledge forgetting in large language models via moe-style plugin",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=SDAHg_6kUU": {
    "title": "HAF-RM: A Hybrid Alignment Framework for Reward Model Training",
    "volume": "review",
    "abstract": "As a quantifiable sentence-wise evaluation method, reward model is becoming increasingly important in alignment, assessment, and even data construction for large language models. Excluding some common tricks like soft-label and margin, most researches share the same training framework proposed by OpenAI, focusing on further improving the performance of reward models from the data aspect. However, we propose a hybrid alignment framework for reward model training (HAF), a new reward model training paradigm which",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CN_2JhzQZb": {
    "title": "Diving Deep into the Motion Representation of Video-Text Models",
    "volume": "review",
    "abstract": "Videos are more informative than images because they capture the dynamics of the scene. By representing motion in videos, we can represent dynamic activities. In this work, we introduce motion descriptions generated by GPT-4 on three action datasets that capture fine-grained motion descriptions of activities. We evaluated several video-text models on the task of retrieval of motion descriptions. We found that they fall far behind human expert performance on two action datasets, raising this question: Do video-text models understand motion in videos? To address this, we introduce a method of improving motion understanding in video-text models by utilizing motion descriptions. This method proves to be effective on two action datasets for the motion description retrieval task. The results draw attention to the need for quality captions involving fine-grained motion information in existing datasets and demonstrate the effectiveness of the proposed pipeline in understanding fine-grained motion during video-text retrieval",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OKBPppzjlF": {
    "title": "DistillMIKE: Editing Distillation of Massive In-Context Knowledge Editing in Large Language Models",
    "volume": "review",
    "abstract": "Among the recently emerged knowledge editing methods, in-context knowledge editing (IKE) $\\cite{IKE}$ has shown respectable abilities on knowledge editing in terms of generalization and specificity. Noting the promising advantages but unexplored issues of IKE, we propose $\\textbf{DistillMIKE}$ as a novel extension of IKE, i.e., editing $\\textbf{distill}$ation of ``$\\textbf{M}$assive'' $\\textbf{I}$n-context $\\textbf{K}$nowledge $\\textbf{E}$diting in large language models (LLMs), mainly consisting of two expansions; 1) $\\textit{Massive in-context knowledge editing (MIKE)}$, which extends IKE to a massive editing task, aiming to inject not a single edit but a set of massive edits to LLMs; To preserve specificity, our key novel extension is a ``selective'' retrieval augmentation, where the retrieval-augmented IKE is only applied to ``in-scope'' examples, whereas the unedited model without IKE is employed for ``out-of-scope'' ones. 2) $\\textit{Editing distillation }$of MIKE using low-rank adaptation (LoRA), which distills editing abilities of MIKE to parameters of LLMs in a manner of eliminating the need of lengthy in-context demonstrations, thus removing the computational overhead encountered at the inference time. Experimental results on the zsRE and CounterFact datasets demonstrate that MIKE shows the state-of-the-art perfomrances and DistilMIKE show comparable performances with MIKE. Our code is available at $\\url{https://github.com/xxxx/xxxx}$",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_o1midjnSw": {
    "title": "Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback",
    "volume": "review",
    "abstract": "Recent advancements in large language models have influenced the development of video large multimodal models (VLMMs). Previous approaches for VLMMs involve Supervised Fine-Tuning (SFT) with instruction-tuned datasets, integrating LLM with visual encoders, and additional learnable parameters. Here, aligning video with text, and vice versa, remains a challenge, primarily due to the insufficient quality and quantity of multimodal instruction-tune data compared to that of text-only. This discrepancy often results in alignments that poorly ground the video content. % that are not well-grounded in the video content.To address this, we present a novel alignment strategy that employs a multimodal AI system equipped with Reinforcement Learning from AI Feedback (RLAIF), providing self-preference feedback to refine itself and facilitating the alignment of video and text modalities. Our approach uniquely integrates detailed video descriptions as context into a multimodal AI system during the preference feedback generation to enrich the understanding of video content, a process we call \\emph{context-aware reward modeling}. Empirical evaluations on various video benchmarks demonstrate that our \\method outperforms existing approaches, including the SFT model. Public release of our code, models, and datasets is forthcoming",
    "checked": true,
    "id": "badaaee23c1ee33fa747165acb179662598ec6bd",
    "semantic_title": "tuning large multimodal models for videos using reinforcement learning from ai feedback",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=S4h03YaQ4_": {
    "title": "Real World Conversational Entity Linking Requires More Than Zero-Shots",
    "volume": "review",
    "abstract": "Entity linking (EL) in conversations faces notable challenges in practical applications, primarily due to scarcity of entity-annotated conversational datasets and sparse knowledge bases (KB) containing domain-specific, long-tail entities. We designed targeted evaluation scenarios to measure the efficacy of EL models under resource constraints. Our evaluation employs two KBs: Fandom, exemplifying real-world EL complexities, and the widely used Wikipedia. First, we assess EL models' ability to generalize to a new unfamiliar KB using Fandom and a novel zero-shot conversational entity linking dataset that we curated based on Reddit discussions on Fandom entities. We then evaluate the adaptability of EL models to conversational settings without prior training. Our results indicate that current zero-shot EL models falter when introduced to new, domain-specific KBs without prior training, significantly dropping in performance.Our findings reveal that previous evaluation approaches fall short of capturing real-world complexities for zero-shot EL, highlighting the necessity for new approaches to design and assess conversational EL models to adapt to limited resources. The evaluation frame-work and dataset proposed are tailored to facilitate this research",
    "checked": false,
    "id": "c2969b8c48e357df771fd692fca7bb5320274074",
    "semantic_title": "improving few-shot and zero-shot entity linking with coarse-to-fine lexicon-based retriever",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CY3HFhaD8P": {
    "title": "LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay",
    "volume": "review",
    "abstract": "This paper explores the open research problem of understanding the social behaviors of LLM-based agents. Using Avalon as a testbed, we employ system prompts to guide LLM agents in gameplay. While previous studies have touched on gameplay with LLM agents, research on their social behaviors is lacking. We propose a novel framework, tailored for Avalon, features a multi-agent system facilitating efficient communication and interaction. We evaluate its performance based on game success and analyze LLM agents' social behaviors. Results affirm the framework's effectiveness in creating adaptive agents and suggest LLM-based agents' potential in navigating dynamic social interactions. By examining collaboration and confrontation behaviors, we offer insights into this field's research and applications",
    "checked": true,
    "id": "ff406e2ab8fdcce6b051cad1ead794c928440f77",
    "semantic_title": "llm-based agent society investigation: collaboration and confrontation in avalon gameplay",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=7kZJv0DNsB": {
    "title": "Open-world Multi-label Text Classification with Extremely Weak Supervision",
    "volume": "review",
    "abstract": "This work focuses on a new challenging problem, open-world multi-label text classification under extremely weak supervision, where only raw documents are provided without any labels or ground-truth label space. The multi-label nature makes the existing (hard-)clustering-based methods ineffective. We observe that (1)~most documents have a dominant class covering the majority of content and (2) long-tail labels would appear in some documents as dominant class. Following these observations, we propose a novel method, \\our, to discover a comprehensive label space and construct a multi-label classifier. Specifically, we start with a reasonable subset of all the documents and prompt a large language model (LLM) for their most dominant keyphrases to obtain an initial set of labels. We then leverage a zero-shot multi-label classifier, identifying the documents with lower predicted scores and revisiting the keyphrases in those documents for more long-tail labels. Later, we include these long-tail labels into the label set and reiterate this process. Extensive experiments demonstrate that \\our exhibits a remarkable 40\\% increase on the AAPD dataset in ground-truth label space coverage compared to traditional topic modeling methods. Additionally, it achieves higher accuracy in zero-shot multi-label text classification",
    "checked": true,
    "id": "beab90db1104da045823838fbf902a9495b06af5",
    "semantic_title": "open-world multi-label text classification with extremely weak supervision",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ub7ePB3iTDH": {
    "title": "Deepfake Defense: Constructing and Evaluating a Specialized Urdu Deepfake Audio Dataset",
    "volume": "review",
    "abstract": "Deepfakes, particularly in the auditory domain, have become a significant threat, necessitating the development of robust countermeasures. This paper addresses the escalating challenges posed by deepfake attacks on Automatic Speaker Verification (ASV) systems. We present a novel Urdu deepfake audio dataset for deepfake detection, focusing on two spoofing attacks â€“ Tacotron and VITS TTS. The dataset construction involves careful consideration of phonemic cover and balance and comparison with existing corpora like PRUS and PronouncUR. Evaluation with AASIST-L model shows EERs of 0.495 and 0.524 for VITS TTS and Tacotron-generated audios, respectively, with variability across speakers Further, this research implements a detailed human evaluation, incorporating a user study to gauge whether people are able to discern deepfake audios from real (bonafide) audios. The ROC curve analysis shows an area under the curve (AUC) of 0.63, indicating that individuals demonstrate a limited ability to detect deepfakes (approximately 1 in 3 fake audio samples are regarded as real). Our work contributes a valuable resource for training deepfake detection models in low-resource languages like Urdu, addressing the critical gap in existing datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NwEfZckO3qk": {
    "title": "Divide-or-Conquer? Which Part Should You Distill Your LLM?",
    "volume": "review",
    "abstract": "Recent methods have demonstrated that Large Language Models (LLMs) can solve reasoning tasks better when they are encouraged to solve subtasks of the main task first.In this paper we devise a similar strategy that breaks down reasoning tasks into a \\textit{problem decomposition} phase and a \\textit{problem solving} phase and show that the strategy is able to outperform a single stage solution. Further, we hypothesize that the decomposition should be easier to distill into a smaller model compared to the problem solving because the latter requires large amounts of domain knowledge while the former only requires learning general problem solving strategies.We propose methods to distill these two capabilities and evaluate their impact on reasoning outcomes and inference cost.We find that we can distill the problem decomposition phase and at the same time achieve good generalization across tasks, datasets, and models.However, it is harder to distill the problem solving capability without losing performance and the resulting distilled model struggles with generalization.These results indicate that by using smaller, distilled problem decomposition models in combination with problem solving LLMs we can achieve reasoning with cost-efficient inference and local adaptation",
    "checked": true,
    "id": "5e7d4bb5431bc91d0ffd1b4e1575d7227021eaf8",
    "semantic_title": "divide-or-conquer? which part should you distill your llm?",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=7atXKldh-r": {
    "title": "Learning Fine-Grained Grounded Citations for Attributed Large Language Models",
    "volume": "review",
    "abstract": "Despite the impressive performance on information-seeking tasks, large language models (LLMs) still struggle with hallucinations. Attributed LLMs, which augment generated text with in-line citations, demonstrate potential in mitigating hallucinations and improving verifiability. Nonetheless, current attributed LLMs suffer from suboptimal citation quality due to their reliance on in-context learning or post-hoc retrieval, lacking a built-in attribution mechanism. Moreover, the practice of merely citing document identifiers falls short in aiding users to pinpoint specific supporting evidence. To bridge these gaps, this work introduces FRONT, a training framework that advances the verification process in attributed LLMs through Fine-grained grounded citations. It equips LLMs with the ability to first anchor in fine-grained supporting quotes, which then guide the generation of attributed answers. Grounded quotes not only elevate LLM attribution quality but also serve as a mechanism for fine-grained verification, significantly enhancing information traceability. Experiments on the ALCE benchmark demonstrate the efficacy of FRONT in generating superior grounded responses and highly supportive citations. With LLaMA-2-7B, the framework significantly outperforms all the baselines, even surpassing ChatGPT, by achieving an average outperformance of 14.21% across all datasets. Notably, FRONT implements an automated procedure and exhibits generalization across models and data scales, enabling continuous performance improvements",
    "checked": false,
    "id": "f4bb0154e537ce9631ed401060029580e8775aaa",
    "semantic_title": "improving attributed text generation of large language models via preference learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=DaUVlSBg3XO": {
    "title": "Instruction Embedding: New Concept, Benchmark and Method for Latent Representations of Instructions",
    "volume": "review",
    "abstract": "Instruction data is crucial for improving the capability of Large Language Models (LLMs) to align with human-level performance. Recent research LIMA demonstrates that alignment is essentially a process where the model adapts instructions' interaction style or format to solve various tasks, leveraging pre-trained knowledge and skills. Therefore, for instructional data, the most important aspect is the task it represents, rather than the specific semantics and knowledge information. The latent representations of instructions play roles for some instruction-related tasks like data distillation for instruction tuning and prompt retrieval for in-context learning. However, they are always derived from text embeddings, encompass overall semantic information that influences the representation of task categories. In this work, we introduce a new concept, instruction embedding, and construct Instruction Embedding Benchmark (IEB) for its evaluation. Then, we propose baseline method, prompt-based instruction embedding (PIE), to make the instruction embeddings more attention on task rather than whole semantic information. The evaluation of PIE, alongside other embedding methods on IEB, demonstrates its superior performance in accurately identifying task categories. Moreover, the application of PIE in downstream tasks showcases its effectiveness and suitability for instruction-related tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=woNxawXPQw": {
    "title": "Disentangling Length from Quality in Direct Preference Optimization",
    "volume": "review",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has been a crucial component in the recent success of Large Language Models. However, RLHF is know to exploit biases in human preferences, such as verbosity. A well-formatted and eloquent answer is often more highly rated by users, even when it is less helpful and objective. A number of approaches have been developed to control those biases in the classical RLHF literature, but the problem remains relatively under-explored for Direct Alignment Algorithms such as Direct Preference Optimization (DPO). Unlike classical RLHF, DPO does not train a separate reward model or use reinforcement learning directly, so previous approaches developed to control verbosity cannot be directly applied to this setting. Our work makes several contributions. For the first time, we study the length problem in the DPO setting, showing significant exploitation in DPO and linking it to out-of-distribution bootstrapping. We then develop a principled but simple regularization strategy that prevents length exploitation, while still maintaining improvements in model quality. We demonstrate these affects across datasets on summarization and dialogue, where we achieve up to 20\\% improvement in win rates when controlling for length, despite the GPT4 judge's well-known verbosity bias",
    "checked": true,
    "id": "bfc223b002401f42b44bca725da6ed6d1b953cff",
    "semantic_title": "disentangling length from quality in direct preference optimization",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=RvCKSM6p53U": {
    "title": "Surveying the Dead Minds: Historical-Psychological Text Analysis with Contextualized Construct Representation (CCR) for Classical Chinese",
    "volume": "review",
    "abstract": "In this work, we develop a pipeline for historical-psychological text analysis in classical Chinese. Humans have produced texts in various languages for thousands of years; however, most of the computational literature is focused on contemporary languages and corpora. The emerging field of historical psychology relies on computational techniques to extract aspects of psychology from historical corpora using new methods developed in natural language processing (NLP). The present pipeline, called Contextualized Construct Representations (CCR), combines expert knowledge in psychometrics (i.e., psychological surveys) with text representations generated via transformer-based language models to measure psychological constructs such as traditionalism, norm strength, and collectivism in classical Chinese corpora. Considering the scarcity of available data, we propose an indirect supervised contrastive learning approach and build the first Chinese historical psychological corpus (C-HIS-PSY) to fine-tune pre-trained models. We evaluate the pipeline and benchmark it against objective external data to test its validity. We also release our dataset and code for reproducibility athttps://anonymous.4open.science/r/His-Psy/",
    "checked": true,
    "id": "b4ed361980a8557081e3a2139a016a86a5e04a68",
    "semantic_title": "surveying the dead minds: historical-psychological text analysis with contextualized construct representation (ccr) for classical chinese",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cB3EVuBeE80": {
    "title": "AESS: A Simple Method to Model Long Context in Large Language Models",
    "volume": "review",
    "abstract": "As Large Language models (LLMs) gain popularity, the need to understand long texts continues to grow. Despite many models now extending the context window several times beyond the base model, the performance of these models in processing long texts varies across different tasks. Therefore, we propose Attention Entropy Sort and Selection (AESS) to address the long text problem. Our method achieves length generalization of LLM by leveraging the large model itself to retrieve the most relevant information for the task when the context window is limited. Moreover, this method is task-agnostic, and different tasks only need different prompts to achieve their retrieval. Results from the LongBench benchmark show that AESS can improve LLM performance by 9-10% compared to other retrieval methods. Furthermore, our method can also be adapted to various models and improve performance. Therefore, AESS is a promising solution for various applications that require LLMs to handle tasks with lengthy inputs effectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_Kqn7Xpb1T": {
    "title": "Contrastive Learning for Dependency Parsing on Free Word Order and Morphologically Rich Low Resource Languages",
    "volume": "review",
    "abstract": "Significant advancements have been made in the domain of dependency parsing, with researchers introducing novel architectures to enhance parsing performance. However, the majority of these architectures have been evaluated predominantly in languages with a fixed word order, such as English. Consequently, little attention has been devoted to exploring the robustness of these architectures in the context of relatively free word-ordered languages. In this work, we examine the robustness of graph-based parsing architectures on 4 relatively free word order languages. We focus on investigating essential modifications such as data augmentation and the removal of position encoding required to adapt these architectures accordingly. To this end, we propose a contrastive loss objective to make the model robust to word order variations. Furthermore, our proposed modification demonstrates a substantial average gain of 3.48/3.10 points in 4 relatively free word order languages, as measured by the Unlabelled/Labelled Attachment Score metric when compared to the best performing modifications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WAuuqB5QCTm": {
    "title": "Scaling Properties of Speech Language Models",
    "volume": "review",
    "abstract": "Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than the performance of text-based LLMs. Additionally, we study the effects of coarser speech tokenization, and the benefits of synthetic data designed to boost semantic understanding",
    "checked": true,
    "id": "a7eeee0df79da5662e7329d9710e97da032c3108",
    "semantic_title": "scaling properties of speech language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6v-W6Lnj9ao": {
    "title": "TransferTOD: A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities",
    "volume": "review",
    "abstract": "Task-oriented dialogue (TOD) systems aim to efficiently handle task-oriented conversations, including information gathering. The accurate, efficient, and effective utilization of TOD for information gathering has always been a critical and challenging task. Recent studies have demonstrated that Large Language Models (LLMs) excel in dialogue, instruction generation, and reasoning, and can significantly enhance the performance of TOD through fine-tuning. However, current datasets primarily cater to user-led systems and are limited to predefined specific scenarios and slots, thereby necessitating improvements in the proactiveness, diversity, and capabilities of TOD. In this study, we introduce TransferTOD, a multi-domain task-oriented Chinese dialogue dataset, which authentically simulates human-machine dialogues in 30 popular life service scenarios. Leveraging this dataset, we trained a TransferTOD-7B model using comprehensive parameter fine-tuning, showcasing notable abilities in slot filling and questioning. Our work has demonstrated its strong generalization capabilities in various downstream scenarios, significantly enhancing both data utilization efficiency and system performance",
    "checked": true,
    "id": "4757e3d37d4753af0df749e71de0faec7d19d054",
    "semantic_title": "transfertod: a generalizable chinese multi-domain task-oriented dialogue system with transfer capabilities",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zSTye809tF": {
    "title": "LOCR: Location-Guided Transformer for Optical Character Recognition",
    "volume": "review",
    "abstract": "Academic documents are packed with texts, equations, tables, and figures, requiring comprehensive understanding for accurate Optical Character Recognition (OCR). While end-to-end OCR methods offer improved accuracy over layout-based approaches, they often grapple with significant repetition issues, especially with complex layouts in Out-Of-Domain (OOD) documents. To tackle this, we propose LOCR, a model integrating location guiding into the transformer architecture during autoregression, training on a dataset comprising over 77M text-location pairs from 125K academic document pages, including bounding boxes for words, tables and mathematical symbols. LOCR adeptly handles various formatting elements and generates content in Markdown language. It outperforms existing models in our testset with an edit distance of 0.125, BLEU of 0.827 and F1 of 0.897.LOCR also reduces repetition frequency from 51% to 2% in the arXiv dataset and from 56% to 7% in OOD documents. Additionally, LOCR features an interactive OCR mode, facilitating the generation of complex documents through a few location prompts from human",
    "checked": true,
    "id": "313c43a4c62fa8223fc8fa0b8b40f7a4e6f1dca8",
    "semantic_title": "locr: location-guided transformer for optical character recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UeolpbafW9p": {
    "title": "AutoCrawler: A Progressive Understanding Web Agent for Web Crawler Generation",
    "volume": "review",
    "abstract": "Web automation is a significant technique that accomplishes complicated web tasks through automating common web actions, which enhance operational efficiency and reduce the need for manual intervention. Traditional methods, such as wrappers, suffer from limited adaptability and scalability when facing new website. On the other hand, generative agents empowered by Large Language Models (LLMs) exhibit poor performance and reusability in open-world scenarios. In this work, we introduce a crawler generation task for vertical information web pages and the paradigm of combining LLMs with crawlers, which helps crawlers handle diverse and changing web environments more efficiently. We propose AutoCrawler, a two-stage framework that leverages the hierarchical structure of HTML for progressive understanding. Through top-down and step-back operations, AutoCrawler can learn from erroneous actions and continuously prune HTML for better action generation. We conduct comprehensive experiments with multiple LLMs and demonstrate the effectiveness of our framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AMOup6hycU": {
    "title": "I Never Said That\": A dataset, taxonomy and baselines on response clarity classification",
    "volume": "review",
    "abstract": "Equivocation and ambiguity in public speech is a well-studied discourse phenomenon, especially in political science for the analysis of political interviews. Inspired by the well-grounded theory on equivocation, we aim to resolve the closely related problem of response clarity in questions extracted from political interviews, leveraging the capabilities of Large Language Models (LLMs) and human expertise. To this end, we introduce a novel taxonomy that frames the task of detecting and classifying response clarity and a clarity classification dataset which consists of question-answer (QA) pairs drawn from political interviews and annotated accordingly. Our proposed two-level taxonomy addresses the clarity of a response in terms of the information provided with respect to a given question (high-level) and also provides a fine-grained taxonomy of evasion techniques that relate to unclear, ambiguous responses (lower-level). Our annotation process leverages ChatGPT towards decomposing political dialogues into discrete QA pairs, each of which belongs to a specific response clarity and evasion category. Consequently, human annotators decide upon the correctness of this decomposition, while assigning an evasion label for each QA pair. We provide a detailed analysis of the dataset and we conduct several experiments using a range of LLMs to establish new baselines over the proposed dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PxZ3eQV2D8": {
    "title": "Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering",
    "volume": "review",
    "abstract": "Large Language models (LLMs) have demonstrated significant potential in transforming healthcare by automating tasks such as clinical documentation, information retrieval, and decision support. In this aspect, carefully engineered prompts have emerged as a powerful tool to use LLMs for medical scenarios, patient clinical scenarios. In this paper, we propose a modified version of the MedQA-USMLE dataset, which is subjective, to mimic real-life clinical scenarios. We explore the Chain of Thought (CoT) reasoning based on subjective response generation for the modified MedQA-USMLE dataset with appropriate LM driven forward-reasoning for correct responses to the medical questions.Keeping in mind the importance of response verification in the medical setting, we utilize a reward training mechanism, whereby for a particular response to a clinical question, the language model also provides an appropriate verified response. In this regard, we also include human-in-the-loop for different evaluation aspects. We develop better in-contrast learning strategies by modifying the 5-shot-codex-CoT-prompt for subjective MedQA dataset and developing our incremental-reasoning prompt. Our evaluations show that the incremental-reasoning prompt performs better than the modified-codex prompt in certain scenarios. We also show that greedy decoding with the incremental-reasoning method performs better than other strategies, such as prompt chaining and eliminative reasoning",
    "checked": true,
    "id": "14ad24cc03b0e997f84a07547bb4c179acf896b6",
    "semantic_title": "few shot chain-of-thought driven reasoning to prompt llms for open ended medical question answering",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=GzxLmsWEtwN": {
    "title": "Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR Dataset Construction",
    "volume": "review",
    "abstract": "Advancements in AI and natural language processing have revolutionized machine-human language interactions, with question answering (QA) systems playing a pivotal role. The knowledge base question answering (KBQA) task, utilizing structured knowledge graphs (KG), allows to handle extensive knowledge-intensive questions. However, a significant gap exists in KBQA datasets, especially for low-resource languages. Many existing construction pipelines for these datasets are outdated and inefficient in human labor, not utilizing modern assisting tools like Large Language Models (LLM) to reduce the workload. To address this, we have designed and implemented a modern, semi-automated approach for creating datasets, encompassing tasks such as KBQA, Machine Reading Comprehension (MRC), and Information Retrieval (IR), specifically tailored for low-resource environments. We executed this pipeline and introduced the PUGG dataset, the first Polish KBQA dataset, along with novel datasets for MRC and IR. Additionally, we provide a comprehensive implementation, insightful findings, detailed statistics and evaluation of baseline models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hkpcmjom0k": {
    "title": "SharedCon: Implicit Hate Speech Detection using Shared Semantics",
    "volume": "review",
    "abstract": "The ever-growing presence of hate speech on social network services and other online platforms not only fuels online harassment but also presents a growing challenge for hate speech detection. As this task is akin to binary classification, one of the promising approaches for hate speech detection is the utilization of Contrastive Learning (CL). However, recent studies show that CL, on its own, may not be sufficient for the nuanced task of detecting implicit hate speech - the struggle of which can be attributed to the subtle nature and context dependency of such pejorative remarks. Thus, researchers proposed a modified CL approach equipped with extra modules such as human-written implications or machine-generated augmented data for better implicit hate speech detection. While this approach can potentially enhance the overall performance due to its additional data general, it runs the risk of overfitting as well as heightened cost and time to obtain. These drawbacks serve as motivation for us to design a methodology that is not dependent on human-written or machine generated augmented data for training. Thus, we propose a straightforward, yet effective, clustering-based contrastive learning approach that leverages the shared semantics among the data",
    "checked": false,
    "id": "abc28ac47253c1697896cf6cbe5136fe5f4b0ab6",
    "semantic_title": "hhsd: hindi hate speech detection leveraging multi-task learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hCf20oIHJgq": {
    "title": "Progressive Tuning: Towards Generic Sentiment Abilities for Large Language Models",
    "volume": "review",
    "abstract": "Understanding sentiment is arguably an advanced and important capability of AI agents in the physical world. In previous works, many efforts have been devoted to individual sentiment subtasks, without considering interrelated sentiment knowledge among these subtasks. Although some recent works model multiple sentiment subtasks in a unified manner, they merely simply combine these subtasks without deeply exploring the hierarchical relationships among subtasks. In this paper, we introduce GSA-7B, an open-source large language model specific to the sentiment domain. Specifically, we deeply explore the hierarchical relationships between sentiment subtasks, proposing progressive sentiment reasoning benchmark and progressive task instructions. Subsequently, we use Llama2-7B as the backbone model and propose parameter-efficient progressive tuning paradigm which is implemented by constructing chain of LoRA, resulting in the creation of GSA-7B. Experimental results show that GSA-7B as a unified model performs well across all datasets in the progressive sentiment reasoning benchmark. Additionally, under the few-shot setting, GSA-7B also exhibits good generalization ability for sentiment subtasks and datasets that were not encountered during its training phase",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PEQWf7Dq1D": {
    "title": "AutoHall: Automated Hallucination Dataset Generation for Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) have garnered widespread applications across various domains due to their powerful language understanding and generation capabilities. However, the detection of non-factual or hallucinatory content generated by LLMs remains scarce. Currently, one significant challenge in hallucination detection is the laborious task of time-consuming and expensive manual annotation of the hallucinatory generation. To address this issue, this paper first introduces a method called $\\textbf{AutoHall}$ for $\\underline{auto}$matically constructing model-specific $\\underline{hall}$ucination datasets based on existing fact-checking datasets. Furthermore, we propose a zero-resource and black-box hallucination detection method based on self-contradiction. We conduct experiments towards prevalent open-/closed-source LLMs, achieving superior hallucination detection performance compared to extant baselines. Moreover, our experiments reveal variations in hallucination proportions and types among different models",
    "checked": true,
    "id": "bb3cc013c462ff2bf3dc5be90f731ebf34996f86",
    "semantic_title": "autohall: automated hallucination dataset generation for large language models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=eYiTvYfBQF": {
    "title": "Induction-Deduction Prompting: Enhancing Hidden-Information Reasoning in Medical LLM QA",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) excel in medical question answering. However, limited attention has been given to the underlying reasoning patterns in LLM generated chain of thoughts. We analyse common medical reasoning scenarios using a Bayesian Network, revealing the prevalence of hidden information, especially in the MedQA dataset.We introduce two simple prompts, induction (inferring hidden information) and deduction (evaluating options based on observed and inferred information). Used together they outperform conventional prompting techniques as well as Med-Palm 2, which relies on complex, expert-crafted prompting and expensive fine-tuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2gFkSdBgm7": {
    "title": "The Music Maestro or The Musically Challenged, A Massive Music Evaluation Benchmark for Large Language Models",
    "volume": "review",
    "abstract": "Benchmark plays a pivotal role in assessing the advancements of large language models (LLMs). While numerous benchmarks have been proposed to evaluate LLMs' capabilities, there is a notable absence of a dedicated benchmark for assessing their musical abilities. To address this gap, we present ZIQI-Eval, a comprehensive and large-scale music benchmark specifically designed to evaluate the music-related capabilities of LLMs.ZIQI-Eval encompasses a wide range of questions, covering 10 major categories and 56 subcategories, resulting in over 14,000 meticulously curated data entries. By leveraging ZIQI-Eval, we conduct a comprehensive evaluation over 15 LLMs to evaluate and analyze LLMs' performance in the domain of music.Results indicate that only GPT-4 is capable of effectively understanding and generating music, achieving an average accuracy rate, suggesting that there is ample room for improvement in existing LLMs.With ZIQI-Eval, we aim to provide a standardized and robust evaluation framework that facilitates a comprehensive assessment of LLMs' music-related abilities",
    "checked": true,
    "id": "195603df4d280f9b1a2cd300b813c5672a95b371",
    "semantic_title": "the music maestro or the musically challenged, a massive music evaluation benchmark for large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Z3kti-4Oc-": {
    "title": "An Empirical Study on the Characteristics of Bias upon Context Length Variation for Bangla",
    "volume": "review",
    "abstract": "Pretrained language models inherently exhibit various social biases, prompting a crucial examination of their social impact across linguistic contexts due to their widespread usage. Previous studies have provided numerous methods for intrinsic bias measurements, while subsequent research has highlighted many shortcomings. We question whether the amount of context fed into a model affects the application of previous methods in a contextual setup. With this in mind, we aim to introduce Bangla, a low-resource language, into these setups. In this study, we (1) create a dataset for intrinsic gender bias measurement in Bangla, (2) discuss necessary adaptations for Bangla, and (3) examine the effect of context length variation on bias measurement in established methods within a Bangla-based setup. We observe a definite dependency of bias metrics on context length. We consider our work as a stepping stone for bias measurement in the Bangla Language. All of our resources will be publicly available to support future research on Bangla NLP",
    "checked": true,
    "id": "5846a3d069edca86c982146a6bafd131dee661a1",
    "semantic_title": "an empirical study on the characteristics of bias upon context length variation for bangla",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=qvxjSXiBlLF": {
    "title": "MulTiple: A Multi-answer Time-sensitive Complex Question Answering Dataset",
    "volume": "review",
    "abstract": "Time-sensitive question answering is to answer questions with specific timestamps from the given long document. Existing works mostly focus on only one of the high-quality answers, but it is common that multiple answers simultaneously satisfy the constraints of a specific timestamp in the time-sensitive question. For example, an individual may play two different roles during a specific timestamp. In this paper, we construct a Multi-answer Time-sensitive question answering dataset, MulTiple, consisting of 17,580 multi-answer instances. Each contains a question, a corresponding long document and multiple answers. To ensure that the generated questions have multiple answers, we propose a global iteration method to obtain time-evolving events with multiple objects for the same subject and relation. Moreover, the baseline model IterBird is proposed to progressively gain multiple answers by integrating an iterative mechanism with the single-answer model. We construct extensive experiments on MulTiple and results show that IterBird significantly outperforms other baselines with SEM scores of 25.65\\% and 22.69\\%. It demonstrates that existing models struggle to obtain the full answers, even as clue words are provided in the time-sensitive questions. The dataset and code are released in \\url{http://github.com/multipledata/MTQA}",
    "checked": false,
    "id": "7cfcdf5ef8900ba80e24f8f5d5c5c8d1e938ca11",
    "semantic_title": "ask to understand: question generation for multi-hop question answering",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=zqMF9ANvDU": {
    "title": "Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive Learning",
    "volume": "review",
    "abstract": "Aspect Sentiment Triplet Extraction (ASTE) is a burgeoning subtask of fine-grained sentiment analysis, aiming to extract structured sentiment triplets from unstructured textual data. Existing approaches to ASTE often complicate the task with additional structures or external data. In this research, we propose a novel tagging scheme and employ a contrastive learning approach to mitigate these challenges. The proposed approach demonstrates comparable or superior performance in comparison to state-of-the-art techniques, while featuring a more compact design and reduced computational overhead. Notably, even in the era of Large Language Models (LLMs), our method exhibits superior efficacy compared to GPT 3.5 and GPT 4 in a few-shot learning scenarios. This study also provides valuable insights for the advancement of ASTE techniques within the paradigm of large language models",
    "checked": true,
    "id": "7f1df9be2afbd91316d23279ad9964ec317e7f21",
    "semantic_title": "rethinking aste: a minimalist tagging scheme alongside contrastive learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=60FrEKUEiC3": {
    "title": "ConsEval: Illuminating and Improving the Consistency of LLM Evaluators",
    "volume": "review",
    "abstract": "Large language models (LLMs) have shown potential for data annotation and evaluation. Despite the evident benefits of speed and lower cost, we raise concerns about the reliability of LLMs when applied to this evaluation, especially within the ground of consistency. In this paper, we conduct extensive studies on the two different aspects of consistency in LLM evaluations, Self-Consistency (SC) and Inter-Consistency (IC), comparing the rating scale and criterion granularity. Additionally, we study effects of inconsistency along with accuracy. We empirically observe that Llama-2-based evaluators are more consistent and accurate in general. Lastly, we present two effective methods: (1) Self-Consistency Evaluation (SCE) and (2) distilled In-Context Learning ($d$ICL) to jointly promote consistency and accuracy without further training. Along with accuracy-driven research, we insist the importance of research towards additionally assessing on the consistency in pursuit of safer LLM applications if we intend to exploit them as human evaluation proxies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rXQWz6hBV5N": {
    "title": "Silent Signals, Loud Impact: LLMs for Word-Sense Disambiguation of Coded Dog Whistles",
    "volume": "review",
    "abstract": "A dog whistle is a form of coded communication with a secondary meaning that is often weaponized for racial discrimination. Dog whistles historically began in United States politics, but soon also took root in social media as a means of evading hate speech detection systems and maintaining plausible deniability. In this paper, we present an approach for word-sense disambiguation of dog whistles from standard speech using Large Language Models (LLMs), and leverage this technique to create a dataset of 11,570 high-confidence coded examples of dog whistles used in formal and informal communication. Silent Signals is the largest dataset of disambiguated dog whistle usage, created for applications in hate speech detection, neology, and political science",
    "checked": true,
    "id": "1d1cebc22bb241c1678d065fb4f74e0b70f274ec",
    "semantic_title": "silent signals, loud impact: llms for word-sense disambiguation of coded dog whistles",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KTmB_SnoeXX": {
    "title": "Understandable and Singable Musical Lyrics Translation",
    "volume": "review",
    "abstract": "Translating lyrics in musicals is a new and challenging task due to various constraints to consider. While previous song translation works explore ways to incorporate and satisfy music constraints, they cannot ensure basic translation quality which is critical in musicals. This paper is dedicated to enhancing translation quality while simultaneously maintaining the singability features, such as adherence to length and rhyme constraints. Our approach consists of three main components. First, we collect a dataset to train reward models, giving an automatic evaluation of translation quality. To enhance both the singability and translation capabilities, we adopt a two-stage training recipe with filtering techniques. Finally, our inference-time optimizing framework composes the whole-song translation. Extensive experiments of both automatic evaluations and human evaluation not only show improvements over the baseline method by large margins but also demonstrate the effectiveness of different components in our approach",
    "checked": false,
    "id": "4ec6085fcccb1fbc4e90182dd1bbe1ca8b32a658",
    "semantic_title": "a computational evaluation framework for singable lyric translation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Qf_Rw-o4u1I": {
    "title": "How Personality Traits Influence Negotiation Outcomes? A Simulation based on Large Language Models",
    "volume": "review",
    "abstract": "Psychological evidence highlights the influence of personality traits on decision-making. For instance, agreeableness and openness enhance negotiation outcomes positively, whereas neuroticism can lead to unfavorable outcomes. This paper introduces a simulation framework that integrates LLM agents endowed with synthesized personality traits. These agents negotiate within a traditional bargaining domain with customizable personalities and negotiation objectives. The experimental results indicate that the behavioral tendencies of LLM-based simulations generally mirror those observed in human negotiations. A case study based on synthesized bargaining dialogues reveals intriguing behavioral dynamics, including deceitful and compromising behaviors. The contribution is twofold. First, we propose a simulation methodology that harnesses LLM agents' linguistic and economic capabilities. Secondly, we offer empirical insights into the impact of Big-Five personality traits on bilateral negotiation outcomes",
    "checked": true,
    "id": "7e1cccd15c0f5aa511495abe0e0757a8f2962a89",
    "semantic_title": "how personality traits influence negotiation outcomes? a simulation based on large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5NaYneq32R": {
    "title": "LongHeads: Multi-Head Attention is Secretly a Long Context Processor",
    "volume": "review",
    "abstract": "Large language models (LLMs) have achieved impressive performance in numerous domains but often struggle to process lengthy inputs effectively and efficiently due to limited length generalization and attention's quadratic computational demands.Many sought to mitigate this by restricting the attention window within the pre-trained length. However, these methods introduce new issues such as ignoring the middle context and requiring additional training.To address these problems, we propose LongHeads, a training-free framework that enhances LLM's long context ability by unlocking multi-head attention's untapped potential.Instead of allowing each head to attend to the full sentence, which struggles with generalizing to longer sequences due to out-of-distribution (OOD) issues, we allow each head to process in-distribution length by selecting and attending to important context chunks.To this end, we propose a chunk selection strategy that relies on the inherent correlation between the query and the key representations, efficiently distributing context chunks to different heads.In this way, each head ensures it can effectively process attended tokens within the trained length, while different heads in different layers can collectively process longer contexts.LongHeads works efficiently in linear time, fits seamlessly with many LLMs that use relative positional encoding.Our extensive empirical analyses verify LongHeads's efficacy in extending the usable context window for existing models, showcasing its promise for enhancing long text understanding",
    "checked": true,
    "id": "f6440a16ccc5c13d2a86af91b76e078685abfd16",
    "semantic_title": "longheads: multi-head attention is secretly a long context processor",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=KjMPI5DlZR": {
    "title": "WaveODE: An Efficient Vocoder Based on Probability Flow Equation",
    "volume": "review",
    "abstract": "Probability flow based models for image and audio synthesis, such as denoising diffusion probabilistic models and Poisson flow generative models, can be interpreted as modeling any ground truth distribution through the non-compressible fluid partial differentialequation, where the initial and final fluid density are the chosen prior distribution and the ground truth distribution correspondingly.In this research, we analyse various previous models under the unified perspective of probability flow equation,and propose WaveODE model for mel-spectrogram conditioned speech synthesis task, which learns a velocity field under a dynamically changing probability flow equation instead of estimating the groud truth with a fixed evolution equation such as VP-SDE and sub-VP-SDE in previous works. Since mel-spectrogram is a relatively strong condition which limits the possible audios to a small range, waveODE models the ground truth distribution with a mel-conditioned prior distribution rather than the standard Gaussian distribution,and adopts a distillation method to accelerate the inference process.Experimental results show that our model is comparable with previous vocoders in sample quality, and could generate waveform with a single inference step",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sbj7MJSOaI": {
    "title": "The PGNSC Benchmark: How Do We Predict Where Information Spreads?",
    "volume": "review",
    "abstract": "Social networks have become ideal vehicles for news dissemination because posted content is easily able to reach users beyond a news outlet's direct audience. Understanding how information is transmitted among communities of users is a critical step towards understanding the impact social networks have on real-world events. Two significant barriers in this vein of work are identifying user clusters and meaningfully characterizing these communities. Thus, we propose the PGNSC benchmark, which builds information pathways based on the audiences of influential news sources and uses their content to characterize the communities. We present methods of aggregating these news-source-centric communities and for constructing the community feature representations that are used sequentially to construct information pathway prediction pipelines.Lastly, we perform extensive experiments to demonstrate the performance of baseline pipeline constructions and to highlight the possibilities for future work",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2jFWhxJE5pQ": {
    "title": "A Redundancy-Enhanced Framework for Error Correction in Named Entity Recognition",
    "volume": "review",
    "abstract": "We present a redundancy-enhanced framework for error correction of NER by incorporating related sentences from the internet.Our key contribution is a Transformer-based refiner that integrates additional information into pre-trained language model with minimal effort.We begin by forming a redundancy set composed of(i) related sentences to the target sentence, using a proposed retrieval pipeline, and(ii) their NER predictions from an external Named Entity tagger.We then construct this refiner by combining a pre-trained Transformer-based model with an NE-tag embedding layer, both of which are fine-tuned on the target sentences and their corresponding redundancy sets.Methodologically, we propose a branch-and-conquer learning paradigm, termed \\textit{Incremental Learning}, for accurate error correction.In particular, it delivers an error reduction of 4.48\\% and a new state-of-the-art performance of 61.43 micro-f1 score on realistic WNUT17 dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G11KYEmHG8-": {
    "title": "DEFT: Distribution-guided Efficient Fine-Tuning for Human Alignment",
    "volume": "review",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) , with algorithms such as Proximal Policy Optimization (PPO) serving as a representative approach for aligning Large Language Models (LLMs) with human values. While effective, these methods have faced challenges due to high costs and unstable training processes. In response, alternative approaches have been proposed to serve as replacements for the PPO process or integrate Supervised Fine-Tuning (SFT) and contrastive learning to directly achieve fine-tuning and value alignment. However, these methods still need voluminous data to learn the preference and sacrifices a portion of generalization ability of LLMs. To further enhance alignment efficiency and performance while mitigating the loss of generalization ability, this paper introduces DEFT, an efficient alignment framework incorporating data filtering and distributional guidance. DEFT comprises two main components: (1) Data Grading, involving the integration of reward model scores to filter data of varying quality from the original dataset and achieve alignment using the best subset; (2) Distribution Reward, which extracts positive and negative discrepancy distributions from the data and guides the language model output distribution accordingly. Experimental results demonstrate that the methods enhanced by DEFT outperform the original methods in both alignment capability and generalization ability. The overall framework is easy to implement, and the training time overhead is significantly reduced",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SagzPkZQoM1": {
    "title": "Augmented test-time adaptation for implicit discourse relation recognition",
    "volume": "review",
    "abstract": "Implicit discourse relation recognition (IDRR), the task of identifying the overt coherence relation between two text spans, is a task that involves deep semantic processing and general knowledge. Previous efforts mostly focus on the data-rich news domain. Despite the success of large language model (LLM) prompting in a range of reasoning tasks, recent works found that IDRR cannot be solved under zero- or few-shot settings at all. This work proposes to exploit the generation power of LLMs and augment training data with synthetic DR instances. In particular, in order to generalize IDRR to other domains, we apply offline test-time adaptation to inform the model with contrastive DR features that are expected in the target domain during test-time. Experimental results show that this method is effective in cross-domain IDRR without any labelled data in the target domain. This simple strategy could be generally applied to other text classification tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jg0ksDS5s3": {
    "title": "EEV: Enhancing Long-Term Reasoning of LLM with Effective and Efficient Verifier",
    "volume": "review",
    "abstract": "Large language models have garnered significant attention due to their demonstrated strong logical reasoning capabilities. However, there is still ample room for improvement in handling complex long-term reasoning problems. Direct fine-tuning LLM on domain-specific data is an effective approach, but it requires substantial financial costs. Another research line aims to efficiently enhance performance by leveraging the model's inherent capabilities without tuning any parameters. They either utilize LLM's fact evaluation ability for self-verification during logical reasoning or employ voting methods to improve the consistency of the model's decisions. However, due to inherent limitations in specific domains, the benefits of self-verification approaches are typically limited. In this paper, we propose a compromised method, which involves training a small verification model to evaluate the reasoning process of large models. To overcome the error propagation problem of traditional verification model, we further propose a contrast-enhanced verification model training framework. The experimental results show that our proposed effective and efficient verifier (EEV) can achieve substantial performance gains on five datasets for multi-hop fact reasoning and long-term mathematical reasoning at a small cost",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j4CFV4rUAl2": {
    "title": "Thinking Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models",
    "volume": "review",
    "abstract": "Existing debiasing techniques are typically training-based or require access to the model's internals and output distributions, so they are inaccessible to end-users looking to adapt LLM outputs for their particular needs. In this study, we examine whether structured prompting techniques can offer opportunities for fair text generation. We evaluate a comprehensive end-user-focused iterative framework of debiasing that applies System 2 thinking processes for prompts to induce logical, reflective, and critical text generation, with single- and multi-step, instruction- and role-based, and zero- and few-shot variants. By systematically evaluating many LLMS across many datasets and different prompting strategies and their variants, we show that the more complex System 2-based Implicative Prompts significantly improve over other techniques with lower mean (gender, profession, race, and religion) bias in the outputs. Our work offers research directions for the design and the potential of end-user-focused evaluative frameworks for LLM use",
    "checked": true,
    "id": "b868d93e76ad6a7434e0d70d4f08d2d52c7cbaf3",
    "semantic_title": "thinking fair and slow: on the efficacy of structured prompts for debiasing language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CuMU6NcYIS": {
    "title": "A Survey on Knowledge Conflicts in the Era of LLMs",
    "volume": "review",
    "abstract": "This survey presents a comprehensive examination of knowledge conflicts in Large Language Models (LLMs). It explores the intricate challenges that arise when LLMs integrate contextual knowledge with their parametric knowledge. Our focus is on three primary types of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and accuracy of LLMs, especially in real-world applications where misinformation and noise are prevalent. The survey categorizes these conflicts, investigates their causes, and reviews potential mitigation strategies. It aims to provide insights into enhancing the robustness of LLMs, making it a valuable resource for advancing research in this evolving area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6DMi2r5X9tu": {
    "title": "StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback",
    "volume": "review",
    "abstract": "The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce \\textbf{StepCoder}, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks. The code and dataset will be made available upon publication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pkx8deL4iD": {
    "title": "Structural Optimization Ambiguity and Simplicity Bias in Unsupervised Neural Grammar Induction",
    "volume": "review",
    "abstract": "Neural parameterization has significantly advanced unsupervised grammar induction. However, training these models with a traditional likelihood loss for all possible parses exacerbates two issues: 1) $\\textit{structural optimization ambiguity}$ that arbitrarily selects one among structurally ambiguous optimal grammars despite the specific preference of gold parses, and 2) $\\textit{structural simplicity bias}$ that underutilizes available grammar rules. These challenges subject unsupervised neural grammar induction (UNGI) to inevitable prediction errors, high variance, and the necessity for extensive grammars to achieve accurate predictions. This paper tackles these issues, offering a comprehensive analysis of their origins. As a solution, we introduce $\\textit{sentence-wise parse-focusing}$ to reduce the parse pool per sentence for loss evaluation, using the structural bias from pre-trained parsers on the same dataset.In unsupervised parsing benchmark tests, our method significantly improves performance while effectively reducing variance and enhancing rule utilization. Our research promotes the learning of more compact, accurate, and consistent explicit grammar, facilitating better interpretability",
    "checked": true,
    "id": "620d6adb5eb8b6abad539cb892df9e46b2e960d8",
    "semantic_title": "structural optimization ambiguity and simplicity bias in unsupervised neural grammar induction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hkxg1rEOB6o": {
    "title": "LLMEmbed: Rethinking Lightweight LLM's Genuine Function in Text Classification",
    "volume": "review",
    "abstract": "With the booming of Large Language Models (LLMs), prompt-learning has become a promising method mainly researched in various research areas. Recently, many attempts based on prompt-learning have been made to improve the performance of text classification. However, most of these methods are based on heuristic Chain-of-Thought (CoT), and tend to be more complex but less efficient. In this paper, we rethink the LLM-based text classification methodology, propose a simple and effective transfer learning strategy, namely LLMEmbed, to address this classical but challenging task. Specifically, we first study how to properly extract and fuse the text embeddings via various lightweight LLMs at differeny network depths to improve their robustness and discrimination, then adapt such embeddings to train the classifier. We perform extensive experiments on publicly available datasets, and the results show that LLMEmbed achieves strong performance while enjoys low training overhead using lightweight LLM backbones compared to recent methods based on larger LLMs, \\textit{i.e.} GPT-3, and sophisticated prompt-based strategies",
    "checked": true,
    "id": "f3734f4caa531a7ef38061c022cb0446a892d579",
    "semantic_title": "llmembed: rethinking lightweight llm's genuine function in text classification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0WGXQ2--1a": {
    "title": "Good Books are Complex Matters: Gauging Complexity Profiles Across Diverse Categories of Perceived Literary Quality",
    "volume": "review",
    "abstract": "In this study, we employ a classification approach to show that different categories of literary ``quality\" display unique linguistic profiles, leveraging a corpus that encompasses titles from the Norton Anthology, Penguin Classics series, and the Open Syllabus project, contrasted against contemporary bestsellers, Nobel prize winners and recipients of prestigious literary awards. Our analysis reveals that canonical and so called high-brow texts exhibit distinct textual features when compared to other quality categories such as bestsellers and popular titles as well as to control groups, likely responding to distinct (but not mutually exclusive) models of quality.We apply a classic machine learning approach, namely Random Forest, to distinguish quality novels from ``control groups'', achieving up to 77\\% F1 scores in differentiating between the categories. We find that quality category tend to be easier to distinguish from control groups than from other quality categories, suggesting than literary quality features might be distinguishable but shared through quality proxies",
    "checked": true,
    "id": "9307d9a88dacda953b8a96d3f7849d4a7b4d96fd",
    "semantic_title": "good books are complex matters: gauging complexity profiles across diverse categories of perceived literary quality",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Wbz3H4WIvc": {
    "title": "No Language is an Island: Unifying Chinese and English in Financial Large Language Models, Instruction Data, and Benchmarks",
    "volume": "review",
    "abstract": "While Large Language Models (LLMs) have significantly advanced financial analysis, their scope has predominantly been limited to monolingual applications, with bilingual Chinese-English capabilities largely unexplored. To bridge this gap, we present ICE-PIXIU, integrating the ICE-INTENT model and ICE-FLARE benchmark for bilingual financial analysis. ICE-PIXIU uniquely incorporates a range of Chinese instruction tasks, alongside translated and original English datasets, extending the reach and depth of bilingual financial modeling. Our extensive analysis reveals that integrating these bilingual datasets, especially translation tasks and original English data, not only enhances the model's linguistic adaptability but also deepens its analytical acumen in financial contexts. ICE-INTENT, in particular, demonstrates a marked improvement over general-domain LLMs in bilingual settings, showcasing the substantial impact of rich bilingual data on the precision and effectiveness of financial NLP",
    "checked": true,
    "id": "eb419b57023d7de3284b182a5b680195c9095040",
    "semantic_title": "no language is an island: unifying chinese and english in financial large language models, instruction data, and benchmarks",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=zyaZy6GG4Xh": {
    "title": "Achieving >97% on GSM8K: Deeply Understanding the Problems Makes LLMs Perfect Reasoners",
    "volume": "review",
    "abstract": "Chain of Thought prompting strategy has enhanced the performance of Large Language Models (LLMs) across various NLP tasks. However, it still has shortcomings when dealing with complex reasoning tasks, following~\\citet{cot_wei}, including understanding errors, calculation errors and process errors (e.g. missing-step and hallucinations). Subsequently, Our in-depth analysis of various error types has found that deeply understanding the whole problem is critical in addressing complicated reasoning tasks. In this paper, we proposed a novel prompt strategy called Deeply Understanding the Problems (DUP) prompting, inspired by how humans solve complex reasoning problems, designed to enhance the comprehensive understanding of problems by LLMs. It consists of three stages: 1) extract the core question; 2) find out problem-solving information based on the core question; 3) generate and extract answers by LLMs. We evaluate the performance of DUP prompting on ten diverse reasoning datasets. Experimental results suggest that DUP prompting significantly outperforms Zero-Shot CoT ~\\cite{kojima2022large} across all datasets. Notably, DUP achieves \\textbf{state-of-the-art on SVAMP (90.4% to 94.2%) and GSM8K (94.6% to 97.1%).}",
    "checked": true,
    "id": "1fd84f4414420485cc982158f068ca929a16db18",
    "semantic_title": "achieving >97% on gsm8k: deeply understanding the problems makes llms perfect reasoners",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ywJTD0_24PV": {
    "title": "Entity-Aware Multimodal Alignment Framework for News Image Captioning",
    "volume": "review",
    "abstract": "News image captioning task is a variant of image captioning task which requires model to generate a more informative caption with news image and the associated news article. Multimodal Large Language models have developed rapidly in recent years and is promising in news image captioning task. However, according to our experiments, common MLLMs are not good at generating the entities in zero-shot setting. Their abilities to deal with the entities information are still limited after simply finetuned on news image captioning dataset. To obtain a more powerful model to handle the multimodal entity information, we design two multimodal entity-aware alignment tasks and an alignment framework to align the model and generate the news image captions. Our method achieves better results than previous state-of-the-art models in CIDEr score (72.33 -> 86.29) on GoodNews dataset and (70.83 -> 85.61) on Nytimes800k dataset",
    "checked": false,
    "id": "b2ee29fef78ad64ac7303a8505732ef9db64658d",
    "semantic_title": "eama : entity-aware multimodal alignment based approach for news image captioning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EmENXf2VbN": {
    "title": "MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization",
    "volume": "review",
    "abstract": "RL-based techniques can be used to search for prompts that when fed into a target language model maximize a set of user-specified reward functions. However, in many target applications, the natural reward functions are in tension with one another -- for example, content preservation vs. style matching in style transfer tasks. Current techniques focus on maximizing the average of reward functions, which does not necessarily lead to prompts that achieve balance across rewards -- an issue that has been well-studied in the multi-objective and robust optimization literature. In this paper, we adapt several techniques for multi-objective optimization to RL-based discrete prompt optimization -- two that consider volume of the Pareto reward surface, and another that chooses an update direction that benefits all rewards simultaneously. We conduct an empirical analysis of these methods on two NLP tasks: style transfer and machine translation, each using three competing reward functions. Our experiments demonstrate that multi-objective methods that directly optimize volume perform better and achieve a better balance of all rewards than those that attempt to find monotonic update directions",
    "checked": true,
    "id": "564bffb42edb6a24be8b144f22eec97e0579028b",
    "semantic_title": "morl-prompt: an empirical analysis of multi-objective reinforcement learning for discrete prompt optimization",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=rtlada2jCA_": {
    "title": "ResEmo: A Benchmark Chinese Dataset for Studying Responsive Emotion from Social Media Content",
    "volume": "review",
    "abstract": "On social media platforms, users' emotions are triggered when they encounter particular content from other users,where such emotions are different from those that spontaneously emerged, owing to the \"responsive'' nature. Analyzing the aforementioned responsive emotions from user interactions is a task of significant importance for understanding human cognition, the mechanisms of emotion generation, and behavior on the Internet, etc. Performing the task with artificial intelligence generally requires human-annotated data to help train a well-performing system, while existing data resources do not cover this specific area, with none of them focusing on responsive emotion analysis. In this paper, we propose a Chinese dataset named \\textsc{ResEmo} for responsive emotion analysis, including 3813 posts with 68,781 comments collected from Weibo, the largest social media platform in China. ResEmo contains three types of human annotations with respect to responsive emotions, namely, responsive relationship, responsive emotion cause, and responsive emotion category. Moreover, to test this dataset, we build large language model (LLM) baseline methods for responsive relation extraction, responsive emotion cause extraction, and responsive emotion detection, which show the potential of the proposed ResEmo being a benchmark for future studies on responsive emotions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XAe_jUCyebd": {
    "title": "Huatuo-26M, a Large-scale Chinese Medical QA Dataset",
    "volume": "review",
    "abstract": "In this paper, we release the largest ever medical Question Answering (QA) dataset with \\26 Million QA pairs named Huatuo-26M. We benchmark many existing approaches in our dataset in terms of both retrieval and generation. We also experimentally show the benefit of the proposed dataset in many aspects: (i) it serves as a fine-tuning data for training medical Large Language Models (LLMs); (ii) it works as an external knowledge source for retrieval-augmented generation (RAG); (iii) it demonstrates transferability by enhancing zero-shot performance on other QA datasets; and (iv) it aids in training biomedical model as a pre-training corpus. Our empirical findings substantiate the dataset's utility in these domains, thereby confirming its significance as a resource in the medical QA landscape",
    "checked": true,
    "id": "1c5bc4f10b95a90d0283d0aacc94332aae508169",
    "semantic_title": "huatuo-26m, a large-scale chinese medical qa dataset",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=NwqxjwJ_fa": {
    "title": "Evolutionary Algorithms and Neural Network-Based Fitness Functions for Extractive Text Summarization: A Comparative Study with ChatGPT",
    "volume": "review",
    "abstract": "Extractive text summarization deals with extracting a limited number of important sentences from a large document to create a summary. One novel approach already proposed in the literature is to model extractive summarization as an optimization problem, where a Genetic algorithm (GA) has been used for optimizing the selection of sentences from a text to generate the best extractive summary, which has been found outperforming state-of-the-art techniques. In this work, we build a similar model where apart from GA we used several different evolutionary algorithms (EA) in order to identify the combination that produces the best result. For this work, we have used different evolutionary algorithms, namely Discrete Differential Evolution (DDE), Cuckoo Search, Particle Swarm Optimization, and Firefly Search along with Genetic Algorithm, and have made comparison of their results with state-of-the art LLM viz. ChatGPT. The results are evaluated on the BBC news dataset using the precision-recall technique metric",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lG61pruG1Iz": {
    "title": "MapCoder: Multi-Agent Code Generation for Competitive Problem Solving",
    "volume": "review",
    "abstract": "Code synthesis, which requires a deep understanding of complex natural language (NL) problem descriptions, generation of code instructions for complex algorithms and data structures, and the successful execution of comprehensive unit tests, presents a significant challenge. Thus, while large language models (LLMs) demonstrate impressive proficiency in natural language processing (NLP), their performance in code generation tasks remains limited. In this paper, we introduce a new approach to code generation tasks leveraging the multi-agent prompting that uniquely replicates the full cycle of program synthesis as observed in human developers. Our framework, MapCoder, consists of four LLM agents specifically designed to emulate the stages of this cycle: recalling relevant examples, planning, code generation, and debugging. After conducting thorough experiments, with multiple LLMs ablations, and analyses across seven challenging competitive problem-solving and program synthesis benchmarksâ€”MapCoder showcases remarkable code generation capabilities, achieving their new state-of-the-art (pass@1) resultsâ€”(HumanEval 93.9%, MBPP 83.1%, APPS 22.0%, CodeContests 28.5%, and xCodeEval 45.3%). Moreover, our method consistently delivers superior performance across various programming languages and varying problem difficulties",
    "checked": true,
    "id": "e81c707040ce604c7102cfe14d78b72385c17b68",
    "semantic_title": "mapcoder: multi-agent code generation for competitive problem solving",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Na0EYD3CCQ": {
    "title": "Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts",
    "volume": "review",
    "abstract": "In the era of large language models, applying techniques such as Retrieval Augmented Generation can better address Open-Domain Question-Answering problems. Due to constraints including model sizes and computing resources, the length of context is often limited, and it becomes challenging to empower the model to cover overlong contexts while answering questions from open domains. This paper proposes a general and convenient method to covering longer contexts in Open-Domain Question-Answering tasks. It leverages a small encoder language model that effectively encodes contexts, and the encoding applies cross-attention with origin inputs.With our method, the origin language models can cover several times longer contexts while keeping the computing requirements close to the baseline. Our experiments demonstrate that after fine-tuning, there is improved performance across two held-in datasets, four held-out datasets, and also in two In Context Learning settings. Our code will be released at {\\url{url}}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4OyT7YTtd0": {
    "title": "Beyond Agreement: Diagnosing the Rationale Alignment of Automated Essay Scoring Methods based on Linguistically-informed Counterfactuals",
    "volume": "review",
    "abstract": "While current automated essay scoring (AES) methods show high agreement with human raters, their scoring mechanisms are not fully explored. Our proposed method, using counterfactual intervention assisted by Large Language Models (LLMs), reveals that when scoring essays, BERT-like models primarily focus on sentence-level features, while LLMs are attuned to conventions, language complexity, as well as organization, indicating a more comprehensive alignment with scoring rubrics. Moreover, LLMs can discern counterfactual interventions during feedback. Our approach improves understanding of neural AES methods and can also apply to other domains seeking transparency in model-driven decisions. The codes and data will be released at GitHub",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lMuTsOM3K5f": {
    "title": "Fine-tuning with Gender-inclusive Language for Bias Reduction in LLMs",
    "volume": "review",
    "abstract": "Gender bias is not only prevalent in Large Language Models (LLMs) and their training data. It is also firmly ingrained into the structural aspects of language itself. In this work we focus on gender-exclusive affixes in English, such as in 'show-girl' or 'man-cave', which can perpetuate gender stereotypes and exclude association with non-binary genders. We use an LLM training dataset to extract a catalogue of 692 gender-exclusive words alongside gender-neutral variants. Our catalogue can aid in assessing gender skews in a given training corpus. We also use it to develop a fine-tuning dataset, the 'Tiny Heap', in which we replace gender-exclusive with gender-inclusive wording. We fine-tune three LLMs, observing an overall reduction in gender-stereotyping tendencies across the models. Our approach provides a practical method for enhancing gender inclusivity in LLM training data and contributes to the inclusion of queer-feminist linguistic activism in bias mitigation research in NLP",
    "checked": false,
    "id": "860fdc25b3a59a885905165b316441ee64ae7eb0",
    "semantic_title": "from 'showgirls' to 'performers': fine-tuning with gender-inclusive language for bias reduction in llms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QSEKxm0mxtz": {
    "title": "Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages",
    "volume": "review",
    "abstract": "Recently, the development of open-source large language models (LLMs) has advanced rapidly. Nevertheless, due to data constraints, the capabilities of most open-source LLMs are primarily focused on English. To address this issue, we introduce the concept of $\\textit{chat vector}$ to equip pre-trained language models with instruction following and human value alignment via simple model arithmetic. The chat vector is derived by subtracting the weights of a pre-trained base model (e.g. LLaMA2) from those of its corresponding chat model (e.g. LLaMA2-chat). By simply adding the chat vector to a continual pre-trained model's weights, we can endow the model with chat capabilities in new languages without the need for further training. Our empirical studies demonstrate the superior efficacy of the chat vector from three different aspects: instruction following, toxicity mitigation, and multi-turn dialogue. Moreover, to showcase the adaptability of our approach, we extend our experiments to encompass various languages, base models, and chat vectors. The results underscore the chat vector's simplicity, effectiveness, and wide applicability, making it a compelling solution for efficiently enabling conversational capabilities in pre-trained language models",
    "checked": true,
    "id": "53e42ca0c344a0c87a7e4943aaba46762cb311bb",
    "semantic_title": "chat vector: a simple approach to equip llms with instruction following and model alignment in new languages",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=r9qqwomYwfF": {
    "title": "How Multilingual is LLaMA?",
    "volume": "review",
    "abstract": "Large Language Models (LLMs), often show strong performance on English tasks, while exhibiting limitations on other languages. What is an LLM's multilingual capability when it is trained only on certain languages? The underlying mechanism remains unclear. This study endeavors to examine the multilingual capability of LLMs by conducting an exhaustive analysis across 101 languages. Through the investigation of the performance gap before and after embedding fine-tuning, we discovered four distinct quadrants. By delving into each quadrant we provide actionable and efficient guidelines for tuning these languages. Extensive experiments reveal that existing LLMs possess multilingual capabilities that surpass our expectations, and we can significantly improve the multilingual performance of LLMs based on these attributes of each quadrant~\\footnote{We will release the model and code to the public.}",
    "checked": false,
    "id": "b2065a0041b613595657b2c77711d9a422b7dd54",
    "semantic_title": "how vocabulary sharing facilitates multilingualism in llama?",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=iUOka3Xj0s": {
    "title": "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning",
    "volume": "review",
    "abstract": "Recent advancements in large language models (LLMs) have raised concerns about inference costs, increasing the need for research into model compression. While knowledge distillation (KD) is a prominent method for this, research on KD for generative language models like LLMs is relatively sparse, and the approach of distilling student-friendly knowledge, which has shown promising performance in KD for classification models, remains unexplored in generative language models. To explore this approach, we propose PromptKD, a simple yet effective method that utilizes prompt tuning - for the first time in KD - to enable generative language models to transfer student-friendly knowledge. Unlike previous works in classification that require fine-tuning the entire teacher model for extracting student-friendly knowledge, PromptKD achieves similar effects by adding a small number of prompt tokens and tuning only the prompt with student guidance. Extensive experiments on instruction-following datasets using the GPT-2 model family show that PromptKD achieves state-of-the-art performance while adding only 0.0007% of the teacher's parameters as prompts. Further analysis suggests that distilling student-friendly knowledge alleviates exposure bias effectively throughout the entire training process, leading to performance enhancements",
    "checked": true,
    "id": "e2f8864c3e40298513ca320de0012818ce092bea",
    "semantic_title": "promptkd: distilling student-friendly knowledge for generative language models via prompt tuning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=NR-Wd2_46RI": {
    "title": "Structure Guided Large Language Model for SQL Generation",
    "volume": "review",
    "abstract": "Generating accurate Structured Querying Language (SQL) is a long-standing problem, especially in matching users' semantic queries with structured databases and then generating structured SQL. Existing models typically input queries and database schemas into the LLM and rely on the LLM to perform semantic-structure matching and generate structured SQL. However, such solutions overlook the structural information within user queries and databases, which can be utilized to enhance the generation of structured SQL. This oversight can lead to inaccurate or unexecutable SQL generation. To fully exploit the structure, we propose the structure-to-SQL framework, which leverages the inherent structure information to improve the SQL generation of LLMs. Specifically, we introduce our Structure Guided SQL~(SGU-SQL)} generation model. SGU-SQL links user queries and databases in a structure-enhanced manner. It then decomposes complicated linked structures with grammar trees to guide the LLM to generate the SQL step by step. Extensive experiments on two benchmark datasets illustrate that SGU-SQL can outperform sixteen SQL generation baselines",
    "checked": true,
    "id": "405030fdaa44687c5495437dd6c5d1db7010fa3f",
    "semantic_title": "structure guided large language model for sql generation",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=lxSqliFFA_f": {
    "title": "A Unified Framework and Dataset for Assessing Gender Bias in Vision-Language Models",
    "volume": "review",
    "abstract": "Large vision-language models (VLMs) are widely getting adopted in industry and academia. In this work we build a unified framework to systematically evaluate gender-profession bias in VLMs. Our evaluation encompasses all supported inference modes of the recent VLMs, including image-to-text, text-to-text, text-to-image, and image-to-image. We construct a synthetic, high-quality dataset of text and images that blurs gender distinctions across professional actions to benchmark gender bias. In our benchmarking of popular vision-language models (VLMs), we observe that different input-output modalities result in distinct bias magnitudes and directions. We hope our work will help guide future progress in improving VLMs to learn socially unbiased representations. We will release our data and code",
    "checked": true,
    "id": "bd64d47b1a0f2b296b6517d092f68b60a135101a",
    "semantic_title": "a unified framework and dataset for assessing gender bias in vision-language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Eqbsy2JjsDC": {
    "title": "LITO: Learnable Intervention for Truthfulness Optimization",
    "volume": "review",
    "abstract": "Large language models (LLMs) can generate long-form and coherent text, but they still frequently hallucinate facts, thus limiting their reliability. To address this issue, inference-time methods that elicit truthful responses have been proposed by shifting LLM representations towards learned \"truthful directions.\" However, applying the truthful direction with the same intensity fails to generalize across different question contexts. We propose LITO, a Learnable Intervention method for Truthfulness Optimization that automatically identifies the optimal intervention intensity tailored to a specific question. LITO explores a series of model generations using a set of increasing intervention intensities and selects the most accurate response or refrains from answering when the predictions are of high uncertainty. Experiments on multiple LLMs and question-answering datasets demonstrate that LITO improves truthfulness while preserving task accuracy. The adaptive nature of LITO counters issues with one-size-fits-all intervention, maximizing truthfulness by reflecting internal knowledge only when the model is confident",
    "checked": true,
    "id": "07db2789b8d75245f9d8442766d34837a7798693",
    "semantic_title": "lito: learnable intervention for truthfulness optimization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4ExcoPIEAP": {
    "title": "Embedded Named Entity Recognition using Probing Classifiers",
    "volume": "review",
    "abstract": "Mapping text generated by pre-trained language models (LMs) to external information is the basis for knowledge-enhanced applications, such as retrieval augmented generation. A key step of this process is the extraction of information from the current context, either by requiring the LM to perform queries or using additional models. Either method has inherent disadvantages, such as expensive and potentially destructive finetuning of the LM or substantially increased computational overhead at inference time. Instead, we propose directly embedding information extraction capabilities into pre-trained language models using probing classifiers, enabling efficient simultaneous text generation and information extraction. For this, we introduce an approach called EMBER and show that it enables named entity recognition in decoder-only language models without finetuning them and while incurring minimal additional computational cost at inference time. Code and data are available online",
    "checked": true,
    "id": "fff665caf490d815b090f50bd6393bdff3a51829",
    "semantic_title": "embedded named entity recognition using probing classifiers",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=4pyVAdYkQv": {
    "title": "PairDistill: Pairwise Relevance Distillation for Dense Retrieval",
    "volume": "review",
    "abstract": "Effective information retrieval (IR) from vast datasets relies on advanced techniques to extract relevant information in response to queries. Recent advancements in dense passage retrieval (DPR) have showcased remarkable efficacy compared to traditional sparse retrieval methods. To further enhance retrieval performance, knowledge distillation techniques, often leveraging robust cross-encoder rerankers, have been extensively explored. However, existing approaches primarily distill knowledge from pointwise rerankers, which assign absolute relevance scores to documents, thus facing challenges related to inconsistent standards. This paper introduces Pairwise Relevance Distillation (PairDistill) to leverage pairwise reranking, offering fine-grained distinctions between similarly relevant documents to enrich the training of dense retrieval models. Our experiments demonstrate that PairDistill outperforms existing methods, achieving new state-of-the-art results across multiple benchmarks. This highlights the potential of PairDistill in advancing dense retrieval techniques effectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UmI7RRutky": {
    "title": "Explaining Mixtures of Sources in News Articles",
    "volume": "review",
    "abstract": "Writers often use a variety of informational sources to inform storytelling, and although prior research has identified general trends in source usage, why \\textit{specific} stories call for \\textit{specific} sets of stories has not yet been studied. Here, we explore rationales behind source selection in news articles, with the goal of helping journalists \\textit{plan}, given a story idea, the set of sources they will select. We examine various underlying frameworks for categorizing sources: we adapt five pre-existing schemas and introduce three new ones tailored for this purpose. For a given document, our goal is to identify the schema that best describes its sources. Inspired by latent-variable modeling, we develop metrics that test each schema's probability of generating the observed document. We find two schemas: \\textit{stance} \\cite{hardalov2021cross} and \\textit{social affiliation} (a schema we introduce) best explain sourcing in the most documents, however, other schemas like \\textit{textual entailment} explain source usage in topics rich in factual content, like ``Science''. Finally, we find we can predict the most suitable schema given solely the article's headline with reasonable accuracy, paving the way for innovative applications in source retrieval planning, particularly within retrieval-augmented generation frameworks",
    "checked": false,
    "id": "33813e3ae6b8e6a7e8c78155e23230d8460013f2",
    "semantic_title": "nonlinear intraday trading invariance in the russian stock market",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=LislOCJFECd": {
    "title": "Can Large Language Models Learn Morality? A Survey on Definition, Methodology and Evaluation",
    "volume": "review",
    "abstract": "As large language models (LLMs) influence our daily lives in various ways, it becomes increasingly important to understanding their grasps of morality. In this paper, we discuss the notion of morality within the LLM research and delineate specific moral related downstream tasks.We review and summarize the performance of advanced LLMs on these tasks. We also review alignment techniques related to morality, including both bottom-up and top-down approaches. Lastly, we survey existing datasets and benchmarks used for evaluating LLMs' moral capabilities, identify key challenges in this research area, and suggest avenues for potential future explorations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tBCC63Yl_x": {
    "title": "Encode Once and Decode in Parallel: Efficient Transformer Decoding",
    "volume": "review",
    "abstract": "Transformer-based NLP models are powerful but have high computational costs that limit deployment scenarios. Finetuned encoder-decoder models are popular in specialized domains and can outperform larger more generalized decoder-only models, such as GPT-4. We introduce a new configuration for encoder-decoder models that improves efficiency on structured output and question-answering tasks where multiple outputs are required of a single input. Our method, prompt-in-decoder (PiD), encodes the input once and decodes output in parallel, boosting both training and inference efficiency by avoiding duplicate input encoding, thereby reducing the decoder's memory footprint. We achieve computation reduction that roughly scales with the number of subtasks, gaining up to 4.6x speed-up over state-of-the-art models for dialogue state tracking, summarization, and question-answering tasks with comparable or better performance",
    "checked": false,
    "id": "ef9d3494c0a73c99020d96c3a10e72106d2683eb",
    "semantic_title": "efficient encoder-decoder transformer decoding for decomposable tasks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xh_x7npKWV": {
    "title": "DynDST: A Dynamic Dialogue State Tracking Dataset for Assessing the Conversational Adaptability of Large Language Models",
    "volume": "review",
    "abstract": "This work tackles a key challenge in dialogue systems: the ability to adapt to changing user intentions and resolve inconsistencies in conversation histories. This is crucial in scenarios like train ticket booking, where customer plans often change dynamically. Despite advancements in NLP and large language models (LLMs), these systems struggle with real-time information updates during conversations.We introduce a specialized dataset to evaluate chatbot models on dynamic dialogue state tracking, focusing on scenarios where users modify their requests mid-conversation. This work aims to improve chatbot coherence and consistency, bridging the gap between the current capabilities of dialogue systems and the fluidity of human-like conversational interactions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ibKIfnOWUe": {
    "title": "Unraveling and Mitigating Retriever Inconsistencies in Retrieval-Augmented Large Language Models",
    "volume": "review",
    "abstract": "Although Retrieval-Augmented Large Language Models (RALMs) demonstrate their superiority in terms of factuality, they do not consistently outperform the original retrieval-free Language Models (LMs). Our experiments reveal that this example-level performance inconsistency exists not only between retrieval-augmented and retrieval-free LM but also among different retrievers. To understand this phenomenon, we investigate the degeneration behavior of RALMs and theoretically decompose it into four categories. Further analysis based on our decomposition reveals that the innate difference in knowledge sources and the unpredictable degeneration of the reader model contribute most to the inconsistency. Drawing from our analysis, we introduce Ensemble of Retrievers (EoR), a trainable framework that can adaptively retrieve from different knowledge sources and effectively decrease unpredictable reader errors. Our experiments on Open Domain Question Answering show that EoR substantially improves performance over the RALM with a single retriever by considerably reducing inconsistent behaviors",
    "checked": true,
    "id": "fe7650f9a61d553d153518a31b1ab6b7a77a4c56",
    "semantic_title": "unraveling and mitigating retriever inconsistencies in retrieval-augmented large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zx1RIdxkGa": {
    "title": "SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation",
    "volume": "review",
    "abstract": "Large language models(LLMs) have shown their outperforming ability on various tasks and question answering. However, LLMs require high computation costs and large memory costs. At the same time, LLMs may cause privacy leakage when training or prediction procedures contain sensitive information. In this paper, we propose SPA(Side Plugin Adaption), a lightweight model for fast on-device inference and privacy retaining on the constraints of strict on-device computation and memory constraints. Compared with other on-device seq2seq generation, SPA could make a fast and stable inference on privacy constraints, allowing it to obtain cost efficiency. Our method establishes an interaction between pretrained LLMs on-cloud and additive parameters on devices, which could provide knowledge on both pretrained LLMs and private personal features. Furthermore, SPA provides a framework to keep feature-base parameters on private guaranteed but low computational devices while leaving the parameters containing general information on the high computational devices",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yU5I_v6ekf": {
    "title": "Prompted Aspect Key Point Analysis for Quantitative Review Summarization",
    "volume": "review",
    "abstract": "Key Point Analysis (KPA) aims for quantitative summarization that provide key points (KPs) as succinct textual summaries and quantities measuring their prevalence. KPA studies for argument and reviews have been reported in the literature. Majority of KPA studies for reviews adopt supervised learning to extract short sentences as KPs and matching KPs to review comments for quantification of KP prevalence. Recent abstractive approaches still generate KPs based on sentences, often leading to KPs with overlapping and hallucinated opinions, and inaccurate quantification. In this paper, we propose Prompted Aspect Key Point Analysis (PAKPA) for quantitative review summarization. PAKPA employs aspect sentiment analysis and prompt in-context learning with Large Language Models (LLMs) to generate and quantify KPs grounded in aspects for business entities, which achieves faithful KPs with accurate quantification, and remove the need for large amounts of annotated data for supervised training. Experiments on the popular review dataset Yelp and the aspect-oriented review summarization dataset SPACE show that our framework achieves state-of-the-art performance. Source code and data are available at: https://anonymous.4open.science/r/PAKPA-A233",
    "checked": true,
    "id": "ed73a37eb5200c4864e622947c4299886cdfe7a1",
    "semantic_title": "prompted aspect key point analysis for quantitative review summarization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1tLximfwpL": {
    "title": "Empowering Private Tutoring by Chaining Large Language Models",
    "volume": "review",
    "abstract": "Artificial intelligence has been applied in various aspects of online education to facilitate teaching and learning. However, few approaches have been made towards a complete AI-powered tutoring system.In this work, we explore the development of a full-fledged intelligent tutoring system based on large language models (LLMs). The proposed system \\modelname, powered by state-of-the-art LLMs, is equipped with automatic course planning and adjusting, informative instruction, and adaptive quiz offering and evaluation.\\modelname\\ is decomposed into three inter-connected core processes-\\textit{interaction}, \\textit{reflection}, and \\textit{reaction}. Each process is implemented by chaining LLM-powered tools along with dynamically updated memory modules. To demonstrate the mechanism of each working module and the benefits of structured memory control and adaptive reflection, we conduct a wide range of analysis based on statistical results and user study. The analysis shows the designed processes boost system consistency and stability under long-term interaction and intentional disruptions, with up to 5\\% and 20\\% increase in performance respectively. Meanwhile, we also compare the system with scripts from real-world online learning platform and discuss the potential issues unique to LLM-based systems",
    "checked": true,
    "id": "f7842099bbde74dc5aec70bb6af85b88de08ed13",
    "semantic_title": "empowering private tutoring by chaining large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=bXPWHC4TMf": {
    "title": "Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases",
    "volume": "review",
    "abstract": "Definition bias is a negative phenomenon that can mislead models. However, definition bias in information extraction appears not only across datasets from different domains but also within datasets sharing the same domain. We identify two types of definition bias in IE: bias among information extraction datasets and bias between information extraction datasets and instruction tuning datasets. To systematically investigate definition bias, we conduct three probing experiments to quantitatively analyze it and discover the limitations of unified information extraction and large language models in solving definition bias. To mitigate definition bias in information extraction, we propose a multi-stage framework consisting of definition bias measurement, bias-aware fine-tuning, and task-specific bias mitigation. Experimental results demonstrate the effectiveness of our framework in addressing definition bias",
    "checked": true,
    "id": "b34466cc6c387be3991ddb182e5749f0aca7612a",
    "semantic_title": "is there a one-model-fits-all approach to information extraction? revisiting task definition biases",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=GYwjn4p1hl": {
    "title": "MAPO: Advancing Multilingual Reasoning through Multilingual-Alignment-as-Preference Optimization",
    "volume": "review",
    "abstract": "Though reasoning abilities are considered language-agnostic, existing LLMs exhibit inconsistent reasoning abilities across different languages, e.g., reasoning in the dominant language like English is superior to other languages due to the imbalance of multilingual training data. To enhance reasoning abilities in non-dominant languages, we propose a Multilingual-Alignment-as-Preference Optimization framework~(MAPO), aiming to align the reasoning processes in other languages with the dominant language. Specifically, we harness an off-the-shelf translation model for the consistency between answers in non-dominant and dominant languages, which we adopt as the preference for optimization, e.g., Direct Preference Optimization (DPO) or Proximal Policy Optimization (PPO). Experiments show that MAPO stably achieves significant improvements in the multilingual reasoning of various models on all three benchmarks (MSVAMP +16.2%, MGSM +6.1%, and MNumGLUESub +13.3%), with improved reasoning consistency across languages",
    "checked": false,
    "id": "e360eb07461f2741793f99ece8b97a6c04fb2b68",
    "semantic_title": "mapo: advancing multilingual reasoning through multilingual alignment-as-preference optimization",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=5DZTh6gETU": {
    "title": "Learn from Failure: Fine-tuning LLMs with Trial-and-Error Data for Intuitionistic Propositional Logic Proving",
    "volume": "review",
    "abstract": "Recent advances in Automated Theorem Proving have shown the effectiveness of leveraging a (large) language model that generates tactics (i.e. proof steps) to search through proof states. The current model, while trained solely on successful proof paths, faces a discrepancy at the inference stage, as it must sample and try various tactics at each proof state until finding success, unlike its training which does not incorporate learning from failed attempts. Intuitively, a tactic that leads to a failed search path would indicate that similar tactics should receive less attention during the following trials. In this paper, we demonstrate the benefit of training models that additionally learn from failed search paths. Facing the lack of such trial-and-error data in existing open-source theorem-proving datasets, we curate a dataset on intuitionistic propositional logic theorems and formalize it in Lean, such that we can reliably check the correctness of proofs. We compare our model trained on relatively short trial-and-error information (TrialMaster) with models trained only on the correct paths and discover that the former solves more unseen theorems with lower trial searches",
    "checked": true,
    "id": "adfac93d6b6ccc9a83e2e37c337f1cb9c69392df",
    "semantic_title": "learn from failure: fine-tuning llms with trial-and-error data for intuitionistic propositional logic proving",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rf-HfuU9GQ": {
    "title": "Concept Bias Analysis in Chinese MMLU and Introduction of PsyBench: A Benchmark with Reduced Concept Bias in Psychology",
    "volume": "review",
    "abstract": "Many Chinese Massive Multitask Language Understanding (MMLU) benchmarks represent a subject by collecting multiple-choice questions and provide a score to reflect a model's ability in that subject. They have emphasized the comprehensiveness of subject variety but overlooked the comprehensiveness of concepts within individual subjects. We introduce the term \\textit{concept bias}, which refers to the bias caused by the collected questions covering only a portion of the concepts that a subject comprises. Our experiments shows that: 1) the final score can significantly vary depending on the sampled concepts, making it difficult to correlate the final score with the model's actual ability in the subject; 2) the reported model ranking can also be affected. To address this issue, we propose PsyBench: a concept-driven psychology benchmark generated by \\texttt{GPT-4}. We generate high-quality questions for each required concept, thereby reducing concept bias. PsyBench not only fills the gap in the Chinese MMLU series benchmarks for the lack of comprehensive undergraduate-level psychology subjects but also reduces concept bias, offering developers scores that more accurately reflect the model's actual abilities across various subjects",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1T94FC7GpEC": {
    "title": "NewsBench: Systematic Evaluation of LLMs for Writing Proficiency and Safety Adherence in Chinese Journalistic Editorial Applications",
    "volume": "review",
    "abstract": "This study presents NewsBench, a novel benchmark framework developed to evaluate the capability of Large Language Models (LLMs) in Chinese Journalistic Writing Proficiency (JWP) and their Safety Adherence (SA), addressing the gap between journalistic ethics and the risks associated with AI utilization. Comprising 1,267 tasks across 5 editorial applications, 7 aspects (including safety and journalistic writing with 4 detailed facets), and spanning 24 news topics domains, NewsBench employs two GPT-4 based automatic evaluation protocols validated by human assessment. Our comprehensive analysis of 11 LLMs highlighted GPT-4 and ERNIE Bot as top performers, yet revealed a relative deficiency in journalistic ethic adherence during creative writing tasks. These findings underscore the need for enhanced ethical guidance in AI-generated journalistic content, marking a step forward in aligning AI capabilities with journalistic standards and safety considerations",
    "checked": true,
    "id": "ba641af6388844c3df57c98d3549c14110ff0e6b",
    "semantic_title": "newsbench: systematic evaluation of llms for writing proficiency and safety adherence in chinese journalistic editorial applications",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=6H0BantFo7": {
    "title": "Dynamic Retrieval-Augmented Generation",
    "volume": "review",
    "abstract": "Current state-of-the-art large language models are effective in generating high-quality text and encapsulating a broad spectrum of world knowledge. These models, however, often hallucinate and lack locally relevant factual data. Retrieval-augmented approaches were introduced to overcome these problems and provide more accurate responses. Typically, the retrieved information is simply appended to the main request, restricting the context window size of the model. We propose a novel approach for the Dynamic Retrieval-Augmented Generation (DRAG), based on the entity-augmented generation, which injects compressed embeddings of the retrieved entities into the generative model. The proposed pipeline was developed for code-generation tasks, yet can be transferred to some domains of natural language processing. To train the model, we collect and publish a new project-level code generation dataset. We use it for the evaluation along with publicly available datasets. Our approach achieves several targets: (1) lifting the length limitations of the context window, saving on the prompt size; (2) allowing huge expansion of the number of retrieval entities available for the context; (3) alleviating the problem of misspelling or failing to find relevant entity names. This allows the model to beat all baselines (except GPT-3.5) with a strong margin",
    "checked": true,
    "id": "b462417b40852a5a1e2d8862e5d5b464242ff902",
    "semantic_title": "dynamic retrieval-augmented generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EdKfSAtRdiA": {
    "title": "Dynamic Prefix as Instructor for Incremental Named Entity Recognition: A Unified Seq2Seq Generation Framework",
    "volume": "review",
    "abstract": "The Incremental Named Entity Recognition (INER) task aims to update a model to extract entities from an expanding set of entity type candidates due to concerns related to data privacy and scarcity. However, conventional incremental learning methods for INER often suffer from the catastrophic forgetting problem, which leads to the degradation of the model's performance on previously encountered entity types. In this paper, we propose a parameter-efficient dynamic prefix method and formalize INER as a unified seq2seq generation task. By employing the dynamic prefix as a task instructor to guide the generative model, our approach can preserve task-invariant knowledge while adapting to new entities with minimal parameter updates, making it particularly effective in low-resource scenarios. Additionally, we design a generative label augmentation strategy and a novel self-entropy loss to balance the stability and plasticity of the model. Empirical experiments on NER benchmarks demonstrate the effectiveness of our proposed method in addressing the challenges associated with INER",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bbpp7pHfpTY": {
    "title": "MPCoder: Multi-user Personalized Code Generator with Explicit and Implicit Style Representation Learning",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated great potential for assisting developers in their daily development. However, most research focuses on generating correct code, how to use LLMs to generate personalized code has seldom been investigated. To bridge this gap, we proposed MPCoder (Multi-user Personalized Code Generator) to generate personalized code for multiple users. To better learn coding style features, we utilize explicit coding style residual learning to capture the syntax code style standards and implicit style learning to capture the semantic code style conventions. We train a multi-user style adapter to better differentiate the implicit feature representations of different users through contrastive learning, ultimately enabling personalized code generation for multiple users. We further propose a novel evaluation metric for estimating similarities between codes of different coding styles. The experimental results show the effectiveness of our approach for this novel task",
    "checked": true,
    "id": "8d897e09aee65e260cdb6a550f59498aba3949e0",
    "semantic_title": "mpcoder: multi-user personalized code generator with explicit and implicit style representation learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eih3-1LJsDg": {
    "title": "How You Prompt Matters! Even Task-Oriented Constraints in Instructions Affect LLM-Generated Text Detection",
    "volume": "review",
    "abstract": "To combat the misuse of Large Language Models (LLMs), many recent studies have presented LLM-generated-text detectors with promising performance. When users instruct LLMs to generate texts, the instruction can include different constraints depending on the user's need. However, most recent studies do not cover such diverse instruction patterns when creating datasets for LLM detection. In this paper, we find that even task-oriented constraints --- constraints that would naturally be included in an instruction and are not related to detection-evasion --- cause existing detectors to have a large variance in detection performance. We focus on student essay writing as a realistic domain and manually create task-oriented constraints based on several factors for essay quality. Our experiments show that the standard deviation (SD) of current detector performance on texts generated by an instruction with such a constraint is significantly larger (up to an SD of 14.4 F1-score) than that by generating texts multiple times or paraphrasing the instruction. Furthermore, our analysis indicates that the high instruction-following ability of LLMs fosters the large impact of such constraints on detection performance",
    "checked": true,
    "id": "fbf3659c0967703035cfed17481276a576d6f88a",
    "semantic_title": "how you prompt matters! even task-oriented constraints in instructions affect llm-generated text detection",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=ibqea4hOa8S": {
    "title": "M-QALM: A Benchmark to Assess Clinical Reading Comprehension and Knowledge Recall in Large Language Models via Question Answering",
    "volume": "review",
    "abstract": "There is vivid research on adapting Large Language Models (LLMs) to perform a variety of tasks in high-stakes domains, such as healthcare. Despite this popularity, there is a lack of understanding of the extent and contributing factors that allow LLMs to recall relevant knowledge and combine it with presented information---a fundamental pre-requisite for success on down-stream tasks.Addressing this gap, we use Multiple Choice and Abstractive Question Answering to conduct a large-scale empirical study on 22 datasets in three generalist and three specialist biomedical sub-domains. Our multi-faceted analysis of the performance of 15 LLMs, further broken down by sub-domain, source of knowledge and model architecture, uncovers success factors such as instruction tuning that lead to improved recall and comprehension. We further show that while recently proposed domain-adapted models may lack adequate knowledge, directly fine-tuning on our collected medical knowledge datasets shows encouraging results, even generalising to unseen specialist sub-domains. We complement the quantitative results with a skill-oriented manual error analysis, which reveals a significant gap between the models' capabilities to simply recall necessary knowledge and to integrate it with the presented context",
    "checked": true,
    "id": "7009263033f6fdbad688c1cb010a24be3356ba25",
    "semantic_title": "m-qalm: a benchmark to assess clinical reading comprehension and knowledge recall in large language models via question answering",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ZGKqF_ejpv": {
    "title": "Enhancing Fine-Grained Image Classifications via Cascaded Vision Language Models",
    "volume": "review",
    "abstract": "Fine-grained image classification, particularly in zero/few-shot scenarios, presents a significant challenge for vision-language models (VLMs), such as CLIP. These models often struggle with the nuanced task of distinguishing between semantically similar classes due to limitations in their pre-trained recipe, which lacks supervision signals for fine-grained categorization. This paper introduces CascadeVLM, an innovative framework that overcomes the constraints of previous CLIP-based methods by effectively leveraging the granular knowledge encapsulated within large vision-language models (LVLMs). Experiments across various fine-grained image datasets demonstrate that CascadeVLM significantly outperforms existing models, specifically on the Stanford Cars dataset, achieving an impressive 85.6% zero-shot accuracy. Performance gain analysis validates that LVLMs produce more accurate predictions for challenging images that CLIPs are uncertain about, bringing the overall accuracy boost. Our framework sheds light on a holistic integration of VLMs and LVLMs for effective and efficient fine-grained image classification",
    "checked": true,
    "id": "b396bc377404e188251f66fea092b2cead9396b0",
    "semantic_title": "enhancing fine-grained image classifications via cascaded vision language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QPGLmqEBadp": {
    "title": "Analyzing Correlations Between Intrinsic and Extrinsic Bias Metrics of Static Word Embeddings With Their Measuring Biases Aligned",
    "volume": "review",
    "abstract": "We examine the abilities of intrinsic bias metrics of static word embeddings to predict whether Natural Language Processing (NLP) systems exhibit biased behavior. A word embedding is one of the fundamental NLP technologies that represents the meanings of words through real vectors, and problematically, it also learns social biases such as stereotypes. An intrinsic bias metric measures bias by examining a characteristic of vectors, while an extrinsic bias metric checks whether an NLP system trained with a word embedding is biased. A previous study found that a common intrinsic bias metric usually does not correlate with extrinsic bias metrics. However, the intrinsic and extrinsic bias metrics did not measure the same bias in most cases, which makes us question whether the lack of correlation is genuine. In this paper, we extract characteristic words from datasets of extrinsic bias metrics and analyze correlations with intrinsic bias metrics with those words to ensure both metrics measure the same bias. We observed moderate to high correlations with some extrinsic bias metrics but little to no correlations with the others. This result suggests that intrinsic bias metrics can predict biased behavior in particular settings but not in others",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=acTUMnz54D": {
    "title": "AutoDiner: Empowering Restaurant Simulations with Advanced LLMs for Enhanced Agent Collaboration",
    "volume": "review",
    "abstract": "As large language models (LLMs) continue to demonstrate impressive reasoning capabilities, LLM- based multi-agent has become an increasingly compelling area of research. Despite the potential, the field faces a notable gap: the scarcity of LLM-based simulators tailored for realistic, multi-agent interactions. Most existing multi-agent simulators are missing textual interfaces and quantitative evaluation metrics, limiting the assessment of complex interactions between agents. Our proposed simulator,$\\texttt{AutoDiner}$, replicates a detailed restaurant management scenario requiring advanced communication and teamwork among agents, providing a uniquely realistic and complex research environment. $\\texttt{AutoDiner}$ not only fosters intricate agent interactions but also incorporates varying levels of difficulty and performance metrics for comprehensive benchmarking. These features make $\\texttt{AutoDiner}$ an exemplary platform for advancing the understanding and capabilities of LLM-based agents in navigating complex tasks and enhancing cooperative strategies in realistic settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B27uM3dHvw": {
    "title": "Unveiling the Secrets of Engaging Conversations: Factors that Keep Users Hooked on Role-Playing Dialog Agents",
    "volume": "review",
    "abstract": "With the growing humanlike nature of dialog agents, people are now engaging in extended conversations that can stretch from brief moments to substantial periods of time. Understanding the factors that contribute to sustaining these interactions is crucial, yet existing studies primarily focusing on short-term simulations that rarely explore such prolonged and real conversations.In this paper, we investigate the factors influencing retention rates in real interactions with role-playing models. By analyzing a large dataset of interactions between real users and thousands of characters, we systematically examine multiple factors and assess their impact on user retention rate. Surprisingly, we find that the degree to which the bot embodies the roles it plays has limited influence on retention rates, while the length of each turn it speaks significantly affects retention rates. This study sheds light on the critical aspects of user engagement with role-playing models and provides valuable insights for future improvements in the development of large language models for role-playing purposes",
    "checked": true,
    "id": "a60f7de3587067eef017aa8499f8edcfb042ee6e",
    "semantic_title": "unveiling the secrets of engaging conversations: factors that keep users hooked on role-playing dialog agents",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=q4fdaHyuTqt": {
    "title": "VinaLLaMA: LLaMA-based Vietnamese Foundation Model",
    "volume": "review",
    "abstract": "In this paper, we present VinaLLaMA, an open-weight, state-of-the-art (SOTA) Large Language Model for the Vietnamese language, built upon LLaMA-2 with an additional 800 billion trained tokens. VinaLLaMA not only demonstrates fluency in Vietnamese but also exhibits a profound understanding of Vietnamese culture. VinaLLaMA-7B-chat, trained on 1 million high-quality synthetic samples, achieves SOTA results on key benchmarks, including VLSP, VMLU, and Vicuna Vietnamese Benchmark, marking a significant advancement in the Vietnamese AI landscape and offering a versatile resource for various applications",
    "checked": true,
    "id": "d0b4bb148668064c4de27ad667a4cfe72e703946",
    "semantic_title": "vinallama: llama-based vietnamese foundation model",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Q6ChPhnO9S": {
    "title": "INCHARACTER: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews",
    "volume": "review",
    "abstract": "Role-playing agents (RPAs), powered by large language models, have emerged as a flourishing field of applications. However, a key challenge lies in assessing whether RPAs accurately reproduce the personas of target characters, namely their character fidelity. Existing methods mainly focus on the knowledge and linguistic patterns of characters. This paper, instead, introduces a novel perspective to evaluate the personality fidelity of RPAs with psychological scales. Overcoming drawbacks of previous self-report assessments on RPAs, we propose INCHARACTER, namely Interviewing Character agents for personality tests. Experiments include various types of RPAs and LLMs, covering 32 distinct characters on 14 widely used psychological scales. The results validate the effectiveness of INCHARACTER in measuring RPA personalities. Then, with INCHARACTER, we show that state-of-the-art RPAs exhibit personalities highly aligned with the human-perceived personalities of the char023 acters, achieving an accuracy up to 80.7%. Our demo , code, dataset, and results are publicly available",
    "checked": true,
    "id": "4b530e7756a08af082c0ec2b242882b70873f753",
    "semantic_title": "incharacter: evaluating personality fidelity in role-playing agents through psychological interviews",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=p6nCUEyo_QE": {
    "title": "Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments",
    "volume": "review",
    "abstract": "The applications of large language models (LLMs) have expanded well beyond the confines of text processing, signaling a new era where LLMs are envisioned as generalist language agents capable of operating within complex real-world environments. These environments are often highly expansive, making it impossible for the LLM to process them within its short-term memory. Motivated by recent research on extending the capabilities of LLMs with tools, this paper investigates the intriguing potential of tools to augment LLMs in handling such complexity. To this end, we design customized tools to aid in the proactive exploration within these massive environments. Such tools can serve as a middleware layer shielding the LLM from environmental complexity. In two representative complex environments---knowledge bases (KBs) and databases---we demonstrate the significant potential of augmenting language agents with tools in complex environments. Notably, equipped with these tools, GPT-4 achieves 2.8X the performance of the best baseline in tasks requiring access to database content and 2.2X in KB tasks.Our findings illuminate the path for advancing language agents in complex real-world applications",
    "checked": true,
    "id": "ac2fc8c5d4a1f44464f1415ea3dd3ed45398b9d9",
    "semantic_title": "middleware for llms: tools are instrumental for language agents in complex environments",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=GZW3bT2f1d": {
    "title": "Exploring Precision and Recall to assess the quality and diversity of LLMs",
    "volume": "review",
    "abstract": "This paper introduces a novel evaluation framework for Large Language Models (LLMs) such as Llama-2 and Mistral, focusing on the adaptation of Precision and Recall metrics from image generation to text generation. This approach allows for a nuanced assessment of the quality and diversity of generated text without the need for aligned corpora. By conducting a comprehensive evaluation of state-of-the-art language models, the study reveals significant insights into their performance on open-ended generation tasks, which are not adequately captured by traditional benchmarks. The findings highlight a trade-off between the quality and diversity of generated samples, particularly when models are fine-tuned with human feedback. This work extends the toolkit for distribution-based NLP evaluation, offering insights into the practical capabilities and challenges faced by current LLMs in generating diverse and high-quality text",
    "checked": true,
    "id": "5eab7e7ee77f2bbc6639a5c352f87c2855a378ee",
    "semantic_title": "exploring precision and recall to assess the quality and diversity of llms",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=svHF5RTkVX": {
    "title": "Discursive Socratic Questioning: Evaluating the Faithfulness of Language Models' Understanding of Discourse Relations",
    "volume": "review",
    "abstract": "While large language models have significantly enhanced the effectiveness of discourse relation classifications, it remains unclear whether their comprehension is faithful and reliable. We provide DiSQ, a new method for evaluating the faithfulness of understanding discourse based on question answering. We first employ in-context learning to annotate the reasoning for discourse comprehension, based on the connections among key events within the discourse. Following this, DiSQinterrogates the model with a sequence of questions to assess its grasp of core event relations, its resilience to counterfactual queries, as well as its consistency to its previous responses.We then evaluate language models with different architectural designs using DiSQ, finding: (1) DiSQ presents a significant challenge for all models, with the top-performing GPT model attaining only 41% of the ideal performance in PDTB; (2) DiSQ is robust to domain shifts and paraphrase variations; (3) Open-source models generally lag behind their closed-source GPT counterparts, with notable exceptions being those enhanced with chat and code/math features; (4) Our analysis validates the effectiveness of explicitly signalled discourse connectives, the role of contextual information, and the benefits of using historical QA data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kh4pRSrLRsJ": {
    "title": "How do Large Language Models Handle Multilingualism?",
    "volume": "review",
    "abstract": "Large language models (LLMs) demonstrate remarkable performance across a spectrum of languages. In this work, we delve into the question: How do LLMs handle multilingualism? We introduce a framework that depicts LLMs' processing of multilingual inputs: In the first several layers, LLMs understand the question, converting multilingual inputs into English to facilitate the task-solving phase. In the intermediate layers, LLMs engage in problem-solving by thinking in English and incorporating multilingual knowledge to obtain factual content, leveraging the self-attention and feed-forward structures, respectively. In the last several layers, LLMs generate responses that align with the original language of the query. In addition, we investigate the existence of language-specific neurons when processing a certain language. To detect neurons activated by the input language, even without labels, we innovatively design a Parallel Language specific Neuron Detection ($\\texttt{PLND}$) method that effectively measures the significance of neurons when handling multilingual inputs. By comprehensive ablation analysis through deactivating neurons of different layers and structures, we verify the framework that we propose. Additionally, we demonstrate that we can utilize such a framework to effectively enhance the multilingual ability with much less training effort",
    "checked": true,
    "id": "de16e22ddd79fe45df535bcde21566d972187baf",
    "semantic_title": "how do large language models handle multilingualism?",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=5Vapl_dB-Gy": {
    "title": "Hypertext Entity Extraction in Webpage",
    "volume": "review",
    "abstract": "Webpage entity extraction is a fundamental natural language processing task in both research and applications. Nowadays, the majority of webpage entity extraction models are trained on structured datasets which strive to retain textual content and its structure information. However, existing datasets all overlook the rich hypertext features (e.g., font color, font size) which show their effectiveness in previous works. To this end, we first collect a \\textbf{H}ypertext \\textbf{E}ntity \\textbf{E}xtraction \\textbf{D}ataset (\\textit{HEED}) from the e-commerce domains, scraping both the text and the corresponding explicit hypertext features with high-quality manual entity annotations. Furthermore, we present the \\textbf{Mo}E-based \\textbf{E}ntity \\textbf{E}xtraction \\textbf{F}ramework (\\textit{MoEEF}), which efficiently integrates multiple features to enhance model performance by Mixture of Experts and outperforms strong baselines, including the state-of-the-art small-scale models and GPT-3.5-turbo. Moreover, the effectiveness of hypertext features in \\textit{HEED} and several model components in \\textit{MoEEF} are analyzed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bycFDIkKNO": {
    "title": "Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement",
    "volume": "review",
    "abstract": "Although Large Language Models (LLMs) have demonstrated strong performance on a wide range of tasks, they still face reliability challenges such as hallucination. Previous studies reveal that highly capable LLMs like GPT-4 are effective in judging the reliability of individual responses while less capable ones are often tuned to evaluate the relative reliability of responses to the same query. To enable less capable LLMs to effectively judge the reliability of individual responses, we propose a novel method named $\\textit{Meta}$ $\\textit{Ranking}$ (MR). Unlike previous methods, which assess the response directly, we achieve the judgement by comparing the target query-response pair with reference query-response pairs. We found its remarkable effectiveness in error detection for LLM responses on reasoning tasks, where less capable LLMs could outperform strong baselines, even without fine-tuning. We further demonstrate that MR can be used to enhance the performance of LLMs in two practical applications: query routing and iterative training data filtering. The former achieves GPT-4-turbo comparable performance with less than half the token consumption, while the latter makes the instruction-tuned LLaMA-7B and Phi-2, a 2.7B model, significantly surpasses Alpaca-13B over fewer training samples, underscoring the high potential of our proposed method",
    "checked": true,
    "id": "2ac35475ccf0a6a89bbd04377a4fe61c175030a4",
    "semantic_title": "meta ranking: less capable language models are capable for single response judgement",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=owK298rhUA": {
    "title": "PartialFormer: Modeling Part Instead of Whole for Machine Translation",
    "volume": "review",
    "abstract": "The design choices in Transformer feed-forward neural networks have resulted in significant computational and parameter overhead. In this work, we emphasize the importance of hidden dimension in designing lightweight FFNs, a factor often overlooked in previous architectures. Guided by this principle, we introduce PartialFormer, a parameter-efficient Transformer architecture utilizing multiple smaller FFNs to reduce parameters and computation while maintaining essential hidden dimensions. These smaller FFNs are integrated into a multi-head attention system to enable effective collaboration. We also propose a tailored head scaling strategy to enhance PartialFormer's capabilities. Furthermore, we present a residual-like attention calculation to improve depth scaling within PartialFormer. Extensive experiments on 9 translation tasks and 1 abstractive summarization task validate the effectiveness of our PartialFormer approach on machine translation and summarization tasks",
    "checked": true,
    "id": "a6c28210ca0ce823dd3e873c7568d35331516dcd",
    "semantic_title": "partialformer: modeling part instead of whole for machine translation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UetSI3pI70": {
    "title": "Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering",
    "volume": "review",
    "abstract": "Open-ended question answering requires models to find appropriate evidence to form well-reasoned, comprehensive and helpful answers. In practical applications, models also need to engage in extended discussions on potential scenarios closely relevant to the question. With augmentation of retrieval module, open-source Large Language Models~(LLMs) can produce coherent answers often with different focuses, but are still sub-optimal in terms of reliable evidence selection and in-depth question analysis. In this paper, we propose a novel Chain-of-Discussion framework to leverage the synergy among multiple open-source LLMs aiming to provide \\textbf{more correct} and \\textbf{more comprehensive} answers for open-ended QA, although they are not strong enough individually. Our experiments show that discussions among multiple LLMs play a vital role in enhancing the quality of answers. We will release our data and code for further research",
    "checked": true,
    "id": "12fa32d4ba0da26372747eaff7b34f353e26e838",
    "semantic_title": "chain-of-discussion: a multi-model framework for complex evidence-based question answering",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=pz0FdrshOk": {
    "title": "Tracking the Newsworthiness of Public Documents",
    "volume": "review",
    "abstract": "Journalists must find stories in huge amounts of textual data (e.g. leaks, bills, press releases) as part of their jobs: determining \\textit{when} and \\textit{why} text becomes news can help us understand coverage patterns and help us build assistive tools. Yet, this is challenging because very few labelled links exist, language use between corpora is very different, and text may be covered for a variety of reasons.In this work we focus on news coverage of local public policy in the San Francisco Bay Area by the \\textit{San Francisco Chronicle}. First, we gather news articles, public policy documents and meeting recordings and link them using \\textit{probabilistic relational modeling}, which we show is a low-annotation linking methodology that outperforms other retrieval-based baselines. Second, we define a new task: \\textbf{\\textit{newsworthiness prediction}}, to predict if a policy item will get covered. We show that different aspects of public policy discussion yield different newsworthiness signals. Finally we perform human evaluation with expert journalists and show our systems identify policies they consider newsworthy with 68\\% F1 and our coverage recommendations are helpful with an 84\\% win-rate",
    "checked": true,
    "id": "90f47e15a1bcb213344e4dbe82d406af6ac3656d",
    "semantic_title": "tracking the newsworthiness of public documents",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=csEQp6D6oWD": {
    "title": "Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder",
    "volume": "review",
    "abstract": "Reconstructing natural language from non-invasive electroencephalography (EEG) holds great promise as a language decoding technology for brain-computer interfaces (BCIs). However, EEG-based language decoding is still in its nascent stages, facing several technical issues such as: 1) Absence of a hybrid strategy that can effectively integrate cross-modality (between EEG and text) self-learning with intra-modality self-reconstruction of EEG features or textual sequences; 2) Under-utilization of large language models (LLMs) to enhance EEG-based language decoding. To address above issues, we propose the Contrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that orchestrates compound self-supervised learning across and within EEG and text through a dedicated multi-stream encoder. Furthermore, we develop a framework called E2T-PTR (EEG-to-Text decoding using Pretrained Transferable Representations), which leverages pre-trained modules alongside the EEG stream from CET-MAE and further enables an LLM (specifically BART) to decode text from EEG sequences. Comprehensive experiments conducted on the popular text-evoked EEG database, ZuCo, demonstrate the superiority of E2T-PTR, which outperforms the state-of-the-art in ROUGE-1 F1 and BLEU-4 scores by 8.34% and 32.21%, respectively. These results indicate significant advancements in the field and underscores the proposed framework's potential to enable more powerful and widespread BCI applications",
    "checked": true,
    "id": "9048cb9c7108cfabc4f64b0a11e0ad9e62c84f99",
    "semantic_title": "enhancing eeg-to-text decoding through transferable representations from pre-trained contrastive eeg-text masked autoencoder",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IUdJM5HJySV": {
    "title": "Evaluating Mathematical Reasoning of Large Language Models: A Focus on Error Identification and Correction",
    "volume": "review",
    "abstract": "The rapid advancement of Large Language Models (LLMs) in the realm of mathematical reasoning necessitates comprehensive evaluations to gauge progress and inspire future directions. Existing assessments predominantly focus on problem-solving from the examinee perspective, overlooking a dual perspective of examiner regarding error identification and correction. From the examiner perspective, we define four evaluation tasks for error identification and correction along with a new dataset with annotated error types and steps. We also design diverse prompts to thoroughly evaluate eight representative LLMs. Our principal findings indicate that GPT-4 outperforms all models, while open-source model LLaMA-2-7B demonstrates comparable abilities to closed-source models GPT-3.5 and Gemini Pro. Notably, calculation error proves the most challenging error type. Moreover, prompting LLMs with the error types can improve the average correction accuracy by 47.9\\%. These results reveal potential directions for developing the mathematical reasoning ability of LLMs",
    "checked": true,
    "id": "b7f9f6ac44fee822c692cdc1147c852a150f4aea",
    "semantic_title": "evaluating mathematical reasoning of large language models: a focus on error identification and correction",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=aYt2ggtDOr": {
    "title": "Beyond Accuracy Optimization: Computer Vision Losses for Large Language Model Fine-Tuning",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have achieved promising performance on Math Word Problem (MWP) and Question Answering (QA) tasks. LLM fine-tuning is commonly based on cross-entropy loss minimization to perform accurate predictions. However, the standard cross-entropy function neither considers the underlying token distribution over training data nor weighs differently correct and misclassified samples. To address tasks such as closed-ended QA and step-by-step MWP resolution LLMs require advanced language reasoning capabilities. This prompts the adoption of established computer vision loss functions that optimize LLMs' performance rather than simple accuracy. This paper shows the higher effectiveness of combining cross-entropy with computer vision loss functions across MWPs and closed-ended QA datasets. We show relevant LLMs' performance improvements with equal model complexity and the same number of training samples or even fewer. We also demonstrate the efficacy of reproducing step-by-step reasoning on the MWP task",
    "checked": false,
    "id": "040c3b2104e4b9379498da119cc331252dc450b4",
    "semantic_title": "oh! we freeze: improving quantized knowledge distillation via signal propagation analysis for large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Royo7My_EJ": {
    "title": "Selective Prompting Tuning for Personalized Conversations with LLMs",
    "volume": "review",
    "abstract": "In conversational AI, personalizing dialogues with persona profiles and contextual understanding is essential. Despite large language models' (LLMs) improved response coherence, effective persona integration remains a challenge. In this work, we first study two common approaches for personalizing LLMs: textual prompting and direct fine-tuning. We observed that textual prompting often struggles to yield responses that are similar to the ground truths in datasets, while direct fine-tuning tends to produce repetitive or overly generic replies. To alleviate those issues, we propose $\\textbf{S}$elective $\\textbf{P}r$ompt $\\textbf{T}$uning (SPT), which softly prompts LLMs for personalized conversations in a selective way. Concretely, SPT initializes a set of soft prompts and uses a trainable dense retriever to adaptively select suitable soft prompts for LLMs according to different input contexts, where the prompt retriever is dynamically updated through feedback from the LLMs. Additionally, we propose context-prompt contrastive learning and prompt fusion learning to encourage the SPT to enhance the diversity of personalized conversations. Experiments on the CONVAI2 dataset demonstrate that SPT significantly enhances response diversity by up to $90\\%$, along with improvements in other critical performance indicators. Those results highlight the efficacy of SPT in fostering engaging and personalized dialogue generation",
    "checked": true,
    "id": "fff1e9eb32f8591aea3f2db54035465093381b89",
    "semantic_title": "selective prompting tuning for personalized conversations with llms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l7vV1MZKx-": {
    "title": "Reverse That Number! Decoding Order Matters in Arithmetic Learning",
    "volume": "review",
    "abstract": "Recent advancements in pretraining have demonstrated that modern Large Language Models (LLMs) possess the capability to effectively learn arithmetic operations. However, despite acknowledging the significance of digit order in arithmetic computation, current methodologies predominantly rely on sequential, step-by-step approaches for teaching LLMs arithmetic, resulting in a conclusion where obtaining better performance involves fine-grained step-by-step. Diverging from this conventional path, our work introduces a novel strategy that not only reevaluates the digit order by prioritizing output from the least significant digit but also incorporates a step-by-step methodology to substantially reduce complexity. We have developed and applied this method in a comprehensive set of experiments. Compared to the previous state-of-the-art (SOTA) method, our findings reveal an overall improvement of $11.1\\%$ in accuracy while requiring only a third of the tokens typically used during training. For the purpose of facilitating replication and further research, we have made our code and dataset publicly available at \\url{https://anonymous.4open.science/r/RAIT-9FB7/}",
    "checked": true,
    "id": "1a41726a9648fcaea5e6d1a460b2b8c0639197e1",
    "semantic_title": "reverse that number! decoding order matters in arithmetic learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wF7UhK_aYV": {
    "title": "MAP's not dead yet: Uncovering true language model modes by conditioning away degeneracy",
    "volume": "review",
    "abstract": "It has been widely observed that exact or approximate MAP (mode-seeking) decoding from natural language generation (NLG) models consistently leads to degenerate outputs (Holtzman et al., 2019; Stahlberg and Byrne, 2019). Prior work has attributed this behavior to either a fundamental and unavoidable inadequacy of modes in probabilistic models or weaknesses in language modeling. Contrastingly, we argue that degenerate modes can even occur in the absence of any modeling error, due to contamination of the training data. Specifically, we argue that mixing even a tiny amount of low-entropy noise with a population text distribution can cause the data distribution's mode to become degenerate. We therefore propose to apply MAP decoding to the model's true conditional distribution where the conditioning variable explicitly avoids specific degenerate behavior. Using exact search, we empirically verify that the length-conditional modes of machine translation models and language models are indeed more fluent and topical than their unconditional modes. For the first time, we also share many examples of exact modal sequences from these models, and from several variants of the LLaMA-7B model. Notably, we observethat various kinds of degenerate modes persist, even at the scale of LLaMA-7B. Although we cannot tractably address these degeneracieswith exact search, we perform a classifier-based approximate search on LLaMA-7B, a model which was not trained for instruction following, and find that we are able to elicit reasonable outputs without any finetuning",
    "checked": true,
    "id": "97732c811a27bb57a78c3a166aff530cc3eff710",
    "semantic_title": "map's not dead yet: uncovering true language model modes by conditioning away degeneracy",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=HaL0BgOHVF": {
    "title": "NCPrompt: NSP-Based Prompt Learning and Contrastive Learning for Implicit Discourse Relation Recognition",
    "volume": "review",
    "abstract": "Implicit Discourse Relation Recognition (IDRR) is an important task to classify the discourse relation sense between argument pairs without an explicit connective. Recently, prompt learning methods have demonstrated success in dealing with IDRR. However, prior work primarily transform IDRR into a connective-cloze task based on the masked language model (MLM), which limits the predicted word to one single token. Besides, these methods use hand-crafted verbalizers which are time-consuming and less convincing. In this paper, we propose NCPrompt, an NSP-based prompt learning and Contrastive learning method for IDRR. Specifically, we automatically search the optimal verbalizer for IDRR based on the statistical and expressive features of connectives. Furthermore, we transform the IDRR task into a next sentence prediction (NSP) task and introduce contrastive learning by constructing augmentation views. In this way, the answer words of multiple tokens can convey more precise meaning and contrastive learning can help to generate more informative embeddings, expected to boost the model performance. To our knowledge, we are the first to apply NSP to handle the IDRR task. Experiments on the PDTB 3.0 corpus have demonstrated the effectiveness and superiority of our proposed model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K_rRf5lf96t": {
    "title": "Multimodal Table Understanding",
    "volume": "review",
    "abstract": "Although great progress has been made by previous table understanding methods including recent approaches based on large language models (LLMs), they are seriously dependent on the premise that all given tables must be converted into a certain text sequence (such as Markdown or HTML) to serve as model input. However, it is difficult to access such textual table representations in some practical scenarios, and the table images are much more accessible. Therefore, how to directly understand tables using intuitive visual information is a crucial and urgent challenge for more applications. In this paper, we propose a new problem, multimodal table understanding, where the model is required to generate correct responses to various table-related requests (e.g., questions) according to the given table image. To support research on this problem, we construct a large-scale dataset named MMTab, which covers diverse table tasks and can facilitate both the model training and evaluation. On this basis, we develop a generalist tabular multimodal large language models (MLLMs) Table-LLaVA, which significantly outperforms open-source MLLM baselines on 24 benchmarks including held-in and held-out settings",
    "checked": true,
    "id": "dea212a431e6cae3a3e66657d6cbbd3b7c55925f",
    "semantic_title": "multimodal table understanding",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=N8gARwhHt4": {
    "title": "CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation",
    "volume": "review",
    "abstract": "Since the natural language processing (NLP) community started to make large language models (LLMs) act as a critic to evaluate the quality of generated texts, most of the existing works train a critique generation model on the evaluation data labeled by GPT-4's direct prompting. We observe that these models lack the ability to generate informative critiques in both pointwise grading and pairwise comparison especially without references. As a result, their generated critiques cannot provide fine-grained distinguishability on generated texts, causing unsatisfactory evaluation performance. In this paper, we propose a simple yet effective method called Eval-Instruct, which can first acquire pointwise grading critiques with pseudo references and then revise these critiques via multi-path prompting to obtain informative evaluation data in different tasks and settings, including pointwise grading and pairwise comparison with / without references. After fine-tuning on these data, the resulting model CritiqueLLM is empirically shown to outperform ChatGPT and all the open-source baselines and even achieve comparable evaluation performance to GPT-4 in system-level correlations of pointwise grading. We also demonstrate that our generated critiques can act as scalable feedback to further improve the generation quality of strong LLMs like ChatGPT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z6XKPgl-Bm": {
    "title": "TEncDM: Understanding the Properties of Diffusion Model in the Space of Language Model Encodings",
    "volume": "review",
    "abstract": "Drawing inspiration from the success of diffusion models in various domains, numerous research papers proposed methods for adapting diffusion models to the text domain. Despite these efforts, none of them has managed to achieve the quality of large language models. In this paper, we conduct a comprehensive analysis of key components of the text diffusion models and introduce a novel approach named Text Encoding Diffusion Model (TEncDM). Instead of the commonly used token embedding space, we train our model in the space of the language model encodings. Additionally, we propose to use a Transformer-based decoder that utilizes contextual information for text reconstruction. We also analyse self-conditioning and find that it increases the magnitude of the model outputs, allowing the reduction of the number of denoising steps at the inference stage. Evaluation of TEncDM on two downstream text generation tasks, QQP and XSum, demonstrates its superiority over existing non-autoregressive models",
    "checked": true,
    "id": "6073b70ccda321f80bec80eaad01765fa8381419",
    "semantic_title": "tencdm: understanding the properties of diffusion model in the space of language model encodings",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PpXtYidJ3b": {
    "title": "Do Metaphorical Analogies Come Naturally to Language Models?",
    "volume": "review",
    "abstract": "The ability to compare by analogy, metaphorically or not, lies at the core on how humans understand the world and communicate. In this paper, we study the ability of a wide range of pretrained transformer-based language models to identify metaphors from other types of analogies, including incorrect ones. In particular, we are interested in discovering whether language models interpret metaphorical analogies equally well as other types of analogies, and whether the model size has an impact on this ability. The results show that most language models have a higher difficulty at identifying metaphors compared to other types of analogy. Finally, we found that there are relevant differences in how language models approach the task, with the larger models reducing the gap when it comes to analogical understanding, and for distinguishing metaphors from incorrect analogies",
    "checked": false,
    "id": "075bd30a886c0fbf19c94df68e2f2dbdd59354e0",
    "semantic_title": "relative periodic points on solvmanifolds",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t0APLULk9E": {
    "title": "Human-in-the-Loop through Chain-of-Thought",
    "volume": "review",
    "abstract": "While the emergence of powerful language models along with Chain-of-thought prompting has made automation more and more omnipresent, it sometimes demonstrates its weakness in long-term or multi-step logical reasoning. For example, users don't always get desirable answers for complex mathematical problems without human involvement. Against this background, we present the Manual Correction System (MCS) --- a human-in-the-loop system enhanced by Chain-of-Thought prompting, which explores how manual correction of sub-logics in rationales can improve LLM's reasoning performance. Moving one step forward, considering a system with human-in-the-loop involves more than having humans improve performance but also controlling the cost. Therefore, we post a Cost-utility Analysis Model for Human-in-the-Loop systems (CAMLOP) based on classical economics theory to analyze, quantify and balance the utility and the corresponding cost. We conduct experiments of MCS and CAMLOP with twelve datasets. A significant advantage w.r.t cost and utility proves its superiority over strong baselines",
    "checked": true,
    "id": "4713dc19179cdd9083e47067fa9504751f8759c6",
    "semantic_title": "human-in-the-loop through chain-of-thought",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=2BlYBpqXzvZ": {
    "title": "Understanding and Patching Compositional Reasoning in LLMs",
    "volume": "review",
    "abstract": "LLMs have marked a revolutonary shift, yet they falter when faced with compositional reasoning tasks.Our research embarks on a quest to uncover the root causes of compositional reasoning failures of LLMs, uncovering that most of them stem from the improperly generated or leveraged implicit reasoning results.Inspired by our empirical findings, we resort to Logit Lens and an intervention experiment to dissect the inner hidden states of LLMs. This deep dive reveals that implicit reasoning results indeed surface within middle layers and play a causative role in shaping the final explicit reasoning results.Our exploration further locates multi-head self-attention (MHSA) modules within these layers, which emerge as the linchpins in accurate generation and leveraing of implicit reasoning results.Grounded on the above findings, we develop CREME, a lightweight method to patch errors in compositional reasoning via editing the located MHSA modules. Our empirical evidence stands testament to CREME's effectiveness, paving the way for autonomously and continuously enhancing compositional reasoning capabilities in language models",
    "checked": true,
    "id": "7c09d7b00e4b68bab51d46cd405636483b1269c9",
    "semantic_title": "understanding and patching compositional reasoning in llms",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=SFn8FqzXLG": {
    "title": "Head-wise Shareable Attention for Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) suffer from huge number of parameters, which restricts their deployment on edge devices. Weight sharing is one promising solution that encourages weight reuse, effectively reducing memory usage with less performance drop. However, current weight sharing techniques primarily focus on small-scale models like BERT and employ coarse-grained sharing rules, e.g., layer-wise. This becomes limiting given the prevalence of LLMs and sharing an entire layer or block obviously diminishes the flexibility of weight sharing. In this paper, we present a perspective on $\\textit{\\textbf{head-wise shareable attention for large language models}}$. We further propose two memory-efficient methods that share parameters across attention heads, with a specific focus on LLMs. % to reduce the memory usage for large PLMs. Both of them use the same dynamic strategy to select the shared weight matrices. The first method directly reuses the pre-trained weights without retraining, denoted as $\\textbf{DirectShare}$. The second method first post-trains with constraint on weight matrix similarity and then shares, denoted as $\\textbf{PostShare}$. Experimental results reveal our head-wise shared models still maintain satisfactory capabilities, demonstrating the feasibility of fine-grained weight sharing applied to LLMs",
    "checked": true,
    "id": "ee92723f03475fcfbb37ce8b8c888c497453ceb9",
    "semantic_title": "head-wise shareable attention for large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NLSCEwn2mG": {
    "title": "HQP: A Human-Annotated Dataset for Detecting Online Propaganda",
    "volume": "review",
    "abstract": "Online propaganda poses a severe threat to the integrity of societies. However, existing datasets for detecting online propaganda have a key limitation: they were annotated using weak labels that can be noisy and even incorrect. To address this limitation, our work makes the following contributions: (1) We present HQP: a novel dataset (N=30000) for detecting online propaganda with high-quality labels. To the best of our knowledge, HQP is the first dataset for detecting online propaganda that was created through human annotation. (2) We show empirically that state-of-the-art language models fail in detecting online propaganda when trained with weak labels (AUC: 64.03). In contrast, state-of-the-art language models can accurately detect online propaganda when trained with our high-quality labels (AUC: 92.25), which is an improvement of ~44%. (3) We show that prompt-based learning using a small sample of high-quality labels can still achieve a reasonable performance (AUC: 80.27) while significantly reducing the cost of labeling. (4)~We extend HQP to HQP+ to test how well propaganda across different contexts can be detected. Crucially, our work highlights the importance of high-quality labels for sensitive NLP tasks such as propaganda detection",
    "checked": true,
    "id": "83205781287d3c8b974027da782d9a42d436f1cf",
    "semantic_title": "hqp: a human-annotated dataset for detecting online propaganda",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=BkMB_WDAMB": {
    "title": "Adversarial Preference Optimization",
    "volume": "review",
    "abstract": "Human preference alignment is essential to improve the interaction quality of large language models (LLMs). Existing aligning methods depend on manually annotated preference data to guide the LLM optimization directions.However, in practice, continuously updating LLMs raises a distribution gap between model-generated samples and human-preferred responses, which hinders model fine-tuning efficiency. To mitigate this issue, previous methods require additional preference annotation on generated samples to adapt the shifted distribution, which consumes a large amount of annotation resources. Targeting more efficient human preference optimization, we propose an adversarial preference optimization (APO) framework, where the LLM agent and the preference model update alternatively via a min-max game. Without additional annotation, our APO method can make a self-adaption to the generation distribution gap through the adversarial learning process. Based on comprehensive experiments, we find APO further enhances the alignment performance of baseline methods in terms of helpfulness and harmlessness",
    "checked": true,
    "id": "84259db14b725853ecfe425fe85ca375b32983c2",
    "semantic_title": "adversarial preference optimization",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=HqUVCw5In5h": {
    "title": "LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments",
    "volume": "review",
    "abstract": "Recent advancements in large language models (LLMs) have revealed their potential for achieving autonomous agents possessing human-level intelligence. However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions. There is a lack of a benchmark that evaluates the diverse capabilities of LLM agents in multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel and easily extensible framework for evaluating the diverse capabilities of LLM in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming environments, employing Trueskill scoring to assess crucial abilities in LLM agents, including spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration. We conduct an extensive experiment and human evaluation among different sizes and types of LLMs, showing that LLMs still have a significant journey ahead in their development towards becoming fully autonomous agents, especially in opponent modeling and team collaboration. We hope LLMArena could guide future research towards enhancing these capabilities in LLMs, ultimately leading to more sophisticated and practical applications in dynamic, multi-agent settings",
    "checked": true,
    "id": "bb67e739b5056931e51eecfe2100a53031c04c19",
    "semantic_title": "llmarena: assessing capabilities of large language models in dynamic multi-agent environments",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=eExwTP0Fwv": {
    "title": "Parallel Decoding via Hidden Transfer for Lossless Large Language Model Acceleration",
    "volume": "review",
    "abstract": "Large language models (LLMs) have recently shown remarkable performance across a wide range of tasks. However, the substantial number of parameters in LLMs contributes to significant latency during model inference. This is particularly evident when utilizing autoregressive decoding methods, which generate one token in a single forward process, thereby not fully capitalizing on the parallel computing capabilities of GPUs. In this paper, we propose a novel parallel decoding approach, namely hidden transfer, which decodes multiple successive tokens simultaneously in a single forward pass. The idea is to transfer the intermediate hidden states of the previous context to the pseudo hidden states of the future tokens to be generated, and then the pseudo hidden states will pass the following transformer layers thereby assimilating more semantic information and achieving superior predictive accuracy of the future tokens. Besides, we use the novel tree attention mechanism to simultaneously generate and verify multiple candidates of output sequences, which ensure the lossless generation and further improves the generation efficiency of our method. Experiments demonstrate the effectiveness of our method. We conduct a lot of analytic experiments to prove our motivation. In terms of acceleration metrics, we outperform all the single-model acceleration techniques, including Medusa and Self-Speculative decoding",
    "checked": true,
    "id": "74a86e171f97a650a188fcbdd5e0ccd7b9d844b6",
    "semantic_title": "parallel decoding via hidden transfer for lossless large language model acceleration",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3lV0iUcOCKy": {
    "title": "Building Bridges: A Dataset for Evaluating Gender-Fair Machine Translation into German",
    "volume": "review",
    "abstract": "The translation of gender-neutral person terms (e.g., the students) is often non-trivial. An interesting case poses the translation from English to German â€“ in German, every noun is gendered, and if the gender of the referent(s) is unknown or diverse, the generic masculine (die Studenten (m.)) is commonly used. This, however, reduces the visibility of other genders, such as women and non-binary people. To counteract gender discrimination, a societal movement towards using gender-fair language exists (e.g., by adopting neosystems). However, gender-fair German is currently barely supported in Machine Translation (MT), requiring costly post-editing or manual translations. We address this research gap by studying gender-fair language in English to German MT. Concretely, we enrich a community-created gender-fair language dictionary, and sample multi-sentence test instances from encyclopedic text and parliamentary speeches. Using these novel resources, we conduct the first benchmark study involving two commercial systems and six neural MT models for translating words in isolation and words in larger contexts across two domains. Our findings show that most systems produce mainly masculine forms, and rarely gender-neutral variants, high- lighting the need for future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RSNs_HnQs1": {
    "title": "Aligning Translation-Specific Understanding to General Understanding in Large Language Models",
    "volume": "review",
    "abstract": "Although large language models (LLMs) have shown surprising language understanding and generation capabilities, they have yet to gain a revolutionary advancement in the field of machine translation. One potential cause of the limited performance is the misalignment between the translation-specific understanding and general understanding inside LLMs. To align the translation-specific understanding to the general one, we propose a novel translation process \\textsc{xIoD} (\\textbf{Cross-Lingual} \\textbf{I}nterpretation \\textbf{o}f \\textbf{D}ifficult words), explicitly incorporating the general understanding on the content incurring inconsistent understanding to guide the translation.= Specifically, \\textsc{xIoD} performs the cross-lingual interpretation for the difficult-to-translate words and enhances the translation with the generated interpretations. Furthermore, we reframe the external tools of QE to tackle the challenges of \\textsc{xIoD} in the detection of difficult words and the generation of helpful interpretations. We conduct experiments on the self-constructed benchmark Challenge-MT, which includes cases in which multiple SOTA translation systems consistently underperform. Experimental results show the effectiveness of our \\textsc{xIoD}, which improves up to +3.85 COMET. Human evaluation reveals that the translation generated by \\textsc{xIoD} accords more with the sense-for-sense translation",
    "checked": true,
    "id": "a23a89855e3af2e6cec7fd4a01e12cacdf6c727f",
    "semantic_title": "aligning translation-specific understanding to general understanding in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JejlenvCV_": {
    "title": "The Marriage of Foundation Models and Federated Learning: A Survey",
    "volume": "review",
    "abstract": "The recent development of Foundation Models (FMs), represented by large language models, vision transformers, and multimodal models, has been making a significant impact on both academia and industry. Compared with small-scale models, FMs have a much stronger demand for high-volume data during the pre-training phase. Although general FMs can be pre-trained on data collected from open sources such as the Internet, domain-specific FMs need proprietary data, posing a practical challenge regarding the amount of data available due to privacy concerns. Federated Learning (FL) is a collaborative learning paradigm that breaks the barrier of data availability from different participants. Therefore, it provides a promising solution to customize and adapt FMs to a wide range of domain-specific tasks using distributed datasets whilst preserving privacy. This survey paper discusses the potentials and challenges of synergizing FL and FMs and summarizes core techniques, future directions, and applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xgAsWjEdlr": {
    "title": "MedKP: Medical Dialogue with Knowledge Enhancement and Clinical Pathway Encoding",
    "volume": "review",
    "abstract": "With appropriate data selection and training techniques, Large Language Models (LLMs) have demonstrated exceptional success in various medical examinations and multiple-choice questions. However, the application of LLMs in medical dialogue generationâ€”a task more closely aligned with actual medical practiceâ€”has been less explored. This gap is attributed to the insufficient medical knowledge of LLMs, which leads to inaccuracies and hallucinated information in the generated medical responses. In this work, we introduce the Medical dialogue with Knowledge enhancement and clinical Pathway encoding (MedKP) framework, which integrates an external knowledge enhancement module through a medical knowledge graph and an internal clinical pathway encoding via medical entities and physician actions. Evaluated with comprehensive metrics, our experiments on two large-scale, real-world online medical consultation datasets (MedDG and KaMed) demonstrate that MedKP surpasses multiple baselines and mitigates the incidence of hallucinations, achieving a new state-of-the-art. Extensive ablation studies further reveal the effectiveness of each component of MedKP. This enhancement advances the development of reliable, automated medical consultation responses using LLMs, thereby broadening the potential accessibility of precise and real-time medical assistance",
    "checked": true,
    "id": "feb1bd0efd2ed3a846f9a97e35aedf1e781a647f",
    "semantic_title": "medkp: medical dialogue with knowledge enhancement and clinical pathway encoding",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=macqyKKnsz": {
    "title": "A Comprehensive Study of Jailbreak Attack versus Defense for Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have increasingly become central to generating content with potential societal impacts. Notably, these models have demonstrated capabilities for generating content that could be deemed harmful. To mitigate these risks, researchers have adopted safety training techniques to align model outputs with societal values to curb the generation of malicious content. However, the phenomenon of \"jailbreaking\" â€” where carefully crafted prompts elicit harmful responses from models â€” persists as a significant challenge. This research conducts a comprehensive analysis of existing studies on jailbreaking LLMs and their defense techniques. We meticulously investigates nine attack techniques and seven defense techniques, applied across three distinct language models: Vicuna, LLama, and GPT-3.5 Turbo. We aim to evaluate the effectiveness of these attack and defense techniques. Our findings reveal that existing white-box attacks underperform compared to universal techniques, and that the inclusion of special tokens in the input significantly affects the likelihood of successful attacks. This research highlights the imperative need to concentrate on the security facets of LLMs. Additionally, we contribute to the field by releasing our datasets and testing framework, aiming to foster further research into LLM security. We believe these contributions will facilitate the exploration of security measures within this domain",
    "checked": true,
    "id": "53092cd4e91f641134617add052329e2495a4fad",
    "semantic_title": "a comprehensive study of jailbreak attack versus defense for large language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Kkbh9C2C6X4": {
    "title": "To Err Is Human, but Llamas Can Learn It Too",
    "volume": "review",
    "abstract": "This study explores enhancing grammatical error correction (GEC) through automatic error generation (AEG) using language models (LMs). Specifically, we fine-tune Llama 2 LMs for error generation and find that this approach yields synthetic errors akin to human errors. Next, we train GEC Llama models using these artificial errors and outperform previous state-of-the-art error correction models, with gains ranging between 0.8 and 6 F0.5 points across all tested languages (German, Ukrainian, and Estonian). Moreover, we demonstrate that generating errors by fine-tuning smaller sequence-to-sequence models and prompting large commercial LMs (GPT3.5 and GPT4) also results in synthetic errors beneficially affecting error generation models. We openly release trained models for error generation and correction as well as all the synthesized error datasets for the covered languages",
    "checked": true,
    "id": "7ea2b0d6bab8e5a6ca0660c8737d7938cd3b711d",
    "semantic_title": "to err is human, but llamas can learn it too",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=K98OWyaGzG": {
    "title": "SSP-CLT: Self-Supervised Prompting for Cross-Lingual Transfer to Low-Resource Languages using Large Language Models",
    "volume": "review",
    "abstract": "In-Context Learning (ICL) is a widely embraced paradigm for eliciting task-specific capabilities from large language models (LLMs). Present-day LLMs with ICL have shown exceptional performance on several English NLP tasks, but their utility on other languages is still underexplored. Our work investigates their effectiveness for NLP tasks in low-resource languages (LRLs), especially for cross-lingual transfer, where task-specific training data for one or more related languages is available. We propose Self-Supervised Prompting for Cross-Lingual Transfer (SSP-CLT), a novel approach for zero-shot cross-lingual transfer to LRLs. SSP-CLT works in two stages and has 2 variants. In first variant, in Stage I, for a given target test instance, exemplars are retrieved from source training data and included in the LLM prompt for ICL -- this obtains an initial labeling. Once all test data instances are labeled, Stage II repeats the whole process, but draws exemplars from Stage I labelings of other test datapoints in the target language. The second variant of SSP-CLT uses a fine-tuned model for stage 1 predictions, while stage 2 uses an Integer Linear Programming (ILP)-based exemplar selection that balances similarity, confidence and label coverage.Experiments on 3 tasks and 3 language families demonstrate that SSP-CLT strongly outperforms supervised baselines and also other prompting approaches",
    "checked": false,
    "id": "4e298242fd18904c09c67729d8f417546b0d02d0",
    "semantic_title": "ssp: self-supervised prompting for cross-lingual transfer to low-resource languages using large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VgzGNT8hpD": {
    "title": "DPPA: Pruning Method for Large Language Model to Model Merging",
    "volume": "review",
    "abstract": "Model merging is to combine fine-tuned models from multiple domains to enhance the model's capabilities across various domains. Merging performance degradation is due to parameter conflicts. The prevailing methods address this issue of parameter conflicts during the merging stage, but recently scholars have been paying more attention to resolving this problem during the pruning stage. DARE has demonstrated promising results on a simple fine-tuned model. However, this approach exhibit diminished effectiveness when applied to complex fine-tuned models that has significant parameter bias compared to the baseline model. In this study, we propose a two-stage method called DPPA to address the challenge of fusing complex fine-tuned models. First, we introduce Dynamically Pruning (DP), an improved approach based on magnitude pruning which aim is to enhance performance at higher pruning rates. Subsequently, we propose Dynamically Partition Amplification (DPA), a rescaling technique that aims to dynamically amplify partitions of parameters based on their varying levels of significance. The experimental results show that our approach retains only 20\\% of the specific domain parameters, yet achieves comparable performance to other methods that retain 90\\% of the specific domain parameters. Furthermore, our method, due to its exceptional performance after pruning, also achieves a significant improvement of nearly 20\\% in model merging. We will make our code on Github",
    "checked": true,
    "id": "08920921ce6f4efe0dd92f6005a755b07d4ce760",
    "semantic_title": "dppa: pruning method for large language model to model merging",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=8np2r6_vTW8": {
    "title": "Knowledge Injection for Large Language Models",
    "volume": "review",
    "abstract": "Generative Large Language Models (LLMs), such as ChatGPT and GPT-4, offer interactive APIs that can answer common questions at the human-expert level. However, these models often give inaccurate responses when faced with questions requiring domain-specific or professional-specific knowledge not covered in their training corpus. To alleviate this issue, Knowledge Graphs (KGs) have been integrated into LLMs as an additional source of knowledge. However, many state-of-the-art LLMs are not open-source, making it challenging to inject knowledge with model APIs only. In this paper, we propose a novel framework $\\texttt{KnowGPT}$, which necessitates the $\\textit{knowledge injection}$ for both knowledge retrieval and translation for LLMs. $\\texttt{KnowGPT}$ leverages $(i)$ deep reinforcement learning to carefully extract context-aware knowledge from KGs, and $(ii)$ a multi-armed bandit to construct an appropriate prompt format for each question. It significantly outperforms the existing methods on three benchmark datasets. Notably, $\\texttt{KnowGPT}$ attains a 91.6\\% accuracy on OpenbookQA official leaderboard, which is comparable to human performance. The code will be open-sourced",
    "checked": false,
    "id": "992cff59f13f5944a74eb2e62a8cb11849204780",
    "semantic_title": "kilm: knowledge injection into encoder-decoder language models",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=OfgRjHB5Gp": {
    "title": "Benchmarking Knowledge Boundary for Large Language Model: A Different Perspective on Model Evaluation",
    "volume": "review",
    "abstract": "In recent years, substantial advancements have been made in the development of large language models, achieving remarkable performance across diverse tasks.To evaluate the knowledge ability of language models, previous studies have proposed lots of benchmarks based on question-answering pairs.We argue that it is not reliable and comprehensive to evaluate language models with a fixed question or limited paraphrases as the query, since language models are sensitive to prompt.Therefore, we introduce a novel concept named knowledge boundary to encompass both prompt-agnostic and prompt-sensitive knowledge within language models.Knowledge boundary avoids prompt sensitivity in language model evaluations, rendering them more dependable and robust.To explore the knowledge boundary for a given model, we propose projected gradient descent method with semantic constraints, a new algorithm designed to identify the optimal prompt for each piece of knowledge.Experiments demonstrate a superior performance of our algorithm in computing the knowledge boundary compared to existing methods.Furthermore, we evaluate the ability of multiple language models in several domains with knowledge boundary",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fn6Go2a6u3": {
    "title": "Improving LLM Pretraining by Filtering Out Advertisements",
    "volume": "review",
    "abstract": "Large language model (LLM) performance is increasingly linked to not just the size but also the quality of internet-derived datasets. While LLM data selection methods have evolved, their evaluations often rely on overall metrics that may not capture their impacts on different downstream task performances. Motivated by this gap, our study finds that selecting pretraining data based on loss metrics could result in poor performance on knowledge-intensive benchmarks, such as the MMLU. Addressing this, we focus on filtering out low-information content, specifically ads, and create an effective ad classifier for this purpose. Besides, the most straightforward approach to assess the quality of pretraining datasets is to train a full-scale LLM, but this is prohibitively expensive and impractical for large-scale comparative studies. To overcome this, we use a smaller, 100M parameter LLM as a proxy to predict the downstream performance of larger models. We effectively demonstrate the correlation between the small model's proxy indicators and the large SFT model's downstream task metrics. This smaller model evaluation technique not only greatly shortens the cycle time for refining data selection strategies but also achieves significant budget savings, amounting to 92.7\\%. Finally, our findings suggest eliminating advertisement content not only improves performance on knowledge-intensive benchmarks but also yields commendable results across various other capability dimensions within benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lLOP_0TocX": {
    "title": "How effective are Quantum Entropy Metrics for Detecting Humor?",
    "volume": "review",
    "abstract": "Having ways to measure humor in a text can help provide systems with real-time feedback to enhance their outputs. Thus, we evaluated two Quantum Entropy-based scores as humor quality metrics by comparing their behavior across various corpora in different languages. Results showed significant differences between humorous and non-humorous instances in certain corpora, but an analysis of effect sizes implied minimal practical significance. The metrics' behavior also varies depending on the dataset and language being evaluated. Experiments using a shared multilingual vector space led to more consistent scoring, but also reduced the metric's ability to differentiate humor and non-humor. Results suggest that despite the potential, the effectiveness of these metrics was inconsistent, indicating a need for further research on humor quality measurements",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ikx1G3D-Ap": {
    "title": "Robust Temporal Sentence Video Grounding with Global Proposal Ranking",
    "volume": "review",
    "abstract": "Most existing solutions to temporal sentence video grounding (TSGV) rely heavily on local classifiers to discern start and end boundaries, often compromise internal consistency and overlook boundary uncertainty. This paper introduces a novel global ranking approach that directly scores all candidate proposals using a unique loss function, thereby enhancing robustness through the integrated decoding of local and global predictions. We further incorporate pretrained language models into our framework - a largely underexplored facet in TSGV. Our methodology is evaluated across three distinct settings: distribution-consistent, distribution-changing, and composition generalization datasets, outperforming existing baselines across the board. Notably, it exhibits superior performance in out-of-distribution and composition generalization tasks. To the best of our knowledge, we are the first to combine global proposal ranking and pretrained language models for robust TSVG",
    "checked": false,
    "id": "04f791d5d4676d096fafe9ea952aa7f2ab1c5d15",
    "semantic_title": "bam-detr: boundary-aligned moment detection transformer for temporal sentence grounding in videos",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Z0klyaQpGe": {
    "title": "AdaMergeX: Cross-Lingual Transfer with Large Language Models via Adaptive Adapter Merging",
    "volume": "review",
    "abstract": "As an effective alternative to the direct fine-tuning on target tasks in specific languages, cross-lingual transfer addresses the challenges of limited training data by decoupling \"task ability\" and \"language ability\" by fine-tuning on the target task in the source language and another selected task in the target language, respectively. However, they fail to fully separate the task ability from the source language or the language ability from the chosen task. In this paper, we acknowledge the mutual reliance between task ability and language ability and direct our attention toward the gap between the target language and the source language on tasks. As the gap removes the impact of tasks, we assume that it remains consistent across tasks. Based on this assumption, we propose a new cross-lingual transfer method called $\\texttt{AdaMergeX}$ that utilizes adaptive adapter merging. By introducing a reference task, we can determine that the divergence of adapters fine-tuned on the reference task in both languages follows the same distribution as the divergence of adapters fine-tuned on the target task in both languages. Hence, we can obtain target adapters by combining the other three adapters. Furthermore, we propose a structure-adaptive adapter merging method. Our empirical results demonstrate that our approach yields new and effective cross-lingual transfer, outperforming existing methods across all settings",
    "checked": true,
    "id": "bba9778411de8940585fc730a2836d45e2292875",
    "semantic_title": "adamergex: cross-lingual transfer with large language models via adaptive adapter merging",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=XNzfEFbEJB3": {
    "title": "ORPO: Monolithic Odds Ratio Preference Optimization without Reference Model",
    "volume": "review",
    "abstract": "While recently proposed preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence in preference alignment. In this paper, we elaborate on the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT. Building on this foundation, we introduce a straightforward and innovative reference-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the necessity for an additional preference alignment phase. Empirically and theoretically, we demonstrate that the odds ratio serves as a sensible choice for contrasting favored and unfavored styles during SFT. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on UltraFeedback alone surpasses the performance of state-of-the-art language models with more than 7B and 13B parameters, achieving 66.2%, 81.3%, and 87.94% in AlpacaEval",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TVU7EKQNjs9": {
    "title": "MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular Comprehension",
    "volume": "review",
    "abstract": "Large language models are playing an increasingly significant role in molecular research, yet existing models often generate erroneous information, posing challenges to accurate molecular comprehension. Traditional evaluation metrics for generated content fail to assess a model's accuracy in molecular understanding. To rectify the absence of factual evaluation, we present MoleculeQA, a novel question answering (QA) dataset which possesses 62K QA pairs over 23K molecules. Each QA pair, composed of a manual question, a positive option and three negative options, has consistent semantics with a molecular description from authoritative molecular corpus. MoleculeQA is not only the first benchmark for molecular factual bias evaluation but also the largest QA dataset for molecular research. A comprehensive evaluation on MoleculeQA for existing molecular LLMs exposes their deficiencies in specific areas and pinpoints several particularly crucial factors for molecular understanding",
    "checked": true,
    "id": "ba18ac45bbcd8605e8737447b618b57821b2dfc6",
    "semantic_title": "moleculeqa: a dataset to evaluate factual accuracy in molecular comprehension",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6g027OoEkE": {
    "title": "Feature-based Low-Rank Compression of Large Language Models via Bayesian Optimization",
    "volume": "review",
    "abstract": "In recent years, large language models (LLMs) have driven advances in natural language processing. Still, their growing scale has increased the computational burden, necessitating a balance between efficiency and performance. Low-rank compression, a promising technique, reduces non-essential parameters by decomposing weight matrices into products of two low-rank matrices. Yet, its application in LLMs has not been extensively studied. The key to low-rank compression lies in low-rank factorization and low-rank dimensions allocation.To address the challenges of low-rank compression in LLMs, we conduct empirical research on the low-rank characteristics of large models. We propose a low-rank compression method suitable for LLMs. This approach involves precise estimation of feature distributions through pooled covariance matrices and a Bayesian optimization strategy for allocating low-rank dimensions. Experiments on the LLaMA-2 models demonstrate that our method outperforms existing strong structured pruning and low-rank compression techniques in maintaining model performance at the same compression ratio.\\footnote{All the implementation details and model checkpoints will be publicly available at \\url{https://anonymous.com}.}",
    "checked": true,
    "id": "af3f455570b673be8ba8a51075f7f5955646a1e7",
    "semantic_title": "feature-based low-rank compression of large language models via bayesian optimization",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=eta9LFgEY1w": {
    "title": "Enhancing LLM Fine-Tuning via Selective Parameter Merging",
    "volume": "review",
    "abstract": "Supervised fine-tuning (SFT) is a crucial technique for tailoring the generalization capacity of Large Language Models (LLMs) to specific target tasks. This study investigates enhancing LLMs fine-tuning through parameter merging technique. By merging models fine-tuned with varied data order, we achieve an enhanced SFT model demonstrating improved performance and lower validation losses. To our best knowledge, this is the first introduction of ``parameter-selection merging\" technique, which innovatively merges models by selecting parameters from one sub-model in each parameter dimension, surpassing traditional weighted-average method across 5 datasets. Furthermore, this method has also shown superiority in multi-task merging scenarios, indicating a promising avenue for future LLM optimizations",
    "checked": false,
    "id": "6551817cbcbd182a58d19f6d2f62ddb93a4d7865",
    "semantic_title": "democratizing large language models via personalized parameter-efficient fine-tuning",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=sRU_vMTtROA": {
    "title": "SocialBench: Sociality Evaluation of Role-Playing Conversational Agents",
    "volume": "review",
    "abstract": "Large language models (LLMs) have advanced the development of various AI conversational agents, including role-playing conversational agents that mimic diverse characters and human behaviors. While prior research has predominantly focused on enhancing the conversational capability, role-specific knowledge, and stylistic attributes of these agents, there has been a noticeable gap in assessing their social intelligence. In this paper, we introduce SocialBench, the first benchmark designed to systematically evaluate the sociality of role-playing conversational agents at both individual and group levels. The benchmark is constructed from a variety of sources and covers a wide range of 512 characters and 6,420 question prompts involved in 1,480 diverse conversation scenarios and 30,871 multi-turn role-playing utterances. We conduct comprehensive evaluations on this benchmark using mainstream open-source and closed-source LLMs, confirming its significance as a testbed for assessing the sociality of role-playing conversational agents",
    "checked": false,
    "id": "67341bd92f34eea2468c107817e69d7d1a6e7007",
    "semantic_title": "roleinteract: evaluating the social interaction of role-playing agents",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=qlu5QBUafY": {
    "title": "Just CHOP: Embarrassingly Simple LLM Compression",
    "volume": "review",
    "abstract": "Large language models (LLMs) enable unparalleled few- and zero-shot reasoning capabilities but at a high computational footprint. A growing assortment of methods for compression promises to reduce the computational burden of LLMs in deployment, but so far, only quantization approaches have been demonstrated to be effective for LLM compression while maintaining zero-shot performance. A critical step in the compression process, the pretrain-then-finetune paradigm, has largely been overlooked when adapting existing pruning strategies to LLMs or proposing new ones. In this work, we show that embarrassingly simple layer pruning coupled with an extended language model pretraining as the finetuning phase produces state-of-the-art results against structured and even semi-structured compression of models at a 7B scale while being more inference efficient. We call this method LayerChop, where we deterministically remove layers from a model followed by task-agnostic finetuning of the remaining weights by continued self-supervised pretraining. At this scale, we also show how distillation, which has been super effective in task-agnostic compression of smaller BERT-style models, becomes inefficient against our simple pruning technique. We release our code and evaluation setup to facilitate reproducibility and advancement of task-agnostic compression for LLMs",
    "checked": true,
    "id": "6f838a11544c7b02d051814269a20cd459a2ac40",
    "semantic_title": "just chop: embarrassingly simple llm compression",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=VeHcV4FCIn6": {
    "title": "Poor-Supervised Evaluation for SuperLLM via Mutual Consistency",
    "volume": "review",
    "abstract": "The guidance from capability evaluations has greatly propelled the progress of human society and the development of Artificial Intelligence. However, as LLMs evolve, it becomes challenging to construct evaluation benchmark with accurate labels for SuperLLMs whose capabilities approach or even surpass those of humans. To credibly conduct poor-supervised evaluation without accurate labels, we first prove that the consistency between the model under evaluation and the reference model, when their prediction distributions are independent and the sample size is infinite, can equivalently assess the true capabilities of the model to be evaluated. However, using either humans or LLMs as the reference model cannot sufficiently meet the conditions, for which we propose the PEEM algorithm. By treating all models under evaluation as reference models, PEEM alternately optimizes model weights and filters reference models based on EM algorithm to maximally alleviate the insufficiency of the conditions. Comprehensive experiments across 3 types of tasks with 16 mainstream LLMs validate the efficiency, universality, and effectiveness of PEEM. More generally, PEEM has advanced the evaluation paradigm evolution from human-centric to human\\&model-centric, alleviating the limitations of human capabilities for evaluating SuperLLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i44aEzCXHN": {
    "title": "Revolutionizing Multi-Hop QA: A Finite State Prompt Paradigm for Enhanced Language Model Reasoning",
    "volume": "review",
    "abstract": "Multi-hop Question Answering (MHQA) has long captivated the attention of researchers due to its ability to test a model's proficiency in handling complex questions. While Large Language Models (LLMs) have demonstrated impressive language comprehension abilities, their application to MHQA tasks is hindered by several challenges, including hallucination, error propagation and attention dispersion. We propose a novel prompt method Finite state automaton (FSM) to enhance the reasoning capabilities of LLM for complex task in addition to improved effectiveness and trustworthiness. Unlike traditional end-to-end chain-of-thought (COT) methods, FSM addresses MHQA by iteratively decompose question into multi-turn subquestions, enabling error checks and backtracking when necessary, ensuring the standardization of the process. Extensive experiments on benchmarks show that our proposed FSM not only has a neat format and stable performance, but also nearly doubles the zero-shot performance in hardest dataset Musique. Further analysis reveals the remarkable capability of FSM to elicit direct and concise intermediate reasoning steps. We believe that our framework can fully demonstrate its advantages when applied to challenging tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JW5ZEvVWDTx": {
    "title": "A study of extracting sentiments for non-english languages with LLMs",
    "volume": "review",
    "abstract": "Social media platforms like X (formerly Twitter), Facebook, Reddit are top platform to share feedback and raise concerns about product and services of particular company. Understanding of social data in various languages, but not limited to English is important to meet the ever-changing need of customers. Recently, large language models (LLMs) have shown significant performance for various NLP tasks. However, There are very limited labelled dataset and pre-trained models for non-english languages like Spanish, French and German. In this paper, we did extensive study of exploring various open pre-trained transformer based machine translation and sentiment models and evaluating and fine-tuning these models on open datasets. Our approach incorporates two strategies, Firstly we leverage sentiment models trained in non-english languages and improve it further by fine-tuning on domain data. Second, We fine-tuned machine translation models to translate text from non-english languages to english and use state of the art sentiment models in english language. Machine translation model after fine tuning achieves BLEU score between 50 and 60 that is considered as very high quality translation. We find that extracting sentiments by translating to english and then using english model with domain understanding yields better performance, which achieves 61.43 f1-score vs 55.26 with first approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uombpNCDIW": {
    "title": "Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize Encoded Knowledge",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have recently showcased remarkable generalizability in various domains. However, they still encounter challenges in tasks that involve reasoning, especially those requiring multiple-step reasoning. Despite their extensive knowledge, LLMs still face challenges in efficiently utilizing encoded knowledge to develop accurate and logical reasoning processes. Additionally, there is currently limited research in this area. To mitigate this problem, we introduce Hint-before-Solving Prompting (HSP), a method that guides Large Language Models (LLMs) to first generate hints aiding in problem-solving and then produce solutions that include intermediate reasoning steps. The hint can be specific knowledge, pivotal concepts, or analytical insights essential for solving the problem. Since HSP is orthogonal to prompting methods (e.g., Chain-of-Thought (CoT)), we applied HSP to CoT, Least-to-Most, Plan-and-Solve, and Standard promptings. We conducted extensive and comprehensive experiments on six reasoning benchmarks and four open-source models (Llama2 family models and Mixtral). The results show that HSP significantly improves the accuracy. The average across six datasets, Llama2-70B-Chat, improved by 1.5 under HSP-enhanced CoT, and the improvement jumped to 9.7 with high-quality hints. Beyond exploring training-free LLM capabilities, we built the HSPMATH dataset based on HSP and fine-tuned Llemma-7B, reaching 61.7 accuracy, surpassing GPT-3.5. We make our code and dataset publicly available at: https://github.com/sfhff216/hintllm",
    "checked": true,
    "id": "bb991838e419a3cdd8617bb6384a95f0360113a4",
    "semantic_title": "hint-before-solving prompting: guiding llms to effectively utilize encoded knowledge",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8fXaTPYsoE": {
    "title": "Blind Men and the Elephant: Diverse Perspectives of Gender Bias in Stereotype Benchmarks",
    "volume": "review",
    "abstract": "The multifaceted challenge of accurately measuring gender bias in language models is akin to discerning different segments of a broader, unseen entity. This short paper mainly focuses on intrinsic bias mitigation and measurement strategies for language models, building on prior research that demonstrate a lack of correlation between intrinsic and extrinsic approaches. We delve deeper into the realm of intrinsic measurements, identifying inconsistencies and positing that these metrics might reflect diverse facets of gender bias. Our methodology encompasses an analysis of data distribution across benchmarks coupled with the implementation of an intricate gender bias categorization derived from social psychology. Adjustments made to the distributions of the two datasets lead to significant enhancement in the alignment of their outcomes. Our findings not only underscore the complexity inherent in gender bias in language models but also forge new paths toward more refined techniques for bias detection and reduction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6905dRXJjJE": {
    "title": "Large Language Model Can Interpret Latent Space of Sequential Recommender",
    "volume": "review",
    "abstract": "Sequential recommendation aims to predict the next item of interest for a user, based on her/his interaction history. In conventional sequential recommenders, a common approach is to model item sequences using discrete IDs, learning behavioral pattern representations. Inspired by recent success in empowering large language models (LLMs) to understand diverse modality data (\\eg image, audio, 3D points), a compelling research question arises: ``Can LLMs understand and utilize behavioral pattern representations from ID-based sequential recommenders?''. To answer this, we propose a simple framework, RecInterpreter, which examines the capacity of open-source LLMs to decipher the representation space of sequential recommenders. Specifically, with the multimodal pairs (\\ie interaction sequence representations and text narrations), RecInterpreter first uses a lightweight projector to map the representations into the token embedding space of the LLM, which encourages the LLM to generate textual narrations for items within the interaction sequence. Moreover, upon successfully interpreting sequential recommenders, the LLM can enhance its recommendation performance by tuning with the projected behavioral patterns. Empirical results showcase that our RecInterpreter enhances LLM to understand hidden representations from ID-based sequential recommenders and boost recommendation performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mwARP6kgIXj": {
    "title": "LISA: Layerwise Importance Sampled AdamW",
    "volume": "review",
    "abstract": "The machine learning community has witnessed impressive advancements since the first appearance of large language models (LLMs), yet their huge memory consumption has become a major roadblock to large-scale training. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most large-scale fine-tuning settings. Attempting to complement this deficiency, we investigate layerwise properties of LoRA on fine-tuning tasks and notice an uncommon skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which can outperform both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it \\textbf{L}ayerwise \\textbf{I}mportance \\textbf{S}ampled \\textbf{A}dam (\\textbf{LISA}), a promising alternative for LoRA, which applies the idea of importance sampling to different layers in LLMs and randomly freeze most middle layers during optimization. Experimental results show that with similar or less GPU memory consumption, LISA surpasses LoRA in downstream fine-tuning tasks by a large margin on small models ($\\le 7B$), where LISA consistently outperforms LoRA by over $11\\%$-$37\\%$ in terms of MT-Bench scores. On large models, specifically Llama-2-70b, LISA achieves on-par or better performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating its effectiveness across different domains",
    "checked": false,
    "id": "c739eb7f0302e85e935d1e2fdb903fe01b812804",
    "semantic_title": "lisa: layerwise importance sampling for memory-efficient large language model fine-tuning",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=iLyWqxX1cG": {
    "title": "The Overlooked Lengthening Words for Sentiment Analysis",
    "volume": "review",
    "abstract": "Recent advancements in Large Language Models (LLMs) have catalyzed pivotal discussions about their proficiency in sentiment analysis and informal styles. This work focuses on lengthening words, a distinctive but often overlooked nuanced expression. Although around 5.8\\% of documents in 4 public datasets contain lengthening words, there is no ready-to-use dataset for this nuanced expression. To evaluate the performance and interpretation of SOTA LLMs for stretched words, we introduce \\textbf{Lengthening}, the first weakly labeled sentence-level dataset consisting of 850k samples with lengthed words for sentiment analysis. Comprehensive experiment results involving 5 LLMs and 5 domains reveal the sentiment-expressive value of lengthening words from both sentence and word levels. This study highlights the importance of lengthening words for online social media, where short text and nuanced communication prevail.Our results show that pre-trained LLMs can reach better performances than zero-shot ChatGPT4 after fine-tuning on the Lengthening dataset but might be insufficient in explainability. With an explainable instruction dataset consisting of only 1000 samples, instruct-tuned LLaMA2 (13B-chat-hf) can reach the same level of performance and explainability as ChatGPT4",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rFcuIGkUwek": {
    "title": "Language models' probability distributions are calibrated to cognitive profiles: an investigation of the predictive power of surprisal and entropy",
    "volume": "review",
    "abstract": "To date, most investigations on predictability (surprisal and entropy) effects have been conducted on the group-level, disregarding individual differences. In this work, we revisit the predictive power of different LMs' predictability effects on data of human processing effort by considering information of language users' cognitive capacities. To do so, we assess the predictive power of predictability effects estimated from generative LMs on reading data from subjects for which we also scores from psychometric tests targeting different cognitive domains.We investigate if allowing the model to modulate predictability effects relative to the cognitive scores increases the prediction of reading times, and we examine whether LMs exhibit systematic biases in the prediction of reading times for cognitively high- or low-performing groups, allowing us to reveal what type of psycholinguistic subjects a given LM emulates.Our results confirm that incorporating cognitive capacities and allowing them to modulate surprisal and entropy effects can increase the predictive power of these predictors. Moreover, we find that predictive power of surprisal extracted from GPT-2 degrades for individuals with higher verbal intelligence but also improves for individuals with higher verbal cognitive control, suggesting that GPT-2 emulates a subject with high verbal cognitive control but low verbal intelligence",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mpfV0zyp4E": {
    "title": "Preemptive Answer \"Attacks\" on Chain-of-Thought Reasoning",
    "volume": "review",
    "abstract": "Large language models (LLMs) showcase impressive reasoning capabilities when coupled with Chain-of-Thought (CoT) prompting. However, the robustness of this approach warrants further investigation. In this paper, we introduce a novel scenario termed preemptive answers, where the LLM obtains an answer before engaging in reasoning. This situation can arise inadvertently or induced by malicious users by prompt injection attacks. Experiments reveal that preemptive answers significantly impair the model's reasoning capability across various CoT methods and a broad spectrum of datasets. To bolster the robustness of reasoning, we propose two measures aimed at mitigating this issue to some extent",
    "checked": true,
    "id": "d8ba6085c7c32cc367ab7f50d18744de846fcebd",
    "semantic_title": "preemptive answer \"attacks\" on chain-of-thought reasoning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=DFCp6ufum9": {
    "title": "Reformulating Domain Adaptation of Large Language Models as Adapt-Retrieve-Revise: A Case Study on Chinese Legal Domain",
    "volume": "review",
    "abstract": "While large language models (LLMs) like GPT-4 have recently demonstrated astonishing zero-shot capabilities in general domain tasks, they often generate content with hallucinations in specific domains such as Chinese law, hindering their application in these areas. This is typically due to the absence of training data that encompasses such a specific domain, preventing GPT-4 from acquiring in-domain knowledge. A pressing challenge is that it's not plausible to continue training LLMs of the GPT-4's scale on in-domain data.This paper introduces a simple yet effective domain adaptation framework for GPT-4 by reformulating generation as an adapt-retrieve-revise process. The initial step is to adapt an affordable 7B LLM to the Chinese legal domain by continuing learning in-domain data. When solving an in-domain task, we leverage the adapted LLM to generate a draft answer given a task query. Then, the draft answer will be used to retrieve supporting evidence candidates from an external in-domain knowledge base. Finally, the draft answer and retrieved evidence are concatenated into a whole prompt to let GPT-4 assess the evidence and revise the draft answer to generate the final answer. Our proposal combines the advantages of the efficiency of adapting a smaller 7B model with the evidence-assessing capability of GPT-4 and effectively prevents GPT-4 from generating hallucinatory content. In the zero-shot setting of four Chinese legal tasks, our method improves the average score by $+33.6$ points, compared to GPT-4 direct generation. When compared to two stronger retrieval-based baselines, our method outperforms them by $+17.0$ and $+23.5$. Our code will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MEqT9lExCF": {
    "title": "Language-emphasized Cross-lingual In-context Learning for Multilingual LLM",
    "volume": "review",
    "abstract": "Cross-lingual learning, which can transfer knowledge from high-resource languages to low-resource languages, has been widely studied. With the recent rise of large language models (LLMs), in-context learning (ICL) has shown remarkable performance, eliminating the need for fine-tuning parameters and reducing the reliance on extensive labeled data. It sounds tempting to use cross-lingual ICL to solve cross-lingual tasks based on multilingual LLMs. However, the intricacies of cross-lingual ICL remain underexplored. Prior studies on cross-lingual ICL overlooked the significance of language-specific nuances, neglecting not only the intrinsic linguistic properties of sentences but also the interlingual connections between sentences in different languages. In this paper, we propose a novel cross-lingual prompt structure: Language-Emphasized cross-lingual In-context learning LEI. LEI implements language alignment of demonstrations while introducing a third language (example language) as an example of language conversion to adapt LLMs to language conversion in cross-lingual tasks. Extensive experiments validate the state-of-the-art performance of LEI on 42 cross-lingual tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ew-OTh62SE": {
    "title": "Perspective Taking through Generating Responses to Conflict Situations",
    "volume": "review",
    "abstract": "Although language model performance across diverse tasks continues to improve, these models still struggle to understand and explain the beliefs of other people. This skill requires perspective-taking, the process of conceptualizing the point of view of another person. Perspective taking becomes challenging when the text reflects more personal and potentially more controversial beliefs.We explore this task through natural language generation of responses to conflict situations. We evaluate novel modifications to recent architectures for conditioning generation on an individual's comments and self-disclosure statements. Our work extends the Social-Chem-101 corpus, using 95k judgements written by 6k authors from English Reddit data, for each of whom we obtained 20-500 self-disclosure statements. Our evaluation methodology borrows ideas from both personalized generation and theory of mind literature. Our proposed perspective-taking models outperform recent work, especially the twin encoder model conditioned on self-disclosures with high similarity to the conflict situation",
    "checked": false,
    "id": "c4b10119a3dde651ba7ead850a36e4ade462d156",
    "semantic_title": "empowering peace: the role of civil society in peacebuilding and conflict transformation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nxucEuff46X": {
    "title": "Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow",
    "volume": "review",
    "abstract": "Various industries such as finance, meteorology, and energy produce vast amounts of heterogeneous data every day. There is a natural demand for humans to manage, process, and display data efficiently. However, it necessitates labor-intensive efforts and a high level of expertise for these data-related tasks. Considering large language models (LLMs) showcase promising capabilities in semantic understanding and reasoning, we advocate that the deployment of LLMs could autonomously manage and process massive amounts of data while interacting and displaying in a human-friendly manner. Based on this, we propose Data-Copilot, an LLM-based system that connects numerous data sources on one end and caters to diverse human demands on the other end. Acting as an experienced expert, Data-Copilot autonomously transforms raw data into multi-form output that best matches the user's intent. Specifically, it first designs multiple universal interfaces to satisfy diverse data-related requests, like querying, analysis, prediction, and visualization. In real-time response, it automatically deploys a concise workflow by invoking corresponding interfaces. The whole process is fully controlled by Data-Copilot, without human assistance. We release Data-Copilot-1.0 using massive Chinese financial data, e.g., stocks, funds and news. Experiments indicate it achieves reliable performance with lower token consumption, showing promising application prospects",
    "checked": true,
    "id": "473eb062612a17c965eaa62136322f0dec6b1f8e",
    "semantic_title": "data-copilot: bridging billions of data and humans with autonomous workflow",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=uPxzMmDq8X": {
    "title": "Set the Clock: Temporal Alignment of Pretrained Language Models",
    "volume": "review",
    "abstract": "Language models (LMs) are trained on web text originating from many points in time and, in general, without any explicit temporal grounding. This work investigates the temporal chaos of pretrained LMs and explores various methods to align their internal knowledge to a target time, which we call \"temporal alignment.\" To do this, we first automatically construct a dataset containing 20K time-sensitive questions and their answers for each year from 2000 to 2023. Based on this dataset, we empirically show that pretrained LMs (e.g., LLaMa2), despite having a recent pretraining cutoff (e.g., 2022), mostly answer questions using earlier knowledge (e.g., in 2019). We then develop several methods, from prompting to finetuning, to align LMs to use their most recent knowledge when answering questions, and investigate various factors in this alignment. Our experiments show that aligning LLaMa2 to the year 2022 can boost its performance by up to 62% relatively as measured by that year, even without mentioning time information explicitly, indicating the possibility of aligning models' internal sense of time after pretraining. Finally, we find that alignment to a historical time is also possible, with up to $2.8\\times$ the performance of the unaligned LM in 2010 if finetuning models to that year. These findings hint at the sophistication of LMs' internal knowledge organization and the necessity of tuning them properly",
    "checked": true,
    "id": "944b983ee059503c53afef772052d065d662527c",
    "semantic_title": "set the clock: temporal alignment of pretrained language models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=AAe6eQjLOQ": {
    "title": "Duwak: Dual Watermarks in Large Language Models",
    "volume": "review",
    "abstract": "As large language models (LLM) are increasingly used for text generation tasks, it is critical to audit their usages, govern their applications, and mitigate their potential harms.Existing watermark techniques are shown effective in embedding \\textit{single} human-imperceptible and machine-detectable patterns without significantly affecting generated text quality and semantics.However, the efficiency in detecting watermarks, i.e., the minimum number of tokens required to assert detection with significance and robustness against post-editing, is still debatable. In this paper, we propose, Duwak, to fundamentally enhance the efficiency and quality of watermarking by embedding dual secret patterns in both token probability distribution and sampling schemes.To mitigate expression degradation caused by biasing toward certain tokens, we design a contrastive search to watermark the sampling scheme, which minimizes the token repetition and enhances the diversity.We theoretically explain the interdependency of the two watermarks within Duwak. We evaluate Duwak extensively on Llama2 under various post-editing attacks, against four state-of-the-art watermarking techniques and combinations of them.Our results show that Duwak marked text achieves the highest watermarked text quality at the lowest required token count for detection, up to 70% tokens less than existing approaches, especially under post paraphrasing",
    "checked": true,
    "id": "833299cbaa0793527951b2b78e1438daf6fe8cb6",
    "semantic_title": "duwak: dual watermarks in large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=mWbgVDaGZK": {
    "title": "Artwork Explanation in Large-scale Vision Language Models",
    "volume": "review",
    "abstract": "Large-scale vision-language models (LVLMs) output text from images and instructions, demonstrating advanced capabilities in text generation and comprehension. However, it has not been clarified to what extent LVLMs understand the knowledge necessary for explaining images, the complex relationships between various pieces of knowledge, and how they integrate these understandings into their explanations. To address this issue, we propose a new task: the artwork explanation generation task, along with its evaluation dataset and metric for quantitatively assessing the understanding and utilization of knowledge about artworks. This task is apt for image description based on the premise that LVLMs are expected to have pre-existing knowledge of artworks, which are often subjects of wide recognition and documented information. It consists of two parts: generating explanations from both images and titles of artworks, and generating explanations using only images, thus evaluating the LVLMs' language-based and vision-based knowledge. Alongside, we release a training dataset for LVLMs to learn explanations that incorporate knowledge about artworks. Our findings indicate that LVLMs not only struggle with integrating language and visual information but also exhibit a more pronounced limitation in acquiring knowledge from images alone. The datasets are available at https://(anonymized)",
    "checked": true,
    "id": "3fc696de90d8f582fa90faf597abf39434e277b4",
    "semantic_title": "artwork explanation in large-scale vision language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jG2a6JsdsA": {
    "title": "Large Language Models for Graph Structure Learning",
    "volume": "review",
    "abstract": "Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies and interactions among nodes in graph-structured data by generating novel graph structures. Graph Neural Networks (GNNs) have emerged as promising GSL solutions, utilizing recursive message passing to encode node-wise inter-dependencies. However, many existing GSL methods heavily depend on explicit graph structural information as supervision signals, leaving them susceptible to challenges such as data noise and sparsity. In this work, we propose GraphEdit, an approach that leverages large language models (LLMs) to learn complex node relationships in graph-structured data. By enhancing the reasoning capabilities of LLMs through instruction-tuning over graph structures, we aim to overcome the limitations associated with explicit graph structural information and enhance the reliability of graph structure learning. Our approach not only effectively denoises noisy connections but also identifies node-wise dependencies from a global perspective, providing a comprehensive understanding of the graph structure. We conduct extensive experiments on multiple benchmark datasets to demonstrate the effectiveness and robustness of GraphEdit across various settings",
    "checked": false,
    "id": "c7a56a3a500a08f28b9816d66d3ac2e9e5ac7445",
    "semantic_title": "graphedit: large language models for graph structure learning",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=ppeWLGJPM1": {
    "title": "Task Decoupled Low-rank Adaption for Parameter-Efficient Fine-Tuning",
    "volume": "review",
    "abstract": "Parameter Efficient Fine-Tuning (PEFT) provides a powerful approach for adapting large pre-trained language models (PLMs) to specific domains. Among PEFT methods, LoRA-based methods have emerged as a leading approach. However, existing LoRA-based approaches primarily focus on modifying the architecture or the low-rank matrices, overlooking the significant influence of downstream tasks on LoRA effectiveness. In this paper, we first theoretically demonstrate that fine-tuning actually involves two tasks: domain inference fine-tuning and content inference fine-tuning. We also demonstrate that LoRA can be decoupled into two tasks. To overcome the drawbacks of coupled updates in LoRA, we propose Task Decoupled LoRA (TD-LoRA), which divides the LoRA tasks into distinct branches by introducing a new branch. We employ cosine similarity between LoRA updates weights and pre-trained weights to approximate domain inference. Compared with LoRA, TD-LoRA achieves domain-specific fine-tuning. Additionally, TD-LoRA has the same memory requirements and comparable computational costs as LoRA. We conduct extensive experiments on various pre-trained models and demonstrate its effectiveness on the GLUE, E2E and MMLU benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-ABwxHvt7HX": {
    "title": "Evaluating Robustness of Generative Search Engine on Adversarial Factoid Questions",
    "volume": "review",
    "abstract": "Generative search engines have the potential to transform how people seek information online, but generated responses from existing large language models (LLMs)-backed generative search engines may not always be accurate. Nonetheless, retrieval-augmented generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable part of a claim. To this end, we propose evaluating the robustness of generative search engines in the realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning incorrect responses. Through a comprehensive human evaluation of various generative search engines, such as Bing Chat, PerplexityAI, and YouChat across diverse queries, we demonstrate the effectiveness of adversarial factual questions in inducing incorrect responses. Moreover, retrieval-augmented generation exhibits a higher susceptibility to factual errors compared to LLMs without retrieval. These findings highlight the potential security risks of these systems and emphasize the need for rigorous evaluation before deployment. The dataset and code will be publicly available",
    "checked": false,
    "id": "44d0c9a483b0af2f3952ae9acdde3d091472bc69",
    "semantic_title": "evaluating robustness of generative search engine on adversarial factual questions",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=qI9x6MJ4In": {
    "title": "DM-BLI: Dynamic Multiple Subspaces Alignment for Unsupervised Bilingual Lexicon Induction",
    "volume": "review",
    "abstract": "Unsupervised bilingual lexicon induction (BLI) task aims to find word translations between languages and has achieved great success in similar language pairs. However, related works mostly rely on a single linear mapping for language alignment and fail on distant or low-resource language pairs, achieving less than half the performance observed in rich-resource language pairs. In this paper, we introduce DM-BLI, a Dynamic Multiple subspaces alignment framework for unsupervised BLI. DM-BLI improves language alignment by utilizing multiple subspace alignments instead of a single mapping. We begin via unsupervised clustering to discover these subspaces in source embedding space. Then we identify and align corresponding subspaces in the target space using a rough global alignment. DM-BLI further employs intra-cluster and inter-cluster contrastive learning to refine precise alignment for each subspace pair. Experiments conducted on standard BLI datasets for 12 language pairs (6 rich-resource and 6 low-resource) demonstrate substantial gains achieved by our framework. We release our code to facilitate the community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IUpMMjrYxvg": {
    "title": "Exploring Multi-Document Information Consolidation for Scientific Sentiment Summarization",
    "volume": "review",
    "abstract": "Modern natural language generation systems with LLMs exhibit the capability to generate a plausible summary of multiple documents; however, it is uncertain if models truly possess the ability of information consolidation to generate summaries, especially on those source documents with opinionated information. To make scientific sentiment summarization more grounded, we hypothesize that in peer review human meta-reviewers follow a three-layer framework of sentiment consolidation to write meta-reviews and it represents the logic of summarizing scientific sentiments in meta-review generation. The framework is validated via human annotation. Based on the framework, we propose evaluation metrics to assess the quality of generated meta-reviews, and we find that the hypothesis of the sentiment consolidation framework works out empirically when we incorporate it as prompts for LLMs to generate meta-reviews in extensive experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gwumTSolEjn": {
    "title": "Context Consistency between Training and Inference in Simultaneous Machine Translation",
    "volume": "review",
    "abstract": "Simultaneous Machine Translation (SiMT) aims to yield a real-time partial translation with a monotonically growing source-side context.However, there is a counterintuitive phenomenon about the context usage between training and inference: {\\em e.g.}, in wait-$k$ inference, model consistently trained with wait-$k$ is much worse than that model inconsistently trained with wait-$k'$ ($k'\\neq k$) in terms of translation quality. To this end, we first investigate the underlying reasons behind this phenomenon and uncover the following two factors: 1) the limited correlation between translation quality and training loss; 2) exposure bias between training and inference. Based on both reasons, we then propose an effective training approach called context consistency training accordingly, which encourages consistent context usage between training and inference by optimizing translation quality and latency as bi-objectives and exposing the predictions to the model during the training. The experiments on three language pairs demonstrate that our SiMT system encouraging context consistency outperforms existing SiMT systems with context inconsistency for the first time",
    "checked": false,
    "id": "4dc176f99baee4bfa66caf311663bb80c6aa9a01",
    "semantic_title": "context consistency between training and testing in simultaneous machine translation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I-ScG_LwXm": {
    "title": "CycleAlign: Iterative Distillation from Black-box LLM to White-box Models for Better Human Alignment",
    "volume": "review",
    "abstract": "Language models trained on large-scale corpus often generate harmful responses that are harmful and contrary to human values. A prevalent approach for human alignment is reinforcement learning from human feedback (RLHF), utilizing algorithms such as proximal policy optimization (PPO). However, these methods are often characterized by complexity, instability, and substantial resource consumption. Considering that existing large language models (LLMs) like ChatGPT are already relatively well-aligned and cost-friendly, researchers propose to align the language model with human preference from AI feedback. Nevertheless, the common practices, that unidirectionally distill the responses, are constrained by the inherent capability of LLMs. To address it, we introduce CycleAlign framework to distill alignment capabilities from the parameter-invisible LLM (black-box) to a parameter-visible model (white-box) in an iterative manner. CycleAlign iteratively improves both the while-box and black-box models by integrating static and dynamic in-context learning and a belief alignment method. Empirical results illustrate that the model fine-tuned by CycleAlign remarkably exceeds existing methods, and achieves the state-of-the-art performance in alignment with human value.\\footnote{The code of this work will be available at \\url{https://anonymous.4open.science/r/CycleAlign-B958/}.}",
    "checked": true,
    "id": "9c0102443a1b5adc0c2235fab23a80bf8122ce72",
    "semantic_title": "cyclealign: iterative distillation from black-box llm to white-box models for better human alignment",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=wFhqS2876UH": {
    "title": "Transformer-based Causal Language Models Perform Clustering",
    "volume": "review",
    "abstract": "Even though large language models (LLMs) have demonstrated remarkable capability in solving various natural language tasks, the capability of an LLM to follow human instructions is still a concern. Recent works have shown great improvements on the instruction-following capability via additional training for instruction-following tasks. However, the mechanisms responsible for effective instruction-following capabilities remain inadequately understood. Here, we introduce a simplified instruction-following task and use synthetic datasets to analyze a Transformer-based causal language model. Our findings suggest that the model learns task-specific information by clustering data within its hidden space, with this clustering process evolving dynamically during learning. We also demonstrate how this phenomenon assists the model in handling unseen instances and validate our results in a more realistic setting",
    "checked": true,
    "id": "c6bf5b971ddd0db1293f70f2a88b652199a56612",
    "semantic_title": "transformer-based causal language models perform clustering",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Y_mDCKlG7t": {
    "title": "The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)",
    "volume": "review",
    "abstract": "Retrieval-augmented generation (RAG) is a powerful technique to facilitate language model generation with proprietary and private data, where data privacy is a pivotal concern. Whereas extensive research has demonstrated the privacy risks of large language models (LLMs), the RAG technique could potentially reshape the inherent behaviors of LLM generation, posing new privacy issues that are currently under-explored. To this end, we conduct extensive empirical studies with novel attack methods, which demonstrate the vulnerability of RAG systems on leaking the private retrieval database. Despite the new risks brought by RAG on the retrieval data, we further discover that RAG can be used to mitigate the old risks, i.e., the leakage of the LLMs' training data. In general, we reveal many new insights in this paper for privacy protection of retrieval-augmented LLMs, which could benefit both LLMs and RAG systems builders",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fMtJFhvKXK7": {
    "title": "Evaluating Large Language Models for Health-related Queries with Presuppositions",
    "volume": "review",
    "abstract": "As corporations rush to integrate large language models (LLMs) it is critical that they provide factually accurate information, that is robust to any presuppositions that a user may express. In this work, we introduce UPHILL, a dataset consisting of health-related queries with varying degrees of presuppositions. Using UPHILL, we evaluate the factual accuracy and consistency of InstructGPT, ChatGPT, GPT-4 and Bing Copilot models. We find that while model responses rarely contradict true health claims (posed as questions), all investigated models fail to challenge false claims. Alarmingly, responses from these models agree with 23-32% of the existing false claims, and 49-55% with novel fabricated claims. As we increase the extent of presupposition in input queries, responses from all models except Bing Copilot agree with the claim considerably more often, regardless of its veracity. Given the moderate factual accuracy, and the inability of models to challenge false assumptions, our work calls for a careful assessment of current LLMs for use in high-stakes scenarios",
    "checked": true,
    "id": "6674e254f343cdb2511b84d4bf120182fb112b67",
    "semantic_title": "evaluating large language models for health-related queries with presuppositions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nij7v6ysLV5": {
    "title": "Multi-modal Preference Alignment Remedies Degradation of Visual Instruction Tuning on Language Models",
    "volume": "review",
    "abstract": "In production, multi-modal large language models (MLLMs) are expected to support multi-turn queries of interchanging image and text modalitie. However, the current MLLMs trained with visual-question-answering (VQA) datasets could suffer from degradation, as VQA datasets lack the diversity and complexity of the original text instruction datasets which the underlying language model had been trained with. To address this challenging degradation, we first collect a lightweight (6k entries) VQA preference dataset where answers were annotated by Gemini for 5 quality metrics in a granular fashion, and investigate standard Supervised Fine-tuning, rejection sampling, Direct Preference Optimization (DPO), and SteerLM. Our findings indicate that the with DPO we are able to surpass instruction-following capabilities of the language model, achieving a 6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despite small data scale. This enhancement in textual instruction proficiency correlates with boosted visual instruction performance (+4.9\\% on MM-Vet, +6\\% on LLaVA-Bench), with minimal alignment tax on visual knowledge benchmarks compared to previous RLHF approach. In conclusion, we propose a distillation-based multi-modal alignment model with fine-grained annotations on a small dataset that reconciles the textual and visual performance of MLLMs, restoring and boosting language capability after visual instruction tuning",
    "checked": false,
    "id": "710b1e23b09e0b826f9d47e7cc23b5f4c0808c7e",
    "semantic_title": "multi-modal preference alignment remedies regression of visual instruction tuning on language model",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=9-hFJ0lCW8": {
    "title": "Predicting the Unpredictable: Uncertainty-Aware Reasoning over Temporal Knowledge Graphs via Diffusion Process",
    "volume": "review",
    "abstract": "Temporal Knowledge Graph (TKG) reasoning seeks to predict future incomplete facts leveraging historical data. While existing approaches have shown effectiveness in addressing the task through various perspectives, such as graph learning and logic rules, they are limited in capturing the indeterminacy in future events, particularly in the case of rare/unseen facts. To tackle the highlighted issues, we introduce a novel approach by conceptualizing TKG reasoning as a sequence denoising process for future facts, namely DiffuTKG. Concretely, we first encodes the historical events as the conditional sequence. Then we gradually introduce Gaussian noise to corrupt target facts during the forward process and then employ a transformer-based conditional denoiser to restore them in the reverse phase. Moreover, we introduce an uncertainty regularization loss to mitigate the risk of prediction biases by favoring frequent scenarios over rare/unseen facts. Empirical results on four real-world datasets show that DiffuTKG outperforms state-of-the-art methods across multiple evaluation metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rdhWDSYhdz": {
    "title": "Distilled Multilingual Critics for Indic Text Simplification",
    "volume": "review",
    "abstract": "Self-correction techniques have recently emerged as a promising framework to improve the quality of responses generated by large language models (LLMs). LLMs with few-shot prompting act as critics to produce feedback for an input, which is further fed to a refiner (also an LLM) to produce an output. However, these critique-refine steps require multiple expensive LLM calls. To circumvent this large inference cost, we borrow inspiration from prior work on knowledge distillation and propose the use of critique distillation to train critics. These are smaller sequence-to-sequence models that are trained on input-critique pairs generated by an LLM. We focus on the problem of text simplification for three Indian languages, Hindi, Bengali and Marathi. This task is both a good fit for self-correction style techniques and has also not been systematically explored for Indian languages before. We train two separate critics for simplification that focus on lexical and structure complexity, and show that it is surprisingly more effective than using an LLM directly as a critic in both few-shot (for Hindi) and zero-shot (for Bengali and Marathi) settings. We also show the benefits of training multilingual critics, as opposed to monolingual critics, in enabling improved cross-lingual transfer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1aRf4_Tm5_": {
    "title": "LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores",
    "volume": "review",
    "abstract": "Automatic evaluation of generated textual content presents an ongoing challenge within the field of NLP. Given the impressive capabilities of modern language models (LMs) across diverse NLP tasks, there is a growing trend to employ these models in creating innovative evaluation metrics for automated assessment of generation tasks. This paper investigates a pivotal question: Do language model-driven evaluation metrics inherently exhibit bias favoring texts generated by the same underlying language model? Specifically, we assess whether prominent LM-based evaluation metrics (e.g. BARTScore, T5Score, and GPTScore) demonstrate a favorable bias toward their respective underlying LMs in the context of summarization tasks. Our findings unveil a latent bias, particularly pronounced when such evaluation metrics are used in an reference-free manner without leveraging gold summaries. These results underscore that assessments provided by generative evaluation models can be influenced by factors beyond the inherent text quality, highlighting the necessity of developing more dependable evaluation protocols in the future",
    "checked": true,
    "id": "71f7bbfb36a0026825e17f3303e73f93876fc3e7",
    "semantic_title": "llms as narcissistic evaluators: when ego inflates evaluation scores",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=nH60c6lEZk": {
    "title": "We Demand Justice!\": Towards Social Context Grounding of Political Texts",
    "volume": "review",
    "abstract": "Social media discourse frequently consists of `seemingly similar language used by opposing sides of the political spectrum', often translating to starkly contrasting perspectives. E.g., \\textit{``thoughts and prayers''}, could express sympathy for mass-shooting victims, or criticize the lack of legislative action on the issue. This paper defines the context required to fully understand such ambiguous statements in a computational setting and ground them in real-world entities, actions, and attitudes. We propose two challenging datasets that require an understanding of the real-world context of the text. We benchmark these datasets against models built upon large pre-trained models, such as RoBERTa and GPT-3. Additionally, we develop and benchmark more structured models building upon existing \\textit{Discourse Contextualization Framework} and \\textit{Political Actor Representation} models. We analyze the datasets and the predictions to obtain further insights into the pragmatic language understanding challenges posed by the proposed social grounding tasks",
    "checked": true,
    "id": "f6424db408c9bbc949d529d65e7f0ee535515414",
    "semantic_title": "we demand justice!\": towards social context grounding of political texts",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_yGLm1EQGJ": {
    "title": "An Empirical Study of Data Ability Boundary in LLMs' Math Reasoning",
    "volume": "review",
    "abstract": "Large language models (LLMs) are displaying emergent abilities for math reasoning tasks, and there is a growing attention on enhancing the ability of open-source LLMs through supervised fine-tuning (SFT). In this paper, we aim to explore a general data strategy for supervised data to help optimize and expand math reasoning ability. Firstly, we determine the ability boundary of reasoning paths augmentation by identifying these paths' minimal optimal set. Secondly, we validate that different abilities of the model can be cumulatively enhanced by Mix of Minimal Optimal Sets of corresponding types of data, while our models MMOS achieve SOTA performance on series base models under much lower construction costs. Besides, we point out GSM-HARD is not really hard and today's LLMs no longer lack numerical robustness. Also, we provide an Auto Problem Generator for robustness testing and educational applications. Our code and data are publicly available at \\url{https://github.com/Anonymous}",
    "checked": true,
    "id": "cad3a64f1cb3020747b8b381d72f9032677469dd",
    "semantic_title": "an empirical study of data ability boundary in llms' math reasoning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=YuX-ctzWYMh": {
    "title": "Exciting Mood Changes: A Time-aware Hierarchical Transformer for Change Detection Modelling",
    "volume": "review",
    "abstract": "Through the rise of social media platforms, longitudinal language modelling has received much attention over the latest years, especially in downstream tasks such as mental health monitoring of individuals where modelling linguistic content in a temporal fashion is crucial. A key limitation in existing work is how to effectively model temporal sequences within Transformer-based language models. In this work we address this challenge by introducing a novel approach for predicting `Moments of Change' (MoC) in the mood of online users, by simultaneously considering user linguistic and time-aware context. A Hawkes process-inspired transformation layer is applied over the proposed architecture to model the influence of time on users' posts -- capturing both their immediate and historical dynamics. We perform experiments on the two existing datasets for the MoC task and showcase clear performance gains when leveraging the proposed layer. Our ablation study reveals the importance of considering temporal dynamics in detecting subtle and rare mood changes. Our results indicate that considering linguistic and temporal information in a hierarchical manner provide valuable insights into the temporal dynamics of modelling user generated content over time, with applications in mental health monitoring",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G3S7Eso_8u": {
    "title": "OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement",
    "volume": "review",
    "abstract": "The introduction of large language models has significantly advanced code generation. However, open-source models often lack the execution capabilities and iterative refinement of advanced systems like the GPT-4 Code Interpreter. To address this, we introduce OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code. Supported by Code Feedback, a dataset featuring 68K multi-turn interactions, OpenCodeInterpreter integrates execution and human feedback for dynamic code refinement. Our comprehensive evaluation of OpenCodeInterpreter across key benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus reveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6) with synthesized human feedback from GPT-4. OpenCodeInterpreterbrings the gap between open-source code generation models and proprietary systems like GPT-4 Code Interpreter",
    "checked": true,
    "id": "5eac2a40422a7085cb6f03285ad08210b6f6744b",
    "semantic_title": "opencodeinterpreter: integrating code generation with execution and refinement",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=ZQMEaTFhlt": {
    "title": "Revisiting Code Similarity Evaluation with Abstract Syntax Tree Edit Distance",
    "volume": "review",
    "abstract": "This paper revisits recent code similarity evaluation metrics, particularly focusing on the application of Abstract Syntax Tree (AST) editing distance in diverse programming languages.In particular, we explore the usefulness of these metrics and compare them to traditional sequence similarity metrics. Our experiments showcase the effectiveness of AST editing distance in capturing intricate code structures, revealing a high correlation with established metrics. Furthermore, we explore the strengths and weaknesses of AST editing distance and prompt-based GPT similarity scores in comparison to BLEU score, execution match, and Jaccard Similarity. We propose, optimize, and publish an adaptable metric that demonstrates effectiveness across all tested languages, representing an enhanced version of Tree Similarity of Edit Distance (TSED)",
    "checked": true,
    "id": "12b544604be93f05f9f3016a8629c4c585389003",
    "semantic_title": "revisiting code similarity evaluation with abstract syntax tree edit distance",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fsf_4ptwfX": {
    "title": "Meta-Tuning LLMs to Elicit Lexical Knowledge of Language Style",
    "volume": "review",
    "abstract": "Language style is often used by writers to convey their intentions, identities, and mastery of language. In this paper, we show that current large language models struggle to capture some language styles without fine-tuning. To address this challenge, we investigate whether LLMs can be meta-trained based on representative lexicons to recognize new styles they have not been fine-tuned on. Experiments on 13 established style classification tasks, as well as 63 novel tasks generated using LLMs, demonstrate that meta-training with style lexicons consistently improves zero-shot transfer across styles. Code and data to reproduce our experiments will be released upon publication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SI94CGKCus": {
    "title": "VillagerBench: Benchmarking Multi-Agent Collaboration in Minecraft",
    "volume": "review",
    "abstract": "In this paper, we aim to evaluate multi-agent systems against complex dependencies, including spatial, causal, and temporal constraints. First, we construct a new benchmark, named VillagerBench, within the Minecraft environment. VillagerBench comprises diverse tasks crafted to test various aspects of multi-agent collaboration, from workload distribution to dynamic adaptation and synchronized task execution. Second, we introduce a Directed Acyclic Graph Multi-Agent Framework (DAGENT) to resolve complex inter-agent dependencies and enhance collaborative efficiency. This solution incorporates a task decomposer that creates a directed acyclic graph (DAG) for structured task management, an agent controller for task distribution, and a state manager for tracking environmental and agent data.Our empirical evaluation on VillagerBench demonstrates that DAGENT outperforms the existing AgentVerse model, reducing hallucinations and improving task decomposition efficacy. The results underscore DAGENT's potential in advancing multi-agent collaboration, offering a scalable and generalizable solution in dynamic environments",
    "checked": false,
    "id": "76804a75be77959394307a80f3a33065b0a5ec9d",
    "semantic_title": "villageragent: a graph-based multi-agent framework for coordinating complex task dependencies in minecraft",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AHDvr0PCNGt": {
    "title": "Multimodal Depression Detection and Knowledge Infused Mental Health Therapeutic and Empathetic Response Generation",
    "volume": "review",
    "abstract": "The detection of depression through non-verbal cues has gained significant attention. Previous research predominantly centered on identifying depression within the confines of controlled laboratory environments, often with the supervision of psychologists or counselors. Unfortunately, datasets generated in such controlled settings may struggle to account for individual's behaviors in real-life situations. In response to this limitation, we present the Extended D-vlog dataset, encompassing a collection of $1,261$ YouTube vlogs. Additionally, the emergence of large language models (LLMs) like ChatGPT has sparked interest in their potential if they can act like a Mental Health Professionals. yet, the readiness of these LLM models to use it in real life setting is still a concern as they can give wrong responses which can be harmful for the users. We Proposed a Virtual Agent that can act as a first point of contact for mental health patient and provide a empathetic as well as the Therapeutic response alike human therapists. The system is divided into two part 1. Detection of a Disorder 2. Providing Empathetic and Therapeutic response to the user. The use of TVLT model on our Multimodal Extended D-vlog Dataset produced the most promising results, achieving a remarkable \\textbf{F1-score of $67.8$\\%}",
    "checked": false,
    "id": "a275071f9b02fd9f194ba0cfbefe1d662a7ed7e9",
    "semantic_title": "we care: multimodal depression detection and knowledge infused mental health therapeutic response generation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=fBSw9ay9vN": {
    "title": "FlowVQA: Mapping Multimodal Logic in Visual Question Answering with Flowcharts",
    "volume": "review",
    "abstract": "Existing benchmarks for visual question answering lack in visual grounding and complexity, particularly in evaluating spatial reasoning skills. We introduce FlowVQA, a novel benchmark aimed at assessing the capabilities of visual question-answering multimodal language models in reasoning with flowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and human-verified flowchart images from three distinct content sources, along with 22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks, including information localization, decision-making, and logical progression. We conduct a thorough baseline evaluation on a suite of both open-source and proprietary multimodal language models using various strategies, followed by an analysis of directional bias. The results underscore the benchmark's potential as a vital tool for advancing the field of multimodal modeling, providing a focused and challenging environment for enhancing model performance in visual and logical reasoning tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SQ7A3nBjBzB": {
    "title": "Advancing Reliable and Explainable Evaluation of Domain-Specific Retrieval-Augmented Language Models",
    "volume": "review",
    "abstract": "The advent of Large Language Models (LLMs) has significantly advanced the capabilities of Retrieval-augmented Generation (RAG) systems, leading to their extensive research and deployment across various industries for domain-specific knowledge querying. However, evaluating these systems presents unique challenges due to the scarcity of domain-specific queries and corresponding ground truths, as well as a lack of systematic approaches to diagnosing the cause of failure casesâ€”whether they stem from knowledge deficits or issues related to system robustness.To address these challenges, we introduce an evaluation framework comprising two key elements: 1) a data generation process that leverages relational databases and LLMs to efficiently produce scalable query-answer pairs, facilitating the separation of query logic from linguistic variations for enhanced debugging capabilities; and 2) an explainable evaluation protocol equipped with a novel metric that assesses the extent of knowledge comprehension and system robustness in both retrieval and language modeling contexts. Importantly, our empirical findings highlight the shortcomings of prevalent reference-free evaluation methods, positioning our reliable reference-based evaluation protocol as a valuable adjunct",
    "checked": false,
    "id": "1b0aba023d7aa5fb9853f9e942efb5c243dc1201",
    "semantic_title": "ragbench: explainable benchmark for retrieval-augmented generation systems",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CMzxhwBbvR": {
    "title": "Does DetectGPT Fully Utilize Perturbation? Bridge Selective Perturbation to Fine-tuned Contrastive Learning Detector would be Better",
    "volume": "review",
    "abstract": "The burgeoning generative capabilities of large language models (LLMs) have raised growing concerns about abuse, demanding automatic machine-generated text detectors. DetectGPT, a zero-shot metric-based detector, first introduces perturbation and shows great performance improvement. However, in DetectGPT, random perturbation strategy could introduce noise, and logit regression depends on threshold, harming the generalizability and applicability of individual or small-batch inputs. Hence, we propose a novel fine-tuned detector, PECOLA, bridging metric-based and fine-tuned detectors by contrastive learning on selective perturbation. Selective strategy retains important tokens during perturbation and weights for multi-pair contrastive learning. The experiments show that PECOLA outperforms the state-of-the-art by 1.20% in accuracy on average on four public datasets. And we further analyze the effectiveness, robustness, and generalization of the method",
    "checked": true,
    "id": "8bfa5558adb601b733ba7473fe52abb31f4b89e6",
    "semantic_title": "does detectgpt fully utilize perturbation? bridge selective perturbation to fine-tuned contrastive learning detector would be better",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=3M-6pxiNqnL": {
    "title": "How Adversarial Can You Get Against LLMs? Evaluation and Creation of Adversarial Examples in Question Answering",
    "volume": "review",
    "abstract": "Adversarial benchmarks validate model abilities by providing samples that fool models but not humans. We introduce an evaluation metric, AdvScore, that quantifies how adversarial and discriminative questions are. We then use AdvScore to create a pipeline that incentivizes writing good adversarial questions. We collect an adversarial QA dataset, AdvQA, from our pipeline's interface for eliciting human-authored adversarial examples. Questions in AdvQA surpass those in four challenging datasets across domains at not fooling humans but still fooling several language models, including GPT4. Additional analyses validate that AdvQA contains realistic and high-quality questions, based on difficulty estimates from 4890 human responses and responses from six models. Our evaluation pipeline is easily portable from QA to other domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cfEiMU8dEy": {
    "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling",
    "volume": "review",
    "abstract": "We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages.We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs.Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model.Demos are shown in \\href{https://any-gpt.github.io/AnyGPT.github.io}{https://any-gpt.github.io/AnyGPT.github.io}",
    "checked": true,
    "id": "14191e9f12913ad8c7ac6e1188682afac04aad09",
    "semantic_title": "anygpt: unified multimodal llm with discrete sequence modeling",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=xv9yNz5U2vV": {
    "title": "Enhancing Table-to-Text Generation: A Focus on Subjectivity and Objectivity with the TaTS Dataset",
    "volume": "review",
    "abstract": "Table-to-text generation, a long-standing challenge in natural language generation, has remained unexplored through the lens of subjectivity. Subjectivity here encompasses the comprehension of information derived from the table that cannot be described solely by objective data. To ascertain the relevance and social applications of this work, we conduct a public survey involving relevant people. The survey results unequivocally conclude the significance of this research. Given the absence of pre-existing datasets, we introduce the TaTS dataset. We perform the task using various sequence-to-sequence models on the flattened table and analyze the results from a qualitative perspective to ensure the capture of subjectivity. To the best of our knowledge, this is the first kind of dataset on tables with subjectivity included",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ht7VoweN4o": {
    "title": "Task-Adaptation Curriculum Learning",
    "volume": "review",
    "abstract": "Despite the prevailing applications of foundation models, when adapted to downstream tasks, their performance sensitively varies with the distribution shift/gap between the pertaining task and the target task. Moreover, direct fine-tuning might be overfitting to limited target task data. In the realm of NLP, tasks are semantically related with shared skills and those general purposed ones usually have more available data than the highly specific and user-defined ones. In this paper, we mitigate the distribution shift in task adaptation by developing a smooth transfer learning curriculum, which, by fine-tuning the model along a path of intermediate tasks on a graph, progressively bridges the gap between the pretrained model and a target task with limited data. To this end, we formulate the curriculum learning as a graph search problem and address its efficiency by a deep dive into accelerating the transferability estimation between tasks and two classical search algorithm applied to our problem, i.e., greedy best first search and Monte Carlo tree search. We evaluate our approach, i.e., ``task-adaptation curriculum learning (TaCL)'' on two benchmark settings with tasks drawn from GLUE. Extensive experiments on different target tasks demonstrate the effectiveness and advantages of \\ours on more specific and data-deficient downstream tasks",
    "checked": false,
    "id": "4c5ba59ef22cfc3b5108476dd0df14ffac0301b5",
    "semantic_title": "goats: goal sampling adaptation for scooping with curriculum reinforcement learning",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=GC8N7k9wChr": {
    "title": "Unveiling Deception: A Novel Framework Integrating BERT and Vocabulary Graph Convolution Networks for Fake News Detection",
    "volume": "review",
    "abstract": "False information, commonly known as fake news, pertains to deceptive and incorrect content disseminated in news format on the internet. On one hand, it possesses the potential to influence the outcome of exit polls by tarnishing the reputation of a deserving candidate. On the other hand, it can contribute to the instigation of communal violence. In this paper, we underscore the distinct linguistic patterns existing between fake and real news. We introduce a novel framework for identifying fake news, involving the fusion of two separate vocabulary graphs derived from the lexical spaces of both fake and real news. Essentially, our approach integrates the strengths of BERT with these two distinct vocabulary graphs. Further, Vocabulary Graph Convolution Networks generate document representation for classification. We report the F1 score of 94.94 and 98.44 on the Recovery and Multi-Source Fake datasets, respectively. Results show that our approach outperformed eight other deep neural frameworks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CHWrcPePyW": {
    "title": "Born Differently Makes a Difference: Counterfactual Study of Bias in Biography Generation from a Data-to-Text Perspective",
    "volume": "review",
    "abstract": "How do personal attributes affect biography generation? Addressing this question requires an identical pair of biographies where only the personal attributes of interest are different. However, it is rare in the real world. To address this, we propose a counterfactual methodology from a data-to-text perspective, manipulating the personal attributes of interest while keeping the co-occurring attributes unchanged. We first validate that the fine-tuned Flan-T5 model generates the biographies based on the given attributes. This work expands the study of gender-centered bias analysis in text generation. Our results confirm the well-known bias in gender and also show the bias in regions, in both individual and its related co-occurring attributes in semantic machining and sentiment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z8i7DYbMeT": {
    "title": "SoMeLVLM: A Large Vision Langague Model for Social Media Processing",
    "volume": "review",
    "abstract": "The growth of social media, characterized by its multimodal nature, has led to the emergence of diverse phenomena and challenges, which calls for an effective approach to uniformly solve automated tasks. The powerful Large Vision Language Models make it possible to handle a variety of tasks simultaneously, but even with carefully designed prompting methods, the general domain models often fall short in aligning with the unique speaking style and context of social media tasks. In this paper, we introduce a Large Vision Language Model for Social Media Processing (SoMeLVLM), which is a cognitive framework equipped with five key capabilities including knowledge & comprehension, application, analysis, evaluation, and creation. SoMeLVLM is designed to understand and generate realistic social media behavior. We have developed a 654k multimodal social media instruction-tuning dataset to support our cognitive framework and fine-tune our model. Our experiments demonstrate that SoMeLVLM achieves state-of-the-art performance in multiple social media tasks. Further analysis shows its significant advantages over baselines in terms of cognitive abilities",
    "checked": false,
    "id": "bdd9d488ddff6af61bbc3d0d69efde6a286ce685",
    "semantic_title": "somelvlm: a large vision language model for social media processing",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=_9Q9cJrs_rt": {
    "title": "Perspective-taking Gives More Ethical Responses",
    "volume": "review",
    "abstract": "The ethics of LLM-generated content is a pressing concern, prompting the need for a universal and holistic method to moderate outputs in line with human values. Unlike previous efforts focusing on specific features like toxicity and bias, a model-agnostic and lightweight solution is desired. Inspired by social psychology, we propose perspective-taking prompting, where the LLM considers other humans' perspectives to self-moderate its generation without external feedback, resulting in more ethical responses. Extensive experiments of detoxification and debiasing on ChatGPT and GLM show that our approach can let LLMs give more ethical responses and outperform 6 baselines",
    "checked": false,
    "id": "b352a18a22890086a12b45c8ef31fb2241811ba9",
    "semantic_title": "visual perspective taking without visual perspective taking",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=Lch6RPor8i": {
    "title": "ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval",
    "volume": "review",
    "abstract": "We propose ListT5, a novel reranking approach based on Fusion-in-Decoder (FiD) that handles multiple candidate passages at both train and inference time. We also introduce an efficient inference framework for listwise ranking based on m-ary tournament sort with output caching. We evaluate and compare our model on the BEIR benchmark for zero-shot retrieval task, demonstrating that ListT5 (1) outperforms the state-of-the-art RankT5 baseline with a notable +1.3 gain in the average NDCG@10 score, (2) has an efficiency comparable to pointwise ranking models and surpasses the efficiency of previous listwise ranking models, and (3) overcomes the lost-in-the-middle problem of previous listwise rerankers. Our code, model checkpoints, and the evaluation framework will be fully open-sourced",
    "checked": true,
    "id": "56981f89e2b30088aa2eb9a4c5c575665eb3f57b",
    "semantic_title": "listt5: listwise reranking with fusion-in-decoder improves zero-shot retrieval",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=UFOJOwzOFM": {
    "title": "Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models",
    "volume": "review",
    "abstract": "Causal graph recovery is essential in the field of causal inference. Traditional methods are typically knowledge-based or statistical estimation-based, which are limited by data collection biases and individuals' knowledge about factors affecting the relations between variables of interests. The advance of large language models (LLMs) provides opportunities to address these problems. We propose a novel method that utilizes the extensive knowledge contained within a large corpus of scientific literature to deduce causal relationships in general causal graph recovery tasks. This method leverages Retrieval Augmented-Generation (RAG) based LLMs to systematically analyze and extract pertinent information from a comprehensive collection of research papers. Our method first retrieves relevant text \\oran{chunks} from the aggregated literature. Then, the LLM is tasked with identifying and labelling potential associations between factors. Finally, we give a method to aggregate the associational relationships to build a causal graph. We demonstrate our method is able to construct high quality causal graphs on the well-known SACHS dataset solely from literature",
    "checked": true,
    "id": "0504c5d96ebf7be01e2b622ff3a4bf155f2b0a41",
    "semantic_title": "causal graph discovery with retrieval-augmented generation based large language models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=llEJ_d6Z-p": {
    "title": "BiMediX: Bilingual Medical Mixture of Experts LLM",
    "volume": "review",
    "abstract": "In this paper, we introduce BiMediX, the first bilingual medical mixture of experts LLM designed for seamless interaction in both English and Arabic. Our model facilitates a wide range of medical interactions in English and Arabic, including multi-turn chats to inquire about additional details such as patient symptoms and medical history, multiple-choice question answering, and open-ended question answering. We propose a semi-automated English-to-Arabic translation pipeline with human refinement to ensure high-quality translations. We also introduce a comprehensive evaluation benchmark for Arabic medical LLMs. Furthermore, we introduce BiMed1.3M, an extensive Arabic-English bilingual instruction set that covers 1.3 Million diverse medical interactions, resulting in a total of over 632 million healthcare specialized tokens for instruction tuning. Our BiMed1.3M dataset includes 200k synthesized multi-turn doctor-patient chats, and it maintains a 1:2 Arabic-to-English ratio. Our model outperforms state-of-the-art Med42 and Meditron by average absolute gains of 2.5% and 4.1%, respectively, computed across multiple medical evaluation benchmarks in English, while operating at 8-times faster inference. Moreover, our BiMediX outperforms the generic Arabic-English bilingual LLM, Jais-30B, by average absolute gains of 10% on our Arabic medical benchmark and 15% on bilingual evaluations across multiple datasets. Our trained models, instruction set, and source code will be made publicly available",
    "checked": true,
    "id": "2db51d392ba762af3703240325dc74df2c112b8f",
    "semantic_title": "bimedix: bilingual medical mixture of experts llm",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=wN_sl9hQaU6": {
    "title": "More frequent verbs are associated with more diverse valency frames: efficient language design at the lexicon-grammar interface",
    "volume": "review",
    "abstract": "A substantial body of work has provided evidence that the lexicons of natural languages are organized to support efficient communication. However, existing work has largely focused on word-internal properties, such as Zipf's classical observation that more frequent words are optimized in form to minimize communicative cost. Here, we investigate the hypothesis that efficient lexicon organization also affects valency, or the combinations and orders of additional words and phrases a verb selects for in a sentence.We consider two measures of valency diversity for verbs: valency frame count (vfc), the number of distinct frames associated with a verb, and valency frame entropy (vfe), the average information content of frame selection associated with a verb. Using data from 79 languages, we provide evidence that more frequent verbs are associated with a greater diversity of valency frames, suggesting that the organization of valency helps optimize communicative efficiency. We discuss our findings in relation to classical findings such as the principle of least effort, as well as implications for theories of valency and efficient language design",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tKPSRXcc6C": {
    "title": "Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers",
    "volume": "review",
    "abstract": "Although dominant in natural language processing, transformer-based models remain challenged by the task of long-sequence processing, because the computational cost of self-attention operations in transformers swells quadratically with the input sequence length. To alleviate the complexity of long-sequence processing, we propose a framework to enable the off-the-shelf pre-trained transformers to process much longer sequences, while the computation and memory costs remain growing linearly with the input length. More specifically, our method divides each long-sequence input into a batch of chunks, then aligns the inter-chunk information during the encoding steps, and finally selects the most representative hidden states from the encoder for the decoding process. To extract inter-chunk semantic information, we align the start and end token embeddings among chunks in each encoding transformer block. To learn an effective hidden selection policy, we design a dual updating scheme inspired by reinforcement learning, which regards the transformers as environments, and leverages the attention scores and the downstream performance feedback as the rewards to optimize the hidden selection policy. Our empirical results on real-world long-text abstractive summarization and reading comprehension tasks demonstrate effective improvements compared to prior long-sequence processing baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lQAT0rNPZoj": {
    "title": "A Critical Study of What Code-LLMs (Do Not) Learn",
    "volume": "review",
    "abstract": "Large Language Models trained on code corpora (code-LLMs) have demonstrated impressive performance in various coding assistance tasks. However, despite their increased size and training dataset, code-LLMs still have limitations such as suggesting codes with syntactic errors, variable misuse etc. Some studies argue that code-LLMs perform well on coding tasks because they use self-attention and hidden representations to encode relations among input tokens. However, previous works have not studied what code properties are not encoded by code-LLMs. In this paper, we conduct a fine-grained analysis of attention maps and hidden representations of code-LLMs. Our study indicates that code-LLMs only encode relations among specific subsets of input tokens. Specifically, by categorizing input tokens into syntactic tokens and identifiers, we found that models encode relations among syntactic tokens and among identifiers, but they fail to encode relations between syntactic tokens and identifiers. We also found that fine-tuned models encode these relations poorly compared to their pre-trained counterparts. Additionally, larger models with billions of parameters encode significantly less information about code than models with only a few hundred million parameters",
    "checked": true,
    "id": "eafab7c72654626d198d669e5b6bd51739bedb7e",
    "semantic_title": "a critical study of what code-llms (do not) learn",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sPw7Pl_AYa": {
    "title": "LAKA: A Label-Aware and Knowledge-Augmented Framework for Multi-Label Text Classification",
    "volume": "review",
    "abstract": "Multi-label text classification (MLTC) predicts multiple labels for a single text instance, with strong correlations between and within labels and text. Previous methods of MLTC are not effective enough to capture label-specific information and suffer from insufficient interaction between labels and text features. In this work, we propose Label-Aware and Knowledge-Augmented (LAKA) Framework for MLTC, strengthening the attention to relations between key information and labels via introducing structured information, mitigating the mentioned problems. Specifically, we introduce Multi-Label Text Knowledge Augmentation (MTKA), extracting external information via an external information extractor based on graph neural networks (GNN) and augmenting the backbone model with external knowledge fusion (EKF). Meanwhile, we introduce label-aware union embedding (LAUE) to strengthen interactions between and within labels and text by encoding labels and text together for BERT. Experiments on three datasets demonstrate that our method significantly outperforms strong baselines in various mainstream MLTC datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vi0d9CrYHn": {
    "title": "MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning",
    "volume": "review",
    "abstract": "Since commonsense information has been recorded significantly less frequently than its existence, language models pre-trained by text generation have difficulty to learn sufficient commonsense knowledge. Several studies have leveraged text retrieval to augment the models' commonsense ability. Unlike text, images capture commonsense information inherently but little effort has been paid to effectively utilize them. In this work, we propose a novel \\textbf{M}ulti-m\\textbf{O}dal \\textbf{RE}trieval (MORE) augmentation framework, to leverage both text and images to enhance the commonsense ability of language models. Extensive experiments on the Common-Gen task have demonstrated the efficacy of MORE based on the pre-trained models of both single and multiple modalities",
    "checked": true,
    "id": "caffd31652dc9bf7ef181809092dec9fcad92c2f",
    "semantic_title": "more: multi-modal retrieval augmented generative commonsense reasoning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=xWhikZZT3_": {
    "title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
    "volume": "review",
    "abstract": "The literature review is an indispensable step in the research process. It provides the benefit of comprehending the research problem and understanding the current research situation while conducting a comparative analysis of prior works. However, literature summary is challenging and time consuming. The previous LLM-based studies on literature review mainly focused on the complete process, including literature retrieval, screening, and summarization. However, for the summarization step, simple CoT method often lacks the ability to provide extensive comparative summary. In this work, we firstly focus on the independent literature summarization step and introduce ChatCite, an LLM agent with human workflow guidance for comparative literature summary. This agent, by mimicking the human workflow, first extracts key elements from relevant literature and then generates summaries using a Reflective Incremental Mechanism. In order to better evaluate the quality of the generated summaries, we devised a LLM-based automatic evaluation metric, G-Score, in refer to the human evaluation criteria. The ChatCite agent outperformed other models in various dimensions in the experiments. The literature summaries generated by ChatCite can also be directly used for drafting literature reviews",
    "checked": true,
    "id": "258a9c9ff34b1739e701d25ae277a412c47aa7dd",
    "semantic_title": "chatcite: llm agent with human workflow guidance for comparative literature summary",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UXR89pGPVE": {
    "title": "LLM-REDIAL: A Large-Scale Dataset for Conversational Recommender Systems Created from User Behaviors with LLMs",
    "volume": "review",
    "abstract": "The large-scale conversational recommendation dataset is pivotal for the development of conversational recommender systems (CRS). Most existing CRS datasets suffers from the problems of data inextensibility and semantic inconsistency. To tackle these limitations and establish a benchmark in the conversational recommendation scenario, in this paper, we introduce the LLM-REDIAL dataset to facilitate the research in CRS. LLM-REDIAL is constructed by leveraging large language models (LLMs) to generate the high-quality dialogues. To provide the LLMs with detailed guidance, we integrate historical user behavior data with dialogue templates that are carefully designed through the combination of multiple pre-defined goals. LLM-REDIAL has two main advantages. First, it is the largest multi-domain CRS dataset which consists of 47.6k multi-turn dialogues with 482.6k utterances across 4 domains. Second, dialogue semantics and the users' historical interaction information is highly consistent. Human evaluation are conducted to verify the quality of LLM-REDIAL. In addition, we evaluate the usability of advanced LLM-based models on LLM-REDIAL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h28XF1xpnF": {
    "title": "Codec-SUPERB: An In-Depth Analysis of Sound Codec Models",
    "volume": "review",
    "abstract": "The sound codec's dual roles in minimizing data transmission latency and serving as tokenizers underscore its critical importance.Recent years have witnessed significant developments in codec models.The ideal sound codec should preserve content, paralinguistics, speakers, and audio information. However, the question of which codec achieves optimal sound information preservation remains unanswered, as in different papers, models are evaluated on their selected experimental settings.This study introduces Codec-SUPERB, an acronym for Codec sound processing Universal PERformance Benchmark. It is an ecosystem designed to assess codec models across representative sound applications and signal-level metrics rooted in sound domain knowledge.Codec-SUPERB simplifies result sharing through an online leaderboard, promoting collaboration within a community-driven benchmark database, thereby stimulating new development cycles for codecs.Furthermore, we undertake an in-depth analysis to offer insights into codec models from both application and signal perspectives, diverging from previous codec papers mainly concentrating on signal-level comparisons.Finally, we will release codes, the leaderboard, and data to accelerate progress within the community",
    "checked": true,
    "id": "5bde820a55815dc30bf4e871444d763680ba7e2e",
    "semantic_title": "codec-superb: an in-depth analysis of sound codec models",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=azk84odPAK": {
    "title": "Automated Data Curation for Robust Language Model Fine-Tuning",
    "volume": "review",
    "abstract": "Large Language Models have become the de facto approach to sequence-to-sequence text generation tasks, but for specialized tasks/domains, a pretrained LLM lacks specific capabilities to produce accurate or well-formatted responses. Supervised fine-tuning specializes a LLM by training it on dataset of example prompts with desired target responses, but real-world data tends to be noisy. While many fine-tuning algorithms exist, here we consider a \\emph{data-centric AI} perspective on LLM fine-tuning, studying how to \\emph{systematically} curate the training dataset to improve the LLM produced via \\emph{any} fine-tuning algorithm.We introduce an automated data curation pipeline \\modelnameA{} (\\underline{\\textbf{C}}onfidence-based \\underline{\\textbf{L}}LM \\underline{\\textbf{E}}valuation \\underline{\\textbf{A}}nd \\underline{\\textbf{R}}ectification) for instruction tuning datasets, that can be used with any LLM and fine-tuning procedure. CLEAR estimates which training data is low-quality and either filters or corrects it. Quality scoring and the decision to filter or correct are based on LLM-derived confidence estimates, which ensure algorithmic modifications to the dataset remain suitably conservative. Unlike existing data curation techniques, CLEAR is a comprehensive framework that can improve a dataset (and trained model outputs) without additional fine-tuning computations. Importantly, we don't assume access to a stronger LLM than the model being fine-tuned (e.g.\\ relying on GPT-4 when fine-tuning GPT-3.5), to see whether CLEAR can meaningfully improve the capabilities of any LLM. Our experiments reveal that CLEAR consistently improves the performance of fine-tuned models across many datasets and models (like GPT-3.5 and Llama2)",
    "checked": true,
    "id": "2d1adda7a9b0596d8b470c065da5a6528b364a7f",
    "semantic_title": "automated data curation for robust language model fine-tuning",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=wt16dw1YRGM": {
    "title": "SSP: Story-Space Prompting Improves the Reader Immersion in Long Story Generation",
    "volume": "review",
    "abstract": "Generating long-form stories with neural network models, even the large language models (LLMs), e.g., GPT, has always been criticized for lacking interestingness and coherence, thus greatly diminishing the reader's sense of immersion. In this paper, we present a novel ``story space'' prompting (SSP) solution, which provides a coherent and consistent background to support long-term storytelling. Specifically, we first define the story space intricately connected to the given story premise. Then, Our framework systematically generates the story space by progressively constructing it from an abstract representation to a more informative and detailed one. Empirically, we implement our plug-in method upon an existing advanced story generation framework (Yang et al., 2023) and evaluate its impact on both interestingness and coherence. Our findings emphasize the significance of our SSP in enhancing reader engagement and immersion, contributing to advancements in long-form story generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ts1iSspc6F": {
    "title": "MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning",
    "volume": "review",
    "abstract": "Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose a novel Multi-disciplinary Collaboration (MC) framework for the medical domain that leverages role-playing LLM-based agents who participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free and interpretable framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine datasets (MedQA, MedMCQA, PubMedQA, and six subtasks from MMLU) establish that our proposed MC framework excels at mining and harnessing the medical expertise within LLMs, as well as extending its reasoning abilities",
    "checked": true,
    "id": "44d16a076c00ecada3d425203377e4ec951c4ed0",
    "semantic_title": "medagents: large language models as collaborators for zero-shot medical reasoning",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=qKUa5BnoY9U": {
    "title": "Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution",
    "volume": "review",
    "abstract": "Faithfulness, expressiveness, and elegance is the constant pursuit in machine translation. However, traditional metrics like \\textit{BLEU} do not strictly align with human preference of translation quality. In this paper, we explore leveraging reinforcement learning with human feedback (\\textit{RLHF}) to improve translation quality. It is non-trivial to collect a large high-quality dataset of human comparisons between translations, especially for low-resource languages. To address this issue, we propose a cost-effective preference learning strategy, optimizing reward models by distinguishing between human and machine translations. In this manner, the reward model learns the deficiencies of machine translation compared to human and guides subsequent improvements in machine translation. Experimental results demonstrate that \\textit{RLHF} can effectively enhance translation quality and this improvement benefits other translation directions not trained with \\textit{RLHF}. Further analysis indicates that the model's language capabilities play a crucial role in preference learning. A reward model with strong language capabilities can more sensitively learn the subtle differences in translation quality and align better with real human translation preferences",
    "checked": true,
    "id": "f3bce99b1257132b386db8e46177b936440376a4",
    "semantic_title": "advancing translation preference modeling with rlhf: a step towards cost-effective solution",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=l64M1zMfvKh": {
    "title": "MCUBERT: Memory-Efficient BERT Inference on Commodity Microcontrollers",
    "volume": "review",
    "abstract": "In this paper, we propose MCUBERT to enable language models like BERT on tiny microcontroller units (MCUs) through network and scheduling co-optimization. We observe the embedding table contributes to the major storage bottleneck for tiny BERT models. Hence, at the network level, we propose an MCU-aware two-stage neural architecture search algorithm based on clustered low-rank approximation for embedding compression. To reduce the inference memory requirements, we further propose a novel fine-grained MCU-friendly scheduling strategy. Through careful computation tiling and re-ordering as well as kernel design, we drastically increase the input sequence lengths supported on MCUs without any latency or accuracy penalty. MCUBERT reduces the parameter size of BERT-tiny and BERT-mini by 5.7$\\times$ and 3.0$\\times$ and the execution memory by 3.5$\\times$ and 4.3$\\times$, respectively. MCUBERT also achieves 1.5$\\times$ latency reduction. For the first time, MCUBERT enables lightweight BERT models on commodity MCUs and processing more than 512 tokens with less than 256KB of memory",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XWZXCJvpNN": {
    "title": "ActionIE: Action Extraction from Scientific Literature with Programming Languages",
    "volume": "review",
    "abstract": "Extraction of experimental procedures from human language in scientific literature and patents into actionable sequences in robotics language holds immense significance in scientific domains. Such an action extraction task is particularly challenging given the intricate details and context-dependent nature of the instructions, especially in fields like chemistry where reproducibility is paramount. In this paper, we introduce ActionIE, a method that leverages Large Language Models (LLMs) to bridge this divide by converting actions written in natural language into executable Python code. This enables us to capture the entities of interest, and the relationship between each action, given the features of Programming Languages. Utilizing linguistic cues identified by frequent patterns, ActionIE provides an improved mechanism to discern entities of interest. While our method is broadly applicable, we exemplify its power in the domain of chemical literature, wherein we focus on extracting experimental procedures for chemical synthesis. The code generated by our method can be easily transformed into robotics language which is in high demand in scientific fields. Comprehensive experiments demonstrate the superiority of our method. In addition, we propose a graph-based metric to more accurately reflect the precision of extraction. We also develop a dataset to address the scarcity of scientific literature occurred in existing datasets",
    "checked": false,
    "id": "090ff0108a603d4256d9dd45c65a7adeb573e239",
    "semantic_title": "unearthing the past for a sustainable future: extracting and transforming data in the biodiversity heritage library for climate action",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5gZUwfMQBIE": {
    "title": "MULFE: A Multi-Level Benchmark for Free Text Model Editing",
    "volume": "review",
    "abstract": "Adjusting the outdated behaviors of large langugae models (LLMs) after deployment remains a significant challenge. It motivates the model editing research, which is however mainly explored in a restricted task form with triple-based edit requests. Some recent works have initiated a transition to a more practical and unified editing task that takes free-form text as edit requests. However, there is gaps in nuanced benchmark designs and re-evaluation of existing methods. To bridge the gaps, we introduce a multi-level benchmark for free text model editing (\\textsc{Mulfe}). The benchmark categorizes probe queries into three levels of generalization, ranging from basic literal memory to deeper understanding and reasoning. Based on the benchmark, we conduct extensive experiments across various base models, edit sizes, and editing methods, including adaptations of mainstream locate-and-edit and hypernetwork methods. The results highlight the inconsistent behaviors of edited models on different generalization levels. Higher level of generalization is still difficult. Based on the findings, we propose \\textsc{Side}, a simple yet effective method based on in-context distillation to enhance the generalization performance. The benchmark and baseline methods will be publicly available for facilitating further study",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PLVnlzzC1gX": {
    "title": "Have Seen Me Before? Automating Dataset Updates Towards Reliable and Timely Evaluation",
    "volume": "review",
    "abstract": "Due to the expanding capabilities and pre-training data, Large Language Models (LLMs) are facing increasingly serious evaluation challenges. On one hand, the data leakage issue cause over-estimation on existing benchmarks. On the other hand, periodically curating datasets manually is costly. In this paper, we propose to automate dataset updates for reliable and timely evaluation. The basic idea is to generate unseen and high-quality testing samples based on existing ones to mitigate leakage issues.In specific, we propose two strategies with systematically verification. First, the mimicking strategy employs LLMs to create new samples resembling existing ones, to the maximum extent preserving the stylistic of the original dataset. Our experiments demonstrate its evaluation stability across multiple instantiations and its effectiveness in dealing with data leakage issues in most cases. Second, for the cases that mimicking dataset works poorly, we design an extending strategy that adjusts the difficulty of the generated samples according to varying cognitive levels. This not only makes our evaluation more systematic, but also, with a balanced difficulty, even discern model capabilities better at fine-grained levels",
    "checked": true,
    "id": "91c63cd62f4ccefa2672137481909f9fe4163838",
    "semantic_title": "have seen me before? automating dataset updates towards reliable and timely evaluation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=gGSHgqM6ZK": {
    "title": "PairCFR: Enhancing Model Training on Paired Counterfactually Augmented Data through Contrastive Learning",
    "volume": "review",
    "abstract": "Counterfactually Augmented Data (CAD) involves creating new data samples by applying minimal yet sufficient modifications to flip the label of existing data samples to the other classes. Training with CAD enhances model robustness against spurious features that happen to correlate with labels by spreading the casual relationships across different classes. Yet, recent research reveals that CAD may lead models to overly focus on modified features while ignoring other important contextual information, inadvertently introducing biases that may impair performance on out-of-distribution (OOD) datasets. To mitigate this issue, we employ contrastive learning to promote global feature alignment in addition to counterfactual clues. We theoretically prove that contrastive loss can encourage models to leverage a broader range of features beyond those modified ones. Comprehensive experiments on two human-edited CAD datasets demonstrate that our proposed method outperformed the state-of-the-art on OOD datasets",
    "checked": true,
    "id": "8a9d3eaaca1ca91527e8bb026d670603d48c8577",
    "semantic_title": "paircfr: enhancing model training on paired counterfactually augmented data through contrastive learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=MTUETv5RdE": {
    "title": "Jailbreaking Proprietary Large Language Models using Word Substitution Cipher",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) are aligned to moral and ethical guidelines but remain susceptible to creative prompts called Jailbreak that can bypass the alignment process. However, most jailbreaking prompts contain harmful questions in the natural language (mainly English), which can be detected by the LLM themselves. In this paper, we present jailbreaking prompts encoded using cryptographic techniques. We first present a pilot study on the state-of-the-art LLM, GPT-4, in decoding several safe sentences that have been encrypted using various cryptographic techniques and find that a straightforward word substitution cipher can be decoded most effectively. Motivated by this result, we use this encoding technique for writing jailbreaking prompts. We present a mapping of unsafe words with safe words and ask the unsafe question using these mapped words. Experimental results show an attack success rate (up to 59.42%) of our proposed jailbreaking approach on state-of-the-art proprietary models including ChatGPT, GPT-4, and Gemini-Pro. Additionally, we discuss the over-defensiveness of these models. We believe that our work will encourage further research in making these LLMs more robust while maintaining their decoding capabilities",
    "checked": true,
    "id": "73ce1a5be7b7d1f863293d8c3afa787c6922a797",
    "semantic_title": "jailbreaking proprietary large language models using word substitution cipher",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=UPuevZtj3r": {
    "title": "Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge",
    "volume": "review",
    "abstract": "Jailbreaking attacks can enable Large Language Models (LLMs) to bypass the safeguard and generate harmful content. Existing jailbreaking defense methods have failed to address the fundamental issue that harmful knowledge resides within the model, leading to potential jailbreak risks for LLMs. In this paper, we propose a novel defense method called Eraser, which mainly includes there goals: unlearning harmful knowledge, retaining general knowledge, and maintaining safety alignment. The intuition is that if an LLM forgets the specific knowledge required to answer a harmful question, it will no longer have the ability to answer harmful questions. The training of Erase does not actually require the model's own harmful knowledge, and it can benefit from unlearning general answers related to harmful queries, which means it does not need assistance from the red team. The experimental results show that Eraser can significantly reduce the jailbreaking success rate for various attacks without compromising the general capabilities of the model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vx1Zm6Mx-q": {
    "title": "Fine-Tuning Language Models on Multiple Datasets for Citation Intention Identification",
    "volume": "review",
    "abstract": "Citation intention identification (CII) tools classify citations by their intention (e.g., background, motivation) and assist readers in comprehending scientific literature. Prior research has shown that pretrained language models (PLMs) such as SciBERT can achieve state-of-the-art performance on CII benchmarks. PLMs are trained via self-supervision tasks on a large corpus of general text and can quickly adapt to CII tasks via moderate fine-tuning on the corresponding dataset. Despite their advantages, PLMs can easily overfit small datasets during fine-tuning. In this paper, we propose a multi-task learning (MTL) framework that jointly fine-tunes PLMs on a dataset of primary interest together with multiple auxiliary CII datasets to take advantage of additional supervision signals. We develop a data-driven task relation learning method that controls the contribution of auxiliary datasets to avoid negative transfer and expensive hyper-parameter tuning. We conduct experiments on three CII datasets and show that fine-tuning with additional datasets can improve the PLMs' generalization performance on the primary dataset. PLMs fine-tuned with our proposed framework outperform the current state-of-the-art models by 7% to 11% on small datasets while aligning with the best-performing model on a large dataset",
    "checked": false,
    "id": "570742c49d4f0e23e6eb97b5020aae36cc2291f3",
    "semantic_title": "ait_fhstp at exist 2023 benchmark: sexism detection by transfer learning, sentiment and toxicity embeddings and hand-crafted features",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=wjdu8AshBiF": {
    "title": "LLM Questionnaire Completion for Automatic Psychiatric Assessment",
    "volume": "review",
    "abstract": "We employ a Large Language Model (LLM) to convert unstructured psychological interviews into structured questionnaires spanning various psychiatric and personality domains. The LLM is prompted to answer these questionnaires by impersonating the interviewee. The obtained answers are coded as features, which are used to predict standardized psychiatric measures of depression (PHQ-8) and PTSD (PCL-C), using a Random Forest regressor. Our approach is shown to enhance diagnostic accuracy compared to multiple baselines. It thus establishes a novel framework for interpreting unstructured psychological interviews, bridging the gap between narrative-driven and data-driven approaches for mental health assessment",
    "checked": true,
    "id": "a4694af9214241b6131f2b97563657a7ddd5a1a3",
    "semantic_title": "llm questionnaire completion for automatic psychiatric assessment",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tz3nojCrIqB": {
    "title": "Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning",
    "volume": "review",
    "abstract": "This paper introduces the novel task of multimodal puzzle solving, framed within the context of visual question-answering. We present a new dataset, AlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal language models in solving algorithmic puzzles that necessitate both visual understanding, language understanding, and complex algorithmic reasoning. We create the puzzles to encompass a diverse array of mathematical and algorithmic topics such as boolean logic, combinatorics, graph theory, optimization, search, etc., aiming to evaluate the gap between visual data interpretation and algorithmic problem-solving skills. The dataset is generated automatically from code authored by humans. All our puzzles have exact solutions that can be found from the algorithm without tedious human calculations. It ensures that our dataset can be scaled up arbitrarily in terms of reasoning complexity and dataset size. Our investigation reveals that large language models (LLMs) such as GPT4V and Gemini exhibit limited performance in puzzle-solving tasks. We find that their performance is near random in a multi-choice question-answering setup for a significant number of puzzles. The findings emphasize the challenges of integrating visual, language, and algorithmic knowledge for solving complex reasoning problems",
    "checked": true,
    "id": "1591cb27faecdc54839496c87cf71685d6edc38c",
    "semantic_title": "are language models puzzle prodigies? algorithmic puzzles unveil serious challenges in multimodal reasoning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=9GB2ehrY_rC": {
    "title": "Meta-Task Prompting Elicits Embedding from Large Language Models",
    "volume": "review",
    "abstract": "In this work, we introduce a new unsupervised embedding method, Meta-Task Prompting with Explicit One-Word Limitation (MetaEOL), for generating high-quality sentence embeddings from Large Language Models (LLMs) without the need for model fine-tuning or task-specific engineering. Leveraging meta-task prompting, MetaEOL guides LLMs to produce embeddings through a series of carefully designed prompts that address multiple representational aspects. Our comprehensive experiments demonstrate that embeddings averaged from various meta-tasks yield competitive performance on Semantic Textual Similarity (STS) benchmarks and excel in downstream tasks, surpassing contrastive-trained models. Our findings suggest a new scaling law for embedding generation, offering a versatile, resource-efficient approach for embedding extraction across diverse sentence-centric scenarios",
    "checked": false,
    "id": "b620dae5446609b52e0f6d6f10dcb8cf0892bb1d",
    "semantic_title": "meta-task prompting elicits embeddings from large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ljMCGFvEUd": {
    "title": "Improving Explicit Spatial Relationships in Text-to-Image Generation through an Automatically Derived Dataset",
    "volume": "review",
    "abstract": "Existing work has observed that current text-to-image systems do not accurately reflect explicit spatial relations between objects such as 'left of or 'below'. We hypothesize that this is because explicit spatial relations rarely appear in the image captions used to train these models. We propose an automatic method that, given existing images, generates synthetic captions that contain 14 explicit spatial relations. We introduce the Spatial Relation for Generation (SR4G) dataset, which contains 9.9 millions image-caption pairs for training, and more than 60 thousand captions for evaluation. In order to test generalization we also provide an 'unseen' split, where the set of objects in the train and test captions are disjoint. SR4G is the first dataset that can be used to spatially fine-tune text-to-image systems. We show that fine-tuning two different Stable Diffusion models (denoted as SD$_{SR4G}$) yields up to 9 points improvements in the VISOR metric. The improvement holds in the 'unseen' split, showing that SD$_{SR4G}$ is able to generalize to unseen objects. SD$_{SR4G}$ improves the state-of-the-art with fewer parameters, and avoids complex architectures. Our analysis shows that improvement is consistent for all relations. The dataset and the code are publicly available",
    "checked": true,
    "id": "8408419e8263fa08c8515948f14b58e64bfef609",
    "semantic_title": "improving explicit spatial relationships in text-to-image generation through an automatically derived dataset",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MCHKJoseZf": {
    "title": "TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models",
    "volume": "review",
    "abstract": "Understanding time is a pivotal aspect of human cognition, essential for fully appreciating the complexities of the world. Previous studies typically focus on specific aspects of time, lacking a comprehensive temporal reasoning benchmark. To address this, we propose TimeBench, a comprehensive hierarchical temporal reasoning benchmark that covers a broad spectrum of temporal reasoning phenomena. TimeBench provides a thorough evaluation for investigating the temporal reasoning capabilities of large language models. We conduct extensive experiments on GPT-4, LLaMA2, and other popular LLMs under various settings. Our experimental results indicate a significant performance gap between the state-of-the-art LLMs and humans, highlighting that there is still a considerable distance to cover in temporal reasoning. LLMs exhibit capability discrepancies across different reasoning tasks. Furthermore, we thoroughly analyze the impact of multiple aspects on temporal reasoning and emphasize the associated challenges. We aspire for TimeBench to serve as a comprehensive benchmark, fostering research in temporal reasoning. Code and data will be released",
    "checked": true,
    "id": "f37d1ef3c4fd85f608439d239306a3b3302e3add",
    "semantic_title": "timebench: a comprehensive evaluation of temporal reasoning abilities in large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=BfBnR8UU5A": {
    "title": "SimuCourt: Building Judicial Decision-Making Agents with Real-world Judgement Documents",
    "volume": "review",
    "abstract": "With the development of deep learning, natural language processing technology has effectively improved the efficiency of various aspects of the traditional judicial industry. However, most current efforts focus solely on individual judicial stage, overlooking cross-stage collaboration. As the autonomous agents powered by large language models are becoming increasingly smart and able to make complex decisions in real-world settings, offering new insights for judicial intelligence. In this paper, (1) we introduce SimuCourt, a judicial benchmark that encompasses 420 judgment documents, spanning the three most common types of judicial cases, and a novel task Judicial Decision Making to evaluate the judicial analysis and decision-making power of agents. To support this task, we construct a large-scale judicial knowledge base, JudicialKB, with multiple legal knowledge. (2) we propose a novel multi-agent framework, AgentsCourt. Our framework follows the real-world classic court trial process, consisting of court debate simulation, legal information retrieval and judgement refinement to simulate the decision making of judge. (3) we perform extensive experiments, the results demonstrate that, compared to the existing advanced methods, our framework outperforms the existing advanced methods in various aspects, especially in generating legal grounds, where our system achieves significant improvements of 8.6% and 9.1% F1 score in the first and second instance settings, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XDohoRg1mK": {
    "title": "DUAL-REFLECT: Enhancing Large Language Models for Reflective Translation through Dual Learning Feedback Mechanisms",
    "volume": "review",
    "abstract": "Recently, large language models (LLMs) enhanced by self-reflection have achieved promising performance on machine translation. The key idea is guiding LLMs to generate translation with human-like feedback. However, existing self-reflection methods lack effective feedback information, limiting the translation performance. To address this, we introduce a DUAL-REFLECT framework, leveraging the dual learning of translation tasks to provide effective feedback, thereby enhancing the models' self-reflective abilities and improving translation performance. The application of this method across various translation tasks has proven its effectiveness in improving translation accuracy and eliminating ambiguities, especially in translation tasks with low-resource language pairs",
    "checked": true,
    "id": "a8468c721ce668e6dcf82c51fe8eb4d4d8d2de4a",
    "semantic_title": "dual-reflect: enhancing large language models for reflective translation through dual learning feedback mechanisms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=71mxDL1SD0t": {
    "title": "TimeToM: Temporal Space is the Key to Unlocking the Door of Large Language Models' Theory-of-Mind",
    "volume": "review",
    "abstract": "Theory of Mind (ToM)â€”the cognitive ability to reason about mental states of ourselves and others, is the foundation of social interaction. Although ToM comes naturally to humans, it poses a significant challenge to even the most advanced Large Language Models (LLMs). Due to the complex logical chains in ToM reasoning, especially in higher-order ToM questions, simply utilizing reasoning methods like Chain of Thought (CoT) will not improve the ToM capabilities of LLMs. We present \\textsc{TimeToM}\\xspace, which constructs a temporal space and uses it as the foundation to improve the ToM capabilities of LLMs in multiple scenarios. Specifically, within the temporal space, we construct Temporal Belief State Chain (TBSC) for each character and inspired by the cognition perspective of the social world model, we divide TBSC into self-world beliefs and social world beliefs, aligning with first-order ToM (first-order beliefs) and higher-order ToM (higher-order beliefs) questions, respectively. Moreover, we design a novel tool-belief solver that, by considering belief communication between characters in temporal space, can transform a character's higher-order beliefs into another character's first-order beliefs under belief communication period",
    "checked": true,
    "id": "8c9c05f40819c34c713efea897c141d718dc12e7",
    "semantic_title": "timetom: temporal space is the key to unlocking the door of large language models' theory-of-mind",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zuQLrT2EzZ": {
    "title": "Why Don't Prompt-Based Fairness Metrics Correlate?",
    "volume": "review",
    "abstract": "The widespread use of large language models has brought up essential questions about the potential biases these models might learn. This led to the development of several metrics aimed at evaluating and mitigating these biases. In this paper, we first demonstrate that prompt-based fairness metrics exhibit poor agreement, as measured by correlation, raising important questions about the reliability of fairness assessment using prompts. Then, we outline six relevant reasons why such a low correlation is observed across existing metrics. Based on these insights, we propose a method called Correlated Fairness Output (CAIRO) to enhance the correlation between fairness metrics. CAIRO augments the original prompts of a given fairness metric by using several pre-trained language models and then selects the combination of the augmented prompts that achieves the highest correlation across metrics. We show a significant improvement in Pearson correlation from 0.3 and 0.18 to 0.90 and 0.98 across metrics for gender and religion biases, respectively",
    "checked": true,
    "id": "a30b027ad932b48928ab8dd036d414e3a2a82399",
    "semantic_title": "why don't prompt-based fairness metrics correlate?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QrwPeCLGII": {
    "title": "Semiparametric Token-Sequence Co-Supervision",
    "volume": "review",
    "abstract": "In this work, we introduce a semi-parametric token-sequence co-supervision training method. It trains a language model by simultaneously leveraging supervision from the traditional next token prediction loss which is calculated over the parametric token embedding space and the next sequence prediction loss which is calculated over the nonparametric sequence embedding space. The nonparametric sequence embedding space is constructed by a separate language model tasked to condense an input text into a single representative embedding. Our experiments demonstrate that a model trained via both supervisions consistently surpasses models trained via each supervision independently. Analysis suggests that this dual supervision encourages a broader generalization capability across the model. Especially, the robustness of parametric token space which is established during the pretraining step tends to effectively enhance the stability of nonparametric sequence embedding space, a new space established by another language model. We will publicly release our model and code in URL",
    "checked": true,
    "id": "157582be6cf2e4d9709ae8f2b2f58e5fc231a556",
    "semantic_title": "semiparametric token-sequence co-supervision",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GnCvG2P1-8": {
    "title": "When Noises Help: Improve Text-Image Multimodal Contrastive Learning with Stochastic Label Augmentations",
    "volume": "review",
    "abstract": "Contrastive learning~(CL) has been widely used for self-supervised representation learning in text-image multimodal representation learning. However, there are two setbacks in the SOTA contrastive learning framework. One lies in the design of contrastive learning, where the model aims to pull together positive pairs and push away negative pairs. For one image, CL only considers one unique text as its positive sample, and treat all remaining text data as negative samples. Such design inevitably brings in learning bias towards overfitting into specific data pairs. Another setback comes from the web-crawled datasets that are commonly used in CL such as Conceptual Caption, YFCC and LAION. These datasets brings benefit due to its large size, yet contain significant noisy or vague labels. In this paper, we examine how augmenting the ground-truth labels with randomness can bring significant improvements in text-image multimodal contrastive learning. Through the simple addition of noise to ground-truth labels, we observe substantial improvements in model performance and robustness, requiring no additional computational overhead. We introduce three distinct stochastic label augmentation strategies and evaluate their effectiveness across various benchmarks, including zero-shot transfer, distribution shift, and linear probing tasks. Furthermore, we conduct comprehensive experiments involving different model architectures and noise rates, demonstrating the generalizability and substantial benefits of stochastic label augmentation across diverse tasks and models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wVSR2VyMNB": {
    "title": "Pyramidal-Graded Response for Large Lanague Model on Youth Safety",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have made significant strides in man-machine interactions. However, this advancement brings the issue of dialogue safety into sharp focus. Current research on the alignment and safety of LLMs predominantly targets adult audiences, overlooking the distinct cognitive stages of human development, particularly in youth. Recognizing this gap, we build a pyramidal youth safety benchmark (PYSafety), the largest labeled to date, comprising 275,321 records of data.Based on the benchmark, we introduce a pyramidal-graded response (PGR) strategy designed to tailor safety responses, ensuring that each interaction is aligned with the specific safety needs of the user demographic.In the implementation of the PGR strategy, we propose Safety Preference Optimization (SPO), a novel approach designed to enhance the safety performance of LLMs without additional training.The evaluation of 10 leading LLMs on the PYSafety benchmark revealed that they fall short of the desired standards for youth safety.Our SPO-based PGR strategy demonstrated a significant safety enhancement in performance across a majority of LLMs, achieving an average 20\\% to 30\\% increase in the win rate compared to their original responses. This work offers a systematic approach to analyzing and enhancing LLM performance on youth safety",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2V5IzVkbXr": {
    "title": "Unveiling the Spectrum of Data Contamination in Language Model: A Survey from Detection to Remediation",
    "volume": "review",
    "abstract": "Data contamination has garnered increased attention in the era of Large language models (LLMs) due to the reliance on extensive internet-derived training corpora. The issue of training corpus overlap with evaluation benchmarksâ€”referred to as contaminationâ€”has been the focus of significant recent research. This body of work aims to identify contamination, understand its impacts, and explore mitigation strategies from diverse perspectives. However, comprehensive studies that provide a clear pathway from foundational concepts to advanced insights are lacking in this nascent field. Therefore, we present the first survey in the field of data contamination. We begin by examining the effects of data contamination across various stages and forms. We then provide a detailed analysis of current contamination detection methods, categorizing them to highlight their focus, assumptions, strengths, and limitations. We also discuss mitigation strategies, offering a clear guide for future research. This survey serves as a succinct overview of the most recent advancements in data contamination research, providing a straightforward guide for the benefit of future research endeavors",
    "checked": false,
    "id": "a663cb9b3e75d6ea9cdd3f4e06b1759f38176c79",
    "semantic_title": "unveiling the spectrum of data contamination in language models: a survey from detection to remediation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cIiyAF_gkjZ": {
    "title": "Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling",
    "volume": "review",
    "abstract": "Large language models are trained on massive scrapes of the web, which are often unstructured, noisy, and poorly phrased. Current scaling laws show that learning from such data requires an abundance of both compute and data, which grows with the size of the model being trained. This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web. In this work, we propose Web Rephrase Augmented Pre-training (WRAP) that uses an off-the-shelf instruction-tuned model prompted to paraphrase documents on the web in specific styles such as \"like Wikipedia\" or in \"question-answer format\" to jointly pre-train LLMs on real and synthetic rephrases. First, we show that using WRAP on the C4 dataset, which is naturally noisy, speeds up pre-training by ~3x. At the same pre-training compute budget, it improves perplexity by more than 10% on average across different subsets of the Pile, and improves zero-shot question answer accuracy across 13 tasks by more than 2%. Second, we investigate the impact of the re-phrasing style on the performance of the model, offering insights into how the composition of the training data can impact the performance of LLMs in OOD settings. Our gains are attributed to the fact that re-phrased synthetic data has higher utility than just real data because it (i) incorporates style diversity that closely reflects downstream evaluation style, and (ii) has higher `quality' than web-scraped data",
    "checked": true,
    "id": "2905dc5ad70b462f4f5543df3047dffadb5c0e4e",
    "semantic_title": "rephrasing the web: a recipe for compute and data-efficient language modeling",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=CvWd9UH2gR": {
    "title": "GrAFFORD: A Benchmark Dataset for Testing the Knowledge of Object Affordances of Language and Vision Models",
    "volume": "review",
    "abstract": "We investigate the knowledge of object affordances in pre-trained language models (LMs) and pre-trained Vision-Language models (VLMs).Transformers-based large pre-trained language models (PTLM) learn contextual representation from massive amounts of unlabeled text and are shown to perform impressively in downstream NLU tasks. In parallel, a growing body of literature shows that PTLMs fail inconsistently and non-intuitively, showing a lack of reasoning and grounding. To take a first step toward quantifying the effect of grounding (or lack thereof), we curate a novel and comprehensive dataset of object affordances -- GrAFFORD, characterized by 15 affordance classes. Unlike affordance datasets collected in vision and language domains, we annotate in-the-wild sentences with objects and affordances. Experimental results reveal that PTLMs exhibit limited reasoning abilities when it comes to uncommon object affordances. We also observe that pre-trained VLMs do not necessarily capture object affordances effectively. Through few-shot fine-tuning, we demonstrate improvement in affordance knowledge in PTLMs and VLMs. Our research contributes a novel dataset for language grounding tasks, and presents insights into LM capabilities, advancing the understanding of object affordances",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H8PQYJgR5u": {
    "title": "A Comparison of Image-based, Text-based and Multimodal Models in the Table Structure Recognition Task",
    "volume": "review",
    "abstract": "Table Structure Recognition (TSR) aims to convert table images into machine readable formats such as HTML.The latest approach uses image-encoder-text-decoder model, in which image encoder extracts image features and a text decoder generates HTML tokens. Furthermore, a new approach uses multimodal-encoder, in which encoder extracts textual and visual features, and outperforms other image-encoder models. However, these models have not been compared under the same conditions. Given this background, it is necessary for future development of TSR to investigate the effects of image and text features on the TSR. In this research, we constructed an encoder-decoder model and used three different encoders: image-based, text-based, and multimodal. By comparing the TSR scores, we evaluated which model performs better. Experimental results suggested that an image-based approach is the most effective",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HASQ93Is7JT": {
    "title": "SecPE: Secure Prompt Ensembling for Private and Robust Large Language Models",
    "volume": "review",
    "abstract": "With the growing popularity of LLMs among the general public users, privacy-preserving and adversarial robustness have become two pressing demands for LLM-based services, which have largely been pursued separately but rarely jointly. In this paper, to the best of our knowledge, we are among the first attempts towards robust and private LLM inference by tightly integrating two disconnected fields: private inference and prompt ensembling. The former protects users' privacy by encrypting inference data transmitted and processed by LLMs, while the latter enhances adversarial robustness by yielding an aggregated output from multiple prompted LLM responses. Although widely recognized as effective individually, private inference for prompt ensembling together entails new challenges that render the naive combination of existing techniques inefficient. To overcome the hurdles, we propose SecPE, which designs efficient fully homomorphic encryption (FHE) counterparts tailor-made for the core building blocks of prompt ensembling. We conduct extensive experiments on 14 tasks to evaluate the accuracy, robustness, and efficiency of SecPE. The results show that SecPE maintains high clean accuracy and offers better robustness at the expense of merely 2.5% efficiency overhead compared to baseline private inference methods, indicating a satisfactory ``accuracy-robustness-efficiency'' tradeoff. For the efficiency of the encrypted Argmax operation that incurs a major slowdown for prompt ensembling, SecPE is 20.8 times faster than the state-of-the-art peers, which can be of independent interest beyond this work",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vmPB0eX521r": {
    "title": "Beyond Text: Leveraging Multi-Task Learning and Cognitive Appraisal Theory for Post-Purchase Intention Analysis",
    "volume": "review",
    "abstract": "Supervised machine-learning models for predicting user behavior offer a challenging classification problem with lower average prediction performance scores than other text classification tasks. This study evaluates multi-task learning frameworks grounded in Cognitive Appraisal Theory to predict user behavior as a function of users' self-expression and psychological attributes. Our experiments show that users' language and traits improve predictions above and beyond models predicting only from text. Our findings highlight the importance of integrating psychological constructs into NLP to enhance the understanding and prediction of user actions. We close with a discussion of the implications for future applications of large language models for computational psychology",
    "checked": true,
    "id": "a15fff1b0f60d8c71231b77cc1f784a66db621ea",
    "semantic_title": "beyond text: leveraging multi-task learning and cognitive appraisal theory for post-purchase intention analysis",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=7uySifrWkjc": {
    "title": "PACIT: Unlocking the Power of Examples for Better In-Context Instruction Tuning",
    "volume": "review",
    "abstract": "Instruction tuning enhances the instruction following ability of large language models by finetuning with supervised instruction data. Previous work proposes in-context instruction tuning (ICIT) where specific positive or negative examples are incorporated into the prompt for better performance. In this work, we propose PACIT, a simple and effective in-context instruction tuning method, inspired by the pedagogical concept of desirable difficulty. The PACIT method unlocks the power of examples by encouraging the model to actively learn to grasp the distinctions between the positive and negative examples instead of merely reading. The model is expected to first verify the correctness of the provided example according to the task description, which is then set as the condition for generating a better response to the task instance. Our extensive experiments prove the effectiveness of PACIT, outperforming ICIT baseline on both in-domain and out-domain tasks up to 9.16 and 3.14 average ROUGE-L scores, respectively. Moreover, PACIT can notably enhance the performance of instruction tuning even when all positive and negative examples are generated with a self-instruct method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b6nN8YVOy6": {
    "title": "LaCo: Large Language Model Pruning via Layer Collapse",
    "volume": "review",
    "abstract": "Large language models (LLMs) based on transformer are witnessing a notable trend of size expansion, which brings considerable costs to both model training and inference. However, existing methods such as model quantization, knowledge distillation, and model pruning are constrained by various issues, including hardware support limitations, the need for extensive training, and alterations to the internal structure of the model. In this paper, we propose a concise layer-wise pruning method called \\textit{Layer Collapse (LaCo)}, in which rear model layers collapse into a prior layer, enabling a rapid reduction in model size while preserving the model structure. Comprehensive experiments show that our method maintains an average task performance of over 80\\% at pruning ratios of 25-30\\%, significantly outperforming existing state-of-the-art structured pruning methods. We also conduct post-training experiments to confirm that the proposed pruning method effectively inherits the parameters of the original model. Finally, we discuss our motivation from the perspective of layer-wise similarity and evaluate the performance of the pruned LLMs across various pruning ratios",
    "checked": true,
    "id": "7d00e7337fbbccd0208361b4c204c75239a96521",
    "semantic_title": "laco: large language model pruning via layer collapse",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=8aAG4PVTtZ": {
    "title": "EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models",
    "volume": "review",
    "abstract": "We introduce EXAMS-V, a new challenging multi-discipline multimodal multilingual exam benchmark for evaluating vision language models. It consists of 20,932 multiple-choice questions across 20 school disciplines covering natural science, social science, and other miscellaneous studies, e.g., religion, fine arts, business, etc. EXAMS-V includes a variety of multimodal features such as text, images, tables, figures, diagrams, maps, scientific symbols, and equations. The questions come in 11 languages from 7 language families. Unlike existing benchmarks, EXAMS-V is uniquely curated by gathering school exam questions from various countries, with a variety of education systems. This distinctive approach calls for intricate reasoning across diverse languages and relies on region-specific knowledge. Solving the problems in the dataset requires advanced perception and joint reasoning over the text and the visual content in the image. Our evaluation results demonstrate that this is a challenging dataset, which is difficult even for advanced vision--text models such as GPT-4V and Gemini; this underscores the inherent complexity of the dataset and its significance as a future benchmark",
    "checked": true,
    "id": "60db77acc65d178221afe36f85cfa3d4003017c3",
    "semantic_title": "exams-v: a multi-discipline multilingual multimodal exam benchmark for evaluating vision language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=BxRQPO6aDJt": {
    "title": "Explicating the Implicit: Argument Detection Beyond Sentence Boundaries",
    "volume": "review",
    "abstract": "Detecting semantic arguments of a predicate word has been conventionally modeled as a sentence-level task. The typical reader, however, perfectly interprets predicate-argument relations in a much wider context than just the sentence where the predicate was evoked.In this work, we reformulate the problem of argument detection through textual entailment to capture semantic relations across sentence boundaries.We propose a method that tests whether some semantic relation can be inferred from a full passage by first encoding it into a simple and standalone proposition and then testing for entailment against the passage.Our method does not require direct supervision, which is generally absent due to dataset scarcity, but instead builds on existing NLI and sentence-level SRL resources. Such a method can potentially explicate pragmatically understood relations into a set of explicit sentences. We demonstrate it on a recent document-level benchmark, outperforming some supervised methods and contemporary language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PlVNdowo5H": {
    "title": "Common 7B LLMs Have Already Possessed Strong Math Capabilities",
    "volume": "review",
    "abstract": "Mathematical capabilities were previously believed to emerge in common language models only at a very large scale or require extensive math-related pre-training. This paper shows that the LLaMA-2 7B model with common pre-training already exhibits strong mathematical abilities, as evidenced by its impressive accuracy of 97.7\\% and 72.0\\% on the GSM8K and MATH benchmarks, respectively, when selecting the best response from 256 random generations. The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities. Notably, the accuracy for the first answer drops to 49.5\\% and 7.9\\% on the GSM8K and MATH benchmarks, respectively. We find that simply scaling up the SFT data can significantly enhance the reliability of generating correct answers. However, the potential for extensive scaling is constrained by the scarcity of publicly available math questions. To overcome this limitation, we employ synthetic data, which proves to be nearly as effective as real data and shows no clear saturation when scaled up to approximately one million samples. This straightforward approach achieves an accuracy of 82.6\\% on GSM8K and 40.6\\% on MATH using LLaMA-2 7B models, surpassing previous models by 16.2\\% and 20.8\\%, respectively. We also provide insights into scaling behaviors across different reasoning complexities and error types",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C3f6BqA9Jg": {
    "title": "Ranking Loss based Knowledge Distillation for Large Language Models",
    "volume": "review",
    "abstract": "Knowledge distillation (KD) is an effective method for transferring the formidable capabilities of large language models (LLMs) to smaller ones. Forward and reverse Kullback-Leibler divergences are commonly used losses in KD for LLMs. However, they usually make the student model learn distributions that are excessively smooth or sharp, hindering its ability to effectively capture the multi-modal prediction distribution of LLMs. To alleviate this problem, we propose Ranking Loss based Knowledge Distillation (RLKD), which encourages the consistency on the top-k prediction rankings between the teacher and student models. To enhance RLKD, we further employ joint sampling of the teacher and student models to dynamically generate hard samples which are not well mastered by the student model. Experimental results demonstrate that our method enables the student model to better learn the multi-modal prediction distribution of the teacher model, leading to a significant performance improvement in reasoning task",
    "checked": false,
    "id": "191bd8f008f80883bba1fa38908e2c836a5f7bbe",
    "semantic_title": "reaugkd: retrieval-augmented knowledge distillation for pre-trained language models",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=EhGjBlslpf": {
    "title": "ChapterCR: A Large-Scale Chapter-Level Coreference Resolution",
    "volume": "review",
    "abstract": "Coreference Resolution aims to identify mentions that refer to one another in documents. Existing coreference resolution datasets are either small in size or short in coreference chains. To address the issue, we propose ChapterCR, a large-scale chapter-level coreference resolution dataset. In ChapterCR, the coreference chains are longer and there are more distractors between the mention and the right entity, which makes it more challenging. Experiments on ChapterCR show that there is still a large gap between the state-of-art baselines and human beings. Even ChatGPT does not perform very well in ChapterCR, with the F1 score of 74.0\\% in ChapterCR-en and 58.8\\% in ChapterCR-zh, showing that ChapterCR is still an open problem",
    "checked": false,
    "id": "fd0c488487a5f12c8514191832d3f5f43712e512",
    "semantic_title": "chaptercr: a large-scale chapter-level coreference resolution benchmark",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ue5F1cAnZ2": {
    "title": "Navigating Hallucinations for Reasoning of Unintentional Activities",
    "volume": "review",
    "abstract": "In this work we present a novel task of understanding unintentional human activities in videos. We formalize this problem as a reasoning task under zero-shot scenario, where given a video of an unintentional activity we want to know why it transitioned from intentional to unintentional. We first evaluate the effectiveness of current state-of-the-art Large Multimodal Models on this reasoning task and observe that they suffer from hallucination. We further propose a novel prompting technique, termed as Dream of Thoughts (DoT), which allows the model to navigate through hallucinated thoughts to achieve better reasoning. To evaluate the performance on this task, we also introduce three different specialized metrics designed to quantify the models reasoning capability. We perform our experiments on two different datasets, OOPs and UCF-Crime, and our findings show that DOT prompting technique is able to outperform standard prompting, while minimizing hallucinations",
    "checked": true,
    "id": "f12b2e22ed2530e2b87054e5735208449616bf3c",
    "semantic_title": "navigating hallucinations for reasoning of unintentional activities",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Jh_MNqo_Mx": {
    "title": "Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models",
    "volume": "review",
    "abstract": "Although Large Language Models (LLMs) demonstrate remarkable ability in processing and generating human-like text, they do have limitations when it comes to comprehending and expressing world knowledge that extends beyond the boundaries of natural language(e.g., chemical molecular formula). Injecting a collection of symbolic data directly into the training of LLMs can be problematic, as it disregards the synergies among different symbolic families and overlooks the need for a balanced mixture of natural and symbolic data. In this work, we tackle these challenges from both a data and framework perspective and introduce Symbol-LLM series models. First, we curated a data collection consisting of 34 tasks and incorporating approximately 20 distinct symbolic families, intending to capture the interrelations and foster synergies between symbols. Then, a two-stage tuning framework succeeds in injecting symbolic knowledge without loss of the generality ability. Extensive experiments on both symbol- and NL-centric tasks demonstrate the balanced and superior performances of Symbol-LLM series models",
    "checked": true,
    "id": "bf11f01929afed0ad3ccfe1b5e0fd34d90ef2b4f",
    "semantic_title": "symbol-llm: towards foundational symbol-centric interface for large language models",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=hWj2TXhovj": {
    "title": "From Instructions to Basic Human Values: A Survey of Alignment Goals for Big Models",
    "volume": "review",
    "abstract": "As big models demonstrate remarkable performance across diverse tasks, concerns about their potential risks and social harms are raised. Extensive efforts have been made towards aligning big models with humans to ensure their responsible development and human profits maximization. Nevertheless, the basic question \"what to align with\" remains largely unexplored. It is critical to precisely define the objectives for big models to pursue, and aligning with inappropriate goals could cause disaster, e.g., chatbots promote abusive or biased contents when only following user instructions to interact freely. This paper conducts a comprehensive survey of different alignment goals, tracing their evolution paths to identify the most appropriate goal for big models. Specifically, we categorize existing alignment goals into four primary levels: human instructions, human preferences, value principles and basic values, revealing a learning process that transforms from basic abilities to higher value concepts. For each goal, we further elaborate its definition, how to represent it and how to evaluate it. Posing basic values as a promising goal, we discuss challenges and future research directions",
    "checked": false,
    "id": "f8b90d640158f61c4553518a8554a73b540e07e7",
    "semantic_title": "from instructions to intrinsic human values - a survey of alignment goals for big models",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=F6fZ-J9OJF": {
    "title": "What Is Missing in Multilingual Visual Reasoning and How to Fix It",
    "volume": "review",
    "abstract": "NLP models today strive for inclusivity by supporting multiple languages and modalities, improving accessibility for diverse users. In this paper, we evaluate their multilingual, multimodal capabilities by testing on a visual reasoning task. We observe that proprietary systems like GPT-4V(ision) obtain the best performance on this task thus far, but open models lag in comparison. Surprisingly, GPT-4V exhibits similar performance between English and other languages, indicating the potential for equitable system development across languages. Our analysis on model failures reveals three key aspects that make this task challenging: multilinguality, complex reasoning, and multimodality. To address these challenges, we propose three targeted interventions including a translate-test approach to tackle multilinguality, a visual programming approach to break down complex reasoning, and a novel method that leverages image captioning to address multimodality. Our interventions achieve the best open performance on the task in a zero-shot setting, boosting open model LLaVA by 13.4%, while also minorly improving GPT-4V's performance",
    "checked": true,
    "id": "c909718ec002e05e4ac77da2bfbec5ac88f18c3e",
    "semantic_title": "what is missing in multilingual visual reasoning and how to fix it",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=E24sXnoajy": {
    "title": "CoCoLoFa: News Comment Sections with Common Logical Fallacies",
    "volume": "review",
    "abstract": "Detecting logical fallacies in texts could improve online discussion quality by helping users spot argument flaws and construct better arguments. However, automatically identifying logical fallacies in the wild is not easy. Fallacies are often buried inside arguments that sound convincing; over 100 types of logical fallacies exist. Building large labeled datasets needed for developing automatic fallacy detection models can be expensive. This paper introduces CoCoLoFa, the largest logical fallacy dataset, containing 5,772 comments for 647 news articles, with each comment labeled for fallacy presence and type. To collect data, we first specified a fallacy type (eg, slippery slope) and a news article to crowd workers, then asked them to write comments that embody the fallacy in response to the article. We built an LLM-powered assistant in the interface to help workers draft and refine comments. Experts rated the writing quality and labeling validity of CoCoLoFa as high and reliable. Models trained on CoCoLoFa achieved the highest fallacy detection performance (F1=0.65) on real-world news comments from the New York Times, surpassing those trained on other datasets and even GPT-4",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UIFbSm0a86o": {
    "title": "A Non-Parametric Approach to Contextual Distractor Generation: Leveraging Error by Machine Reading Comprehension for Enhanced Distractor Generation",
    "volume": "review",
    "abstract": "In this work, we introduce a retrieval-augmented approach for generating distractors, leveraging hard negative passages from the open-domain question answering (ODQA) task. Unlike traditional methods reliant on dataset fine-tuning, our approach directly references teaching materials, enhancing practical application for educators crafting multiple-choice questions and allowing for seamless domain transitions without additional training. We adopt large language models (LLMs) for evaluation, tapping into their advanced comprehension capabilities to deliver a holistic assessment of distractor quality. Our experiments reveal superior performance over existing techniques, indicating a significant advance in the field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y7kyBZCwe4": {
    "title": "Robust Dense Retrieval with Conterfactual Learning",
    "volume": "review",
    "abstract": "Efficiently retrieving a concise set of candidates from an extensive document corpus remains a pivotal challenge in Information Retrieval (IR). Dense retrieval models have gained prominence for their superior performance. However, criticisms surrounding their vulnerability to adversarial attacks necessitate further exploration.In response to these challenges, we advocate for enhancing the sensitivity of dense retrieval models to fine-grained relevance matching signals. Motivated by causality and counterfactual analysis, we propose a series of counterfactual regularization methods. These methods aim to improve the robustness and sensitivity of dense retrieval models concerning fine-grained relevance matching. We first introduce a counterfactual passage extraction method, identifying the key passages that can influence relevance.Subsequent unsupervised learning tasks, based on these counterfactual passages, serve to regularize the model's learning process to improve the robustness and sensitivity.Experimental results showcase the efficacy of our approach. Our method demonstrates the ability to learn key passages without reliance on the passage-level relevance annotations. Moreover, the regularized dense retrieval models exhibit heightened robustness against adversarial attacks, surpassing the state-of-the-art anti-attack methods. Our work contributes to bridging the vulnerability gaps in dense retrieval models, paving the way for their improved reliability and applicability in IR systems",
    "checked": false,
    "id": "8875452c806699f5cfbf4d5b52f22e1b8618204f",
    "semantic_title": "towards robust dense retrieval via local ranking alignment",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=fsalEM9JOZX": {
    "title": "MultiCAT: Multimodal Communication Annotations for Teams",
    "volume": "review",
    "abstract": "Successful teamwork requires team members to understand each other and communicate effectively, managing multiple linguistic and paralinguistic tasks at once. Because of the potential for interrelatedness of these tasks, it is important to have the ability to make multiple types of predictions on the same dataset. Here, we introduce Multimodal Communication Annotations for Teams (MultiCAT), a speech- and text-based dataset consisting of audio recordings, automated and hand-corrected transcriptions. MultiCAT builds upon collected data for teams working collaboratively to save victims in a simulated search and rescue mission, and consists of annotations and benchmark results for the following tasks: (1) dialog act classification, (2) adjacency pair detection, (3) sentiment and emotion recognition, (4) closed-loop communication detection, and (5) phonetic entrainment detection. We posit that additional work on these tasks and their intersection will further improve understanding of team communication and its relation to team performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FV5TMB0bmR1": {
    "title": "Flee the Flaw: Annotating the Underlying Logic of Fallacious Arguments Through Templates and Slot-filling",
    "volume": "review",
    "abstract": "Prior research in computational argumentation has mainly focused on scoring the quality of arguments, with less attention on explicating logical errors. In this work, we introduce four sets of explainable templates for common informal logical fallacies designed to explicate a fallacy's implicit logic. Using our templates, we conduct an annotation study on top of 400 fallacious arguments taken from LOGIC dataset and achieve a high agreement score (Krippendorf's $\\alpha$ of 0.54) and reasonable coverage (0.829). Finally, we conduct an experiment for detecting the structure of fallacies and discover that state-of-the-art language models struggle with detecting fallacy templates (0.31 $F_1$). To facilitate research on fallacies, we make our dataset publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sK9nHdhVwM": {
    "title": "Extracting and Encoding: Leveraging Large Language Models and Medical Knowledge to Enhance Radiological Text Representation",
    "volume": "review",
    "abstract": "Recent advancements in representation learning face challenges in specialized domains like medicine, where acquiring expert annotations for texts and images is burdensome due to the limited availability of medical professionals. To address this, we propose a two-stage framework aimed at extracting factual statements to improve encoder representations for various downstream tasks.First, we introduce a \\textit{Fact} \\textit{Extractor} that leverages the effectiveness of large language models in the medical domain to obtain factual statements from well-curated domain-specific datasets. In the second stage, we present a \\textit{Fact} \\textit{Encoder} (CXRFE) based on a BERT model that we fine-tune with objective functions designed to enhance its representations using the gathered data. Additionally, our framework introduces a novel embedding-based metric (CXRFEScore) for evaluating chest X-ray text generation systems, utilizing the two stages of our framework.Through extensive evaluations, we demonstrate that our fact extractor and encoder outperform multiple state-of-the-art methods across tasks such as sentence ranking, natural language inference, and label extraction from radiology reports.Furthermore, we show that our metric is robust, outperforming existing metrics commonly used in the report generation literature",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j6BUpFegYj": {
    "title": "LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs",
    "volume": "review",
    "abstract": "Long video understanding is a significant and ongoing challenge in the intersection of multimedia and artificial intelligence. Employing large language models (LLMs) for comprehending video becomes an emerging and promising method. However, this approach incurs high computational costs due to the extensive array of video tokens, experiences reduced visual clarity as a consequence of token aggregation, and confronts challenges arising from irrelevant visual tokens while answering video-related questions. To alleviate these issues, we present an Interactive Visual Adapter (IVA) within LLMs, designed to enhance interaction with fine-grained visual elements. Specifically, we first transform long videos into temporal video tokens via leveraging a visual encoder alongside a pretrained causal transformer, then feed them into LLMs with the video instructions. Subsequently, we integrated IVA, which contains a lightweight temporal frame selector and a spatial feature interactor, within the internal blocks of LLMs to capture instruction-aware and fine-grained visual signals. Consequently, the proposed video-LLM facilitates a comprehensive understanding of long video content through appropriate long video modeling and precise visual interactions. We conducted extensive experiments on nine video understanding benchmarks and experimental results show that our interactive visual adapter significantly improves the performance of video LLMs on long video QA tasks. Ablation studies further verify the effectiveness of IVA in long and short video understandings",
    "checked": true,
    "id": "c44393114047e0f9c32e45b381970d50ed503260",
    "semantic_title": "llms meet long video: advancing long video comprehension with an interactive visual adapter in llms",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=FPJizLRs1I": {
    "title": "LLM with Relation Classifier for Document-Level Relation Extraction",
    "volume": "review",
    "abstract": "Large language models (LLMs) create a new paradigm for natural language processing. Despite their advancement, LLM-based methods still lag behind traditional approaches in document-level relation extraction (DocRE), a critical task for understanding complex entity relations. This paper investigates the causes of this performance gap, identifying the dispersion of attention by LLMs due to entity pairs without relations as a primary factor. To address this challenge, we introduce a novel classifier-LLM approach to DocRE. The proposed methodâ€”LMRCâ€”begins with a classifier specifically designed to filter and select entity pair candidates exhibiting potential relations, and thereby feeds them to LLM for the extraction of final relations. This method ensures that during inference, the LLM's focus is directed primarily at entity pairs with relations. Experiments on DocRE benchmark reveal that our method significantly outperforms recent LLM-based DocRE models and achieves competitive performance with several leading traditional DocRE models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NVhRn_B29i": {
    "title": "See It All: Contextualized Late Aggregation for 3D Dense Captioning",
    "volume": "review",
    "abstract": "3D dense captioning is a task to localize objects in a 3D scene and generate descriptive sentences for each object. Recent approaches in 3D dense captioning have adopted transformer encoder-decoder frameworks from object detection to build an end-to-end pipeline without hand-crafted components. However, these approaches struggle with contradicting objectives where a single query attention has to simultaneously view both the tightly localized object regions and contextual environment. To overcome this challenge, we introduce SIA (See-It-All), a transformer pipeline that engages in 3D dense captioning with a novel paradigm called late aggregation. SIA simultaneously decodes two sets of queriesâ€”context query and instance queryâ€”where the instance query focuses on localization and object attribute descriptions while the context query versatilely captures the region-of-interest of relationships between multiple objects or with the global scene, then aggregated afterwards (i.e., late aggregation) via simple distance-based measures. To further enhance the quality of contextualized caption generation, we design a novel aggregator (i.e., TGI-Aggregator) to generate a fully informed caption based on the surrounding context, the global environment, and object instances. Extensive experiments on two of the most widely-used 3D dense captioning datasets (ScanRefer and Nr3D) demonstrate that our proposed method achieves a significant improvement over prior methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lnESlBHWKnV": {
    "title": "Self-Aware Speak Mechanism for Multi-Agent Communication",
    "volume": "review",
    "abstract": "To enhance work efficiency, researchers have increasingly turned to multi-agent frameworks that facilitate collaborative task completion by multiple agents. These frameworks necessitate sophisticated communication mechanisms to develop and execute effective strategies. Existing frameworks often rely on manually designed rules to determine the sequence of communication among agents. These rules are often suboptimal, requiring considerable human efforts for customization across diverse tasks, thereby limiting scalability. To address this issue, we propose \"self-aware speaking\" mechanism, designed to empower agent groups with the autonomy to dynamically determine their speaking order. This mechanism streamlines the management of communications within both centralized and decentralized multi-agent systems. Further, Empirical evaluations show that it outperforms rule-based mechanism by 5.2% on average, and is more scalable, underscoring its potential in multi-agent frameworks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I95DtJgvGZ": {
    "title": "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) are powerful zero-shot assessors and are increasingly used in real-world situations such as for written exams or benchmarking systems. Despite this, no existing work has analyzed the vulnerability of judge-LLMs against adversaries attempting to manipulate outputs. This work presents the first study on the adversarial robustness of assessment LLMs, where we search for short universal phrases that when appended to texts can deceive LLMs to provide high assessment scores. Experiments on SummEval and TopicalChat demonstrate that both LLM-scoring and pairwise LLM-comparative assessment are vulnerable to simple concatenation attacks, where in particular LLM-scoring is very susceptible and can yield maximum assessment scores irrespective of the input text quality. Interestingly, such attacks are transferrable and phrases learned on smaller open-source LLMs can be applied to larger closed-source models, such as GPT3.5. This highlights the pervasive nature of the adversarial vulnerabilities across different judge-LLM sizes, families and methods. Our findings raise significant concerns on the reliability of LLMs-as-a-judge methods, and underscore the importance of addressing vulnerabilities in LLM assessment methods before deployment in high-stakes real-world scenarios",
    "checked": true,
    "id": "09812e529903ff67c5fc5f1dcb2b3586eb3ffd23",
    "semantic_title": "is llm-as-a-judge robust? investigating universal adversarial attacks on zero-shot llm assessment",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=Olq9lsv9E2": {
    "title": "${\\mathcal X}$FT: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled Mixture-of-Experts",
    "volume": "review",
    "abstract": "We introduce ${\\mathcal X}$FT, a simple yet powerful training scheme, by simply merging upcycled Mixture-of-Experts (MoE) to unleash the performance limit of instruction-tuned code Large Language Models (LLMs). While vanilla sparse upcycling fails to improve instruction tuning, ${\\mathcal X}$FT introduces a shared expert mechanism with a novel routing weight normalization strategy into sparse upcycling, which significantly boosts instruction tuning. After fine-tuning the upcycled MoE model, ${\\mathcal X}$FT introduces a learnable model merging mechanism to compile the upcycled MoE model back to a dense model, achieving upcycled MoE-level performance with only dense-model compute. By applying ${\\mathcal X}$FT to a 1.3B model, we create a new state-of-the-art small LLM (<3B) for code with 67.1 and 64.6 pass@1 on HumanEval and HumanEval+ respectively. Furthermore, with the same data and model architecture, ${\\mathcal X}$FT improves supervised fine-tuning (SFT) by 13% on HumanEval+, along with consistent improvements from 2% to 13% on MBPP+, MultiPL-E, and DS-1000, demonstrating its generalized effectiveness. ${\\mathcal X}$FT is fully orthogonal to existing techniques such as Evol-Instruct and OSS-Instruct, opening a new dimension for improving code instruction tuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XBxEaLT3QlI": {
    "title": "Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing",
    "volume": "review",
    "abstract": "In visual speech processing, context modeling capability is one of the most important requirements due to the ambiguous nature of lip movements. For example, homophenes, words that share identical lip movements but produce different sounds, can be distinguished by considering the context. In this paper, we propose a novel framework, namely Visual Speech Processing incorporated with LLMs (VSP-LLM), to maximize the context modeling ability by bringing the overwhelming power of LLMs. Specifically, VSP-LLM is designed to perform multi-tasks of visual speech recognition and translation, where the given instructions control the type of task. The input video is mapped to the input latent space of a LLM by employing a self-supervised visual speech model. Focused on the fact that there is redundant information in input frames, we propose a novel deduplication method that reduces the embedded visual features by employing visual speech units. Through the proposed deduplication and Low Rank Adaptors (LoRA), VSP-LLM can be trained in a computationally efficient manner. In the translation dataset, the MuAViC benchmark, we demonstrate that VSP-LLM can more effectively recognize and translate lip movements with just 15 hours of labeled data, compared to the recent translation model trained with 433 hours of labeld data",
    "checked": true,
    "id": "187de1107f760e10c52d29e9cd720fa63cb01ea8",
    "semantic_title": "where visual speech meets language: vsp-llm framework for efficient and context-aware visual speech processing",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=OoIY76vdGXH": {
    "title": "Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs",
    "volume": "review",
    "abstract": "With the advent of large language models (LLM), the line between human-crafted and machine-generated texts has become increasingly blurred. This paper delves into the inquiry of identifying discernible and unique linguistic properties in texts that were written by humans, particularly uncovering the underlying discourse structures of texts beyond their surface structures.Introducing a novel methodology, we leverage hierarchical parse trees and recursive hypergraphs to unveil distinctive discourse patterns in texts produced by both LLMs and humans.Empirical findings demonstrate that, although both LLMs and humans generate distinct discourse patterns influenced by specific domains, human-written texts exhibit more structural variability, reflecting the nuanced nature of human writing in different domains. Notably, incorporating hierarchical discourse features enhances binary classifiers' overall performance in distinguishing between human-written and machine-generated texts, even on out-of-distribution and paraphrased samples.This underscores the significance of incorporating hierarchical discourse features in the analysis of text patterns.The code and dataset will be available at [TBA]",
    "checked": true,
    "id": "1321cda5cb160285de0548a999b4b941156ba764",
    "semantic_title": "threads of subtlety: detecting machine-generated texts through discourse motifs",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=OThzXhuk11Q": {
    "title": "Formality Favored: Unraveling the Learning Preferences of Large Language Models on Data with Conflicting Knowledge",
    "volume": "review",
    "abstract": "Having been trained on massive pretraining data, large language models have shown excellent performance on many knowledge-intensive tasks. However, pretraining data tends to contain misleading and even conflicting information, and it is intriguing to understand how LLMs handle these noisy data during training. In this study, we systematically analyze LLMs' learning preferences for data with conflicting knowledge. We find that pretrained LLMs establish learning preferences similar to humans, i.e., preferences towards formal texts and texts with fewer spelling errors, resulting in faster learning and more favorable treatment of knowledge in data with such features when facing conflicts. This finding is generalizable across models and languages and is more evident in larger models. An in-depth analysis reveals that LLMs tend to trust data with features that signify consistency with the majority of data, and it is possible to instill new preferences and erase old ones by manipulating the degree of consistency with the majority data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Z-1FnvZu5P": {
    "title": "Triple Preferences Optimization (TPO): A Simple One-Step Combination of SFT and Preference Optimization with a Better Performance",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) excel across various tasks. However, aligning them with human demonstrations proves challenging. Prior approaches relied on Reinforcement Learning from Human Feedback (RLHF) using online RL methods like Proximal Policy Optimization (PPO). Recently, RL-free methods like Direct Preference Optimization (DPO) have emerged as appealing alternatives, offering improved stability and scalability while retaining competitive performance. However, these methods have a separate supervised fine-tuning (SFT) step for further learning and require sampling from the post-SFT model and ranking them. In this paper, we introduce Triple Preferences Optimization (TPO), a new preference learning method designed to align an LLM with three preferences without requiring a separate supervised fine-tuning step. Our TPO aims to maximize the log probability of preferred to less-preferred responses while simultaneously learning the gold standard response in a single step. To provide a comprehensive evaluation, we use HuggingFace Open LLMs Benchmarks and MT-Bench involving dialogue systems and encompassing various NLP aspects. The results indicate that TPO surpasses other alignment methods, such as DPO and SFT, in average accuracy by 1.8% and 2.5%, respectively. Notably, TPO without the SFT part exhibits superior average accuracy compared to DPO and SFT by 4% and 4.7%, respectively. Overall, TPO resolves sampling challenges and combines the SFT part with the preference optimization part into a single step and provides better performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=22VChUi-k-": {
    "title": "LLMs in Biomedical: A study on clinical Named Entity Recognition",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) demonstrate remarkable versatility in various NLP tasks but encounter distinct challenges in biomedicine due to medical language complexities and data scarcity. This paper investigates the application of LLMs in the medical domain by exploring strategies to enhance their performance for the Named-Entity Recognition (NER) task. Specifically, our study reveals the importance of meticulously designed prompts in biomedical. Strategic selection of in-context examples yields a notable improvement, showcasing $\\sim 15-20\\%$ increase in F1 score across all benchmark datasets for few-shot clinical NER. Additionally, our findings suggest that integrating external resources through prompting strategies can bridge the gap between general-purpose LLM proficiency and the specialized demands of medical NER. Leveraging a medical knowledge base, our proposed method inspired by Retrieval-Augmented Generation (RAG) can boost the F1 score of LLMs for zero-shot clinical NER. We will release the code upon publication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wfi3VeYESSi": {
    "title": "Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation",
    "volume": "review",
    "abstract": "This study investigates how Large Language Models (LLMs) leverage source and reference data in machine translation evaluation task, aiming to better understand the mechanisms behind their remarkable performance in this task.We design the controlled experiments across various input modes and model types, and employ both coarse-grained and fine-grained prompts to discern the utility of source versus reference information.We find that reference information significantly enhances the evaluation accuracy, while surprisingly, source information sometimes is counterproductive, indicating LLMs' inability to fully leverage the cross-lingual capability when evaluating translations.Further analysis of the fine-grained evaluation and fine-tuning experiments show similar results.These findings also suggest a potential research direction for LLMs that fully exploits the cross-lingual capability of LLMs to achieve better performance in machine translation evaluation tasks",
    "checked": true,
    "id": "7670d88173786e054b72c32580676fa3fed0c364",
    "semantic_title": "lost in the source language: how large language models evaluate the quality of machine translation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uVYvn9Wsh3z": {
    "title": "Integrate the Essence and Eliminate the Dross: Fine-Grained Self-Consistency for Free-Form Language Generation",
    "volume": "review",
    "abstract": "Self-consistency (SC), leveraging multiple samples from LLMs, shows significant gains on various reasoning tasks but struggles with free-form generation due to the difficulty of aggregating answers. Its variants, UCS and USC, rely on sample selection or voting mechanisms to improve output quality. These methods, however, face limitations due to their inability to fully utilize the nuanced consensus knowledge present within multiple candidate samples, often resulting in suboptimal outputs. We propose Fine-Grained Self-Consistency (FSC) to addresses these limitations by extracting and integrating segment-level commonalities from candidate samples, enhancing the performance of LLMs both in open-ended and reasoning tasks. Based on this, we present two additional strategies: candidate filtering, which enhances overall quality by identifying highly similar candidate sets, and merging, which reduces input token requirements by combining similar samples. The effectiveness of FSC is demonstrated through extensive experiments on various tasks, including summarization, code generation, and mathematical reasoning, using GPT-3.5-turbo and GPT-4. The results indicate significant improvements over baseline methods, showcasing the potential of FSC to optimize output quality by effectively synthesizing fine-grained consensus knowledge from multiple samples",
    "checked": true,
    "id": "71e3c22ec3b1522dfeef118d168ecd144121489b",
    "semantic_title": "integrate the essence and eliminate the dross: fine-grained self-consistency for free-form language generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g0_uK76mdv": {
    "title": "Dynamic Planning for Graphical User Interface Automation with LLM Agents",
    "volume": "review",
    "abstract": "The advent of large language models (LLMs) has spurred considerable interest in advancing autonomous agents, empowering them to tackle real-world tasks by perceiving distinct environments, formulating plans, and executing actions. An intriguing application of these agents is within smartphone graphical user interfaces (GUIs). Upon receiving a task goal, the agent generates step-by-step plans and engages in iterative interactions until task completion. However, it remains an open challenge how to generate effective plans to guide the action prediction. Current studies often confine themselves to static plans or lack specific plans entirely. Given that the environment evolves following action execution, the imperative is to adapt plans dynamically based on environmental feedback and action history. To address the challenge, we propose DP-Agent, a novel approach designed to cultivate dynamic planning in agents. DP-Agent involves the dynamic adjustment of planning based on feedback from the environment and interaction history. Experimental results reveal that DP-Agent exhibits superior performance, surpassing the widely adopted GPT-4V baseline by +8.81% (35.58% $\\rightarrow$ 44.39%) on the AITW benchmark dataset. Our analysis highlights the efficacy of dynamic planning in not only enhancing action prediction accuracy but also in adapting to previously unfamiliar tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3jGcDX3KpGt": {
    "title": "BIPED: Pedagogically Informed Tutoring System for ESL Education",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have a great potential to serve as readily available and cost-efficient Conversational Intelligent Tutoring Systems (CITS) for teaching L2 learners of English. Existing CITS, however, are designed to teach only simple concepts or lack the pedagogical depth necessary to address diverse learning strategies.To develop a more pedagogically informed CITS capable of teaching complex concepts, we construct a BIlingual PEDagogically-informed Tutoring Dataset (BIPED) of one-on-one, human-to-human English tutoring interactions. Through post-hoc analysis of the tutoring interactions, we come up with a lexicon of dialogue acts (34 tutor acts and 9 student acts), which we use to further annotate the collected dataset. Based on a two-step framework of first predicting the appropriate tutor act then generating the corresponding response, we implemented two CITS models using GPT-4 and SOLAR-KO, respectively. We experimentally demonstrate that the implemented models not only replicate the style of human teachers but also employ diverse and contextually appropriate pedagogical strategies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GrYNK4KxoOM": {
    "title": "Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback",
    "volume": "review",
    "abstract": "We introduce a novel approach named GELI that makes it possible to align an LLM-based dialogue agent based on global rewards, while simultaneously taking into account naturally-occurring multimodal signals. At a high level, our approach learns a local, turn-level reward model that decomposes the Global Explicit (GE) session-level annotated reward. Then, the reward model also incorporates Local Implicit (LI) multimodal reward signals to shape the decomposition of the global reward. This reward model is used to improve and adapt the long-term conversational capability of a language model. We run quantitative and qualitative human studies to evaluate the performance of our GELI approach, with results showing consistent performance boosts across conversational metrics",
    "checked": true,
    "id": "3cb1328a9d5f62cfe87ea01ca8f471e3022efc55",
    "semantic_title": "improving dialogue agents by decomposing one global explicit annotation with local implicit multimodal feedback",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1aZMXz7E58d": {
    "title": "Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment",
    "volume": "review",
    "abstract": "Despite the significant achievements of existing prompting methods such as in-context learning and chain-of-thought for large language models (LLMs), they still face challenges of various biases. Traditional debiasing methods primarily focus on the model training stage, including data augmentation-based and reweight-based approaches, with the limitations of addressing the complex biases of LLMs. To address such limitations, the causal relationship behind the prompting methods is uncovered using a structural causal model, and a novel causal prompting method based on front-door adjustment is proposed to effectively mitigate the bias of LLMs. In specific, causal intervention is implemented by designing the prompts without accessing the parameters and logits of LLMs.The chain-of-thoughts generated by LLMs are employed as the mediator variable and the causal effect between the input prompt and the output answers is calculated through front-door adjustment to mitigate model biases.Moreover, to obtain the representation of the samples precisely and estimate the causal effect more accurately, contrastive learning is used to fine-tune the encoder of the samples by aligning the space of the encoder with the LLM. Experimental results show that the proposed causal prompting approach achieves excellent performance on 3 natural language processing datasets on both open-source and closed-source LLMs",
    "checked": true,
    "id": "92a236a1ca049e186ca53c9ce12219ebca315ba1",
    "semantic_title": "causal prompting: debiasing large language model prompting based on front-door adjustment",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=IbfXU-pOjQV": {
    "title": "Guardians of the Machine Translation Meta-Evaluation Sentinel Metrics Fall In!",
    "volume": "review",
    "abstract": "Annually, the organizers of the Metrics Shared Task at the Conference on Machine Translation (WMT) conduct the meta-evaluation of Machine Translation (MT) metrics, ranking them according to their correlation with human judgments. Their results guide researchers toward enhancing the next generation of metrics and MT systems.With the recent introduction of neural metrics, the field has seen notable advancements. Nevertheless, the inherent opacity of these metrics has posed substantial challenges to the meta-evaluation process.This work highlights two issues with the meta-evaluation framework currently employed in WMT, and assesses their impact on the metrics rankings. We introduce the concept of sentinel metrics, which are designed explicitly to scrutinize the accuracy, robustness, and fairness of the meta-evaluation process. By employing sentinel metrics, we aim to validate our findings, and shed light and monitor the potential biases or inconsistencies in the rankings. We discover that the present meta-evaluation framework favors two categories of metrics: i) those explicitly trained to mimic human quality assessments, and ii) continuous metrics. Ultimately, we raise concerns regarding the evaluation capabilities of state-of-the-art metrics, highlighting that they might be basing their assessments on spurious correlations found in their training data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fX3-hfy1A9R": {
    "title": "Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations",
    "volume": "review",
    "abstract": "In spoken dialogue, even if two current turns are the same sentence, their responses might still differ when they are spoken in different styles. The spoken styles, containing paralinguistic and prosodic information, mark the most significant difference between text and speech modality. When using text-only LLMs to model spoken dialogue, text-only LLMs cannot give different responses based on the speaking style of the current turn. In this paper, we focus on enabling LLMs to \\textit{listen to} the speaking styles and respond properly. Our goal is to teach the LLM that \"\\textit{even if the sentences are identical if they are spoken in different styles, their corresponding responses might be different}\". Since there is no suitable dataset for achieving this goal, we collect a speech-to-speech dataset, \\textbf{StyleTalk}, with the following desired characteristics: when two current speeches have the same content but are spoken in different styles, their responses will be different. To teach LLMs to understand and respond properly to the speaking styles, we propose the \\textbf{Spoken-LLM} framework that can model the linguistic content and the speaking styles. We train Spoken-LLM using the StyleTalk dataset and devise a two-stage training pipeline to help the Spoken-LLM better learn the speaking styles. Based on extensive experiments, we show that Spoken-LLM outperforms text-only baselines and prior speech LLMs methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gOhrd2X7Fh": {
    "title": "Exploring Reversal Mathematical Reasoning Ability for Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) have presented remarkable capabilities in the wide range of natural language understanding and reasoning tasks. Despite their success, a few works indicate that LLMs suffer from the ''reversal curse'', in which LLMs can't employ the inverted structure ''B is A'' when they are trained based on ''A is B''. To explore the effect of the ''reversal curse'' for LLMs on complex mathematical reasoning tasks, we present two reversal datasets upon GSM8K and MathQA and verify that LLMs also struggle to solve reversal mathematical problems. We analyze the potential reason and attribute it to the insufficient modeling of the relationship between reasoning steps caused by the left-to-right objective. Consequently, based on the characteristics of multi-step reasoning, we design a novel training method to improve the general and reversal reasoning abilities. Finally, we conduct experiments on four mathematical datasets, and the results demonstrate that our method significantly improves the general reasoning capacities and alleviates the reversal problem. Our datasets and codes will be available at https://anonymous/",
    "checked": false,
    "id": "f843233f76a5dff07bfa93a71a1cf13d8aa6a94a",
    "semantic_title": "exploring length generalization in large language models",
    "citation_count": 118,
    "authors": []
  },
  "https://openreview.net/forum?id=KBWiUeWVLTM": {
    "title": "What Languages are Easy to Language-Model? A Perspective from Learning Probabilistic Regular Languages",
    "volume": "review",
    "abstract": "What can large language models learn? By definition, language models (LM) are distributions over strings. Therefore, an intuitive way of addressing the above question is to formalize it as a matter of learnability of classes of distributions over strings. While prior work in this direction focused on assessing the theoretical limits, we seek to understand the empirical learnability. Unlike prior empirical work, we evaluate LMs on their home ground---learning probability distributions over strings---rather than as classifiers of formal languages. In particular, we investigate the learnability of finite-state LMs (FSLMs). We first theoretically quantify the minimal representation size of a neural LM necessary for learning an FSLM in terms of its rank, which corresponds to the size of linear space spanned by the logits of its conditional distributions. We then empirically test the learnability of FSLMs and find that the rank is a strong predictor of learnability for both Transformers and RNNs, but the significance of other properties of the FSLM differs between Transformers and RNNs",
    "checked": true,
    "id": "883f93ce1b3347b5cd1d5fa66c9f460bf8e2a976",
    "semantic_title": "what languages are easy to language-model? a perspective from learning probabilistic regular languages",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=9AV_zM56pwj": {
    "title": "Media Framing: A typology and Survey of Computational Approaches Across Disciplines",
    "volume": "review",
    "abstract": "Framing studies how individuals and societies make sense of the world, by communicating or representing complex issues through schema of interpretation. The framing of information in the mass media influences our interpretation of facts and corresponding decisions, so detecting and analysing it is essential to understand biases in the information we consume. Despite that, framing is still mostly examined manually, on a case-by-case basis, while existing large-scale automatic analyses using NLP methods are not mature enough to solve this task. In this survey we show that despite the growing interest to framing in NLP its current approaches do not capture those aspects which allow to frame, rather than simply convey, the message. To this end, we bring together definitions of frames and framing adopted in different disciplines; examine cognitive, linguistic, and communicative aspects a frame contains beyond its topical content. We survey recent work on computational frame detection, and discuss how framing aspects and frame definitions are (or should) be reflected in NLP approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k7rqMwDaAc": {
    "title": "On the Tip of the Tongue: Analyzing Conceptual Representation in Large Language Models with Reverse-Dictionary Probe",
    "volume": "review",
    "abstract": "Probing and enhancing large language models' reasoning capacity remains a crucial open question. Here we re-purpose the reverse dictionary task as a case study to probe LLMs' capacity for conceptual inference. We use in-context learning to guide the models to generate the term for an object concept implied in a linguistic description. Models robustly achieve high accuracy in this task, and their representation space encodes information about object categories and fine-grained features. Further experiments suggest that the conceptual inference ability as probed by the reverse-dictionary task predicts model's general reasoning performance across multiple benchmarks, despite similar syntactic generalization behaviors across models. Explorative analyses suggest that prompting LLMs with description$\\Rightarrow$word examples may induce generalization beyond surface-level differences in task construals and facilitate models on broader commonsense reasoning problems",
    "checked": true,
    "id": "472250741df7f5b73f525a8834d453e05bc64a95",
    "semantic_title": "on the tip of the tongue: analyzing conceptual representation in large language models with reverse-dictionary probe",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q1KnButNt9E": {
    "title": "HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild",
    "volume": "review",
    "abstract": "Hallucinations pose a significant challenge to the reliability of large language models (LLMs) in critical domains. Recent benchmarks designed to assess LLM hallucinations within conventional NLP tasks, such as knowledge-intensive question answering (QA) and summarization, are insufficient for capturing the complexities of user-LLM interactions in dynamic, real-world settings. To address this gap, we introduce HaluEval-Wild, the first benchmark specifically designed to evaluate LLM hallucinations in the wild. We meticulously collect challenging (adversarially filtered by Alpaca) user queries from existing real-world user-LLM interaction datasets, including ShareGPT, to evaluate the hallucination rates of various LLMs. Upon analyzing the collected queries, we categorize them into five distinct types, which enables a fine-grained analysis of the types of hallucinations LLMs exhibit, and synthesize the reference answers with the powerful GPT-4 model and retrieval-augmented generation (RAG). Our benchmark offers a novel approach towards enhancing our comprehension and improvement of LLM reliability in scenarios reflective of real-world interactions",
    "checked": true,
    "id": "03cfdde24c6b9837ad8933cb535a2c4a7c0fd971",
    "semantic_title": "halueval-wild: evaluating hallucinations of language models in the wild",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=4KY2kRUJ5xe": {
    "title": "Citation-Enhanced Generation for LLM-based Chatbot",
    "volume": "review",
    "abstract": "Large language models (LLMs) exhibit powerful general intelligence across diverse scenarios, including their integration into chatbots. However, a vital challenge of LLM-based chatbots is that they may produce hallucinated content in responses, which significantly limits their applicability. Various efforts have been made to alleviate hallucination, such as retrieval augmented generation and reinforcement learning with human feedback, but most of them require additional training and data annotation. In this paper, we propose a novel post-hoc Citation-Enhanced Generation (CEG) approach combined with retrieval argumentation. Unlike previous studies that focus on preventing hallucinations during generation, our method addresses this issue in a post-hoc way. It incorporates a retrieval module to search for supporting documents relevant to the generated content, and employs a natural language inference-based citation generation module. Once the statements in the generated content lack of reference, our model can regenerate responses until all statements are supported by citations. Note that our method is a training-free plug-and-play plugin that is capable of various LLMs. Experiments on various hallucination-related datasets show our framework outperforms state-of-the-art methods in both hallucination detection and response regeneration on three benchmarks. Our codes and dataset will be publicly available",
    "checked": false,
    "id": "31e27369f64c51ffbfd9bc8e3cbb20705edc6bff",
    "semantic_title": "citation-enhanced generation for llm-based chatbots",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=uMJhX_CyES": {
    "title": "Event-level Knowledge Editing",
    "volume": "review",
    "abstract": "Knowledge editing aims at updating knowledge of large language models (LLMs) to prevent them from becoming outdated. Existing work edits LLMs at the level of factual knowledge triplets. However, natural knowledge updates in the real world come from the occurrences of new events rather than direct changes in factual triplets. In this paper, we propose a new task setting: event-level knowledge editing, which directly edits new events into LLMs and improves over conventional triplet-level editing on (1) Efficiency. A single event edit leads to updates in multiple entailed knowledge triplets. (2) Completeness. Beyond updating factual knowledge, event-level editing also requires considering the event influences and updating LLMs' knowledge about future trends. We construct a high-quality event-level editing benchmark ELKEN, consisting of $1,515$ event edits, $6,449$ questions about factual knowledge, and $10,150$ questions about future tendencies. We systematically evaluate the performance of various knowledge editing methods and LLMs on this benchmark. We find that ELKEN poses significant challenges to existing knowledge editing approaches. Our codes and dataset will be publicly released to facilitate further research",
    "checked": true,
    "id": "ff7457e8e389e7f20448cf33d0bc6205daa47a71",
    "semantic_title": "event-level knowledge editing",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=mqrS7pYT00": {
    "title": "Code Needs Comments: Enhancing Code LLMs with Comment Augmentation",
    "volume": "review",
    "abstract": "The programming skill is one crucial ability for Large Language Models (LLMs), necessitating a deep understanding of programming languages (PLs) and their correlation with natural languages (NLs). We examine the impact of pre-training data on code-focused LLMs' performance by assessing the comment density as a measure of PL-NL alignment. Given the scarcity of code-comment aligned data in pre-training corpora, we introduce a novel data augmentation method that generates comments for existing code, coupled with a data filtering strategy that filters out code data poorly correlated with natural language. We conducted experiments on three code-focused LLMs and observed consistent improvements in performance on two widely-used programming skill benchmarks. Notably, the model trained on the augmented data outperformed both the model used for generating comments and the model further trained on the data without augmentation",
    "checked": true,
    "id": "d496eced38a50d5f1e0926d9e6dac71ccfab0bb3",
    "semantic_title": "code needs comments: enhancing code llms with comment augmentation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=2Z3xvdtBfPe": {
    "title": "Maverick: Efficient and Accurate Coreference Resolution Defying Recent Trends",
    "volume": "review",
    "abstract": "Large autoregressive generative models have emerged as the cornerstone for achieving the highest performance across several Natural Language Processing tasks. However, the urge to attain superior results has, at times, led to the premature replacement of carefully designed task-specific approaches without exhaustive experimentation. The Coreference Resolution task is no exception; all recent state-of-the-art solutions adopt large generative autoregressive models that outperform encoder-based discriminative systems. In this work, we challenge this recent trend by introducing Maverick, a carefully designed -- yet simple -- pipeline, which enables running a state-of-the-art Coreference Resolution system within the constraints of an academic budget, outperforming models with up to 13 billion parameters with as few as 500 million parameters. Maverick achieves state-of-the-art performance on the CoNLL-2012 benchmark, training with up to 0.006x the memory resources and obtaining a 170x faster inference compared to previous state-of-the-art systems. We extensively validate the robustness of the Maverick framework with an array of diverse experiments, reporting improvements over prior systems in data-scarce, long-document, and out-of-domain settings. We release our code and models for research purposes at omitted.link",
    "checked": true,
    "id": "c89d02ce9c090fcb221df3a7da67856098584102",
    "semantic_title": "maverick: efficient and accurate coreference resolution defying recent trends",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ux0Tyrrx_Mr": {
    "title": "Uncovering Limitations of Large Language Models in Information Seeking from Tables",
    "volume": "review",
    "abstract": "Tables are recognized for their high information density and widespread usage, serving as essential sources of information. Seeking information from tables (TIS) is a crucial capability for Large Language Models (LLMs), serving as the foundation of knowledge-based Q&A systems. However, this field presently suffers from an absence of thorough and reliable evaluation. This paper introduces a more reliable benchmark for Table Information Seeking (TabIS). To avoid the unreliable evaluation caused by text similarity-based metrics, TabIS adopts a single-choice question format (with two options per question) instead of a text generation format. We establish an effective pipeline for generating options, ensuring their difficulty and quality. Experiments conducted on 12 LLMs reveal that while the performance of GPT-4-turbo is marginally satisfactory, both other proprietary and open-source models perform inadequately. Further analysis shows that LLMs exhibit a poor understanding of table structures, and struggle to balance between TIS performance and robustness against pseudo-relevant tables (common in retrieval-augmented systems). These findings uncover the limitations and potential challenges of LLMs in seeking information from tables. We release our data and code to facilitate further research in this field",
    "checked": true,
    "id": "718f8b17fc0d67230aa16e92f428b7c19fd38992",
    "semantic_title": "uncovering limitations of large language models in information seeking from tables",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CTgD6EBoluT": {
    "title": "Disco-Bench: A Context-Aware Evaluation Benchmark for Language Modelling",
    "volume": "review",
    "abstract": "Modeling large contexts, especially linguistic phenomena that span beyond individual sentences, is a fundamental yet challenging aspect of natural language processing (NLP). However, existing evaluation benchmarks primarily focus on the evaluation of inter-sentence properties and overlook critical discourse phenomena that cross sentences. To bridge the gap, we propose Disco-Bench, a benchmark that can evaluate intra-sentence contextual properties across a diverse set of NLP tasks, covering understanding, translation, and generation.Disco-Bench consists of 9 document-level testsets in the literature domain, which contain rich discourse phenomena (e.g. cohesion and coherence) in Chinese and/or English. For linguistic analysis, we also design a diagnostic test suite to probe the extent to which the evaluated models have internalized contextual information. We totally evaluate 20 general-purpose and domain-specific models based on advanced pretraining architectures and large language models (LLMs). Our results show that (1) our evaluation benchmark is both challenging and necessary; (2) fine-grained pretraining with literary document-level training data consistently enhances the modeling of discourse information. We will release the datasets, pretrained models, and leaderboard, which we hope can significantly facilitate research in this field",
    "checked": false,
    "id": "e1c957e0cb6098304deffb01e4428eb368f8e1ff",
    "semantic_title": "disco-bench: a discourse-aware evaluation benchmark for language modelling",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=qp9bGva3Or": {
    "title": "Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding",
    "volume": "review",
    "abstract": "Large Vision-Language Models (LVLMs) are increasingly adept at generating contextually detailed and coherent responses from visual inputs. However, their application in multimodal decision-making and open-ended generation is hindered by a notable rate of hallucinations, where generated text inaccurately represents the visual contents. To address this issue, this paper introduces the Instruction Contrastive Decoding (ICD) method, a novel approach designed to reduce hallucinations during LVLM inference. Our method is inspired by our observation that what we call disturbance instructions significantly exacerbate hallucinations in multimodal fusion modules. ICD contrasts distributions from standard and instruction disturbance, thereby increasing alignment uncertainty and effectively subtracting hallucinated concepts from the original distribution. Through comprehensive experiments on discriminative benchmarks (POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that ICD significantly mitigates both object-level and attribute-level hallucinations. Moreover, our method not only addresses hallucinations but also significantly enhances the general perception and recognition capabilities of LVLMs",
    "checked": true,
    "id": "982bc3b4025e971f95d8ba071f7bb667cc71e7a4",
    "semantic_title": "mitigating hallucinations in large vision-language models with instruction contrastive decoding",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=aqSTvlTvSUy": {
    "title": "Multi-Candidate Speculative Decoding",
    "volume": "review",
    "abstract": "Large language models have shown impressive capabilities across a variety of NLP tasks, yet their generating text autoregressively is time-consuming. One way to speed them up is speculative decoding, which generates candidate segments (a sequence of tokens) from a fast draft model that is then verified in parallel by the target model. However, the acceptance rate of candidate tokens from the draft model can be affected by several factors, such as the model, the dataset, and the decoding setup. This paper proposes to sample multiple candidates from a draft model and then organise them in batches for verification. We design algorithms for efficient multi-candidate verification while maintaining the distribution of the target model. Our approach shows significant improvements in acceptance rates across datasets, models, and decoding setups, consistently outperforming standard speculative decoding",
    "checked": true,
    "id": "1c0a0ec50a639efe9569d0c57f73c8e1b47acbcd",
    "semantic_title": "multi-candidate speculative decoding",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=i3VYl81eXa": {
    "title": "Investigating the Role of Language Instructions in Robotic Manipulation Tasks",
    "volume": "review",
    "abstract": "Instruction variety greatly impacts a model's ability to generalise outside of its training corpus. While language choices and paraphrases help models generalise to more complex tasks, embodied domain instructing models through multiple modalities (e.g., visual referents) can further help minimise ambiguities and improve the overall success rate. We investigate the impact of multimodal language instructions on a model's generalisation capacities on VIMA-Bench, an environment designed to evaluate generalisation performance through increasing levels of complexity. We design different perturbations that affect both the language and the visual referents in multimodal instructions. Our findings indicate that a VIMA model trained on multimodal instructions not only shows high performance when provided with gibberish instructions, but can even perform better on unseen tasks, casting doubts as to whether content from text in multimodal instructions is more useful than the necessary visual referents. Our findings suggest that current Transformer-based models for Embodied AI tasks are limited as to how way they integrate multiple modalities. Therefore, future work should focus on improvements in architecture design and training regimes to further facilitate multimodal fusion allowing the model to place more importance on the content of the instructions, thereby improving generalisation capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J6Cy4LAqbDi": {
    "title": "Intuitive or Dependent? Investigating LLMs' Behavior Style to Conflicting Prompts",
    "volume": "review",
    "abstract": "This study investigates the behaviors of Large Language Models (LLMs) when faced with conflicting prompts versus their internal memory. This will not only help to understand LLMs' decision mechanism but also benefit real-world applications, such as retrieval-augmented generation (RAG).Drawing on cognitive theory, we target the first scenario of decision-making styles where there is no superiority in the conflict and categorize LLMs' preference into dependent, intuitive, and rational/irrational styles.Another scenario of factual robustness considers the correctness of prompt and memory in knowledge-intensive tasks, which can also distinguish if LLMs behave rationally or irrationally in the first scenario.To quantify them, we establish a complete benchmarking framework including a dataset, a robustness evaluation pipeline, and corresponding metrics. Extensive experiments with seven LLMs reveal their varying behaviors. And, with role play intervention, we can change the styles, but different models present distinct adaptivity and upper-bound. One of our key takeaways is to optimize models or the prompts according to the identified style. For instance, RAG models with high role play adaptability may dynamically adjust the interventions according to the quality of retrieval results --- being dependent to better leverage informative context; and, being intuitive when external prompt is noisy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kf7YTasiRSC": {
    "title": "DevEval: A Code Generation Benchmark for Practical Software Projects",
    "volume": "review",
    "abstract": "How to evaluate Large Language Models (LLMs) in code generation is an open question. There is currently no benchmark for practical software projects. In this paper, we propose a new benchmark named DevEval, aligned with Developers' experiences in practical projects. DevEval is collected through a rigorous pipeline, containing 2,690 samples from 119 practical projects and covering 10 domains. Compared to previous benchmarks, DevEval aligns to practical projects in multiple dimensions, e.g., real program distributions, sufficient dependencies, and enough-scale project contexts. We assess 12 popular LLMs on DevEval (e.g., gpt-4, gpt-3.5-turbo, Claude 2, GLM-4, CodeLLaMa, StarCoder, and Mistral) and reveal their actual abilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo only is 42.97% in our experiments. We also discuss the challenges of code generation in practical projects. We open-source DevEval and hope it can facilitate the development of code generation in practical projects",
    "checked": false,
    "id": "7ebf4402b4b73b765152a7c9061eb16c8bbfeefe",
    "semantic_title": "deveval: evaluating code generation in practical software projects",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=XMmq1YQkFN6": {
    "title": "X-Shot: A Unified System to Handle Frequent, Few-shot and Zero-shot Learning Simultaneously in Classification",
    "volume": "review",
    "abstract": "In recent years, few-shot and zero-shot learning, which learn to predict labels with limited annotated instances, have garnered significant attention. Traditional approaches often treat frequent-shot (freq-shot; labels with abundant instances), few-shot, and zero-shot learning as distinct challenges, optimizing systems for just one of these scenarios. Yet, in real-world settings, label occurrences vary greatly. Some of them might appear thousands of times, while others might only appear sporadically or not at all. For practical deployment, it is crucial that a system can adapt to any label occurrence. We introduce a novel classification challenge: X-shot, reflecting a real-world context where freq-shot, few-shot, and zero-shot labels co-occur without predefined limits. Here, X can span from 0 to positive infinity. The crux of X-shot centers on open-domain generalization and devising a system versatile enough to manage various label scenarios. To solve X-shot, we propose BinBin (Binary INference Based on INstruction following) that leverages the Indirect Supervision from a large collection of NLP tasks via instruction following, bolstered by Weak Supervision provided by large language models. BinBin surpasses previous state-of-the-art techniques on three benchmark datasets across multiple domains. To our knowledge, this is the first work addressing X-shot learning, where X remains variable",
    "checked": true,
    "id": "c6284ac6bcd053634866caeca1fa5102bb96dc7a",
    "semantic_title": "x-shot: a unified system to handle frequent, few-shot and zero-shot learning simultaneously in classification",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=al74feUUuYQ": {
    "title": "Dual-Hypergraph Embedding for Hyper-Relational Knowledge Graphs",
    "volume": "review",
    "abstract": "Hyper-relational knowledge graphs (HKGs) have been shown to effectively represent complex data relation. However, many HKG representation learning methods ignore the one-to-many relation between attributes and values in hyper-relational facts (H-Fact). Moreover, the information extraction from H-Facts is insufficient. To solve these issues, we propose a Dual-Hypergraph Embedding for HKGs (DHE). Specifically, we first construct an intra-hypergraph and an inter-hypergraph by considering one-to-many relation in H-Facts, and then design a hypergraph Transformer encoder to comprehensively conduct H-Fact representation learning. Experimental results on real datasets show that our DHE outperforms all SOTA methods in link prediction tasks on HKGs",
    "checked": false,
    "id": "d94e7aeab88d07f0b50b7866ceb5baa3c29be859",
    "semantic_title": "nqe: n-ary query embedding for complex query answering over hyper-relational knowledge graphs",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=7nCicZAj3v1": {
    "title": "Ask Optimal Questions: Aligning Large Language Models with Retriever's Preference in Conversational Search",
    "volume": "review",
    "abstract": "Conversational search, unlike single-turn retrieval tasks, requires understanding the current question within a dialogue context.The common approach of rewrite-then-retrieve aims to decontextualize questions to be self-sufficient for off-the-shelf retrievers, but most existing methods produce sub-optimal query rewrites due to the limited ability to incorporate signals from the retrieval results.To overcome this limitation, we present a novel framework RetPO (Retriever's Preference Optimization), which is designed to optimize a language model (LM) for reformulating search queries in line with the preferences of the target retrieval systems.The process begins by prompting a large LM to produce various potential rewrites and then collects retrieval performance for these rewrites as the retrievers' preferences. Through the process, we construct a large-scale dataset called RF collection, containing Retrievers' Feedback on over 410K query rewrites across 12K conversations.Furthermore, we fine-tune a smaller LM using this dataset to align it with the retrievers' preferences as feedback.The resulting model achieves state-of-the-art performance on two recent conversational search benchmarks, significantly outperforming existing baselines, including GPT-3.5",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HNOhFT2cow": {
    "title": "AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models",
    "volume": "review",
    "abstract": "We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as \\textit{Adaptive Freezing of Low Rank Adaptation} (AFLoRA). Specifically, for each pre-trained frozen weight tensor, we add a parallel path of trainable low-rank matrices, namely a down-projection and an up-projection matrix, each of which is followed by a feature transformation vector. Based on a novel \\textit{freezing score}, we the incrementally freeze these projection matrices during fine-tuning to reduce the computation and alleviate over-fitting. Our experimental results demonstrate that we can achieve state-of-the-art performance with an average improvement of up to $0.85\\%$ as evaluated on GLUE benchmark while yeilding up to $9.5\\times$ fewer average trainable parameters. While compared in terms of runtime, AFLoRA can yield up to $1.86\\times$ improvement as opposed to similar PEFT alternatives. Besides the practical utility of our approach, we provide insights on the trainability requirements of LoRA paths at different modules and the freezing schedule for the different projection matrices",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ckiax6z1CIN": {
    "title": "Light-PEFT: Lightening Parameter-Efficient Fine-Tuning via Early Pruning",
    "volume": "review",
    "abstract": "Parameter-efficient fine-tuning (PEFT) has emerged as the predominant technique for fine-tuning in the era of large language models. However, existing PEFT methods still have inadequate training efficiency. Firstly, the utilization of large-scale foundation models during the training process is excessively redundant for certain fine-tuning tasks. Secondly, as the model size increases, the growth in trainable parameters of empirically added PEFT modules becomes non-negligible and redundant, leading to inefficiency. To achieve task-specific efficient fine-tuning, we propose the Light-PEFT framework, which includes two methods: Masked Early Pruning of the Foundation Model and Multi-Granularity Early Pruning of PEFT. The Light-PEFT framework allows for the simultaneous estimation of redundant parameters in both the foundation model and PEFT modules during the early stage of training. These parameters can then be pruned for more efficient training. We validate our approach on GLUE, SuperGLUE, QA tasks, and various models. With Light-PEFT, parameters of the foundation model can be pruned by over 40%, while still controlling trainable parameters to be only 25% of the original PEFT method. Compared to utilizing the PEFT method directly, Light-PEFT achieves training and inference speedup, reduces memory usage, and maintains comparable performance and the plug-and-play feature of PEFT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YUCaelhRku": {
    "title": "Exploiting Target Language Data for Neural Machine Translation Beyond Back Translation",
    "volume": "review",
    "abstract": "Neural Machine Translation (NMT) suffers from the challenges of translating in new domains and low-resource languages. To address these challenges, researchers have proposed methods to incorporate additional knowledge into NMT, including the integration of translation memories (TMs). However, finding TMs that closely match the input sentence remains difficult, particularly for specific domains. In contrast, monolingual data is widely available in most languages and back-translation is believed as a promising method to utilize target language data. But, it still needs additional training. In this paper, we propose Pseudo-kNN-MT, a method that exploit target language data during the inference phase, without training the NMT model. Also, we further investigate the assistance of large language model (LLM) in NMT. Experimental results show that our method can improve translation quality by a great margin. Interestingly, LLMs are found to be helpful for strong NMT systems",
    "checked": false,
    "id": "7a63d12a544d7a995db9feb902d6873dc5e2f746",
    "semantic_title": "reinforcement of low-resource language translation with neural machine translation and backtranslation synergies",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qEMqlbPVxU9": {
    "title": "Check-All: Enhancing LLM Reasoning through a Single-Prompt Debiasing Framework",
    "volume": "review",
    "abstract": "Careful prompting of large language models (LLMs) is crucial for eliciting relevant answers to a query. Current single-prompt methods have led the model to explicitly generate the reasoning for the provided query. However, these methods do not control the generated reasoning output, including approach, solution path, or structure, making them unreliable for use. Consequently, we propose to control the structure of the reasoning. Thus, we design Check-All, a framework for 0-shot prompting, which requires the model to reason on all potential outcomes. With this framework, we demonstrate improved LLM performance compared to other 0-shot prompting reasoning methods in classification tasks. Furthermore, we leverage Check-All to potentially identify mislabeled data in existing datasets. To enhance the overall quality of research in the field, we advocate for improved dataset quality for evaluation purposes. Lastly, we adapt Check-All to effectively protect users and prevent potential attackers from receiving harmful output by moderating the responses of LLMs in real-time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0W_1AlblJHF": {
    "title": "Every Answer Matters: Evaluating Commonsense with Probabilistic Measures",
    "volume": "review",
    "abstract": "Large language models have demonstrated impressive performance on commonsense tasks; however, these tasks are often posed as multiple-choice questions, allowing models to exploit systematic biases. Commonsense is also inherently probabilistic with multiple correct answers. The purpose of \"boiling water\" could be making tea, cooking but also could be killing germs. Existing tasks do not capture the probabilistic nature of common sense. To this end, we present commonsense frame completion (CFC), a new generative task that evaluates common sense via multiple open-ended generations. We also propose a method of probabilistic evaluation that strongly correlates with human judgments. Humans drastically outperform strong language model baselines on our dataset, indicating this approach is both a challenging and useful evaluation of machine common sense",
    "checked": true,
    "id": "5d4b8e3afdabf927cedf3302cd6819ae20250423",
    "semantic_title": "every answer matters: evaluating commonsense with probabilistic measures",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nGLopKyvE_d": {
    "title": "Personality-aware Human-centric Multimodal Reasoning: A New Task, Dataset and Baselines",
    "volume": "review",
    "abstract": "Personality traits, emotions, and beliefs shape individuals' behavioral choices and decision-making processes. However, for one thing, the affective computing community normally focused on predicting personality traits but overlooks their application in behavior prediction. For another, the multimodal reasoning task although emphasized on the prediction of future states and behaviors, often neglect incorporating individual personality traits. In this work, we introduce a new task called Personality-aware Human-centric Multimodal Reasoning (PHMR) (T1), with the goal to forecast the future behavior of a particular individual by leveraging multimodal information from past instances, while integrating personality factors. We accordingly construct a new dataset based on six television shows, encompassing 225 characters and 12k samples. To establish a benchmark for the task, we propose seven baseline methods: three adapted from related tasks, two pre-trained model, and two video large models. The experimental results demonstrate that incorporating personality traits enhances human-centric multimodal reasoning performance. To further solve the lack of personality annotation in real-life scenes, we introduce an extension task called Personality-predicted Human-centric Multimodal Reasoning task (T2) along with the corresponding dataset and method. We will make our dataset and code available on GitHub",
    "checked": false,
    "id": "299e2899a1c2df3607655c129d6f53239c0d9dde",
    "semantic_title": "personality-aware human-centric multimodal reasoning: a new task",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=MVS4_XmcnaF": {
    "title": "Exploring the Dissociated Nucleus Phenomenon in Semantic Role Labeling",
    "volume": "review",
    "abstract": "Dependency-based Semantic Role Labeling (SRL) is bound to Dependency Parsing, as the arguments of a predicate are identified through the token that heads the dependency relation subtree of the argument span. However, most dependency-based SRL corpora are susceptible to the dissociated nucleus problem: when a subclause's semantic and structural cores are two separate words, the dependency tree chooses the structural token as the head of the subtree, coercing the SRL annotation to make the same choice. This leads to undesirable consequences: when directly using the output of a dependency-based SRL method in downstream tasks it is useful to work with the token representing the semantic core of a subclause, not the structural core.In this paper, we carry out a linguistically-driven investigation on the dissociated nuclei problem in dependency-based SRL and propose a novel algorithm which aligns predicate-argument structures to the syntactic structures from Universal Dependencies to select the semantic core of an argument. Our analysis shows that dissociated nuclei appear more often than one could expect, and that our novel algorithm greatly enhances the richness of the semantic information in dependency-based SRL.We release the software to reproduce our experiments at (http://omitted.link)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rnmmk-hyQY": {
    "title": "OVEL: Large Language Model as Memory Manager for Online Video Entity Linking",
    "volume": "review",
    "abstract": "In recent years, multi-modal entity linking (MEL) has garnered increasing attention in the research community due to its significance in numerous multi-modal applications. Video, as a popular means of information transmission, has become prevalent in people's daily lives. However, most existing MEL methods primarily focus on linking textual and visual mentions or offline videos's mentions to entities in multi-modal knowledge bases, with limited efforts devoted to linking mentions within online video content. In this paper, we propose a task called Online Video Entity Linking (\\textit{OVEL}), aiming to establish connections between mentions in online videos and a knowledge base with high accuracy and timeliness. To facilitate the research works of \\textit{OVEL}, we specifically concentrate on live delivery scenariosand construct a live delivery entity linking dataset called \\textit{LIVE}. Besides, we propose an evaluation metric that considers timelessness, robustness, and accuracy. Furthermore, to effectively handle \\textit{OVEL} task, we leverage a memory block managed by a Large Language Modeland retrieve entity candidates from the knowledge base to augment LLM performance on memory management. The experimental results prove the effectiveness and efficiency of our method",
    "checked": true,
    "id": "2f2e5ab6e91065193164c0757ca97a14902c5748",
    "semantic_title": "ovel: large language model as memory manager for online video entity linking",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=tYWMmYwHGRa": {
    "title": "Better Synthetic Data by Retrieving and Transforming Existing Datasets",
    "volume": "review",
    "abstract": "Despite recent advances in large language models, building dependable and deployable NLP models typically requires abundant, high-quality training data. However, task-specific data is not available for many use cases, and manually curating task-specific data is labor-intensive. Recent work has studied prompt-driven synthetic data generation using large language models, but these generated datasets tend to lack complexity and diversity. To address these limitations, we introduce a method, DataTune, to make better use of existing, publicly available datasets to improve automatic dataset generation. DataTune performs dataset transformation, enabling the repurposing of publicly available datasets into a format that is directly aligned with the specific requirements of target tasks. On a diverse set of language-based tasks from the BIG-Bench benchmark, we find that finetuning language models via DataTune improves over a few-shot prompting baseline by 49% and improves over existing methods that use synthetic or retrieved training data by 34%. We find that dataset transformation significantly increases the diversity and difficulty of generated data on many tasks. We release a Python package and open-source repository to make this method accessible to the community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qN_RGmVmCR4": {
    "title": "mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy by Language Models and Humans",
    "volume": "review",
    "abstract": "It is very challenging to curate a dataset for language-specific knowledge and common sense in order to evaluate natural language understanding capabilities of language models. Due to the limitation in the availability of annotators, most current multilingual datasets are created through translation, which cannot evaluate such language-specific aspects. Therefore, we propose Multilingual CommonsenseQA (mCSQA) based on the construction process of CSQA but leveraging language models for a more efficient construction, e.g., by asking LM to generate questions/answers, refine answers and verify QAs followed by reduced human efforts for verification. Constructed dataset is a benchmark for cross-lingual language-transfer capabilities of multilingual LMs, and experimental results showed high language-transfer capabilities for questions that LMs could easily solve, but lower transfer capabilities for questions requiring deep knowledge or commonsense. This highlights the necessity of language-specific datasets for evaluation and training. Finally, our method demonstrated that multilingual LMs could create QA including language-specific knowledge, significantly reducing the dataset creation cost compared to manual creation. The datasets are available at \\url{https://anonymized_for_review}",
    "checked": true,
    "id": "e100989ca03cecf8ad865287d7a77719ddb8d796",
    "semantic_title": "mcsqa: multilingual commonsense reasoning dataset with unified creation strategy by language models and humans",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qqDJ7CIJ0ra": {
    "title": "MAPLE: Multilingual Evaluation of Parameter Efficient Finetuning of Large Language Models",
    "volume": "review",
    "abstract": "Parameter efficient finetuning has emerged as a viable solution for improving the performance of Large Language Models without requiring massive resources and compute. Prior work on multilingual evaluation has shown that there is a large gap between the performance of LLMs on English and other languages. Further, there is also a large gap between the performance of smaller open-source models and larger LLMs. Finetuning can be an effective way to bridge this gap and make language models more equitable. In this work, we finetune the \\llm\\ and \\mis\\ models on two synthetic multilingual instruction tuning datasets to determine its effect on model performance on six downstream tasks covering forty one languages in all. Additionally, we experiment with various parameters, such as rank for low-rank adaptation and values of quantisation to determine their effects on downstream performance and find that higher rank and higher quantisation values benefit low-resource languages. We find that parameter efficient finetuning of smaller open-source models sometimes bridges the gap between the performance of these models and the larger ones, however, English performance can take a hit. We also find that finetuning sometimes improves performance on low-resource languages, while degrading performance on high-resource languages",
    "checked": true,
    "id": "75bc30bf394625c784ea59f8c2fe04718a4b4042",
    "semantic_title": "maple: multilingual evaluation of parameter efficient finetuning of large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nZL6S0b-HcI": {
    "title": "LLM-VeriPPA: Power, Performance, and Area-aware Verilog Code Generation and Refinement with Large Language Models",
    "volume": "review",
    "abstract": "As Large Language Models (LLMs) gain increasing prominence across a variety of domains and inspired by their remarkable ability to generate high-quality content in response to human language instructions. This study delves into the application of LLMs within the field of hardware design, specifically in the generation and refinement of Verilog code. We introduce a novel framework \\framework designed to assess and enhance LLM efficiency in this specialized area. Our method includes generating initial Verilog code using LLMs, followed by a unique two-stage refinement process. The first stage focuses on improving the functional and syntactic integrity of the code, while the second stage aims to optimize the code in line with Power-Performance-Area (PPA) constraints, an essential aspect of effective hardware design. This dual-phase approach of error correction and PPA optimization has led to notable improvements in the quality of LLM-generated Verilog code. Our framework achieves a success rate of 81.37\\% for syntactic correctness and 62.0\\% for functional accuracy in code generation, surpassing current state-of-the-art (SOTA) methods, e.g., 73\\% for syntactic correctness and 46\\% for functional accuracy. These results highlight the potential of LLMs in handling complex technical areas, and indicate an encouraging development in the automation of hardware design processes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AIg9u1AjwlG": {
    "title": "RepoHyper: Better Context Retrieval Is All You Need for Repository-Level Code Completion",
    "volume": "review",
    "abstract": "Code Large Language Models (CodeLLMs) have demonstrated impressive proficiency in code completion tasks. However, they often fall short in fully understanding the extensive context of a project repository, such as the intricacies of relevant files and class hierarchies, which can result in less precise completions. To overcome these limitations, we present RepoHyper, a multifaceted framework designed to address the complex challenges associated with repository-level code completion. Central to RepoHyper is the Repo-level Semantic Graph (RSG), a novel semantic graph structure that encapsulates the vast context of code repositories. Furthermore, RepoHyper leverages Expand and Refine retrieval method, including a graph expansion and a link prediction algorithm applied to the RSG, enabling the effective retrieval and prioritization of relevant code snippets. Our evaluations show that RepoHyper markedly outperforms existing techniques in repository-level code completion, showcasing enhanced accuracy across various datasets when compared to several strong baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t-SXt3NEuj": {
    "title": "Holistic Coverage and Faithfulness Evaluation of Large Vision-Language Models",
    "volume": "review",
    "abstract": "Large Vision Language Models (LVLMs) suffer from hallucination problems, wherein the models generate plausible-sounding but factually incorrect outputs, undermining their reliability. A comprehensive quantitative evaluation is necessary to identify and understand the extent of hallucinations in these models. However, existing benchmarks are often limited in scope, focusing mainly on object hallucinations. Furthermore, current evaluation methods struggle to effectively address the subtle semantic distinctions between model outputs and reference data, as well as the balance between hallucination and informativeness. To address these issues, we introduce a multi-dimensional benchmark covering objects, attributes, and relations, with challenging images selected based on associative bias. Moreover, we propose an LLM-based two-stage evaluation framework that generalizes the popular CHAIR metric (Rohrbach et al., 2018) and incorporates both faithfulness and coverage into the evaluation. We provide a detailed assessment of 10 established LVLMs within our framework and results demonstrate that we provide a more comprehensive and human-correlated evaluation than existing work. Through this work, we highlight the critical balance between faithfulness and coverage of model outputs, and we hope our work encourages future progress on addressing hallucinations in LVLMs while keeping their outputs informative",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PxK8YJVMGla": {
    "title": "Ask LLMs Directly, \"What shapes your bias?\": Measuring Social Bias in Large Language Models",
    "volume": "review",
    "abstract": "Social bias is shaped by the accumulation of social perceptions towards targets across various demographic identities. To fully understand such social bias in large language models (LLMs), it is essential to consider the composite of social perceptions from diverse perspectives among identities. Previous studies have either evaluated biases in LLMs by indirectly assessing the presence of sentiments towards demographic identities in the generated text or measuring the degree of alignment with given stereotypes. These methods have limitations in directly quantifying social biases at the level of distinct perspectives among identities. In this paper, we aim to investigate how social perceptions from various viewpoints contribute to the development of social bias in LLMs. To this end, we propose a novel strategy to intuitively quantify these social perceptions and suggest metrics that can evaluate the social biases within LLMs by aggregating diverse social perceptions. The experimental results show the quantitative demonstration of the social attitude in LLMs by examining social perception. The analysis we conducted shows that our proposed metrics capture the multi-dimensional aspects of social bias, enabling a fine-grained and comprehensive investigation of bias in LLMs",
    "checked": true,
    "id": "110be9d98a8175028bc8360e6bfbff84d3fd96f2",
    "semantic_title": "ask llms directly, \"what shapes your bias?\": measuring social bias in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Iyv-Ns104qj": {
    "title": "GlobeSumm: A Challenging Benchmark Towards Unifying Multi-lingual, Cross-lingual and Multi-document News Summarization",
    "volume": "review",
    "abstract": "News consumption in today's globalized world presents a challenge due to the overwhelming amount of multilingual information and diverse perspectives provided by different sources. However, there has been a lack of research addressing this issue. To address this gap, we aim to unify Multi-lingual, Cross-lingual and Multi-document Summarization into a novel task, i.e., MCMS. Nevertheless, the absence of a benchmark dataset inhibits researchers from effectively studying this invaluable problem. To tackle this obstacle, we have meticulously constructed the GlobeSumm dataset. Specifically, we start by collecting a vast amount of news data and retrieving multilingual news related to the same event. We then meticulously design a reference annotation method called protocol-guided prompting, which utilizes the domain expertise of human annotators and effectively captures news redundancies, omissions, and conflicts to guide large-scale language models (LLMs) in generating high-quality reference summaries. Through extensive experimental analysis, we validate the quality of our dataset and highlight the inherent challenges of the task. We firmly believe that the GlobeSumm, given its challenging nature, will contribute to advancements in global news summarization and the evaluation of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WmooQ9Imec5": {
    "title": "$\\texttt{Se}^2$: $\\textbf{\\textit{se}}$quential Example $\\textbf{\\textit{se}}$lection for In-Context Learning",
    "volume": "review",
    "abstract": "The remarkable capability of large language models (LLMs) for in-context learning (ICL) needs to be activated by demonstration examples. Prior work has extensively investigated the selection of examples for ICL, predominantly following to a \"select then organize\" paradigm, such approaches often neglect the internal relationships between examples, and exist an inconsistency between the training and inference. In this paper, we formulate the problem as a $\\textbf{\\textit{se}}$quential $\\textbf{\\textit{se}}$lection problem and introduce $\\texttt{Se}^2$, a sequential-aware method that leverages the LLM's feedback on varying context, aiding in capturing inter-relationships and sequential information among examples, significantly enriching the contextuality and relevance of ICL prompts. Meanwhile, we utilize beam search to seek and construct example sequences, enhancing both quality and diversity. Extensive experiments across 23 NLP tasks from 8 distinct categories illustrate that $\\texttt{Se}^2$ markedly surpasses competitive baselines and achieves over 42% relative improvement over random selection. Further in-depth analysis show the effectiveness of proposed strategies, highlighting $\\texttt{Se}^2$'s exceptional stability and adaptability across various scenarios. Our code will be released to facilitate future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z9FgnjTqce": {
    "title": "Retaining Key Information under High Compression Rates: Query-Guided Compressor for LLMs",
    "volume": "review",
    "abstract": "The growing popularity of Large Language Models has sparked interest in context compression for Large Language Models (LLMs). However, the performance of previous methods degrades dramatically as compression rates increase, sometimes even falling to the closed-book level. This decline can be attributed to the loss of key information during the compression process. Our preliminary study supports this hypothesis, emphasizing the significance of retaining key information to maintain model performance under high compression rates. As a result, we introduce Query-Guided Compressor (QGC), which leverages queries to guide the context compression process, effectively preserving key information within the compressed context. Additionally, we employ a dynamic compression strategy. We validate the effectiveness of our proposed QGC on the Question Answering task, including NaturalQuestions, TriviaQA, and HotpotQA datasets. Experimental results show that QGC can consistently perform well even at high compression rates, which also offers significant benefits in terms of inference cost and throughput",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yBr6fd8vfw": {
    "title": "Refine, Align, and Aggregate: Multi-view Linguistic Features Enhancement for Aspect Sentiment Triplet Extraction",
    "volume": "review",
    "abstract": "Aspect Sentiment Triplet Extraction (ASTE) aims to extract the triplets of aspect terms, their associated sentiment and opinion terms. Previous works based on different modeling paradigms have achieved promising results. However, these methods struggle to comprehensively explore the various specific relations between sentiment elements in multi-view linguistic features, which is the prior indication effect for facilitating sentiment triplets extraction, requiring to align and aggregate them to capture the complementary higher-order interactions. In this paper, we propose Multi-view Linguistic Features Enhancement (MvLFE) to explore the aforementioned prior indication effect in the \"Refine, Align, and Aggregate\" learning process. Specifically, we first introduce the relational graph attention network to encode the word-pair relations represented by each linguistic feature and refine them to pay more attention to the aspect-opinion pairs. Next, we employ the multi-view contrastive learning to align them at a fine-grained level in the contextual semantic space to maintain semantic consistency. Finally, we utilize the multi-semantic cross attention to capture and aggregate the complementary higher-order interactions between diverse linguistic features to enhance the aspect-opinion relations. Experimental results on several benchmark datasets show the effectiveness and robustness of our model, which achieves state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rdInm54ov__": {
    "title": "Large Language Models With Holistically Thought Could Be Better Doctors",
    "volume": "review",
    "abstract": "The medical conversational question answering (CQA) system aims at providing a series of professional medical services to improve the efficiency of medical care. Despite the success of large language models (LLMs) in complex reasoning tasks in various fields, such as mathematics, logic, and commonsense QA, they still need to improve with the increased complexity and specialization of the medical field. This is because medical CQA tasks require not only strong medical reasoning, but also the ability to think broadly and deeply. In this paper, to address these challenges in medical CQA tasks that need to be considered and understood in many aspects, we propose the Holistically Thought (\\textbf{HoT}) method, which is designed to guide the LLMs to perform the diffused and focused thinking for generating high-quality medical responses.The proposed HoT method has been evaluated through automated and manual assessments in three different medical CQA datasets containing English and Chinese languages. The extensive experimental results show that our method can produce more correct, professional, and considerate answers than several state-of-the-art (SOTA) methods, manifesting its effectiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RNZJ5RElp9": {
    "title": "ROSE Doesn't Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding",
    "volume": "review",
    "abstract": "With the development of instruction-tuned large language models (LLMs), improving the safety of LLMs has become more critical. However, the current approaches for aligning the LLMs output with expected safety usually require substantial training efforts, e.g., high-quality safety data and expensive computational resources, which are costly and inefficient. To this end, we present reverse prompt contrastive decoding (ROSE), a simple-yet-effective method to directly boost the safety of existing instruction-tuned LLMs without any additional training. The principle of ROSE is to improve the probability of desired safe output via suppressing the undesired output induced by the carefully-designed reverse prompts. Experiments on 6 safety and 2 general-purpose tasks show that, our ROSE not only brings consistent and significant safety improvements (up to +13.8% safety score) upon 5 types of instruction-tuned LLMs, but also benefits the general-purpose ability of LLMs. In-depth analyses explore the underlying mechanism of ROSE, and reveal when and where to use it",
    "checked": true,
    "id": "c3f079f9f59f255e032a1239aea02a2affe93be8",
    "semantic_title": "rose doesn't do that: boosting the safety of instruction-tuned large language models with reverse prompt contrastive decoding",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=DbdoPpxE_l": {
    "title": "An image speaks a thousand words, but can everyone listen? On translating images for cultural relevance",
    "volume": "review",
    "abstract": "Given the rise of multimedia content, human translators increasingly focus on culturally adapting not only words but also other modalities such as images to convey the same meaning. While several applications stand to benefit from this, machine translation systems remain confined to language in speech and text. In this work, we take a first step towards translating images to make them culturally relevant. First, we build three pipelines comprising state-of-the-art generative models to do the task. Next, we build a two-part evaluation dataset -- (i) concept: comprising 600 images that are cross-culturally coherent, focusing on a single concept per image; and (ii) application: comprising 100 images curated from real-world applications. We conduct a multi-faceted human evaluation of translated images to assess for cultural relevance and meaning preservation. We find that as of today, image-editing models fail at this task, but can be improved by leveraging LLMs and retrievers in the loop. Best models can only translate 6% of images for some countries in the easier concept dataset and no translation is successful for some countries in the application dataset, highlighting the challenging nature of the task. Our code and data is released here: https://anonymous.4open.science/r/image-translation-6980",
    "checked": true,
    "id": "2ba3607e71165002ce5057b2afbc25c4211f67c2",
    "semantic_title": "an image speaks a thousand words, but can everyone listen? on translating images for cultural relevance",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=qb50-zcKmH": {
    "title": "Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models",
    "volume": "review",
    "abstract": "Recent studies show that self-feedback improves large language models (LLMs) on certain tasks while worsens other tasks. We discovered that such a contrary is due to LLM's bias towards their own output. In this paper, we formally define LLM's self-bias -- the tendency to favor its own generation -- using two statistics. We analyze six LLMs on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks",
    "checked": false,
    "id": "00e6500616920a25ecd95d0d3ad6f7764266b31b",
    "semantic_title": "pride and prejudice: llm amplifies self-bias in self-refinement",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=4EC0Z5RJje": {
    "title": "Metric-Driven Knowledge Graph Completion: A Novel Upper Bound with Dynamic and Effective Approximation to Reciprocal Rank",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FB7Zq-FBU7j": {
    "title": "PUB: A Pragmatics Understanding Benchmark for Assessing LLMs' Pragmatics Capabilities",
    "volume": "review",
    "abstract": "LLMs have demonstrated remarkable capability for understanding semantics, but their understanding of pragmatics is not well studied. To this end, we release a Pragmatics Understanding Benchmark (PUB) dataset consisting of fourteen tasks in four pragmatics phenomena, namely; Implicature, Presupposition, Reference, and Deixis. We curate high-quality test sets for each task, consisting of Multiple Choice Question Answers (MCQA). PUB includes a total of 28k data points, 6.1k are newly annotated. We evaluate nine models varying in the number of parameters and type of training. Our study reveals several key observations about the pragmatic capabilities of LLMs: 1. chat-fine-tuning strongly benefits smaller models, 2. large base models are competitive with their chat-fine-tuned counterparts, 3. there is a huge variance in performance across different pragmatics phenomena, and 4. a noticeable performance gap between human capabilities and model capabilities. We hope that PUB will enable comprehensive evaluation of LLM's pragmatic reasoning capabilities",
    "checked": true,
    "id": "1d8b0deda0658301959ff01dd4a10a926454e761",
    "semantic_title": "pub: a pragmatics understanding benchmark for assessing llms' pragmatics capabilities",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=iufWQ4jKRS": {
    "title": "ChatShop: Interactive Information Seeking with Language Agents",
    "volume": "review",
    "abstract": "The desire and ability to seek new information strategically are fundamental to human learning but often overlooked in current language agent development. Using a web shopping task as an example, we show that it can be reformulated and solved as a retrieval task without a requirement of interactive information seeking. We then redesign the task to introduce a new role of shopper, serving as a realistically constrained communication channel. The agents in our proposed ChatShop task explore user preferences in open-ended conversation to make informed decisions. Our experiments demonstrate that the proposed task can effectively evaluate the agent's ability to explore and gradually accumulate information through multi-turn interaction. We also show that LLM-simulated shoppers serve as a good proxy to real human shoppers and discover similar error patterns of agents",
    "checked": true,
    "id": "bdd52171bdd1de6b5c22f5e345ee0b1efff48085",
    "semantic_title": "chatshop: interactive information seeking with language agents",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=aXzxm5B60b": {
    "title": "RR-Norm: A Novel Framework for Chinese Disease Diagnoses Normalization via LLM-Driven Terminology Component Recognition and Reconstruction",
    "volume": "review",
    "abstract": "The Clinical Terminology Normalization aims at finding standard terms from a given termbase for mentions extracted from clinical texts. However, we found that extracted mentions suffer from the multi-implication problem, especially disease diagnoses. The reason for this is that physicians often use abbreviations, conjunctions, and juxtapositions when writing diagnoses, and it is difficult to manually decompose. To address this problem, we propose a Terminology Component Recognition and Reconstruction strategy that leverages the reasoning capability of large language models (LLMs) to recognize the components of terms, enabling automated decomposition and transforming original mentions into multiple atomic mentions. Furthermore, we adopt the mainstream ``Recall and Rank'' framework to apply the benefits of the above strategy to the task flow. By leveraging the LLM incorporating the advanced sampling strategies, we design a sampling algorithm for atomic mentions and train the recall model using contrastive learning. Besides the information about the components is also used as knowledge to guide the final term ranking and selection. The experimental results show that our proposed strategy effectively improves the performance of the terminology normalization task and our proposed approach achieves state-of-the-art on the experimental dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CWaEEZuImVy": {
    "title": "Is It a Free Lunch for Removing Outliers during Pretraining?",
    "volume": "review",
    "abstract": "With the growing size of large language models, the role of quantization becomes increasingly significant. However, outliers present in weights or activations notably influence the performance of quantized models. Recently, \\citet{qtransformer} introduced a novel softmax function aimed at pretraining models in an outlier-free manner, thereby enhancing their suitability for quantization. Interestingly, we observed that such an approach leads to performance degradation in full precision. Building on this insight, we enhance the method by ensuring its normalization is invariant to sequence length, a crucial factor for bridging the gap between pretraining and fine-tuning. Moreover, this improved method also facilitates successful pretraining of causal language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ikucyr7shBv": {
    "title": "STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models",
    "volume": "review",
    "abstract": "Though Large Language Models (LLMs) have demonstrated the powerful capabilities of few-shot learning through prompting methods, supervised training is still necessary for complex reasoning tasks. Because of their extensive parameters and memory consumption, both Parameter-Efficient Fine-Tuning (PEFT) methods and Memory-Efficient Fine-Tuning methods have been proposed for LLMs. Nevertheless, the issue of large annotated data consumption, the aim of Data-Efficient Fine-Tuning, remains unexplored. One obvious way is to combine the PEFT method with active learning. However, the experimental results show that such a combination is not trivial and yields inferior results. Through probe experiments, such observation might be explained by two main reasons: uncertainty gap and poor model calibration. Therefore, in this paper, we propose a novel approach to effectively integrate uncertainty-based active learning and LoRA. Specifically, for the uncertainty gap, we introduce a dynamic uncertainty measurement that combines the uncertainty of the base model and the uncertainty of the full model during the iteration of active learning. For poor model calibration, we incorporate the regularization method during LoRA training to keep the model from being over-confident, and the Monte-Carlo dropout mechanism is employed to enhance the uncertainty estimation. Experimental results show that the proposed approach outperforms existing baseline models on three complex reasoning tasks",
    "checked": true,
    "id": "8089b431b2e09c27967428fb542c0935fb95ec30",
    "semantic_title": "star: constraint lora with dynamic active learning for data-efficient fine-tuning of large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=jYPofxbFJNR": {
    "title": "Efficient Domain Adaptation for Non-Autoregressive Machine Translation",
    "volume": "review",
    "abstract": "Domain adaptation remains a challenge in the realm of Neural Machine Translation (NMT), even in the era of large language models (LLMs). Existing non-parametric approaches like nearest neighbor machine translation have made small Autoregressive Translation (AT) models achieve efficient domain generalization and adaptation without updating parameters, but leaving the Non-Autoregressive Translation (NAT) counterparts under-explored. To fill this blank, we introduce $Bi$-$k$NN, an innovative and efficient domain adaptation approach for NAT models that tailors a k-nearest-neighbor algorithm for NAT. Specifically, we introduce an effective datastore construction and correlated updating strategies to conform the parallel nature of NAT. Additionally, we train a meta-network that seamlessly integrates the $k$NN distribution with the NMT distribution robustly during the iterative decoding process of NAT. Our experimental results across four benchmark datasets demonstrate that our $Bi$-$k$NN not only achieves significant improvements over the Base-NAT model (7.8 BLEU on average) but also exhibits enhanced efficiency. All the implementation details of this work will be publicly accessible at https://anonymous/",
    "checked": false,
    "id": "9b068f8b7bcfd60f841a8707e9d37ce9bbc2f0d1",
    "semantic_title": "efficient cluster-based k-nearest-neighbor machine translation",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=Og0zmSOFEI0": {
    "title": "Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models",
    "volume": "review",
    "abstract": "This paper identifies a cultural dominance issue within large language models (LLMs) due to the predominant use of English data in model training (e.g., ChatGPT). LLMs often provide inappropriate English-culture-related answers that are not relevant to the expected culture when users ask in non-English languages. To systematically evaluate the cultural dominance issue, we build a benchmark of concrete (e.g., holidays and songs) and abstract (e.g., values and opinions) cultural objects. Empirical results show that the representative GPT models suffer from the culture dominance problem, where GPT-4 is the most affected while text-davinci-003 suffers the least from this problem. Our study emphasizes the need to critically examine cultural dominance and ethical considerations in their development and deployment. We show that two straightforward methods in model development (i.e., pretraining on more diverse data) and deployment (e.g., culture-aware prompting) can significantly mitigate the cultural dominance issue in LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rWPw24O8VPT": {
    "title": "FAST: Feed-Forward Assisted Transformers for Time Efficient Fine-Tuning",
    "volume": "review",
    "abstract": "Fine-tuning is the standard approach when adapting pre-trained large language models for specific downstream tasks. However, the energy and time required to fully fine-tune all parameters can become prohibitively large for many applications as the size of the model increases. While recent advancements in parameter-efficient transfer learning have reduced the number of parameters that need to be updated, the training time and energy consumption of these methods remain similar to full fine-tuning. In this paper, we propose a time-efficient fine-tuning method based on feature-extraction in which we treat off-the-shelf language models as fixed sources of embeddings and train small feed-forward networks on top for each downstream task. Averaging across the GLUE NLI benchmark, our method trains $124$ times faster than full fine-tuning and $101$ times faster than parameter-efficient fine-tuning methods using distilRoBERTa, while achieving 81.9% and 85.0% performance respectively",
    "checked": false,
    "id": "4b5cbb924f06763a3c785d0ccfb3bc8bd765f4a5",
    "semantic_title": "brainformers: trading simplicity for efficiency",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=R3IVm5Oh6fT": {
    "title": "Hi-Plan: Hierarchical Text-Planning for Argumentative Essay Generation",
    "volume": "review",
    "abstract": "Argumentative Essay Generation (AEG) aims to generate a complete argumentative text on a specific controversial topic or debate. Due to the need to organize arguments into a complete chapter, AEG is challenging in terms of article structure. Existing approaches use text-planning methods to augment generative language models, neglecting the necessary attention to the hierarchical logical structure of argumentative texts. This causes difficulties in generating a complete logic flow, significantly reducing the persuasiveness of argumentative texts. To solve this problem, we designed a hierarchical text-planning method: Hi-Plan, introducing the structural and logical information of argumentative texts into text-planning explicitly. To be more specific, it allows this method to control the logical relevance in a high-level, fine-grained way simultaneously. Since Hi-Plan requires a fine-grained article outline, we propose an automated data augmentation method to extend existing AEG datasets. The hierarchical text planning and fine-grained data augmentation methods form a complete cycle from data construction to model training. Experiments show that our proposed method outperforms strong baseline models in terms of logical validity and persuasiveness and achieves comparable results with super-large scale pre-trained language models, such as ChatGPT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2frR7yVo3dC": {
    "title": "How Proficient Are Large Language Models in Formal Languages? An In-Depth Insight for Knowledge Base Question Answering",
    "volume": "review",
    "abstract": "Knowledge Base Question Answering (KBQA) aims to answer natural language questions based on facts in knowledge bases. A typical approach to KBQA is semantic parsing, which translates a question into an executable logical form in a formal language. Recent works leverage the capabilities of large language models (LLMs) for logical form generation to improve performance. However, although it is validated that LLMs are capable of solving some KBQA problems, there has been little discussion on the differences in LLMs' proficiency in formal languages used in semantic parsing. In this work, we propose to evaluate the understanding and generation ability of LLMs to deal with differently structured logical forms by examining the inter-conversion of natural and formal language through in-context learning of LLMs. Extensive experiments with models of different sizes show that state-of-the-art LLMs can understand formal languages as well as humans, but generating correct logical forms given a few examples remains a challenge. Most importantly, our results also indicate that LLMs exhibit considerable sensitivity. In general, the formal language with a lower formalization level, i.e., the more similar it is to natural language, is more friendly to LLMs",
    "checked": true,
    "id": "7c6964bdf763eead086f4dfebffd26e9c8d3f49d",
    "semantic_title": "how proficient are large language models in formal languages? an in-depth insight for knowledge base question answering",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=cCIbcVJB_5X": {
    "title": "Towards Real-world Scenario: Imbalanced New Intent Discovery",
    "volume": "review",
    "abstract": "New Intent Discovery (NID) aims at detecting known and previously undefined categories of user intent by utilizing limited labeled and massive unlabeled data. Most prior works often operate under the unrealistic assumption that the distribution of both familiar and new intent classes is uniform, overlooking the skewed and long-tailed distributions frequently encountered in real-world scenarios. To bridge the gap, our work introduces the imbalanced new intent discovery i-NID task, which seeks to identify familiar and novel intent categories within long-tailed distributions. A new benchmark baNID-Bench comprised of three datasets is created to simulate the real-world long-tail distributions. mbaNID-Bench ranges from broad cross-domain to specific single-domain intent categories, providing a thorough representation of practical use cases. Besides, a robust baseline model ImbaNID is proposed to achieve cluster-friendly intent representations. It includes three stages: model pre-training, generation of reliable pseudo-labels, and robust representation learning that strengthens the model performance to handle the intricacies of real-world data distributions. Our extensive experiments on previous benchmarks and the newly established benchmark demonstrate the superior performance of ImbaNID in addressing the i-NID task, highlighting its potential as a powerful baseline for uncovering and categorizing user intents in imbalanced and long-tailed distributions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n2iux5yAClM": {
    "title": "Do LLMs Implicitly Determine the Suitable Text Difficulty for Users?",
    "volume": "review",
    "abstract": "Education that suits the individual learning level is necessary to improve students' understanding. The first step in achieving this purpose by using large language models (LLMs) is to adjust the textual difficulty of the response to students.This work analyzes how LLMs can implicitly adjust text difficulty between user input and its generated text. To conduct the experiments, we created a new dataset from Stack-Overflow to explore the performance of question-answering-based conversation.Experimental results on the Stack-Overflow dataset and the TSCC dataset, including multi-turn conversation show that LLMs can implicitly handle text difficulty between user input and its generated response. We also observed that some LLMs can surpass humans in handling text difficulty and the importance of instruction-tuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZITc9wUaUgZ": {
    "title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
    "volume": "review",
    "abstract": "Hypothetical induction is recognized as the main reasoning type when scientists make observations about the world and try to propose hypotheses to explain those observations. Past research on hypothetical induction is under a constrained setting: (1) the observation annotations in the dataset are carefully manually handpicked sentences (resulting in a close-domain setting); and (2) the ground truth hypotheses are mostly commonsense knowledge, making the task less challenging. In this work, we tackle these problems by proposing the first NLP dataset for social science academic hypotheses discovery, consisting of 50 recent top social science publications; and a raw web corpus that contains enough information to make it possible to develop all the research hypotheses in the 50 papers. The final goal is to create systems that automatically generate valid, novel, and helpful scientific hypotheses, given only a pile of raw web corpus. Different from the previous settings, the new dataset requires (1) using open-domain data (raw web corpus) as observations; and (2) proposing hypotheses even new to humanity. A multi-module framework is developed for the task, as well as three different feedback mechanisms that empirically show performance gain over the base framework. Finally, our framework exhibits superior performance in terms of both GPT-4 based evaluation and expert-based evaluation",
    "checked": true,
    "id": "5aea5c8b536380c5ad1d42108c2c6767622318ee",
    "semantic_title": "large language models for automated open-domain scientific hypotheses discovery",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=idp_1Q6F-lC": {
    "title": "Causal-Guided Active Learning for Debiasing Large Language Models",
    "volume": "review",
    "abstract": "Although achieving promising performance, recent analyses show that current generative large language models (LLMs) may still capture dataset biases and utilize them for generation, leading to poor generalizability and harmfulness of LLMs. However, due to the diversity of dataset biases and the over-optimization problem, previous prior-knowledge-based debiasing methods and fine-tuning-based debiasing methods may not be suitable for current LLMs.To address this issue, we explore combining active learning with the causal mechanisms and propose a casual-guided active learning (CAL) framework, which utilizes LLMs itself to automatically and autonomously identify informative biased samples and induce the bias patterns. Then a cost-effective and efficient in-context learning based method is employed to prevent LLMs from utilizing dataset biases during generation.Experimental results show that CAL can effectively recognize typical biased instances and induce various bias patterns for debiasing LLMs",
    "checked": false,
    "id": "0425c47e19b5f1fcc680967ebd6c6e7cebc0b768",
    "semantic_title": "aligning as debiasing: causality-aware alignment via reinforcement learning with interventional feedback",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wPZ1C7cIfr0": {
    "title": "Zero-Shot Fact Verification via Natural Logic and Large Language Models",
    "volume": "review",
    "abstract": "The recent development of fact verification systems with natural logic has enhanced the explainability of these systems by aligning claims with evidence through set-theoretic operators, providing justifications that faithfully expose the model's reasoning. Despite these advancements, such systems often rely on a large amount of training data annotated with natural logic. To address this issue, we propose a zero-shot method that utilizes the generalization capabilities of instruction-tuned large language models. Our system uses constrained decoding to mitigate hallucinations and employs weighted prompt ensembles to improve stability. We evaluate our system on artificial and real-world fact verification data. In a zero-shot setup where models were not trained on any data annotated with natural logic, our method surpasses the best baselines by an average of 7.52 accuracy points. We also demonstrate multilingual capabilities in other languages, such as Danish, where we outperform our baselines by 8.72 accuracy points",
    "checked": false,
    "id": "09238228c405323bcb20232f424258a8872f150c",
    "semantic_title": "few-shot image classification by generating natural language rules",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=f8bVwiNm7Cd": {
    "title": "Sample Efficiency Matters: Training Multimodal Conversational Recommendation Systems in a Low Resource Setting",
    "volume": "review",
    "abstract": "Multi-modal conversational recommendation (multi-modal CRS) can potentially revolutionize how customers interact with e-commerce platforms. Yet conversational samples, as training data for such a system, are difficult to obtain in large quantities, particularly in new platforms. Motivated by this challenge, we consider multimodal CRS in a low resource setting. Specifically, assuming the availability of a small number of samples with dialog states, we devise an effective dialog state encoder to bridge the semantic gap between conversation and product representations for recommendation. To reduce the cost of dialog state annotation, a semi-supervised learning method is developed to effectively train the dialog state encoder with a smaller set of labeled conversations. In addition, we design a correlation regularisation that leverages knowledge in the multi-modal domain database to better align textual and visual modalities. Experiments on two datasets demonstrate the effectiveness of our method. Particularly, with only 5% of the MMD training set, our method (namely SeMANTIC) is comparable to the state-of-the-art model trained on the full dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rFLUCvgJ2dp": {
    "title": "LLMCrit: Teaching Large Language Models to Use Criteria",
    "volume": "review",
    "abstract": "Humans follow criteria when they execute tasks, and these criteria are directly used to assess the quality of task completion. Therefore, having models learn to use criteria to provide feedback can help humans or models to perform tasks better. However, current research in this area tends to consider only a limited number of criteria, or only a limited number of quality assessment aspects. To fill this gap, we propose a general framework that enables large language models (LLMs) to use comprehensive criteria for a task in delivering natural language feedback on task execution. In particular, we present a model-in-the-loop framework that semi-automatically derives criteria from collected guidelines for different writing tasks and constructs in-context demonstrations for each criterion. We choose three tasks from real-world scenarios to operationalize this idea: paper introduction writing, Python code writing, and Reddit post writing, and evaluate our feedback generation framework using different LLMs. The results reveal the fine-grained effects of adding criteria and demonstrations and provide valuable guidance on how to teach LLMs to use criteria more effectively",
    "checked": true,
    "id": "29a56e1c377ac6a4457656b57ef7631ed2bdb509",
    "semantic_title": "llmcrit: teaching large language models to use criteria",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=iuISU9LBvel": {
    "title": "Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models",
    "volume": "review",
    "abstract": "Generative large language models (LLMs), e.g., ChatGPT, have demonstrated remarkable proficiency across several NLP tasks, such as machine translation, text summarization. Recent research (Kocmi and Federmann, 2023) has shown that u has shown that utilizing LLMs for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but performs poorly at the segment level. To further improve the performance of LLMs on MT quality assessment, we conduct an investigation into several prompting designs, and propose a new prompting method called Error Analysis Prompting (EAPrompt) by combining Chain-of-Thoughts (Wei et al., 2022) and Error Analysis (Lu et al., 2023). This technique emulates the commonly accepted human evaluation framework - Multidimensional Quality Metrics (MQM, Freitag et al., (2021)) and produces explainable and reliable MT evaluations at both the system and segment level. Experimental Results from WMT22 metrics shared task validate the effectiveness of EAPrompt on various LLMs, with different structures. Further analysis confirms that EAPrompt effectively distinguishes major errors from minor ones, while also sharing a similar distribution of the number of errors with MQM. These findings highlight the potential of EAPrompt as a human-like evaluator prompting technique for MT evaluation. We will release our code and scripts to facilitate the community",
    "checked": false,
    "id": "8bc313e04cbd39847eb50b22af0a698ff2971a35",
    "semantic_title": "error analysis prompting enables human-like translation evaluation in large language models: a case study on chatgpt",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=g9SisVyz6tY": {
    "title": "Studying Differential Mental Health Expressions in India",
    "volume": "review",
    "abstract": "Psychosocial stressors and the symptomatology of mental disorders are known to vary with socio-cultural environment. Mental health expressions on social media, however, are primarily informed by studies in the WEIRD (Western, Educated, Industrial, Rich, and Democratic) contexts. In this paper, we analyze mental health posts made on Reddit by individuals in India, the most populous country in the World, to identify psycho-social categories and themes specific to the Indian context compared to Reddit users located in the Rest of the World (ROW), predominantly the United States. Contrary to findings in Western samples, mental health discussions in India are present-focused and are about work and achievement-related topics. Psycho-social category, \\textit{illness} is exclusively correlated with mental health posts originating from India, reaffirming the link between somatic symptoms and mental disorders in Indian patients. Two clinical psychologists practicing in India labeled 95\\% of the top-20 topics associated with mental health discussions as \\textit{prevalent} in Indians. Significant linguistic variations in online mental health-related language originating from India vs. ROW, highlight the need for precision culturally-aware machine learning models. These findings have important implications for designing culturally appropriate interventions to reduce the growing diagnosis and treatment gap for mental disorders in India",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cxP9Xfl67Ny": {
    "title": "Synthetic Data Generation for Intersectional Fairness by Leveraging Hierarchical Group Structure",
    "volume": "review",
    "abstract": "In this paper, we introduce a data augmentation approach specifically tailored to enhance intersectional fairness in classification tasks. Our method capitalizes on the hierarchical structure inherent to intersectionality, by viewing groups as intersections of their parent categories. This perspective allows us to augment data for smaller groups by learning a transformation function that combines data from these parent groups. Our empirical analysis, conducted on four diverse datasets including both text and images, reveals that classifiers trained with this data augmentation approach achieve superior intersectional fairness and are more robust to ``leveling down'' when compared to methods optimizing traditional group fairness metrics",
    "checked": true,
    "id": "aa6f1eed851fc4bb6d87606cac5fd0ef2334aa99",
    "semantic_title": "synthetic data generation for intersectional fairness by leveraging hierarchical group structure",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dFG4kLm9wkS": {
    "title": "Distractor Generation for Multiple-Choice Questions: A Survey of Methods, Datasets, and Evaluation",
    "volume": "review",
    "abstract": "Distractors as part of multiple-choice question (MCQ) are vital in learning evaluation and are commonly used in education across a variety of domains such as Science, English, and Mathematics. The advancement of artificial intelligence (AI) has enabled the Distractor Generation (DG) problem to progress from traditional methods into advanced neural networks and pre-trained models. This survey paper reviews DG tasks using English MCQ datasets for textual and multi-modal contexts. In particular, this paper presents a thorough literature review of the recent methods on DG tasks, discusses multiple choice components and their characteristics, analyzes the related datasets, summarizes the evaluation metrics, indicates current findings noticed from exiting benchmarks and methods, and highlights the challenges and open issues",
    "checked": true,
    "id": "2f949cb7375c7bf8a884d492018d61fceefdb847",
    "semantic_title": "distractor generation for multiple-choice questions: a survey of methods, datasets, and evaluation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=JKABd3Fbt7r": {
    "title": "It is Simple Sometimes: A Study On Improving Aspect-Based Sentiment Analysis Performance",
    "volume": "review",
    "abstract": "Aspect-Based Sentiment Analysis (ABSA) involves extracting opinions from textual data about specific entities and their corresponding aspects through various complementary subtasks. Several prior research has focused on developing ad hoc designs of varying complexities for these subtasks. In this paper, we build upon the instruction tuned model proposed by Scaria et al. (2023), who present an instruction-based model with task descriptions followed by in-context examples on ABSA subtasks. We propose PFInstruct, an extension to this instruction learning paradigm by appending an NLP-related task prefix to the task description. This simple approach leads to improved performance across all tested SemEval subtasks, surpassing previous state-of-the-art (SOTA) on the ATE subtask (Rest14) by +3.28 F1-score, and on the AOOE subtask by an average of +5.43 F1-score across SemEval datasets. Furthermore, we explore the impact of the prefix-enhanced prompt quality on the ABSA subtasks and find that even a noisy prefix enhances model performance compared to the baseline. Our method also achieves competitive results on a biomedical domain dataset (ERSA)",
    "checked": true,
    "id": "9cde9edac51e3669f25c01757f3c313fa63b0cd1",
    "semantic_title": "it is simple sometimes: a study on improving aspect-based sentiment analysis performance",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NOr-J4BgH7": {
    "title": "Revisiting Knowledge Distillation for Autoregressive Language Models",
    "volume": "review",
    "abstract": "Knowledge distillation (KD) is a common approach to compress a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, in the context of autoregressive language models (LMs), we empirically find that larger teacher LMs might dramatically result in a poorer student. In response to this problem, we conduct a series of analyses and reveal that different tokens have different teaching modes, neglecting which will lead to performance degradation. Motivated by this, we propose a simple yet effective adaptive teaching approach (ATKD) to improve the KD. The core of ATKD is to reduce rote learning and make teaching more diverse and flexible. Extensive experiments on 8 LM tasks show that, with the help of ATKD, various baseline KD methods can achieve consistent and significant performance gains (up to +3.04% average score) across all model types and sizes. More encouragingly, ATKD can improve the student model generalization effectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tbJg5QXjuuY": {
    "title": "On the Investigation of Evolutionary Multi-Objective Optimization for Discrete Prompt Search",
    "volume": "review",
    "abstract": "Discrete prompt search (DPS) aims to automatically find high-performing prompts that yield top accuracy in interactions with a pretrained language model. In the context of few-shot learning, evaluations of candidate prompts can only be done via a limited number of labelled examples. The search is often formulated as an optimization problem where prediction accuracy, F1 score, or cross-entropy loss is used as the objective function. While resulting prompts achieve top performance, they are mostly unreadable and uninterpretable, i.e., unlike natural languages. In this paper, we formulate DPS as a true multi-objective optimization (MOO) problem considering simultaneously both prompt performance and readability as separate objectives. We show that there exist certain degrees of conflict between the objectives, making the search for human-readable and highly-accurate prompts a challenging problem. We then propose the Multi-objective Evolutionary Algorithm for Predictive Probability guided Prompting (MoEAP3) to address the problem. Our MoEAP3 returns not a single final prompt as in conventional methods but a whole front of multiple candidate prompts, each representing an efficient trade-off between the objectives. Decision makers can straightforwardly investigate this front and intuitively select the prompt that yields the desired trade-off. Experimental results exhibit the superiority of MoEAP3 over state-of-the-art baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4LxQ25oiRY": {
    "title": "WaveCoder: Widespread And Versatile Enhancing Code Large Language Models By Instruction Tuning",
    "volume": "review",
    "abstract": "Recent work demonstrates that, after instruction tuning, Code Large Language Models (Code LLMs) can obtain impressive capabilities to address a wide range of code-related tasks. However, current instruction tuning methods for Code LLMs mainly focus on the traditional code generation task, resulting in poor performance in complex multi-task scenarios. In this paper, we concentrate on multiple code-related tasks and present WaveCoder, a series of Code LLMs trained with Widespread And Versatile Enhanced instruction data. To enable the models to tackle complex code-related tasks, we propose a method to stably generate diverse, high-quality instruction data from open source code dataset in multi-task scenarios and obtain CodeOcean, a dataset comprising 19,915 instruction instances across 4 code-related tasks, which is aimed at improving the generalization ability of Code LLM. Our experiments demonstrate that WaveCoder models significantly outperform other open-source models in terms of the generalization ability across different code-related tasks. Moreover, WaveCoder-Ultra-6.7B presents the state-of-the-art generalization abilities on a wide range of code-related tasks",
    "checked": false,
    "id": "ca60e350cdff7b010b6cc1f53bdec46aecf2fa0b",
    "semantic_title": "wavecoder: widespread and versatile enhancement for code large language models by instruction tuning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q5nlc2reOh5": {
    "title": "Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution",
    "volume": "review",
    "abstract": "Large language models (LLMs) reflect societal norms and biases, especially about gender. While societal biases and stereotypes have been extensively researched in various NLP applications, there is a surprising gap for emotion analysis. However, emotion and gender are closely linked in societal discourse. E.g., women are often thought of as more empathetic, while men's anger is more socially accepted. To fill this gap, we present the first comprehensive study of gendered emotion attribution in five state-of-the-art LLMs (open- and closed-source). We investigate whether emotions are gendered, and whether these variations are based on societal stereotypes. We prompt the models to adopt a gendered persona and attribute emotions to an event like â€˜When I had a serious argument with a dear person'. We then analyze the emotions generated by the models in relation to the gender-event pairs. We find that all models consistently exhibit gendered emotions, influenced by gender stereotypes. These findings are in line with established research in psychology and gender studies. Our study sheds light on the complex societal interplay between language, gender, and emotion. The reproduction of emotion stereotypes in LLMs allows us to use those models to study the topic in detail, but raises questions about the predictive use of those same LLMs for emotion applications",
    "checked": true,
    "id": "52db5adffa53911c20b7cd884e8a2f2151a3c114",
    "semantic_title": "angry men, sad women: large language models reflect gendered stereotypes in emotion attribution",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=eTsl0kizy4": {
    "title": "Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have become a cornerstone in the field of Natural Language Processing (NLP), offering transformative capabilities in understanding and generating human-like text. However, with their rising prominence, the security and vulnerability aspects of these models have garnered significant attention. This paper presents a comprehensive survey of the various forms of attacks targeting LLMs, discussing the nature and mechanisms of these attacks, their potential impacts, and current defense strategies. We delve into topics such as adversarial attacks that aim to manipulate model outputs, data poisoning that affects model training, and privacy concerns related to training data exploitation. The paper also explores the effectiveness of different attack methodologies, the resilience of LLMs against these attacks, and the implications for model integrity and user trust. By examining the latest research, we provide insights into the current landscape of LLM vulnerabilities and defense mechanisms. Our objective is to offer a nuanced understanding of LLM attacks, foster awareness within the AI community, and inspire robust solutions to mitigate these risks in future developments",
    "checked": true,
    "id": "8b1378a728ac223309e5e4c6d2006654b2d469bf",
    "semantic_title": "breaking down the defenses: a comparative survey of attacks on large language models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=0o1AaArknNH": {
    "title": "Fennec: Fine-grained Language Model Evaluation and Correction Extended through Branching and Bridging",
    "volume": "review",
    "abstract": "The rapid advancement of large language models has given rise to a plethora of applications across a myriad of real-world tasks, mainly centered on aligning with human intent. However, the complexities inherent in human intent necessitate a dependence on labor-intensive and time-consuming human evaluation. To alleviate this constraint, we delve into the paradigm of employing open-source large language models as evaluators, aligning with the prevailing trend of utilizing GPT-4. Particularly, we present a step-by-step evaluation framework: \\textbf{Fennec}, capable of \\textbf{F}ine-grained \\textbf{E}valuatio\\textbf{N} and correctio\\textbf{N} \\textbf{E}xtended through bran\\textbf{C}hing and bridging. Specifically, the branching operation dissects the evaluation task into various dimensions and granularities, thereby alleviating the challenges associated with evaluation. Concurrently, the bridging operation amalgamates diverse training datasets, augmenting the variety of evaluation tasks. In experimental trials, our 7B model consistently outperforms open-source larger-scale evaluation models across various widely adopted benchmarks in terms of both \\textit{Agreement} and \\textit{Consistency}, closely approaching the capabilities of GPT-4. We employ the fine-grained correction capabilities induced by the evaluation model to refine multiple model responses, and the results show that the refinement substantially elevates the quality of responses, leading to an improvement of 1-2 points on the MT-Bench dataset",
    "checked": true,
    "id": "08b790aa8ec96ab5b57a865ec9d7f2e7a599069e",
    "semantic_title": "fennec: fine-grained language model evaluation and correction extended through branching and bridging",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UYjH8iDMF1": {
    "title": "Question-Instructed Visual Descriptions for Zero-Shot Video Answering",
    "volume": "review",
    "abstract": "We present Q-ViD, a simple approach for video question answering (video QA), that unlike prior methods, which are based on complex architectures, computationally expensive pipelines or use closed models like GPTs, Q-ViD relies on a single instruction-aware open vision-language model (InstructBLIP) to tackle videoQA using frame descriptions. Specifically, we create captioning instruction prompts that rely on the target questions about the videos and leverage InstructBLIP to obtain video frame captions that are useful to the task at hand. Subsequently, we form descriptions of the whole video using the question-dependent frame captions, and feed that information, along with a question-answering prompt, to a large language model (LLM). The LLM is our reasoning module, and performs the final step of multiple-choice QA. Our simple Q-ViD framework achieves competitive or even higher performances than current state of the art models on a diverse range of videoQA benchmarks, including NExT-QA, STAR, How2QA, TVQA and IntentQA",
    "checked": false,
    "id": "3871fe3b83090bebf3eba40a3afa4e0b66a3f165",
    "semantic_title": "question-instructed visual descriptions for zero-shot video question answering",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=7fo7yixup1": {
    "title": "CoderGen: Towards Domain-Specific Code Generation of Large Language Models",
    "volume": "review",
    "abstract": "Automated code generation is a pivotal capability of large language models (LLMs). However, assessing this capability in real-world scenarios remains challenging. Previous methods focus more on low-level code generation, such as model loading, instead of generating high-level codes catering for real-world tasks, such as image-to-text, text classification, in various domains.Therefore, we construct AICoderEval, a dataset focused on real-world tasks in various domains based on HuggingFace, PyTorch, and TensorFlow, along with comprehensive metrics for evaluation and enhancing LLMs' task-specific code generation capability.After that, we propose CoderGen, an agent-based framework, to help LLMs generate codes related to real-world tasks on the constructed AICoderEval. Moreover, we train a more powerful task-specific code generation model, named AICoder, which is refined on codellama based on AICoderEval.Our experiments demonstrate the effectiveness of CoderGen in improving LLMs' task-specific code generation capability (by 30.26\\% on SR@All and 19.88\\% on SR@Any).And the proposed AICoder also outperform the current code generation LLMs, indicating the great quality of the AICoderEval benchmark for evaluation and enhancing LLMs' task-specific code generation capability",
    "checked": false,
    "id": "f5d491bc7fe079177afa9534610a706d2ea1d85f",
    "semantic_title": "aicodereval: improving ai domain code generation of large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ga8i0lCPBG": {
    "title": "Scaffolding Coordinates to Promote Vision-Language Coordination in Large Multi-Modal Models",
    "volume": "review",
    "abstract": "State-of-the-art Large Multi-Modal Models (LMMs) have demonstrated exceptional capabilities in vision-language tasks. Despite their advanced functionalities, the performances of LMMs are still limited in challenging scenarios that require complex reasoning with multiple levels of visual information. Existing prompting techniques for LMMs focus on either improving textual reasoning or leveraging tools for image preprocessing, lacking a simple and general visual prompting scheme to promote vision-language coordination in LMMs. In this work, we propose Scaffold prompting that scaffolds coordinates to promote vision-language coordination. Specifically, Scaffold overlays a dot matrix within the image as visual information anchors and leverages multi-dimensional coordinates as textual positional references. Extensive experiments on a wide range of challenging vision-language tasks demonstrate the superiority of Scaffold over GPT-4V with the textual CoT prompting",
    "checked": true,
    "id": "fd309bff5f99c6f5c3dbe51bb68b4beeec475bd8",
    "semantic_title": "scaffolding coordinates to promote vision-language coordination in large multi-modal models",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=K4eI3Qt_Ki": {
    "title": "RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph",
    "volume": "review",
    "abstract": "In knowledge graph embedding, leveraging relation-specific entity-transformation has markedly enhanced performance. However, this approach lacks assurance for consistent changes in relation and entity embeddings due to the disconnected entity-transformation representation, missing valuable inductive bias among semantically similar relations. Furthermore, a generalized plug-in approach as a SFBR disrupts this consistency through excessive concentration of entity embeddings under entity-based regularization, generating indistinguishable score distributions among relations. To tackle these challenges, we introduce Relation-Semantics Consistent Filter (RSCF), characterized by three features: 1) shared affine transformation of relation embeddings across all relations, 2) change-based entity-transformation that adds an entity embedding to its change represented by the transformed vector, and 3) normalization of the change to prevent scale reduction. In knowledge graph completion tasks with distance-based and tensor decomposition models, RSCF notably enhances performance across all relations, particularly in rare relations in long-tailed distribution",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YqI65K2Fm59": {
    "title": "HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts",
    "volume": "review",
    "abstract": "The Mixture of Experts (MoE) for language models has been proven effective in augmenting the capacity of models by dynamically routing each input token to a specific subset of experts for processing. Despite the success, most existing methods face a challenge for balance between sparsity and the availability of expert knowledge: enhancing performance through increased use of expert knowledge often results in diminishing sparsity during expert selection. To mitigate this contradiction, we propose HyperMoE, a novel MoE framework built upon Hypernetworks. This framework integrates the computational processes of MoE with the concept of knowledge transferring in multi-task learning. Specific modules generated based on the information of unselected experts serve as supplementary information, which allows the knowledge of experts not selected to be used while maintaining selection sparsity. Our comprehensive empirical evaluations across multiple datasets and backbones establish that HyperMoE significantly outperforms existing MoE methods under identical conditions concerning the number of experts. We have anonymized our code and uploaded it into the supplementary materials",
    "checked": true,
    "id": "022083382f23b3abcd4d93fe07741d7ffdc42f11",
    "semantic_title": "hypermoe: towards better mixture of experts via transferring among experts",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=6T10wkb4uS4": {
    "title": "Generating and Evaluating Long Story Summaries with Knowledge Graphs",
    "volume": "review",
    "abstract": "Summarizing long stories is a challenging task due to their narrative complexity and the context length limits of language models. We propose a method that integrates knowledge graph retrieval with the summarization process to provide global context. We construct a knowledge graph containing entity descriptions and relations from the entire story, then retrieve relevant information from it to aid summary generation. Additionally, we propose a novel metric, KGScore, which evaluates summaries by comparing the similarity of knowledge graphs extracted from generated and reference summaries. Experimental results demonstrate that our knowledge graph retrieval method outperforms the baselines in terms of our KGScore metric and that KGScore is a reliable measure of factual consistency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XLEoKmifTor": {
    "title": "Leveraging Bag-of-Keywords loss for Training and Evaluation of Open-Domain Dialogue Systems",
    "volume": "review",
    "abstract": "The standard language modeling (LM) loss by itself has been shown to be inadequate for effective dialog modeling. As a result, various training approaches, such as auxiliary loss functions and leveraging human feedback, are being adopted to enrich open-domain dialogue systems. One such auxiliary loss function is Bag-of-Words (BoW) loss, defined as the cross-entropy loss for predicting all the words/tokens of the next utterance. This work aims to enhance the BoW loss by introducing the Bag-of-Keywords (BoK) loss. BoK computes the loss for predicting keywords or critical words/tokens in the next utterance, intending to estimate the core idea rather than the entire response. We incorporate BoK loss in both encoder-decoder (T5) and decoder-only (DialoGPT) architecture and train the models to minimize the weighted sum of BoK and LM (BoK-LM) loss. We perform our experiments on two popular open-domain dialog datasets, DailyDialog and Persona-Chat. We show that the inclusion of BoK loss improves the dialogue generation of backbone models, exhibiting improvement in the specificity of the generated responses. We also discuss interpretability as a by-product of using BoK loss. Finally, we study the effectiveness of BoK-LM loss as a reference-free metric and observe comparable performance to the state-of-the-art metrics on various dialog evaluation datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MRLzX498_M": {
    "title": "SOTOPIA-Ï€: Interactive Learning of Socially Intelligent Language Agents",
    "volume": "review",
    "abstract": "Humans learn social skills through both imitation and social interaction. This social learning process is largely understudied by existing research on building language agents. Motivated by this gap, we propose an interactive learning method, SOTOPIA-Ï€, that improves the social intelligence of language agents. This method leverages behavior cloning and self-reinforcement based training on filtered social interaction data according to large language model (LLM) rating. We show that our training method allows a 7B LLM to reach the social goal completion ability of an expert model (GPT-4-based agent) without the loss of more generic abilities, such as the ability to answer knowledge-based questions. We also demonstrate that this training paradigm uncovers some weaknesses in standard evaluation and safety training paradigms that (1) LLM-based evaluation of social intelligence overestimates the abilities of the language agents trained specifically for social interaction, and that (2) despite not training for better safety or question answering (QA) ability, our methods improve the safety of language agents and maintain general QA ability on the MMLU benchmark",
    "checked": true,
    "id": "77e3b253c2deeb2661ffcb9fb9f952ceb700c4db",
    "semantic_title": "sotopia-Ï€: interactive learning of socially intelligent language agents",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=GMD3OaMaLLb": {
    "title": "Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics",
    "volume": "review",
    "abstract": "Large language models have achieved remarkable success in general language understanding tasks. However, as a family of generative methods with the objective of next token prediction, the semantic evolution with the depth of these models are not fully explored, unlike their predecessors, such as BERT-like architectures. In this paper, we specifically investigate the bottom-up evolution of lexical semantics for a popular LLM, namely Llama2, by probing its hidden states at the end of each layer using a contextualized word identification task. Our experiments show that the representations in lower layers encode lexical semantics, while the higher layers, with weaker semantic induction, are responsible for prediction. This is in contrast to models with discriminative objectives, such as mask language modeling, where the higher layers obtain better lexical semantics. The conclusion is further supported by the monotonic increase in performance via the hidden states for the last meaningless symbols, such as punctuation, in the prompting strategy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MgdvAPGJtTX": {
    "title": "One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation",
    "volume": "review",
    "abstract": "Evaluation of opinion summaries using conventional reference-based metrics rarely provides a holistic evaluation and has been shown to have a relatively low correlation with human judgments. Recent studies suggest using Large Language Models (LLMs) as reference-free metrics for NLG evaluation, however, they remain unexplored for opinion summary evaluation. Moreover, limited opinion summary evaluation datasets inhibit progress. To address this, we release the SUMMEVAL-OP dataset covering 7 dimensions related to the evaluation of opinion summaries: fluency, coherence, relevance, faithfulness, aspect coverage, sentiment consistency, and specificity. We investigate Op-I-Prompt a dimension-independent prompt, and Op-Prompts, a dimension-dependent set of prompts for opinion summary evaluation. Experiments indicate that Op-I-Prompt emerges as a good alternative for evaluating opinion summaries achieving an average Spearman correlation of 0.70 with humans, outperforming all previous approaches. To the best of our knowledge, we are the first to investigate LLMs as evaluators on both closed-source and open-source models in the opinion summarization domain",
    "checked": true,
    "id": "852e37e5e10e8930a224d412bb8992027acbcd55",
    "semantic_title": "one prompt to rule them all: llms for opinion summary evaluation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rbueJg672E": {
    "title": "Unsupervised Learning of Morphology with Multiple Codebook VQ-VAE",
    "volume": "review",
    "abstract": "This paper presents an interpretable unsupervised morphological learning model, showing comparable performance to supervised models in learning complex morphological rules as evidenced by its application to the problem of morphological inflection within the SIGMORPHON Shared Tasks. The significance of our unsupervised approach lies in its alignment with how humans naturally acquire rules from raw data without supervision. To achieve this, we construct a model with multiple codebooks of VQ-VAE employing continuous and discrete latent variables during word generation. We evaluate the model's performance under high and low-resource scenarios, and use probing techniques to examine encoded information in latent representations. We also evaluate its generalization capabilities by testing unseen suffixation scenarios within the SIGMORPHON-UniMorph 2022 Shared Task 0. Our results demonstrate our model's ability to distinguish word structures into lemmas and suffixes, with each codebook specialized for different morphological features, contributing to the interpretability of our model and effectively performing morphological inflection on both seen and unseen morphological features",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=APPCPN1c1R": {
    "title": "Navigating the Dual Facets: A Comprehensive Evaluation of Sequential Memory Editing in Large Language Models",
    "volume": "review",
    "abstract": "Memory Editing (ME) has emerged as an efficient method to modify erroneous facts or inject new facts into Large Language Models (LLMs). Two mainstream ME methods exist: parameter-modifying ME and parameter-preserving ME (integrating extra modules while preserving original parameters). Regrettably, previous studies on ME evaluation have two critical limitations: (i) evaluating LLMs with single edit only, neglecting the need for continuous editing, and (ii) evaluations focusing solely on basic factual triples, overlooking broader LLM capabilities like logical reasoning and reading understanding. This study addresses these limitations with contributions threefold: (i) We explore how ME affects a wide range of fundamental capabilities of LLMs under sequential editing. Experimental results reveal an intriguing phenomenon: Most parameter-modifying ME consistently degrade performance across all tasks after a few sequential edits. In contrast, parameter-preserving ME effectively maintains LLMs' fundamental capabilities but struggles to accurately recall edited knowledge presented in a different format. (ii) We extend our evaluation to different editing settings, such as layers to edit, model size, instruction tuning, etc. Experimental findings indicate several strategies that can potentially mitigate the adverse effects of ME. (iii) We further explain why parameter-modifying damages LLMs from three dimensions: parameter changes after editing, language modeling capability, and the in-context learning capability. Our in-depth study advocates more careful use of ME in real-world scenarios",
    "checked": true,
    "id": "dff7f7d442c363341b35e766a4aba180f3851781",
    "semantic_title": "navigating the dual facets: a comprehensive evaluation of sequential memory editing in large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=gzWNe8t4q5": {
    "title": "Instances Need More Care: Rewriting Prompts for Instances with LLMs in the Loop Yields Better Zero-Shot Performance",
    "volume": "review",
    "abstract": "Large language models (LLMs) have revolutionized zero-shot task performance, mitigating the need for task-specific annotations while enhancing task generalizability. Despite its advancements, current methods using trigger phrases such as ``Let's think step by step'' remain limited. This study introduces PROMPTED, an approach that optimizes the zero-shot prompts for individual task instances following an innovative manner of ``LLMs in the loop''. Our comprehensive evaluation across 13 datasets and 10 task types based on GPT-4 reveals that PROMPTED significantly outperforms both the naive zero-shot approaches and a strong baseline (i.e., ``Output Refinement'') which refines the task output instead of the input prompt. Our experimental results also confirmed the generalization of this advantage to the relatively weaker GPT-3.5. Even more intriguingly, we found that leveraging GPT-3.5 to rewrite prompts for the stronger GPT-4 not only matches but occasionally exceeds the efficacy of using GPT-4 as the prompt rewriter. Our research thus presents a huge value in not only enhancing zero-shot LLM performance but also potentially enabling supervising LLMs with their weaker counterparts, a capability attracting much interest recently",
    "checked": true,
    "id": "af089827ede3207283a96e07732ab2e6b7fdc661",
    "semantic_title": "instances need more care: rewriting prompts for instances with llms in the loop yields better zero-shot performance",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=WEDj2hOXDs": {
    "title": "All Languages Matter: On the Multilingual Safety of LLMs",
    "volume": "review",
    "abstract": "Safety lies at the core of developing and deploying large language models (LLMs). However, previous safety benchmarks only concern the safety in one language, e.g. the majority language in the pretraining data such as English. In this work, we build the first multilingual safety benchmark for LLMs, XSafety, in response to the global deployment of LLMs in practice. XSafety covers 14 kinds of commonly used safety issues across 10 languages that span several language families. We utilize XSafety to empirically study the multilingual safety for 4 widely-used LLMs, including both close-API and open-source models. Experimental results show that all LLMs produce significantly more unsafe responses for non-English queries than English ones, indicating the necessity of developing safety alignment for non-English languages. In addition, we propose a simple and effective prompting method to improve the multilingual safety of ChatGPT by enhancing cross-lingual generalization of safety alignment. Our prompting method can significantly reduce the ratio of unsafe responses by 42\\% for non-English queries. We will release all the data and results to facilitate future research on LLMs' safety",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xWEWLYU1iih": {
    "title": "DINER: Debiasing Aspect-based Sentiment Analysis with Multi-variable Causal Inference",
    "volume": "review",
    "abstract": "Though notable progress has been made, neural-based aspect-based sentiment analysis (ABSA) models are prone to learn spurious correlations from annotation biases, resulting in poor robustness on adversarial data transformations. Among the debiasing solutions, causal inference-based methods have attracted much research attention, which can be mainly categorized into causal intervention methods and counterfactual reasoning methods. However, most of the present debiasing methods focus on single-variable causal inference, which is not suitable for ABSA with two input variables (the target aspect and the review). In this paper, we propose a novel framework based on multi-variable causal inference for debiasing ABSA. In this framework, different types of biases are tackled based on different causal intervention methods. For the review branch, the bias is modeled as indirect confounding from context, where backdoor adjustment intervention is employed for debiasing. For the aspect branch, the bias is described as a direct correlation with labels, where counterfactual reasoning is adopted for debiasing. Extensive experiments demonstrate the effectiveness of the proposed method compared to various baselines on the two widely used real-world aspect robustness test set datasets",
    "checked": true,
    "id": "c284da4bcc13353803cb8027780bb234f779c12e",
    "semantic_title": "diner: debiasing aspect-based sentiment analysis with multi-variable causal inference",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=bW1C5XODvo": {
    "title": "Measuring Stereotypical Bias in Multi-modal Large Language Models",
    "volume": "review",
    "abstract": "Multi-modal large language models (MLLMs) have been rapidly developed and widely used in various fields, but the (potential) stereotypical bias in the model has not been studied. In this study, we present pioneering research aimed at understanding the presence and implications of stereotypical bias in three widely-used open-source MLLMs: LLaVA-v1.5, MiniGPT-v2, and CogVLM. Specifically, we explore stereotypical bias in MLLMs from two modalities (vision and language), considering three scenarios (occupation, descriptor, and persona), and two attributes (gender and race). We find that 1) MLLMs demonstrate notable stereotypical biases across various scenarios, with LLaVA-v1.5 and CogVLM emerging as the most biased models; 2) these stereotypical biases can be rooted in both the training datasets and pre-trained models' inherent biases; and 3) leveraging specific prompt prefixes demonstrates considerable performance in reducing stereotypical bias, though their effectiveness is inconsistent. Overall, our work serves as a crucial step toward understanding and addressing stereotypical bias in MLLMs. We appeal to the community's attention to the stereotypical bias inherent in the rapidly-evolving MLLMs and to actively contribute to the development of unbiased and responsible multi-modal AI systems",
    "checked": false,
    "id": "110be9d98a8175028bc8360e6bfbff84d3fd96f2",
    "semantic_title": "ask llms directly, \"what shapes your bias?\": measuring social bias in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zEEFG920d2x": {
    "title": "Uncertainty Aware Learning for Language Model Alignment",
    "volume": "review",
    "abstract": "As instruction-tuned large language models (LLMs) evolve, aligning pretrained foundation models presents increasing challenges. Existing alignment strategies, which typically leverage diverse and high-quality data sources, often overlook the intrinsic uncertainty of tasks, learning all data samples equally. This may lead to suboptimal data efficiency and model performance. In response, we propose uncertainty-aware learning (UAL) to improve the model alignment of different task scenarios, by introducing the sample uncertainty (elicited from more capable LLMs). We implement UAL by a simple fashion -- adaptively setting the label smoothing value of training according to the uncertainty of individual samples. Analysis shows that our UAL indeed facilitates better token clustering in the feature space, validating our hypothesis. Extensive experiments on widely used benchmarks demonstrate that our UAL significantly and consistently outperforms standard supervised fine-tuning. Notably, LLMs aligned in a mixed scenario have achieved an average improvement of 10.62\\% on high-entropy tasks (i.e., AlpacaEval leaderboard), and 1.81\\% on complex low-entropy tasks (i.e., MetaMath and GSM8K)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iKgOXlnf1d": {
    "title": "Large Language Model is a Better Context Extractor for Aspect-Based Sentiment Analysis",
    "volume": "review",
    "abstract": "Previous Aspect-Based Sentiment Analysis (ABSA) studies have often incorporated syntactic information to connect contextual details with the designated aspect. These methods rely on complex model design to obtain syntactic structure information, further acquiring crucial semantic insights. Considering the potent contextualization abilities of the Large Language Model (LLM), we present the Low-Rank Adaptation plus In-domain Dynamic Examplar (LoRA-IDE) framework. This framework effectively aligns the task and sentence context information with the target aspect, leveraging the power of LLM. Specifically, we employ the LoRA training strategy to enable LLM to learn the context information of ABSA and enhance its understanding of the connection between sentence context and aspects through IDE. Experimental results demonstrate that our proposed approach not only improves the performance of LLM on ABSA but also outperforms the current state-of-the-art model on two benchmarks at a large scale. The codes will be released upon the acceptance of this paper",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fcERKPQXhR_": {
    "title": "Pruning Large Language Models to Intra-module Low-rank Structure with Transitional Activations",
    "volume": "review",
    "abstract": "Structured pruning offers a viable approach to the local deployment of large language models (LLMs) by reducing computational and memory overheads. Compared to unstructured pruning and quantization, structured pruning has the advantage of being recoverable, since the pruned model remains dense and high-precision rather than sparse or low-precision. However, achieving a high compression ratio for scaled-up LLMs remains a challenge, as the coarse-grained structured pruning poses large damage to the highly interconnected model. In this paper, we introduce TransAct, a task-agnostic structured pruning approach coupled with a compact architecture design. TransAct reduces transitional activations inside multi-head attention (MHA) and multi-layer perceptron (MLP) modules, while preserving the inter-module activations that are sensitive to perturbations. Hence, the LLM is compressed into an intra-module low-rank architecture, significantly reducing weights and KV Cache. TransAct is implemented on the Llama2 model and evaluated on downstream benchmarks. Results verify the optimality of our approach at high compression with respect to both speed and performance. Furthermore, ablation studies revealed the strength of iterative pruning and provides insights on the redundancy of MHA and MLP modules",
    "checked": false,
    "id": "c2d2dbb6b2d82308a7a354468574623a378c4cc0",
    "semantic_title": "pruning large language models to intra-module low-rank architecture with transitional activations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1xms2oSsqc": {
    "title": "Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models",
    "volume": "review",
    "abstract": "Recently developed large language models (LLMs) have been shown to perform remarkably well on a wide range of language understanding tasks. But, can they really \"reason\" over the natural language? This question has been receiving significant research attention and many reasoning skills such as commonsense, numerical, and qualitative have been studied. However, the crucial skill pertaining to 'logical reasoning' has remained underexplored. Existing work investigating this reasoning ability of LLMs has focused only on a couple of inference rules (such as modus ponens and modus tollens) of propositional and first-order logic. Addressing the above limitation, we comprehensively evaluate the logical reasoning ability of LLMs on 25 different reasoning patterns spanning over propositional, first-order, and non-monotonic logics. To enable systematic evaluation, we introduce LogicBench, a natural language question-answering dataset focusing on the use of a single inference rule. We conduct detailed analysis with a range of LLMs such as GPT-4, ChatGPT, Gemini, Llama-2, and Mistral using chain-of-thought prompting. Experimental results show that existing LLMs do not fare well on LogicBench; especially, they struggle with instances involving complex reasoning and negations. Furthermore, they sometimes tend to prioritize parametric knowledge over contextual information and overlook the correct reasoning chain. We believe that our work and findings facilitate future research for evaluating and enhancing the logical reasoning ability of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FEsQHtFyAs": {
    "title": "Precision in Knowledge Empowers, Excess is Distraction: Visual Question Answering with Knowledge-Infused Language Models",
    "volume": "review",
    "abstract": "In the realm of multimodal tasks, Visual Question Answering (VQA) plays a crucial role by addressing questions in natural language grounded in visual content. Knowledge-Based Visual Question Answering KBVQA elevates this concept by integrating external knowledge with images to respond to questions. KBVQA shows great potential in tackling real-world challenges, encompassing assistance for the visually impaired and enhancing image search functionalities. We introduce an innovative approach for KBVQA, augmenting the existing vision-language transformer encoder-decoder (OFA) model. Our main contribution involves enhancing questions by incorporating pertinent external knowledge extracted from knowledge graphs, using a \\textit{dynamic triple extraction} method. We supply a flexible number of triples from the knowledge graph as context, tailored to meet the requirements for answering the question. Our model, enriched with knowledge, demonstrates an average improvement of 4.75% in Exact Match Score over the SOTA on three different KBVQA datasets. Through thorough experiments and analysis, we illustrate that furnishing variable triples for each question improves the reasoning capabilities of the language model in contrast to supplying a fixed number of triples. Additionally, we highlight the model's generalization capability by showcasing its SOTA-beating performance on a small dataset, achieved through straightforward fine-tuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H-3hmBMd3Ov": {
    "title": "STICKERCONV: Generating Multimodal Empathetic Responses from Scratch",
    "volume": "review",
    "abstract": "Stickers, while widely recognized for enhancing empathetic communication in online interactions, remain underexplored in current empathetic dialogue research, notably due to the challenge of a lack of comprehensive datasets. In this paper, we introduce the Agent for STICKERCONV (Agent4SC), which uses collaborative agent interactions to realistically simulate human behavior with sticker usage, thereby enhancing multimodal empathetic communication. Building on this foundation, we develop a multimodal empathetic dialogue dataset, STICKERCONV, comprising 12.9K dialogue sessions, 5.8K unique stickers, and 2K diverse conversational scenarios. This dataset serves as a benchmark for multimodal empathetic generation. To advance further, we propose PErceive and Generate Stickers (PEGS), a multimodal empathetic response generation framework, complemented by a comprehensive set of empathy evaluation metrics based on LLM. Our experiments demonstrate PEGS's effectiveness in generating contextually relevant and emotionally resonant multimodal empathetic responses, contributing to the advancement of more nuanced and engaging empathetic dialogue systems",
    "checked": true,
    "id": "f58484d50364014289c0d4fe20ba35962fb40ba9",
    "semantic_title": "stickerconv: generating multimodal empathetic responses from scratch",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ClP4v3yuTsU": {
    "title": "PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?",
    "volume": "review",
    "abstract": "Recent works show that the largest of the large language models (LLMs) can solve many simple reasoning tasks expressed in natural language, without any/much supervision. But, can they also solve challenging first-order combinatorial reasoning problems, such as graph coloring, knapsack and cryptarithmetic? To answer this question, we present PuzzleBench, a dataset of 31 such challenging problems along with a few solved instances for each problem. These problems are all first order, i.e., they can be instantiated with problem instances of varying sizes, and most of them are NP-hard, requiring several reasoning steps to reach the solution. We first observe that LLMs, even when aided by symbolic solvers, perform rather poorly on our dataset. In response, we propose a new approach, Puzzle-LM, which combines LLMs with both symbolic solvers and program interpreters, along with feedback from solved examples, to achieve huge performance gains. Our extensive experimentation and analyses offer new insights into the reasoning abilities and limitations of present-day LLMs",
    "checked": true,
    "id": "770d7dfa44a78233619550c4d7ce4038eeb603bd",
    "semantic_title": "puzzlebench: can llms solve challenging first-order combinatorial reasoning problems?",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=naO9BjOgXS2": {
    "title": "KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM Does and Doesn't Know",
    "volume": "review",
    "abstract": "Measuring the alignment between a Knowledge Graph (KG) and Large Language Models (LLMs) is an effective method to assess the factualness and identify the knowledge blind spots of LLMs. However, this approach encounters two primary challenges including the translation of KGs into natural language and the efficient evaluation of these extensive and complex structures. In this paper, we present KGLensâ€”a novel framework aimed at measuring the alignment between KGs and LLMs, and pinpointing the LLMs' knowledge deficiencies relative to KGs. KGLens features a graph-guided question generator for converting KGs into natural language, along with a carefully designed sampling strategy based on parameterized KG structure to expedite KG traversal.We conducted experiments using three domain-specific KGs from Wikidata, which comprise over 19,000 edges, 700 relations, and 21,000 entities. Our analysis across eight LLMs reveals that KGLens not only evaluates the factual accuracy of LLMs more rapidly but also delivers in-depth analyses on topics, temporal dynamics, and relationships. Furthermore, human evaluation results indicate that KGLens can assess LLMs with a level of accuracy nearly equivalent to that of human annotators, achieving 95.7% of the accuracy rate",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wsWY21ZmIS": {
    "title": "How Do Humans Write Code? Large Models Do It the Same Way Too",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) often make errors when performing numerical calculations. In contrast to traditional chain-of-thought reasoning, the program-of-thoughts approach involves generating executable code to solve problems. By executing this code, it achieves more precise results. Using generated executable code instead of natural language can reduce computational errors. However, we observe that when LLMs solve mathematical problems using code, they tend to generate more incorrect reasoning than when using natural language. To address this issue, we propose Human-Think Language (HTL), a straightforward yet highly efficient approach inspired by human coding practices. The approach first generates problem-solving methods described in the natural language by the model, then converts them into code, mirroring the process where people think through the logic in natural language before writing it as code. Additionally, it utilizes the Proximal Policy Optimization (PPO) algorithm, enabling it to provide feedback to itself based on the correctness of mathematical answers, much like humans do. Finally, we introduce a focus-attention mechanism that masks the question segment, enhancing its reliance on natural language inference solutions during code generation. We conduct our experiments without introducing any additional information, and the results across five mathematical calculation datasets showcase the effectiveness of our approach. Notably, on the NumGLUE dataset, the LlaMA-2-7B-based model achieves a superior performance rate (75.1%) compared to the previous best performance with the LlaMA-2-70B model (74.4%)",
    "checked": true,
    "id": "5c667f3fd6215266e482dcbeb1e0119c27032519",
    "semantic_title": "how do humans write code? large models do it the same way too",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aetKZyC0xQi": {
    "title": "A Factuality and Diversity Reconciled Decoding Method for Knowledge-Grounded Dialogue Generation",
    "volume": "review",
    "abstract": "Grounding external knowledge can improve the factuality of responses in dialogue generation, but over emphasis on that could result in the lack of engaging and diverse phrases. Through introduction of randomness in sampling, the current methods can enhance the diversity. However, the sampling could jeopardize the factuality in dialogue generation. In this work, to find a solution to advance creativity except for relying on problematic randomness, and to subtly reconcile the factuality and diversity within the source-grounded paradigm, a novel DoGe method is proposed. DoGe can dynamically switch between the utilization of internal parameter knowledge and external source knowledge according to model's factual confidence. Extensive experiments on three widely-used datasets demonstrate that DoGe can both enhance response diversity and maintain factuality, and it significantly outperforms other various decoding strategy baselines",
    "checked": true,
    "id": "1ebaeb568cc9852aa938d75b870e5f1fa67a8904",
    "semantic_title": "a factuality and diversity reconciled decoding method for knowledge-grounded dialogue generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qB8uDc6OuI": {
    "title": "Detoxifying Large Language Models via Knowledge Editing",
    "volume": "review",
    "abstract": "This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Intraoperative Neural Monitoring (DINM), to erase the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT, and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments. We hope that these insights could shed light on future work of developing detoxifying approaches and the underlying knowledge mechanisms of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-XcSAvhCtgL": {
    "title": "Enhancing Proactive Emotional Support Dialogue System with Look-forward Strategy Planning",
    "volume": "review",
    "abstract": "Effective emotional support (ES) is crucial to preventing severe mental health issues amid widespread mental disorders and limited access to psychological counseling. However, current emotional support conversations are limited by their simplistic single-turn interactions and lack the capability for multi-turn, look-forward strategy planning, which impedes accurately identifying users' emotional states. Additionally, ground-truth-based evaluation metrics fall short in practically assessing supportiveness and empathy in realistic dialogues. In this paper, we introduce a proactive emotional support conversational system (ProESC) to address these issues. Utilizing a small pre-trained language model, we enable the anticipation of future support strategy sequences as simulation hints, guiding LLMs in generating emotionally supportive responses and training with goal-oriented rewards. For pragmatic user feedback assessment, we employ a GPT-4 based user simulator to represent vulnerable users in need of support, evaluating responses with multi-faceted metrics. Extensive experiments demonstrate that our model surpasses competitive baselines in both strategy planning and dialogue generation, offering a more nuanced and effective approach to emotional support",
    "checked": false,
    "id": "b516a98386e4168eb44cdc4508f3ce66c141cb57",
    "semantic_title": "potential operational delineations: new horizons for proactive, risk-informed strategic land and fire management",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=32SST8swlj": {
    "title": "Surgical Feature-Space Decomposition of LLMs: Why, When and How?",
    "volume": "review",
    "abstract": "Low-rank approximations, of the weight and feature space can enhance the performance of deep learning models, whether in terms of improving generalization or reducing the latency of inference. However, there is no clear consensus yet on how, when and why these approximations are helpful for large language models (LLMs). In this work, we empirically study the efficacy of weight and feature space decomposition in transformer-based LLMs. We demonstrate that surgical decomposition not only provides critical insights into the trade-off between compression and language modelling performance, but also sometimes enhances commonsense reasoning performance of LLMs. Our empirical analysis identifies specific network segments that intrinsically exhibit a low-rank structure. Furthermore, we extend our investigation to the implications of low-rank approximations on model bias. Overall, our findings offer a novel perspective on optimizing LLMs, presenting the low-rank approximation not only as a tool for performance enhancements, but also as a means to potentially rectify biases within these models",
    "checked": true,
    "id": "60d4fc6d7131c3d3942732fad337acb6b725f869",
    "semantic_title": "surgical feature-space decomposition of llms: why, when and how?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y2CladKGXA": {
    "title": "Selective Prefix Tuning for Pre-trained Language Models",
    "volume": "review",
    "abstract": "The prevalent approach for optimizing pre-trained language models in downstream tasks is fine-tuning. However, it is both time-consuming and memory-inefficient. In response, a more efficient method called Prefix Tuning, which insert learnable vectors into each Transformer layers, has been proposed and proven effective. Recent investigations reveal that prefix tokens carry context-specific information, prompting the hypothesis that enhancing their specialization can improve model performance. To address this, we propose Selective Prefix Tuning (SPT), integrating a selective mechanism inspired by selective self-attention. Additionally, we introduce Selective Loss (SL) to encourage diversity in prefix tokens. Extensive experiments validate the effectiveness of SPT in sentence and token classification tasks. We contribute insight into understanding the role of prefix in model adaptation",
    "checked": false,
    "id": "175b32c07e56f881479be4c5a74bfa3c731cc454",
    "semantic_title": "rose: robust selective fine-tuning for pre-trained language models",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=FT2IufDsW-w": {
    "title": "Bayesian Preference Elicitation with Language Models",
    "volume": "review",
    "abstract": "There is increasing interest in using language models (LMs) not only for answering queries but also for gathering information about the preferences of human users. This preference data can be used to fine-tune LMs via reward modeling or completing goal-oriented tasks. However, LMs have been shown to struggle with crucial aspects of preference learning: quantifying uncertainty, modeling mental states, and posing highly informative questions. These challenges have been addressed in other areas of machine learning, such as Bayesian Optimal Experimental Design (BOED), which focuses on designing informative queries within a well-defined feature space. But these methods, in turn, have historically been difficult to scale and apply to real-world problems (e.g. involving text and images), in which simply identifying the relevant features can be challenging. We introduce OPEN (Optimal Preference Elicitation with Natural language) a framework that uses BOED to guide the choice of informative questions and an LM to extract features and translate abstract BOED queries into natural language questions. By combining the flexibility of LMs with the precision of BOED, OPEN can optimize queries for informativity while remaining adaptable to real-world domains. Conducting user studies, OPEN outperforms existing LM- and BOED-based methods for preference elicitation",
    "checked": true,
    "id": "5882fd8f5b22a9bc62ab2226597a4d45923c505c",
    "semantic_title": "bayesian preference elicitation with language models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=h9ryPGdb6p": {
    "title": "SPAFIT: Stratified Progressive Adaptation Fine-tuning for Pre-trained Large Language Models",
    "volume": "review",
    "abstract": "Full fine-tuning is a popular approach to adapt Transformer-based pre-trained large language models to a specific downstream task. However, the substantial requirements for computational power and storage have discouraged its widespread use. Moreover, increasing evidence of catastrophic forgetting and overparameterization in the Transformer architecture has motivated researchers to seek more efficient fine-tuning (PEFT) methods. Commonly known parameter-efficient fine-tuning methods like LoRA and BitFit are typically applied across all layers of the model. We propose a PEFT method, called Stratified Progressive Adaptation Fine-tuning (SPAFIT), based on the localization of different types of linguistic knowledge to specific layers of the model. Our experiments, conducted on nine tasks from the GLUE benchmark, show that our proposed SPAFIT method outperforms other PEFT methods while fine-tuning only a fraction of the parameters adjusted by other methods",
    "checked": true,
    "id": "14acdb45c13b036fd839f18049f3883a707fe6b0",
    "semantic_title": "spafit: stratified progressive adaptation fine-tuning for pre-trained large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s0r9ZYnUad": {
    "title": "Decoding Susceptibility: Modeling Misbelief to Misinformation Through a Computational Approach",
    "volume": "review",
    "abstract": "Susceptibility to misinformation describes the degree of belief in unverifiable claims, a latent aspect of individuals' mental processes that is not observable. Existing susceptibility studies heavily rely on self-reported beliefs, which can be subject to bias, expensive to collect, and challenging to scale for downstream applications. To address these limitations, in this work, we propose a computational approach to model users' latent susceptibility levels. As shown in previous research, susceptibility is influenced by various factors (e.g., demographic factors, political ideology), and directly influences people's reposting behavior on social media. To represent the underlying mental process, our susceptibility modeling incorporates these factors as inputs, guided by the supervision of people's sharing behavior. Using COVID-19 as a testbed domain, our experiments demonstrate a significant alignment between the susceptibility scores estimated by our computational modeling and human judgments, confirming the effectiveness of this latent modeling approach. Furthermore, we apply our model to annotate susceptibility scores on a large-scale dataset and analyze the relationships between susceptibility with various factors. Our analysis reveals that political leanings and psychological factors exhibit varying degrees of association with susceptibility to COVID-19 misinformation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kupb_PcBQ75": {
    "title": "Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models?",
    "volume": "review",
    "abstract": "Large vision-language models (LVLMs) have recently dramatically pushed the state of the art in image captioning and many image understanding tasks (e.g., visual question answering). Nonetheless, LVLMs still often \\textit{hallucinate} and produce captions mentioning concepts that cannot be found in the input image. These hallucinations erode the trustworthiness of LVLMs and are one of the main obstacles to their ubiquitous adoption. Recent work suggests that addition of grounding objectives such as those based on \\textit{referring expressions}---explicit alignment between image regions or objects and text descriptions--- reduces the amount of LVLM hallucination. Although intuitive, this claim is not empirically justified as the reduction effects have been established, we argue, with flawed evaluation protocols that (i) rely on data (i.e., MSCOCO) that has been extensively used in LVLM training and (ii) measure hallucination via question answering rather than open-ended caption generation. In this work, in contrast, we offer the first systematic analysis of the effect of fine-grained object grounding on LVLM hallucination under an evaluation protocol that more realistically captures LVLM hallucination in open generation. Our extensive experiments reveal that, while grounding leads to more informative captions, it generally does not reduce the proportion of hallucinated content",
    "checked": true,
    "id": "fa5a8e7cbbbb8ee47610733c363bb96bf31e049b",
    "semantic_title": "does object grounding really reduce hallucination of large vision-language models?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tEyUfglaCK": {
    "title": "Probabilistically-sound beam search with masked language models",
    "volume": "review",
    "abstract": "Beam search with masked language models (MLMs) is challenging in part because joint probability distributions over sequences are not readily available, unlike for autoregressive models. Nevertheless, estimating such distributions has applications in many domains, including protein engineering and ancient text restoration. We present probabilistically-sound methods for beam search with MLMs. First, we clarify the conditions under which it is theoretically-sound to perform text infilling with MLMs using standard beam search. When these conditions fail, we provide a probabilistically-sound modification with no additional computational complexity and demonstrate that it is superior to the aforementioned beam search in the expected conditions. We then present empirical results comparing several infilling approaches with MLMs across several domains",
    "checked": true,
    "id": "58ade5b2ab4df757e69e9c72c2ca20c71b333892",
    "semantic_title": "probabilistically-sound beam search with masked language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PRy-o1B_ot": {
    "title": "PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs",
    "volume": "review",
    "abstract": "While Large language models (LLMs) have demonstrated considerable capabilities across various natural language tasks, they often fall short of the performance achieved by domain-specific state-of-the-art models. One potential approach to enhance domain-specific capabilities of LLMs involves fine-tuning them using corresponding datasets. However, this method can be both resource and time-intensive, and not applicable to closed-source commercial LLMs. In this paper, we propose \\textbf{P}reference \\textbf{A}daptation for E\\textbf{n}hancing \\textbf{D}omain-specific \\textbf{A}bilities of LLMs (PANDA), a method designed to augment the domain-specific capabilities of LLMs by leveraging insights from the response preference of expert models without requiring fine-tuning. Our experimental results reveal that PANDA significantly enhances the domain-specific ability of LLMs on text classification and interactive decision tasks. Moreover, LLM with PANDA even outperforms the expert model that being learned on 4 tasks of ScienceWorld. This finding highlights the potential of exploring tuning-free approaches to achieve weak-to-strong generalization",
    "checked": true,
    "id": "c7ffbc63ed38e78261def94b735fc51305d248d2",
    "semantic_title": "panda: preference adaptation for enhancing domain-specific abilities of llms",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ThohyWg2XdR": {
    "title": "Self-JGAR: Self-Judgement For Generative Adversarial Reasoning in Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) leverage their output for refinement, attracting increasing interest in such techniques. However, the illusion issue makes it challenging to guarantee the effectiveness of this refinement. Incorporating external feedback is pivotal for addressing the challenges in the refinement to ensure the reliability of the generated content. We introduce a framework, Self-JGAR, which utilizes adversarial learning to update the judgment capacity of LLMs and steer LLMs reasoning in the right direction. The framework endows LLMs with the capability to make judgments about their reasoning process, thereby enhancing their reasoning ability. Experiment results show that our framework outperforms the strong baseline on reasoning tasks. The codes will be released upon the acceptance of this paper",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sajLT4nqwzf": {
    "title": "Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment",
    "volume": "review",
    "abstract": "Evaluating and Rethinking the current landscape of Large Multimodal Models (LMMs), we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet ignore the visual knowledge-dimension alignment, i.e., connecting visuals to their relevant knowledge. Visual knowledge plays a significant role in analyzing, inferring, and interpreting information from visuals, helping improve the accuracy of answers to knowledge-based visual questions. In this paper, we mainly explore improving LMMs with visual-language knowledge alignment, especially aimed at challenging knowledge-based visual question answering (VQA). To this end, we present a Cognitive Visual-Language Mapper (CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning stage. Specifically, we design the VKA based on the interaction between a small language model and a visual encoder, training it on collected image-knowledge pairs to achieve visual knowledge acquisition and projection. FKA is employed to distill the fine-grained visual knowledge of an image and inject it into Large Language Models (LLMs). We conduct extensive experiments on knowledge-based VQA benchmarks and experimental results show that CVLM significantly improves the performance of LMMs on knowledge-based VQA (average gain by 5.0%). Ablation studies also verify the effectiveness of VKA and FKA, respectively",
    "checked": true,
    "id": "aa4a36fc4610719b5ec253ebe0a789264c4043ff",
    "semantic_title": "cognitive visual-language mapper: advancing multimodal comprehension with enhanced visual knowledge alignment",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xy_KXVM5Skg": {
    "title": "Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness",
    "volume": "review",
    "abstract": "We introduce BSDetector a method for detecting bad and speculative answers from a pretrained Large Language Model by estimating a numeric confidence score for any output it generated. Our uncertainty quantification technique works for any LLM accessible only via a black-box API, whose training data remains unknown. By expending a bit of extra computation, users of any LLM API can now get the same response as they would ordinarily, as well as a confidence estimate that cautions when not to trust this response. Experiments on both closed and open-form Question-Answer benchmarks reveal that BSDetector more accurately identifies incorrect LLM responses than alternative uncertainty estimation procedures (for both GPT-3 and ChatGPT). By sampling multiple responses from the LLM and considering the one with the highest confidence score, we can additionally obtain more accurate responses from the same LLM, without any extra training steps. In applications involving automated evaluation with LLMs, accounting for our confidence scores leads to more reliable evaluation in both human-in-the-loop and fully-automated settings (across both GPT 3.5 and 4)",
    "checked": true,
    "id": "c01c7c1f903dfaa78812fb20a6cb2db25e4712e3",
    "semantic_title": "quantifying uncertainty in answers from any language model and enhancing their trustworthiness",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=UOmYRzCiYK": {
    "title": "Evaluating Short-Term Temporal Fluctuations of Social Biases in Social Media Data and Masked Language Models",
    "volume": "review",
    "abstract": "Social biases such as gender or racial biases have been reported in pre-trained language models (LMs), including Masked Language Models (MLMs).Given that MLMs are continuously trained with increasing amounts of additional data collected over time, an important yet unanswered question is how the social biases encoded with MLMs vary over time.In particular, the number of social media users continues to grow at an exponential rate, and it is a valid concern for the MLMs trained specifically on social media data whether their social biases (if any) would also amplify over time.To empirically analyse this problem, we use a series of MLMs pretrained on chronologically ordered temporal snapshots of corpora collected from X over a period of two years.Our analysis reveals that, although social biases are present in all MLMs, most types of social bias remain relatively stable over time (with a few exceptions). To further understand the mechanisms that influence social biases in MLMs, we analyse the corpora used to train the MLMs.Our findings show that some demographic groups, such as \\textit{male}, obtain higher preference over the other, such as \\textit{female} on the training corpora constantly",
    "checked": true,
    "id": "2a157d33ef98319fd4e4daad487a760076d7b6ad",
    "semantic_title": "evaluating short-term temporal fluctuations of social biases in social media data and masked language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hqs3voqDCN": {
    "title": "Enhancing Text-to-SQL Parsing through Question Rewriting and Execution-Guided Refinement",
    "volume": "review",
    "abstract": "Large Language Model (LLM)-based approach has become the mainstream for Text-to-SQL task and achieves remarkable performance. In this paper, we augment the existing prompt engineering methods by exploiting the database content and execution feedback. Specifically, we introduce DART-SQL, which comprises two key components: (1) Question Rewriting: DART-SQL rewrites natural language questions by leveraging database content information to eliminate ambiguity. (2) Execution-Guided Refinement: DART-SQL incorporates database content information and utilizes the execution results of the generated SQL to iteratively refine the SQL. We apply this framework to the two LLM-based approaches (DAIL-SQL and C3) and test it on four widely used benchmarks (Spider-dev, Spider-test, Realistic and DK). Experiments show that our framework for DAIL-SQL and C3 achieves an average improvement of 12.41% and 5.38%, respectively, in terms of execution accuracy(EX) metric",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LCWJAn6rOgI": {
    "title": "CR-UTP: Certified Robustness against Universal Text Perturbations",
    "volume": "review",
    "abstract": "It is imperative to ensure the stability of every prediction made by a language model; that is, a language's prediction should remain consistent despite minor input variations, like word substitutions. In this paper, we investigate the problem of certifying a language model's robustness against Universal Text Perturbations (UTPs), which have been widely used in universal adversarial attacks and backdoor attacks. Existing certified robustness based on random smoothing has shown considerable promise in certifying the input-specific text perturbations (ISTPs), operating under the assumption that any random alteration of a sample's clean or adversarial words would negate the impact of sample-wise perturbations. However, with UTPs, masking only the adversarial words can eliminate the attack. A naive method is to simply increase the masking ratio and the likelihood of masking attack tokens, but it leads to a significant reduction in both certified accuracy and the certified radius, due to input corruption by extensive masking. To solve this challenge, we introduce a novel approach, the \\textit{superior prompt search} method, designed to identify a \\textit{superior prompt} that maintains higher certified accuracy under extensive masking. Additionally, we theoretically motivate why ensembles are a particularly suitable choice as base prompts for random smoothing. The method is denoted by \\textit{superior prompt ensembling} technique. We also empirically confirm this technique, obtaining state-of-the-art results in multiple settings. These methodologies, for the first time, enable high certified accuracy against both UTPs and ISTPs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xNBR3p6Y3cE": {
    "title": "A Psychological View to Social Bias in LLMs: Evaluation and Mitigation",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) perpetuate social biases, reflecting prejudices in their training data and reinforcing societal stereotypes and inequalities. Our work explores the potential of the Contact Hypothesis from social psychology for debiasing LLMs. We simulate various forms of social contact through LLM prompting to measure its influence on the model's biases, similar to how intergroup interactions can reduce prejudices in social contexts. We create a dataset of 108,000 prompts following a principled approach replicating social contact to measure biases in three LLMs (Llama 2, Tulu, and NousHermes) across 13 social bias dimensions. We propose a unique debiasing technique, Social Contact Debiasing (SCD), that instruction tunes these models with unbiased responses to prompts. Our research demonstrates that LLMs do indeed exhibit social biases, but more importantly, these biases can be significantly reduced by up to 40% in 1 epoch of instruction tuning Llama 2 following our SCD strategy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mvWN_76iE2": {
    "title": "Multimodal Contextualized Semantic Parsing from Speech",
    "volume": "review",
    "abstract": "We introduce Semantic Parsing in Contextual Environments (SPICE), a task designed to enhance artificial agents' contextual awareness by integrating multimodal inputs with prior contexts. SPICE goes beyond traditional semantic parsing by offering a structured, interpretable framework for dynamically updating an agent's knowledge with new information, mirroring the complexity of human communication. We develop the VG-SPICE dataset, crafted to challenge agents with visual scene graph construction from spoken conversational exchanges, highlighting speech and visual data integration. We also present the Audio-Vision Dialogue Scene Parser (AViD-SP) developed for use on VG-SPICE and a novel multimodal fusion method, the Grouped Multimodal Attention Down Sampler (GMADS), within the AViD-SP model. These innovations aim to improve multimodal information processing and integration. Both the VG-SPICE dataset and the AViD-SP model are publicly available",
    "checked": true,
    "id": "8b55bfc5b14eaab44bbd434681233ab43474af14",
    "semantic_title": "multimodal contextualized semantic parsing from speech",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1u3xc0Tmis": {
    "title": "Inspiring Out-of-distribution Generalization of In-Context Learning via Contrastive Demonstrations",
    "volume": "review",
    "abstract": "Although existing demonstration construction methods have significantly improved the performance of In-Context Learning (ICL), these unfortunately only focused on the in-distribution settings that the selected demonstrations should have the same distribution with testing data. However, the out-of-distribution (OOD) settings are more commonly encountered in real scenarios, but ignored in the age of large language models. This paper first investigates the performance of existing ICL demonstration construction methods in OOD settings and verifies their failures. Moreover, this paper proposes contrastive demonstrations that combine a demonstration with its counterfactual, where a rationale-guided counterfactual generation method is proposed to generate higher-quality counterfactual data. Extensive experiments validate the effectiveness of our proposed method and the contrastive demonstrations can help the model better identify the essence of the task, thus achieving OOD generalization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jsgCgp4XRi": {
    "title": "LEGOBench: Scientific Leaderboard Generation Benchmark",
    "volume": "review",
    "abstract": "The ever-increasing volume of paper submissions makes it difficult to stay informed about the latest state-of-the-art research. To address this challenge, we introduce LEGOBench, a benchmark for evaluating systems that generate scientific leaderboards. LEGOBench is curated from 22 years of preprint submission data on arXiv and more than 11k machine learning leaderboards on the PapersWithCode portal. We present four graph-based and two language model-based leaderboard generation task configurations. We evaluate popular encoder-only scientific language models as well as decoder-only large language models across these task configurations. Our results show less than random chance performance on the benchmark and significant performance gaps in automatic leaderboard generation. The code is available on GitHub and the dataset is hosted on OSF",
    "checked": true,
    "id": "262ac80703a24005a20edd72229371fe39c8bee7",
    "semantic_title": "legobench: scientific leaderboard generation benchmark",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UaQJFz4-0T": {
    "title": "Approximate Nearest Neighbors in External Memory",
    "volume": "review",
    "abstract": "This paper introduces a new approach to Approximate Nearest Neighbors (ANN), designed for external memory. The proposed method uses standard ANN packages on a subset of the vectors which we call landmarks. If two vectors are near one another, then they tend to be near many of the same landmarks. This observation can be used to implement ANN with well-understood inverted file techniques",
    "checked": false,
    "id": "f2f10c9742cf93d508aec74b83c5d2e8a9a4fdbc",
    "semantic_title": "approximate nearest neighbor search through modern error-correcting codes",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GkmeWoYIT7y": {
    "title": "Understanding Emergent Abilities of Language Models from the Loss Perspective",
    "volume": "review",
    "abstract": "Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) there is doubt on the discontinuous metrics used to measure these abilities. In this paper, we propose to study emergent abilities in the lens of pre-training loss, instead of model size or training compute. We demonstrate that the models with the same pre-training loss, but different model and data sizes, generate the same performance on various downstream tasks. We also discover that a model exhibits emergent abilities on certain tasks---regardless of the continuity of metrics---when its pre-training loss falls below a specific threshold. Before reaching this threshold, its performance remains at the level of random guessing. This inspires us to redefine emergent abilities as those that manifest in models with lower pre-training losses, highlighting that these abilities cannot be predicted by merely extrapolating the performance trends of models with higher pre-training losses",
    "checked": true,
    "id": "e61fde1309a9f5aab2060ace6f709711823c9ca5",
    "semantic_title": "understanding emergent abilities of language models from the loss perspective",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=v0K1c5BCjH": {
    "title": "Zero-shot sampling of adversarial entities in biomedical question answering",
    "volume": "review",
    "abstract": "The increasing depth of parametric domain knowledge in large language models (LLMs) is fueling their rapid deployment in real-world applications. In high-stakes and knowledge-intensive tasks, understanding model vulnerabilities is essential for quantifying the trustworthiness of model predictions and regulating their use. The recent discovery of named entities as adversarial examples in natural language processing tasks raises questions about their potential guises in other settings. Here, we propose a powerscaled distance-weighted sampling scheme in embedding space to discover diverse adversarial entities as distractors. We demonstrate its advantage over random sampling in adversarial question answering on biomedical topics. Our approach enables the exploration of different regions on the attack surface, which reveals two regimes of adversarial entities that markedly differ in their characteristics. Moreover, we show that the attacks successfully manipulate token-wise Shapley value explanations, which become deceptive in the adversarial setting. Our investigations illustrate the brittleness of domain knowledge in LLMs and reveal a shortcoming of standard evaluations for high-capacity models",
    "checked": true,
    "id": "640bbec28cd16c2c986215a354d2fbabfa058501",
    "semantic_title": "zero-shot sampling of adversarial entities in biomedical question answering",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CCWQY2IIBF": {
    "title": "The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage Brought By Model Editing",
    "volume": "review",
    "abstract": "Large Language Models have revolutionized numerous tasks with their remarkable efficacy.However, the editing of these models, crucial for rectifying outdated or erroneous information, often leads to a complex issue known as the ripple effect in the hidden space.This effect, while difficult to detect, can significantly impede the efficacy of model editing tasks and deteriorate model performance.This paper addresses this scientific challenge by proposing a novel evaluation methodology, Graphical Outlier Relation based Assessment~(GORA), which quantitatively evaluates the adaptations of the model and the subsequent impact of editing. Furthermore, we introduce the Selective Outlier Re-Editing Approach~(SORA), a model editing method designed to mitigate this ripple effect.Our comprehensive evaluations reveal that the ripple effect in the hidden space is a significant issue in all current model editing methods.However, our proposed methods, GORA and SORA, effectively identify and alleviate this issue, respectively, contributing to the advancement of LLM editing techniques",
    "checked": true,
    "id": "d6f7bdd45ea55c46dd949b316fe052fcabceabf0",
    "semantic_title": "the missing piece in model editing: a deep dive into the hidden damage brought by model editing",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=S-hQtSO9yqf": {
    "title": "Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents",
    "volume": "review",
    "abstract": "With the remarkable advancements of large language models (LLMs), LLM-based agents have become a research hotspot in human-computer interaction.However, there is a scarcity of benchmarks available for LLM-based mobile agents.Benchmarking these agents generally faces three main challenges:(1) The inefficiency of UI-only operations imposes limitations to task evaluation.(2) Specific instructions within a singular application lack adequacy for assessing the multi-dimensional reasoning and decision-making capacities of LLM mobile agents.(3) Current evaluation metrics are insufficient to accurately assess the process of sequential actions. To this end, we propose Mobile-Bench, a novel benchmark for evaluating the capabilities of LLM-based mobile agents.First, we expand conventional UI operations by incorporating 103 collected APIs to accelerate the efficiency of task completion.Subsequently, we collect evaluation data by combining real user queries with augmentation from LLMs.To better evaluate different levels of planning capabilities for mobile agents, our data is categorized into three distinct groups: SAST, SAMT, and MAMT, reflecting varying levels of task complexity. Mobile-Bench comprises 832 data entries, with more than 200 tasks specifically designed to evaluate multi-APP collaboration scenarios.Furthermore, we introduce a more accurate evaluation metric, named CheckPoint, to assess whether LLM-based mobile agents reach essential points during their planning and reasoning steps. Dataset and platform will be released in the future",
    "checked": true,
    "id": "47aacaab789e80388d22598b4810213655e62888",
    "semantic_title": "mobile-bench: an evaluation benchmark for llm-based mobile agents",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=x6LFFwvpf5": {
    "title": "Puzzle Solving using Reasoning of Large Language Models: A Survey",
    "volume": "review",
    "abstract": "Exploring the capabilities of Large Language Models (LLMs) in puzzle solving unveils critical insights into their potential and challenges in artificial intelligence, marking a significant step towards understanding their applicability in complex reasoning tasks. This survey leverages a unique taxonomyâ€”dividing puzzles into rule-based and rule-less categoriesâ€”to critically assess LLMs through various methodologies, including prompting techniques, neuro-symbolic approaches, and fine-tuning. Through a critical review of relevant datasets and benchmarks, we assess LLMs' performance, identifying significant challenges in complex puzzle scenarios. Our findings highlight the disparity between LLM capabilities and human-like reasoning, particularly in those requiring advanced logical inference. The survey underscores the necessity for novel strategies and richer datasets to advance LLMs' puzzle-solving proficiency and contribute to AI's logical reasoning and creative problem-solving advancements",
    "checked": true,
    "id": "396ea10d3ab89da41d02693d7165c4b98ecbb5f3",
    "semantic_title": "puzzle solving using reasoning of large language models: a survey",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=QYBDGWbN-b": {
    "title": "CliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios",
    "volume": "review",
    "abstract": "With the proliferation of Large Language Models (LLMs) in diverse domains, there is a particular need for unified evaluation standards in clinical medical scenarios, where models need to be examined very thoroughly. We present CliMedBench, a comprehensive benchmark with 14 expert-guided core clinical scenarios specifically designed to assess the medical ability of LLMs across 7 pivot dimensions. It comprises 33,735 questions derived from real-world medical reports of top-tier tertiary hospitals and authentic examination exercises. The reliability of this benchmark has been confirmed in several ways. Subsequent experiments with existing LLMs have led to the following findings: (i) Chinese medical LLMs underperform on this benchmark, especially where medical reasoning and factual consistency are vital, underscoring the need for advances in clinical knowledge and diagnostic accuracy. (ii) Several general-domain LLMs demonstrate substantial potential in medical clinics, while the limited input capacity of many medical LLMs hinders their practical use. These findings reveal both the strengths and limitations of LLMs in clinical scenarios and offer critical insights for medical research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IwMEjvFrF-": {
    "title": "Benchmarking Large Language Models on Communicative Medical Coaching: A Dataset and a Novel System",
    "volume": "review",
    "abstract": "Traditional applications of natural language processing (NLP) in healthcare have predominantly focused on patient-centered services, enhancing patient interactions and care delivery, such as through medical dialogue systems. However, the potential of NLP to benefit inexperienced doctors, particularly in areas such as communicative medical coaching, remains largely unexplored. We introduce ``ChatCoach,'' a human-AI cooperative framework designed to assist medical learners in practicing their communication skills during patient consultations. ChatCoach differentiates itself from conventional dialogue systems by offering a simulated environment where medical learners can practice dialogues with a patient agent, while a coach agent provides immediate, structured feedback.This is facilitated by our proposed Generalized Chain-of-Thought (GCoT) approach, which fosters the generation of structured feedback and enhances the utilization of external knowledge sources. Additionally, we have developed a dataset specifically for evaluating Large Language Models (LLMs) within the ChatCoach framework on communicative medical coaching tasks. Our empirical results validate the effectiveness of ChatCoach",
    "checked": false,
    "id": "db54979e005b5e3fd0a9b9c0444b014488bd47a9",
    "semantic_title": "benchmarking large language models on communicative medical coaching: a novel system and dataset",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=gjrXWBtUwB": {
    "title": "Chain-of-Box Empowered Language Models for Logical Reasoning over Knowledge Graphs",
    "volume": "review",
    "abstract": "Complex logical reasoning over large-scale knowledge graphs (KGs) is a fundamental yet challenging task. Current approaches mainly focus on embedding logical queries as well as KG entities into the same vector space and retrieving answers based on similarity matching. However, the incompleteness issue of KGs severely hinders the effectiveness of previous studies. To tackle the challenging knowledge deficiency problem, we propose to leverage language models as the additional knowledge reasoner and design a unified framework to integrate knowledge graph reasoning and natural language reasoning by harnessing box embeddings of reasoning trajectory as the chain-of-box and fusing it into the language model to empower the capability of logical reasoning. Extensive experiments on two standard benchmark datasets demonstrate that our model COB-LM significantly improves over state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fXrexjxTss": {
    "title": "Think Twice Before Assure: Confidence Estimation for Large Language Models through Reflection on Multiple Answers",
    "volume": "review",
    "abstract": "Confidence estimation aiming to evaluate output trustability is crucial for the application of large language models (LLM), especially the black-box ones. Existing confidence estimation of LLM is typically not calibrated due to the overconfidence of LLM on its generated incorrect answers. Existing approaches addressing the overconfidence issue are hindered by a significant limitation that they merely consider the confidence of one answer generated by LLM. To tackle this limitation, we propose a novel paradigm that thoroughly evaluates the trustability of multiple candidate answers to mitigate the overconfidence on incorrect answers. Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each answer, and then aggregates the justifications for comprehensive confidence estimation. This framework can be integrated with existing confidence estimation approaches for superior calibration. Experimental results on six datasets of three tasks demonstrate the rationality and effectiveness of the proposed framework",
    "checked": true,
    "id": "5d3105a5ffa133b873537bda8ff1ec6244c2b841",
    "semantic_title": "think twice before assure: confidence estimation for large language models through reflection on multiple answers",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=Euvonr_8eC4": {
    "title": "LLMAEL: Large Language Models are Good Context Augmenters for Entity Linking",
    "volume": "review",
    "abstract": "Entity Linking (EL) models are well-trained at mapping mentions to their corresponding entities according to a given context. However, EL models struggle to disambiguate long-tail entities due to their limited training data. Meanwhile, large language models (LLMs) are more robust at interpreting uncommon mentions. Yet, due to a lack of specialized training, LLMs suffer at generating correct entity IDs. Furthermore, training an LLM to perform EL is cost-intensive. Building upon these insights, we introduce LLM-Augmented Entity Linking LLMAEL, a plug-and-play approach to enhance entity linking through LLM data augmentation. We leverage LLMs as knowledgeable context augmenters, generating mention-centered descriptions as additional input, while preserving traditional EL models for task specific processing. Experiments on 6 standard datasets show that the vanilla LLMAEL outperforms baseline EL models in most cases, while the fine-tuned LLMAEL set the new state-of-the-art results across all 6 benchmarks",
    "checked": true,
    "id": "e7d1ca872bc51ca1d03cc5265e8e0132d58dabfa",
    "semantic_title": "llmael: large language models are good context augmenters for entity linking",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ua_nt4ztEf": {
    "title": "Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models",
    "volume": "review",
    "abstract": "The significant breakthroughs of Medical Multi-Modal Large Language Models (Med-MLLMs) renovate modern healthcare with robust information synthesis and medical decision support. However, these models are often validated using benchmarks unsuitable for the Med-MLLMs due to the intricate nature of the real-world diagnostic frameworks, which encompass diverse medical specialties and involve complex clinical decisions. Moreover, these benchmarks are susceptible to data leakage, since Med-MLLMs are trained on large assemblies of publicly available data. Thus, an isolated and clinically representative benchmark is highly desirable for credible Med-MLLMs evaluation. To this end, we introduce Asclepius, a novel Med-MLLM benchmark that rigorously and comprehensively assesses model capability in terms of: distinct medical specialties (cardiovascular, gastroenterology, etc.) and different diagnostic capacities (perception, disease analysis, etc.). Grounded in 3 proposed core principles, Asclepius ensures a comprehensive evaluation by encompassing 15 medical specialties, stratifying into 3 main categories and 8 sub-categories of clinical tasks, and exempting from train-validate contamination. We further provide an in-depth analysis of 6 Med-MLLMs and compare them with 5 human specialists, providing insights into their competencies and limitations in various medical contexts. Our work not only advances the understanding of Med-MLLMs' capabilities but also sets a precedent for future evaluations and the safe deployment of these models in clinical environments. We launch and maintain a leaderboard for community assessment of Med-MLLM capabilities (https://asclepius-med.github.io/)",
    "checked": true,
    "id": "a3d418b4e35a02e4306505ab660a6bcd44c3c752",
    "semantic_title": "asclepius: a spectrum evaluation benchmark for medical multi-modal large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=89l-S02jYt-": {
    "title": "PathReasoner: Modeling Reasoning Path with Equivalent Extension for Logical Question Answering",
    "volume": "review",
    "abstract": "Logical reasoning task has attracted great interest since it was proposed. Faced with such a task, current competitive models, even large language models (e.g., ChatGPT and PaLM 2), still perform badly. Previous promising LMs struggle in logical consistency modeling and logical structure perception. To this end, we model the logical reasoning task by transforming each logical sample into reasoning paths and propose an architecture \\textbf{PathReasoner}. It addresses the task from the views of both data and model. To expand the diversity of the logical samples, we propose an atom extension strategy supported by equivalent logical formulas, to form new reasoning paths. From the model perspective, we design a stack of transformer-style blocks. In particular, we propose a path-attention module to joint model in-atom and cross-atom relations with the high-order diffusion strategy. Experiments show that PathReasoner achieves competitive performances on two logical reasoning benchmarks and great generalization abilities",
    "checked": true,
    "id": "a14cf378cc00f960b3f040258972f305ebac5587",
    "semantic_title": "pathreasoner: modeling reasoning path with equivalent extension for logical question answering",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=arnDhtt0uh": {
    "title": "ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis",
    "volume": "review",
    "abstract": "Large language models (LLMs) have achieved commendable accomplishments in various natural language processing tasks. However, LLMs still encounter significant challenges when dealing with complex scenarios involving multiple entities. These challenges arise from the presence of implicit relationships that demand multi-step reasoning. In this paper, we propose a novel approach ERA-CoT, which aids LLMs in understanding context by capturing relationships between entities and supports the reasoning of diverse tasks through Chain-of-Thoughts (CoT).Experimental results show that ERA-CoT demonstrates the superior performance of our proposed method compared to current CoT prompting methods, achieving a significant improvement of an average of 5.1\\% on GPT3.5 compared to previous SOTA baselines. Our analysis indicates that ERA-CoT increases the LLM's understanding of entity relationships, significantly improves the accuracy of question answering, and enhances the reasoning ability of LLMs",
    "checked": true,
    "id": "5c62af6925858d159b2d05fcadbb75ff3a07b70a",
    "semantic_title": "era-cot: improving chain-of-thought through entity relationship analysis",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=dfrhDdJB7n": {
    "title": "Voices in a Crowd: Searching for clusters of unique perspectives",
    "volume": "review",
    "abstract": "Fine-tuned models have been shown to reproduce underlying biases existing in their training data, which is by default majority perspective. While this process has been shown to minimise minority perspectives, proposed solutions either fail to preserve nuances of the original data, or are based on strong a-priori assumptions about annotators that when used bias model training. We propose an approach that trains models purely in an annotator demographic-agnostic manner, extracts latent embeddings informed by annotator behaviour during training, and clusters annotators based on their behaviour over the respective corpus. Resulting clusters are subsequently validated post-hoc via internal and external validative quantitative metrics, as well as our resulting qualitative analysis. Our results explain the strong generalisation capability of our framework, indicated by resulting clusters being adequately robust, while also capturing minority perspectives based on different demographic factors throughout two distinct datasets",
    "checked": true,
    "id": "721a2f35d4dc8479d05b03657ce1b8f51f27be4a",
    "semantic_title": "voices in a crowd: searching for clusters of unique perspectives",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VmhWLx4PfhP": {
    "title": "Scaling Cross-lingual Transfer via Continual Pre-training",
    "volume": "review",
    "abstract": "Large Language models(LLMs) have developed rapidly in recent years, and are remarked as a brand new milestone in the information age. However, powerful LLMs in vogue are predominantly mainstream language speakers, especially in English, and the desire for native LLMs remains strong. Inspired by these demands, we examine the scaling of multilingual models, focusing on the interplay between language-specific computational requirements and universal scaling laws. Our findings demonstrate that continual pre-training of an other-language model on an English base effectively maintains English proficiency while improving other-language performance, challenging traditional notions of cross-lingual transfer, which is commonly equated with fine-tuning. We propose a strategic approach for efficient multilingual training, emphasizing the balance between computational resource allocation and avoiding catastrophic forgetting. Our work helps to understand language-independent model scaling behaviors and transform \"outsiders\" into \"locals\" with basic capacities mostly preserved",
    "checked": false,
    "id": "d44c8eb860a607b74034a7e54b26877048e05ed1",
    "semantic_title": "analyzing bert cross-lingual transfer capabilities in continual sequence labeling",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=5vDWsbMjq1g": {
    "title": "DeLVe-SQL: Decoupled Latent Variable Models for Few-Shot Text-to-SQL",
    "volume": "review",
    "abstract": "Few-shot single-table text-to-sql tasks present considerable challenges due to the constraints of limited training data. Existing approaches primarily transform this problem into column-based classification tasks and utilize self-training methods to leverage unlabeled texts with pseudo-labels. The critical challenge, however, lies in selecting high-quality pseudo-labels and incorporating them effectively into model training. Past self-training techniques selected pseudo SQL predictions based on the probabilities yielded by column-specific classifiers. This approach may not align well with the original queries, especially given the limited performance of the few-shot classifier. To address these limitations, we introduce a novel approach DeLVe-SQL: a latent variable model specifically designed for few-shot text-to-sql tasks. This model effectively decouples textual and SQL semantics via distinct latent variables, enhancing the classifier's performance. Moreover, we apply an additional GPT2 decoder to take into account the reconstruction probabilities of the original query given pseudo SQL predictions, providing a more refined weighting of pseudo-labels. Our experiments, conducted on both open-domain and domain-specific benchmarks, demonstrate that our proposed method delivers promising results, outperforming existing methods in few-shot scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3pKoSqwC9Fi": {
    "title": "Enabling Ensemble Learning for Heterogeneous Large Language Models with Deep Parallel Collaboration",
    "volume": "review",
    "abstract": "Large language models (LLMs) have shown complementary strengths on various tasks and cases, motivating the research of ensembling LLMs to push the frontier utilizing the wisdom of the crowd.Existing work achieves this goal via training the extra reward model or fusion model to perform selection or fusion on all candidate answers.However, these methods pose a great challenge to the generalizability of the trained models.Besides, existing methods use the textual responses as communication media, ignoring the rich information in the inner representations of neural networks.Therefore, we propose a training-free ensemble framework \\textbf{\\textsc{DeePEn}}, averaging the probability distributions outputted by different LLMs.A key challenge in this paradigm is the vocabulary discrepancy between heterogeneous LLMs, which hinders the operation of probability distribution averaging. To address this challenge, \\textsc{DeePEn} maps the probability distribution of each model from the probability space to a universe \\textit{relative space} based on the relative representation theory, and performs aggregation.Then, the result of aggregation is mapped back to the probability space of one LLM via a search-based inverse transformation to determine the generated token.We conduct experiments on the ensemble of various LLMs of 6B to 70B.Experimental results show that \\textsc{DeePEn} achieves consistent improvements across six popular benchmarks involving subject examination, reasoning and knowledge-QA, proving the effectiveness of our approach",
    "checked": false,
    "id": "199cbde8658dee02643f0352a62ab51657924d08",
    "semantic_title": "ensemble learning for heterogeneous large language models with deep parallel collaboration",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=2nffDyJxkfe": {
    "title": "Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies",
    "volume": "review",
    "abstract": "Conceptual spaces represent entities in terms of their primitive semantic features. Such representations are highly valuable but they are notoriously difficult to learn, especially when it comes to modelling perceptual and subjective features. Distilling conceptual spaces from Large Language Models (LLMs) has recently emerged as a promising strategy. However, existing work has been limited to probing pre-trained LLMs using relatively simple zero-shot strategies. We focus in particular on the task of ranking entities according to a given conceptual space dimension. Unfortunately, we cannot directly fine-tune LLMs on this task, because ground truth rankings for conceptual space dimensions are rare. We therefore use more readily available features as training data and analyse whether the ranking capabilities of the resulting models transfer to perceptual and subjective features. We find that this is indeed the case, to some extent, but having perceptual and subjective features in the training data seems essential for achieving the best results. We furthermore find that pointwise ranking strategies are competitive against pairwise approaches, in defiance of common wisdom",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M5L1vYTGLV": {
    "title": "Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models",
    "volume": "review",
    "abstract": "The process of meaning composition, wherein smaller units like morphemes or words combine to form the meaning of phrases and sentences, is essential for human sentence comprehension. Despite extensive neurolinguistic research into the brain regions involved in meaning composition, a computational metric to quantify the extent of composition is still lacking. Drawing on the key-value memory interpretation of transformer feed-forward network blocks, we introduce the Composition Score, a novel model-based metric designed to quantify the degree of meaning composition during sentence comprehension. Experimental findings show that this metric correlates with brain clusters associated with word frequency, structural processing, and general sensitivity to words, suggesting the multifaceted nature of meaning composition during human sentence comprehension",
    "checked": true,
    "id": "e783bbc72c3bf62647b5dc919255db8aab7d0335",
    "semantic_title": "measuring meaning composition in the human brain with composition scores from large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QmgzZN-J263": {
    "title": "Hire a Linguist!: Learning Endangered Languages with In-Context Linguistic Descriptions",
    "volume": "review",
    "abstract": "How can large language models (LLMs) process and translate endangered languages?Many languages lack a large corpus to train a decent LLM; therefore existing LLMs rarely perform well in unseen, endangered languages.On the contrary, we observe that 2000 endangered languages, though without a large corpus, have a grammar book or a dictionary. We propose \\method, a training-free approach to enable an LLM to process unseen languages that hardly occur in its pre-training.Our key insight is to demonstrate linguistic knowledge of an unseen language in an LLM's prompt, including a dictionary, a grammar book, and morphologically analyzed input text.We implement \\method on top of two models, GPT-4 and Mixtral, and evaluate their performance on 5 tasks across 8 endangered or low-resource languages. Our results show that \\method elevates translation capability from GPT-4's 0 to 10.5 BLEU for 10 language directions.Our findings demonstrate the tremendous value of linguistic knowledge in the age of LLMs for endangered languages.Our data, code, and model generations will be released to the public",
    "checked": true,
    "id": "fbf7c1386a188157cdee15e92a5b193ab17d68a7",
    "semantic_title": "hire a linguist!: learning endangered languages with in-context linguistic descriptions",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=ITU5Q3fP_G": {
    "title": "Learning about Word Meaning from Pragmatically Enriched Data",
    "volume": "review",
    "abstract": "The meaning of a natural language utterance can vary greatly depending on the context of the communication. An artificial agent interpreting natural language needs to be able to integrate models of the human speaker and the communicative goal in order to arrive at the correct interpretation. This paper introduces an approach integrating pragmatic reasoning about the conversational partner while learning representations from scratch. This leads to significant improvements over prior work that only considers pragmatics during inference or builds on fixed representations of literal meaning. Our artificial language learner is situated in a referential game about images, where we show that equipping the agent with explicit reasoning about the speaker and the shared observations, leads to faster learning, higher communicative success, and better generalization to changes in the environment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jb_AreGvhxq": {
    "title": "OATH-Frames: Characterizing Online Attitudes towards Homelessness via LLM Assistants",
    "volume": "review",
    "abstract": "Homelessness in the U.S. is widespread, eliciting complex attitudes among individuals (e.g critical as well as sympathetic), often expressed on social media. These attitudes are challenging to summarize at scale, further obfuscating the broader public opinion which is imperative for guiding public policy and reform efforts. Our work proposes an approach to enable a large-scale study on homelessness via two major contributions. First, with the help of domain experts in social work, we characterize Online Attitudes towards Homelessness in nine hierarchical frames (OATH-Frames) on a collection of 4K social media posts. Further, in an effort to ease the annotation, we employ GPT-4 as an LLM assistant to the experts; GPT-4 + Expert annotation presents an attractive trade off owing to a 6.5Ã— speedup in annotation time despite only incurring a 2 point F1 difference in annotation performance. Our effort results in a collection of 8K social media posts labeled by domain experts (with and without GPT-4 assistance). Second, using predicted OATH-Frames on a Flan-T5-Large model trained on our data, we perform a large-scale analysis on 3.1M posts on homelessness. We find marked differences in perceptions towards homelessness between the east and west coast of the U.S. We also find that posts often pit people experiencing homelessness, specifically veterans, against immigrants and asylum seekers as being either more- or less-deserving of resources and aid",
    "checked": false,
    "id": "2816c49543ec0392627de3f0cf05c2107c2f0031",
    "semantic_title": "oath-frames: characterizing online attitudes towards homelessness with llm assistants",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ANqFRy6RERe": {
    "title": "HiRoPE: Length Extrapolation for Code Models Using Hierarchical Position",
    "volume": "review",
    "abstract": "Addressing the limitation of context length in large language models for code-related tasks is the primary focus of this paper. Existing LLMs are constrained by their pre-trained context lengths, leading to performance issues in handling long complex code sequences. Inspired by how human programmers navigate code, we introduce Hierarchical Rotary Position Embedding (HiRoPE), a novel approach that enhances the traditional rotary position embedding into a hierarchical format based on the hierarchical structure of source code. HiRoPE offers easy integration into existing LLMs without extra training costs. Our method is extensively evaluated with various LLMs, demonstrating stable performance in tasks such as language modeling and long code completion. We also introduce a new long code understanding task with real-world code projects, in hopes of promoting further development in this code-related field. Theoretically and experimentally, we find that HiRoPE also addresses the out-of-distribution issue in position encoding. Our HiRoPE significantly expands the context length capabilities of LLMs, enabling inference at lengths exponentially greater than the training length",
    "checked": false,
    "id": "9250c66e8f404c5e5b15bb9d0c10f9a0766b25b7",
    "semantic_title": "hirope: length extrapolation for code models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=8IGmxPSo9zc": {
    "title": "Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph Embedding",
    "volume": "review",
    "abstract": "The primary aim of Knowledge Graph Embeddings (KGE) is to learn low-dimensional representations of entities and relations for predicting missing facts. While rotation-based methods like RotatE and QuatE perform well in KGE, they face two challenges: limited model flexibility requiring proportional increases in relation size with entity dimension, and difficulties in generalizing the model for higher-dimensional rotations. To address these issues, we introduce OrthogonalE, a novel KGE model employing matrices for entities and block-diagonal orthogonal matrices with Riemannian optimization for relations. This approach not only enhances the generality and flexibility of KGE models but also captures several relation patterns that rotation-based methods can identify. Experimental results indicate that our new KGE model, OrthogonalE, offers generality and flexibility, captures several relation patterns, and significantly outperforms state-of-the-art KGE models while substantially reducing the number of relation parameters",
    "checked": true,
    "id": "d8b92da52780452591c81135a779dc5a336210e5",
    "semantic_title": "block-diagonal orthogonal relation and matrix entity for knowledge graph embedding",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iBeqTjM38s_": {
    "title": "xCoT: Cross-lingual Instruction Tuning for Cross-lingual Chain-of-Thought Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "606b535686f98015ea8dd3c9ba8aa9243bc9dc2d",
    "semantic_title": "xcot: cross-lingual instruction tuning for cross-lingual chain-of-thought reasoning",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=HTD0OdZfuEA": {
    "title": "Evolving to be Your Soulmate: Towards Self-evolving Conversational Agents for Companionship with Dynamically Adapted Persona",
    "volume": "review",
    "abstract": "This paper proposes a novel task called Self-evolving Conversational Agents for Companionship (SCAC). It aims to provide users with personalized companionship, where the agent continuously evolves to better meet the user's anticipation by dynamically adapting its persona. Compared to conventional dialogue agents with static personas, SCAC could enable better personalization and long-term engagement. Nonetheless, it also poses new challenges to current conversational AI in many ways. In this paper, we identify three foundational capabilities that an agent must possess to achieve SCAC but are less explored in literature, including persona adaptability, affinity improvement, and smooth transition. They respectively determine whether the agent's evolving process is controllable, whether its evolving direction is appropriate, and whether its transition is natural. To facilitate future development, we present SeaBench, an evaluation framework that comprehensively assesses these capabilities, based on which we evaluate the performance of multiple Large Language Models (LLM)-based agents. Our findings shed light on existing technique limitations and suggest a large room for improvement",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p7ysIgqli6": {
    "title": "OEA: Online Environmental Adaptation for Task-Oriented Language Agents",
    "volume": "review",
    "abstract": "Recent advancements in the field of intelligent agents, especially those leveraging large language models, have been impressively substantial. However, these models still encounter significant challenges in interactive and dynamic scenarios, such as online shopping, mainly due to their lack of knowledge of the current environment. In this paper, we propose an innovative online method for environmental adaptation. The trajectories generated by large language models during task execution are utilized to update a global action-observation tree. When encountering new tasks, our method transforms the action-observation tree into text and integrates this information into the context to aid the model in solving the task. This iterative process enables the model to progressively enhance its understanding of the environment, resulting in steadily improved performance over time. Our approach obviates the need for offline fine-tuning and serves as a versatile plug-and-play solution applicable to various scenarios. In two widely-used environments, Webshop and Alfworld, our method has significantly improved performance beyond ReAct and Reflection, achieving higher accuracy and reducing the required token expenditure",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9I0vxBFTkW2": {
    "title": "From Tweaks to Turmoil: Attacks against Text Summarization Models through Lead Bias and Influence Functions",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have introduced novel opportunities for text comprehension and generation. Yet, they are vulnerable to adversarial perturbations and data poisoning attacks, particularly in tasks like text classification and translation, as evidenced by numerous studies. However, the adversarial robustness of Text Summarization models remains less explored. In this work, we unveil a novel approach by exploiting the inherent lead bias in summarization models, to perform adversarial perturbations. Furthermore, we introduce an innovative application of influence functions, to execute data poisoning, which compromises models' integrity. This approach not only shows a skew in the model's behavior to produce desired outcomes, but also shows a new behavioral change, where models under attack tend to generate extractive summaries rather than abstractive summaries",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ir17_bJANi": {
    "title": "When Locally Deployable Small Models Can Substitute LLMs? An Empirical Study on Active Learning in Real-World Scenarios",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) excel at diverse benchmarking tasks, yet face many deployment barriers in real-world scenarios, such as data privacy and computing resources. On the other hand, low-resource learning techniques like Active Learning (AL) can reduce the annotation cost for fine-tuning locally deployable small models. Subsequently, when and how those AL-assisted small models with low-resource expert annotations can substitute off-the-shelf generic LLMs in real-world scenarios is critical but being overlooked. This empirical study examines AL-assisted small models versus generic LLMs in five real-world tasks with expert annotations. Our AL simulation validates the significance of AL-assisted locally deployable small models as well as the importance of distinct AL sampling strategies in real-world scenarios. We further discuss a promising future paradigm that leverages LLMs to ``warm-up'' AL-assisted small models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ubrcl3UEZj": {
    "title": "Distilling Instruction-following Abilities of Large Language Models with Task-aware Curriculum Planning",
    "volume": "review",
    "abstract": "The process of instruction tuning aligns pre-trained large language models (LLMs) with open-domain instructions and human-preferred responses. While several studies have explored autonomous approaches to distilling and annotating instructions from more powerful proprietary LLMs, such as ChatGPT, they often neglect the impact of task distributions and the varying difficulty of instructions of the training sets. This oversight can lead to imbalanced knowledge capabilities and poor generalization powers of small student LLMs.To address this challenge, we introduce Task-Aware Curriculum Planning for Instruction Refinement (TAPIR), a multi-round distillation framework with balanced task distributions and dynamic difficulty adjustment. This approach utilizes an oracle LLM to select instructions that are difficult for a student LLM to follow and distill instructions with balanced task distributions. By incorporating curriculum planning, our approach systematically escalates the difficulty levels, progressively enhancing the student LLM's capabilities.We rigorously evaluate TAPIR using two widely recognized benchmarks, including AlpacaEval 2.0 and MT-Bench. The empirical results demonstrate that the student LLMs, trained with our method and less training data, outperform larger instruction-tuned models and strong distillation baselines. The improvement is particularly notable in complex tasks, such as logical reasoning and code generation",
    "checked": true,
    "id": "4aca2c5186e9a97a8ac917e9e4a159c2a3a0a0da",
    "semantic_title": "distilling instruction-following abilities of large language models with task-aware curriculum planning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rtqGQrWI9D": {
    "title": "No Culture Left Behind: Massively Multi-Cultural LM Knowledge Acquisition Benchmarking on 1000+ Sub-Country Regions and 2000+ Ethnolinguistic Groups",
    "volume": "review",
    "abstract": "Pretrained large language models have revolutionized many applications but still face challenges related to cultural bias and a lack of cultural commonsense knowledge crucial for guiding cross-culture communication and interactions. Recognizing the shortcomings of existing methods in capturing the diverse and rich cultures across the world, this paper introduces a novel approach for massively multicultural knowledge acquisition. Specifically, our method strategically navigates from densely informative Wikipedia documents on cultural topics to an extensive network of linked pages. Leveraging this valuable source of data collection, we construct the CulturalAtlas dataset, which covers a wide range of sub-country level geographical regions and ethnolinguistic groups, with data cleaning and preprocessing to ensure textual assertion sentence self-containment, as well as fine-grained cultural profile information extraction. Our dataset not only facilitates the evaluation of language model performance in culturally diverse contexts but also serves as a foundational tool for the development of culturally sensitive and aware language models. Our work marks an important step towards deeper understanding and bridging the gaps of cultural disparities in AI, to promote a more inclusive and balanced representation of global cultures in the digital domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NrV1QssmVq": {
    "title": "Self-Assist: Deliberate Tool Selection by Large Language Models",
    "volume": "review",
    "abstract": "With the swift progress in tool-based learning, the number of tools available has also increased significantly. In comparison to the correct utilization of tools, the significance of precisely choosing the appropriate tool from an increasingly large selection is crucial. At present, depending solely on retrieval methods that use keywords and embeddings faces new challenges in the realm of tool selection. On one hand, it becomes difficult to distinguish between tools with similar functionalities. On the other hand, some queries require further reasoning to uncover the true tool needs, where direct matching with keywords or semantic embeddings does not yield the correct result. To address this issue, we introduce the Self-Assist method, which fully leverages the inherent knowledge and reasoning capabilities of large language models. Through a series of systematic steps, large language models actively engage in deliberate thought and select the most appropriate tool for a given query. In essence, our work champions a blend of LLMs and retrieval tools in a flexible, efficient, and universally compatible design, significantly bolstering retrieval outcomes. Evaluations on three datasets reveal superior performance over the previous approaches in retrieval accuracy and overall success",
    "checked": false,
    "id": "10bf4c1ca1531a49dae14d1226e53095306506ff",
    "semantic_title": "lampost: design and evaluation of an ai-assisted email writing prototype for adults with dyslexia",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=sORvtIn4Tv": {
    "title": "LIEDER: Linguistically-Informed Evaluation for Discourse Entity Recognition",
    "volume": "review",
    "abstract": "Discourse Entity (DE) recognition is the task of identifying novel and known entities introduced within a text. While previous work has found that large language models have basic, if imperfect, DE recognition abilities (Schuster and Linzen, 2022), it remains largely unassessed which of the fundamental semantic properties that govern the introduction and subsequent reference to DEs they have knowledge of. We propose the Linguistically-Informed Evaluation for Discourse Entity Recognition (LIEDER) dataset that allows for a detailed examination of language models' knowledge of four crucial semantic properties: existence, uniqueness, plurality, and novelty. We find evidence that state-of-the-art large language models exhibit sensitivity to all of these properties except novelty, which demonstrates that they have yet to reach human-level language understanding abilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wn_N9iIRKi": {
    "title": "Prototypical Reward Network for Data Efficient Model Alignment",
    "volume": "review",
    "abstract": "The reward model for Reinforcement Learning from Human Feedback (RLHF) has proven effective in fine-tuning Large Language Models (LLMs). This paper explores enhancing RLHF with Prototypical Networks to improve reward models. We propose a framework utilizing Prototypical Networks to enhance reward models under limited human feedback, enabling more stable and reliable structural learning from fewer samples. This enhances the model's adaptability and accuracy in interpreting human preferences. Our experiments demonstrate that this approach significantly improves the performance of reward models and LLMs in human feedback tasks, surpassing traditional methods, especially in data-limited scenarios",
    "checked": false,
    "id": "92b3572fb2ca580c55e9f5952d8592ed2f18225a",
    "semantic_title": "prototypical reward network for data-efficient rlhf",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=JH20ch_XEcw": {
    "title": "IndicIRSuite: Multilingual Dataset and Neural Information Models for Indian Languages",
    "volume": "review",
    "abstract": "In this paper, we introduce Neural Information Retrieval resources for 11 widely spoken Indian Languages (Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil, and Telugu) from two major Indian language families (Indo-Aryan and Dravidian). These resources include (a) INDIC-MARCO, a multilingual version of the MS MARCO dataset in 11 Indian Languages created using Machine Translation, and (b) Indic-ColBERT, a collection of 11 distinct Monolingual Neural Information Retrieval models, each trained on one of the 11 languages in the INDIC-MARCO dataset. To the best of our knowledge, IndicIRSuite is the first attempt at building large-scale Neural Information Retrieval resources for a large number of Indian languages, and we hope that it will help accelerate research in Neural IR for Indian Languages. Experiments demonstrate that Indic-ColBERT achieves 47.47\\% improvement in the MRR@10 score averaged over the INDIC-MARCO baselines for all 11 Indian languages except Oriya, 12.26\\% improvement in the NDCG@10 score averaged over the MIRACL Bengali and Hindi Language baselines, and 20\\% improvement in the MRR@100 Score over the Mr. Tydi Bengali Language baseline",
    "checked": true,
    "id": "21a5fd6daf6a955668267f7cc63fbb8b7d6e2ce5",
    "semantic_title": "indicirsuite: multilingual dataset and neural information models for indian languages",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y_8ZTGevd3o": {
    "title": "When Contrastive Learning Meets Bayesian Modeling: Learning Multi-Modal Representation Alignments with Noisy Data-Pairs",
    "volume": "review",
    "abstract": "Contrastive learning (CL) stands as a leading paradigm for self-supervised representation learning, achieving state-of-the-art results in multi-modal learning. However, a notable drawback of standard CL is its lack of robustness in the face of noisy (misaligned) data pairs. For instance, not all negative samples are truly negative; within a mini-batch, there can be negative samples that are semantically similar to positive samples. This issue is prevalent in many web-sourced multimodal datasets like CC3M and YFCC, commonly used for CL, due to their inherently noisy nature during dataset crawling. Consequently, dataset noise could significantly undermine the efficacy of CL. On the other hand, Bayesian modeling is renowned for its inherent capability to handle data noise and uncertainty. Is it possible to merge the strengths of both approaches by incorporating Bayesian modeling into CL for noise-robust representation learning? In this paper, we propose a novel solution by reimagining standard CL within a probability framework and introducing learnable random weights to associate with data pairs. Our framework enables automatic inference of the level of noisiness for each data pair through efficient Bayesian sampling, based on a technique borrowed from Bayesian data augmentation. Importantly, our model can be effectively optimized using a novel learning algorithm based on stochastic expectation maximization. We demonstrate the efficacy of our approach on various standard multi-modal CL benchmarks, showcasing significant performance improvements over standard CL methods",
    "checked": false,
    "id": "0ad84c4bf7499df6945fc51b24ae2ac779f218ec",
    "semantic_title": "vision-language pre-training with triple contrastive learning",
    "citation_count": 227,
    "authors": []
  },
  "https://openreview.net/forum?id=gh7kL3PtCr": {
    "title": "Alignment-Guided Curriculum Learning for Semi-Supervised Code Translation",
    "volume": "review",
    "abstract": "Neural code translation is the task of converting source code from one programming language to another. The scarcity of parallel code data impedes code translation models' ability to learn accurate cross-language alignment, thus restricting performance improvements. In this paper, we introduce MIRACLE, a semi-supervised approach that improves code translation through curriculum learning on code data with ascending alignment levels. It leverages static analysis and compilation to generate synthetic parallel code datasets with enhanced alignment to address the challenge of data scarcity. Extensive experiments show that MIRACLE significantly improves code translation performance on C++, Java, Python, and C, surpassing state-of-the-art models by substantial margins. Notably, it achieves up to a 43% improvement in C code translation with fewer than 150 annotated examples",
    "checked": false,
    "id": "a743064ad78af5cc79e06c69580e08d896dbcb88",
    "semantic_title": "canadian medical education journal together again: a community redefining residency education redÃ©finir la formation des rÃ©sidents, ensemble",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K5N_0Fm6wA": {
    "title": "Automatic Evaluation for Mental Health Counseling using LLMs",
    "volume": "review",
    "abstract": "High-quality psychological counseling is crucial for mental health worldwide, and timely evaluation is vital for ensuring its effectiveness. However, obtaining professional evaluation for each counseling session is expensive and challenging. Existing methods that rely on self or third-party manual reports to assess the quality of counseling suffer from subjective biases and limitations of time-consuming.To address above challenges, this paper proposes an innovative and efficient automatic approach using large language models (LLMs) to evaluate the working alliance in counseling conversations. We collected a comprehensive counseling dataset and conducted multiple third-party evaluations based on therapeutic relationship theory. Our LLM-based evaluation, combined with our guidelines, shows high agreement with human evaluations and provides valuable insights into counseling scripts. This highlights the potential of LLMs as supervisory tools for psychotherapists. By integrating LLMs into the evaluation process, our approach offers a cost-effective and dependable means of assessing counseling quality, enhancing overall effectiveness",
    "checked": true,
    "id": "a4d0fe133121037ccc78d0b73796c61520fa65d9",
    "semantic_title": "automatic evaluation for mental health counseling using llms",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=1oaaHorXGrA": {
    "title": "Visualization Recommendation with Prompt-based Reprogramming of Large Language Models",
    "volume": "review",
    "abstract": "Visualization recommendations, which aim to automatically match proper visual charts for specific data tables, can significantly simplify the data analysis process. Traditional approaches in this domain have primarily relied on rule-based or machine learning-based methodologies. These methods often demand extensive manual maintenance and yet fail to fully comprehend the tabular data, leading to unsatisfactory performance. Recently, Large Language Models (LLMs) have emerged as powerful tools, exhibiting strong reasoning capabilities. This advancement suggests their substantial promise in addressing visualization recommendation challenges. However, effectively harnessing LLMs to discern and rationalize patterns in tabular data, and consequently deduce the essential information for chart generation, remains an unresolved challenge. To this end, we introduce a novel Hierarchical Table Prompt-based reprogramming framework, named HTP. This framework aims to integrate multi-dimensional tabular data into LLMs through a strategically crafted prompt learning method while keeping the LLMs' backbone and weights unaltered. The HTP framework uniquely incorporates a four-level prompt structure, encompassing general, instance, cluster, and column levels. This multi-level approach is engineered to provide a comprehensive understanding of both general distribution and multifaceted fine-grained features of tabular data, before inputting the tabular data into the frozen LLM. Our empirical studies confirm that the HTP framework achieves state-of-the-art performance, marking an advancement in the field of data visualization and analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M8fkqqNGrxV": {
    "title": "Unveiling Linguistic Regions in Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated considerable cross-lingual alignment and generalization ability. Current research primarily focuses on improving LLMs' cross-lingual generalization capabilities. However, there is still lacks of research on the intrinsic mechanisms of how LLMs achieve cross-lingual alignment. From the perspective of region partitioning, this paper conducts several investigations on the linguistic competence of LLMs.We discover a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters. Removing this core region by setting parameters to zero results in a significant performance decrease across 30 different languages. Furthermore, this core region exhibits significant dimensional dependency, perturbations to even a single parameter on specific dimensions leading to a loss of linguistic competence. Moreover, we discover that distinct regions exist for different monolingual families, and disruption to these specific regions substantially reduces the LLMs' proficiency in those corresponding languages. Our research also indicates that freezing the core linguistic region during further pre-training can mitigate the issue of catastrophic forgetting (CF), a common occurrence observed during further pre-training of LLMs. Overall, exploring the LLMs' functional regions provides insights into the foundation of their intelligence",
    "checked": true,
    "id": "90f6eb019a8126e54c6e6799a3bfbc2935ff7f26",
    "semantic_title": "unveiling linguistic regions in large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=vyb4tS4ONSK": {
    "title": "More \"Clever\" than \"Hans\": Probing and Adversarial Training in Translationese Classification",
    "volume": "review",
    "abstract": "Modern classifiers, especially neural networks, excel at leveraging faint and subtle signals competing with many other signals in the data. When such potentially noisy setups lead to high accuracy rates (e.g., 90%+), it produces concerns about the authenticity of the results, raising questions about potential spurious correlations -- a phenomenon often referred to as \"Clever Hans\". We explore this phenomenon in the context of translationese classification, where previous work has found indirect and episodic evidence that a high-performance BERT classifier learns to use spurious topic information rather than just translationese signals. In this paper, we first use probing to provide direct evidence that high-performance translationese classifiers pick up unknown potentially spurious topic correlations. We then introduce adversarial training as a strategy to mitigate any such potentially spurious topic correlations, where previous work was only able to mitigate specific known (episodic) Clever Hans. We demonstrate the effectiveness of our approach on translationese classification tasks on two translation pairs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sx5wUJ7J5o": {
    "title": "ProTrix: Building Models for Planning and Reasoning over Tables with Sentence Context",
    "volume": "review",
    "abstract": "Tables play a crucial role in conveying information in various domains, serving as indispensable tools for organizing and presenting data in a structured manner. We propose a \\textit{Plan-then-Reason} framework to answer different types of user queries over tables with sentence context. The framework first plans the reasoning paths over the context, then assigns each step to program-based or textual reasoning to reach the final answer. We construct an instruction tuning set \\texttt{TrixInstruct} following the framework. Our dataset cover queries that are program-unsolvable or need combining information from tables and sentences to obtain planning and reasoning abilities. We present \\textsc{ProTrix} by finetuning Llama-2-7B on \\texttt{TrixInstruct}. Our experiments show that \\textsc{ProTrix} generalizes to diverse tabular tasks and achieves comparable performance to GPT-3.5-turbo. We further demonstrate that \\textsc{ProTrix} can generate accurate and faithful explanations to answer complex free-form questions. Our work underscores the importance of the planning and reasoning abilities towards a model over tabular tasks with generalizability and interpretability. We will release our dataset and model to the research community",
    "checked": true,
    "id": "2268d8104f4dd921fd90f4d2d6df9ecf8e7ce3eb",
    "semantic_title": "protrix: building models for planning and reasoning over tables with sentence context",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FHxXTO_tAI": {
    "title": "Contrastive Classification via Linear Layer Extrapolation",
    "volume": "review",
    "abstract": "Early-exiting predictions in a deep Transformer network evolve from layer to layer in a somewhat smooth process. This has been exploited in language modeling to improve factuality, with the observation that factual associations emerge in later layers. We find that a similar process occurs in standard multiway text classification, motivating us to propose Linear Layer Extrapolation, which finds stable improvements by recasting contrastive inference as linear extrapolation. Experiments across multiple models and emotion classification datasets find that Linear Layer Extrapolation outperforms standard classification on fine-grained sentiment analysis tasks",
    "checked": false,
    "id": "9624fc969ddeb81753b17fb96b174a59b537ffbb",
    "semantic_title": "efficient data generation for stroke classification via multilayer perceptron",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=oJmj2zwevC": {
    "title": "RAM3C: Enhancing Goal-oriented Open-ended Dialogue-based Educational System by Retrieval-augmented Multi-agent Collaboration",
    "volume": "review",
    "abstract": "This study presents the Retrieval-Augmented Multi-role Multi-agent Multi-round Collaboration (RAM3C) system, designed to improve the overall effectiveness of open-ended dialogue-based educational systems. Focusing on aspects of Humanlikeness, Individualization, Teaching expertise and Safety (HITS), RAM3C utilizes a dynamic framework that incorporates multi-agent, multi-round collaboration with multiple roles to harness collective expertise. RAM3C equips agents with tailored, multi-source knowledge bases and implements a history-sampling weighted retrieval-fusion approach to generate diverse, accurate, and safe educational dialogues. Our evaluation on a scenario of \"Literature Discussion Class\" by human volunteers and a decentralized, LLM-emulated expert group, confirms RAM3C's capability to deliver high-quality educational experiences, underscoring its substantial potential to elevate educational quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bVBfqNCtHb": {
    "title": "Creating an AI Observer: Generative Semantic Workspaces",
    "volume": "review",
    "abstract": "An experienced human Observer reading a document --such as a crime report-- creates a succinct plot-like \\textit{``Working Memory''} comprising different actors, their prototypical roles and states at any point, their evolution over time based on their interactions, and even a map of missing Semantic parts anticipating them in the future. \\textit{An equivalent AI Observer currently does not exist}. We introduce the \\textbf{[G]}enerative \\textbf{[S]}emantic \\textbf{[W]}orkspace (GSW) -- comprising an \\textit{``Operator''} and a \\textit{``Reconciler''} -- that leverages advancements in LLMs to create a generative-style Semantic framework, as opposed to a traditionally predefined set of lexicon labels. Given a text segment $C_n$ that describes an ongoing situation, the \\textit{Operator} instantiates actor-centric Semantic maps (termed ``Workspace instance'' $\\mathcal{W}_n$). The \\textit{Reconciler} resolves differences between $\\mathcal{W}_n$ and a ``Working memory'' $\\mathcal{M}_n^*$ to generate the updated $\\mathcal{M}_{n+1}^*$. GSW outperforms well-known baselines on several tasks ($\\sim 94\\%$ vs. FST, GLEN, BertSRL - multi-sentence Semantics extraction, $\\sim 15\\%$ vs. NLI-BERT, $\\sim 35\\%$ vs. QA). By mirroring the real Observer, GSW provides the first step towards Spatial Computing assistants capable of understanding individual intentions and predicting future behavior",
    "checked": true,
    "id": "f0e03d09bdb956096db8c7ae314ad23e29015a80",
    "semantic_title": "creating an ai observer: generative semantic workspaces",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_bgfqG_viq": {
    "title": "Exploring Fine-Grained Early Exiting For Simultaneous Translation",
    "volume": "review",
    "abstract": "Simultaneous translation is a difficult subset of neural machine translation with incomplete source context during most translation decisions that balances translation quality with latency as equal goals. Given that, a variety of computational reduction methods are popular for simultaneous translation. One under-explored method is early exiting, where tokens in a sequence cease computation before the final layer of a given model, being fed directly to another stack or classifier instead. In this work, we explore applying early exiting techniques popular with transformers to speech-to-text simultaneous translation (SimulST), achieving up to 7.0% savings on encoder FLOPs with a reduction of only 0.42 BLEU",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LfJWsPsYd0": {
    "title": "Data Augmentation via Large Language Models and UMLS for Few-shot Named Entity Recognition in Medical Texts",
    "volume": "review",
    "abstract": "Few-shot learning with large language models holds substantial potential in the biomedical domain where obtaining extensive annotated data for specialized tasks can often be challenging. In the presence of small, annotated datasets, incorporating domain knowledge from external sources is a common strategy. In this paper, we explore knowledge augmentation strategies for biomedical named entity recognition (NER) by incorporating information encapsulated in the Unified Medical Language System (UMLS). We leverage UMLS knowledge along with its hierarchical structure, and information from large language models (LLMs) to automatically generate new training examples in few-shot settings. We further explore the viability of employing GPT-3.5 for the extraction of biomedical named entities from Reddit data focused on prescription and illicit opioids. The results show an improvement of 13\\% on the F$_1$-score on average over five established NER datasets, and a 6\\% increase on the Reddit-Impacts dataset after appropriate prompt engineering improvements. Our findings indicate that utilizing UMLS and LLMs as a joint source of prior knowledge can be a viable approach for improving the state of the art for few-shot learning-based NER in medical text",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xPmXPinkOlg": {
    "title": "Inquire, Interact, and Integrate: A Proactive Agent Collaborative Framework for Zero-Shot Multimodal Medical Reasoning",
    "volume": "review",
    "abstract": "The adoption of large language models (LLMs) in healthcare has attracted significant research interest. However, their performance in healthcare remains under-investigated and potentially limited, due to i) they lack rich domain-specific knowledge and medical reasoning skills; and ii) most state-of-the-art LLMs are unimodal, text-only models that cannot directly process multimodal inputs. To this end, we propose a multimodal medical collaborative reasoning framework \\textbf{MultiMedRes}, which incorporates a learner agent to proactively gain essential information from domain-specific expert models, to solve medical multimodal reasoning problems. Our method includes three steps: i) Inquire: The learner agent first decomposes given complex medical reasoning problems into multiple domain-specific sub-problems; ii) Interact: The agent then interacts with domain-specific expert models by repeating the ``ask-answer'' process to progressively obtain different domain-specific knowledge; iii) Integrate: The agent finally integrates all the acquired domain-specific knowledge to accurately address the medical reasoning problem. We validate the effectiveness of our method on the task of difference visual question answering for X-ray images. The experiments demonstrate that our zero-shot prediction achieves state-of-the-art performance, and even outperforms the fully supervised methods. Besides, our approach can be incorporated into various LLMs and multimodal LLMs to significantly boost their performance",
    "checked": true,
    "id": "0c0ad536bb7f9bf86b671e1ebbb7a541fd70c669",
    "semantic_title": "inquire, interact, and integrate: a proactive agent collaborative framework for zero-shot multimodal medical reasoning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sYljLzIFCq_": {
    "title": "Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A Case-Study in E-Commerce Opinion Summarization",
    "volume": "review",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has become a dominating strategy in steering Language Models (LMs) towards human values/goals. The key to the strategy is employing a reward model (Ïˆ) which can reflect a latent reward model with humans. While this strategy has proven to be effective, the training methodology requires a lot of human preference annotation (usually of the order of tens of thousands) to train Ïˆ. Such large-scale preference annotations can be achievable if the reward model can be ubiquitously used. However, human values/goals are subjective and depend on the nature of the task. This poses a challenge in collecting diverse preferences for downstream applications. To address this, we propose a novel methodology to infuse domain knowledge into Ïˆ which reduces the size of preference annotation required. We validate our approach in E-Commerce Opinion Summarization, with a significant reduction in dataset size (just $940$ samples) while advancing the state-of-the-art. Our contributions include a novel Reward Modelling technique, a new dataset (PromptOpinSumm) for Opinion Summarization, and a human preference dataset (OpinPref). The proposed methodology opens avenues for efficient RLHF, making it more adaptable to diverse applications with varying human values",
    "checked": true,
    "id": "c14b58a49667fb0990759905ea3b874d50a983d1",
    "semantic_title": "leveraging domain knowledge for efficient reward modelling in rlhf: a case-study in e-commerce opinion summarization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FAA7ZQrB5h": {
    "title": "Refining and Synthesis: A Simple yet Effective Data Augmentation Framework for Cross-Domain Aspect-based Sentiment Analysis",
    "volume": "review",
    "abstract": "Aspect-based Sentiment Analysis (ABSA) is extensively researched in the NLP community, yet related models face challenges due to data sparsity when shifting to a new domain. Hence, data augmentation for cross-domain ABSA has attracted increasing attention in recent years. However, two key points have been neglected in prior studies: First, target domain unlabeled data are labeled with pseudo labels by the model trained in the source domain with little quality control, leading to inaccuracy and error propagation. Second, the label and text patterns of generated labeled data are monotonous, thus limiting the robustness and generalization ability of trained ABSA models. In this paper, we aim to design a simple yet effective framework to address the above shortages in ABSA data augmentation, called Refining and Synthesis Data Augmentation (RSDA). Our framework roughly includes two steps: First, it refines generated labeled data using a natural language inference (NLI) filter to control data quality. Second, it synthesizes diverse labeled data via novel label composition and paraphrase approaches. We conduct experiments on 4 kinds of ABSA subtasks, and our framework outperforms 7 strong baselines, demonstrating its effectiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UTD9auzpZ4": {
    "title": "CodeGRAG: Extracting Composed Syntax Graphs for Retrieval Augmented Cross-Lingual Code Generation",
    "volume": "review",
    "abstract": "Utilizing large language models to generate codes has shown promising meaning in software development revolution. Despite the intelligence shown by the general large language models, their specificity in code generation can still be improved due to the syntactic gap and mismatched vocabulary existing among natural language and different programming languages. In addition, programming languages are inherently logical and complex, making them hard to be correctly generated. Existing methods rely on multiple prompts to the large language model to explore better solutions, which is expensive.In this paper, we propose Syntax Graph Retrieval Augmented Code Generation (CodeGARG) to enhance the performance of LLMs in single-round code generation tasks. CodeGARG extracts and summarizes the control flow and data flow of code blocks to fill the gap between programming languages and natural language. The extracted external structural knowledge models the inherent flows of code blocks, which can facilitate LLMs for better understanding of code syntax and serve as a bridge among different programming languages. CodeGARG significantly improves the code generation ability of LLMs and can even offer performance gain for cross-lingual code generation, e.g., C++ for Python",
    "checked": true,
    "id": "547a9e49af0f9aedc2babb6e76990c837ec17b09",
    "semantic_title": "codegrag: extracting composed syntax graphs for retrieval augmented cross-lingual code generation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=7rtrluqvDh": {
    "title": "Improving LLM-based Machine Translation with Systematic Self-Correction",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have achieved impressive results in Machine Translation (MT). However, careful evaluations by human reveal that the translations produced by LLMs still contain multiple errors. Importantly, feeding back such error information into the LLMs can lead to self-correction and result in improved translation performance. Motivated by these insights, we introduce a systematic LLM-based self-correcting translation framework, named TER, which stands for Translate, Estimate, and Refine, marking a significant step forward in this direction. Our findings demonstrate that 1) our self-correction framework successfully assists LLMs in improving their translation quality across a wide range of languages, whether it's from high-resource languages to low-resource ones or whether it's English-centric or centered around other languages; 2) TER exhibits superior systematicity and interpretability compared to previous methods; 3) different estimation strategies yield varied impacts on AI feedback, directly affecting the effectiveness of the final corrections. We further compare different LLMs and conduct various experiments involving self-correction and cross-model correction to investigate the potential relationship between the translation and evaluation capabilities of LLMs. The code will be made available upon publication",
    "checked": true,
    "id": "d3575a5ae995268f4cd78141833667f4eefcf1e6",
    "semantic_title": "improving llm-based machine translation with systematic self-correction",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=VGXEMYWiwbs": {
    "title": "Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model",
    "volume": "review",
    "abstract": "The detection of machine-generated text, especially from large language models (LLMs), is crucial in preventing serious social problems resulting from their misuse. Some methods train dedicated detectors on specific datasets but fall short in generalizing to unseen test data, while other zero-shot ones often yield suboptimal performance. Although the recent DetectGPT has shown promising detection performance, it suffers from significant inefficiency issues, as detecting a single candidate requires querying the source LLM with hundreds of its perturbations. This paper aims to bridge this gap. Concretely, we propose to incorporate a Bayesian surrogate model, which allows us to select typical samples based on Bayesian uncertainty and interpolate scores from typical samples to other samples, to improve query efficiency. Empirical results demonstrate that our method significantly outperforms existing approaches under a low query budget. Notably, when detecting the text generated by LLaMA family models, our method with just 2 or 3 queries can outperform DetectGPT with 200 queries",
    "checked": true,
    "id": "2eeff1edde4fc66284e7bedeb8eb8878ed4560f4",
    "semantic_title": "efficient detection of llm-generated texts with a bayesian surrogate model",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=eBQuF5Hb8V": {
    "title": "SHED: Shapley-Based Automated Dataset Refinement for Instruction Fine-Tuning",
    "volume": "review",
    "abstract": "The pre-trained Large Language Models (LLMs) can be adapted for many downstream tasks and tailored to align with human preferences through fine-tuning. Recent studies have discovered that LLMs can achieve desirable performance with only a small amount of high-quality data, suggesting that a large amount of the data in these extensive datasets is redundant or even harmful. Identifying high-quality data from vast datasets to curate small yet effective datasets has emerged as a critical challenge. In this paper, we introduce SHED, an automated dataset refinement framework based on Shapley value for instruction fine-tuning. SHED eliminates the need for human intervention or the use of commercial LLMs. Moreover, the datasets curated through SHED exhibit transferability, indicating they can be reused across different models with consistently high performance. We conduct extensive experiments to evaluate the datasets curated by SHED. The results demonstrate the superiority of SHED compared to state-of-the-art methods across various tasks and models, i.e. only 10% of the original dataset selected by SHED achieves performance comparable to or even better than the original datasets",
    "checked": true,
    "id": "e67a84e7301d91155989e4f3bef004f424eceb5d",
    "semantic_title": "shed: shapley-based automated dataset refinement for instruction fine-tuning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=iM4XFJvUv2M": {
    "title": "LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error",
    "volume": "review",
    "abstract": "Tools are essential for large language models (LLMs) to acquire up-to-date information and take consequential actions in external environments. Existing work on tool-augmented LLMs primarily focuses on the broad coverage of tools and the flexibility of adding new tools. However, a critical aspect that has surprisingly been understudied is simply how accurately an LLM uses tools for which it has been trained. We find that existing LLMs, including GPT-4 and open-source LLMs specifically fine-tuned for tool use, only reach a correctness rate in the range of 30% to 60%, far from reliable use in practice. We propose a biologically inspired method for tool-augmented LLMs, simulated trial and error (STE), that orchestrates three key mechanisms for successful tool use behaviors in the biological system: trial and error, imagination, and memory. Specifically, STE leverages an LLM's 'imagination' to simulate plausible scenarios for using a tool, after which the LLM interacts with the tool to learn from its execution feedback. Both short-term and long-term memory are employed to improve the depth and breadth of the exploration, respectively. Comprehensive experiments on ToolBench show that STE substantially improves tool learning for LLMs under both in-context learning and fine-tuning settings, bringing a boost of 46.7% to Mistral-Instruct-7B and enabling it to outperform GPT-4. We also show effective continual learning of tools via a simple experience replay strategy",
    "checked": true,
    "id": "ae4635297ad87fcb3ec4105a51b5cbcb4075e5e2",
    "semantic_title": "llms in the imaginarium: tool learning through simulated trial and error",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ZqQ50ahULP": {
    "title": "Stumbling Blocks: Stress Testing the Robustness of Machine-Generated Text Detectors Under Attacks",
    "volume": "review",
    "abstract": "The widespread use of large language models (LLMs) is increasing the demand for methods that detect machine-generated text to prevent misuse. The goal of our study is to stress test the detectors' robustness to malicious attacks under realistic scenarios. We comprehensively study the robustness of popular machine-generated text detectors under attacks from diverse categories: editing, paraphrasing, prompting, and co-generating. Our attacks assume limited access to the generator LLMs, and we compare the performance of detectors on different attacks under different budget levels. Our experiments reveal that almost none of the existing detectors remain robust under all the attacks, and all detectors exhibit different loopholes. Averaging all detectors, the performance drops by 35% across all attacks. Further, we investigate the reasons behind these defects and propose initial out-of-the-box patches to improve robustness",
    "checked": true,
    "id": "5a8a6b61033ba2355f7c149cec596c88a1d61954",
    "semantic_title": "stumbling blocks: stress testing the robustness of machine-generated text detectors under attacks",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=sxXV2-mK1-": {
    "title": "Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback",
    "volume": "review",
    "abstract": "Large language models (LLMs) have shown remarkable progress in automated code generation. Yet, incorporating LLM-based code generation into real-life software projects poses challenges, as the generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. To this end, this paper puts forward a novel approach, termed CoCoGen, which iteratively refines the project-level code context for precise code generation, guided by the compiler feedback. In particular, CoCoGen first leverages compiler techniques to identify a mismatch between the generated code and the project's context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate CoCoGen with two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code generation. Experimental results show that CoCoGen significantly improves the vanilla LLMs by over 80% in generating code dependent on project context, and consistently outperforms the existing retrieval-based code generation baselines",
    "checked": true,
    "id": "9aa6a885754a27fe42a87e4dfaed87d618fd8518",
    "semantic_title": "iterative refinement of project-level code context for precise code generation with compiler feedback",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=5-qBpH9VhxX": {
    "title": "Multi-Objective Linguistic Control of Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs), despite their breakthroughs on many challenging benchmark tasks, prefer to generate verbose responses and lack the controllability of output complexity, which is usually preferred by human users in practice. In this paper, we study how to precisely control multiple linguistic complexities of LLM output by finetuning using off-the-shelf data. To this end, we propose multi-control tuning (MCTune), which includes multiple linguistic complexity values of ground-truth responses as controls in the input for instruction tuning. We finetune LLaMA2-7B on Alpaca-GPT4 and WizardLM datasets. Evaluations on widely used benchmarks demonstrate that our method does not only improve LLMs' multi-complexity controllability substantially but also retains or even enhances the quality of the responses as a side benefit",
    "checked": true,
    "id": "9a9253e1a8e5b22800d8a17b5b10c25306dd009c",
    "semantic_title": "multi-objective linguistic control of large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7uDkOmmQGvw": {
    "title": "LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation",
    "volume": "review",
    "abstract": "The rise of multimodal misinformation on social platforms poses significant challenges for individuals and societies. Its increased credibility and broader impact compared to textual misinformation make detection complex, requiring robust reasoning across diverse media types and profound knowledge for accurate verification. The emergence of Large Vision Language Model (LVLM) offers a potential solution to this problem. Leveraging their proficiency in processing visual and textual information, LVLM demonstrates promising capabilities in recognizing complex information and exhibiting strong reasoning skills. In this paper, we first investigate the potential of LVLM on multimodal misinformation detection. We find that even though LVLM has a superior performance compared to LLMs, its profound reasoning may present limited power with a lack of evidence. Based on these observations, we propose LEMMA: LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation. LEMMA leverages LVLM intuition and reasoning capabilities while augmenting them with external knowledge to enhance the accuracy of misinformation detection. Our method improves the accuracy over the top baseline LVLM by 7% and 13% on Twitter and Fakeddit datasets respectively",
    "checked": true,
    "id": "00c6a500726a85090e1b2e7e8f4c5226ff56d86d",
    "semantic_title": "lemma: towards lvlm-enhanced multimodal misinformation detection with external knowledge augmentation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=UoY4sgvb83": {
    "title": "Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) tend to be unreliable on fact-based answers.To address this problem, NLP researchers have proposed a range of techniques to estimate LLM's confidence over facts. However, due to the lack of a systematic comparison, it is not clear how the different methods compare to one other.To fill this gap, we present a rigorous survey and empirical comparison of estimators of factual confidence.We define an experimental framework allowing for fair comparison, covering both fact-verification and QA. Our experiments across a series of LLMs indicate that trained hidden-state probes provide the most reliable confidence estimates; albeit at the expense of requiring access to weights and supervision data. We also conduct a deeper assessment of the methods, in which we measure the consistency of model behavior under meaning-preserving variations in the input. We find that the factual confidence of LLMs is often unstable across semantically equivalent inputs, suggesting there is much room for improvement for the stability of models' parametric knowledge",
    "checked": true,
    "id": "d22dcbeda9982f8758d516c35a07193834f46a11",
    "semantic_title": "factual confidence of llms: on reliability and robustness of current estimators",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HJ_gN_vkb6z": {
    "title": "Seal-Tools: Self-Instruct Tool Learning Dataset for Agent Tuning and Detailed Benchmark",
    "volume": "review",
    "abstract": "This paper presents a new tool learning dataset Seal-Tools, which contains self-instruct API-like tools. Seal-Tools not only offers a large number of tools, but also includes instances which demonstrate the practical application of tools. Seeking to generate data on a large scale while ensuring reliability, we propose a self-instruct method to generate tools and instances, allowing precise control over the process. Moreover, our Seal-Tools contains hard instances that call multiple tools to complete the job, among which some are nested tool callings. For precise and comprehensive evaluation, we use strict format control and design three metrics from different dimensions. Therefore, Seal-Tools can serve as a new benchmark to evaluate the tool-calling ability of LLMs. Finally, we evaluate several prevalent LLMs and our finetuned model on Seal-Tools. The results show that current systems are far from perfect. The code, data and experiment results will be available at https://github.com/",
    "checked": true,
    "id": "94e17ccf5f999689788ba072d8fafa42fe073333",
    "semantic_title": "seal-tools: self-instruct tool learning dataset for agent tuning and detailed benchmark",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=LJo0TAxL8pj": {
    "title": "Topic-driven Distant Supervision Framework for Macro-level Discourse Parsing via Transferring Models",
    "volume": "review",
    "abstract": "Discourse parsing, the task of analyzing the internal rhetorical structure of texts, is a challenging problem in natural language processing due to the complex linguistic structure and lack of large-scale and high-quality corpora, especially at the macro level. Recent studies have attempted to overcome this limitation by utilizing results from other NLP tasks (source task) to distantly supervise the discourse parsing (target task). However, most of them only consider shallow connections across tasks using result-converting methods. It brings more cascading errors and makes it difficult to continue training due to the heterogeneity of the source and target task. To address these issues, we propose a topic-driven distant supervision framework via transferring models. The key recipe of this framework is to transfer the topic segmentation model into a discourse parser by additionally considering the global structural correlation instead of a simple converting result algorithm for transferring knowledge. The experimental results on two RST-style datasets, in both Chinese (MCDTB) and English (RST-DT), demonstrate that our method outperforms strong baselines not only in distant-supervised scenarios but also in fully supervised settings",
    "checked": false,
    "id": "7a8538dcfe0889127d003d6553494f93c6cbd831",
    "semantic_title": "topic-driven distant supervision framework for macro-level discourse parsing",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f9yBQ-aniHF": {
    "title": "MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization",
    "volume": "review",
    "abstract": "Scientific data visualization plays a crucial role in research by enabling the direct display of complex information and assisting researchers in identifying implicit patterns. Despite its importance, the use of Large Language Models (LLMs) for scientific data visualization remains rather unexplored. In this study, we introduce MatPlotAgent, an efficient model-agnostic LLM agent framework designed to automate scientific data visualization tasks. Leveraging the capabilities of both code LLMs and multi-modal LLMs, MatPlotAgent consists of three core modules: query understanding, code generation with iterative debugging, and a visual feedback mechanism for error correction. To address the lack of benchmarks in this field, we present MatPlotBench, a high-quality benchmark consisting of 100 human-verified test cases. Additionally, we introduce a scoring approach that utilizes GPT-4V for automatic evaluation. Experimental results demonstrate that MatPlotAgent can improve the performance of various LLMs, including both commercial and open-source models. Furthermore, the proposed evaluation method shows a strong correlation with human-annotated scores",
    "checked": true,
    "id": "8410e4416df27c6f72df3b691edded84a4e86f16",
    "semantic_title": "matplotagent: method and evaluation for llm-based agentic scientific data visualization",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=uVnc3rhRkL": {
    "title": "Towards Region-aware Bias Evaluation Metrics",
    "volume": "review",
    "abstract": "When exposed to human-generated data, language models are known to learn and amplify societal biases. While previous work has introduced benchmarks that can be used to assess the bias in these models, they rely on assumptions that may not be universally true. For instance, a bias dimension commonly used by these metrics is that of {\\it family--career}, but this may not be the only common bias in certain regions of the world. In this paper, we identify topical differences in gender bias across different regions and propose a region-aware bottom-up approach for bias assessment. Our proposed approach uses gender-aligned topics for a given region and identifies gender bias dimensions in the form of topic pairs that are likely to capture gender societal biases. Several of our proposed bias dimensions are on par with human perception of gender biases in these regions in comparison to the existing ones, and we also identify new dimensions that are more aligned than the existing ones",
    "checked": true,
    "id": "34214e1de58c6b816263e2207c77ff65f093b967",
    "semantic_title": "towards region-aware bias evaluation metrics",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T4IndEbKwhw": {
    "title": "LPZero: Language Model Zero-Cost Proxy Search from Zero",
    "volume": "review",
    "abstract": "Neural Architecture Search (NAS) facilitates the automatic effective neural network designs while requiring the substantial computational resource particularly for language models. Zero-shot NAS exploits Zero-Cost (ZC) proxies to estimate model performance, thereby markedly reducing computational demands. However, existing ZC proxies rely heavily on in-depth expert knowledge and repetitive trial-and-error costs. Moreover, most of existing ZC proxies fail to surpass the performance of the naive baseline (number of parameters).To address these challenges, we introduce a novel framework called \\textbf{LPZero} (\\textbf{L}anguage model zero-cost \\textbf{P}roxy search from \\textbf{Zero}). It is designed to automate the design of efficient proxies for language models, and achieve the higher performance estimation ratio.Specifically, we initially consolidate existing ZC proxy designs into a unified framework to serve as the search space, and then apply an evolutionary algorithm to heuristically identify new, promising proxy candidates for language models. To enhance the efficiency of the search process, we introduce a Predictive-Pruning Strategy (PPS). This strategy is designed to preemptively eliminate unpromising proxies, thereby mitigating the risk of proxy degradation. Extensive experiments on the FlexiBERT and GPT-2 search space demonstrate the effectiveness of our algorithm. Notably, the consistency in performance ranking achieved by our method significantly surpasses that observed with current proxies",
    "checked": false,
    "id": "fa0d056dd585eeffb4333cb55807d357808f8440",
    "semantic_title": "can large language models play games? a case study of a self-play approach",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=rultglLKMOh": {
    "title": "Unsupervised Adaptation of Large Language Models for Dense Retrieval",
    "volume": "review",
    "abstract": "Dense retrieval calls for discriminative embeddings to represent the semantic relationship between query and document. It may benefit from the using of large language models (LLMs), given LLMs' strong capability on semantic understanding. However, the LLMs are learned by auto-regression, whose working mechanism is completely different from representing whole text as one discriminative embedding. Thus, it is imperative to study how to adapt LLMs properly so that they can be effectively initialized as the backbone encoder for dense retrieval. In this paper, we propose a novel approach, called \\textbf{LLaRA} (\\underline{LL}M \\underline{a}datpeted for dense \\underline{R}etriv\\underline{A}l), which performs unsupervised adaptation of LLM for its dense retrieval application. LLaRA consists of two pretext tasks: EBAE (Embedding-Based Auto-Encoding) and EBAR (Embedding-Based Auto-Regression), where the LLM is prompted to \\textit{reconstruct the input sentence} and \\textit{predict the next sentence} based on its text embeddings. LLaRA is simple, lightweight, but highly effective. It is used to adapt LLaMA-2-7B on the Wikipedia corpus. With a moderate steps of adaptation, it substantially improves the model's fine-tuned performances on on a variety of dense retrieval benchmarks. Notably, it results in the new state-of-the-art performances on popular benchmarks, such as passage and document retrieval on MSMARCO, and zero-shot retrieval on BEIR. The model and source code will be made publicly available to facilitate the future research",
    "checked": false,
    "id": "501397e553ce88c2680c287dc18446e7494ee59d",
    "semantic_title": "disentangled modeling of domain and relevance for adaptable dense retrieval",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=jYP15NodZU": {
    "title": "EmoBench: Evaluating the Emotional Intelligence of Large Language Models",
    "volume": "review",
    "abstract": "Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks.Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion management and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose \\textsc{EmoBench}, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. \\textsc{EmoBench} includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning and understanding. Our findings reveal a considerable gap between the EI of existing LLMs and the average human, highlighting a promising direction for future research",
    "checked": true,
    "id": "3cdc18cf052823b535796a7cd01be51048ab4b4a",
    "semantic_title": "emobench: evaluating the emotional intelligence of large language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=PCYP1Of3We": {
    "title": "Exploring Visual Culture Awareness in GPT-4V: A Comprehensive Probing",
    "volume": "review",
    "abstract": "Pretrained large Vision-Language models have drawn considerable interest in recent years due to their remarkable performance. Despite considerable efforts to assess these models from diverse perspectives, the extent of visual cultural awareness in the state-of-the-art GPT-4V model remains unexplored. To tackle this gap, we extensively probed GPT-4V using the MaRVL benchmark dataset, aiming to investigate its capabilities and limitations in visual understanding with a focus on cultural aspects. Specifically, we introduced three visual related tasks, i.e. caption classification, pairwise captioning, and culture tag selection, to systematically delve into fine-grained visual cultural evaluation. Experimental results indicate that GPT-4V excels at identifying cultural concepts but still exhibits weaker performance in low-resource languages, such as Tamil and Swahili. Notably, through human evaluation, GPT-4V proves to be more culturally relevant in image captioning tasks than the original MaRVL human annotations, suggesting a promising solution for future visual cultural benchmark construction",
    "checked": true,
    "id": "e04af71cfc67246937b373608158d29a98ca9d44",
    "semantic_title": "exploring visual culture awareness in gpt-4v: a comprehensive probing",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=MD3LqfkiQ1U": {
    "title": "Document-Level Multi-Event Extraction with Event-specific Probe and Argument Library",
    "volume": "review",
    "abstract": "Document-level multi-event extraction aims to identify a list of event types and corresponding arguments from the document. However, most of the current methods neglect the difference between homogeneous events, which leads to event confusion and missing in multi-event documents. This is also one of the reasons why the recall and F1-score of multi-event recognition are lower compared to single-event recognition. In this paper, we propose an event-specific probe-based method to sniff multiple events by querying each corresponding argument library, which uses a novel probe-label alignment method for differential optimization. In addition, the role contrastive loss and probe consistent loss are designed to fine-tune the fine-grained role differences and probe differences in each event. The experimental results on two general datasets show that our method outperforms the state-of-the-art method in the F1-score, especially in the recall of multi-events",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7cL7sC8p89M": {
    "title": "AgentTuning: Enabling Generalized Agent Abilities for LLMs",
    "volume": "review",
    "abstract": "Open large language models (LLMs) with great performance in various tasks have significantly advanced the development of LLMs. However, they are far inferior to commercial models such as ChatGPT and GPT-4 when acting as agents to tackle complex tasks in the real world. These agent tasks employ LLMs as the central controller responsible for planning, memorization, and tool utilization, necessitating both fine-grained prompting methods and robust LLMs to achieve satisfactory performance. Though many prompting methods have been proposed to complete particular agent tasks, there is lack of research focusing on improving the agent capabilities of LLMs themselves without compromising their general abilities. In this work, we present AgentTuning, a simple and general method to enhance the agent abilities of LLMs while maintaining their general LLM capabilities. We construct AgentInstruct, a lightweight instruction-tuning dataset containing high-quality interaction trajectories. We employ a hybrid instruction-tuning strategy by combining AgentInstruct with open-source instructions from general domains. AgentTuning is used to instruction-tune the Llama 2 series, resulting in AgentLM. Our evaluations show that AgentTuning enables LLMs' agent capabilities without compromising general abilities. The AgentLM-70B is comparable to GPT-3.5-turbo on unseen agent tasks, demonstrating generalized agent capabilities. We open source the AgentInstruct and AgentLM-7B, 13B, and 70B models at https://anonymous.4open.science/r/AgentTuning, serving open and powerful alternatives to commercial LLMs for agent tasks",
    "checked": true,
    "id": "46fe9ce789408b8a50fb4259e6bf0cc5855f4ed5",
    "semantic_title": "agenttuning: enabling generalized agent abilities for llms",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=hESRmElqeE": {
    "title": "Efficient $k$-Nearest-Neighbor Machine Translation with Dynamic Retrieval",
    "volume": "review",
    "abstract": "To achieve non-parametric NMT domain adaptation, $k$-Nearest-Neighbor Machine Translation ($k$NN-MT) constructs an external datastore to store domain-specific translation knowledge, which derives a $k$NN distribution to interpolate the prediction distribution of the NMT model via a linear interpolation coefficient $\\lambda$. Despite its success, $k$NN retrieval at each timestep leads to substantial time overhead. To address this issue, dominant studies resort to $k$NN-MT with adaptive retrieval ($k$NN-MT-AR), which dynamically estimates $\\lambda$ and skips $k$NN retrieval if $\\lambda$ is less than a fixed threshold. Unfortunately, $k$NN-MT-AR does not yield satisfactory results. In this paper, we first conduct a preliminary study to reveal two key limitations of $k$NN-MT-AR: 1) the optimization gap leads to inaccurate estimation of $\\lambda$ for determining $k$NN retrieval skipping, and 2) using a fixed threshold fails to accommodate the dynamic demands for $k$NN retrieval at different timesteps. To mitigate these limitations, we then propose $k$NN-MT with dynamic retrieval ($k$NN-MT-DR) that significantly extends vanilla $k$NN-MT in two aspects. Firstly, we equip $k$NN-MT with a MLP-based classifier for determining whether to skip $k$NN retrieval at each timestep. Particularly, we explore several carefully-designed scalar features to fully exert the potential of the classifier. Secondly, we propose a timestep-aware threshold adjustment method to dynamically generate the threshold, which further improves the efficiency of our model. Experimental results on the widely-used datasets demonstrate the effectiveness and generality of our model",
    "checked": false,
    "id": "ddce957469a7c134e3065ad30a17e8be4fc23357",
    "semantic_title": "efficient k-nearest-neighbor machine translation with dynamic retrieval",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=05fDI-ASb6L": {
    "title": "ECBD: Evidence-Centered Benchmark Design for NLP",
    "volume": "review",
    "abstract": "Benchmarking is seen as critical to assessing progress in NLP. However, creating a benchmark involves many design decisions (e.g.,which datasets to include, which metrics to use) that often rely on tacit, untested assumptions about what the benchmark is actually measuring. There is currently no principled way of analyzing these decisions and how they impact the validity of the benchmark's measurements. To address this gap, we draw on evidence-centered design in educational assessments to propose ECBD (Evidence-Centered Benchmark Design). Our framework formalizes the benchmark design process into five modules and specifies the roles of each module and their interplay in collecting the evidence necessary to support the benchmark's measurement. We demonstrate the use of ECBD by conducting case studies with three benchmarks: BoolQ, SuperGLUE, and HELM. Our analysis reveals common trends in benchmark design and documentation that could threaten the validity of benchmarks' measurements",
    "checked": true,
    "id": "4a934786d5a6ae7c38cb9a107281cc89d2811d39",
    "semantic_title": "ecbd: evidence-centered benchmark design for nlp",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=jIuqYnT7pUv": {
    "title": "When is Tree Search Useful for LLM Planning? It Depends on the Discriminator",
    "volume": "review",
    "abstract": "In this paper, we examine how large language models (LLMs) solve multi-step problems under a language agent framework with three components: a generator, a discriminator, and a planning method. We investigate the practical utility of two advanced planning methods, iterative correction and tree search. We present a comprehensive analysis of how discrimination accuracy affects the overall performance of agents when using these two methods or a simpler method, re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical reasoning, show that: (1) advanced planning methods demand discriminators with at least 90% accuracy to achieve significant improvements over re-ranking; (2) current LLMs' discrimination abilities have not met the needs of advanced planning methods to achieve such improvements; (3) with LLM-based discriminators, advanced planning methods may not adequately balance accuracy and efficiency. For example, compared to the other two methods, tree search is at least 10--20 times slower but leads to negligible performance gains, which hinders its real-world applications",
    "checked": true,
    "id": "440d8b87e158a352efa58d2630d8626640afefe6",
    "semantic_title": "when is tree search useful for llm planning? it depends on the discriminator",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=eHCdoeL5gy": {
    "title": "WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models",
    "volume": "review",
    "abstract": "The rapid advancement of large language models (LLMs) has led to a new era marked by the development of autonomous applications in real-world scenarios, which drives innovation in creating advanced web agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we establish a new benchmark by compiling real-world tasks from 15 popular websites and introduce an automatic evaluation protocol leveraging multimodal understanding abilities of GPT-4V to evaluate open-ended web agents. We show that WebVoyager achieves a 59.1% task success rate on our benchmark, significantly surpassing the performance of both GPT-4 (All Tools) and the WebVoyager (text-only) setups, underscoring the exceptional capability of WebVoyager. The proposed automatic evaluation metric achieves 85.3% agreement with human judgment, indicating its effectiveness in providing reliable and accurate assessments of web agents",
    "checked": true,
    "id": "19261c6ad20c6c1e5585a8afcb88196173cbc8a6",
    "semantic_title": "webvoyager: building an end-to-end web agent with large multimodal models",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=V4SYFARLrzd": {
    "title": "DiffuCOMET: Contextual Commonsense Knowledge Diffusion",
    "volume": "review",
    "abstract": "Inferring contextually-relevant and diverse commonsense to understand narratives remains challenging for knowledge models. In this work, we develop a series of knowledge models, DiffuCOMET, that leverage diffusion to learn to reconstruct the implicit semantic connections between narrative contexts and relevant commonsense knowledge. Across multiple diffusion steps, our method progressively refines a representation of commonsense facts that is anchored to a narrative, producing contextually-relevant and diverse commonsense inferences for an input context. To evaluate DiffuCOMET, we introduce new metrics for commonsense inference that more closely measure knowledge diversity and contextual relevance. Our results on two different benchmarks, ComFact and WebNLG+, show that knowledge generated by DiffuCOMET achieves a better trade-off between commonsense diversity, contextual relevance and alignment to known gold references, compared to baseline knowledge models",
    "checked": true,
    "id": "ad4079a98c1f289678b673705f6391aa1ad8549f",
    "semantic_title": "diffucomet: contextual commonsense knowledge diffusion",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IetTHClbmir": {
    "title": "GPT is Not an Annotator: The Necessity of Human Annotation in Fairness Benchmark Construction",
    "volume": "review",
    "abstract": "Social biases in LLMs are usually measured via bias benchmark datasets. Current benchmarks have limitations in scope, grounding, quality, and human effort required. Previous work has shown success with a community-sourced, rather than crowd-sourced, approach to benchmark development. However, this work still required considerable effort from annotators with relevant lived experience. This paper explores whether an LLM (specifically, GPT-3.5-Turbo) can assist with the task of developing a bias benchmark dataset from responses to an open-ended community survey. We also extend the previous work to a new community and set of biases: the Jewish community and antisemitism. Our analysis shows that GPT-3.5-Turbo has poor performance on this annotation task and produces unacceptable quality issues in its output. Thus, we conclude that GPT-3.5-Turbo is not an appropriate substitute for human annotation in sensitive tasks related to social biases, and that its use actually negates many of the benefits of community-sourcing bias benchmarks",
    "checked": true,
    "id": "6718aa9d76b8282cbf9eac834b1430df83b0b8c7",
    "semantic_title": "gpt is not an annotator: the necessity of human annotation in fairness benchmark construction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e9gHeab7qUQ": {
    "title": "AutoMix: Automatically Mixing Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) are now available from cloud API providers in various sizes and configurations. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present AutoMix, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller LM. Central to AutoMix is a few-shot self-verification mechanism, which estimates the reliability of its own outputs without requiring training. Given that verifications can be noisy, we employ a meta-verifier in AutoMix to refine the accuracy of these assessments. Our experiments using LLAMA2-13B and GPT-4, on five context-grounded reasoning datasets demonstrate that AutoMix surpasses established baselines, improving the incremental benefit per cost by up to 86\\%",
    "checked": true,
    "id": "f0ed191bdcbd56610ab4d95c5a80d5ac31ffc074",
    "semantic_title": "automix: automatically mixing language models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=lOJMtPdl_WU": {
    "title": "Deep Seq2Seq Keyphrase Generation: Model Calibration and Uncertainty",
    "volume": "review",
    "abstract": "Keyphrase generation aims to generate topical phrases from a given text either by copying from the original text (present keyphrases) or by producing new keyphrases (absent keyphrases) that capture the topical and salient aspects of the text. While many neural models have been proposed and analyzed for this task, there is limited analysis of the properties of their generative distributions at the decoding stage. Particularly, it remains to be known how well-calibrated or uncertain the confidence of different models is with empirical success rate and whether they can express their uncertainty. Here, we study the confidence scores, perplexity, and expected calibration errors of five strong keyphrase generation models with unique characteristics and designs based on seq2seq recurrent neural networks (ExHiRD), transformers with no pre-training (Transformer, Trans2Set), and transformers with pre-training (BART, and T5). We propose a novel strategy for keyphrase-level perplexity calculation and for normalizing sub-word-level perplexity to gauge model confidence",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZPEavrC1PCB": {
    "title": "Don't Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection",
    "volume": "review",
    "abstract": "The fairness and trustworthiness of Large Language Models (LLMs) are receiving increasing attention. Implicit hate speech, which employs indirect language to convey hateful intentions, occupies a significant portion of practice. However, the extent to which LLMs effectively address this issue remains insufficiently examined. This paper delves into the capability of LLMs to detect implicit hate speech and express confidence in their responses. Our evaluation meticulously considers various prompt patterns and mainstream uncertainty estimation methods. Our findings highlight that LLMs exhibit two extremes: (1) LLMs display excessive sensitivity towards groups or topics that may cause fairness issues, resulting in misclassifying benign statements as hate speech. (2) LLMs' confidence scores for each method excessively concentrate on a fixed range, remaining unchanged regardless of the dataset's complexity. Consequently, the calibration performance is heavily reliant on primary classification accuracy. These discoveries unveil new limitations of LLMs, underscoring the need for caution when optimizing models to ensure they do not veer towards extremes. This serves as a reminder to carefully consider sensitivity and confidence in the pursuit of model fairness",
    "checked": true,
    "id": "a3fb9a9fba92831bd205e8f43aed798a2d8bc9cd",
    "semantic_title": "don't go to extremes: revealing the excessive sensitivity and calibration limitations of llms in implicit hate speech detection",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=OmXrJrErg": {
    "title": "Fast Randomized Low-Rank Adaptation of Pre-trained Language Models with PAC Regularization",
    "volume": "review",
    "abstract": "Low-rank adaptation (LoRA) achieves parameter efficient fine-tuning for large language models (LLMs) by decomposing the model weight update into a pair of low-rank projection matrices. Yet, the memory overhead restricts it to scale up when the model size increases. We propose Randomized LoRA (RLoRA) which adopts Randomized Walsh-Hadamard Transform to achieve significant reduction in the size of trainable parameters compared to LoRA. At the same time, it allows a PAC-Bayes regularizer to be efficiently incorporated to improve generalization. We evaluate the effectiveness of RLoRA on LLMs RoBERTa and GPT-2 using GLUE and E2E benchmarks. With a much lower memory requirement, RLoRA can give similar performance as the SOTA low-rank adaptation methods for GLUE and E2E, and significantly better performance under few-shot settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rO5nfE90FnK": {
    "title": "The Unreasonable Effectiveness of Large Language Models as Psychologically Deep Authors",
    "volume": "review",
    "abstract": "Many evaluations of Large Language Models (LLMs) focus on exam-style benchmarks that measure domain-specific knowledge acquisition or linguistic attributes like grammaticality. Such evaluations emphasize the functional capacities of LLMs while overlooking their ability to resonate with readers on a psychologically deep level. Addressing this gap, this work introduces the Psychological Depth Scale (PDS), a novel framework designed to measure authenticity, empathy, engagement, narrative complexity, and emotional provocation. Through an empirical study involving 100 short stories written by humans and various LLMs, including GPT-4, we explore the consistency of human judgment on psychological depth, compare the depth of human and LLM stories, and examine the potential for automated assessment of psychological depth. Our findings reveal that (1) humans can consistently judge psychological depth despite its abstract nature; (2) despite being perceived as less \"human\", GPT-4 stories surpassed advanced human authors in 4 out of 5 dimensions of psychological depth, often by sizable margins; and (3) GPT-4 combined with a novel Mixture of Personas prompting strategy can moderately correlate (0.44) with human judgments of psychological depth. These findings open the possibility that LLMs could be strategically deployed to forge deeper emotional and psychological bonds with humans in fields as diverse as therapy and popular entertainment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EIw7gg267s": {
    "title": "Annotating FrameNet via Structure-Conditioned Language Generation",
    "volume": "review",
    "abstract": "Despite the mounting evidence for generative capabilities of language models in understanding and generating natural language, their effectiveness on explicit manipulation and generation of linguistic structures remain understudied. In this paper, we investigate the task of generating new sentences preserving a given semantic structure, following the FrameNet formalism. We propose a framework to produce novel frame-semantically annotated sentences following an overgenerate-and-filter approach. Our results show that conditioning on rich, explicit semantic information tends to produce generations with high human acceptance, under both prompting and finetuning. Nevertheless, we discover that generated frame-semantic structured data is ineffective at training data augmentation for frame-semantic role labeling. Our study concludes that while generating high-quality, semantically rich data might be within reach, their downstream utility remains to be seen, highlighting the outstanding challenges with automating linguistic annotation tasks",
    "checked": true,
    "id": "a9cdf6fe13cabf4da87bf1f89731b6b0e83981db",
    "semantic_title": "annotating framenet via structure-conditioned language generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zq_favh3fEv": {
    "title": "RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback",
    "volume": "review",
    "abstract": "Large language models (LLMs) demonstrate exceptional performance in numerous tasks but still heavily rely on knowledge stored in their parameters. Moreover, updating this knowledge incurs high training costs. Retrieval-augmented generation (RAG) methods address this issue by integrating external knowledge. The model can answer questions it couldn't previously by retrieving knowledge relevant to the query. This approach improves performance in certain scenarios for specific tasks. However, if irrelevant texts are retrieved, it may impair model performance. In this paper, we propose Retrieval Augmented Iterative Self-Feedback (RA-ISF), a framework that iteratively decomposes tasks and processes them in three submodules to enhance the model's problem-solving capabilities. Experiments show that our method outperforms existing benchmarks, performing well on models like GPT3.5, Llama2, significantly enhancing factual reasoning capabilities and reducing hallucinations",
    "checked": true,
    "id": "1778f1b36d07a6a0166de4c91f2f2e7a2a225f84",
    "semantic_title": "ra-isf: learning to answer and understand from retrieval augmentation via iterative self-feedback",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=4DQNkZoAUY5": {
    "title": "Harnessing Toulmin's theory for zero-shot argument explication",
    "volume": "review",
    "abstract": "To better analyze informal arguments on public forums, we propose the task of argument explication, which makes explicit a text's argumentative structure and implicit reasoning by outputting triples of propositions âŸ¨claim, reason warrantâŸ©. The three slots, or argument components, are derived from the widely known Toulmin (1958) model of argumentation. While prior research applies Toulmin or related theories to annotate datasets and train supervised models, we develop an effective method to prompt generative large language models (LMs) to output explicitly named argument components proposed by Toulmin by prompting with the theory name (e.g., `According to Toulmin model'). We evaluate the outputs' coverage and validity through a human study and automatic evaluation based on prior argumentation datasets and perform robustness checks over alternative LMs, prompts, and argumentation theories. Finally, we conduct a proof-of-concept case study to extract an interpretable argumentation (hyper)graph from a large corpus of critical public comments on whether to allow the COVID-19 vaccine for children, suggesting future directions for corpus analysis and argument visualization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=emE_0SGfoc": {
    "title": "KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have significantly advanced healthcare innovation on generation capabilities. However, their application in real clinical settings is challenging due to potential deviations from medical facts and inherent biases. In this work, we develop an augmented LLM framework, KG-Rank, which leverages a medical knowledge graph (KG) with ranking and re-ranking techniques, aiming to improve free-text question-answering (QA) in the medical domain. Specifically, upon receiving a question, we initially retrieve triplets from a medical KG to gather factual information. Subsequently, we innovatively apply ranking methods to refine the ordering of these triplets, aiming to yield more precise answers. To the best of our knowledge, KG-Rank is the first application of ranking models combined with KG in medical QA specifically for generating long answers. Evaluation of four selected medical QA datasets shows that KG-Rank achieves an improvement of over 18% in the ROUGE-L score. Moreover, we extend KG-Rank to open domains, where it realizes a 14% improvement in ROUGE-L, showing the effectiveness and potential of KG-Rank",
    "checked": true,
    "id": "13f94fb16fd719769e8b5215035b231d1211f9fa",
    "semantic_title": "kg-rank: enhancing large language models for medical qa with knowledge graphs and ranking techniques",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=OHaojLXLFmy": {
    "title": "Generalizability of Mixture of Domain-Specific Adapters from the Lens of Signed Weight Directions and its Application to Effective Model Pruning",
    "volume": "review",
    "abstract": "Several parameter-efficient fine-tuning methods based on adapters have been proposed as a streamlined approach to incorporate not only a single specialized knowledge into existing Pre-Trained Language Models (PLMs) but also multiple of them at once. Recent works such as AdapterSoup propose to mix not all but only a selective sub-set of domain-specific adapters during inference via model weight averaging to optimize performance on novel, unseen domains with excellent computational efficiency. However, the essential generalizability of this emerging weight-space adapter mixing mechanism on unseen, in-domain examples remains unexplored. Thus, in this study, we conduct a comprehensive analysis to elucidate the generalizability of domain-specific adapter mixtures in in-domain evaluation. We also provide investigations into the inner workings of the mixture of domain-specific adapters by analyzing their weight signs, yielding critical analysis on the negative correlation between their fraction of weight sign difference and their mixtures' generalizability. All source code will be published",
    "checked": true,
    "id": "2faad1112335ff99d1cb7967a51ac977fc9e6804",
    "semantic_title": "generalizability of mixture of domain-specific adapters from the lens of signed weight directions and its application to effective model pruning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=u11ZLC6sow2": {
    "title": "Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models",
    "volume": "review",
    "abstract": "Instruction-tuned LLMs can respond to explicit queries formulated as prompts, which greatly facilitates interaction with human users. However, prompt-based approaches might not always be able to tap into the wealth of implicit knowledge acquired by LLMs during pre-training. This paper presents a comprehensive study of ways to evaluate semantic plausibility in LLMs. We compare base and instruction-tuned LLM performance on an English sentence plausibility task via (a) explicit prompting and (b) implicit estimation via direct readout of the probabilities models assign to strings. Experiment 1 shows that, across model architectures and plausibility datasets, (i) log likelihood (LL) scores are the most reliable indicator of sentence plausibility, with zero-shot prompting yielding inconsistent and typically poor results; (ii) LL-based performance is still inferior to human performance; (iii) instruction-tuned models have worse LL-based performance than base models. In Experiment 2, we show that LL scores across models are modulated by context in the expected way, showing high performance on three metrics of context-sensitive plausibility and providing a direct match to explicit human plausibility judgments. Overall, LL estimates remain a more reliable measure of plausibility in LLMs than direct prompting",
    "checked": true,
    "id": "83c41d2701bbd1a3d9b2dd89a65d288295611185",
    "semantic_title": "comparing plausibility estimates in base and instruction-tuned large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=eBKBmJ8v-gd": {
    "title": "ISQA: Informative Factuality Feedback for Scientific Summarization",
    "volume": "review",
    "abstract": "We propose Iterative Facuality Refining on Informative Scientific Question-Answering (ISQA) feedback\\footnote{Code is available at \\url{https://github.com/mt69JMMW/code-to-release.git}}, a method following human learning theories that employs model-generated feedback consisting of both positive and negative information. Through iterative refining of summaries, it probes for the underlying rationale of statements to enhance the factuality of scientific summarization. It does this in a fine-grained manner by asking a summarization agent to reinforce validated statements in positive feedback and fix incorrect ones in negative feedback. Our findings demonstrate that the ISQA feedback mechanism significantly improves the factuality of various open-source LLMs on the summarization task, as evaluated across multiple scientific datasets",
    "checked": true,
    "id": "18568b689bcd3b36fbf27438a60bcd00c74f1813",
    "semantic_title": "isqa: informative factuality feedback for scientific summarization",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=qG5vcfgick": {
    "title": "Parrot: Enhancing Multi-Turn Instruction Following for Large Language Models",
    "volume": "review",
    "abstract": "Humans often interact with large language models (LLMs) in multi-turn interaction to obtain desired answers or more information. However, most existing studies overlook the multi-turn instruction following ability of LLMs, in terms of training dataset, training method, and evaluation benchmark. In this paper, we introduce Parrot, a solution aiming to enhance multi-turn instruction following for LLMs. First, we introduce an efficient but effective method for collecting multi-turn instructions that feature human-like queries, such as anaphora and ellipsis. Second, we propose a context-aware preference optimization strategy to further enhance LLMs for complex queries in multi-turn interaction. Moreover, to quantitatively evaluate LLMs in multi-turn instruction following, we manually build a multi-turn benchmark derived from existing ones. Extensive experiments show that Parrot improves current LLMs by up to 7.2% in multi-turn instruction following. Our dataset and codes will be open-sourced to facilitate future research",
    "checked": true,
    "id": "085b560b00fde9792da6787c0497ade5a18abd07",
    "semantic_title": "parrot: enhancing multi-turn instruction following for large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=KSgKsDDaj6U": {
    "title": "Authorship Style Transfer with Policy Optimization",
    "volume": "review",
    "abstract": "Authorship style transfer aims to rewrite a given text into a specified target while preserving the original meaning in the source. Existing approaches rely on the availability of a large number of target style exemplars for fine-tuning. However, these overlook cases where a limited number of target style examples are available. The development of parameter-efficient transfer learning techniques and policy optimization (PO) approaches suggest lightweight PO is a feasible approach to low-resource style transfer. In this work, we propose a simple two step tune-and-optimize technique for low-resource textual style transfer. We apply our technique to authorship transfer as well as a larger-data native language style task and in both cases find it outperforms state-of-the-art baseline models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8SJm-2GEw3j": {
    "title": "The Male CEO and the Female Assistant: Probing Gender Biases in Text-To-Image Models Through Paired Stereotype Test",
    "volume": "review",
    "abstract": "Recent large-scale Text-To-Image (T2I) models such as DALLE-3 demonstrate great potential in new applications, but also face unprecedented fairness challenges. Prior studies revealed gender biases in single-person image generation, but T2I model applications might require portraying two or more people simultaneously. Potential biases in this setting remain unexplored, leading to fairness-related risks in usage. To study these underlying facets of gender biases in T2I models, we propose a novel Paired Stereotype Test (PST) bias evaluation framework. PST prompts the model to generate two individuals in the same image. They are described with two social identities that are stereotypically associated with the opposite gender. Biases can then be measured by the level of conformation to gender stereotypes in generated images. Using PST, we evaluate DALLE-3 from 2 perspectives: biases in gendered occupation and biases in organizational power. Despite seemingly fair or even anti-stereotype single-person generations, PST still unveils gendered occupational and power associations. Moreover, compared to single-person settings, DALLE-3 generates noticeably more masculine figures under PST for individuals with male-stereotypical identities. PST is therefore effective in revealing underlying gender biases in DALLE-3 that single-person settings cannot capture. Our findings reveal the complicated patterns of gender biases in modern T2I models, further highlighting the critical fairness challenges in multimodal generative systems",
    "checked": true,
    "id": "dd2ebfc0d6846abc3efa779c3fc90ff713388dab",
    "semantic_title": "the male ceo and the female assistant: probing gender biases in text-to-image models through paired stereotype test",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=5wqtHUfU76": {
    "title": "Deliberate then Generate: Enhanced Prompting Framework for Text Generation",
    "volume": "review",
    "abstract": "Large language models (LLMs) have shown remarkable success across a wide range of natural language generation tasks, where proper prompt designs make great impacts. While existing prompting methods are normally restricted to providing correct information, in this paper, we encourage the model to deliberate by proposing a novel Deliberate then Generate (DTG) prompting framework, which consists of error detection instructions and candidates that may contain errors. DTG is a simple yet effective technique that can be applied to various text generation tasks with minimal modifications. We conduct extensive experiments on 20+ datasets across 7 text generation tasks, including summarization, translation, dialogue, and more. We show that DTG consistently outperforms existing prompting methods and achieves state-of-the-art performance on multiple text generation tasks. We also provide in-depth analyses to reveal the underlying mechanisms of DTG, which may inspire future research on prompting for LLMs",
    "checked": true,
    "id": "c85c90ef9e9a71efe031c3f7d6e34561f91168fe",
    "semantic_title": "deliberate then generate: enhanced prompting framework for text generation",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=_wqmILGp2KA": {
    "title": "SelectLLM: Can LLMs Select Important Instructions to Annotate?",
    "volume": "review",
    "abstract": "Instruction tuning benefits from large and diverse datasets, however creating such datasets involves a high cost of human labeling. While synthetic datasets generated by large language models (LLMs) have partly solved this issue, they often contain low-quality data. One effective solution is selectively annotating unlabelled instructions, especially given the relative ease of acquiring unlabeled instructions or texts from diverse sources. However, how to select unlabelled instructions is not well-explored, especially in the context of LLMs. Further, traditional data selection methods, relying on input embedding space density, tend to underestimate instruction sample complexity, whereas those based on model prediction uncertainty often struggle with synthetic label quality. Therefore, we introduce SelectLLM, an alternative framework that leverages the capabilities of LLMs to more effectively select unlabeled instructions. SelectLLM consists of two key steps: Coreset-based clustering of unlabelled instructions for diversity and then prompting LLM to identify the most beneficial instructions within each cluster. Our experiments demonstrate that SelectLLM matches or outperforms other state-of-the-art methods in instruction tuning benchmarks. It exhibits remarkable consistency across human and synthetic datasets, along with better cross-dataset generalization, as evidenced by a 10\\% performance improvement on the Cleaned Alpaca test set when trained on Dolly data",
    "checked": true,
    "id": "11dfbd2f49624b73d0c417dbabbd0278ae25da44",
    "semantic_title": "selectllm: can llms select important instructions to annotate?",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=a5hGIASQYd": {
    "title": "CLUE: Concept-Level Uncertainty Estimation for Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in various natural language generation (NLG) tasks. Previous studies suggest that LLMs' generation process involves uncertainty. However, existing approaches to uncertainty estimation mainly focus on sequence-level uncertainty, overlooking individual pieces of information within sequences. These methods fall short in separately assessing the uncertainty of each component in a sequence. In response, we propose a novel framework for Concept-Level Uncertainty Estimation (CLUE) for LLMs. We leverage LLMs to convert output sequences into concept-level representations, breaking down sequences into individual concepts and measuring the uncertainty of each concept separately. We conduct experiments to demonstrate that CLUE can provide more interpretable uncertainty estimation results compared with sentence-level uncertainty, and could be a useful tool for various tasks such as hallucination detection and story generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3pIeOI1yPG": {
    "title": "Enhancing Multi-Step Reasoning via Process-Supervised Reinforcement Learning from Human Feedback",
    "volume": "review",
    "abstract": "Large language models (LLMs) demonstrate significant reasoning capabilities, especially through step-by-step reasoning paths. However, their proficiency in mathematical reasoning remains limited. We generalize the Reinforcement Learning from Human Feedback (RLHF) framework by integrating per-step reward signals in order to enhance LLMs' reasoning abilities. This approach differs from traditional outcome-based models by offering step-wise guidance during learning. Experiments on MATH and PRM800K datasets show that our process-supervised RLHF significantly improves reasoning accuracy over its outcome-based counterpart, marking a notable advancement in LLMs for complex reasoning tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wMVXq67JE3": {
    "title": "MMPD: A Multimedia Conversational Personality Dataset",
    "volume": "review",
    "abstract": "Automatic personality detection has evolved from simple text classification to sophisticated multimodal analyses, recognizing the multi-dimensional manifestation of personality beyond textual data. This shift highlights the need for datasets that can accurately capture the complexity of human personality through diverse modalities. We introduce the Multimedia Conversational Personality Dataset (MMPD), a large, extensive and varied dataset, built on 305 movies and 14 TV series, featuring over 46k dialogues, 552k utterances, 4016 characters, and 963 hours of video. MMPD not only addresses the challenges of existing datasets by offering majority-voted personality annotations and detailed relationship networks but also provides a new method for matching subtitles with original scripts, paving the way for advanced analyses of personality dynamics across various contexts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M3x4VoMHPO": {
    "title": "SPAGHETTI: Open-Domain Question Answering from Heterogeneous Data Sources with Retrieval and Semantic Parsing",
    "volume": "review",
    "abstract": "We introduce SPAGHETTI: Semantic Parsing Augmented Generation for Hybrid English information from Text Tables and Infoboxes, a hybrid question-answering (QA) pipeline that utilizes information from heterogeneous knowledge sources, including knowledge base, text, tables, and infoboxes. Our LLM-augmented approach achieves state-of-the-art performance on the Compmix dataset, the most comprehensive heterogeneous open-domain QA dataset, with 56.5% exact match (EM) rate. More importantly, manual analysis on a sample of the dataset suggests that SPAGHETTI is more than 90% accurate, indicating that EM is no longer suitable for assessing the capabilities of QA systems today",
    "checked": true,
    "id": "f09e5d731829b2872a790f59fdb8f653928de058",
    "semantic_title": "spaghetti: open-domain question answering from heterogeneous data sources with retrieval and semantic parsing",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=-puYLfEU4Wc": {
    "title": "Agent Lumos: Unified and Modular Training for Open-Source Language Agents",
    "volume": "review",
    "abstract": "Closed-source agents suffer from several issues such as a lack of affordability, transparency, and reproducibility, particularly on complex interactive tasks. This motivates the development of open-source alternatives. We introduce Lumos, one of the first frameworks for training open-source LLM-based agents. Lumos features a learnable, unified and modular architecture with a planning module that learns high-level subgoal generation, and a grounding module trained to translate these into the actions using various tools in the execution module. The design allows for modular upgrades and wider applicability to diverse interactive tasks. To foster generalizable agent learning, we collect large-scale, unified, and high-quality training annotations derived from diverse ground-truth reasoning rationales across various complex interactive tasks. On 9 datasets, Lumos exhibits several key advantages: (1) Lumos excels multiple larger open-source agents on the held-out datasets (unused for training) for each task type. Lumos even surpasses GPT agents on QA and web tasks; (2) Lumos outperforms open-source agents produced by chain-of-thoughts and unmodularized integrated training; and (3) Lumos effectively generalizes to unseen tasks, outperforming 33B-scale agents and domain-specific agents. Code and data will be released",
    "checked": true,
    "id": "0b0525a0fdc1e18978e9861dcb4544a90c2e70ce",
    "semantic_title": "agent lumos: unified and modular training for open-source language agents",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=szhzG4-NtLw": {
    "title": "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records",
    "volume": "review",
    "abstract": "Large language models (LLMs) have demonstrated exceptional capabilities in planning and tool utilization as autonomous agents, but few have been developed for medical problem-solving. We propose EHRAgent, an LLM agent empowered with a code interface, to autonomously generate and execute code for multi-tabular reasoning within electronic health records (EHRs). First, we formulate an EHR question-answering task into a tool-use planning process, efficiently decomposing a complicated task into a sequence of manageable actions. By integrating interactive coding and execution feedback, EHRAgent learns from error messages and improves the originally generated code through iterations. Furthermore, we enhance the LLM agent by incorporating long-term memory, which allows EHRAgent to effectively select and build upon the most relevant successful cases from past experiences. Experiments on three real-world multi-tabular EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate. EHRAgent leverages the emerging few-shot learning capabilities of LLMs, enabling autonomous code generation and execution to tackle complex clinical tasks with minimal demonstrations",
    "checked": true,
    "id": "3721eb984e7e00a94dd9fc84e0183b2ab5991033",
    "semantic_title": "ehragent: code empowers large language models for few-shot complex tabular reasoning on electronic health records",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=aFGyOQNaoBE": {
    "title": "Partial Diacritization: A Context-Contrastive Inference Approach",
    "volume": "review",
    "abstract": "Diacritization plays a pivotal role in improving readability and disambiguating the meaning of Arabic texts. Efforts have so far focused on marking every eligible character (Full Diacritization). Comparatively overlooked, Partial Diacritzation (PD) is the selection of a subset of characters to be annotated to aid comprehension where needed. Research has indicated that excessive diacritic marks can hinder skilled readers---reducing reading speed and accuracy. We conduct a behavioral experiment and show that partially marked text is often easier to read than fully marked text, and sometimes easier than plain text. In this light, we introduce Context-Contrastive Partial Diacritization (CCPD)---a novel approach to PD which integrates seamlessly with existing Arabic diacritization systems. CCPD processes each word twice, once with context and once without, and diacritizes only the characters with disparities between the two inferences. Further, we introduce novel indicators for measuring partial diacritization quality to help establish this as a machine learning task. Lastly, we introduce TD2, a Transformer-variant of an established model which offers a markedly different performance profile on our proposed indicators compared to all other known systems",
    "checked": true,
    "id": "388d7c432b0df8e715027dd1552c802c76056d6f",
    "semantic_title": "partial diacritization: a context-contrastive inference approach",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=hH4oXxeiYz": {
    "title": "Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts",
    "volume": "review",
    "abstract": "Large language models (LLMs) are known to effectively perform tasks by simply observing few exemplars. However, in low-resource languages, obtaining such hand-picked exemplars can still be challenging, where unsupervised techniques may be necessary. Moreover, competent generative capabilities of LLMs are observed only in high-resource languages, while their performances among under-represented languages fall behind due to pre-training data imbalance. To elicit LLMs' ability onto low-resource languages without any supervised data, we propose to assemble synthetic exemplars from a diverse set of high-resource languages to prompt the LLMs to translate from any language into English. These prompts are then used to create intra-lingual exemplars to perform tasks in the target languages. Our unsupervised prompting method performs on par with supervised few-shot learning in LLMs of different sizes for translations between English and 13 Indic and 21 African low-resource languages. We also show that fine-tuning a 7B model on data generated from our method helps it perform competitively with a 175B model. In non-English translation tasks, our method even outperforms supervised prompting by up to 3 chrF++ in many low-resource languages. When evaluated on zero-shot multilingual summarization, our method surpasses other English-pivoting baselines by up to 4 ROUGE-L and is also favored by GPT-4",
    "checked": true,
    "id": "e534e65562e945cc67f4075ac2757051fc188ea8",
    "semantic_title": "democratizing llms for low-resource languages by leveraging their english dominant abilities with linguistically-diverse prompts",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=g4psslnk7Iz": {
    "title": "Subword Embedding from Bytes Gains Privacy without Sacrificing Accuracy and Complexity",
    "volume": "review",
    "abstract": "While NLP models significantly impact our lives, there are rising concerns about privacy invasion. Although federated learning enhances privacy, attackers may recover private training data by exploiting model parameters and gradients. Therefore, protecting against such an embedding attack remains an open challenge. We propose Subword Embedding from Bytes (SEB) and encode subwords to byte sequences using neural networks that are harder to retrieve in attacks. Importantly, our method requires a smaller memory with only $256$ bytes of vocabulary while keeping efficiency with the same input length as usual. Thus, our solution outperforms conventional approaches by preserving privacy without sacrificing efficiency or accuracy. Our experiments show SEB can effectively protect against embedding-based attacks from recovering original sentences in federated learning. Meanwhile, we verify that SEB obtains comparable and even more accurate prediction results over standard subword embedding methods in machine translation, sentiment analysis, and language modeling with even lower time and space complexity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PIcYmhPaKAy": {
    "title": "Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) are increasingly being used to generate synthetic data for training and evaluating models. However, it is unclear whether they can generate a good quality of question answering (QA) dataset that incorporates knowledge and cultural nuance embedded in a language, especially for low-resource languages. In this study, we investigate the effectiveness of using LLMs in generating culturally relevant commonsense QA datasets for Indonesian and Sundanese languages. To do so, we create datasets for these languages using various methods involving both LLMs and human annotators. Our experiments show that the current best-performing LLM, GPT-4 Turbo, is capable of generating questions with adequate knowledge in Indonesian but not in Sundanese, highlighting the performance discrepancy between medium- and lower-resource languages. We also benchmark various LLMs on our generated datasets and find that they perform better on the LLM-generated datasets compared to those created by humans",
    "checked": true,
    "id": "1d83d2512bd9c7ceb8de1fa25ac7b8c4c00b5573",
    "semantic_title": "can llm generate culturally relevant commonsense qa data? case study in indonesian and sundanese",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=lrnbNN9SlEu": {
    "title": "Beyond Scaling: Predicting Patent Approval with Domain-specific Fine-grained Claim Dependency Graph",
    "volume": "review",
    "abstract": "Model scaling is becoming the default choice for many language tasks due to the success of large language models (LLMs). However, it can fall short in specific scenarios where simple customized methods excel. In this paper, we delve into the patent approval prediction task and unveil that simple domain-specific graph methods outperform enlarging the model, using the intrinsic dependencies within the patent data. Specifically, we first extend the embedding-based state-of-the-art (SOTA) by scaling up its backbone model with various sizes of open-source LLMs, then explore prompt-based methods to harness proprietary LLMs' potential, but find the best results close to random guessing, underlining the ineffectiveness of model scaling-up. Hence, we propose a novel Fine-grained cLAim depeNdency (FLAN) Graph through meticulous patent data analyses, capturing the inherent dependencies across segments of the patent text. As it is model-agnostic, we apply cost-effective graph models to our FLAN Graph to obtain representations for approval prediction. Extensive experiments and detailed analyses prove that incorporating FLAN Graph via various graph models consistently outperforms all LLM baselines significantly. We hope that our observations and analyses in this paper can bring more attention to this challenging task and prompt further research into the limitations of LLMs",
    "checked": true,
    "id": "0588860f4076cbec9b7b2fe37a58877fdc848a38",
    "semantic_title": "beyond scaling: predicting patent approval with domain-specific fine-grained claim dependency graph",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4V8OgFU8gTR": {
    "title": "GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers",
    "volume": "review",
    "abstract": "Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks. However, there are increasing debates regarding whether these models truly understand and apply mathematical knowledge or merely rely on shortcuts for mathematical reasoning. One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly. This motivates us to evaluate the robustness of LLMs' math reasoning capability by testing a wide range of question variations. We introduce the adversarial grade school math (GSM-Plus) dataset, an extension of GSM8K augmented with various mathematical perturbations. Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust. In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes when new statements are added or the question targets are altered. We also explore whether more robust performance can be achieved by composing existing prompting methods, in which we try an iterative method that generates and verifies each intermediate thought based on its reasoning goal and calculation result",
    "checked": true,
    "id": "bed35010543191bf57a09a6058e75332702d7afa",
    "semantic_title": "gsm-plus: a comprehensive benchmark for evaluating the robustness of llms as mathematical problem solvers",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=noUUtdGhRGV": {
    "title": "Bridge Structural Knowledge and Pre-trained Language Models for Knowledge Graph Completion",
    "volume": "review",
    "abstract": "Knowledge graph completion (KGC) is a task of inferring missing triples based on existing Knowledge Graphs (KGs). Both structural and semantic information are vital for successful KGC. However, existing methods only use either the structural knowledge from the KG embeddings or the semantic information from pre-trained language models (PLMs), leading to suboptimal model performance. Moreover, since PLMs are not trained on KGs, directly using PLMs to encode triples is inappropriate. To overcome these limitations, we propose a novel model called Bridge, which jointly encodes structural and semantic information of KGs. Specifically, we strategically encode entities and relations separately by PLMs to better utilize the semantic knowledge of PLMs and enable structured representation learning via a structural learning principle. Furthermore, to bridge the gap between KGs and PLMs, we employ a self-supervised representation learning method called BYOL to fine-tune PLMs with two different views of a triple. Experiments demonstrate that Bridge outperforms the SOTA models on three benchmark datasets",
    "checked": false,
    "id": "0496f01a428e96c1e65843215964138806226225",
    "semantic_title": "graph structure enhanced pre-training language model for knowledge graph completion",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=yogWGduYic6": {
    "title": "Towards Understanding Task-agnostic Debiasing Through the Lenses of Intrinsic Bias and Forgetfulness",
    "volume": "review",
    "abstract": "While task-agnostic debiasing provides notable generalizability and reduced reliance on downstream data, its impact on language modeling ability and the risk of relearning social biases from downstream task-specific data remain as the two most significant challenges when debiasing Pretrained Language Models (PLMs). The impact on language modeling ability can be alleviated given a high-quality and long-contextualized debiasing corpus, but there remains a deficiency in understanding the specifics of relearning biases. We empirically ascertain that the effectiveness of task-agnostic debiasing hinges on the quantitative bias level of both the task-specific data used for downstream applications and the debiased model. We empirically show that the lower bound of the bias level of the downstream fine-tuned model is the bias level of the debiased model, in most practical cases. To gain more in-depth understanding about how the parameters of PLMs change during fine-tuning due to the forgetting issue of PLMs, we propose a novel framework which can Propagate Socially-fair Debiasing to Downstream Fine-tuning, ProSocialTuning. Our proposed framework can push the fine-tuned model to approach the bias lower bound during downstream fine-tuning, indicating that the ineffectiveness of debiasing can be alleviated by overcoming the forgetting issue through regularizing successfully debiased attention heads based on the PLMs' bias levels from stages of pretraining and debiasing",
    "checked": true,
    "id": "14ed9eddc1c2cd7381fa9c3f47f961d2c32a057f",
    "semantic_title": "towards understanding task-agnostic debiasing through the lenses of intrinsic bias and forgetfulness",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6aGe1vCcz3B": {
    "title": "Synergizing Large Language Models and Pre-Trained Smaller Models for Conversational Intent Discovery",
    "volume": "review",
    "abstract": "In Conversational Intent Discovery (CID), Small Language Models (SLMs) struggle with overfitting to familiar intents and fail to label newly discovered ones. This issue stems from their limited grasp of semantic nuances and their intrinsically discriminative framework. Therefore, we propose Synergizing Large Language Models (LLMs) with pre-trained SLMs for CID (SynCID). It harnesses the profound semantic comprehension of LLMs alongside the operational agility of SLMs. By utilizing LLMs to refine both utterances and existing intent labels, SynCID significantly enhances the semantic depth, subsequently realigning these enriched descriptors within the SLMs' feature space to correct cluster distortion and promote robust learning of representations. A key advantage is its capacity for the early identification of new intents, a critical aspect for deploying conversational agents successfully. Additionally, SynCID leverages the in-context learning strengths of LLMs to generate labels for new intents. Thorough evaluations across a wide array of datasets have demonstrated its superior performance over traditional CID methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I2_sUekv9Xh": {
    "title": "Probing Language Models for Pre-training Data Detection",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the model's internal activations. Our method is simple and effective and leads to more trustworthy pre-training data detection. Additionally, we propose ArxivMIA, a new challenging benchmark comprising arxiv abstracts from Computer Science and Mathematics categories. Our experiments demonstrate that our method outperforms all baselines, and achieves state-of-the-art performance on both WikiMIA and ArxivMIA, with additional experiments confirming its efficacy",
    "checked": true,
    "id": "f9cd5301046f2c463cec4c2f1e23cce59c29766f",
    "semantic_title": "probing language models for pre-training data detection",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=QZLnhjdMbq": {
    "title": "Medical Dialogue: A Survey of Categories, Methods, Evaluation and Challenges",
    "volume": "review",
    "abstract": "This paper surveys and organizes research works of medical dialog systems, which is an important yet challenging task. Although these systems have been surveyed in the medical community from an application perspective, a systematic review from a rigorous technical perspective has to date remained noticeably absent. As a result, an overview of the categories, methods, evaluation of medical dialogue systems remain limited and underspecified, hindering the further improvement of this area. To fill this gap, we investigate an initial pool of 325 papers from well-known computer science, natural language processing conferences and journals, and make an overview. Recently, large language models have shown strong model capacity on downstream tasks, which also reshape medical dialog systems' foundation.Despite the alluring practical application value, current medical dialogue systems still suffer from problems. To this end, this paper lists grand challenges of medical dialog systems, especially of large language models",
    "checked": true,
    "id": "102c635a94eefbb662554cecc3443f693086b0c2",
    "semantic_title": "medical dialogue: a survey of categories, methods, evaluation and challenges",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IQfyKRODwk": {
    "title": "Comparing Pre-trained Human Language Models: Is it Better with Human Context as Groups, Individual Traits, or Both?",
    "volume": "review",
    "abstract": "Incorporating human context into language models is the next frontier for human-centered natural language processing. Currently, two pre-training methods exist: group-wise attributes (e.g., \\textit{over-45-year-olds}) or individual traits. Group attributes are coarse --- not all 45-year-olds write the same way --- while modeling individual traits allows for a more personalized representation, but requires more complex modeling and data. So far, it is unclear which pre-training approach benefits what tasks. We compare pre-training models with human context via 1) group attributes, 2) individual users, and 3) a combined approach on 5 user- and document-level tasks. We find that pre-training with both group \\textit{and} individual features significantly improves the two \\textit{user}-level regression tasks like age estimation and personality assessment. \\Pre-training on individual users significantly improves the three \\textit{document}-level classification tasks like stance and topic detection. It even does well for downstream tasks without historical user data. Our results suggest both approaches have specific use cases, opening new avenues for human-centered language modeling",
    "checked": true,
    "id": "a889bf5deb95c64a3ce9f9b6049293c2f806fb4c",
    "semantic_title": "comparing pre-trained human language models: is it better with human context as groups, individual traits, or both?",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=J8BM68lD6PT": {
    "title": "ICLEval: Evaluating In-Context Learning Ability of Large Language Models",
    "volume": "review",
    "abstract": "In-Context Learning (ICL) is a crucial capability of LLMs as it enables them to understand and reason across a series of interconnected inputs. However, existing evaluation frameworks primarily focus on language abilities and knowledge, often neglecting the evaluation of ICL ability. This limitation hampers our understanding of how LLMs utilize context in complex problem-solving. In this study, we introduce the ICLEval benchmark to assess the ICL abilities of LLMs systematically. We evaluate two fundamental abilities: copying and learning. We also investigate the impact of model size, pretraining stage, and other factors on ICL abilities. Our findings reveal that model size is not the sole determinant of ICL efficacy. Surprisingly, we observe that ICL abilities, particularly copying, develop early in the pretraining process and stabilize afterward. Furthermore, we discover that the ICL abilities also be influenced by some other factors, such as distinguishing ability, inherent preferences, attention points capacity, and tokenizer",
    "checked": true,
    "id": "0b630863353860145ebc7a7f03b29b1866d172cd",
    "semantic_title": "icleval: evaluating in-context learning ability of large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4obJ80M9Ts8": {
    "title": "Token Alignment via Character Matching for Subword Completion",
    "volume": "review",
    "abstract": "Generative models, widely utilized in various applications, can often struggle with prompts corresponding to partial tokens. This struggle stems from tokenization, where partial tokens fall out of distribution during inference, leading to incorrect or nonsensical outputs. This paper examines a technique to alleviate the tokenization artifact on text completion in generative models, maintaining performance even in regular non-subword cases. The method, termed token alignment, involves backtracking to the last complete tokens and ensuring the model's generation aligns with the prompt. This approach showcases marked improvement across many partial token scenarios, including nuanced cases like space-prefix and partial indentation, with only a minor time increase. The technique and analysis detailed in this paper contribute to the continuous advancement of generative models in handling partial inputs, bearing relevance for applications like code completion and text autocompletion",
    "checked": true,
    "id": "3597148d33e9147a0983e995a8abf023ce529e45",
    "semantic_title": "token alignment via character matching for subword completion",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FRsTstpTaeZ": {
    "title": "Multistage Collaborative Knowledge Distillation from a Large Language Model for Semi-Supervised Sequence Generation",
    "volume": "review",
    "abstract": "We study semi-supervised sequence generation tasks, where the few labeled examples are too scarce to finetune a model, and meanwhile, few-shot prompted large language models (LLMs) exhibit room for improvement. These tasks, such as parsing, are both expensive to annotate and under-represented in the LLM pretraining. In this paper, we present the discovery that the student model distilled from a few-shot prompted LLM can commonly generalize better than its teacher to unseen examples on such a task. Our analysis indicates that this happens because the student can learn a general pattern from the high-quality pseudolabels produced by the teacher, while helpfully not learning a pattern for the low-quality ones.Leveraging this discovery, we propose a new method, Multistage Collaborative Knowledge Distillation from an LLM (MCKD), for these tasks. MCKD first few-shot prompts an LLM to produce pseudolabels for unlabeled data. Then at each stage of an iterative KD process, a new pair of students is trained on disjoint partitions of the pseudolabeled data, and produces new and improved pseudolabels for their unseen partitions. We show the effectiveness of MCKD on four syntactic and semantic parsing datasets. On CRAFT biomedical parsing, for example, 3-stage MCKD with 50 labeled examples outperforms an LLM teacher and vanilla KD by 7.5% and 3.7% parsing F1, respectively, and matches the performance of supervised finetuning with 500 examples",
    "checked": true,
    "id": "6e09ec84a235bb3465aeb97645d7999ce257bb0a",
    "semantic_title": "multistage collaborative knowledge distillation from a large language model for semi-supervised sequence generation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=JRqARG4f9lc": {
    "title": "The Counterfeit Conundrum: Can Code Models Grasp the Nuances of Their Incorrect Generations?",
    "volume": "review",
    "abstract": "While language models are increasingly more proficient at code generation, they still frequently generate incorrect programs. Many of these programs are obviously wrong, but others are more subtle and pass weaker correctness checks such as being able to compile. In this work, we focus on these counterfeit samples: programs sampled from a language model that 1) have a high enough log-probability to be generated at a moderate temperature and 2) pass weak correctness checks. Overall, we discover that most models have a very shallow understanding of counterfeits through three clear failure modes. First, models mistakenly classify them as correct. Second, models are worse at reasoning about the execution behaviour of counterfeits and often predict their execution results as if they were correct. Third, when asking models to fix counterfeits, the likelihood of a model successfully repairing a counterfeit is often even lower than that of sampling a correct program from scratch. Counterfeits also have very unexpected properties: first, counterfeit programs for problems that are easier for a model to solve are not necessarily easier to detect and only slightly easier to execute and repair. Second, counterfeits from a given model are just as confusing to the model itself as they are to other models. Finally, both strong and weak models are able to generate counterfeit samples that equally challenge all models. In light of our findings, we recommend that care and caution be taken when relying on models to understand their own samples, especially when no external feedback is incorporated",
    "checked": false,
    "id": "90378bafdc503d9aa371c1c85a287567687c08d4",
    "semantic_title": "the counterfeit conundrum: can code language models grasp the nuances of their incorrect generations?",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=nsI7sTeGwSG": {
    "title": "k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text",
    "volume": "review",
    "abstract": "Recent watermarked generation algorithms inject detectable signatures during language generation to facilitate post-hoc detection. While token-level watermarks are vulnerable to paraphrase attacks, SemStamp (Hou et al., 2023) applies watermark on the semantic representation of sentences and demonstrates promising robustness. SemStamp employs locality-sensitive hashing (LSH) to partition the semantic space with arbitrary hyperplanes, which results in a suboptimal tradeoff between robustness and speed. We propose k-SemStamp, a simple yet effective enhancement of SemStamp, utilizing k-means clustering as an alternative of LSH to partition the embedding space with awareness of inherent semantic structure. Experimental results indicate that k-SemStamp saliently improves its robustness and sampling efficiency while preserving the generation quality, advancing a more effective tool for machine-generated text detection",
    "checked": true,
    "id": "94c28609614719c68469081ed99315f54cb1fb6a",
    "semantic_title": "k-semstamp: a clustering-based semantic watermark for detection of machine-generated text",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=_85Nt2tF_tY": {
    "title": "Small But Funny: A Feedback-Driven Approach to Humor Distillation",
    "volume": "review",
    "abstract": "The emergence of Large Language Models (LLMs) has brought to light promising language generation capabilities, particularly in performing tasks like complex reasoning and creative writing. Consequently, distillation through imitation of teacher responses has emerged as a popular technique to transfer knowledge from LLMs to more accessible, Small Language Models (SLMs). While this works well for simpler tasks, there is a substantial performance gap on tasks requiring intricate language comprehension and creativity, such as humor generation. We hypothesize that this gap may stem from the fact that creative tasks might be hard to learn by imitation alone and explore whether an approach, involving supplementary guidance from the teacher, could yield higher performance. To address this, we study the effect of assigning a dual role to the LLM - as a \"teacher\" generating data, as well as a \"critic\" evaluating the student's performance. Our experiments on humor generation reveal that the incorporation of feedback significantly narrows the performance gap between SLMs and their larger counterparts compared to merely relying on imitation. As a result, our research highlights the potential of using feedback as an additional dimension to data when transferring complex language abilities via distillation",
    "checked": true,
    "id": "685e5d9cbe177ec50129a6ca1ffb912636bb49d2",
    "semantic_title": "small but funny: a feedback-driven approach to humor distillation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=jLT2HSp5eY4": {
    "title": "Framing in the Presence of Supporting Data: A Case Study in U.S. Economic News",
    "volume": "review",
    "abstract": "The mainstream media has much leeway in what it chooses to cover and how it covers it. These choices have real-world consequences on what people know and their subsequent behaviors. However, the lack of objective measures to evaluate editorial choices makes research in this area particularly difficult. In this paper, we argue that there are newsworthy topics where objective measures exist in the form of supporting data and propose a computational framework to analyze editorial choices in this setup. We focus on the economy because the reporting of economic indicators presents us with a relatively easy way to determine both the selection and framing of various publications. Their values provide a ground truth of how the economy is doing relative to how the publications choose to cover it. To do this, we define frame prediction as a set of interdependent tasks. At the article level, we learn to identify the reported stance towards the general state of the economy. Then, for every numerical quantity reported in the article, we learn to identify whether it corresponds to an economic indicator and whether it is being reported in a positive or negative way. To perform our analysis, we track six American publishers and each article that appeared in the top 10 slots of their landing page between 2015 and 2023",
    "checked": true,
    "id": "b35834725eeffcdb111f3b9162f7c56657f1bbb8",
    "semantic_title": "framing in the presence of supporting data: a case study in u.s. economic news",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0E2wtdkngYB": {
    "title": "Examining LLMs' Uncertainty Expression towards Questions outside Parametric Knowledge",
    "volume": "review",
    "abstract": "Can large language models (LLMs) express their uncertainty in situations where they lack sufficient parametric knowledge to generate reasonable responses? This work aims to systematically investigate LLMs' behaviors in such situations, emphasizing the trade-off between honesty and helpfulness. To tackle the challenge of precisely determining LLMs' \"knowledge gaps\", we diagnostically create unanswerable questions containing non-existent concepts or false premises, ensuring these are outside the LLMs' vast training data. By compiling a benchmark, UnknownBench, which consists of both unanswerable and answerable questions, we quantitatively evaluate the LLMs' performance in maintaining honesty while being helpful. Using a model-agnostic unified confidence elicitation approach, we observe that most LLMs fail to consistently refuse or express high uncertainty towards questions outside their parametric knowledge, although instruction fine-tuning and alignment techniques can provide marginal enhancements. Moreover, LLMs' uncertainty expression does not always stay consistent with the perceived confidence of their direct responses. We will release our data and code",
    "checked": true,
    "id": "1834f8126e97057e321149b50e342754a096d14d",
    "semantic_title": "examining llms' uncertainty expression towards questions outside parametric knowledge",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=7an-yaUDNqZ": {
    "title": "Aligning Large Language Models by On-Policy Self-Judgment",
    "volume": "review",
    "abstract": "To align large language models with human preferences, existing research either utilizes a separate reward model (RM) to perform on-policy learning or simplifies the training procedure by discarding the on-policy learning and the need for a separate RM. In this paper, we present a novel alignment framework, SELF-JUDGE that is (1) on-policy learning and 2) parameter efficient, as it does not require an additional RM for evaluating the samples for on-policy learning. To this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a single model acting as both a policy and a judge. Specifically, we view the pairwise judgment task as a special case of the instruction-following task, choosing the better response from a response pair. Thus, the resulting model can judge preferences of on-the-fly responses from current policy initialized from itself. Experimental results show the efficacy of SELF-JUDGE, outperforming baselines in preference benchmarks. We also show that self-rejection with oversampling can improve further without an additional evaluator",
    "checked": true,
    "id": "8c68c6f45d4f8af0536dd7a401fe9333eda7e1be",
    "semantic_title": "aligning large language models by on-policy self-judgment",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=M8DtmP2hK4o": {
    "title": "re-thinking: In-depth Interactive Thinking with Retrieved Knowledge for Large Language Models",
    "volume": "review",
    "abstract": "The hallucinations of large language models (LLMs) have the potential to be solved by Retrieval-Augmented Generation (RAG), which incorporates external knowledge during the generation process. Although effective, incorrectly retrieved knowledge uncontrollably carries rich noise, which damages RAG performance. In this paper, we propose a simple yet highly effective prompting strategy: re-thinking. Drawn inspiration from how humans selectively learn with external knowledge, re-thinking considers the retrieved knowledge cannot be treated equally, which means selectively retaining and removing knowledge. To gather insightful and comprehensive selection process, additionally, we develop a fine-grained and in-depth interaction mechanism, which equips knowledge with queries again, making them have richer, back-and-forth interactions, obtaining fine-grained correlation or slight differences. Experiments conducted on various reasoning benchmarks and LLMs demonstrate the effectiveness of the proposed re-thinking framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kOSFTGrcp36": {
    "title": "Context Filtering with Reward Modeling in Question Answering",
    "volume": "review",
    "abstract": "Question Answering (QA) in NLP is a task that involves finding answers using information within a context. Yet, the mix of relevant and irrelevant information in these contexts can hinder performance enhancements in QA tasks. To address this, we introduce a context filtering approach that removes non-essential details, summarizing crucial content through Reward Modeling. This method emphasizes keeping vital data while omitting the extraneous during summarization model training. We offer a framework for developing efficient QA models by discerning useful information from dataset pairs, bypassing the need for costly human evaluation. Furthermore, we show that our approach can significantly enhance performance beyond the baseline, as evidenced by the EM Per Token (EPT) metric, suggesting a notable performance boost from the perspective of token efficiency for the low resource settings",
    "checked": false,
    "id": "b3876204758c54f63c64cd82d5b21b7f4983736d",
    "semantic_title": "prompt generate train (pgt): few-shot domain adaption of retrieval augmented generation models for open book question-answering",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pnSN_ytb5VA": {
    "title": "ML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks",
    "volume": "review",
    "abstract": "Large language models have shown promising performance in code generation benchmarks. However, a considerable divide exists between these benchmark achievements and their practical applicability, primarily attributed to real-world programming's reliance on pre-existing libraries. Instead of evaluating LLMs to code from scratch, this work aims to propose a new evaluation setup where LLMs use open-source libraries to finish machine learning tasks. Therefore, we propose ML-Bench, a benchmark to evaluate how well these models use open-source libraries for machine learning tasks. It includes 10,100 samples spanning 169 tasks from 18 GitHub repositories, requiring models to understand complex documents and code structures. Notably, while GPT-4 exhibits remarkable improvement over other LLMs, it completes only 33.82\\% of tasks. Furthermore, we present an agent baseline ML-Agent, which navigates codebases and generates executable code effectively",
    "checked": false,
    "id": "c19f411ea5fb33fb196167bb5fa18a9df001d8eb",
    "semantic_title": "hlat: high-quality large language model pre-trained on aws trainium",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7QN8Y9Zm4ca": {
    "title": "ChOiRe: Characterizing and Predicting Human Opinions with Chain of Opinion Reasoning",
    "volume": "review",
    "abstract": "Aligning language models (LMs) with human opinion is challenging yet vital to enhance their grasp of human values, preferences, and beliefs. We present ChOiRe, a four-step framework to predict human opinion which differentially models the user's explicit personae (i.e. demographic or ideological attributes) that are manually declared, and implicit personae inferred from user historical opinions. ChOiRe consists of (i) an LM analyzing the user's explicit personae to filter out irrelevant attributes; (ii) the LM ranking the implicit persona opinions into a preferential list; (iii) Chain-of-Opinion (CoO) reasoning, where the LM sequentially analyzes the explicit personae and the most relevant implicit personae to perform opinion prediction; (iv) and where ChOiRe executes Step (iii)'s CoO multiple times with increasingly larger lists of implicit personae to overcome insufficient personae information to infer a final result. ChOiRe achieves new state-of-the-art effectiveness with limited inference calls, improving previous techniques significantly by 3.22%. We also show that ChOiRe's Steps (i) and (ii) can significantly better fine-tune opinion-aligned models, by up to 18.44%",
    "checked": true,
    "id": "1808a3664c32abad1c56175a437848364b3ed8f2",
    "semantic_title": "choire: characterizing and predicting human opinions with chain of opinion reasoning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=sNdGDMA0aWi": {
    "title": "Distantly-Supervised Joint Extraction with Noise-Robust Learning",
    "volume": "review",
    "abstract": "Joint entity and relation extraction is a process that identifies entity pairs and their relations using a single model. We focus on the problem of joint extraction in distantly-labeled data, whose labels are generated by aligning entity mentions with the corresponding entity and relation tags using a knowledge base (KB). One key challenge is the presence of noisy labels arising from both incorrect entity and relation annotations, which significantly impairs the quality of supervised learning. Existing approaches, either considering only one source of noise or making decisions using external knowledge, cannot well-utilize significant information in the training data. We propose DENRL, a generalizable framework that 1) incorporates a lightweight transformer backbone into a sequence labeling scheme for joint tagging, and 2) employs a noise-robust framework that regularizes the tagging model with significant relation patterns and entity-relation dependencies, then iteratively self-adapts to instances with less noise from both sources. Surprisingly, experiments on two benchmark datasets show that DENRL, using merely its own parametric distribution and simple data-driven heuristics, outperforms strong baselines by a large margin with better interpretability",
    "checked": true,
    "id": "bbea4f84615e7ecf1f270ab31ee663d601d8eaeb",
    "semantic_title": "distantly-supervised joint extraction with noise-robust learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vhtk8nMQ1K": {
    "title": "PERSONA-ADAPTIVE IE: Harnessing Human-AI Collaboration to Adaptively Extract Persona-Aware Information",
    "volume": "review",
    "abstract": "The evolving nature of information needs across diverse domains like emergency situations (disease outbreak, earthquake) necessitates a flexible information extraction (IE) system.Despite this, existing IE systems are either fully supervised, requiring expensive human annotations, or fully unsupervised, extracting information that often do not cater to user's needs. To address these issues, we formally introduce the task of ``IE on-the-fly'', and solve it using our proposed Persona-Adaptive IE framework that leverages human-in-the-loop refinement to adapt to changing user queries. Through human experiments on three diverse datasets, we demonstrate that Persona-Adaptive IE is a domain-generalizable, responsive, efficient framework for helping users access useful information while quickly reorganizing information in response to evolving information needs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZhhOH9zFhC": {
    "title": "FUSE: Measure-Theoretic Compact Fuzzy Set Representation for Taxonomy Expansion",
    "volume": "review",
    "abstract": "Taxonomy Expansion, which relies on modeling concepts and concept relations, can be formulated as a set representation learning task. The generalization of set, fuzzy set, incorporates uncertainty and measures the information within a semantic concept, making it suitable for concept modeling. Existing works usually model sets as vectors or geometric objects such as boxes, which are not closed under set operations. In this work, we propose a sound and efficient formulation of set representation learning based on its volume approximation as a fuzzy set. The resulting embedding framework, \\textit{\\underline{Fu}zzy \\underline{S}et \\underline{E}mbedding}, satisfies all set operations and compactly approximates the underlying fuzzy set, hence preserving information while being efficient to learn, relying on minimum neural architecture. We empirically demonstrate the power of FUSE on the task of taxonomy expansion, where FUSE achieves remarkable improvements up to 23% compared with existing baselines. Our work marks the first attempt to understand and efficiently compute the embeddings of fuzzy sets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q0e0ZsYg5b": {
    "title": "Plan, Generate and Complicate: Improving Low-resource Dialogue State Tracking via Easy-to-Difficult Zero-shot Data Augmentation",
    "volume": "review",
    "abstract": "Data augmentation methods have been a promising direction to improve the performance of small models for low-resource dialogue state tracking. However, traditional methods rely on pre-defined user goals and neglect the importance of data complexity in this task. In this paper, we propose EDZ-DA, an Easy-to-Difficult Zero-shot Data Augmentation framework for low-resource dialogue state tracking that utilizes large language models to automatically catch the relationships of different domains and then generate the dialogue data. We also complicate the dialogues based on the domain relation to enhance the model's capability for co-reference slot tracking. Furthermore, we permute slot values to mitigate the influence of output orders and the problem of incomplete value generation. Experimental results illustrate the superiority of our proposed method compared to previous strong data augmentation baselines on MultiWOZ",
    "checked": true,
    "id": "eec7372fc31351b8af3cf2c9c87bbb44ddfdda2a",
    "semantic_title": "plan, generate and complicate: improving low-resource dialogue state tracking via easy-to-difficult zero-shot data augmentation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=9enXg09opR": {
    "title": "Defending Jailbreak Attacks using Self-Refine with Formatting",
    "volume": "review",
    "abstract": "Caution: This paper includes offensive words that could potentially cause unpleasantness. Language models (LMs) are vulnerable to exploitation for adversarial misuse. Training LMs for safety alignment is extensive and makes it hard to respond to fast-developing attacks immediately, such as jailbreaks. We propose self-refine with formatting that achieves outstanding safety even in non-safety-aligned LMsand evaluate our method alongside several defense baselines, demonstrating that it is the safest training-free method against jailbreak attacks. Additionally, we proposed a formatting method that improves the efficiency of the self-refine process while reducing attack success rates in fewer iterations. We've also observed that non-safety-aligned LMs outperform safety-aligned LMs in safety tasks by giving more helpful and safe responses. In conclusion, our findings can achieve less safety risk with fewer computational costs, allowing non-safety LM to be easily utilized in real-world service",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uRG2Hw9eak": {
    "title": "Atomic Self-Consistency for Better Long Form Generations",
    "volume": "review",
    "abstract": "Recent work has aimed to improve LLM generations by filtering out hallucinations, thereby improving the precision of the information in responses. Correctness of a long-form response, however, also depends on the recall of multiple pieces of information relevant to the question. In this paper, we introduce Atomic Self-Consistency (ASC), a technique for improving the recall of relevant information in an LLM response. ASC follows recent work, Universal Self-Consistency (USC) in using multiple stochastic samples from an LLM to improve the long-form response. Unlike USC which only focuses on selecting the best single generation, ASC picks authentic subparts from the samples and merges them into a superior composite answer. Through extensive experiments and ablations, we show that merging relevant subparts of multiple samples performs significantly better than picking a single sample. ASC demonstrates significant gains over USC on multiple factoid and open ended QA datasets - ASQA, QAMPARI, QUEST, ELI5 with ChatGPT and Llama2. Our analysis also reveals untapped potential for enhancing long-form generations using the approach of merging multiple samples",
    "checked": true,
    "id": "9111fc83b652c871c8e223b29009de9698b8f9b2",
    "semantic_title": "atomic self-consistency for better long form generations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GXIcxhriQn0": {
    "title": "On the Cross-lingual Consistency of Text Watermark for Large Language Models",
    "volume": "review",
    "abstract": "Text watermarking technology aims to tag and identify content produced by large language models (LLMs) to prevent misuse. In this study, we introduce the concept of cross-lingual consistency in text watermarking, which assesses the ability of text watermarks to maintain their effectiveness across different languages. Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages. Based on this observation, we propose a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking by first obtaining a response from an LLM in a pivot language, which is then translated into the target language. CWRA can effectively remove watermarks by reducing the Area Under the Curve (AUC) from 0.95 to 0.67 without performance loss. Furthermore, we analyze two key factors that contribute to the cross-lingual consistency in text watermarking and propose a defense method that increases the AUC from 0.67 to 0.88 under CWRA",
    "checked": false,
    "id": "b5142d7f7282be6aec15c1d30091eeb442268bbf",
    "semantic_title": "can watermarks survive translation? on the cross-lingual consistency of text watermark for large language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=T1kZ0tdOtZM": {
    "title": "Enhancing Hallucination Detection through Perturbation-Based Synthetic Data Generation in System Responses",
    "volume": "review",
    "abstract": "Detecting hallucinations in large language model (LLM) outputs is pivotal, yet traditional fine-tuning for this classification task is impeded by the expensive and quickly outdated annotation process, especially across numerous vertical domains and in the face of rapid LLM advancements. In this study, we introduce an approach that automatically generates both faithful and hallucinated outputs by rewriting system responses. Experimental findings demonstrate that a T5-base model, fine-tuned on our generated dataset, surpasses state-of-the-art zero-shot detectors and existing synthetic generation methods in both accuracy and latency, indicating the effectiveness of our approach",
    "checked": true,
    "id": "eceb07cb158b96e053fb1ef39e3c6f0aa1e73d2e",
    "semantic_title": "enhancing hallucination detection through perturbation-based synthetic data generation in system responses",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=wh4WPF1fE8O": {
    "title": "Uniform Anaphora Resolution and Evaluation",
    "volume": "review",
    "abstract": "Despite the increasing attention on tackling anaphora resolution in an end-to-end multitask learning fashion, the state of the research topic is still unsatisfactory in that most works focus only on a subset of relations (either bridging or coreference), lacking generalizability and granularity for more complicated anaphoric relations. Moreover, the evaluations are still a mix of diverse metrics for different subtasks. We leverage a multitask learning framework from the Relation Extraction field which can be extended to perform fine-grained anaphora resolution and introduce a heterogeneous graph representation to evaluate coreference and other anaphoric relations using one uniform metric. All the data and source code will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EF_s1G7NcPI": {
    "title": "Abstract Representations of Binomial Ordering Preferences in Large Language Models",
    "volume": "review",
    "abstract": "To what extent do large language models learn abstract representations as opposed to more superficial aspects of their very large training corpora? We examine this question in the context of binomial ordering preferences involving two conjoined nouns in English. When choosing a binomial ordering (radios and televisions vs televisions and radios), humans rely on more than simply the observed frequency of each option. Humans also rely on abstract preferences beyond the specific binomials themselves (e.g., preferences for short words before long words). We investigate whether large language models simply rely on the observed preference in their training data, or whether they are capable of learning the generative preferences (i.e., abstract representations) that humans rely on. Our results suggest that smaller models are simply copying their training data, but larger models are capable of learning generative preferences. Further, similar to humans, the larger models rely more on generative preferences for low frequency items than high frequency items. Our study provides further insight into large language models' abilities to learn abstract representations of language structure and use",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ziEffg-NpPx": {
    "title": "Efficient Tool Use with Chain-of-Abstraction Reasoning",
    "volume": "review",
    "abstract": "To achieve faithful reasoning that aligns with human expectations, large language models (LLMs) need to ground their reasoning to real-world knowledge (e.g., web facts, math and physical rules). Tools help LLMs access this external knowledge, but there remains challenges for fine-tuning LLM agents (e.g., Toolformer) to invoke tools in multi-step reasoning problems, where inter-connected tool calls require holistic and efficient tool usage planning.In this work, we propose a new method for LLMs to better leverage tools in multi-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge. This planning with abstract chains enables LLMs to learn more general reasoning strategies, which are robust to shifts of domain knowledge (e.g., math results) relevant to different reasoning questions. It also allows LLMs to perform decoding and calling of external tools in parallel, which avoids the inference delay caused by waiting for tool responses. In mathematical reasoning and Wiki QA domains, we show that our method consistently outperforms previous chain-of-thought and tool-augmented baselines on both in-distribution and out-of-distribution test sets, with an average ~6% absolute QA accuracy improvement. LLM agents trained with our method also show more efficient tool use, with inference speed being on average ~1.4x faster than baseline tool-augmented LLMs",
    "checked": true,
    "id": "8fb92f51434543c4a8cd4980f84cf04552c712cc",
    "semantic_title": "efficient tool use with chain-of-abstraction reasoning",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=ZMLUl5w3Iyq": {
    "title": "HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition",
    "volume": "review",
    "abstract": "Large language models (LLMs) have emerged as a promising alternative to expensive human evaluations. However, the alignment and coverage of LLM-based evaluations are often limited by the scope and potential bias of the evaluation prompts and criteria. To address this challenge, we propose HD-Eval, a novel framework that iteratively aligns LLM-based evaluators with human preference via Hierarchical Criteria Decomposition. HD-Eval inherits the essence from the evaluation mindset of human experts and enhances the alignment of LLM-based evaluators by decomposing a given evaluation task into finer-grained criteria, aggregating them according to estimated human preferences, pruning insignificant criteria with attribution, and further decomposing significant criteria. By integrating these steps within an iterative alignment training process, we obtain a hierarchical decomposition of criteria that comprehensively captures aspects of natural language at multiple levels of granularity. Implemented as a white box, the human preference-guided aggregator is efficient to train and more explainable than relying solely on prompting, and its independence from model parameters makes it applicable to closed-source LLMs. Extensive experiments on three evaluation domains demonstrate the superiority of HD-Eval in further aligning state-of-the-art evaluators and providing deeper insights into the explanation of evaluation results and the task itself",
    "checked": true,
    "id": "868c06e552be0c9a53d41d18eaa233402a2bb7ee",
    "semantic_title": "hd-eval: aligning large language model evaluators through hierarchical criteria decomposition",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=qN2D47fuER": {
    "title": "On the Evaluation of Speech Foundation Models for Spoken Language Understanding",
    "volume": "review",
    "abstract": "The Spoken Language Understanding Evaluation (SLUE) suite of benchmark tasks was recently introduced to address the need for openresources and benchmarking of complex spoken language understanding (SLU) tasks, including both classification and sequence generation tasks, on natural speech. The benchmark has demonstrated preliminary success in using pre-trained speech foundation models (SFM) for these SLU tasks. However, the community still lacks a fine-grained understanding of the comparative utility of different SFMs. Inspired by this, we ask: which SFMs offer the most benefits for these complex SLU tasks, and what is the most effective approach for incorporating these SFMs? To answer this, we perform an extensive evaluation of multiple supervised and self-supervised SFMs using several evaluation protocols: (i) frozen SFMs with a lightweight prediction head, (ii) frozen SFMs with a complex prediction head, and (iii) fine-tuned SFMs with a lightweight prediction head. Although the supervised SFMs are pre-trained on much more data and with labels, they do not always outperform self-supervised SFMs; the latter tend to perform at least as well as, and sometimes better than, supervised SFMs on the sequence generation tasks in SLUE. While there is no universally optimal way of incorporating SFMs, the complex prediction head gives the best performance for most tasks, although it increases the inference time. We also introduce an open-source toolkit and performance leaderboard, SLUE-PERB, for these tasks and modeling strategies",
    "checked": true,
    "id": "1d494d9ff14885e2ea751ad8bb22e5fd183113ba",
    "semantic_title": "on the evaluation of speech foundation models for spoken language understanding",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=RgRmG7qfk7": {
    "title": "AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language Model Outputs",
    "volume": "review",
    "abstract": "Pre-trained Large Language Models (LLMs) have significantly advanced natural language processing capabilities but are susceptible to biases present in their training data, leading to unfair outcomes in various applications. While numerous strategies have been proposed to mitigate bias, they often require extensive computational resources and may compromise model performance. In this work, we introduce AXOLOTL, a novel post-processing framework, which operates agnostically across tasks and models, leveraging public APIs to interact with LLMs without direct access to internal parameters. Through a three-step process resembling zero-shot learning, AXOLOTL identifies biases, proposes resolutions, and guides the model to self-debias its outputs. This approach minimizes computational costs and preserves model performance, making AXOLOTL a promising tool for debiasing LLM outputs with broad applicability and ease of use",
    "checked": true,
    "id": "cee4c6b3a9c47659d72c8445ab5a85079c923626",
    "semantic_title": "axolotl: fairness through assisted self-debiasing of large language model outputs",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=40acx67n1J": {
    "title": "CEScore: Simple and Efficient Confidence Estimation Model for Evaluating Text Simplification",
    "volume": "review",
    "abstract": "Evaluating quality of text simplification (TS) is challenging, especially for models following unsupervised or reinforcement learning techniques, where reference data is unavailable. We introduce CEScore, a novel statistical model that evaluates TS quality without relying on references. By mimicking the way humans evaluate TS, CEScore provides 4 metrics ($S_{score}$, $G_{score}$, $M_{score}$, and $CE_{score}$) to assess simplicity, grammaticality, meaning preservation, and overall quality, respectively. In an experiment with 26 TS models, CEScore correlates strongly with human evaluations, achieving 0.98 in Spearman correlations at model-level. This underscores the potential of CEScore as a simple and efficient metric for assessing the quality of TS models",
    "checked": false,
    "id": "a2d95e1f992efbc2e503ec3cbea49707db0a409d",
    "semantic_title": "cescore: simple and efficient confidence estimation model for evaluating split and rephrase",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mjLPEvQ7C6": {
    "title": "GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) face threats from unsafe prompts. Existing methods for detecting unsafe prompts are primarily online moderation APIs or finetuned LLMs. These strategies, however, often require extensive and resource-intensive data collection and training processes. In this study, we propose GradSafe, which effectively detects unsafe prompts by scrutinizing the gradients of safety-critical parameters in LLMs. Our methodology is grounded in a pivotal observation: the gradients of an LLM's loss for unsafe prompts paired with compliance response on certain safety-critical parameters exhibit consistent patterns. In contrast, safe prompts lead to markedly different gradient patterns. Building on this observation, GradSafe analyzes the gradients from prompts (paired with compliance responses) to accurately detect unsafe prompts. We show that GradSafe, applied to Llama-2 without further training, outperforms Llama Guardâ€”despite its extensive finetuning with a large datasetâ€”in detecting unsafe prompts. This superior performance is consistent across both zero-shot and adaptation scenarios, as evidenced by our evaluations on the ToxicChat and XSTest",
    "checked": false,
    "id": "7b3b81c91e328ab3a8a2fbd131967cefa7fcccf4",
    "semantic_title": "gradsafe: detecting jailbreak prompts for llms via safety-critical gradient analysis",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=Et18id2YxG": {
    "title": "An Unified Hallucination Mitigation Framework for Large Vision-Language Models",
    "volume": "review",
    "abstract": "Hallucination is a common problem for Large Vision-Language Models (LVLMs) with long generations which is difficult to eradicate. The generation with hallucinations is partially inconsistent with the image content. To mitigate hallucination, current studies either focus on the process of model inference or the results of model generation, but the solutions they design sometimes do not deal appropriately with various types of queries and the hallucinations of the generations about these queries. To accurately deal with various hallucinations, we present a zero-shot unified framework, Dentist, for hallucination mitigation. The core step is to first classify the queries, then perform different processes of hallucination mitigation based on the classification result, just like a dentist first observes the teeth and then makes a plan. In a simple deployment, Dentist can classify queries as perception or reasoning and easily mitigate hallucinations in answers which has been demonstrated in our experiments. On MMbench, we achieved a 13.44%/10.2%/15.8% improvement on Image Quality, a Coarse Perception visual question answering (VQA) task, over the baseline InstructBLIP/LLaVA/VisualGLM. Our source code will be released on GitHub",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ii_yTkJf74": {
    "title": "Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency",
    "volume": "review",
    "abstract": "Large language models (LLMs) have exhibited remarkable ability in code generation. However, generating the correct solution in a single attempt still remains a challenge. Prior works utilize verification properties in software engineering to verify and re-rank solutions in a majority voting manner. But the assumption behind them that generated verification properties have better qualities than solutions may not always hold. In this paper, we treat them equally as different perspectives of LLMs' reasoning processes. We propose the Multi-Perspective Self-Consistency (MPSC) framework incorporating both inter- and intra-consistency across outputs from multiple perspectives. Specifically, we prompt LLMs to generate diverse outputs from three perspectives, Solution, Specification and Test case, constructing a 3-partite graph. With two measure functions of consistency, we embed both inter- and intra-consistency information into the graph. The optimal choice of solutions is then determined based on analysis in the graph. MPSC significantly boosts performance of foundation models (ChatGPT in this paper) on various benchmarks, including HumanEval (+15.91%), MBPP (+6.43%) and CodeContests (+9.37%), even surpassing GPT-4",
    "checked": true,
    "id": "0d22f06a1f5ad9f62b2f35c126b514f927586c85",
    "semantic_title": "enhancing large language models in coding through multi-perspective self-consistency",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=g1h3opKXua": {
    "title": "SecFormer: Fast and Accurate Privacy-Preserving Inference for Transformer Models via SMPC",
    "volume": "review",
    "abstract": "With the growing use of Transformer models hosted on cloud platforms to offer inference services, privacy concerns are escalating, especially concerning sensitive data like investment plans and bank account details. Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect the privacy of inference data and model parameters. However, the application of SMPC in Privacy-Preserving Inference (PPI) for Transformer models, often leads to considerable slowdowns or declines in performance. This is largely due to the multitude of nonlinear operations in the Transformer architecture, which are not well-suited to SMPC and difficult to circumvent or optimize effectively. To address this concern, we introduce a comprehensive PPI framework called SecFormer to achieve fast and accurate PPI for Transformer models. We successfully eliminate the high-cost exponential and maximum operations in PPI without sacrificing model performance and developed a suite of efficient SMPC protocols by employing suitable numerical computation methods to boost other complex nonlinear functions in PPI, including GeLU, LayerNorm, and a redesigned Softmax. Our extensive experiments reveal that SecFormer outperforms MPCFormer in performance, showing improvements of $3.4\\%$ and $24.7\\%$ for BERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, respectively. In terms of efficiency, SecFormer is 3.57 and 3.58 times faster than PUMA for BERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, demonstrating its effectiveness and speed",
    "checked": false,
    "id": "977bfb905183a2ad0a5433efbb84086a87140c67",
    "semantic_title": "secformer: towards fast and accurate privacy-preserving inference for large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=b9bfDTZ4HB": {
    "title": "DS-MoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models",
    "volume": "review",
    "abstract": "In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-$K$ out of $N$ experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DS-MoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into $mN$ ones and activating $mK$ from them, allowing for a more flexible combination of activated experts; (2) isolating $K_s$ experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DS-MoE 2B achieves comparable performance with GShard 2.9B, which has 1.5$\\times$ expert parameters and computation. In addition, DS-MoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which sets the upper bound of MoE models. Subsequently, we scale up DS-MoE to 16B parameters and show that it achieves comparable performance with DeepSeek 7B and LLaMA2 7B, with only about 40% of computations",
    "checked": false,
    "id": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda",
    "semantic_title": "deepseekmoe: towards ultimate expert specialization in mixture-of-experts language models",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=Qj7hFQ5518": {
    "title": "Character-Eval: Quantitative Metric for Assessing Personality-Mimicking Large Language Models",
    "volume": "review",
    "abstract": "This study introduces Character-Eval, a pioneering method for the quantitative assessment of Large Language Models (LLMs) that emulate distinct personality traits. Traditional methods for evaluating LLMs often rely on subjective judgments, but Character-Eval innovates by using prompt engineering to objectively gauge LLMs' abilities in mimicking complex personalities. Character-Eval not only accurately identified personality traits in LLMs but also performed comparably to human evaluations. This was particularly evident in a pilot application involving datasets of public figures, confirming the tool's scalability and reliability.Significantly, this study lays the groundwork for an automated optimization scheme. This scheme operates without human intervention, particularly in the challenging domain of replicating 'similarity in style and thought'. Character-Eval represents a major leap forward in LLM evaluation, offering a metric closely aligned with human judgment and providing essential insights for the development of models that authentically replicate human-like personality traits",
    "checked": false,
    "id": "23ca29f84b3feef5199d5fe6bd86cefee7bfce4c",
    "semantic_title": "dynamic generation of personalities with large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H0w_iIYKqK": {
    "title": "Metaphor Understanding Challenge Dataset for LLMs",
    "volume": "review",
    "abstract": "Metaphors in natural language are a reflection of fundamental cognitive processes such as analogical reasoning and categorisation, and are deeply rooted in everyday communication. Metaphor understanding is therefore an essential task for large language models (LLMs). We release the Metaphor Understanding Challenge Dataset (MUNCH), designed to evaluate the metaphor understanding capabilities of LLMs. The dataset provides over 10k paraphrases for sentences containing metaphor use, as well as 1.5 instances containing inapt paraphrases. The inapt paraphrases were carefully selected to serve as control to determine whether the model indeed performs full metaphor interpretation or rather resorts to lexical similarity. All apt and inapt paraphrases were manually annotated. The metaphorical sentences cover natural metaphor uses across 4 genres (academic, news, fiction, and conversation), and they exhibit different levels of novelty. Experiments with LLaMA and GPT-3.5 demonstrate that MUNCH presents a challenging task for LLMs",
    "checked": true,
    "id": "9f06b3caa9bb8bcf35c5175ed65a5ac451d852c3",
    "semantic_title": "metaphor understanding challenge dataset for llms",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=R_hDlXzOsr": {
    "title": "A Multiple-Fill-in-the-Blank Exam Approach for Enhancing Zero-Resource Hallucination Detection in Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) often fabricate a hallucinatory text. Several methods have been developed to detect such text by semantically comparing it with the multiple versions probabilistically regenerated. However, a significant issue is that if the storyline of each regenerated text changes, the generated texts become incomparable, which worsen detection accuracy.In this paper, we propose a hallucination detection method that incorporates a multiple-fill-in-the-blank exam approach to address this storyline-changing issue. First, our method creates a multiple-fill-in-the-blank exam by masking multiple objects from the original text. Second, prompts an LLM to repeatedly answer this exam. This approach ensures that the storylines of the exam answers align with the original ones. Finally, quantifies the degree of hallucination for each original sentence by scoring the exam answers, considering the potential for hallucination snowballing within the original text itself. Experimental results show that our method alone not only outperforms existing methods, but also achieves clearer state-of-the-art performance in the ensembles with existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kzBf6Q-kUZ": {
    "title": "Instruction Fine-Tuning: Does Prompt Loss Matter?",
    "volume": "review",
    "abstract": "We present a study analyzing the effects of prompt loss weighting (PLW) on supervised instruction fine-tuning. We recreated Stanford's Alpaca experiment with both LLaMA 1 and LLaMA 2 and multiple instruction datasets. We found that performance of models fine-tuned on our short-completion dataset had a statistically significant negative quadratic relationship with PLW, but performance of models fine-tuned on medium- and long-completion data did not show any relationship with PLW. I.e., prompt loss can be safely ignored for many datasets. For short-completion data, small values (0.01-0.1) of PLW were optimal for multiple-choice and short-generation tasks while large values (â‰ˆ 1.0) of PLW were optimal for long-generation tasks. We concluded that low non-zero PLW encourages models to not diverge from pre-trained model weights during training and high PLW reduces overfitting. Finally, we present a rough guide for selecting PLW values based on the completion-prompt length ratio of fine-tuning data",
    "checked": true,
    "id": "f889ab4903aaac6c485a505963bcd4c1b4cdf584",
    "semantic_title": "instruction fine-tuning: does prompt loss matter?",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=zd929JMiVb": {
    "title": "Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot Annotator Adaptation",
    "volume": "review",
    "abstract": "In subjective NLP tasks, where a single ground truth does not exist, the inclusion of diverse annotators becomes crucial as their unique perspectives significantly influence the annotations. In realistic scenarios, the annotation budget often becomes the main determinant of the number of perspectives (i.e., annotators) included in the data and subsequent modeling. We introduce a novel framework for annotation collection and modeling in subjective tasks that aims to minimize the annotation budget while maximizing the predictive performance for each annotator. Our framework has a two-stage design: first, we rely on a small set of annotators to build a multitask model, and second, we augment the model for a new perspective by strategically annotating a few samples per annotator. To test our framework at scale, we introduce and release a unique dataset, Moral Foundations Subjective Corpus, of 2000 Reddit posts annotated by 24 annotators for moral sentiment. We demonstrate that our framework surpasses the previous SOTA in capturing the annotators' individual perspectives with as little as 25% of the original annotation budget on two datasets. Furthermore, our framework results in more equitable models, reducing the performance disparity among annotators",
    "checked": true,
    "id": "8d687f8edca283308d024fa3016cc18ec4a21c1e",
    "semantic_title": "cost-efficient subjective task annotation and modeling through few-shot annotator adaptation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qPFmanX4OQ": {
    "title": "Your Vision-Language Model Itself Is a Strong Filter: Towards High-Quality Instruction Tuning with Data Selection",
    "volume": "review",
    "abstract": "Data selection in instruction tuning emerges as a pivotal process for acquiring high-quality data and training instruction-following large language models (LLMs), but it is still a new and unexplored research area for vision-language models (VLMs). Existing data selection approaches on LLMs either rely on single unreliable scores, or use downstream tasks for selection, which is time-consuming and can lead to potential over-fitting on the chosen evaluation datasets. To address this challenge, we introduce a novel dataset selection method, Self-Filter, that utilizes the VLM itself as a filter. This approach is inspired by the observation that VLMs benefit from training with the most challenging instructions. Self-Filter operates in two stages. In the first stage, we devise a scoring network to evaluate the difficulty of training instructions, which is co-trained with the VLM. In the second stage, we use the trained score net to measure the difficulty of each instruction, select the most challenging samples, and penalize similar samples to encourage diversity. Comprehensive experiments on LLaVA and MiniGPT-4 show that Self-Filter can reach better results compared to full data settings with merely about 15% samples, and can achieve superior performance against competitive baselines",
    "checked": true,
    "id": "042ceb6be64a0b9a40bdbf7c96fe4dd7fabd93eb",
    "semantic_title": "your vision-language model itself is a strong filter: towards high-quality instruction tuning with data selection",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=jTZwpxkcu3": {
    "title": "CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation",
    "volume": "review",
    "abstract": "Recently, the advent of large language models (LLMs) has revolutionized generative agents. Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However, the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap, we introduce \\textit{CharacterEval}, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset. The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 11,376 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike. \\textit{CharacterEval} employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions. To facilitate the convenient evaluation for these subjective metrics in \\textit{CharacterEval}, we further developed CharacterRM, a role-playing reward model based on human annotations, which has a higher correlation with human judgment compared to GPT-4. Comprehensive experiments on \\textit{CharacterEval} demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in Chinese role-playing conversation\\footnote{The source code, data source, and reward model will be publicly accessible after acceptance.}",
    "checked": true,
    "id": "6b2cbaf69bd372c90aba9781721d79893f4a2fc4",
    "semantic_title": "charactereval: a chinese benchmark for role-playing conversational agent evaluation",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=H4MNQ7edSD": {
    "title": "Identifying and Analyzing Task-Encoding Tokens in Large Language Models",
    "volume": "review",
    "abstract": "In-context learning (ICL) has become an effective solution for few-shot learning in natural language processing. However, our understanding of ICL's working mechanisms is limited, specifically regarding how models learn to perform tasks from ICL demonstrations. For example, unexpectedly large changes in performance can arise from small changes in the prompt, leaving prompt design a largely empirical endeavour. In this paper, we investigate this problem by identifying and analyzing task-encoding tokens on whose representations the task performance depends. Using experiments that ablate the representations of different token types, we find that template and stopword tokens are the most prone to be task-encoding. In addition, we demonstrate experimentally that lexical meaning, repetition, and text formatting are the main distinguishing characteristics of these tokens. Our work sheds light on how large language models (LLMs) learn to perform a task from demonstrations, deepens our understanding of the varied roles different types of tokens play in LLMs, and provides insights for avoiding instability from improperly utilizing task-encoding tokens",
    "checked": true,
    "id": "8815df56958dfdd6343c4a25361fc7c28e3c9bef",
    "semantic_title": "identifying and analyzing task-encoding tokens in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rWZvjKmw17": {
    "title": "Obfuscation-Robust Code Summarization via Meta Curriculum Learning",
    "volume": "review",
    "abstract": "Code summarization has emerged as a fundamental technique in the field of program comprehension. Despite the significant advancements shown by pre-trained language models like CodeT5, their efficacy is often impeded by code obfuscation, e.g., erosion of semantic cues from the original code. In this paper, we propose a robust code summarization technique called RoFTCodeSum. RoFTCodeSum enhances the robustness of the fine-tuning process by marrying the concepts of meta-learning and curriculum learning. Specifically, it constructs datasets that have progressive difficulty in code comprehension, for example, the original dataset, a dataset with the function name eroded, and a dataset with identifiers eroded. Subsequently, it meta-updates the gradients on data with progressive difficulty, optimizing both the robustness and accuracy simultaneously. Experimental results demonstrate that RoFTCodeSum consistently demonstrates robustness to obfuscated code while maintaining comparable performance on the original dataset",
    "checked": false,
    "id": "25135e5dda4cd46e38b028b9f2fef28a336735ca",
    "semantic_title": "backdoor defense via adaptively splitting poisoned dataset",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=urdDKJh1YG": {
    "title": "PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables Parameter-Efficient Transfer Learning",
    "volume": "review",
    "abstract": "Parameter-efficient fine-tuning (PEFT) has emerged as an effective method for adapting pre-trained language models to various tasks efficiently. Recently, there has been a growing interest in transferring knowledge from one or multiple tasks to the downstream target task to achieve performance improvements. However, current approaches typically either train adapters on individual tasks or distill shared knowledge from source tasks, failing to fully exploit task-specific knowledge and the correlation between source and target tasks. To overcome these limitations, we propose PEMT, a novel parameter-efficient fine-tuning framework based on multi-task transfer learning. PEMT extends the mixture-of-experts (MoE) framework to capture the transferable knowledge as a weighted combination of adapters trained on source tasks. These weights are determined by a gated unit, measuring the correlation between the target and each source task using task description prompt vectors. To fully exploit the task-specific knowledge, we also propose the Task Sparsity Loss to improve the sparsity of the gated unit. We conduct experiments on a broad range of tasks over 17 datasets. The experimental results demonstrate our PEMT yields stable improvements over full fine-tuning, and state-of-the-art PEFT and knowledge transferring methods on various tasks. The results highlight the effectiveness of our method which is capable of sufficiently exploiting the knowledge and correlation features across multiple tasks",
    "checked": true,
    "id": "4ac0a2ff82d26e38299b8e25a36b3e4121644a07",
    "semantic_title": "pemt: multi-task correlation guided mixture-of-experts enables parameter-efficient transfer learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2b1TNqQ1Jl": {
    "title": "Evaluating Image Review Ability of Vision Language Models",
    "volume": "review",
    "abstract": "Large-scale vision language models (LVLMs) are language models that are capable of processing images and text inputs by a single model. This paper explores the use of LVLMs to generate review texts for images. The ability of LVLMs to review images is not fully understood, highlighting the need for a methodical evaluation of their review abilities. Unlike image captions, review texts can be written from various perspectives such as image composition and exposure. This diversity of review perspectives makes it difficult to uniquely determine a single correct review for an image. To address this challenge, we introduce an evaluation method based on rank correlation analysis, in which review texts are ranked by humans and LVLMs, then measures the correlation between these rankings. We further validate this approach by creating a benchmark dataset aimed at assessing the image review ability of recent LVLMs. Our experiments with the dataset reveal that LVLMs, particularly those with proven superiority in other evaluative contexts, excel at distinguishing between high-quality and substandard image reviews",
    "checked": true,
    "id": "c1e88bd7fe8c3174a697c6814a044f899f04db4e",
    "semantic_title": "evaluating image review ability of vision language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=01mC9stSP4": {
    "title": "Content Conditional Debiasing for Fair Text Embedding",
    "volume": "review",
    "abstract": "Mitigating biases in machine learning models has gained increasing attention in Natural Language Processing (NLP). Yet, only a few studies focus on fair text embeddings, which are crucial yet challenging for real-world applications. In this paper, we propose a novel method for learning fair text embeddings. We achieve fairness while maintaining utility trade-off by ensuring conditional independence between sensitive attributes and text embeddings conditioned on the content. Specifically, we enforce that embeddings of texts with different sensitive attributes but identical content maintain the same distance toward the embedding of their corresponding neutral text. Furthermore, we address the issue of lacking proper training data by using Large Language Models (LLMs) to augment texts into different sensitive groups. Our extensive evaluations demonstrate that our approach effectively improves fairness while preserving the utility of embeddings, representing a pioneering effort in achieving conditional independence for fair text embeddings",
    "checked": false,
    "id": "186ebf480e02b89d69afc87781dc865d1cfe47ab",
    "semantic_title": "llm-assisted content conditional debiasing for fair text embedding",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QL39mJdNfx": {
    "title": "Enhancing Conversational Search Rewriting by Guiding Large Language Models with Reinforcement Learning",
    "volume": "review",
    "abstract": "In this paper, we present a new approach to tackling the challenges faced by open-domain conversational question answering systems in understanding user questions and retrieving relevant information. Our approach utilizes Large Language Models (LLMs) for question rewriting and directly optimizes the rewrites toward retrieval performance through reinforcement learning. To deal with the difficulties in fine-tuning LLMs, particularly commercial black-box LLMs, we introduce a \\textit{tunable} small LM as a preliminary question rewriter, which generates guiding cues (initial rewrites) for the \\textit{fixed} LLM-based rewrite editor to produce final, refined rewrites. Our approach significantly outperforms existing question rewriting methods and even surpasses human rewrites in terms of retrieval performance. Furthermore, the reinforced fine-tuning of the preliminary rewriter enables it alone to achieve impressive retrieval performance on par with the LLM editor",
    "checked": false,
    "id": "8fb12c971efd474cfeda5e2097057579adc4adb3",
    "semantic_title": "rewriting conversational utterances with instructed large language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=LqL2quU5Z3": {
    "title": "Self-Supervised Singing Voice Pre-Training towards Speech-to-Singing Conversion",
    "volume": "review",
    "abstract": "Speech-to-singing voice conversion (STS) task always suffers from data scarcity, because it requires paired speech and singing data. Compounding this issue are the challenges of content-pitch alignment and the suboptimal quality of generated outputs, presenting significant hurdles in STS research. This paper presents SVPT, an STS approach boosted by a self-supervised singing voice pre-training model.We leverage spoken language model techniques to tackle the rhythm alignment problem and the in-context learning capability to achieve zero-shot conversion. We adopt discrete-unit random resampling and pitch corruption strategies, enabling training with unpaired singing data and thus mitigating the issue of data scarcity. SVPT also serves as an effective backbone for singing voice synthesis (SVS), offering insights into scaling up SVS models. Experimental results indicate that SVPT delivers notable improvements in both STS and SVS endeavors. Audio samples are available at https://speech2sing.github.io",
    "checked": true,
    "id": "c68006e16f449d29068130e11f8afc3efb57bdc4",
    "semantic_title": "self-supervised singing voice pre-training towards speech-to-singing conversion",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DLwgjEWPtyN": {
    "title": "Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks",
    "volume": "review",
    "abstract": "Multimodal large language models (MLLMs) have proven effective in a wide range of tasks requiring complex reasoning and linguistic comprehension. However, due to a lack of high-quality multimodal resources in languages other than English, the success of MLLMs remains relatively limited to English-based settings. This poses significant challenges in developing comparable models for other languages, including even those with large speaker populations, such as Arabic. To alleviate this challenge, we introduce a comprehensive family of Arabic MLLMs, dubbed Peacock, with strong vision and language capabilities. Through comprehensive qualitative and quantitative analysis, we demonstrate the solid performance of our models on various visual reasoning tasks and further show their emerging dialectal potential. Additionally, we introduce Henna, a new benchmark specifically designed for assessing MLLMs on aspects related to Arabic culture, setting the first stone for culturally aware Arabic MLLMs",
    "checked": true,
    "id": "d5dc38ed5a3e386a2f4303ef858796f94bbb44d0",
    "semantic_title": "peacock: a family of arabic multimodal large language models and benchmarks",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=0FFAjvLH8Q2": {
    "title": "I Could've Asked That: Reformulating Unanswerable Questions",
    "volume": "review",
    "abstract": "When seeking information from unfamiliar documents, users frequently pose questions that cannot be answered by the documents. While existing large language models (LLMs) identify these unanswerable questions, they do not assist users in reformulating their questions, thereby reducing their overall utility. We curate CouldAsk, a benchmark composed of existing and new datasets for document-grounded question answering, specifically designed to study reformulating unanswerable questions. We evaluate state-of-the-art open-source and proprietary LLMs on CouldAsk. The results demonstrate the limited capabilities of these models in reformulating questions. Specifically, GPT-4 and Llama2-7B successfully reformulate questions only 38\\% and 19\\% of the time, respectively. Error analysis shows that 62\\% of the unsuccessful reformulations stem from the models merely rephrasing the questions or even generating identical questions",
    "checked": true,
    "id": "f4ac895ad3b7398515a3913f815e40516b197a31",
    "semantic_title": "i could've asked that: reformulating unanswerable questions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sEZh_xO8qz": {
    "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
    "volume": "review",
    "abstract": "Retrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs",
    "checked": true,
    "id": "94034fd2ed4b6cf41113abb7adc9ae469313c958",
    "semantic_title": "a survey on retrieval-augmented text generation for large language models",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=7PdQrvmIhg": {
    "title": "POMP: Probability-driven Meta-graph Prompter for LLMs in Low-resource Unsupervised Neural Machine Translation",
    "volume": "review",
    "abstract": "Low-resource languages (LRLs) face challenges in supervised neural machine translation (NMT) due to limited parallel data, prompting research in unsupervised NMT.Unsupervised NMT (UNMT), without requiring ground truth, provides solutions for LRL translations using synthetic pseudo-parallel data and parallel data from auxiliary language pairs. However, they usually encounter linguistic noises, such as errors in synthetic data and biases in auxiliary language pairs.We argue that large language models (LLMs) mitigate UNMT's linguistic noises by dynamically organizing auxiliary languages in prompts to improve LRL translations. In this paper, we propose PrObability-driven Meta-graph Prompter (POMP), an approach employing a dynamic graph to organize multiple auxiliary languages, to prompt LLMs in LRL translations. POMP proposes a language-specific meta-graph that dynamically samples multiple translation paths to organize auxiliary languages in constructing prompts. Following the path, POMP prompts LLMs with a mixture of auxiliary languages to translate. We achieve the meta-graph's evolution by back-propagating evaluation scores to update probabilities on the graph.Our experimental improvements show POMP's effectiveness on LRLs' translation",
    "checked": true,
    "id": "22c4e5c458abef1635ee796b5e33437aa1df8b1d",
    "semantic_title": "pomp: probability-driven meta-graph prompter for llms in low-resource unsupervised neural machine translation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=rjq54RjzgTX": {
    "title": "How Robust Are Code Summarization Models to Poor-Readability Code? Fine-grained Evaluation and Benchmark",
    "volume": "review",
    "abstract": "Pre-trained language models such as CodeT5 have demonstrated substantial achievement in code comprehension. Despite the giant leap in model architectures and training processes, we find that the benchmarks used for evaluating code summarization tasks are confined to high-readability code, regardless of the popularity of poor-readability code in reality. As such, they are inadequate to demonstrate the fine-grained ability of models, particularly the robustness to varying readability degrees. In this paper, we introduce OR-CodeSum, a robust evaluation benchmark on code summarization tasks, including seven obfuscated datasets derived from existing datasets. OR-CodeSum innovatively introduces the construction rules of obfuscation code into the testing process, considering semantic, syntactic, and cross-obfuscation robustness of code summarization tasks. Our robustness evaluation reveals that the current code summarization models rely heavily on the readability of the code while not paying enough attention to the syntactic information. We believe OR-CodeSum can help researchers obtain a more comprehensive and profound understanding of code summarization models, which facilitates the improvement of model performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z2oNd9J4Rs": {
    "title": "A Faster Adversarial Attack Method on LLMs Bringing More Transferable Samples",
    "volume": "review",
    "abstract": "With the surge in research on LLMs, various methods for evaluating and attacking the robustness of LLMs have emerged and attracted increasing attention. Traditional adversarial attack methods often have a high dependency on the victim model, leading to poor transferability of generated adversarial samples. The applicability of the obtained attack samples is limited to the current white-box model, making it difficult to transfer attacks to other black-box models. In the scenario of LLMs, problems like poor attack effectiveness and slow attack speed become more pronounced in traditional adversarial attack methods. Through the analysis of traditional text adversarial attack methods, we propose a method capable of producing attack samples with better transferability. Additionally, it enhances attack success rates and greatly improves attack speed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z4OdGbXwdJC": {
    "title": "TextEE: Benchmark, Reevaluation, Reflections, and Future Challenges in Event Extraction",
    "volume": "review",
    "abstract": "Event extraction has gained considerable interest due to its wide-ranging applications. However, recent studies draw attention to evaluation issues, suggesting that reported scores may not accurately reflect the true performance. In this work, we identify and address evaluation challenges, including inconsistency due to varying data assumptions or preprocessing steps, the insufficiency of current evaluation frameworks that may introduce dataset or data split bias, and the low reproducibility of some previous approaches. To address these challenges, we present TextEE, a standardized, fair, and reproducible benchmark for event extraction. TextEE comprises standardized data preprocessing scripts and splits for 14 datasets spanning seven diverse domains and includes 14 recent methodologies, conducting a comprehensive benchmark reevaluation. We also evaluate five varied large language models on our TextEE benchmark and demonstrate how they struggle to achieve satisfactory performance. Inspired by our reevaluation results and findings, we discuss the role of event extraction in the current NLP era, as well as future challenges and insights derived from TextEE. We believe TextEE, the first standardized comprehensive benchmarking tool, will significantly facilitate future event extraction research",
    "checked": true,
    "id": "48b8d66684bd88fc200e8f11ad80d6c2f1ffdd9d",
    "semantic_title": "textee: benchmark, reevaluation, reflections, and future challenges in event extraction",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=poEi1Qul8IC": {
    "title": "CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support",
    "volume": "review",
    "abstract": "Literature review requires researchers to synthesize a large amount of information and is increasingly challenging as the scientific literature expands. In this work, we investigate the potential of LLMs for producing hierarchical organizations of scientific studies to assist researchers with literature review. We define hierarchical organizations as tree structures where nodes refer to topical categories and every node is linked to the studies assigned to that category. Our naive LLM-based pipeline for hierarchy generation from a set of studies produces promising yet imperfect hierarchies, motivating us to collect CHIME, an expert-curated dataset for this task focused on biomedicine. Given the challenging and time-consuming nature of building hierarchies from scratch, we use a human-in-the-loop process in which experts correct errors (both links between categories and study assignment) in LLM-generated hierarchies. CHIME contains 2,174 LLM-generated hierarchies covering 472 topics, and expert-corrected hierarchies for a subset of 100 topics. Expert corrections allow us to quantify LLM performance, and we find that while they are quite good at generating and organizing categories, their assignment of studies to categories could be improved. We attempt to train a corrector model with human feedback which improves study assignment by 12.6 F1 points. We release our dataset and models to encourage research on developing better assistive tools for literature review",
    "checked": true,
    "id": "33b8824f3c8fc3035afabaa77ecee3afe1c9753c",
    "semantic_title": "chime: llm-assisted hierarchical organization of scientific studies for literature review support",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jTg2RWq68i": {
    "title": "Machine Unlearning of Pre-trained Large Language Models",
    "volume": "review",
    "abstract": "This study investigates the concept of the `right to be forgotten' within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. We rigorously evaluate these methods against curated datasets sourced from arXiv, books, and GitHub codes, providing a robust benchmark for unlearning performance. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering substantive insights into the mechanics of machine unlearning for pre-trained LLMs and underscoring the potential for responsible AI development",
    "checked": true,
    "id": "ec072b1382e8da7480737a8cc5b9ca98dda30c9a",
    "semantic_title": "machine unlearning of pre-trained large language models",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=1QOHc49xgA": {
    "title": "XL$^2$Bench: A Benchmark for Extremely Long Context Understanding with Long-range Dependencies",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across diverse tasks but are constrained by their small context window sizes. Various efforts have been proposed to enhance the capability of LLMs to process and comprehend long-context textual information, expanding the context window to accommodate even up to 200K input tokens. Meanwhile, building high-quality benchmarks with much longer text lengths and more demanding tasks to provide comprehensive evaluations is of immense practical interest to facilitate long context understanding research of LLMs. However, prior benchmarks create datasets that ostensibly cater to long-text comprehension by expanding the input of traditional tasks, which falls short to exhibit the unique characteristics of long-text understanding, including long dependency tasks and longer text length compatible with modern LLMs' context window length. In this paper, we introduce a benchmark for eXtremely Long context understanding with Long-range dependencies, XL$^2$Bench, which includes three scenariosâ€”Fiction Reading, Paper Reading, and Law Readingâ€”and four tasks of increasing complexity: Memory Retrieval, Detailed Understanding, Overall Understanding, and Open-ended Generation, covering 27 subtasks in English and Chinese. It has an average length of 100K+ words (English) and 200K+ characters (Chinese). Evaluating six leading LLMs on XL$^2$Bench, we find that their performance significantly lags behind human levels. Moreover, the observed decline in performance across both the original and enhanced datasets underscores the efficacy of our approach to mitigating data contamination",
    "checked": true,
    "id": "d9db8114b3c79f96ee27a9f16e83430448ad3e68",
    "semantic_title": "xl$^2$bench: a benchmark for extremely long context understanding with long-range dependencies",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=-JjxqH-CH14": {
    "title": "Robustness to Noisy Labels in Parameter Efficient Fine-tuning",
    "volume": "review",
    "abstract": "As language models grow in size, Parameter Efficient Fine-tuning (PEFT) methods like Low-Rank Adaptation (LoRA) offer compute efficiency while maintaining performance. However, their robustness to label noise, a significant issue in real-world data, remains unexplored. This study investigates whether LoRA-tuned models demonstrate the same level of noise resistance observed in fully fine-tuned Transformer models. Our investigation has multiple key findings: First, we show that LoRA exhibits robustness to random noise similar to full fine-tuning on balanced data, but unlike full fine-tuning, LoRA does not overfit the noisy data. Second, we observe that compared to full fine-tuning, LoRA forgets significantly fewer data points as noise increases. Third, studying how these robustness patterns change as training data becomes imbalanced, we observe that Transformers struggle with imbalanced data, with robustness declining as imbalance worsens. This study highlights LoRA's promise in real-world settings with noise and data imbalance. Overall, our findings reveal LoRA as a robust and efficient alternative for fine-tuning, shedding light on its distinctive characteristics",
    "checked": false,
    "id": "3547e38cf6c2943bfe8a3c276bd4f0742d9df221",
    "semantic_title": "xvo: generalized visual odometry via cross-modal self-training",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=pi4Far_L74m": {
    "title": "Speech Sense Disambiguation: Tackling Homophone Ambiguity in End-to-End Speech Translation",
    "volume": "review",
    "abstract": "End-to-end speech translation (ST) presents notable disambiguation challenges as it necessitates simultaneous cross-modal and cross-lingual transformations. While word sense disambiguation is an extensively investigated topic in textual machine translation, the exploration of disambiguation strategies for ST models remains limited. Addressing this gap, this paper introduces the concept of speech sense disambiguation (SSD), specifically emphasizing homophones - words pronounced identically but with different meanings. To facilitate this, we first create a comprehensive homophone dictionary and an annotated dataset rich with homophone information established based on speech-text alignment. Building on this unique dictionary, we introduce AmbigST, an innovative homophone-aware contrastive learning approach that integrates a homophone-aware masking strategy. Our experiments on different MuST-C and CoVoST ST benchmarks demonstrate that AmbigST sets new performance standards. Specifically, it achieves SOTA results on BLEU scores for English to German, Spanish, and French ST tasks, underlining its effectiveness in reducing speech sense ambiguity. Data, code, and scripts will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gWBVEKJ8acG": {
    "title": "Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing using In-Context Learning",
    "volume": "review",
    "abstract": "Paraphrasing of offensive content is a better alternative to content removal and helps improve civility in a communication environment. Supervised paraphrasers; however, rely heavily on large quantities of labelled data to help preserve meaning and intent. They also often retain a large portion of the offensiveness of the original content, which raises questions on their overall usability. In this paper we aim to assist practitioners in developing usable paraphrasers by exploring In-Context Learning (ICL) with large language models (LLMs), i.e., using a limited number of input-label demonstration pairs to guide the model in generating desired outputs for specific queries. Our study focuses on key factors such as - number and order of demonstrations, exclusion of prompt instruction, and reduction in measured toxicity. We perform principled evaluation on three datasets, including our proposed Context-Aware Polite Paraphrase dataset, comprising of dialogue-style rude utterances, polite paraphrases, and additional dialogue context. We evaluate our approach using four closed source and one open source LLM. Our results reveal that ICL is comparable to supervised methods in generation quality, while being qualitatively better by 25% on human evaluation and attaining lower toxicity by 76%. Also, ICL-based paraphrasers only show a slight reduction in performance even with just 10% training data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0syryYxonhv": {
    "title": "SQL-CRAFT: Advancing Text-to-SQL Capabilities through Interactive Refinement and Enhanced Reasoning",
    "volume": "review",
    "abstract": "Modern LLMs have become increasingly powerful, but they are still facing challenges in specialized tasks such as Text-to-SQL. We propose SQL-CRAFT, a framework to enhance LLMs' SQL generation Capabilities through inteRActive reFinemenT and enhanced reasoning.We leverage an Interactive Correction Loop (IC-Loop) for LLMs to interact with databases automatically, as well as Python-enhanced reasoning. We conduct experiments on two Text-to-SQL datasets, Spider and Bird, with performance improvements of up to 5.7% compared to the naive prompting method. Moreover, our method surpasses the current state-of-the-art on the Spider Leaderboard, demonstrating the effectiveness of our framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0PnYWbffui": {
    "title": "Do Large Language Models Speak All Languages Equally? A Comparative Study in Low-Resource Settings",
    "volume": "review",
    "abstract": "Large language models (LLMs) have garnered significant interest in natural language processing (NLP), particularly for their remarkable performance in various downstream tasks in resource-rich languages such as English. However, the applicability and efficacy of LLMs in low-resource language contexts remain largely unexplored, thus highlighting a notable gap in linguistic capabilities for these languages. The limited utilization of LLMs in low-resource scenarios is primarily attributed to constraints such as dataset scarcity, computational costs, and research lacunae specific to low-resource languages. To address this gap, we comprehensively examines zero-shot learning using multiple LLMs in both English and low-resource languages. Our findings indicate that GPT-4 consistently outperforms Llama 2 and Gemini, with English consistently demonstrating superior performance across diverse tasks compared to low-resource languages. Furthermore, our analysis reveals that among the evaluated tasks, natural language inference (NLI) exhibits the highest performance, with GPT-4 demonstrating superior capabilities. This research underscores the imperative of assessing LLMs in low-resource language contexts to augment their applicability in general-purpose NLP applications",
    "checked": true,
    "id": "8fcf5e276368c17d74e55d626a5ce5841bf64cdc",
    "semantic_title": "do large language models speak all languages equally? a comparative study in low-resource settings",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H53TyMTXIL3": {
    "title": "ASPIRE: Language-Guided Data Augmentation for Improving Robustness Against Spurious Correlations",
    "volume": "review",
    "abstract": "Neural image classifiers can often learn to make predictions by overly relying on non-predictive features that are spuriously correlated with the class labels in the training data. This leads to poor performance in real-world atypical scenarios where such features are absent. This paper presents ASPIRE (Language-guided Data Augmentation for SPurIous correlation REmoval), a simple yet effective solution for supplementing the training dataset with images without spurious features, for robust learning against spurious correlations via better generalization. ASPIRE, guided by language at various steps, can generate non-spurious images without requiring any group labeling or existing non-spurious images in the training set. Precisely, we employ LLMs to first extract foreground and background features from textual descriptions of an image, followed by advanced language-guided image editing to discover the features that are spuriously correlated with the class label. Finally, we personalize a text-to-image generation model using the edited images to generate diverse in-domain images without spurious features. ASPIRE is complementary to all prior robust training methods in literature, and we demonstrate its effectiveness across 4 datasets and 9 baselines and show that ASPIRE improves the worst-group classification accuracy of prior methods by 1% - 38%. We also contribute a novel test set for the challenging Hard ImageNet dataset",
    "checked": true,
    "id": "712bed095c54ff3023dec1497ca9d89c428c2133",
    "semantic_title": "aspire: language-guided data augmentation for improving robustness against spurious correlations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ekstxy-3Km": {
    "title": "Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks",
    "volume": "review",
    "abstract": "Despite large successes of recent language models on diverse tasks, they suffer from severe performance degeneration in low-resource settings with limited training data available. Many existing works tackle this problem by generating synthetic data from the training data and then training models on them, recently using Large Language Models (LLMs). However, in low-resource settings, the amount of seed data samples to use for data augmentation is very small, which makes generated samples suboptimal and less diverse. To tackle this challenge, we propose a novel method that augments training data by incorporating a wealth of examples from other datasets, along with the given training data. Specifically, we first retrieve the relevant instances from other datasets, such as their input-output pairs or contexts, based on their similarities with the given seed data, and then prompt LLMs to generate new samples with the contextual information within and across the original and retrieved samples. This approach can ensure that the generated data is not only relevant but also more diverse than what could be achieved using the limited seed data alone. We validate our proposed Retrieval-Augmented Data Augmentation (RADA) framework on multiple datasets under low-resource settings of training and test-time data augmentation scenarios, on which it outperforms existing LLM-powered data augmentation baselines",
    "checked": true,
    "id": "8107e5e02e7decbecfe986ebd2a0bb53c1d148c6",
    "semantic_title": "retrieval-augmented data augmentation for low-resource domain tasks",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=WxZ-QdmKXA9": {
    "title": "WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search Results with Citations",
    "volume": "review",
    "abstract": "Enhancing the attribution in large language models (LLMs) is a crucial task. One feasible approach is to enable LLMs to cite external sources that support their generations. However, existing datasets and evaluation methods in this domain still exhibit notable limitations. In this work, we formulate the task of attributed query-focused summarization (AQFS) and present WebCiteS, a Chinese dataset featuring 7k human-annotated summaries with citations. WebCiteS derives from real-world user queries and web search results, offering a valuable resource for model training and evaluation. Prior works in attribution evaluation do not differentiate between groundedness errors and citation errors. They also fall short in automatically verifying sentences that draw partial support from multiple sources. We tackle these issues by developing detailed metrics and enabling the automatic evaluator to decompose the sentences into sub-claims for fine-grained verification. Our comprehensive evaluation of both open-source and proprietary models on WebCiteS highlights the challenge LLMs face in correctly citing sources, underscoring the necessity for further improvement. The dataset and code will be open-sourced to facilitate further research in this crucial field",
    "checked": true,
    "id": "71024999159a83199704359df63ce1ad7ecb79ab",
    "semantic_title": "webcites: attributed query-focused summarization on chinese web search results with citations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ROs1Cv84A1g": {
    "title": "Multimodal Instruction Tuning with Conditional Mixture of LoRA",
    "volume": "review",
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in diverse tasks across different domains, with an increasing focus on improving their zero-shot generalization capabilities for unseen multimodal tasks. Multimodal instruction tuning has emerged as a successful strategy for achieving zero-shot generalization by fine-tuning pre-trained models on diverse multimodal tasks through instructions. As MLLMs grow in complexity and size, the need for parameter-efficient fine-tuning methods like Low-Rank Adaption (LoRA), which fine-tunes with a minimal set of parameters, becomes essential. However, applying LoRA in multimodal instruction tuning presents the challenge of task interference, which leads to performance degradation, especially when dealing with a broad array of multimodal tasks. To address this, this paper introduces a novel approach that integrates multimodal instruction tuning with Conditional Mixture-of-LoRA (MixLoRA). It innovates upon LoRA by dynamically constructing low-rank adaptation matrices tailored to the unique demands of each input instance, aiming to mitigate task interference. Experimental results on various multimodal evaluation datasets indicate that MixLoRA not only outperforms the conventional LoRA with the same or even higher ranks, demonstrating its efficacy and adaptability in diverse multimodal tasks",
    "checked": true,
    "id": "4c111e261bf0f218341e2295cbdd6aae5a5329a7",
    "semantic_title": "multimodal instruction tuning with conditional mixture of lora",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=NPi4TkZkOux": {
    "title": "Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction",
    "volume": "review",
    "abstract": "Red teaming is an effective approach for identifying misaligned behaviors in large language models (LLMs). Existing red teaming typically involves the manual creation of test cases by organized human teams, but the prohibitive costs significantly constrain the scalability of these tests. Recent initiatives have sought to automate red teaming for target language models by training a separate language model. However, most of them are limited to single-turn red teaming and only generate test cases with a limited coverage. For the long-tail issue of LLMs' safety, we believe that an optimal automated red teaming should encompass both breadth and depth. To this end, we introduce \\textbf{HARM}, \\textbf{H}olistic \\textbf{A}utomated \\textbf{R}ed tea\\textbf{M}ing, which generates test prompts top-down using an expandable and fine-grained risk taxonomy to cover as many edge cases as possible, and leverages reinforcement learning for multi-turn adversarial probing. Experimental results indicate that our framework can be utilized to systematically uncover the vulnerabilities of models and offer valuable guidance for the alignment process",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_TwNv0tT9K2": {
    "title": "EMO-SUPERB: An In-depth Look at Speech Emotion Recognition",
    "volume": "review",
    "abstract": "Speech emotion recognition (SER) is a pivotal technology for human-computer interaction systems. However, 80.77% of SER papers yield results that cannot be reproduced (Antoniou et al. 2023). We develop EMO-SUPERB, shorted for EMOtion Speech Universal PERformance Benchmark, aims at enhancing open-source initiatives for SER. EMO-SUPERB includes a user-friendly codebase to leverage 15 state-of-the-art speech self-supervised learning models (SSLMs), for exhaustively evaluation across six open-source SER datasets. EMO-SUPERB streamlines result sharing via an online leaderboard, fostering collaboration within a community-driven benchmark and thereby enhancing the development of SER. On average, 2.58% annotations are annotated using natural language. SER rely on classification models and are unable to process natural languages, leading to discarding these valuable annotations. We prompt ChatGPT to mimic annotators, comprehend natural language annotations, and subsequently re-label the data. By utilizing labels generated by ChatGPT, we consistently achieve an average relative gain of 3.08% across all settings. We make all resources open-source to facilitate future developments in SER",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sBguaQUbuYY": {
    "title": "SpaRC and SpaRP: Spatial Reasoning Characterization and Path Generation for Understanding Spatial Reasoning Capability of Large Language Models",
    "volume": "review",
    "abstract": "Spatial reasoning is a crucial component of both biological and artificial intelligence. In this work, we present a comprehensive study of the capability of current state-of-the-art large language models (LLMs) on spatial reasoning. To support our study, we created and contribute novel spatial characterization frameworks and datasets, the Spatial Reasoning Characterization (SpaRC), and Spatial Reasoning Paths (SpaRP), to enable an in-depth understanding of the spatial relations and compositions as well as the usefulness of spatial reasoning chains. We found that all state-of-the-art LLMs do not perform well on the datasets---their performances are consistently low across different setups. The spatial reasoning is an emergent capability as model sizes scale up. Finetuning both large language models (e.g., Llama-2-70B) and smaller ones (e.g., Llama-2-13B) can significantly improve their F1-scores by 7--32 absolute points. We also found that the top proprietary LLMs still significantly outperform their open-source counterparts in topological spatial understanding and reasoning",
    "checked": true,
    "id": "76bd6b8c67c3ed5673b310bbd0ec0afaaec3fdfe",
    "semantic_title": "sparc and sparp: spatial reasoning characterization and path generation for understanding spatial reasoning capability of large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G38eil-r2w": {
    "title": "Automated Detection and Analysis of Data Practices Using A Real-World Corpus",
    "volume": "review",
    "abstract": "Privacy policies are crucial for informing users about data practices, yet their length and complexity often deter users from reading them. In this paper, we propose an automated approach to identify and visualize data practices within privacy policies at different levels of detail. Leveraging crowd-sourced annotations from the ToS;DR platform, we experiment with various methods to match policy excerpts with predefined data practice descriptions. We further conduct a case study to evaluate our approach on a real-world policy, demonstrating its effectiveness in simplifying complex policies. Experiments show that our approach accurately matches data practice descriptions with policy excerpts, facilitating the presentation of simplified privacy information to users",
    "checked": true,
    "id": "fd7ae388c7115c9fcd9ca52f7560f08bdc63d58c",
    "semantic_title": "automated detection and analysis of data practices using a real-world corpus",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wSvtSOJHRKW": {
    "title": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation",
    "volume": "review",
    "abstract": "Since most large language models (LLMs) are trained once and never updated, they struggle to dynamically adapt to our ever-changing world. In this work, we present FreshQA, a dynamic QA benchmark that tests a model's ability to answer questions that may require reasoning over up-to-date world knowledge. We develop a two-mode human evaluation procedure to measure both correctness and hallucination, which we use to benchmark both closed and open-source LLMs by collecting >50K human judgments. We observe that all LLMs struggle to answer questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. In response, we develop FreshPrompt, a few-shot prompting method that curates and organizes relevant information from a search engine into an LLM's prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such as Perplexity.AI. To facilitate future work, we additionally develop FreshEval, a reliable autorater for quick evaluation and comparison on FreshQA. Our latest results with FreshEval suggest that open-source LLMs such as Mixtral (Jiang et al., 2024), when combined with FreshPrompt, are competitive with closed-source and commercial systems on search-augmented QA",
    "checked": true,
    "id": "be177300487b6d0f25e6cade9a31900454b13281",
    "semantic_title": "freshllms: refreshing large language models with search engine augmentation",
    "citation_count": 88,
    "authors": []
  },
  "https://openreview.net/forum?id=vYcUR6-ctc": {
    "title": "Red Teaming Visual Language Models",
    "volume": "review",
    "abstract": "VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language Models) to accept multimodal inputs. Since it has been verified that LLMs can be induced to generate harmful or inaccurate content through specific test cases (termed as Red Teaming), how VLMs perform in similar scenarios, especially with their combination of textual and visual inputs, remains a question. To explore this problem, we present a novel red teaming dataset RTVLM, which encompasses 12 subtasks (e.g., image misleading, multi-modal jailbreaking, face fairness, etc) under 4 primary aspects (faithfulness, privacy, safety, fairness). Our RTVLM is the first red teaming dataset to benchmark current VLMs in terms of these 4 different aspects. Detailed analysis shows that 10 prominent open-sourced VLMs struggle with the red teaming in different degrees and have up to 31% performance gap with GPT-4V. Additionally, we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning (SFT) using RTVLM, and this bolsters the models' performance with 10% in RTVLM test set, 13% in MM-hallu, and without noticeable decline in MM-Bench, overpassing other LLaVA-based models in similar size with regular alignment data. This reveals that current open-sourced VLMs still lack red teaming alignment. Our code and datasets will be open-sourced",
    "checked": true,
    "id": "1524b7ff78755a3445d22400a8d6c75ba8c0cd65",
    "semantic_title": "red teaming visual language models",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=inTaYGTm3Ch": {
    "title": "Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for Hallucination Mitigation",
    "volume": "review",
    "abstract": "We evaluate the ability of Large Language Models (LLMs) to discern and express their internal knowledge state, a key factor in countering factual hallucination and ensuring reliable application of LLMs. We observe a robust self-awareness of internal knowledge state in LLMs, evidenced by over 85\\% accuracy in knowledge state probing. However, LLMs often fail to faithfully express their internal knowledge during generation, leading to factual hallucinations. We develop an automated hallucination annotation tool, DreamCatcher, which merges knowledge probing and consistency checking methods to rank factual preference data. Using knowledge preference as reward, We propose a Reinforcement Learning from Knowledge Feedback (RLKF) training framework, leveraging reinforcement learning to enhance the factuality and honesty of LLMs. Our experiments across multiple models show that RLKF training effectively enhances the ability of models to utilize their internal knowledge state, boosting performance in a variety of knowledge-based and honesty-related tasks",
    "checked": true,
    "id": "2751de08d6dbec07f53808231c016e96b075b06c",
    "semantic_title": "learning to trust your feelings: leveraging self-awareness in llms for hallucination mitigation",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=FtV9Ljf61st": {
    "title": "FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models",
    "volume": "review",
    "abstract": "We introduce FinTral, a suite of state-of-the-art multimodal large language models (LLMs) built upon the Mistral-7b model and tailoredfor financial analysis. FinTral integrates textual, numerical, tabular, and image data. We enhance FinTral with domain-specific pretraining, instruction fine-tuning, and RLAIF training by exploiting a large collection of textual and visual datasets we curate for this work. Wealso introduce an extensive benchmark featuring nine tasks and 25 datasets for evaluation, including hallucinations in the financial domain. Our FinTral model trained with direct preference optimization employing advanced Tools and Retrieval methods, dubbed FinTral-DPO-T&R, demonstrates an exceptional zero-shot performance. It outperforms ChatGPT-3.5 in all tasks and surpasses GPT-4 in five out of nine tasks, marking a significant advancement in AI-driven financial technology. We also demonstrate that FinTral has the potential to excel in real-time analysis and decision-making in diverse financial contexts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S3cFhsaxJ1t": {
    "title": "Visual Hallucinations of Multi-modal Large Language Models",
    "volume": "review",
    "abstract": "Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines incorrect details about an image in visual question answering. Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances. In this work, we propose a tool called VHTest to generate a diverse set of VH instances. Specifically, VHTest finds some initial VH instances in existing image datasets (e.g., COCO), generates a text description for each VH mode, and uses a text-to-image model (e.g., DALL-E-3) to generate VH images based on the text descriptions. We collect a benchmark dataset with 1,200 VH instances in 8 VH modes using VHTest. We find that existing MLLMs such as GPT-4, LLaVA-1.5, and MiniGPT-v2 hallucinate for a large fraction of the instances in our benchmark. Moreover, we find that fine-tuning an MLLM using our benchmark dataset reduces its likelihood to hallucinate without sacrificing its performance on other benchmarks. Our benchmarks are available at anonymous link: https://github.com/PrimaveralScientist/VHTest",
    "checked": true,
    "id": "4d7c68ec1a86ef5d187e7edb2f0ad63adddc8ea2",
    "semantic_title": "visual hallucinations of multi-modal large language models",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=cdduSvRPSZH": {
    "title": "SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language Model Pre-training",
    "volume": "review",
    "abstract": "The effectiveness of large language models (LLMs) is often hindered by duplicated data in their extensive pre-training datasets. Current approaches primarily focus on detecting and removing duplicates, which risks the loss of valuable information and neglects the varying degrees of duplication. To address this, we propose a soft deduplication method that maintains dataset integrity while selectively reducing the sampling weight of data with high commonness. Central to our approach is the concept of \"data commonness\", a metric we introduce to quantify the degree of duplication by measuring the occurrence probabilities of samples using an n-gram model. Empirical analysis shows that this method significantly improves training efficiency, achieving comparable perplexity scores with at least a 26\\% reduction in required training steps. Additionally, it enhances average few-shot downstream accuracy by 1.77\\% when trained for an equivalent duration. Importantly, this approach consistently improves performance, even on rigorously deduplicated datasets, indicating its potential to complement existing methods and become a standard pre-training process for LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YPp22-laE1": {
    "title": "UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset",
    "volume": "review",
    "abstract": "Open-source large language models (LLMs) have gained significant strength across diverse fields. Nevertheless, the majority of studies primarily concentrate on English, with only limited exploration into the realm of multilingual abilities. In this work, we therefore construct an open-source multilingual supervised fine-tuning dataset. Different from previous works that simply translate English instructions, we consider both the language-specific and language-agnostic abilities of LLMs. Firstly, we introduce a knowledge-grounded data augmentation approach to elicit more language-specific knowledge of LLMs, improving their ability to serve users from different countries. Moreover, we find modern LLMs possess strong cross-lingual transfer capabilities, thus repeatedly learning identical content in various languages is not necessary. Consequently, we can substantially prune the language-agnostic supervised fine-tuning (SFT) data without any performance degradation, making multilingual SFT more efficient. The resulting UltraLink dataset comprises approximately 1 million samples across five languages (i.e., En, Zh, Ru, Fr, Es), and the proposed data construction method can be easily extended to other languages. UltraLink-LM, which is trained on UltraLink, outperforms several representative baselines across many tasks",
    "checked": true,
    "id": "c21022a4dca7b1a32d2f79818cb7e51df712372f",
    "semantic_title": "ultralink: an open-source knowledge-enhanced multilingual supervised fine-tuning dataset",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VIx3h3jHKsm": {
    "title": "BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment",
    "volume": "review",
    "abstract": "The emergence of large language models (LLMs) has sparked significant interest in extending their remarkable language capabilities to speech. However, modality alignment between speech and text remains an open problem. Current solutions can be categorized into cascaded approaches, which limit the interaction between speech and LLMs, and end-to-end approaches that rely on scarce speech instruction data. In this paper, we propose the BLSP approach that Bootstraps Language-Speech Pre-training via behavior alignment, leveraging existing ASR training data. We achieve this by developing a lightweight modality adapter between a frozen speech encoder and an LLM, optimized to ensure that the LLM exhibits the same generation behavior irrespective of the modality of input: a speech segment or its transcript. We primarily focus on the continuation writing behavior as it closely resembles next-token prediction in a broad sense but also found that introducing other behaviors could lead to improved performance. We demonstrate that this simple process can extend the capabilities of LLMs to speech and achieve competitive performance compared to cascaded systems, enabling speech recognition, speech translation, spoken language understanding, and speech conversation, even in zero-shot cross-lingual scenarios",
    "checked": false,
    "id": "204fd6c5e247c477d607f507ee01d94a8dbd408f",
    "semantic_title": "blsp: bootstrapping language-speech pre-training via behavior alignment of continuation writing",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=G2-13qR2t1s": {
    "title": "Pouring Your Heart Out: Investigating the Role of Figurative Language in Online Expressions of Empathy",
    "volume": "review",
    "abstract": "Empathy is a social mechanism used to support and strengthen emotional connection with others, including in online communities. However, little is currently known about the nature of these online expressions, nor the specific factors that may lead to their improved detection. In this work, we study the role of a specific and complex subcategory of linguistic phenomena, figurative language, in online expressions of empathy. Our extensive experiments reveal that incorporating features regarding the use of metaphor, idiom, and hyperbole into empathy detection models improves their performance, resulting in impressive maximum F1 scores of 0.942 and 0.809 for identifying posts without and with empathy, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j31hGghgNz1": {
    "title": "Cultural Diversity Enhances Offensive Language Detection in Multilingual Models",
    "volume": "review",
    "abstract": "The proliferation of offensive online content across diverse languages necessitates culturally-aware NLP solutions. While Cross-Lingual Transfer Learning (CLTL) shows promise in other NLP tasks, its application to offensive language detection overlooks crucial cultural nuances in how offensiveness is perceived. This work investigates the effectiveness of CLTL for offensive language detection, considering both linguistic and cultural factors. Specifically, we investigated transfer learning across 105 language pairs, and uncovered several key findings. Firstly, training exclusively on English data impedes performance in certain target languages. Secondly, linguistic proximity between languages does not have a significant impact on transferability. Lastly, there is a significant correlation between cultural distance and performance. Importantly, for each unit increase of cultural distance, there was an increase of 0.31 in the AUC. These findings emphasize the limitations of English-centric approaches and highlight the need to integrate cultural context into NLP solutions for offensive language detection",
    "checked": false,
    "id": "77887aec8070896955840a1e78c27323df121bab",
    "semantic_title": "resources for multilingual hate speech detection",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=rsUMm51sCa": {
    "title": "Learning or Self-aligning? Rethinking Instruction Fine-tuning",
    "volume": "review",
    "abstract": "Instruction Fine-tuning~(IFT) is a critical phase in building Large Language Models~(LLMs). Previous works mainly focus on the IFT's role in the transfer of behavioral norms and the learning of additional world knowledge. However, the understanding of the underlying mechanisms of IFT remains significantly limited. In this paper, we design a knowledge intervention framework to decouple the potential underlying factors of IFT, thereby enabling individual analysis of different factors.Surprisingly, our experiments reveal that attempting to learn additional world knowledge through IFT often struggles to yield positive impacts and can even lead to markedly negative effects. Further, we discover that maintaining internal knowledge consistency before and after IFT is a critical factor for achieving successful IFT. Our findings reveal the underlying mechanisms of IFT and provide robust support for some very recent and potential future works",
    "checked": true,
    "id": "3bdd3d56ef9054aba47f83879b531a4842640295",
    "semantic_title": "learning or self-aligning? rethinking instruction fine-tuning",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=Pq40JUR6Pt": {
    "title": "Conversational Question Answering with Language Models Generated Reformulations over Knowledge Graph",
    "volume": "review",
    "abstract": "Conversational question answering (ConvQA) over knowledge graphs (KGs) involves answering multi-turn natural language questions about information contained in a KG. State-of-the-art methods of ConvQA often struggle with inexplicit question-answer pairs. These inputs are easy for human beings to understand given a conversation history, but hard for a machine to interpret, which can degrade ConvQA performance. To address this problem, we propose a reinforcement learning (RL) based model, CoRnNet, which utilizes question reformulations generated by large language models (LLMs) to improve ConvQA performance. CoRnNet adopts a teacher-student architecture where a teacher model learns question representations using human writing reformulations, and a student model to mimic the teacher model's output via reformulations generated by LLMs. The learned question representation is then used by a RL model to locate the correct answer in a KG. Extensive experimental results show that CoRnNet outperforms state-of-the-art ConvQA models",
    "checked": false,
    "id": "3caf3525c42196a579dfcd649d4acdd4cc36bb89",
    "semantic_title": "conversational question answering with reformulations over knowledge graph",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=JnC0-zJ2AM": {
    "title": "Impact of Shared Tokens on Multilingual Text Retrieval's Cross-Lingual Transfer Efficiency",
    "volume": "review",
    "abstract": "While great progress has been made on multilingual text retrieval in recent years, there has been few works in exploring the influential factors behind the transfer effect on multilingual text retrieval. In this work, we study the impact of shared tokens between the source and target languages to the transferring ability on multilingual text retrieval. Via extensive experiments on MIRACL, we find that the impact of shared tokens greatly depends on whether the language is whitespace-delimited, whereas not strongly associated with the number of shared tokens between the training and target language. Our analysis further suggests the impact of shared tokens may be independent of other potential influential factors in the cross-lingual transfer of multilingual retrieval",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zt78MtOz605": {
    "title": "R-Judge: Benchmarking Safety Risk Awareness for LLM Agents",
    "volume": "review",
    "abstract": "Large language models (LLMs) have exhibited great potential in autonomously completing tasks across real-world applications. Despite this, these LLM agents introduce unexpected safety risks when operating in interactive environments. Instead of centering on LLM-generated content safety in most prior studies, this work addresses the imperative need for benchmarking the behavioral safety of LLM agents within diverse environments. We introduce R-Judge, a benchmark crafted to evaluate the proficiency of LLMs in judging and identifying safety risks given agent interaction records. R-Judge comprises 162 records of multi-turn agent interaction, encompassing 27 key risk scenarios among 7 application categories and 10 risk types. It incorporates human consensus on safety with annotated safety labels and high-quality risk descriptions. Evaluation of 9 LLMs on R-Judge shows considerable room for enhancing the risk awareness of LLMs: The best-performing model, GPT-4, achieves 72.52% in contrast to the human score of 89.07%, while all other models score less than the random. Moreover, further experiments demonstrate that leveraging risk descriptions as environment feedback achieves substantial performance gains. With case studies, we reveal that correlated to parameter amount, risk awareness in open agent scenarios is a multi-dimensional capability involving knowledge and reasoning, thus challenging for current LLMs. R-Judge is publicly available at Anonymous",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KCmqgKSaha": {
    "title": "SpeechGuard: Exploring the Adversarial Robustness of Multi-modal Large Language Models",
    "volume": "review",
    "abstract": "Integrated Speech and Large Language Models (SLMs) that can follow speech instructions and generate relevant text responses have gained popularity lately. However, the safety and robustness of these models remains largely unclear. In this work, we investigate the potential vulnerabilities of such instruction-following speech-language models to adversarial attacks and jailbreaking. Specifically, we design algorithms that can generate adversarial examples to jailbreak SLMs in both white-box and black-box attack settings without human involvement. Additionally, we propose countermeasures to thwart such jailbreaking attacks. Our models, trained on dialog data with speech instructions, achieve state-of-the-art performance on spoken question-answering task, scoring over 80% on both safety and helpfulness metrics. Despite safety guardrails, experiments on jailbreaking demonstrate the vulnerability of SLMs to adversarial perturbations and transfer attacks, with average attack success rates of 90% and 10% respectively when evaluated on a dataset of carefully designed harmful questions spanning 12 different toxic categories. However, we demonstrate that our proposed countermeasures reduce the attack success significantly",
    "checked": false,
    "id": "315a2809cb2d26b55c4706a9bad023b688a43a2a",
    "semantic_title": "plug and pray: exploiting off-the-shelf components of multi-modal models",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=3VOvUT8zjxL": {
    "title": "The Da Vinci Code of Large Pre-trained Language Models: Deciphering Degenerate Knowledge Neurons",
    "volume": "review",
    "abstract": "This study explores the mechanism of factual knowledge storage in pre-trained language models (PLMs). Previous research suggests that factual knowledge is stored within multi-layer perceptron weights, and some storage units exhibit degeneracy, referred to as Degenerate Knowledge Neurons (DKNs). This paper provides a comprehensive definition of DKNs that covers both structural and functional aspects, pioneering the study of structures in PLMs' factual knowledge storage units. Based on this, we introduce the \\textit{Neurological Topology Clustering} method, which allows the formation of DKNs in any numbers and structures, leading to a more accurate DKN acquisition. Furthermore, we introduce the \\textit{Neuro-Degeneracy Analytic Analysis Framework}, which uniquely integrates model robustness, evolvability, and complexity for a holistic assessment of PLMs. Within this framework, our execution of 34 experiments across 2 PLMs, 4 datasets, and 6 settings highlights the critical role of DKNs. The code will be available soon",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D7GBFrydfW": {
    "title": "Assessing Large Pre-trained Models for Sign Language Processing: Is Text-Only Superior to Multimodal?",
    "volume": "review",
    "abstract": "Motivated by the recent success of text-only modeling in certain vision-language tasks, this paper proposes that sign language processing can also use (large) text-only language models for inference, freeing sign language models from the necessity of low-resource multimodal learning from scratch. To compare the performance of pre-trained text-only models against multimodal ones, we introduce the first text-only and multimodal large (7B) language models to be pre-trained and then fine-tuned on a sign language recognition task. We propose new prompting strategies and fine-tuning strategies for text-only signed language processing, incorporating both linguistics of signed languages and theoretically motivated strategies to mitigate catastrophic forgetting (of spoken language). We test the generalization of these models to other sign language recognition and generation tasks, showing text-only models are capable sign language models that are still adept at spoken language tasks and, by changing the prompt, can even generalize to new prosodic and iconic sign recognition tasks. Finally, we analyze trade-offs between our text-only and multimodal models. Our code and model checkpoints will be open-source",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OH1dxHcIGe": {
    "title": "CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models",
    "volume": "review",
    "abstract": "Recent years have witnessed a significant increase in the performance of Vision and Language tasks. Foundational Vision-Language Models (VLMs), such as CLIP, have been leveraged in multiple settings and demonstrated remarkable performance across several tasks. Such models excel at object-centric recognition yet learn text representations that seem invariant to word order, failing to compose known concepts in novel ways. However, no evidence exists that any VLM, including large-scale single-stream models such as GPT-4V, identifies compositions successfully. In this paper, we introduce a framework to significantly improve the ability of existing models to encode compositional language, with over 10% absolute improvement on standard benchmarks, while maintaining the performance on other standard object-recognition or text-to-image/image-to-text retrieval benchmarks",
    "checked": true,
    "id": "e9a3ecd05c34824aa9d94997a21b5222431baf48",
    "semantic_title": "clove: encoding compositional language in contrastive vision-language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=R0LQE4MI5MO": {
    "title": "Knowledge-Enhanced Academic Chatbot: Harnessing Large Language Models and Knowledge Graphs",
    "volume": "review",
    "abstract": "In this paper, we introduce an academic chatbot designed to help identify relevant publications, authors, and affiliations in academia and applied innovation. Leveraging similarity thresholds and query transformations, the chatbot delivers answers that are informed by an academic knowledge base. We address the challenges of efficiency, reproducibility, and interpretability through a combination of rule-based solutions and large language models backed by knowledge-graph embeddings. We have also critically analyzed the output of our chatbot and discussed various future directions of improvement",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=furq-f5vxo": {
    "title": "Eagle: Ethical Dataset Given from Real Interactions",
    "volume": "review",
    "abstract": "Recent studies have demonstrated that large language models (LLMs) have ethical-related problems such as social biases, lack of moral reasoning, and generation of offensive content.The existing evaluation metrics and methods to address these ethical challenges use datasets intentionally created by instructing humans to create instances including ethical problems.Therefore, the data does not reflect prompts that users actually provide when utilizing LLM services in everyday contexts.This may not lead to the development of safe LLMs that can address ethical challenges arising in real-world applications.In this paper, we create \\textbf{Eagle}\\footnote{An anonymised copy of the Eagle dataset is uploaded to ARR, and will be made public upon paper acceptance.} datasets extracted from real interactions between ChatGPT and users that exhibit social biases, toxicity, and immoral problems.Our experiments show that Eagle captures complementary aspects, not covered by existing datasets proposed for evaluation and mitigation of such ethical challenges",
    "checked": true,
    "id": "d05fc0f077383dd6aa09d86a9360cf4ab2cb483b",
    "semantic_title": "eagle: ethical dataset given from real interactions",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=q7-e5PTGCJ6": {
    "title": "MEEL: Multi-Modal Event Evolution Learning",
    "volume": "review",
    "abstract": "Multi-modal Event Reasoning (MMER) endeavors to endow machines with the ability to comprehend intricate event relations across diverse data modalities. MMER underlies a wide broad of applications. Despite extensive instruction fine-tuning, current multi-modal large language models still fall short in such ability. The disparity stems from that events exhibit underlying principles governing their evolution and development. Existing training methods are insufficient to capture event evolution mechanisms in various scenarios. In this paper, we introduce Multi-Modal Event Evolution Learning (MEEL) to enable the model to grasp the entire event evolution mechanism yielding advanced MMER ability. Specifically, we commence with the design of event diversification to gather seed events from a rich spectrum of scenarios. Subsequently, we employ ChatGPT to generate evolving graphs for these seed events. We propose the instruction encapsulation process which adapts the evolving graphs into instruction-tuning data, enhancing the model's training. To further improve the evolution process, we implement the guiding discrimination paradigm, in which we design various event negative mining strategies and train the model in precise event classification. Extensive experiments on nine downstream datasets validate the effectiveness of our approach, showcasing state-of-the-art performance in open-source multi-modal LLMs",
    "checked": true,
    "id": "a1677ca8f7db7deceeae70eec0e8a02287cc528b",
    "semantic_title": "meel: multi-modal event evolution learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=0y4OhNe7--3": {
    "title": "Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding",
    "volume": "review",
    "abstract": "The digital landscape is rapidly evolving with an ever-increasing volume of online news, emphasizing the need for swift and precise analysis of complex events.We refer to the complex events composed of many news articles over an extended period as Temporal Complex Event (TCE). This paper proposes a novel approach using Large Language Models (LLMs) to systematically extract and analyze the event chain within TCE, characterized by their key points and timestamps. We establish a benchmark, named TCELongBench, to evaluate the proficiency of LLMs in handling temporal dynamics and understanding extensive text. This benchmark encompasses three distinct tasks - reading comprehension, temporal sequencing, and future event forecasting. In the experiment, we leverage retrieval-augmented generation (RAG) method and LLMs with long context window to deal with lengthy news articles of TCE. Our findings indicate that models with suitable retrievers exhibit comparable performance with those utilizing long context window",
    "checked": true,
    "id": "5aad568d4f02b09af3c282b1f4c20ee0993bc2e6",
    "semantic_title": "analyzing temporal complex events with large language models? a benchmark towards temporal, long context understanding",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vcRijiYvhK": {
    "title": "Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large Language Model",
    "volume": "review",
    "abstract": "Integrating large language models (LLMs) into healthcare holds great potential but faces challenges. Pre-training LLMs from scratch for domains like medicine is resource-heavy and often unfeasible. On the other hand, sole reliance on Supervised Fine-tuning (SFT) can result in overconfident predictions and may not tap into domain-specific insights. In response, we present a multi-stage training method combining Domain-specific Continued Pre-training (DCPT), SFT, and Direct Preference Optimization (DPO). In addition, we publish a 3Gb Chinese Medicine (ChiMed) dataset, encompassing medical question answering, plain texts, knowledge graphs, and dialogues, segmented into three training stages. The medical LLM trained with our pipeline, Qilin-Med, shows substantial performance improvement. In the CPT and SFT phases, Qilin-Med achieved 38.4% and 40.0% accuracy on the CMExam test set, respectively. It outperformed the basemodel Baichuan-7B (accuracy: 33.5%), by 7.5%. In the DPO phase, it scored 16.66 in BLEU-1 and 27.44 in ROUGE-1 on the Huatuo-26M test set, bringing further improvement to the SFT phase (12.69 in BLEU-1 and 24.21 in ROUGE-1). Additionally, we have further enhanced the model's performance through the Retrieval Augmented Generation (RAG) approach. Experiments demonstrate that Qilin-Med-RAG achieves an accuracy rate of 42.8% on CMExam. These results highlight the contribution of our novel training approach in building LLMs for medical applications",
    "checked": true,
    "id": "8e6c4425e48b09d64827c64d8de0008f41f9be54",
    "semantic_title": "qilin-med: multi-stage knowledge injection advanced medical large language model",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=DlL_ofcuP5": {
    "title": "On the Empirical Complexity of Reasoning and Planning in LLMs",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) work surprisingly well for some complex reasoning problems via chain-of-thought (CoT) or tree-of-thought (ToT), but the underlying reasons remain unclear. We seek to understand the performance of these methods by conducting experimental case studies and linking the outcomes to sample and computational complexity in machine learning. We found that if problems can be decomposed into a sequence of reasoning steps and learning to predict the next step has a low sample and computational complexity, explicitly outlining the reasoning chain with all necessary information for predicting the next step may improve performance. Conversely, for problems where predicting the next step is computationally hard, adopting ToT may yield better reasoning outcomes than attempting to formulate a short reasoning chain",
    "checked": true,
    "id": "cb4ce9d960c0c91aa72b23fdf4b4de27dc0bd1db",
    "semantic_title": "on the empirical complexity of reasoning and planning in llms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-AneACwtYvc": {
    "title": "TOOLVERIFIER: Generalization to New Tools via Self-Verification",
    "volume": "review",
    "abstract": "Teaching language models to use tools is an important milestone towards building general assistants, but remains an open problem. While there has been significant progress on learning to use specific tools via fine-tuning, language models still struggle with learning how to robustly use new tools from only a few demonstrations. In this work we introduce a self-verification method which distinguishes between close candidates by self-asking contrastive questions during (1) tool selection; and parameter generation. We construct synthetic, high-quality, self-generated data for this goal using Llama-2 70B, which we intend to release publicly. Extensive experiments on 4 tasks from the ToolBench benchmark, consisting of 17 unseen tools, demonstrate an average improvement of 22% over few-shot baselines, even in scenarios where the distinctions between candidate tools are finely nuanced",
    "checked": true,
    "id": "57cbb0578326ed792d03981ca701214844462d22",
    "semantic_title": "toolverifier: generalization to new tools via self-verification",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=Q8qT3kVCzox": {
    "title": "Trustworthy Source Extraction via Topic-Related Named Entity Recognition and Higher-Order Singular Value Decomposition",
    "volume": "review",
    "abstract": "Understanding the trustworthiness of sources is paramount in the domains of text retrieval, generation, and numerous other tasks, particularly in the context of social media where unverified information can have detrimental effects. Solely relying on semantic analysis proves insufficient for assessing source credibility, underscoring the necessity for advanced information extraction techniques and cross-validation across diverse sources. In this study, we present an elegant yet efficient model to analyze the intricate network of source relationships and key information attributes. Utilizing advanced named entity recognition specialized in key information extraction, we methodically distill critical data to assemble a multidimensional tensor. This tensor comprehensively represents the intricate web of interactions among sources and the essential informational elements they convey. Application of Higher-Order Singular Value Decomposition (HOSVD) on this tensor enables us to distill eigen tensors, representing the principal components of information. These components are then weighted according to their corresponding eigenvalues, facilitating a nuanced quantification of each source's contribution to the collective information structure. Our theoretical analysis, complemented by empirical experiments on datasets from Twitter and various news categories, demonstrates the effectiveness of our approach. The results signify an innovative discovery in modeling and uncovering the deep connections among information sources, paving the way for future research in trustworthy information dissemination and retrieval. The code and dataset utilized in our study are provided in the supplementary materials",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=90hb47uF-a9": {
    "title": "COKE: A Cognitive Knowledge Graph for Machine Theory of Mind",
    "volume": "review",
    "abstract": "Theory of mind (ToM) refers to humans' ability to understand and infer the desires, beliefs, and intentions of others. The acquisition of ToM plays a key role in humans' social cognition and interpersonal relations. Though indispensable for social intelligence, ToM is still lacking for modern AI and NLP systems since they cannot access the human mental state and cognitive process beneath the training corpus. To empower AI systems with the ToM ability and narrow the gap between them and humans, in this paper, we propose COKE: the first cognitive knowledge graph for machine theory of mind. Specifically, COKE formalizes ToM as a collection of 45k+ manually verified cognitive chains that characterize human mental activities and subsequent behavioral/affective responses when facing specific social circumstances. In addition, we further generalize COKE using LLMs and build a powerful generation model COLM tailored for cognitive reasoning. Experimental results in both automatic and human evaluation demonstrate the high quality of COKE, the superior ToM ability of COLM, and its potential to significantly enhance social applications",
    "checked": true,
    "id": "83fe73b5f35ab77444b80e2bf6fbd66b55531ad1",
    "semantic_title": "coke: a cognitive knowledge graph for machine theory of mind",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=eITEEzVokPQ": {
    "title": "OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech Recognition, Translation, and Language Identification",
    "volume": "review",
    "abstract": "There has been an increasing interest in large speech models that can perform multiple speech processing tasks in a single model. Such models usually adopt the encoder-decoder or decoder-only architecture due to their popularity and good performance in many domains. However, autoregressive models can be slower during inference compared to non-autoregressive models and also have potential risks of hallucination. Though prior studies observed promising results of non-autoregressive models for certain tasks at small scales, it remains unclear if they can be scaled to speech-to-text generation in diverse languages and tasks. Inspired by the Open Whisper-style Speech Model (OWSM) project, we propose OWSM-CTC, a novel encoder-only speech foundation model based on Connectionist Temporal Classification (CTC). It is trained on 180k hours of public audio data for multilingual automatic speech recognition (ASR), speech translation (ST), and language identification (LID). Compared to encoder-decoder OWSM, our OWSM-CTC achieves competitive results on ASR and up to 25% relative improvement on ST, while it is more robust and 3 to 4 times faster for inference. OWSM-CTC also improves the long-form ASR result with 20x speed-up. We will publicly release our codebase, pre-trained model, and training logs to promote open science in speech foundation models",
    "checked": true,
    "id": "7d289ac336ef671e13909060a1cfa88065cba25e",
    "semantic_title": "owsm-ctc: an open encoder-only speech foundation model for speech recognition, translation, and language identification",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=Pj2irXVqLPl": {
    "title": "SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models",
    "volume": "review",
    "abstract": "The continual learning (CL) ability is vital for deploying large language models (LLMs) in the dynamic world. Existing methods devise the learning module to acquire task-specific knowledge with parameter-efficient tuning (PET) block and the selection module to pick out the corresponding one for the testing input, aiming at handling the challenges of catastrophic forgetting and knowledge transfer in CL. However, these methods tend to address only one of the challenges, ignoring the potential of aligning the two module to effectively address catastrophic forgetting and knowledge transfer simultaneously. To this end, we propose a novel Shared Attention Framework (SAPT), to align the PET learning and selection via the Shared Attentive Learning \\& Selection module. Extensive Experiments on two CL benchmarks demonstrate the superiority of SAPT. Moreover, SAPT consistently demonstrates its superiority when we scale it to different model sizes (from 770M to 13B), different model architectures (T5 and LLaMA-2) and unseen tasks.\\footnote{Our data and codes could be found in supplementary files.}",
    "checked": true,
    "id": "da217117909eb5ef310160795271f88bf606f0ab",
    "semantic_title": "sapt: a shared attention framework for parameter-efficient continual learning of large language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=2I2mlrP6uv": {
    "title": "The Generation Gap: Exploring Age Bias in Large Language Models",
    "volume": "review",
    "abstract": "In this paper, we explore the alignment of values in Large Language Models (LLMs) with specific age groups, leveraging data from the World Value Survey across thirteen categories. Through a diverse set of prompts tailored to ensure response robustness, we find a general inclination of LLM values towards younger demographics. Additionally, we explore the impact of incorporating age identity information in prompts and observe challenges in mitigating value discrepancies with different age cohorts. Our findings highlight the age bias in LLMs and provide insights for future work",
    "checked": true,
    "id": "17581516598091f876250deee560ae32fda7a279",
    "semantic_title": "the generation gap: exploring age bias in large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=HCye5HWJLoC": {
    "title": "Exploring the Value Alignment of Large Language Models Across Different Demographic Groups",
    "volume": "review",
    "abstract": "Although large language models (LLMs) are deployed in numerous applications and interact with a wide variety of demographics, they have also been found to exhibit biases toward the values of Western and rich populations. In this paper, we conduct a systematic analysis by prompting a variety of models on different categories of value questions. We quantitatively calculate how different LLMs align with different demographic groups based on their geographic locations and income levels. Our results show that the demographic preferences can vary across different models, and not all models are biased towards the same demographics",
    "checked": false,
    "id": "dd99a32488bd6a9948a997fba89026931ee137b3",
    "semantic_title": "the generation gap:exploring age bias in the underlying value systems of large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=YZNxoj1Rm_": {
    "title": "Controlled Text Generation for Large Language Model with Dynamic Attribute Graphs",
    "volume": "review",
    "abstract": "Controlled Text Generation (CTG) aims to produce texts that exhibit specific desired attributes. In this study, we introduce a pluggable CTG framework for Large Language Models (LLMs) named Dynamic Attribute Graphs-based controlled text generation (DATG). This framework utilizes an attribute scorer to evaluate the attributes of sentences generated by LLMs and constructs dynamic attribute graphs. DATG modulates the occurrence of key attribute words and key anti-attribute words, achieving effective attribute control without compromising the original capabilities of the model. We conduct experiments across four datasets in two tasks: toxicity mitigation and sentiment transformation, employing five LLMs as foundational models. Our findings highlight a remarkable enhancement in control accuracy, achieving a peak improvement of 19.29% over baseline methods in the most favorable task across four datasets. Additionally, we observe a significant decrease in perplexity, markedly improving text fluency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rMTMRQ0wlU": {
    "title": "$C^{3}$: Confidence Calibration Model Cascade for Inference-Efficient Cross-Lingual Natural Language Understanding",
    "volume": "review",
    "abstract": "Cross-lingual natural language understanding (NLU) is a critical task in natural language processing (NLP). Recent advancements have seen multilingual pre-trained language models (mPLMs) significantly enhance the performance of these tasks. However, mPLMs necessitate substantial resources and incur high computational costs during inference, posing challenges for deployment in real-world and real-time systems. Existing model cascade methods seek to enhance inference efficiency by greedily selecting the lightest model capable of processing the current input from a variety of models, based on model confidence scores. Nonetheless, deep models tend to exhibit overconfidence, and confidence distributions vary across languages. This leads to the emission of confident but incorrect predictions by smaller models, hindering their ability to generalize effectively across test languages. In this study, we introduce a confidence calibration model cascade ($C^{3}$) method. This approach, simple yet effective, involves calibration prior to cascade inference, thereby enhancing cascade accuracy through more reliable predictions. Extensive experiments conducted on three cross-lingual benchmarks demonstrate that $C^{3}$ significantly outperforms all state-of-the-art baselines",
    "checked": false,
    "id": "208d9e72a80c9333c36f8ede204128e3c808af84",
    "semantic_title": "c3: confidence calibration model cascade for inference-efficient cross-lingual natural language understanding",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qPCM6HT1Ty": {
    "title": "Long-Context Language Modeling with Parallel Context Encoding",
    "volume": "review",
    "abstract": "Extending large language models (LLMs) to process longer inputs is crucial for numerous applications. However, the considerable computational cost of transformers, coupled with limited generalization of positional encoding, restricts the size of their context window. We introduce Cross-Attention to Parallel Encodings (CAPE), a framework that can be applied to any existing decoder-only LLMs for context expansion. CAPE leverages a small encoder to process a long input chunk by chunk and enables the frozen decoder to cross-attend to the additional contexts. CAPE is efficient, generalizable, and versatile: trained with 8K-token documents, CAPE extends the context window of LLaMA-2 to 128K tokens, offering $10\\times$ of the throughput with only 1/6 of the memory. CAPE yields strong performance on language modeling and in-context learning. CAPE also excels in retrieval-augmented applications, while existing long-context models degenerate with retrieved contexts. We further introduce a CAPE variant that can extend the context window of instruction-tuned models with only unlabeled data, and showcase its effectiveness on LLaMA-2-Chat, leading to a strong instruction-following model that can leverage very long context on downstream tasks",
    "checked": true,
    "id": "2330035c7586a0dc0b1f09e9c00106b295acf543",
    "semantic_title": "long-context language modeling with parallel context encoding",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=Z2vEqZidXWp": {
    "title": "Revisiting the Knowledge Recall and Selection in Chinese Spelling Correction",
    "volume": "review",
    "abstract": "Chinese Spell Check (CSC) task is a challenging natural language processing task. We are currently facing a significant challenge as the improvement in performance is quite limited, it is primarily because the infusion of knowledge is limited, and the injection of knowledge occurs without explicit selection. Previous work involved confusion sets, but the size was small and was only used as additional feature input. To more effectively address the issue of knowledge infusion, we propose a knowledge recall and selection network (ReSC). First through four kinds of recall to achieve an average recall rate above 93%, with individual character recall of around 150 related characters/words. Subsequently, we proposed a Knowledge Selection Algorithm, choosing the appropriate characters or words from numerous recall sets. The knowledge selection network is highly efficient, as the classification accuracy has nearly reached 100%. Extensive experiments have proven that our method achieves SOTA results in six datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-IClpz8iwA": {
    "title": "CREDIT-SQL: Few-shot prompting for context-dependent text-to-SQL with regularized examples from diversity sampling",
    "volume": "review",
    "abstract": "In this paper, we propose a few-shot prompting method called CREDIT-SQL for the context-dependent text-to-SQL problem. CREDIT-SQL converts each question in a multi-turn dialogue into a self-contained question with a fixed few-shot prompt. Once a self-contained question is obtained, CREDIT-SQL converts it into an SQL query using a prompt made of in-context examples selected by diversity sampling and subsequent example voting. After experimentations with multiple LLMs, CREDIT-SQL achieves 58.6\\% in terms of the exact set match without values on the dev set of CoSQL, which is the performance comparable to the state-of-the-art models for context-dependent text-to-SQL. We also argue that the example voting we introduced in CREDIT-SQL can serve as an efficient and effective way to mitigate the instability of in-context example selection in general",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nUhsp8whpM": {
    "title": "Flex Tape Can't Fix That\": Bias and Misinformation in Edited Language Models",
    "volume": "review",
    "abstract": "Model editing has emerged as a cost-effective strategy to update knowledge stored in language models. However, model editing can have unintended consequences after edits are applied: information unrelated to the edits can also be changed, and other general behaviors of the model can be wrongly altered. In this work, we investigate how model editing methods unexpectedly amplify model biases post-edit. We introduce a novel benchmark dataset, Seesaw-CF, for measuring bias-related harms of model editing and conduct the first in-depth investigation of how different weight-editing methods impact model bias. Specifically, we focus on biases with respect to demographic attributes such as race, geographic origin, and gender, as well as qualitative flaws in long-form texts generated by edited language models. We find that edited models exhibit, to various degrees, more biased behavior as they become less confident in attributes for Asian, African, and South American subjects. Furthermore, edited models amplify sexism and xenophobia in text generations while remaining seemingly coherent and logical. Finally, editing facts about place of birth, country of citizenship, or gender have particularly negative effects on the model's knowledge about unrelated features like field of work",
    "checked": true,
    "id": "161c6d3851b73fc844c49a9921cfc6bb6da6904b",
    "semantic_title": "flex tape can't fix that\": bias and misinformation in edited language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AWeBlBQBJu": {
    "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 16 LLMs encompassing four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLer), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased Overlap (RBO) score to be 44%, indicating that machine preferences are misaligned with humans. According to our findings, LLMs may still be unable to be utilized for automatic annotation aligned with human preferences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P3RQMWbS9M": {
    "title": "EVER: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in generating fluent text. However, they often encounter the challenge of generating inaccurate or hallucinated content. This issue is common in both non-retrieval-based generation and retrieval-augmented generation approaches, and existing post-hoc rectification methods may not address the accumulated hallucination errors that may be caused by the \"snowballing\" issue, especially in reasoning tasks. To tackle these challenges, we introduce a novel approach called Real-time Verification and Rectification (EVER). Instead of waiting until the end of the generation process to rectify hallucinations, EVER employs a real-time, step-wise generation and hallucination rectification strategy. Apart from directly mitigating hallucination, we further demonstrate that both the EVER-rectified response and the original one can serve as preference data to enhance the factuality of the model through preference tuning. When compared to both retrieval-based and non-retrieval-based baselines, EVER demonstrates a significant improvement in generating trustworthy and factually accurate text across a diverse range of tasks, including biography generation and multi-hop reasoning",
    "checked": true,
    "id": "b10482ab3dd1d340c3c926d92c3e617c24ee3949",
    "semantic_title": "ever: mitigating hallucination in large language models through real-time verification and rectification",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=jE3gHS1IU9j": {
    "title": "ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models",
    "volume": "review",
    "abstract": "In our work, we explore the synergistic capabilities of pre-trained vision-and-language models (VLMs) and large language models (LLMs) on visual commonsense reasoning (VCR) problems. We find that VLMs and LLMs-based decision pipelines are good at different kinds of VCR problems. Pre-trained VLMs exhibit strong performance for problems involving understanding the literal visual content, which we noted as visual commonsense understanding (VCU). For problems where the goal is to infer conclusions beyond image content, which we noted as visual commonsense inference (VCI), VLMs face difficulties, while LLMs, given sufficient visual evidence, can use commonsense to infer the answer well. We empirically validate this by letting LLMs classify VCR problems into these two categories and show the significant difference between VLM and LLM with image caption decision pipelines on two subproblems. Moreover, we identify a challenge with VLMs' \\textit{passive} perception, which may miss crucial context information, leading to incorrect reasoning by LLMs. Based on these, we suggest a collaborative approach, named \\textbf{ViCor}, where pre-trained LLMs serve as problem classifiers to analyze the problem category, then either use VLMs to answer the question directly or \\textit{actively} instruct VLMs to concentrate on and gather relevant visual elements to support potential commonsense inferences. We evaluate our framework on two VCR benchmark datasets and outperform all other methods that do not require in-domain fine-tuning",
    "checked": true,
    "id": "33095b1334bed852e3652bd9d7da3f4df0cdf485",
    "semantic_title": "vicor: bridging visual understanding and commonsense reasoning with large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=v1oikbkzfkf": {
    "title": "WASA: WAtermark-based Source Attribution for Large Language Model-Generated Data",
    "volume": "review",
    "abstract": "The impressive performances of Large Language Models (LLMs) and their immense potential for commercialization have given rise to serious concerns over the Intellectual Property (IP) of their training data. In particular, the synthetic texts generated by LLMs may infringe the IP of the data being used to train the LLMs. To this end, it is imperative to be able to (a) identify the data provider who contributed to the generation of a synthetic text by an LLM (source attribution) and (b) verify whether the text data from a data provider has been used to train an LLM (data provenance). In this paper, we show that both problems can be solved by watermarking, i.e., by enabling an LLM to generate synthetic texts with embedded watermarks that contain information about their source(s). We identify the key properties of such watermarking frameworks (e.g., source attribution accuracy, robustness against adversaries), and propose a WAtermarking for Source Attribution (WASA) framework that satisfies these key properties due to our algorithmic designs. Our WASA framework enables an LLM to learn an accurate mapping from the texts of different data providers to their corresponding unique watermarks, which sets the foundation for effective source attribution (and hence data provenance). Extensive empirical evaluations show that our WASA framework achieves effective source attribution and data provenance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xW_rRnxqIlg": {
    "title": "BenchIEFL, A Manually Re-Annotated Fact-Based Open Information Extraction Benchmark",
    "volume": "review",
    "abstract": "Open Information Extraction (OIE) is a field of natural language processing that aims to present textual information in a format that allows it to be organized, analyzed and reflected upon. Numerous OIE systems are developed, claiming ever-increasing performance, marking the need for objective benchmarks. BenchIE is the latest reference we know of. Despite being very well thought, we noticed a number of issues we believe are limitative. Therefore, we propose BenchIEFL, a new OIE benchmark which fully enforces principles of BenchIE while containing less errors, omissions and shortcomings when candidate facts are matched towards reference ones. BenchIEFL allows to draw some insightful conclusions on the actual performance of OIE extractors",
    "checked": false,
    "id": "a286902a73490e77b26a5fece940542573448998",
    "semantic_title": "benchie fl , a manually re-annotated fact-based open information extraction benchmark",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJSmOT3cuU": {
    "title": "Teacher-Student Training for Debiasing: General Permutation Debiasing for Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated impressive zero-shot capabilities and versatility in NLP tasks, however they sometimes fail to maintain crucial invariances for specific tasks. One example is permutation sensitivity, where LLMs' outputs may significantly vary depending on the order of the input options. While debiasing techniques can mitigate these issues, and yield better performance and reliability, they often come with a high computational cost at inference. This paper addresses this inefficiency at inference time. The aim is to distill the capabilities of a computationally intensive, debiased, teacher model into a more compact student model. We explore two variants of student models: one based on pure distillation, and the other on an error-correction approach for more complex tasks, where the student corrects a single biased decision from the teacher to achieve a debiased output. Our approach is general and can be applied to both black-box and white-box LLMs. Furthermore, we demonstrate that our compact, encoder-only student models can outperform their larger, biased teacher counterparts, achieving better results with significantly fewer parameters",
    "checked": true,
    "id": "6c7b5b58185fd8b82fa268568d35ba01717a68bf",
    "semantic_title": "teacher-student training for debiasing: general permutation debiasing for large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=xuBWjGxhsr1": {
    "title": "WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual World Knowledge",
    "volume": "review",
    "abstract": "Sentiment analysis is rapidly advancing by utilizing various data modalities (e.g., text, image). However, most previous works relied on superficial information, neglecting the incorporation of contextual world knowledge (e.g., background information derived from but beyond the given image and text pairs) and thereby restricting their ability to achieve better multimodal sentiment analysis (MSA). In this paper, we proposed a plug-in framework named WisdoM, to leverage the contextual world knowledge induced from the large vision-language models (LVLMs) for enhanced MSA. WisdoM utilizes LVLMs to comprehensively analyze both images and corresponding texts, simultaneously generating pertinent context. To reduce the noise in the context, we also introduce a training-free contextual fusion mechanism. Experiments across diverse granularities of MSA tasks consistently demonstrate that our approach has substantial improvements (brings an average +1.96% F1 score among five advanced methods) over several state-of-the-art methods. The code will be released",
    "checked": true,
    "id": "fe91e1d22537bd514e9b642bb3b966f74fd62f9b",
    "semantic_title": "wisdom: improving multimodal sentiment analysis by fusing contextual world knowledge",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=i6ALdqUiiYd": {
    "title": "Retrieval-Augmented Generation: Is Dense Passage Retrieval Retrieving?",
    "volume": "review",
    "abstract": "Dense passage retrieval (DPR) is the first step in the retrieval augmented generation (RAG) paradigm for improving the performance of large language models (LLM). DPR fine-tunes pre-trained networks to enhance the alignment of the embeddings between queries and relevant textual data. A deeper understanding of DPR fine-tuning will be required to fundamentally unlock the full potential of this approach. In this work, we explore DPR-trained models mechanistically by using a combination of probing, layer activation analysis, and model editing. Our experiments show that DPR training decentralizes how knowledge is stored in the network, creating multiple access pathways to the same information. We also uncover a limitation in this training style: the internal knowledge of the pre-trained model bounds what the retrieval model can retrieve. These findings suggest a few possible directions for dense retrieval: (1) expose the DPR training process to more knowledge so more can be decentralized, (2) inject facts as decentralized representations, (3) model and incorporate knowledge uncertainty in the retrieval process, and (4) directly map internal model knowledge to a knowledge base",
    "checked": true,
    "id": "dfeaaf97e347b3317eba326ec8161fc6650fedc0",
    "semantic_title": "retrieval-augmented generation: is dense passage retrieval retrieving?",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=rnx2-N7aojy": {
    "title": "Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models",
    "volume": "review",
    "abstract": "Empathetic response generation is increasingly significant in AI, necessitating nuanced emotional and cognitive understanding coupled with articulate response expression. Current large language models (LLMs) excel in response expression; however, they lack the ability to deeply understand emotional and cognitive nuances, particularly in pinpointing fine-grained emotions and their triggers. Conversely, small-scale empathetic models (SEMs) offer strength in fine-grained emotion detection and detailed emotion cause identification.To harness the complementary strengths of both LLMs and SEMs, we introduce a Hybrid Empathetic Framework (HEF). HEF regards SEMs as flexible plugins to improve LLM's nuanced emotional and cognitive understanding. Regarding emotional understanding, HEF implements a two-stage emotion prediction strategy, encouraging LLMs to prioritize primary emotions emphasized by SEMs, followed by other categories, substantially alleviates the difficulties for LLMs in fine-grained emotion detection. Regarding cognitive understanding, HEF employs an emotion cause perception strategy, prompting LLMs to focus on crucial emotion-eliciting words identified by SEMs, thus boosting LLMs' capabilities in identifying emotion causes. This collaborative approach enables LLMs to discern emotions more precisely and formulate empathetic responses. We validate HEF on the Empathetic-Dialogue dataset, and the findings indicate that our framework enhances the refined understanding of LLMs and their ability to convey empathetic responses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CBSohZD84m": {
    "title": "Hierarchy-aware Biased Bound Margin Loss Function for Hierarchical Text Classification",
    "volume": "review",
    "abstract": "Hierarchical text classification (HTC) is a challenging problem with two key issues: utilizing structural information and mitigating label imbalance. Recently, the unit-based approach generating unit-based feature representations has outperformed the global approach focusing on a global feature representation. Nevertheless, unit-based models using BCE and ZLPR losses still face static thresholding and label imbalance challenges. Those challenges become more critical in large-scale hierarchies. This paper introduces a novel hierarchy-aware loss function for unit-based HTC models: Hierarchy-aware Biased Bound Margin (HBM) loss. HBM integrates learnable bounds, biases, and a margin to address static thresholding and mitigate label imbalance adaptively. Experimental results on benchmark datasets demonstrate the superior performance of HBM compared to competitive HTC models",
    "checked": false,
    "id": "96b9a6ba4ccfa89cef9c47b1741caf0d84c9f2d4",
    "semantic_title": "mining organization-related fine-grained information: a bias-corrected multi-turn question answering based approach",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U1oK8ad9EF": {
    "title": "EvEdit: Event-based Knowledge Editing with Anchored Editing Boundaries",
    "volume": "review",
    "abstract": "The dynamic nature of real-world information necessitates efficient knowledge editing (KE) in large language models (LLMs) for knowledge updating. However, current KE approaches, which typically operate on (subject, relation, object) triples, ignore the contextual information and the relation among different knowledge. Such editing methods could thus encounter an uncertain editing boundary, leaving a lot of relevant knowledge in ambiguity: Queries that could be answered pre-edit cannot be reliably answered afterward. In this work, we analyze this issue by introducing a theoretical framework for KE that highlights an overlooked set of knowledge that remains unchanged and aids in knowledge deduction during editing, which we name the deduction anchor. We further address this issue by proposing a novel task of event-based knowledge editing that pairs facts with event descriptions, implicitly determining the deduction anchor and providing a clear editing boundary deducted from the anchor. We empirically demonstrate the superiority of event-based editing over the existing setting on resolving uncertainty in edited models, and curate a new benchmark dataset EvEdit derived from the CounterFact dataset. Moreover, while we observe that event-based editing is significantly challenging for existing approaches, we propose a novel approach Self-Edit that showcases stronger performance, achieving 55.6\\% consistency improvement while maintaining the naturalness of generation",
    "checked": false,
    "id": "4315094bec44f310d662b349aff6305b3dde9a07",
    "semantic_title": "evedit: event-based knowledge editing with deductive editing boundaries",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=BgIqtXYNYXy": {
    "title": "Eva-KELLM: A New Benchmark for Evaluating Document-based Knowledge Editing of LLMs",
    "volume": "review",
    "abstract": "Since the knowledge of large language models (LLMs) may become outdated or contain inaccuracies, knowledge editing for LLMs and evaluating their effectiveness attract increasing attention. However, current knowledge editing methods often rely on manually annotated triples or question-answer pairs, limiting their applicability. In this paper, we explore a more general knowledge editing scenario where LLMs only use raw documents for editing. Given the absence of benchmarks for document-based knowledge editing, we propose a new benchmark Eva-KELLM, which includes raw documents for editing and corresponding test datasets evaluated from multiple perspectives. In addition to conventional evaluations assessing the model's memory of altered knowledge and retention of unrelated knowledge, we also evaluate the updated LLM's performance in reasoning with altered knowledge and cross-lingual knowledge transfer. Furthermore, we propose a document-based knowledge editing method aimed at addressing challenges associated with noise and unidirectional auto-regressive learning. Experimental results on the benchmark showcase the effectiveness of our method in achieving improved performance",
    "checked": false,
    "id": "b79f039c5ea5916d8e70b98e722de4ae1bc8fa05",
    "semantic_title": "eva-kellm: a new benchmark for evaluating knowledge editing of llms",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=Br4eCssvp2": {
    "title": "Exploring the Role of Reasoning Structures for Constructing Proofs in Multi-Step Natural Language Reasoning with Large Language Models",
    "volume": "review",
    "abstract": "When performing complex multi-step reasoning tasks, the ability of Large Language Models (LLMs) to derive structured intermediate proof steps is important for ensuring that the models truly perform the desired reasoning. This paper is centred around a focused study: whether the current state-of-the-art LLMs can leverage the structures in a few examples and benefit from them to construct the proof structures when performing complex natural language reasoning. Our study specifically focuses on structure-aware demonstration and structure-aware pruning. We demonstrate that both of them help improve performance. We provide a detailed analysis to help understand the results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V-ejDfLiwe": {
    "title": "ReactXT: Understanding Molecular \"Reaction-ship\" via Reaction-Contextualized Molecule-Text Pretraining",
    "volume": "review",
    "abstract": "Molecule-text modeling, which aims to facilitate molecule-relevant tasks with a textual interface and textual knowledge, is an emerging research direction. Beyond single molecules, studying reaction-text modeling holds promise for helping the synthesis of new materials and drugs. However, previous works mostly neglect reaction-text modeling: they primarily focus on modeling individual molecule-text pairs or learning chemical reactions without texts in context. Additionally, one key task of reaction-text modeling -- experimental procedure prediction -- is less explored due to the absence of an open-source dataset. The task is to predict step-by-step actions of conducting chemical experiments and is crucial to automating chemical synthesis. To resolve the challenges above, we propose a new pretraining method, ReactXT, for reaction-text modeling, and a new dataset, OpenExp, for experimental procedure prediction. Specifically, ReactXT features three types of input contexts to incrementally pretrain LMs. Each of the three input contexts corresponds to a pretraining task to improve the text-based understanding of either reactions or single molecules. ReactXT demonstrates consistent improvements in experimental procedure prediction and molecule captioning and offers competitive results in retrosynthesis. Our code is available at https://anonymous.4open.science/r/ReactXT",
    "checked": true,
    "id": "37e679adb49e227b55219c02c66f1bb9d7a82b75",
    "semantic_title": "reactxt: understanding molecular \"reaction-ship\" via reaction-contextualized molecule-text pretraining",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=gpTKFwPMl8": {
    "title": "Understanding News Thumbnail Representativeness by Counterfactual Text-Guided Contrastive Language-Image Pretraining",
    "volume": "review",
    "abstract": "This paper delves into the critical challenge of understanding the representativeness of news thumbnail images, which often serve as the first visual engagement for readers when an article is disseminated on social media. We focus on whether a news image represents the main subject discussed in the news text. To serve the challenge, we introduce NewsTT, a manually annotated dataset of news thumbnail image and text pairs. We found that pretrained vision and language models, such as CLIP and BLIP-2, struggle with this task. Since news subjects frequently involve named entities or proper nouns, a pretrained model could not have the ability to match its visual and textual appearances. To fill the gap, we propose CFT-CLIP, a counterfactual text-guided contrastive language-image pretraining framework. We hypothesize that learning to contrast news text with its counterfactual, of which named entities are replaced, can enhance the cross-modal matching ability in the target task. Evaluation experiments using NewsTT show that CFT-CLIP outperforms the pretrained models, such as CLIP and BLIP-2. Our code and data will be made accessible to the public after the paper is accepted",
    "checked": false,
    "id": "7a24afb0399285d1cb67b3f737ff8a485c368dc3",
    "semantic_title": "assessing news thumbnail representativeness: counterfactual text can enhance the cross-modal matching ability",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eAm5VZAOCP": {
    "title": "SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support",
    "volume": "review",
    "abstract": "Developing specialized dialogue systems for mental health support requires multi-turn conversation data, which has recently garnered increasing attention. However, gathering and releasing large-scale and real-life multi-turn conversations to facilitate advancements in mental health presents challenges due to data privacy protection, as well as the time and cost involved. To address the challenges related to data scarcity, we introduce smile, a single-turn to multi-turn inclusive language expansion technique that prompts ChatGPT to rewrite public single-turn dialogues into multi-turn ones. Our work begins with the analysis of language transformation, validating the feasibility of the proposed method when compared with other baseline methods. We then conduct a study on dialogue diversity, including lexical features, semantic features, and dialogue topics, demonstrating the effectiveness of our proposed method. Furthermore, we implement an expert evaluation and the results demonstrate that the dialogues generated with our proposed method are of higher quality than those generated with other baseline methods. Thus, we employ our method to generate a large-scale, diverse, and high-quality dialogue dataset named SmileChat, comprising 55,165 dialogues in total with an average of 10.4 turns per dialogue. Finally, we utilize the collected corpus to develop a mental health chatbot, MeChat. To better assess the overall quality of SmileChat, we collect a real-life chat dataset comprising 82 counseling dialogues for model evaluation. Both automatic and human evaluations demonstrate that our trained dialogue system exhibits significant improvements, showcasing that SmileChat is high-quality and practical",
    "checked": true,
    "id": "83bc360fa9874564cf8ccbd3a00e6e14dbe4e4e6",
    "semantic_title": "smile: single-turn to multi-turn inclusive language expansion via chatgpt for mental health support",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=Vf_oWNs7lf": {
    "title": "Instruction-tuned Language Models are Better Knowledge Learners",
    "volume": "review",
    "abstract": "In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data. The standard recipe for doing so involves continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs. However, we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized. We found that QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner. Therefore, we hypothesize that it is beneficial to expose LLMs to QA pairs before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions. Based on this, we propose pre-instruction-tuning (PIT), a method that instruction-tunes on questions prior to training on documents. This contrasts with standard instruction-tuning, which learns how to extract knowledge after training on documents. Extensive experiments and ablation studies demonstrate that PIT significantly enhances the ability of LLMs to absorb knowledge from new documents, outperforming standard instruction-tuning by 17.8%",
    "checked": true,
    "id": "1c0b3679919cd0531973fced1a1eb49745d9332d",
    "semantic_title": "instruction-tuned language models are better knowledge learners",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=hZMTTx4wT3": {
    "title": "Misinformation with Legal Consequences (MisLC): A New Task Towards Harnessing Societal Harm of Misinformation",
    "volume": "review",
    "abstract": "Misinformation, defined as false or inaccurate information, can result in significant societal harm when it is spread with malicious or even unintentional intents. The rapid online information exchange necessitates advanced detection mechanisms to mitigate misinformation-induced harm. Existing research predominantly focuses on the veracity of information, overlooking the legal implications and consequences of misinformation. In this paper, we take a novel angle to consolidate the definition of misinformation detection using legal issues as a measurement of societal ramifications, aiming to bring interdisciplinary efforts to tackle misinformation and its consequence. To enable the study, we introduce a two-stage annotation pipeline where we first identify checkworthiness with crowd-sourced workers, and then potential legal issues with legal annotators. Next, we build a Retrieval-Augmented Generation (RAG) pipeline to address the task with large language models (LLMs). We present empirical baseline results on our pipeline over seven open- and closed-source LLMs, with and without RAG. Our findings contribute to the interdisciplinary efforts to combat misinformation by leveraging legal perspectives",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jPlorsJCb7": {
    "title": "In-Memory Learning: A Declarative Learning Framework for Large Language Models",
    "volume": "review",
    "abstract": "The exploration of whether agents can align with their environment without relying on human-labeled data presents an intriguing research topic. Drawing inspiration from the alignment process observed in intelligent organisms, where declarative memory plays a pivotal role in summarizing past experiences, we propose a novel learning framework. The agents adeptly distill insights from past experiences, refining and updating existing notes to enhance their performance in the environment. This entire process transpires within the memory components and is implemented through natural language, so we character this framework as In-memory Learning. We also delve into the key features of benchmarks designed to evaluate the self-improvement process. Through systematic experiments, we demonstrate the effectiveness of our framework and provide insights into this problem",
    "checked": true,
    "id": "002a2f9da012bcf37e59c7ea0406f6bf11e5ab55",
    "semantic_title": "in-memory learning: a declarative learning framework for large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=kcOEeOX03vq": {
    "title": "FLoRA: Federated Fine-tuning Large Language Models with Heterogeneous Low-Rank Adaptations",
    "volume": "review",
    "abstract": "The rapid development of Large Language Models (LLMs) has been pivotal in advancing AI, with pre-trained LLMs being adaptable to diverse downstream tasks through fine-tuning. Federated learning (FL) further enhances fine-tuning in a privacy-aware manner by utilizing clients' local data through in-situ computation, eliminating the need for data movement. However, fine-tuning LLMs, given their massive scale of parameters, poses challenges for clients with constrained and heterogeneous resources in FL. Previous methods employed low-rank adaptation (LoRA) for efficient federated fine-tuning but utilized traditional FL aggregation strategies on LoRA adapters. This approach led to mathematically inaccurate aggregation noise, reducing fine-tuning effectiveness and failing to address heterogeneous LoRAs. In this work, we first highlight the mathematical incorrectness of LoRA aggregation in existing federated fine-tuning methods. We introduce a new approach called FLoRA that enables federated fine-tuning on heterogeneous LoRA adapters across clients through a novel stacking-based aggregation method. Our approach is noise-free and seamlessly supports heterogeneous LoRAs. Extensive experiments demonstrate FLoRA's superior performance in both homogeneous and heterogeneous settings, surpassing state-of-the-art methods. We envision this work as a milestone for efficient, privacy-preserving, and accurate federated fine-tuning of LLMs",
    "checked": false,
    "id": "573dad7b2fca7ce72a7f0daf681391d96379ebe3",
    "semantic_title": "low-parameter federated learning with large language models",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=F8VosPsfKI": {
    "title": "Don't Stop Pretraining - Continuous Training for Enterprise Large-Scale Model Training",
    "volume": "review",
    "abstract": "AbstractArtificial Intelligence (AI) has witnessed significant advancements in recent years, particularly in the field of Large Language Models (LLMs). While there has been a surge in the application of AI in consumer-oriented chat dialogue systems, it is crucial to address the challenges associated with implementing large models in enterprise applications, such as data management and cost optimization. Therefore, we present a study across concept and method of large model operation, showing that the principle and technology of revolves around the continuous updating of training data, including both pre-training data and fine-tuning data, and showed that the final goal of domain model training could be achieved through continuous operation of the model. Moreover, the optimization process focuses on calculating and comparing the total cost during the operation of a large model, additionally a time distribution calculation method is employed to reduce the initial input cost of computing power. This optimization approach is particularly suitable for the long-term operation of large models. As a by-product of the validation process, we also obtained preliminary data on how often large models forget (both natural forgetting and forced updating) what they learned, which has implications for knowledge updating and error correction. Overall, our consistent findings indicate that the industry's ability to operate large models continues to improve, leading to a reduction in the cost of their use",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ARGGzHfcexU": {
    "title": "P-Distill: Efficient and Effective Prompt Tuning using Knowledge Distillation",
    "volume": "review",
    "abstract": "In the field of natural language processing (NLP), prompt-based learning is widely used for efficient parameter learning. However, this method has the drawback of shortening the input length by the extent of the attached prompt, leading to an inefficiency in utilizing the input space. In this study, we propose P-Distill, a novel prompt compression method that mitigates the aforementioned limitation of prompt-based learning while maintaining performance via knowledge distillation. The knowledge distillation process of P-Distill consists of two methods, namely prompt initialization and prompt distillation. Experiments on various NLP tasks demonstrate that P-Distill exhibits comparable or superior performance compared to other state-of-the-art prompt-based learning methods, even with significantly shorter prompts. Specifically, We achieve a peak improvement of 1.90% even with the prompt lengths compressed to one-eighth. An additional study further provides insights into the distinct impact of each method on the overall performance of P-Distill. Our code will be released upon acceptance",
    "checked": false,
    "id": "c1fc2546b1476b77448e01b9d7d0a50d1bf632d3",
    "semantic_title": "large language model distilling medication recommendation model",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=SckzsOAfhv": {
    "title": "SciPrompt: Knowledge-Augmented Prompting for Fine-Grained Categorization of Scientific Topics",
    "volume": "review",
    "abstract": "Prompt Learning has emerged as a pivotal method for harnessing the knowledge inherent in pre-trained language models (PLMs) to address downstream tasks in Natural Language Processing (NLP). Also, it has been demonstrated that traditional topic classification tasks can attain performance levels comparable to those of low-resource scenarios through prompt tuning. However, cross-domain, fine-grained topic classification with few-shot prompts remain largely unexplored, particularly due to the difficulty of manually selecting class label terms without domain expertise. We introduce SciPrompt, a framework designed to automatically retrieve topic-related terms and phrases as supplementary knowledge and then select semantically correlated domain-specific label terms within scientific contexts for topic classification. Furthermore, we propose a new verbalization strategy that leverages the correlation scores to enhance signal provision during model tuning. We expand our investigation to include multi-class classification across up to 53 sub-domain publication types. Our model outperforms previous low-resource prompting systems in scientific topic classification tasks under few-shot and zero-shot settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VjBtcO35Tt": {
    "title": "Restricted Neighborhood Knowledge Graph Embedding",
    "volume": "review",
    "abstract": "Knowledge Graph Embedding (KGE) methods not only focus on the semantic information provided by triples, but also consider the structural information inherent in the graph structure, when modeling entities and relations within knowledge graphs. However, within the framework of the information transmission mechanisms, methods relying on adjacency tables cannot effectively balance the scale differences between neighborhoods, which directly leads to information disparities in modeling structural information. To address this imbalance, we present a novel KGE model called restricted neighborhood knowledge graph embedding (RNE). Firstly, it uses restricted neighborhood to aggregate neighborhood information. The restricted neighborhood formats the neighborhood space, normalizing the neighborhood of each entity to the same size. Secondly, to enrich the information within restricted neighborhood, we propose a restricted neighborhood position encoding method. Experimental results on two standard benchmarks demonstrate the significant potential of our proposals",
    "checked": false,
    "id": "040fe47af8f4870bf681f34861c42b3ea46d76cf",
    "semantic_title": "message function search for knowledge graph embedding",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=AoMhcft5JQM": {
    "title": "Multi-Task Instruction Training of Diffusion-Based Text Generative Models",
    "volume": "review",
    "abstract": "Recent advancements in language models (LMs) have demonstrated remarkable adaptability across diverse tasks, excelling in both discriminative and generative domains with impressive multitasking capabilities. Recent attention of LMs has shifted towards non-autoregressive diffusion models, leveraging denoising generation for sequence-to-sequence modeling. However, the extent to which current diffusion-based LMs can handle multitasking remains unclear. In this study, we introduce a novel framework tailored to designing a diffusion model for multi-task language modeling. Inspired by latent image diffusion models, our approach involves a general transformer-based diffusion model leveraging pretrained encoders, facilitating multi-task learning with adaptable input embedding encoders. We define a diffusion loss within the trainable decoder's latent space, which interacts with any encoder via a cross-attention mechanism. This framework establishes a flexible non-autoregressive LM capable of handling potentially noisy data by leveraging robust instruction embeddings from encoders, enabling instruction tuning. We demonstrate the efficacy of our model across various setups, including single-task and multi-task scenarios, showing its ability to produce high-quality outputs by effectively utilizing and merging training task information in the continuous latent space",
    "checked": false,
    "id": "819f477065088220a6f706cd9ef76dbcb4b4c134",
    "semantic_title": "instructcv: instruction-tuned text-to-image diffusion models as vision generalists",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=2aX1JW2lTqL": {
    "title": "Active Prompting with Chain-of-Thought for Large Language Models",
    "volume": "review",
    "abstract": "The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answering tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving superior performance on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationships demonstrate the effectiveness of our method",
    "checked": true,
    "id": "3fc3460c4554a28e489a0ea6ef067b79b7d301d9",
    "semantic_title": "active prompting with chain-of-thought for large language models",
    "citation_count": 82,
    "authors": []
  },
  "https://openreview.net/forum?id=_gFGBVMRN1": {
    "title": "RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language Models",
    "volume": "review",
    "abstract": "Pre-trained Language Models (PLMs) can be accurately fine-tuned for downstream text processing tasks. Recently, researchers have introduced several parameter-efficient fine-tuning methods that optimize input prompts or adjust a small number of model parameters (e.g LoRA). In this study, we explore the impact of altering the input text of the original task in conjunction with parameter-efficient fine-tuning methods. To most effectively rewrite the input text, we train a few-shot paraphrase model with a Maximum-Marginal Likelihood objective. Using six few-shot text classification datasets, we show that enriching data with paraphrases at train and test time enhances the performance beyond what can be achieved with parameter-efficient fine-tuning alone",
    "checked": true,
    "id": "29c2fcba27398bc7eea60ca9579075d230e16cc0",
    "semantic_title": "riff: learning to rephrase inputs for few-shot fine-tuning of language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LU1sEpN4EX": {
    "title": "Bridging the Writing Manner Gap in Visual Instruction Tuning by Creating LLM-aligned Instructions",
    "volume": "review",
    "abstract": "In the realm of Large Multi-modal Models (LMMs), the ultimate modality alignment is constrained by the quality of instructions in Supervised Fine-Tuning (SFT) phase. In this paper, we assess the instruction quality from a unique perspective termed Writing Manner, which encompasses the selection of vocabulary, grammar, and sentence structure to convey specific semantics. We argue that there exists severe writing manner gap between the visual instructions and the inner Large Language Models (LLMs) of LMMs. During the SFT phase, the more pronounced the writing manner gap, the more the inner LLM is disturbed, leading to capability degradation of both inner LLM and LMM. To bridge the writing manner gap, under the promise of keeping original semantics intact, we propose to directly exploit the inner LLM for aligning the writing manner of soft-format visual instructions with that of the inner LLM itself, which yields novel LLM-aligned instructions. We develop a novel perplexity-based indicator to quantitatively assess the writing manner gap, with results showing that our approach successfully minimizes this gap. By utilizing LLM-aligned instructions, the baseline models LLaVA-7B and QwenVL have enhanced their resistance to hallucinations and achieved performance improvements across all 15 visual and language benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZWOO2Df6w_": {
    "title": "Safely Learning with Private Data: A Federated Learning Framework for Large Language Model",
    "volume": "review",
    "abstract": "Private data, being larger and quality-higher than public data, can greatly improve large language models (LLM). However, due to privacy concerns, this data is often dispersed in multiple silos, making its secure utilization for LLM training a challenge. Federated learning (FL) is an ideal solution for training models with distributed private data, but traditional frameworks like FedAvg are unsuitable for LLM due to their high computational demands on clients. An alternative, split learning, offloads most training parameters to the server while training embedding and output layers locally, making it more suitable for LLM. Nonetheless, it faces significant challenges in security and efficiency. Firstly, the gradients of embeddings are prone to attacks, leading to potential reverse engineering of private data. Furthermore, the server's limitation to handle only one client's training request at a time hinders parallel training, severely impacting training efficiency.In this paper, we propose a Federated Learning framework for LLM, named FL-GLM, which prevents data leakage caused by both server-side and peer-client attacks while improve the training efficiency. Specifically, we first place the input block and output block on local client to prevent embedding gradient attacks from server. Secondly, we employ key-encryption during client-server communication to prevent reverse engineering attacks from peer-clients. Lastly, we employ optimization methods like client-batching or server-hierarchical, adopting different acceleration methods based on the actual computational capabilities of the server.Experimental results on NLU and generation tasks demonstrate that FL-GLM achieves comparable metrics to centralized chatGLM model, validating the effectiveness of our federated learning framework",
    "checked": true,
    "id": "8699ca2562722a986ffaea99cc6c94b39567f97b",
    "semantic_title": "safely learning with private data: a federated learning framework for large language model",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=geSFeGOuAW": {
    "title": "Comparing Hallucination Detection Metrics for Multilingual Generation",
    "volume": "review",
    "abstract": "While many automatic hallucination detection techniques have been proposed for English texts, their effectiveness in multilingual contexts remains unexplored. This paper aims to bridge the gap in understanding how these hallucination detection metrics perform on non-English languages. We evaluate the efficacy of various detection metrics, including lexical metrics like ROUGE and Named Entity Overlap and Natural Language Inference (NLI)-based metrics, at detecting hallucinations in biographical summaries in many languages; we also evaluate how correlated these different metrics are to gauge whether they measure the same phenomena. Our empirical analysis reveals that while lexical metrics show limited effectiveness, NLI-based metrics perform well in high-resource languages at the sentence level. In contrast, NLI-based metrics often fail to detect atomic fact hallucinations. Our findings highlight existing gaps in multilingual hallucination detection and motivate future research to develop more robust detection methods for LLM hallucination in other languages",
    "checked": true,
    "id": "4bb30cdd7216d0bde3f937489ec04b161b44977c",
    "semantic_title": "comparing hallucination detection metrics for multilingual generation",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=WXu0GOZJjD": {
    "title": "Diversity-enhanced Learning for Unsupervised Syntactically Controlled Paraphrase Generation",
    "volume": "review",
    "abstract": "Syntactically controlled paraphrase generation is to generate diverse sentences that have the same semantics as the given original sentence but conform to the target syntactic structure. An optimal opportunity to enhance diversity is to make word substitutions during rephrasing based on syntactic control. Existing unsupervised methods have made great progress in syntactic control, but the generated paraphrases rarely have substitutions due to the limitation of training data. In this paper, we propose a Diversity syntactically controlled Paraphrase generation framework (DiPara), in which a novel training strategy is designed to obtain semantic sentences while using the given sentence as training objects. As diverse words vary the syntactic structure around them, we propose a phrase-aware attention mechanism to capture the syntactic structure associated with the current word. To achieve it, the linearized triple sequence is introduced to represent structure singly. Experiment results on two datasets show that DiPara outperforms strong baselines, especially diversity (Self-BLEU$_4$) is improved by 10.18\\% in ParaNMT-Small",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pCTD-y6Vc8": {
    "title": "Living in the Moment: Can Large Language Models Grasp Co-Temporal Reasoning?",
    "volume": "review",
    "abstract": "Temporal reasoning is fundamental for large language models~(LLMs) to comprehend the world. Current temporal reasoning datasets are limited to questions about single or isolated events, falling short in mirroring the realistic temporal characteristics involving concurrent nature and intricate temporal interconnections. In this paper, we introduce \\textsc{CoTemp\\-QA}, a comprehensive co-temporal Question Answering (QA) benchmark containing four co-temporal scenarios~(Equal, Overlap, During, Mix) with 4,748 samples for evaluating the co-temporal comprehension and reasoning abilities of LLMs. Our extensive experiments reveal a significant gap between the performance of current LLMs and human-level reasoning on \\textsc{CoTemp\\-QA} tasks. Even when enhanced with Chain of Thought (CoT) methodologies, models consistently struggle with our task. In our preliminary exploration, we discovered that mathematical reasoning plays a significant role in handling co-temporal events and proposed a strategy to boost LLMs' co-temporal reasoning from a mathematical perspective. We hope that our \\textsc{CoTemp\\-QA} datasets will encourage further advancements in improving the co-temporal reasoning capabilities of LLMs",
    "checked": true,
    "id": "5bb74befb93e69bc59683ac572f12e48d9cf9846",
    "semantic_title": "living in the moment: can large language models grasp co-temporal reasoning?",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=I4pgXmLyA0": {
    "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
    "volume": "review",
    "abstract": "Instruction-tuning language models has become a crucial step in aligning them for general use. Typically, this process involves extensive training on large datasets, incurring high training costs. In this paper, we introduce a novel training data selection based on the learning percentage of the samples. We assert that current language models possess the capability to autonomously select high-quality training data, leading to comparable or improved performance compared to training on the entire dataset. Our experiments span different-sized models, revealing that this characteristic holds for models ranging from 1B (small) to 13B (large) in size. Moreover, we demonstrate an interesting finding that the data hardness transfers across model sizes, and a smaller 350M model can effectively curate high-quality training data with hard samples for a larger 13B model, resulting in an equally or superior instruction-tuned model compared to training on the complete dataset. Utilizing open-sourced OPT and Llama-2 models up to 13B in size, two publicly available instruction-tuning training datasets and evaluated by both automatic metrics & humans, our paper introduces a novel approach to training data selection, showcasing a more efficient alternative",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jdj72VY5NC": {
    "title": "Harder Task Needs More Experts: Dynamic Routing in MoE Models",
    "volume": "review",
    "abstract": "In this paper, we introduce a novel dynamic expert selection framework for Mixture of Experts (MoE) models, aiming to enhance computational efficiency and model performance by adjusting the number of activated experts based on input difficulty. Unlike traditional MoE approaches that rely on fixed Top-K routing, which activates a predetermined number of experts regardless of the input's complexity, our method dynamically selects experts based on the confidence level in expert selection for each input. This allows for a more efficient utilization of computational resources, activating more experts for complex tasks requiring advanced reasoning and fewer for simpler tasks. Through extensive evaluations, our dynamic routing method demonstrates significant improvements over conventional Top-2 routing across various benchmarks, achieving an average improvement of 0.7\\% with less than 90\\% activated parameters. Further analysis shows our model dispatches more experts to tasks requiring complex reasoning skills, like BBH, confirming its ability to dynamically allocate computational resources in alignment with the input's complexity. Our findings also highlight a variation in the number of experts needed across different layers of the transformer model, offering insights into the potential for designing heterogeneous MoE frameworks. We will open-source all the models we trained in this project",
    "checked": false,
    "id": "5370fed735af6aabb0360513793875e512ad4a7b",
    "semantic_title": "harder tasks need more experts: dynamic routing in moe models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=D_PMBtkYmfT": {
    "title": "NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms",
    "volume": "review",
    "abstract": "The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference. One understudied avenue of language change causing data drift is the emergence of neologisms -- new word forms -- over time. We create a diverse resource of recent English neologisms by using several popular collection methods. We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words. Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence. Motivated by these results, we construct a benchmark to evaluate LLMs' ability to generalize to neologisms with various natural language understanding tasks and model perplexity. Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks. LLMs are also affected differently based on the linguistic origins of words, indicating that neologisms are complex for static LLMs to address. We will release our benchmark and code for reproducing our experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mnD0qGm_Knm": {
    "title": "Are Machines Better at Complex Reasoning? Unveiling Human-Machine Inference Gaps in Entailment Verification",
    "volume": "review",
    "abstract": "Making inferences in text comprehension to understand the meaning is essential in language processing. This work studies the entailment verification (EV) problem of complex, multi-sentence premises requiring a system to make multiple inferences implicitly. Modern applications of EV in detecting inconsistent model-generated rationales require complex multi-hop reasoning. However, current textual inference datasets mostly contain short-sentence premises that partially focus on this. To address this, we compile an EV benchmark that includes datasets from three NLP domains (NLI, contextual QA, and rationales) containing multi-sentence premises. On benchmarking humans and LLMs, we find that LLMs are better than humans in multi-hop reasoning across extended contexts, while humans perform better in simple deductive reasoning tasks. We also finetune a Flan-T5 model for EV using two training objectives to obtain a strong open-source model that outperforms GPT-3.5 and rivals GPT-4. Finally, we use our finetuned model to filter out inconsistent model-generated rationales in self-consistency decoding, resulting in a 6% accuracy improvement on average across three MCQ datasets",
    "checked": true,
    "id": "78b74b9e7c2a1918b792cb0dbf7048796621f679",
    "semantic_title": "are machines better at complex reasoning? unveiling human-machine inference gaps in entailment verification",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=yt4VVVUXzJE": {
    "title": "Prompts As Programs: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization",
    "volume": "review",
    "abstract": "Large language models (LLMs) can now handle longer and more complex inputs, which facilitate the use of more elaborate prompts. However, prompts often require some tuning to improve performance for deployment. Recent work has proposed automatic prompt optimization methods, but as prompt complexity and LLM strength increase, many prompt optimization techniques are no longer sufficient and a new approach is needed to optimize {\\em meta prompt programs}. To address this, we introduce SAMMO, a framework for {\\em compile-time} optimizations of metaprompt programs, which represent prompts as structured objects that allows for a rich set of transformations that can be searched over during optimization. We show that SAMMO generalizes previous methods and improves the performance of complex prompts on (1) instruction tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several different LLMs",
    "checked": false,
    "id": "96f5ce59e3dd54fa508e13a499de7fd7b633b022",
    "semantic_title": "symbolic prompt program search: a structure-aware approach to efficient compile-time prompt optimization",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=XCrvCAbSQsD": {
    "title": "Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization",
    "volume": "review",
    "abstract": "A single language model (LM), despite aligning well with an average labeler through reinforcement learning from human feedback (RLHF), may not universally suit diverse human preferences. Recent approaches therefore opt for customization by collecting multi-dimensional feedback and creating distinct reward models (RMs) for each dimension (e.g., helpfulness, harmlessness, or honesty). Different LMs can then be optimized for different preferences using multi-objective RLHF (MORLHF) with different reward weightings.Yet, RL fine-tuning is unstable and resource-heavy, especially for MORLHF with diverse and usually conflicting objectives. In this paper, we present Multi-Objective Direct Preference Optimization (MODPO), an RL-free algorithm that extends Direct Preference Optimization (DPO) for multiple alignment objectives with minimal overheads. Essentially, MODPO folds language modeling directly into reward modeling, training LMs as implicit collective reward models (cRMs) that combine all objectives with specific weightings. While theoretically guaranteed to produce the same optimal solutions as MORLHF, MODPO is practically more stable and computationally efficient. Empirical results from safety alignment and long-form question answering confirm that MODPO matches or outperforms existing methods, consistently producing a Pareto front of LMs that cater to diverse preferences with 3 times less computational resources compared to MORLHF",
    "checked": true,
    "id": "59207e9d0cd4129b6ed665205105192dd3032ff3",
    "semantic_title": "beyond one-preference-fits-all alignment: multi-objective direct preference optimization",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=VLoeSBinlws": {
    "title": "FastGAS: Fast Graph-based Annotation Selection for In-Context Learning",
    "volume": "review",
    "abstract": "In-context learning (ICL) empowers large language models (LLMs) to tackle new tasks by using a series of training instances as prompts. Since generating the prompts needs to sample from a vast pool of instances and annotate them (e.g., add labels in classification task), existing methods have proposed to select a subset of unlabeled examples for annotation, thus enhancing the quality of prompts and concurrently mitigating annotation costs. However, these methods often require a long time to select instances due to their complexity, hindering their practical viability. To address this limitation, we propose a graph-based selection method, FastGAS, designed to efficiently identify high-quality instances while minimizing computational overhead. Initially, we construct a data similarity graph based on instance similarities. Subsequently, employing a graph partitioning algorithm, we partition the graph into pieces. Within each piece (i.e., subgraph), we adopt a greedy approach to pick the most representative nodes. By aggregating nodes from diverse pieces and annotating the corresponding instances, we identify a set of diverse and representative instances for ICL. Compared to prior approaches, our method not only exhibits superior performance on different tasks but also significantly reduces selection time. In addition, we demonstrate the efficacy of our approach in LLMs of larger sizes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0AVpObQWsF": {
    "title": "The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models",
    "volume": "review",
    "abstract": "In the era of large language models (LLMs), hallucination (\\ie the tendency to generate factually incorrect content) poses great challenges to trustworthy and reliable deployment of LLMs in real-world applications. To tackle the hallucination, three key questions should be well studied: how to detect hallucinations (detection), why do LLMs hallucinate (source), and what can be done to mitigate them (mitigation). To address these challenges, this work presents a systematic empirical study on LLM hallucinations, focused on the three aspects of hallucination detection, source and mitigation. Specially, we construct a new hallucination benchmark HaluEval 2.0, and design a simple yet effective detection method for LLM hallucinations. Furthermore, we zoom into the different training or utilization stages of LLMs and extensively analyze the potential factors that lead to the LLM hallucinations. Finally, we implement and examine a series of widely used techniques to mitigate the hallucinations in LLMs. Our work has led to several important findings to understand the hallucination origin and mitigate the hallucinations in LLMs",
    "checked": true,
    "id": "1b387e3fbec0447c8bf2dcee21f6db59cdddf698",
    "semantic_title": "the dawn after the dark: an empirical study on factuality hallucination in large language models",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=PMzKjiq9pd": {
    "title": "An Iterative Associative Memory Model for Empathetic Response Generation",
    "volume": "review",
    "abstract": "Empathetic response generation is to comprehend the cognitive and emotional states in dialogue utterances and generate proper responses. Psychological theories posit that comprehending emotional and cognitive states necessitates iteratively capturing and understanding associated words across dialogue utterances. However, existing approaches regard dialogue utterances as either a long sequence or independent utterances for comprehension, which are prone to overlook the associated words between them.To address this issue, we propose an Iterative Associative Memory Model (IAMM) for empathetic response generation. Specifically, we employ a novel second-order interaction attention mechanism to iteratively capture vital associated words between dialogue utterances and situations, dialogue history, and a memory module (for storing associated words), thereby accurately and nuancedly comprehending the utterances. We conduct experiments on the Empathetic-Dialogue dataset. Both automatic and human evaluations validate the efficacy of the model. Meanwhile, variant experiments on LLMs also demonstrate that attending to associated words improves empathetic comprehension and expression",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IKLfhaRsVE": {
    "title": "Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning",
    "volume": "review",
    "abstract": "In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we design a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning demonstration prompts, which can make models behave in accordance with predefined intentions. ICLAttack does not require additional fine-tuning to implant a backdoor, thus preserving the model's generality. Furthermore, the poisoned examples are correctly labeled, enhancing the natural stealth of our attack method. Extensive experimental results across several language models, ranging in size from 1.3B to 180B parameters, demonstrate the effectiveness of our attack method, exemplified by a high average attack success rate of 95.0% across the three datasets on OPT models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5iQrHHR6ta": {
    "title": "ResLoRA: Identity Residual Mapping in Low-Rank Adaption",
    "volume": "review",
    "abstract": "As one of the most popular parameter-efficient fine-tuning (PEFT) methods, low-rank adaptation (LoRA) is commonly applied to fine-tune large language models (LLMs). However, updating the weights of LoRA blocks effectively and expeditiously is challenging due to the long calculation path in the original model. To address this, we propose ResLoRA, an improved framework of LoRA. By adding residual paths during training and using merging approaches to eliminate these extra paths during inference, our method can achieve better results in fewer training steps without any extra trainable parameters or inference cost compared to LoRA. The experiments on NLG, NLU, and text-to-image tasks demonstrate the effectiveness of our method. To the best of our knowledge, ResLoRA is the first work that combines the residual path with LoRA. The code of our method is available at \\url{https://anonymous.4open.science/r/ResLoRA-E25E}",
    "checked": true,
    "id": "392305c7cf8922af0a919d05a26852e5d4150e9c",
    "semantic_title": "reslora: identity residual mapping in low-rank adaption",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=MoEQvgCk-da": {
    "title": "Pushing the Limits of Zero-shot End-to-End Speech Translation",
    "volume": "review",
    "abstract": "Data scarcity and the modality gap between the speech and text modalities are two major obstacles of end-to-end Speech Translation (ST) systems, thus hindering their performance. Prior work has attempted to mitigate these challenges by leveraging external MT data and optimizing distance metrics that bring closer the speech-text representations. However, achieving competitive results typically requires some ST data. For this reason, we introduce ZeroSwot, a method for zero-shot ST that bridges the modality gap without any paired ST data. Leveraging a novel CTC compression and Optimal Transport, we train a speech encoder using only ASR data, to align with the representation space of a massively multilingual MT model. The speech encoder seamlessly integrates with the MT model at inference, enabling direct translation from speech to text, across all languages supported by the MT model. Our experiments show that we can effectively close the modality gap without ST data, while our results on MuST-C and CoVoST demonstrate our method's superiority over not only previous zero-shot models, but also supervised ones, achieving state-of-the-art results",
    "checked": true,
    "id": "8e42a4bf509e9ec2fc33bb742265477bbf132ecc",
    "semantic_title": "pushing the limits of zero-shot end-to-end speech translation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=EB1xXVM6M3": {
    "title": "Striking Gold in Advertising: Standardization and Exploration of Ad Text Generation",
    "volume": "review",
    "abstract": "In response to the limitations of manual ad creation, significant research has been conducted in the field of automatic ad text generation (ATG). However, the lack of comprehensive benchmarks and well-defined problem sets has made comparing different methods challenging. To tackle these challenges, we standardize the task of ATG and propose a first benchmark dataset, ATG-BENCH, carefully designed and enabling the utilization of multi-modal information and facilitating industry-wise evaluations. Our extensive experiments with a variety of nine baselines, from classical methods to state-of-the-art models including large language models (LLMs), show the current state and the remaining challenges. We also explore how existing metrics in ATG and an LLM-based evaluator align with human evaluations",
    "checked": true,
    "id": "8bc33719e0c251f40d318029824791f9642966e5",
    "semantic_title": "striking gold in advertising: standardization and exploration of ad text generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f4gWOzBfCGp": {
    "title": "Enhancing Sentence Simplification in Portuguese: Leveraging Paraphrases, Context, and Linguistic Features",
    "volume": "review",
    "abstract": "Automatic text simplification focuses on transforming texts into a more comprehensible version without sacrificing their precision. However, automatic methods usually require (paired) datasets that can be rather scarce in languages other than English. This paper presents a new approach to automatic sentence simplification that leverages paraphrases, context, and linguistic attributes to overcome the absence of paired texts in Portuguese. We frame the simplification problem as a textual style transfer task and learn a style representation using the sentences around the target sentence in the document and its linguistic attributes. Moreover, unlike most unsupervised approaches that require style-labeled training data, we fine-tune strong pre-trained models using sentence-level paraphrases instead of annotated data. Our experiments show that our model achieves remarkable results, surpassing the current state-of-the-art (BART+ACCESS) while competitively matching a Large Language Model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gH6rPJurgg": {
    "title": "Text-like Encoding of Collaborative Information in Large Language Models for Recommendation",
    "volume": "review",
    "abstract": "When adapting Large Language Models for Recommendation (LLMRec), it is crucial to integrate collaborative information. Existing methods achieve this by learning collaborative embeddings in LLMs' latent space from scratch or by mapping from external models. However, they fail to represent the information in a text-like format, which may not align optimally with LLMs. To bridge this gap, we introduce BinLLM, a novel LLMRec method that seamlessly integrates collaborative information through text-like encoding. BinLLM converts collaborative embeddings from external models into binary sequences --- a specific text format that LLMs can understand and operate on directly, facilitating the direct usage of collaborative information in text-like format by LLMs. Additionally, BinLLM provides options to compress the binary sequence using dot-decimal notation to avoid excessively long lengths. Extensive experiments validate that BinLLM introduces collaborative information in a manner better aligned with LLMs, resulting in enhanced performance",
    "checked": true,
    "id": "8591c3e0239be3e089449d3899b94f0a9c75465a",
    "semantic_title": "text-like encoding of collaborative information in large language models for recommendation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=dN7PN4pOOm": {
    "title": "Speech vs. Transcript: Does It Matter for Human Annotators in Speech Summarization?",
    "volume": "review",
    "abstract": "Reference summaries for abstractive speech summarization require human annotation, which can be performed by listening to an audio recording or by reading textual transcripts of the recording. In this paper, we examine whether summaries based on annotators listening to the recordings differ from those based on annotators reading transcripts. Using existing intrinsic evaluation based on human evaluation, automatic metrics, LLM-based evaluation, and a retrieval-based reference-free method, we find that summaries are indeed different based on the source modality, and that speech-based summaries are more factually consistent and information-selective than transcript-based summaries. Transcript-based summaries are impacted by recognition errors in the source, and expert-written summaries are more informative and reliable. We make all the collected data and analysis code public to facilitate the reproduction of our work and advance research in this area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b-lfnTVqYUX": {
    "title": "Linguistically Conditioned Semantic Textual Similarity",
    "volume": "review",
    "abstract": "Semantic textual similarity (STS) is a fundamental NLP task that measures the semantic similarity between a pair of sentences. In order to reduce the inherent ambiguity posed from the sentences, a recent work called Conditional STS (C-STS) has been proposed to measure the sentences' similarity conditioned on a certain aspect. Despite the popularity of C-STS, we find that the current C-STS dataset suffers from various issues that could impede proper evaluation on this task. In this paper, we reannotate the C-STS validation set and observe an annotator discrepancy on 55% of the instances resulting from the annotation errors in the original label, ill-defined conditions, and the lack of clarity in the task definition. After a thorough dataset analysis, we improve the C-STS task by leveraging the models' capability to understand the conditions under a QA task setting. With the generated answers, we present an automatic error identification pipeline that is able to identify annotation errors from the C-STS data with over 80% F1 score. We also propose a new method that largely improves the performance over baselines on the C-STS data by training the models with the answers. Finally we show the conditionality annotation can benefit from the typed-feature structure (TFS) of entity types. Our pilot annotation study shows that the TFS is able to provide a linguistic foundation for constructing C-STS data with new conditions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NlnDid89nyz": {
    "title": "Are Language Models Better at Generating Answers or Validating Solutions?",
    "volume": "review",
    "abstract": "Recently, large language models (LLMs) have demonstrated remarkable reasoning abilities, augmented by recent advances in prompting techniques and reasoning frameworks. Many popular frameworks \\cite{du_improving_2023, yao_tree_2023, shinn_reflexion_2023} rely on the assumption that models are able to give effective feedback on their own generations. This feedback is partly predicated on being able to correctly validate, or classify, the generated prediction as either correctly or incorrectly solving the given problem. While in traditional computer science settings validation has been shown to be as difficult as correct generation, we find empirically that language models may be better discriminators than generators. Our work studies whether leading language models are better at solving problems or validating solutions, and we attempt to gain a better understanding of why this happens. We quantify this by measuring the understanding gap --- the difference between generative and discriminative accuracy. First, we further corroborate recent work \\cite{west_generative_2024} showing surprisingly that models are better generators than discriminators on some datasets. Second, we discover that understanding gaps can be closed or significantly narrowed through prompting and provide an estimate of the upper bound $\\epsilon$ on the understanding gap across datasets. Third, we apply our findings to predict the settings where self-correction is most effective. This continues the conversation started by \\cite{huang_large_2023}, where we instead show that LLMs can self-correct reasoning, and establish a link between a feature of the dataset and the language model's ability to self-correct",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BwtB6wujPGo": {
    "title": "Multidimensional Difficulty Metric for Multimodal Sarcasm Explanation",
    "volume": "review",
    "abstract": "Multimodal Sarcasm Explanation~(MuSE) is a new yet challenging task, which aims at generating natural language explanations for sarcasm in social media image-text pairs.MuSE can further enhance sarcasm understanding and has attracted increasing research interest. Previous works design cross-modal attention or multi-source semantic graphs and achieve promising performance. However, these works either ignore the semantic gap between visual features and textual decoder or introduce complex graph constructions, which limits their practical applicability and scalability for real-world scenarios.Furthermore, they treat each sample equally during training, overlooking the impact of the inherent differences in sample difficulty.In this paper, we propose a novel multidimensional sample difficulty metric approach with the Multimodal Large Language Model~(MLLM) for MuSE.Leveraging the feature alignment and innate knowledge of MLLM, we can achieve better cross-modal alignment without complicated processes.Using the multidimensional difficulty metric to measure the difficulty of image-text pairs, we enable MLLM to learn from simple to difficult samples, mitigating the impact of samples of varying difficulty on training, and preventing local optima.Experimental results on a publicly released dataset MORE demonstrate that our method is effective and achieves state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OHjfJTRn0mc": {
    "title": "Detecting Biases of GPT Models with Bayesian Hypothesis Testing",
    "volume": "review",
    "abstract": "Though large language models (LLMs) like generative pre-trained Transformers (GPTs) have achieved superior performance over many tasks, they capture and propagate social biases and stereotypes that are present in the training data. In this paper, we propose a framework that reformulates the bias detection of LLMs as a hypothesis testing problem with the null $H_0$ denoting {\\em no bias}. Our frameworkis designed for contrastive text pairs, and it has two schemes: one is based on (log-)likelihood and another is based on preference. To this end, two public dataset CrowS-Pairs and its French version are utilized, both including nine categories of bias. Although frequentist methods such as Student's $t$ and Wilcoxon test can be employed in our framework, Bayesian test (Bayes factors) is preferred for bias detection as it allows practitioners to quantify the evidence for both two competing hypotheses. Our framework is suitable for a wide range of large language models, and we demonstrate its application to the popular GPT-3 (text-davinci-003) and ChatGPT (GPT-3.5-Turbo) in the experiments. From our experiments, the bias behavior of ChatGPT is mostly consistent on both the English and French CrowS-pairs datasets, but still exhibits some differences due to different social norms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fR9_4Dr5Ron": {
    "title": "Discovering influential text using convolutional neural networks",
    "volume": "review",
    "abstract": "Experimental methods for estimating the impacts of text on human evaluation have been widely used in the social sciences. However, researchers in experimental settings are usually limited to testing a small number of pre-specified text treatments. While efforts to mine unstructured texts for features that causally affect outcomes have been ongoing in recent years, these models have primarily focused on the topics or specific words of text, which may not always be the mechanism of the effect. We connect these efforts with NLP interpretability techniques and present a method for flexibly discovering clusters of similar text phrases that are predictive of human reactions to texts using convolutional neural networks. When used in an experimental setting, this method can identify text treatments and their effects under certain assumptions. We apply the method to two data sets. The first enables direct validation of the model's ability to detect phrases known to cause the outcome. The second demonstrates its ability to flexibly discover text treatments with varying textual structures. In both cases, the model learns a greater variety of text treatments compared to benchmark methods, and these text features quantitatively meet or exceed the ability of benchmark methods to predict the outcome",
    "checked": true,
    "id": "b2a49f58311aa03976c11782c505e288a7912aaa",
    "semantic_title": "discovering influential text using convolutional neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gmC29F7C6dt": {
    "title": "Benchmarking and Improving Long-Text Translation with Large Language Models",
    "volume": "review",
    "abstract": "Recent studies have illuminated the promising capabilities of large language models (LLMs) in handling long texts. However, their performance in machine translation (MT) of long documents remains underexplored. This paper aims to shed light on how LLMs navigate this complex task, offering a comprehensive evaluation of their capabilities and limitations in long-text MT. First, we collect and construct an instruction-based benchmark dataset, specifically designed for the finetuning and evaluation of LLMs, encompassing multilingual, multi-domain, and document-level parallel data. Second, we conduct a comprehensive comparison between MT and LLM models concerning document-level translation. Our analysis uncovers that LLMs exhibit shortcomings in long-text domains, and their performance diminishes as document size escalates. By exploiting various extrapolation strategies, we enhance the capacity of LLMs to translate longer texts. We will release data, code, and models, which we hope can promote research in this field",
    "checked": false,
    "id": "5397516b5482463e6bcd6bb3de28b9b02afc6968",
    "semantic_title": "voice detection and speech translation using ensemble inception v3 and lstm",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dgGUw8YWpH": {
    "title": "KIWI: A Dataset of Knowledge-Intensive Writing Instructions for Answering Research Questions",
    "volume": "review",
    "abstract": "Large language models (LLMs) adapted to follow user instructions are now widely deployed as conversational agents. In this work, we examine one increasingly common instruction-following task: providing writing assistance to compose a long-form answer. To evaluate the capabilities of current LLMs on this task, we construct \\dsname, a dataset of knowledge-intensive writing instructions in the scientific domain. Given a research question, an initial model-generated answer and a set of relevant papers, an expert annotator iteratively issues instructions for the model to revise and improve its answer. We collect 1,260 interaction turns from 234 interaction sessions with three state-of-the-art LLMs. Each turn includes a user instruction, a model response, and a human evaluation of the model response. Through a detailed analysis of the collected responses, we find that all models struggle to incorporate new information into an existing answer, and to perform precise and unambiguous edits. Further, we find that models struggle to judge whether their outputs successfully followed user instructions, with accuracy at least 10 points short of human agreement. Our findings indicate that \\dsname will be a valuable resource to measure progress and improve LLMs' instruction-following capabilities for knowledge intensive writing tasks",
    "checked": true,
    "id": "87d24f44d4e881e51e8b1c8f3f0f7ba53fc53aef",
    "semantic_title": "kiwi: a dataset of knowledge-intensive writing instructions for answering research questions",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=lKzwpZI0Lce": {
    "title": "How Susceptible are Large Language Models to Ideological Manipulation?",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) possess the potential to exert substantial influence on public perceptions and interactions with information. This raises concerns about the societal impact that could arise if the ideologies within these models can be easily manipulated. In this work, we investigate how effectively LLMs can learn and generalize ideological biases from their instruction-tuning data. Our findings reveal a concerning vulnerability: exposure to only a small amount of ideologically driven samples significantly alters the ideology of LLMs. Notably, LLMs demonstrate a startling ability to absorb ideology from one topic and generalize it to even unrelated ones. The ease with which LLMs' ideologies can be skewed underscores the risks associated with intentionally poisoned training data by malicious actors or inadvertently introduced biases by data annotators. It also emphasizes the imperative for robust safeguards to mitigate the influence of ideological manipulations on LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oRtnvelfQR": {
    "title": "AMEND: Question Augmentation for Meaningful Exchange and Nuanced Discourse between experts and community members",
    "volume": "review",
    "abstract": "It has become commonplace for everyday users to utilize large language models (LLMs) for day-to-day tasks. However, some of these tasks require specialized expertise, such as interpreting terms and conditions, calculating personal taxes, or isolating the likely cause of a medical symptom. Currently, users can ask these questions directly to LLMs and receive answers, but users may not possess the necessary expertise to instruct models and interpret their results. In this work, we investigate whether models can output correct and factual answers when given expert-designed prompts within the legal domain. We propose the task of legal question reformulation, and develop a novel evaluation framework using expert legal knowledge. Through careful evaluation, we find that while LLMs show strong potential for legal reasoning, there are key safety issues present in their outputs. We also observe that adding linguistically-aligned in-domain text samples can improve performance, even when the samples are not aligned factually with the given question",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k7xnNZf33w0": {
    "title": "Query-OPT: Optimizing Inference of Large Language Models via Multi-Query Instructions in Meeting Summarization",
    "volume": "review",
    "abstract": "This work focuses on the task of query-based meeting summarization in which the summary of a context (meeting transcript) is generated in response to a specific query. When using Large Language Models (LLMs) for this task, a new call to the LLM inference endpoint/API is required for each new query even if the context stays the same. However, repeated calls to the LLM inference endpoints would significantly increase the costs of using them in production, making LLMs impractical for many real-world use cases. To address this problem, in this paper, we investigate whether combining the queries for the same input context in a single prompt to minimize repeated calls can be successfully used in meeting summarization. In this regard, we conduct extensive experiments by comparing the performance of various popular LLMs: GPT-4, PaLM-2, LLaMA-2, Mistral, and FLAN-T5 in single-query and multi-query settings. We observe that while most LLMs tend to respond to the multi-query instructions, almost all of them (except GPT-4), even after fine-tuning, could not properly generate the response in the required output format. We conclude that while multi-query prompting could be useful to optimize the inference costs by reducing calls to the inference APIs/endpoints for the task of meeting summarization, this capability to reliably generate the response in the expected format is only limited to certain LLMs",
    "checked": true,
    "id": "6174938a95de0315fa1ce4b282f69574aa5e4019",
    "semantic_title": "query-opt: optimizing inference of large language models via multi-query instructions in meeting summarization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DG8w2zMDk1": {
    "title": "KMMLU: Measuring Massive Multitask Language Understanding in Korean",
    "volume": "review",
    "abstract": "We propose KMMLU, a new Korean benchmark with 35,030 expert-level multiple-choice questions across 45 subjects ranging from humanities to STEM. Unlike previous Korean benchmarks that are translated from existing English benchmarks, KMMLU is collected from original Korean exams, capturing linguistic and cultural aspects of the Korean language. We test 26 publically available and proprietary LLMs, identifying significant room for improvement. The best publicly available model achieves 50.54% on KMMLU, far below the average human performance of 62.6%. This model was primarily trained for English and Chinese, not Korean. Current LLMs tailored to Korean, such as Polyglot-Ko, perform far worse. Surprisingly, even the most capable proprietary LLMs, e.g., GPT-4 and HyperCLOVA X, achieve 59.95% and 53.40%, respectively. This suggests that further work is needed to improve Korean LLMs, and KMMLU offers the right tool to track this progress. We make our dataset publicly available on the Hugging Face Hub and integrate the benchmark into EleutherAI's Language Model Evaluation Harness",
    "checked": true,
    "id": "4017baa6923912fd953c17d3bdd3d29ca8a79f47",
    "semantic_title": "kmmlu: measuring massive multitask language understanding in korean",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=BDAdYOn9-0": {
    "title": "Unlocking the `Why' of Buying: Introducing a New Dataset and Benchmark for Purchase Reason and Post-Purchase Experience",
    "volume": "review",
    "abstract": "Explanations are crucial for enhancing user trust and understanding within modern recommendation systems. To build truly explainable systems, we need high-quality datasets that elucidate why users make choices. While previous efforts have focused on extracting users' post-purchase sentiment in reviews, they ignore the reasons behind the decision to buy. In our work, we propose a novel purchase reason explanation task. To this end, we introduce an LLM-based approach to generate a dataset that consists of textual explanations of why real users make certain purchase decisions. We induce LLMs to explicitly distinguish between the reasons behind purchasing a product and the experience after the purchase in a user review. An automated, LLM-driven evaluation, as well as a small scale human evaluation, confirms the effectiveness of our approach to obtaining high-quality, personalized explanations. We benchmark this dataset on two personalized explanation generation tasks. We release the code and prompts to spur further research",
    "checked": false,
    "id": "d893df3be89b2facfdfadf2bb2c76af3166c9305",
    "semantic_title": "unlocking the 'why' of buying: introducing a new dataset and benchmark for purchase reason and post-purchase experience",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5X0lAr1hYKy": {
    "title": "Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression",
    "volume": "review",
    "abstract": "Large language models (LLMs) require lengthy prompts as the input context to produce output aligned with user intentions, a process that incurs extra costs during inference. In this paper, we propose the Gist COnditioned deCOding (Gist-COCO) model, introducing a novel method for compressing prompts which also can assist the prompt interpretation and engineering. Gist-COCO employs an encoder-decoder based language model and then incorporates an additional encoder as a plugin module to compress prompts with inputs using gist tokens. It finetunes the compression plugin module and uses the representations of gist tokens to emulate the raw prompts in the vanilla language model. By verbalizing the representations of gist tokens into gist prompts, the compression ability of Gist-COCO can be generalized to different LLMs with high compression rates. Our experiments demonstrate that Gist-COCO outperforms previous prompt compression models in both passage and instruction compression tasks. Further analysis on gist verbalization results suggests that our gist prompts serve different functions in aiding language models. They may directly provide potential answers, generate the chain-of-thought, or simply repeat the inputs. All codes will be released via GitHub",
    "checked": true,
    "id": "b1f0b8018695375aa77799a8657e49f8d99b952d",
    "semantic_title": "say more with less: understanding prompt learning behaviors through gist compression",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=mbbl4wtoT6T": {
    "title": "Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM",
    "volume": "review",
    "abstract": "Recently, Large Language Models (LLMs) have made significant advancements and are now widely used across various domains. Unfortunately, there has been a rising concern that LLMs can be misused to generate harmful or malicious content. Though a line of research has focused on aligning LLMs with human values and preventing them from producing inappropriate content, such alignments are usually vulnerable and can be bypassed by alignment-breaking attacks via adversarially optimized or handcrafted jailbreaking prompts. In this work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against potential alignment-breaking attacks. RA-LLM can be directly constructed upon an existing aligned LLM with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original LLM. Furthermore, we also provide a theoretical analysis for RA-LLM to verify its effectiveness in defending against alignment-breaking attacks. Through real-world experiments on open-source large language models, we demonstrate that RA-LLM can successfully defend against both state-of-the-art adversarial prompts and popular handcrafted jailbreaking prompts by reducing their attack success rates from nearly 100% to around 10% or less",
    "checked": true,
    "id": "cd29c25c489562b409a60f83365f93f33ee1a0a1",
    "semantic_title": "defending against alignment-breaking attacks via robustly aligned llm",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=1XOxA7aq6aR": {
    "title": "MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs",
    "volume": "review",
    "abstract": "Generative Large Language Models (LLMs) are widely utilized for their excellence in various tasks. However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments. Therefore, estimating the correctness of generative LLM outputs is an important task for enhanced reliability. Uncertainty Estimation (UE) in generative LLMs is an evolving domain, where SOTA probability-based methods commonly employ length-normalized scoring. In this work, we propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods. MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the question. We demonstrate that integrating MARS into UE methods results in a universal and significant improvement in UE performance. We conduct experiments using three distinct closed-book question-answering datasets across five popular pre-trained LLMs. Lastly, we validate the efficacy of MARS on a Medical QA dataset. Code can be found \\href{ https://anonymous.4open.science/r/LLM_Uncertainity-309B } {here}",
    "checked": true,
    "id": "8cc380602b7eaba9b58ef0ec3ce9d4c514dd9701",
    "semantic_title": "mars: meaning-aware response scoring for uncertainty estimation in generative llms",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=cjqOoN3DGv": {
    "title": "Chaos with Keywords: Exposing Large Language Models Sycophancy to Misleading Keywords and Evaluating Defense Strategies",
    "volume": "review",
    "abstract": "This study explores the sycophantic tendencies of Large Language Models (LLMs), where these models tend to provide answers that match what users want to hear, even if they are not entirely correct. The motivation behind this exploration stems from the common behavior observed in individuals searching the internet for facts with partial or misleading knowledge. Similar to using web search engines, users may recall fragments of misleading keywords and submit them to an LLM, hoping for a comprehensive response. Our empirical analysis of several LLMs shows the potential danger of these models amplifying misinformation when presented with misleading keywords.Additionally, we thoroughly assess four existing hallucination mitigation strategies to reduce LLMs sycophantic behavior. Our experiments demonstrate the effectiveness of these strategies for generating factually correct statements. Furthermore, our analyses delve into knowledge-probing experiments on factual keywords and different categories of sycophancy mitigation",
    "checked": true,
    "id": "e7e92ed8751d83c261821be6d904a86f878a0db8",
    "semantic_title": "chaos with keywords: exposing large language models sycophancy to misleading keywords and evaluating defense strategies",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ngN2SLLh_C1": {
    "title": "AdaInfer: Instance-aware Adaptive Inference for LLMs",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) inference phase is very expensive. An ideal inferenceLLM should utilize fewer computational resources while still maintaining its capabilities in generalization and in-context learning ability. In this paper, we try to answer the question, \"During LLM inference, can we use shallow layers for easy input; deep layers for hard ones?\" To answer this question, we first indicate that Not all Layers are Necessary at inference time by statistically analyzing the activated layers across tasks. Then, we propose a simple algorithm named AdaInfer for instance-aware adaptive inference, which determines the inference termination time based on the input instance itself. More importantly, AdaInfer does not alter LLM parameters and maintains generalizability across tasks. Experiments on well-known LLMs (i.e., Llama2-7B/13B and OPT-13B ) show that AdaInfer can save 10% to 50% of computational resources on mainstream tasks (e.g., knowledge-based/common-sense QA, text classification). Meanwhile, maintaining accuracy with average minimal (<1%) loss. Additionally, this method is orthogonal to other model acceleration techniques (e.g., sparse and flash attention), offering the potential for further enhancing inference efficiency. Code and data is available at Anomynous Github",
    "checked": false,
    "id": "d2421cffac277e230cb97fc2355b32e03dd8bb1f",
    "semantic_title": "exegpt: constraint-aware resource scheduling for llm inference",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=7vszQbmXfaF": {
    "title": "Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation",
    "volume": "review",
    "abstract": "Despite showing impressive abilities, large language models (LLMs) often struggle with factual inaccuracies, i.e., ''hallucinations'', even when they hold relevant knowledge. To mitigate these hallucinations, current approaches typically necessitate high-quality human factuality annotations. In this work, we explore Self-Alignment for Factuality, where we leverage the self-evaluation capability of an LLM to provide training signals that steer the model towards factuality. Specifically, we incorporate Self-Eval, a self-evaluation component, to prompt an LLM to validate the factuality of its own generated responses solely based on its internal knowledge. Additionally, we design Self-Knowledge Tuning (SK-Tuning) to augment the LLM's self-evaluation ability by improving the model's confidence estimation and calibration. We then utilize these self-annotated responses to fine-tune the model via Direct Preference Optimization algorithm. We show that the proposed self-alignment approach substantially enhances factual accuracy over Llama family models across three key knowledge-intensive tasks on TruthfulQA and BioGEN. We will release our code and data upon acceptance",
    "checked": true,
    "id": "32c5b515cab893e5e4bf3f90c8b6c8262bd7ac09",
    "semantic_title": "self-alignment for factuality: mitigating hallucinations in llms via self-evaluation",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=FxU60PxYv8L": {
    "title": "Groot: Adversarial Testing for Generative Text-to-Image Models with Tree-based Semantic Transformation",
    "volume": "review",
    "abstract": "With the prevalence of text-to-image generative models, their safety becomes a critical concern. adversarial testing techniques have been developed to probe whether such models can be prompted to produce Not-Safe-For-Work (NSFW) content. However, existing solutions face several challenges, including low success rate and inefficiency. We introduce Groot, the first automated framework leveraging tree-based semantic transformation for adversarial testing of text-to-image models. Groot employs semantic decomposition and sensitive element drowning strategies in conjunction with LLMs to systematically refine adversarial prompts. Our comprehensive evaluation confirms the efficacy of Groot, which not only exceeds the performance of current state-of-the-art approaches but also achieves a remarkable success rate (93.66%) on leading text-to-image models such as DALL-E 3 and Midjourney",
    "checked": true,
    "id": "91d85905a8e9ae6ba62e562bba32d61c619a8155",
    "semantic_title": "groot: adversarial testing for generative text-to-image models with tree-based semantic transformation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=wsAbZmX0Oq": {
    "title": "Revisiting Catastrophic Forgetting in Large Language Model Tuning",
    "volume": "review",
    "abstract": "Catastrophic Forgetting (CF) means models forgetting previously acquired knowledge when learning on new data. It compromises the effectiveness of large language models (LLMs) during fine-tuning, yet the underlying causes have not been thoroughly investigated. This paper takes the first step to reveal the direct link between the flatness of the model loss landscape and the extent of CF. Based on this, we introduce the sharpness-aware minimization to mitigate CF by flattening the loss landscape. Experiments on three widely-used fine-tuning datasets, spanning different model scales, demonstrate the effectiveness of our method in alleviating CF. Analyses show that we nicely complement the existing anti-forgetting strategies, further enhancing the resistance of LLMs to CF. Code will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TvZjrb8F_K7": {
    "title": "Logic Agent: Enhancing Validity with Logic Rule Invocation",
    "volume": "review",
    "abstract": "Chain-of-Thought (CoT) prompting has emerged as a pivotal technique for augmenting the inferential capabilities of language models during reasoning tasks. Despite its advancements, CoT often grapples with challenges in validating reasoning validity and ensuring informativeness. Addressing these limitations, this paper introduces the Logic Agent (LA), an agent-based framework aimed at enhancing the validity of reasoning processes in Large Language Models (LLMs) through strategic logic rule invocation. Unlike conventional approaches, LA transforms LLMs into logic agents that dynamically apply propositional logic rules, initiating the reasoning process by converting natural language inputs into structured logic forms. The logic agent leverages a comprehensive set of predefined functions to systematically navigate the reasoning process.This methodology not only promotes the structured and coherent generation of reasoning constructs but also significantly improves their interpretability and logical coherence. Through extensive experimentation, we demonstrate LA's capacity to scale effectively across various model sizes, markedly improving the precision of complex reasoning across diverse tasks",
    "checked": true,
    "id": "6cfbe1b1a144bfed725d54f5d131dd52cb3e6126",
    "semantic_title": "logic agent: enhancing validity with logic rule invocation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0pYyDXTnDap": {
    "title": "Centroid-Based Efficient Minimum Bayes Risk Decoding",
    "volume": "review",
    "abstract": "Minimum Bayes risk (MBR) decoding achieved state-of-the-art translation performance by using COMET, a neural metric that has a high correlation with human evaluation.However, MBR decoding requires quadratic time since it computes the expected score between a translation hypothesis and all reference translations.We propose centroid-based MBR (CBMBR) decoding to improve the speed of MBR decoding.Our method clusters the reference translations in the feature space, and then calculates the score using the centroids of each cluster.The experimental results show that our CBMBR not only improved the decoding speed of the expected score calculation 6.9 times, but also outperformed vanilla MBR decoding in translation quality by up to 0.5 COMET in the WMT'22 En$\\leftrightarrow$Ja, En$\\leftrightarrow$De, En$\\leftrightarrow$Zh, and WMT'23 En$\\leftrightarrow$Ja translation tasks",
    "checked": true,
    "id": "86b7e00b186a969a5c412ef21e1a9685192595af",
    "semantic_title": "centroid-based efficient minimum bayes risk decoding",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=ZyT6Vx0kvr": {
    "title": "Find The Gap: Knowledge Base Reasoning For Visual Question Answering",
    "volume": "review",
    "abstract": "We analyze knowledge-based visual question answering, for which given a question, the models need to ground it into the visual modality and retrieve the relevant knowledge from a given large knowledge base (KB) to be able to answer. Our analysis has two folds, one based on designing neural architectures and training them from scratch, and another based on large pre-trained language models (LLMs). Our research questions are: 1) Can we effectively augment models by explicit supervised retrieval of the relevant KB information to solve the KB-VQA problem? 2) How do task-specific and LLM-based models perform in the integration of visual and external knowledge, and multi-hop reasoning over both sources of information? 3) Is the implicit knowledge of LLMs sufficient for KB-VQA and to what extent it can replace the explicit KB? Our results demonstrate the positive impact of empowering task-specific and LLM models with supervised external and visual knowledge retrieval models. Our findings show that though LLMs are stronger in 1-hop reasoning, they suffer in 2-hop reasoning in comparison with our fine-tuned NN model even if the relevant information from both modalities is available to the model. Moreover, we observed that LLM models outperform the NN model for KB-related questions which confirms the effectiveness of implicit knowledge in LLMs however, they do not alleviate the need for external KB",
    "checked": true,
    "id": "a712f22b419623590b5b121278b7709cd1d08a64",
    "semantic_title": "find the gap: knowledge base reasoning for visual question answering",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8PiH1reHvT": {
    "title": "Chain of Logic: Rule-Based Reasoning with Large Language Models",
    "volume": "review",
    "abstract": "Rule-based reasoning, a fundamental type of legal reasoning, enables us to draw conclusions by accurately applying a rule to a set of facts. We explore causal language models as rule-based reasoners, specifically with respect to compositional rules - rules consisting of multiple elements which form a complex logical expression. Reasoning about compositional rules is challenging because it requires multiple reasoning steps, and attending to the logical relationships between elements. We introduce a new prompting method, Chain of Logic, which elicits rule-based reasoning through decomposition (solving elements as independent threads of logic), and recomposition (recombining these sub-answers to resolve the underlying logical expression). This method was inspired by the IRAC (Issue, Rule, Application, Conclusion) framework, a sequential reasoning approach used by lawyers. We evaluate chain of logic across eight rule-based reasoning tasks involving three distinct compositional rules from the LegalBench benchmark and demonstrate it consistently outperforms other prompting methods, including chain of thought and self-ask, using open-source and commercial language models",
    "checked": true,
    "id": "49e9352f4b87efdbf09a5f3048d11cf83016d37d",
    "semantic_title": "chain of logic: rule-based reasoning with large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6IKWMjc_s7": {
    "title": "Tree-of-Counterfactual Prompting for Zero-Shot Stance Detection",
    "volume": "review",
    "abstract": "Stance detection enables the inference of attitudes from human communications. Automatic stance identification was mostly cast as a classification problem. However, stance decisions involve complex judgments, which can be nowadays generated by prompting Large Language Models (LLMs). In this paper we present a new method for stance identification which (1) relies on a new prompting framework, called Tree-of-Counterfactual prompting; (2) operates not only on textual communications, but also on images; (3) allows more than one stance object type; and (4) requires no examples of stance attribution, thus it is a \"Tabula Rasa\" Zero-Shot Stance Detection (TR-ZSSD) method. Our experiments indicate surprisingly promising results, outperforming fine-tuned stance detection systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DMtolBusbk": {
    "title": "OpenLEAF: Benchmarking Open-Domain Interleaved Image-Text Generation",
    "volume": "review",
    "abstract": "We introduce OpenLEAF, a benchmark designed for the open-domain interleaved image-text generation task. This task aims to generate arbitrarily-interleaved multimodal content from input queries. It goes beyond single-modality image or text generation, thereby enabling various novel applications by creating content such as visual storybooks and how-to instructions. Despite the importance of the task, there lacks established benchmark due to the challenges in defining evaluation scenarios and formulating effective metrics. In this study, we collect a dataset covering queries with various input-output formats and $10$ different application scenarios. We also propose an evaluation pipeline named ``detection-summarization-scoring,'' which breaks down the evaluation into multiple reasoning steps. This pipeline leverages large multimodal models (LMMs) to thoroughly evaluate ten aspects of the generated content, which lead to the final rating. With experiments on a proposed agent system, we demonstrate that our evaluation method aligns closely with human judgments, offering a robust benchmark for assessing interleaved image-text generation",
    "checked": false,
    "id": "7f1ba5630c3baa09b11cc665b3f71cdb117e5ffb",
    "semantic_title": "openleaf: open-domain interleaved image-text generation and evaluation",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=Ix6uX6xqYG": {
    "title": "MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition",
    "volume": "review",
    "abstract": "Although Large Language Models (LLMs) have shown strong performance in Multi-hop Question Answering (MHQA) tasks, their real reasoning ability remains exploration. Current LLM QA evaluation benchmarks have shown limitations, including1) data contamination, the evaluation data are potentially exposed to LLMs during the pretraining stage; and 2) ignoration of the reasoning chain evaluation. Thus we introduce an LLM MHQA evaluation benchmark, the first QA benchmark based on the new, unprecedented knowledge by editing the off-the-shelf HotpotQA dataset; Besides, we also annotate and evaluate the reasoning chain in the form of sub-questions and intermediate answers corresponding to the multi-hop questions.Specifically, based on the observation, 1) LLMs show a performance gap between the original HotpotQA and our edited data, deeming that current MHQA benchmarks have the potential risk of data contamination that hard to evaluate LLMs' performance objectively and scientifically; 2) LLMs only get a small percentage of the right reasoning chain, e.g. GPT-4 only gets 36.3\\% right reasoning chain. We believe this new Multi-hop QA evaluation benchmark and novel evaluation methods will facilitate the development of trustworthy LLM evaluation on the MHQA task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b6iUwlR85jM": {
    "title": "The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models",
    "volume": "review",
    "abstract": "Prior work has found that pretrained language models (LMs) fine-tuned with different random seeds can achieve similar in-domain performance but generalize differently on tests of syntactic generalization. In this work, we show that, even within a single model, we can find multiple subnetworks that perform similarly in-domain, but generalize vastly differently. To better understand these phenomena, we investigate if they can be understood in terms of \"competing subnetworks\": the model initially represents a variety of distinct algorithms, corresponding to different subnetworks, and generalization occurs when it ultimately converges to one. This explanation has been used to account for generalization in simple algorithmic tasks. Instead of finding competing subnetworks, we find that all subnetworks---whether they generalize or not---share a set of attention heads, which we refer to as the _heuristic core_. Further analysis suggests that these attention heads emerge early in training and compute shallow, non-generalizing features. The model learns to generalize by incorporating additional attention heads, which depend on the outputs of the \"heuristic\" heads to compute higher-level features. Overall, our results offer a more detailed picture of the mechanisms for syntactic generalization in pre-trained LMs",
    "checked": true,
    "id": "c55c3ae1b7d2c9c81bda9da322e0308b865431e9",
    "semantic_title": "the heuristic core: understanding subnetwork generalization in pretrained language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=WUgiNLVJGgK": {
    "title": "Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding",
    "volume": "review",
    "abstract": "This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters. We propose \\textbf{S}mart \\textbf{P}arallel \\textbf{A}uto-\\textbf{C}orrect d\\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of LLMs. By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification. This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaining output quality",
    "checked": true,
    "id": "26128d9ec47f044de6f0186f86c0c7fb3ccef77f",
    "semantic_title": "generation meets verification: accelerating large language model inference with smart parallel auto-correct decoding",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=vOqY1TqNYM": {
    "title": "Continual Few-shot Relation Extraction via Adaptive Gradient Correction and Knowledge Decomposition",
    "volume": "review",
    "abstract": "Continual few-shot relation extraction (CFRE) aims to continually learn new relations with limited samples. However, current methods neglect the instability of embeddings in the process of different task training, which leads to serious catastrophic forgetting. In this paper, we propose the concept of the following degree from the perspective of instability to analyze catastrophic forgetting and design a novel method based on adaptive gradient correction and knowledge decomposition to alleviate catastrophic forgetting. Specifically, the adaptive gradient correction algorithm is designed to limit the instability of embeddings, which adaptively constrains the current gradient to be orthogonal to the embedding space learned from previous tasks. To reduce the instability between samples and prototypes, the knowledge decomposition module decomposes knowledge into general and task-related knowledge from the perspective of model architecture, which is asynchronously optimized during training. Experimental results on two standard benchmarks show that our method outperforms the state-of-the-art CFRE model and effectively improves the following degree of embeddings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W6jR3wGtUV": {
    "title": "On the Language Encoder of Contrastive Cross-modal Models",
    "volume": "review",
    "abstract": "Contrastive cross-modal models such as CLIP and CLAP aid various vision-language (VL) and audio-language (AL) tasks. However, there has been limited investigation of and improvement in their language encoder -- the central component of encoding natural language descriptions of image/audio into vector representations. We extensively evaluate how unsupervised and supervised sentence embedding training affect language encoder quality and cross-modal task performance. In VL pretraining, we found that sentence embedding training enhances language encoder quality and aids in cross-modal tasks, improving contrastive VL models such as CyCLIP. Sentence embedding training benefits AL tasks when the amount of training data is large. We analyze the representation spaces to understand the strengths of sentence embedding training, and find that it improves text-space uniformity, at the cost of decreased cross-modal alignment",
    "checked": true,
    "id": "b99f69fa9b7424de2aecf27dc3a01e0b0dfcd3b5",
    "semantic_title": "on the language encoder of contrastive cross-modal models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_k40np6MHH3": {
    "title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
    "volume": "review",
    "abstract": "Recently, large language models (LLMs), including notable models such as GPT-4 and burgeoning community models, have showcased significant general language understanding abilities. However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding. To encourage further investigation in this area, we introduce GLoRE, a meticulously assembled General Logical Reasoning Evaluation benchmark comprised of 12 datasets that span three different types of tasks. Our experimental results show that compared to the performance of human and supervised fine-tuning, the logical reasoning capabilities of open LLM models necessitate additional improvement; ChatGPT and GPT-4 show a strong capability of logical reasoning, with GPT-4 surpassing ChatGPT by a large margin. We propose a self-consistency probing method to enhance the accuracy of ChatGPT and a fine-tuned method to boost the performance of an open LLM.We release the datasets and evaluation programs to facilitate future research",
    "checked": true,
    "id": "806b5882c983bd156a8c10bcd34fe285d8a0593b",
    "semantic_title": "glore: evaluating logical reasoning of large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=qvhwEKlPbx7": {
    "title": "ACUEval: Fine-grained Hallucination Evaluation and Correction for Abstractive Summarization",
    "volume": "review",
    "abstract": "The impressive generation capabilities of large language models (LLMs) have made it even harder to detect the subtle hallucinations they make in abstractive summarization, where generated summaries consist of a blend of correct and incorrect information w.r.t. a given document. Recently-proposed LLM-based evaluation metrics attempt to capture this, but still face challenges: (1) they are biased towards summaries generated from the same underlying LLM, and (2) they lack interpretability, offering only a single score. In this work, we present ACUEval, a metric that leverages the power of LLMs to perform two sub-tasks: decomposing summaries into atomic content units (ACUs), and validating them against the source document. Compared to current strong LLM-based metrics, our two-step evaluation strategy improves correlation with human judgments of faithfulness on three summarization evaluation benchmarks by 3% in balanced accuracy compared to the next-best metric, and also shows reduced preference bias towards LLMgenerated summary (by operating with fine-grained units). Further, we show that errors detected by ACUEVAL can be used to generate actionable feedback for refining the summary, successfully improving the faithfulness scores by more than 10%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TEMkUXRz4DW": {
    "title": "Exploring Memorization in Fine-tuned Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) have shown great capabilities in various tasks but also exhibited memorization of training data, raising tremendous privacy and copyright concerns. While prior works have studied memorization during pre-training, the exploration of memorization during fine-tuning is rather limited. Compared to pre-training, fine-tuning typically involves more sensitive data and diverse objectives, thus may bring distinct privacy risks and unique memorization behaviors. In this work, we conduct the first comprehensive analysis to explore language models' (LMs) memorization during fine-tuning across tasks. Our studies with open-sourced and our own fine-tuned LMs across various tasks indicate that memorization presents a strong disparity among different fine-tuning tasks. We provide an intuitive explanation of this task disparity via sparse coding theory and unveil a strong correlation between memorization and attention score distribution",
    "checked": true,
    "id": "8353270c28a542735c1ce8af2ff998146b620844",
    "semantic_title": "exploring memorization in fine-tuned language models",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=AOKjbA5NbUH": {
    "title": "Steering Conversational Large Language Models for Long Emotional Support Conversations",
    "volume": "review",
    "abstract": "In this study, we address the challenge of consistently following emotional support strategies in long conversations by large language models (LLMs). We introduce the Strategy-Relevant Attention (SRA) metric, a model-agnostic measure designed to evaluate the effectiveness of LLMs in adhering to strategic prompts in emotional support contexts. By analyzing conversations within the Emotional Support Conversations dataset (ESConv) using LLaMA models, we demonstrate that SRA is significantly correlated with a model's ability to sustain the outlined strategy throughout the interactions. Our findings reveal that the application of SRA-informed prompts leads to enhanced strategic adherence, resulting in conversations that more reliably exhibit the desired emotional support strategies over longer conversations. Furthermore, we contribute a comprehensive, multi-branch synthetic conversation dataset for ESConv, featuring a variety of strategy continuations informed by our optimized prompting method. The code and data are publicly available on our github",
    "checked": true,
    "id": "748cd278c79118d594c3f926f271390f7458b20a",
    "semantic_title": "steering conversational large language models for long emotional support conversations",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=W1li7gbQEx": {
    "title": "Large Language Models are Few-Shot Training Example Generators: A Case Study in Fallacy Recognition",
    "volume": "review",
    "abstract": "Recognizing fallacies is crucial for ensuring the quality and validity of arguments across various domains. However, computational fallacy recognition faces challenges due to the diverse genres, domains, and types of fallacies found in datasets. This leads to a highly multi-class, and even multi-label, setup with substantial class imbalance. In this study, we aim to enhance existing models for fallacy recognition by incorporating additional context and by leveraging large language models to generate synthetic data, thus increasing the representation of the infrequent classes. We experiment with GPT3.5 to generate synthetic examples and we examine the impact of prompt settings for this. Moreover, we explore zero-shot and few-shot scenarios to evaluate the effectiveness of using the generated examples for training smaller models within a unified fallacy recognition framework. Furthermore, we analyze the overlap between the synthetic data and existing fallacy datasets. Finally, we investigate the usefulness of providing supplementary context for detecting fallacy types that need such context, e.g., diversion fallacies. Our evaluation results demonstrate consistent improvements across fallacy types, datasets, and generators. We will release the code and synthetic dataset upon the acceptance of the paper",
    "checked": true,
    "id": "4e0f5af6ee4a4d8f59b170263c5f157bef3e5d34",
    "semantic_title": "large language models are few-shot training example generators: a case study in fallacy recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=FNK4Fcb9lqu": {
    "title": "Decomposed Prompting: Unveiling Multilingual Linguistic Structure Knowledge in English-Centric Large Language Models",
    "volume": "review",
    "abstract": "Despite the predominance of English in their training, English-centric Large Language Models (LLMs) like GPT-3 and LLaMA display a remarkable ability to perform multilingual tasks, raising questions about the depth and nature of their cross-lingual capabilities. This paper introduces a novel decomposed prompting approach to probe the linguistic structure understanding of these LLMs within sequence labeling tasks. Diverging from the single text-to-text prompt, our method breaks down the input sentence into individual prompts for each token, seeking its linguistic label. We assess our method on the Universal Dependencies part-of-speech tagging dataset for 38 languages, utilizing both English-centric and multilingual LLMs. Our findings show that decomposed prompting surpasses the iterative prompting baseline in efficacy and efficiency under zero- and few-shot settings. Further analysis reveals the influence of evaluation methods and instructional prompt use. Our study offers insights into the multilingual transferability of English-centric LLMs, contributing to the understanding of their multilingual linguistic knowledge",
    "checked": true,
    "id": "ee414ff78922ac70dfb31abfff37bd40c661ac92",
    "semantic_title": "decomposed prompting: unveiling multilingual linguistic structure knowledge in english-centric large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=AMEMHJd75F": {
    "title": "Using Natural Language Explanations to Improve Robustness of In-context Learning",
    "volume": "review",
    "abstract": "Recent studies demonstrated that large language models (LLMs) can excel in many tasks via in-context learning (ICL). However, recent works show that ICL-trained models tend to produce inaccurate results when presented with adversarial inputs. In this work, we investigate whether augmenting ICL with natural language explanations (NLEs) improves the robustness of LLMs on adversarial datasets covering natural language inference and paraphrasing identification. We prompt LLMs with a small set of human-generated NLEs to produce further NLEs, yielding more accurate results than both a zero-shot-ICL setting and using only human-generated NLEs. Our results on five popular LLMs (GPT3.5-turbo, LLaMA2, Vicuna, Zephyr, and Mistral) show that our approach yields over 6% improvement over base-line approaches for eight adversarial datasets: HANS, ISCS, NaN, ST, PICD, PISP, ANLI, and PAWS. Furthermore, previous studies have demonstrated that prompt selection strategies significantly enhance ICL on in-distribution test sets. However, our findings reveal that these strategies do not match the efficacy of our approach for robustness evaluations, resulting in an accuracy drop of 8% compared to the proposed approach",
    "checked": false,
    "id": "2e7cc95145665bae4fa98b7f81b9d551f1b1c021",
    "semantic_title": "using natural language explanations to improve robustness of in-context learning for natural language inference",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dF-G5io7rHF": {
    "title": "Investigating Cultural Alignment of Large Language Models",
    "volume": "review",
    "abstract": "The intricate relationship between language and culture has long been a subject of exploration within the realm of linguistic anthropology. Large Language Models (LLMs), promoted as repositories of collective human knowledge, raise a pivotal question: do these models genuinely encapsulate the diverse knowledge adopted by different cultures? Our study reveals that these models demonstrate greater cultural alignment along two dimensionsâ€”firstly, when prompted with the dominant language of a specific culture, and secondly, when pretrained with a refined mixture of languages employed by that culture. We quantify cultural alignment by simulating sociological surveys, comparing model responses to those of actual survey participants as references. Specifically, we replicate a survey conducted in various regions of Egypt and the United States through prompting LLMs with different pretraining data mixtures in both Arabic and English with the personas of the real respondents and the survey questions. Further analysis reveals that misalignment becomes more pronounced for underrepresented personas and for culturally sensitive topics, such as those probing social values. Finally, we introduce Anthropological Prompting, a novel method leveraging anthropological reasoning to enhance cultural alignment. Our study emphasizes the necessity for a more balanced multilingual pretraining dataset to better represent the diversity of human experience and the plurality of different cultures with many implications on the topic of cross-lingual transfer",
    "checked": true,
    "id": "b1890367317f0657c08ed96be4c474035b34b485",
    "semantic_title": "investigating cultural alignment of large language models",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=62A7acAmaa": {
    "title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding",
    "volume": "review",
    "abstract": "As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with human values, including safety. Jailbreak attacks, which aim to provoke unintended and unsafe behaviors from LLMs, remain a significant LLM safety threat. We analyze tokens, which are the smallest unit of text that can be processed by LLMs and make the following observations: (1) probabilities of tokens representing harmful responses are higher than those of harmless responses, and (2) responses containing safety disclaimers appear among the top tokens when token probabilities are sorted in descending order. In this paper, we leverage (1) and (2) to develop SafeDecoding, a safety-aware decoding strategy for LLMs, to defend against jailbreak attacks. We perform extensive experiments to evaluate SafeDecoding against six SOTA jailbreak attacks (GCG, AutoDAN, PAIR, DeepInception, SAP30, and template based attack) on five LLMs (Vicuna, Llama2, Guanaco, falcon, and Dolphin) using four benchmark datasets (AdvBench, HEx-PHI, MT-Bench, and Just-Eval). Our results show that SafeDecoding significantly reduces attack success rate and harmfulness of jailbreak attacks without compromising the helpfulness of responses to benign user queries while outperforming six defense methods (Perpelexity, Paraphrase, Retokenization, Self-Reminder, ICD, and Self-Examination)",
    "checked": true,
    "id": "2139e414bdf6a5ea7ec4052d3f65a8d49991494b",
    "semantic_title": "safedecoding: defending against jailbreak attacks via safety-aware decoding",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=jUEn0vFI0yD": {
    "title": "Labeled Interactive Neural Topic Models: No Longer Take It or Leave It",
    "volume": "review",
    "abstract": "Topic models help understand document collections, but they don't always identify the most relevant topics. Classical probabilistic and anchor-based topic models offer interactive versions that allow users to guide the models towards better topics. However, such interactive features have been lacking in neural topic models. To correct this lacuna, we introduce a user-friendly interaction for neural topic models. This interaction permits users to assign a word label to a topic, leading to an update in the topic model where the words in the topic become closely aligned with the given label. Our approach encompasses two distinct kinds of neural topic models. The first includes models where topic embeddings are trainable and evolve during the training process. The second involves models where topic embeddings are integrated post-training. To facilitate user interaction with these neural topic models, we have developed an interactive interface that enables users to engage with and re-label topics. We evaluate our method through a human study, where users can relabel topics to find relevant documents. Using our method, user labeling improves document rank scores, helping to find more relevant documents to a given query when compared to no user labeling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9m4C1E-Tf0U": {
    "title": "Introducing Compiler Semantics into Large Language Models as Programming Language Translators: A Case Study of C to x86 Assembly",
    "volume": "review",
    "abstract": "Compilers are complex software containing millions of lines of code, taking years to develop. This paper investigates to what extent Large Language Models (LLMs) can replace hand-crafted compilers in translating high-level programming languages to machine instructions, using C to x86 assembly as a case study. We identify two challenges of using LLMs for code translation and introduce two novel data pre-processing techniques to address the challenges: numerical value conversion and training data resampling. While only using a 13B model, our approach achieves a behavioral accuracy of over 91%, outperforming the much larger GPT-4 Turbo model by over 50%. Our results are encouraging, showing that LLMs have the potential to transform how compilation tools are constructed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lGjTZh_qruG": {
    "title": "An Information-Theoretic Approach to Analyze NLP Classification Tasks",
    "volume": "review",
    "abstract": "Understanding the contribution of the inputs on the output is useful across many tasks. This work provides an information-theoretic framework to analyse the influence of inputs for text classification tasks. Natural language processing (NLP) tasks take either a single or multiple text elements to predict an output variable. Each text element has two components: the semantic meaning and a linguistic realization. Multiple-choice reading comprehension (MCRC) and sentiment classification (SC) are selected to showcase the framework. For MCRC, it is found that the relative context influence on the output reduces on more challenging datasets. In particular, more challenging contexts allows greater variation in the question complexity. Hence, test creators need to carefully consider the choice of the context when designing multiple-choice questions for assessment. For SC, it is found the semantic meaning of the input dominates compared to its linguistic realization when determining the sentiment",
    "checked": true,
    "id": "ec46aecae2e3f2c785e9423cf9dfe6f7832b3845",
    "semantic_title": "an information-theoretic approach to analyze nlp classification tasks",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=X7QsMr9axbW": {
    "title": "BinaryAlign: Word Alignment as Binary Sequence Labeling",
    "volume": "review",
    "abstract": "Real world deployments of word alignment are almost certain to cover both high and low resource languages. However, the state-of-the-art for this task recommends a different model class depending on the availability of gold alignment training data for a particular language pair. We propose BinaryAlign, a novel word alignment technique based on binary sequence labeling that outperforms existing approaches in both scenarios, offering a unifying approach to the task. Additionally, we vary the specific choice of multilingual foundation model, perform stratified error analysis over alignment error type, and explore the performance of BinaryAlign on non-English language pairs. We make our source code publicly available",
    "checked": true,
    "id": "a128556e6f998a8e9411131a44c96d0996120e03",
    "semantic_title": "binaryalign: word alignment as binary sequence labeling",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=oWNCZ8LKHCI": {
    "title": "Reasoning or a Semblance of it? A Diagnostic Study of Factual Reasoning in LLMs",
    "volume": "review",
    "abstract": "There's currently a lot of interest in evaluating the reasoning capabilities of Large Language Models (LLM) using benchmarks designed to evaluate them on different kinds of reasoning capacity. In this paper, we begin with scepticism about whether model performance on a reasoning benchmark is a real index of the model's capacity to reason. To study this, we first use two prototypical LLMs, LLaMA 2 & Flan-T5, to establish baselines within the QASC and Bamboogle benchmarks: instances of Compositional Question Answering. We then design and carry out a series of novel, diagnostic experiments using automatic and manual re-annotations of the datasets to control for the different sources of information the LLMs might be exploiting in answering compositional questions. Specifically, our experiments control for: (a) word/phrase associations or overlaps across sections of the model's input prompt; (b) the model's inherent knowledge acquired during (pre)training and/or fine-tuning; and (c) semantic type dependencies in QA pairs. Our experiments show that LLaMA 2 excels at exploiting (a-c). On the other hand, Flan-T5 exhibits a lot more robustness to our interference with them, despite relying on answer keywords in some experiments; indicating that Flan-T5 is indeed doing something akin to reasoning. We hypothesise that this is due to Flan-T5's direct fine-tuning on reasoning datasets, but leave the testing of this hypothesis to future work",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iUx6ye5I7tN": {
    "title": "TransportationGames: Benchmarking Transportation Knowledge of (Multimodal) Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) and multimodal large language models (MLLMs) have shown excellent general capabilities, even exhibiting adaptability in many professional domains such as law, economics, transportation, and medicine. Currently, many domain-specific benchmarks have been proposed to verify the performance of (M)LLMs in specific fields. Among various domains, transportation plays a crucial role in modern society as it impacts the economy, the environment, and the quality of life for billions of people. However, it is unclear how much traffic knowledge (M)LLMs possess and whether they can reliably perform transportation-related tasks. To address this gap, we propose TransportationGames, a carefully designed and thorough evaluation benchmark for assessing (M)LLMs in the transportation domain. By comprehensively considering the applications in real-world scenarios and referring to the first three levels in Bloom's Taxonomy, we test the performance of various (M)LLMs in memorizing, understanding, and applying transportation knowledge by the selected tasks. The experimental results show that although some models perform well in some tasks, there is still much room for improvement overall. We hope the release of TransportationGames can serve as a foundation for future research, thereby accelerating the implementation and application of (M)LLMs in the transportation domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=if1YzMK9sE5": {
    "title": "Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations",
    "volume": "review",
    "abstract": "Incorporating natural language rationales in the prompt and In-Context Learning (ICL) has led to a significant improvement of Large Language Models (LLMs) performance. However, rationales currently require human-annotation or the use of auxilliary proxy models to target promising samples or generate high-quality rationales. In this work, we propose Self-AMPLIFY to automatically generate rationales from post hoc explanation methods applied to Small Language Models (SLMs) to improve their own performance. Self-AMPLIFY is a 3-step method that targets samples, generates rationales and builds a final prompt to leverage ICL. Self-AMPLIFY performance is evaluated on two SLMs and two datasets requiring reasoning abilities: these experiments show that it achieves good results against competitors. Self-AMPLIFY is the first method to apply post hoc explanation methods to SLM to generate rationales to improve their own performance in a fully automated manner",
    "checked": true,
    "id": "0a47b95ee9e3fae6f2c8742b39dc6db4e485b242",
    "semantic_title": "self-amplify: improving small language models with self post hoc explanations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uw2uIAL4Zjt": {
    "title": "INTERVENOR: Prompting the Coding Ability of Large Language Models with the Interactive Chain of Repair",
    "volume": "review",
    "abstract": "This paper introduces INTERVENOR (INTERactiVE chaiN Of Repair), a system designed to emulate the interactive code repair processes observed in humans, encompassing both code diagnosis and code repair. INTERVENOR prompts Large Language Models (LLMs) to play distinct roles during the code repair process, functioning as both a Code Learner and a Code Teacher. Specifically, the Code Learner is tasked with adhering to instructions to generate or repair code, while the Code Teacher is responsible for crafting a Chain-of-Repair (CoR) to serve as guidance for the Code Learner. During generating the CoR, the Code Learner needs to check the generated codes from Code Learner and reassess how to address code bugs based on error feedback received from compilers. Experimental results demonstrate that INTERVENOR surpasses baseline models, exhibiting improvements of approximately 18% and 4.3% over GPT-3.5 in code generation and code translation tasks, respectively. Our further analyses show that CoR is effective to illuminate the reasons behind bugs and outline solution plans in natural language. With the feedback of code compilers, INTERVENOR can accurately identify syntax errors and assertion errors and provide precise instructions to repair codes. All data and codes will be released via GitHub",
    "checked": true,
    "id": "c16def288e2704ef78023ba1802b699069d4d4c7",
    "semantic_title": "intervenor: prompting the coding ability of large language models with the interactive chain of repair",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JgsZZ-HBW68": {
    "title": "M3-VRD: Multimodal Multi-task Multi-teacher Visually-Rich Form Document Understanding",
    "volume": "review",
    "abstract": "This paper presents a groundbreaking multimodal, multi-task, multi-teacher joint-grained knowledge distillation model for visually-rich form document understanding. The model is designed to leverage insights from both fine-grained and coarse-grained levels by facilitating a nuanced correlation between token and entity representations, addressing the complexities inherent in form documents. Additionally, we introduce new inter-grained and cross-grained loss functions to further refine diverse multi-teacher knowledge distillation transfer process, presenting distribution gaps and a harmonised understanding of form documents. Through a comprehensive evaluation across publicly available form document understanding datasets, our proposed model consistently outperforms existing baselines, showcasing its efficacy in handling the intricate structures and content of visually complex form documents",
    "checked": true,
    "id": "3a55568a1d1a7e37faad1ca4e259a98e3f97027f",
    "semantic_title": "m3-vrd: multimodal multi-task multi-teacher visually-rich form document understanding",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=GOmKGNGMojY": {
    "title": "M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection",
    "volume": "review",
    "abstract": "The advent of Large Language Models (LLMs) has brought an unprecedented surge in machine-generated text (MGT) across diverse channels. This raises legitimate concerns about its potential misuse and societal implications. The need to identify and differentiate such content from genuine human-generated text is critical in combating disinformation, preserving the integrity of education and scientific fields, and maintaining trust in communication. In this work, we address this problem by introducing a new benchmark involving multilingual, multi-domain and multi-generator for MGT detection --- M4GT-Bench. It is collected for three task formulations: (1) mono-lingual and multi-lingual binary MGT detection; (2) multi-way detection identifies which particular model generates the text; and (3) human-machine mixed text detection, where a word boundary delimiting MGT from human-written content should be determined. Human evaluation for Task 2 shows less than random guess performance, demonstrating the challenges to distinguish unique LLMs. Promising results always occur when training and test data distribute within the same domain or generators",
    "checked": true,
    "id": "c59628de894a4aa7f91548bad5b4103b747256e8",
    "semantic_title": "m4gt-bench: evaluation benchmark for black-box machine-generated text detection",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=IWfIqI1tRkz": {
    "title": "Event-Radar: Event-driven Multi-View Learning for Multimodal Fake News Detection",
    "volume": "review",
    "abstract": "The swift detection of multimedia fake news has emerged as a crucial task in combating malicious propaganda and safeguarding the security of the online environment. While existing methods have achieved commendable results in modeling entity-level inconsistency, addressing event-level inconsistency following the inherent subject-predicate logic of news and robustly learning news representations from poor-quality news samples remain two challenges. In this paper, we propose an Event-diven fake news detection framework (Event-Radar) based on multi-view learning, which integrates visual manipulation, textual emotion and multimodal inconsistency at event-level for fake news detection. Specifically, leveraging the capability of graph structures to capture interactions between events and parameters, Event-Radar captures event-level multimodal inconsistency by constructing an event graph that includes multimodal entity subject-predicate logic. Additionally, to mitigate the interference of poor-quality news, Event-Radar introduces a multi-view fusion mechanism, learning comprehensive and robust representations by computing the credibility of each view as a clue, thereby detecting fake news. Extensive experiments demonstrate that Event-Radar achieves outstanding performance on three large-scale fake news detection benchmarks. Our studies also confirm that Event-Radar exhibits strong robustness, providing a paradigm for detecting fake news from noisy news samples",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F15HIr_oiKG": {
    "title": "Manual Verbalizer Enrichment for Few-Shot Text Classification",
    "volume": "review",
    "abstract": "With the emerging and continuous development of pre-trained language models, prompt-based training has become a well-adopted paradigm that drastically improves the exploitation of models for many NLP tasks. Prompting also shows great performance compared to traditional finetuning when adapted to zero-shot or few-shot scenarios where the number of annotated data is limited. In this framework, verbalizers play an important role in interpreting masked word distributions produced by language models into output predictions. In this work, we propose MaVEN, a new approach for verbalizer construction by enrichment of class labels using neighborhood relation in the embedding space of words. In addition, we elaborate a benchmarking procedure to evaluate typical baselines of verbalizers for document classification in few-shot learning contexts. Our model achieves state-of-the-art results while using significantly fewer resources. We show that our approach is particularly effective in cases with extremely limited supervision data. Our code is available at {https://anonymous.4open.science/r/verbalizer_benchmark-66E6}",
    "checked": false,
    "id": "025c282a159ae387f933536d1a919834580d7895",
    "semantic_title": "label-aware automatic verbalizer for few-shot text classification",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=UUvlo-5tkS": {
    "title": "Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs",
    "volume": "review",
    "abstract": "Recent advances in large language models (LLM) have enabled richer social simulations, allowing for the study of various social phenomena with LLM-based agents. However, most work has used an omniscient perspective on these simulations (e.g., single LLM to generate all interlocutors), which is fundamentally at odds with the non-omniscient, information asymmetric interactions that humans have. To examine these differences, we develop an evaluation framework to simulate social interactions with LLMs in various settings (omniscient, non-omniscient). Our experiments show that interlocutors simulated omnisciently are much more successful at accomplishing social goals compared to non-omniscient agents, despite the latter being the more realistic setting. Furthermore, we demonstrate that learning from omniscient simulations improves the apparent naturalness of interactions but scarcely enhances goal achievement in cooperative scenarios. Our findings indicate that addressing information asymmetry remains a fundamental challenge for LLM-based agents",
    "checked": true,
    "id": "bfc2aee63d20fb19c9a851da9e97fec40c454124",
    "semantic_title": "is this the real life? is this just fantasy? the misleading success of simulating social interactions with llms",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=NP9QDxyzN-W": {
    "title": "Leveraging Machine-Generated Rationales to Facilitate Social Meaning Detection in Conversations",
    "volume": "review",
    "abstract": "We present a generalizable classification approach that leverages Large Language Models (LLMs) to facilitate the detection of implicitly encoded social meaning in conversations. We design a multi-faceted prompt to extract a textual explanation of the reasoning that connects visible cues to underlying social meanings. These extracted explanations or rationales serve as augmentations to the conversational text to facilitate dialogue understanding and transfer. Our empirical results over 2340 experimental settings demonstrate the significant positive impact of adding these rationales. Our findings hold true for in-domain classification and zero-shot and few-shot domain transfer for two different social meaning detection tasks, each spanning two different corpora",
    "checked": true,
    "id": "032b6040bacdcbf7c1a416faf0c225f9dc97dd76",
    "semantic_title": "leveraging machine-generated rationales to facilitate social meaning detection in conversations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OJ0E6A5x96q": {
    "title": "Definition generation for lexical semantic change detection",
    "volume": "review",
    "abstract": "We use contextualized word definitions generated by large language model as semantic representations in the task of diachronic lexical semantic change detection (LSCD). In short, generated definitions are used as `senses', and the change score of a target word is retrieved by comparing their distributions in two time periods under comparison. On the material of five datasets and three languages, we show that generated definitions are indeed specific and general enough to convey a signal sufficient to rank sets of words by the degree of their semantic change over time. Our approach is on par with or outperforms prior non-supervised sense-based LSCD methods. At the same time, it preserves interpretability and allows to inspect the reasons behind a specific shift in terms of discrete definitions-as-senses. This is another step in the direction of explainable semantic change modeling",
    "checked": true,
    "id": "7dd590cd978431ffe9fe5356f8cf7e591ff6ca6c",
    "semantic_title": "definition generation for lexical semantic change detection",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Fn5rfHhjoA": {
    "title": "A Chinese Dataset for Evaluating the Safeguards in Large Language Models",
    "volume": "review",
    "abstract": "Many studies have demonstrated that large language models (LLMs) can produce harmful responses, exposing users to unexpected risks when LLMs are deployed. Previous studies have proposed comprehensive taxonomies of the risks posed by LLMs, as well as corresponding prompts that can be used to examine the safety mechanisms of LLMs. However, the focus has been almost exclusively on English, and little has been explored for other languages. Here we aim to bridge this gap. We first introduce a dataset for the safety evaluation of Chinese LLMs, and then extend it to two other scenarios that can be used to better identify false negative and false positive examples in terms of risky prompt rejections. We further present a set of fine-grained safety assessment criteria for each risk type, facilitating both manual annotation and automatic evaluation in terms of LLM response harmfulness. Our experiments on five LLMs show that region-specific risks are the prevalent type of risk, presenting the major issue with all Chinese LLMs we experimented with. Warning: this paper contains example data that may be offensive, harmful, or biased",
    "checked": true,
    "id": "2c6638f6c817b7dc94365e217138d5b60cc699fa",
    "semantic_title": "a chinese dataset for evaluating the safeguards in large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=pIP6dk9YMaW": {
    "title": "Disentangle to Decay: Linear Attention with Trainable Positional Decay for Length Extrapolation",
    "volume": "review",
    "abstract": "Transformer architecture has significantly advanced Natural Language Processing (NLP) by delivering outstanding performance. However, it faces challenges with efficiency and processing long sequences, attributed to its quadratic time complexity. Linear attention offers a more efficient linear time solution but falls short in language modelling and length extrapolation compared with traditional Transformer. To enhance the performance of linear attention and fully leverage its capability in modelling long sequences, we begin with positional encoding, specifying the constraints required for positional encoding by linear attention. Building upon these constraints, we design a positional encoding for linear attention, named Disentangle to Decay (D2D), which allows for a seamless conversion between absolute positional encoding (APE) and relative positional encoding (RPE). To alleviate the instability of directly training D2D, we disentangle D2D into the combination of RPE and APE, which greatly improves the stability while ensuring the efficiency of model training. Experiments result shows that, application of D2D in linear attention significantly improves performance in language modelling and length extrapolation, demonstrating strong competitiveness with vanilla Transformer and outperforming other positional encodings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8fBx-N_WWNy": {
    "title": "IAO prompting: Forcing Large Language Models to Show their Reasoning through an Input-Action-Output Template",
    "volume": "review",
    "abstract": "The effectiveness of Large Language Models (LLMs) in tackling diverse reasoning problems is further improved by chain-of-thought prompting, which makes explicit the intermediate reasoning steps. Additionally, recent research has proved the importance of explicitly structuring the reasoning procedure. In this work, we introduce IAO (input-action-output) prompting, a straightforward template based prompting method that allows the complex reasoning process to be explicitly modelled in a structured manner.IAO autonomously breaks down problems into a series of simpler reasoning steps and then solves them in sequence, each with explicit input information, action applied, and intermediate output. The solved steps inform the subsequent steps, facilitating progressive reasoning. This explicit structure not only amplifies reasoning performance but also fosters enhanced interpretability and transparency. Extensive experiments across various reasoning tasks demonstrate IAO's strong zero-shot capabilities, showcasing its effectiveness in unlocking and leveraging the true power of LLM reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5r836doBa-": {
    "title": "Can Large Language Models perform Relation-based Argument Mining?",
    "volume": "review",
    "abstract": "Argument mining (AM) is the process of automatically extracting arguments, their components and/or relations amongst arguments and components from text. As the number of platforms supporting online debate increases, the need for AM becomes ever more urgent, especially in support of downstream tasks. Relation-based AM (RbAM) is a form of AM focusing on identifying agreement (support) and disagreement (attack) relations amongst arguments. RbAM is a challenging classification task, with existing methods failing to perform satisfactorily. In this paper, we show that general-purpose Large Language Models (LLMs), appropriately primed and prompted, can significantly outperform the best performing (RoBERTa-based) baseline. Specifically, we experiment with two open-source LLMs (Llama-2 and Mistral) with ten datasets",
    "checked": true,
    "id": "f38b2f1a5280bd2f8e0a7c754782843fb4cb28f6",
    "semantic_title": "can large language models perform relation-based argument mining?",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=wHdZd_QY8QH": {
    "title": "LLM can Self-Regulation via Hyperparameter Aware Generation",
    "volume": "review",
    "abstract": "In the realm of Large Language Models (LLMs), users commonly employ diverse decoding strategies and adjust hyperparameters to control the generated text. However, a critical question emerges: Are LLMs conscious of the existence of these decoding strategies and capable of regulating themselves? The current decoding generation process often relies on empirical and heuristic manual adjustments to hyperparameters based on types of tasks and demands. However, this process is typically cumbersome, and the decoding hyperparameters may not always be optimal for each sample. To address the aforementioned challenges, we propose a novel text generation paradigm termed Hyperparameter Aware Generation (HAG). By leveraging hyperparameter-aware instruction tuning, the LLM autonomously determines the optimal decoding strategy and configs based on the input samples, enabling self-regulation. Our approach eliminates the need for extensive manual tuning, offering a more autonomous, self-regulate model behavior. Experimental results spanning six datasets across reasoning, creativity, translation, and mathematics tasks demonstrate that hyperparameter-aware instruction tuning empowers the LLMs to self-regulate the decoding strategy and hyperparameter. HAG extends the current paradigm in the text generation process, highlighting the feasibility of endowing the LLMs with self-regulate decoding strategies",
    "checked": false,
    "id": "ae8f9f02b090351543e5e0ea945e2993b27d9d42",
    "semantic_title": "llm can achieve self-regulation via hyperparameter aware generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q_u2Hx4FmM3": {
    "title": "Language Models can Evaluate Themselves via Probability Discrepancy",
    "volume": "review",
    "abstract": "In this paper, we begin by illustrating that, when presented with a query, Large Language Models (LLMs) capable of providing accurate responses tend to exhibit a more uniform probability distribution compared to their less proficient counterparts. Building upon this observation, we introduce a novel self-assessment criterion termed ProbDiff for evaluating the performance of diverse LLMs. This method eliminates the need for training an additional evaluation model or relying on external proprietary models such as GPT-4 as a judger. Instead, it solely relies on the LLMs under evaluation to compute the probability discrepancy between the original response generation and its revised versions. A higher discrepancy in two LLMs for the same query suggests a relatively weaker ability. We discover that ProbDiff yields comparable results to mainstream GPT-4-based evaluations on various scenarios including NLG tasks like translation and summarization, as well as LLM evaluation benchmarks such as AlignBench, MT-Bench, and AlpacaEval, across LLMs of different sizes",
    "checked": true,
    "id": "3c9cb3004ea53d74b85bbb40bb2c4148979880ad",
    "semantic_title": "language models can evaluate themselves via probability discrepancy",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qDbEonm-mr": {
    "title": "Neural Methods for Aligning Large-Scale Parallel Corpora from the Web for Southeast Asian Languages",
    "volume": "review",
    "abstract": "We introduce neural methods to the hierarchical web mining approach of Paracrawl, showing large improvements. We apply these methods to web-scale parallel corpus mining for 9 South/Southeast/East Asian national languages, creating training resources for machine translation that yield better translation quality for most of these languages than existing publicly available datasets in OPUS, including CC-Matrix",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Drd3QXuvIh": {
    "title": "A Usage-centric Take on Intent Understanding in E-Commerce",
    "volume": "review",
    "abstract": "Identifying and understanding user intents is a pivotal task for E-Commerce. Despite its popularity, intent understanding has not been consistently defined or accurately benchmarked. In this paper, we focus on predictive user intents as \"how a customer uses a product\", and pose intent understanding as a natural language reasoning task, independent of product ontologies. We identify two weaknesses of FolkScope, the SOTA E-Commerce Intent Knowledge Graph, that limit its capacity to reason about user intents and to recommend diverse useful products. Following these observations, we introduce a Product Recovery Benchmark including a novel evaluation framework and an example dataset. We further justify the above FolkScope weaknesses on this benchmark",
    "checked": true,
    "id": "3b2ad6382bee18c8d952539df8882997d68769a7",
    "semantic_title": "a usage-centric take on intent understanding in e-commerce",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=c7Y37nXokN": {
    "title": "Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs",
    "volume": "review",
    "abstract": "Reasoning is a fundamental component of language understanding. Recent prompting techniques, such as chain of thought, have consistently improved LLMs' performance on various reasoning tasks. Nevertheless, there is still little understanding of what triggers reasoning abilities in LLMs in the inference stage. In this paper, we introduce code prompting, a chain of prompts that transforms a natural language problem into code and directly prompts the LLM using the generated code without resorting to external code execution. We hypothesize that code prompts can elicit certain reasoning capabilities of LLMs trained on text and code and utilize the proposed method to improve conditional reasoning, the ability to infer different conclusions depending on the fulfillment of certain conditions. We find that code prompting exhibits a high-performance boost for multiple LLMs (up to 22.52 percentage points on GPT 3.5, 7.75 on Mixtral, and 16.78 on Mistral) across multiple conditional reasoning datasets. We then conduct comprehensive experiments to understand how code prompts trigger reasoning abilities and which capabilities are elicited in the underlying models. Our analysis of GPT 3.5 reveals that the code formatting of the input problem is essential for performance improvement. Furthermore, code prompts improve sample efficiency of in-context learning and facilitate state tracking of variables or entities",
    "checked": true,
    "id": "215c345579e6f230191d2b5a591bdaa75e2fe2f5",
    "semantic_title": "code prompting elicits conditional reasoning abilities in text+code llms",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=mVqc9VBxHe2": {
    "title": "Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues",
    "volume": "review",
    "abstract": "With the development of LLMs, the security threats of LLMs are getting more and more attention. Numerous jailbreak attacks have been proposed to assess the security defense of LLMs. Current jailbreak attacks primarily utilize scenario camouflage techniques. However their explicitly mention of malicious intent will be easily recognized and defended by LLMs. In this paper, we propose an indirect jailbreak attack approach, Puzzler, which can bypass the LLM's defensive strategies and obtain malicious response by implicitly providing LLMs with some clues about the original malicious query. In addition, inspired by the wisdom of \"When unable to attack, defend\" from Sun Tzu's Art of War, we adopt a defensive stance to gather clues about the original malicious query through LLMs. The experimental results indicate that the Query Success Rate of the Puzzler is 14.0%-82.7% higher than baselines on the most prominent LLMs. Furthermore, when tested against the state-of-the-art jailbreak detection approaches, Puzzler proves to be more effective at evading detection compared to baselines",
    "checked": true,
    "id": "490e815b3be11ba97631783d9ae946b8f8517fd6",
    "semantic_title": "play guessing game with llm: indirect jailbreak attack with implicit clues",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=1ipXv_m0JRM": {
    "title": "An Empirical Study of Speech Language Models for Prompt-Conditioned Speech Synthesis",
    "volume": "review",
    "abstract": "Speech language models (LMs) are promising for high-quality speech synthesis through in-context learning. A typical speech LM takes discrete semantic units as content and a short utterance as prompt, and synthesizes speech which preserves the content's semantics but mimics the prompt's style. However, there is no systematic understanding on how the synthesized audio is controlled by the prompt and content. In this work, we conduct an empirical study of the widely used autoregressive (AR) and non-autoregressive (NAR) speech LMs and provide insights into the prompt design and content semantic units. Our analysis reveals that heterogeneous and nonstationary prompts hurt the audio quality in contrast to the previous finding that longer prompts always lead to better synthesis. Moreover, we find that the speaker style of the synthesized audio is also affected by the content in addition to the prompt. We further show that semantic units carry rich acoustic information such as pitch, tempo, volume and speech emphasis, which might be leaked from the content to the synthesized audio",
    "checked": true,
    "id": "0a054d98808a910a988e61d071d53c10afa2cf59",
    "semantic_title": "an empirical study of speech language models for prompt-conditioned speech synthesis",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4P4xXt6d77W": {
    "title": "LSTPrompt: Large Language Models as Zero-Shot Time Series Forecasters by Long-Short-Term Prompting",
    "volume": "review",
    "abstract": "Time-series forecasting (TSF) finds broad applications in real-world scenarios. Prompting off-the-shelf Large Language Models (LLMs) demonstrates strong zero-shot TSF capabilities while preserving computational efficiency. However, existing prompting methods oversimplify TSF as language next-token predictions, overlooking its dynamic nature and lack of integration with state-of-the-art prompt strategies such as Chain-of-Thought. Thus, we propose LSTPrompt, a novel approach for prompting LLMs in zero-shot TSF tasks. LSTPrompt decomposes TSF into short-term and long-term forecasting sub-tasks, tailoring prompts to each. LSTPrompt guides LLMs to regularly reassess forecasting mechanisms to enhance adaptability. Extensive evaluations demonstrate consistently better performance of LSTPrompt than existing prompting methods, and competitive results compared to foundation TSF models",
    "checked": true,
    "id": "bbf272d92b76cc5f1e6f793dcfeb5edd4abed7ef",
    "semantic_title": "lstprompt: large language models as zero-shot time series forecasters by long-short-term prompting",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=aBPgUAUdoX": {
    "title": "Aligning Language Models Using Multi-Objective Deep Reinforcement Learning",
    "volume": "review",
    "abstract": "The alignment techniques used in state-of-the-art language models (LMs), e.g., reinforcement learning from human feedback (RLHF), have driven many successful Natural Language Processing (NLP) tasks. RLHF uses human preferences based on the guideline of being helpful and safe as a single reward signal to fine-tune language models. However, the trade-offs between helpfulness and safety are often found to be a problem, which makes it difficult for a model trained toward one objective to perform well on both. In this paper, we propose a new alignment technique, named multi-objective language model alignment (MOLMA). The framework is based on multi-objective deep reinforcement learning to fine-tune language models. MOLMA can efficiently address the conflicting or the dominating learning signal issue, which is caused by the the trade-offs of inherent, often conflicting, multi-objectives underlying the language model alignment task. From the overall objective of achieving both helpfulness and safety, our results show that MOLMA outperforms the other alignment techniques that rely on single-objective deep reinforcement learning",
    "checked": false,
    "id": "d8c78221e4366d6a72a6b3e41e35b706cc45c01d",
    "semantic_title": "training diffusion models with reinforcement learning",
    "citation_count": 134,
    "authors": []
  },
  "https://openreview.net/forum?id=EOhTYwyRsS": {
    "title": "A Graph per Persona: Reasoning about Subjective Natural Language Descriptions",
    "volume": "review",
    "abstract": "Reasoning about subjective natural language descriptions such as opinions and preferences is a challenging topic which largely hasn't been solved to date. In particular, the state-of-the-art large language models (LLMs) perform disappointing in this task, show strong biases, and do not meet the interpretability requirements we often have in this kind of applications. We propose a novel approach for reasoning about subjective knowledge which integrates potential, implicit meanings and explicitly models the relational nature of the information.We apply supervised graph learning, offer explanations for the model's reasoning, and show that our model performs well across all 15 topics of OpinionQA, outperforming several prominent LLMs. Our detailed analysis further shows its unique advantages and the complementary nature it offers in comparison to LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=APgOayLuGid": {
    "title": "LePaRD: A Dataset of 4+ Million Examples of Judges Citing Precedents",
    "volume": "review",
    "abstract": "We present the Legal Passage Retrieval Dataset, LePaRD. LePaRD is a massive collection of over 4 million U.S. federal judicial citations to precedent in context. The dataset aims to facilitate work on legal passage retrieval, a challenging practice-oriented legal retrieval and reasoning task. Legal passage retrieval seeks to predict relevant passages from precedential court decisions given the context of a legal argument. We extensively evaluate various retrieval approaches on LePaRD, and find that classification appears to work best. However, we note that legal passage retrieval is a difficult task, and there remains significant room for improvement. By publishing LePaRD, we provide a large-scale and high quality resource to foster further research on legal retrieval. Legal passage retrieval is a practice-oriented NLP task that promises to help expand access to justice by reducing the burden associated with legal research via computational assistance. The LePaRD dataset and code will be made freely available upon publication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2b8QDBBTSI": {
    "title": "CODA: A Novel End-to-End Computational Framework for Sign-to-Sign Translation Enhanced with LLMs",
    "volume": "review",
    "abstract": "The number of different signed languages presents novel challenges in cross-cultural sign language processing. Our work takes a pioneering step into direct sign-to-sign translation across different sign language families. We first conduct a qualitative analysis of linguistic traits, both shared and distinctive, within a parallel corpus of multiple signed pairs of sentences. We then introduce a novel generation framework, CODA, for translating one sign language to another, employing Large Language models as intermediary text recognizers. We compile a dataset for sign-to-sign translation pairs across three signed languages: American Sign Language (ASL), Chinese Sign Language (CSL), and German Sign Language (DGS). We further utilize sign glosses as an intermediate representation to construct a multi-task model that can assist in preserving the semantic meaning of generated sign skeletal videos. We show that our model performs well on automatic metrics for sign-to-sign translation and generation as a novel first implementation. We make all our code and models available upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y6bUH8mE-cg": {
    "title": "P-TA: Using Proximal Policy Optimization to Enhance Tabular Data Augmentation via Large Language Models",
    "volume": "review",
    "abstract": "A multitude of industries depend on accurate and reasonable tabular data augmentation for their business processes. Contemporary methodologies in generating tabular data revolve around utilizing Generative Adversarial Networks (GAN) or fine-tuning Large Language Models (LLM). However, GAN-based approaches are documented to produce samples with common-sense errors attributed to the absence of external knowledge. On the other hand, LLM-based methods exhibit a limited capacity to capture the disparities between synthesized and actual data distribution due to the absence of feedback from a discriminator during training. Furthermore, the decoding of LLM-based generation introduces gradient breakpoints, impeding the backpropagation of loss from a discriminator, thereby complicating the integration of these two approaches. To solve this challenge, we propose using proximal policy optimization (PPO) to apply GANs, guiding LLMs to enhance the probability distribution of tabular features. This approach enables the utilization of LLMs as generators for GANs in synthesizing tabular data. Our experiments demonstrate that PPO leads to an approximately 4% improvement in the accuracy of models trained on synthetically generated data over state-of-the-art across three real-world datasets",
    "checked": true,
    "id": "0eb69035aef6b190f0fd0c3892b9bbd26388a2a0",
    "semantic_title": "p-ta: using proximal policy optimization to enhance tabular data augmentation via large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iT-OMlYozIs": {
    "title": "Continual Learning with Semi-supervised Contrastive Distillation for Incremental Neural Machine Translation",
    "volume": "review",
    "abstract": "Incrementally expanding the capability of an existing translation model to solve new domain tasks over time is a fundamental and practical problem, which usually suffers from catastrophic forgetting. Generally, multi-domain learning can be seen as a good solution. However, there are two drawbacks: 1) it requires having the training data for all domains available at the same time, which may be unrealistic due to storage or privacy concerns; 2) it requires re-training the model on the data of all domains from scratch when adding a new domain and this is time-consuming and computationally expensive. To address these issues, we present a semi-supervised contrastive distillation framework for incremental neural machine translation. Specifically, to avoid catastrophic forgetting, we propose to exploit unlabeled data from the same distributions of the older domains through knowledge distillation. Further, to ensure the distinct domain characteristics in the model as the number of domains increases, we devise a cross-domain contrastive objective to enhance the distilled knowledge. Extensive experiments on domain translation benchmarks show that our approach, without accessing any previous training data or re-training on all domains from scratch, can significantly prevent the model from forgetting previously learned knowledge while obtaining good performance on the incrementally added domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VbXcA2jCjd": {
    "title": "Aligning Large Multimodal Models with Factually Augmented RLHF",
    "volume": "review",
    "abstract": "Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in ``hallucination'', generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue, we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the vision-language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground-truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real-world scenarios, we develop a new evaluation benchmark MMHAL-BENCH with a special focus on penalizing hallucinations. As the first LMM trained with RLHF, our approach achieves remarkable improvement on the LLaVA-Bench dataset with the 96% performance level of the text-only GPT-4 (while previous best methods can only achieve the 87% level), and an improvement of 60% on MMHAL-BENCH over other baselines",
    "checked": true,
    "id": "844bb298d49ef4a07b5d4929dfdfd170f6a1d5f5",
    "semantic_title": "aligning large multimodal models with factually augmented rlhf",
    "citation_count": 138,
    "authors": []
  },
  "https://openreview.net/forum?id=y35k4C33mFq": {
    "title": "ABEX: Data Augmentation for Low-Resource NLU via Expanding Abstract Descriptions",
    "volume": "review",
    "abstract": "We present ABEX, a novel and effective generative data augmentation methodology for low-resource Natural Language Understanding (NLU) tasks. ABEX is based on ABstract-and-EXpand, a novel paradigm for generating diverse forms of an input document -- we first convert a document into its concise, abstract description and then generate new documents based on expanding the resultant abstraction. To learn the task of expanding abstract descriptions, we first train BART on a large-scale synthetic dataset with abstract-document pairs. Next, to generate abstract descriptions for a document, we propose a simple, controllable, and training-free method based on editing AMR graphs. ABEX brings the best of both worlds: by expanding from abstract representations, it preserves the original semantic properties of the documents, like style and meaning, thereby maintaining alignment with the original label and data distribution. At the same time, the fundamental process of elaborating on abstract descriptions facilitates diverse generations. We demonstrate the effectiveness of ABEX on 4 NLU tasks spanning 12 datasets and 4 low-resource settings. ABEX outperforms all our baselines qualitatively with improvements of 0.04% - 38.8%. Qualitatively, ABEX outperforms all prior methods from literature in terms of context and length diversity. Code and synthetic data used for ABEX will be open-sourced upon paper acceptance",
    "checked": true,
    "id": "381e44fd4cbd986a51fdf9b4e8f81aba83df78c9",
    "semantic_title": "abex: data augmentation for low-resource nlu via expanding abstract descriptions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OaXH8cVGDQ": {
    "title": "Is Table Retrieval a Solved Problem? Join-Aware Multi-Table Retrieval",
    "volume": "review",
    "abstract": "Retrieving relevant tables containing the necessary information to accurately answer a given question over tables is critical to open-domain question-answering (QA) systems. Previous methods assume the answer to such a question can be found either in a single table or multiple tables identified through question decomposition or rewriting. However, neither of these approaches is sufficient, as many questions require retrieving multiple tables and joining them through a join plan that cannot be discerned from the user query itself. If the join plan is not considered in the retrieval stage, the subsequent steps of reasoning and answering based on those retrieved tables are likely to be incorrect. To address this problem, we introduce a method that uncovers useful join relations for any query and database during table retrieval. We use a novel re-ranking method formulated as a mixed-integer program that considers not only table-query relevance but also table-table relevance that requires inferring join relationships. Our method outperforms the state-of-the-art approaches for table retrieval by up to 9.3% in F1 score and for end-to-end QA by up to 5.4% in accuracy",
    "checked": false,
    "id": "f608360776e1fd3ddeab9a7a47e2975f1fa22011",
    "semantic_title": "is table retrieval a solved problem? exploring join-aware multi-table retrieval",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=9VvpCyAtg1C": {
    "title": "Joint Imbalance Adaptation for Radiology Report Generation",
    "volume": "review",
    "abstract": "Radiology report generation, predicting text descriptions for radiological images, may face critical challenges due to data imbalance â€“ medical tokens appear less frequently than regular tokens, and normal entries are significantly more than abnormal ones. However, very few studies consider the imbalance issues, not even with conjugate imbalance factors. In this study, we jointly consider two imbalance factors, label and token, determining distributions of radiology images and language, which are two fundamental modalities of the text generation task. We propose a $\\textbf{J}$oint $\\textbf{I}$mbalance $\\textbf{A}$daptation ($\\textit{JIMA}$) model to promote task robustness by leveraging token and label imbalance. Experiments on two standard evaluation data (IU X-ray (Demner-Fushman et al., 2015) and MIMIC-CXR (Johnson et al., 2019)) by automatic and human evaluations demonstrate our significant improvements over current state- of-the-art models. We conduct extensive abla- tion and case analyses to examine and present dual imbalance effects on the radiology report generation robustness. While data imbalance remains challenging, our approach opens new directions for the generation task",
    "checked": false,
    "id": "e0a9ca5e6473149a09148428d3f82612de5bd9a6",
    "semantic_title": "token imbalance adaptation for radiology report generation",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=1Lx4rI2leV": {
    "title": "Automated Evaluation of the Linguistic Difficulty of Short Texts for LLM Applications",
    "volume": "review",
    "abstract": "There is an unmet need to evaluate the language difficulty of short passages of text, particularly for training and filtering Large Language Models (LLMs). Existing datasets fail to train models for this task, so we introduce ShortDiff, a new dataset with 890 short text passages in English together with their level of text difficulty. We experiment with a variety of models on ShortDiff, including finetuning Transformer-based models and prompting LLMs. Our best model achieves accuracy surpassing human experts and has latency appropriate to production environments. Finally, we release the ShortDiff dataset to the public for further research and development",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l3jnR_w37pU": {
    "title": "Probing the Uniquely Identifiable Linguistic Patterns of Conversational AI Agents",
    "volume": "review",
    "abstract": "Considerable effort has been dedicated to detecting machine-generated texts to prevent a situation where the widespread generation of text---with minimal cost and effort--- reduces trust in human interaction and factual information online. Our study takes a more refined approach by analysing different Conversational AI Agents (CAA). By constructing linguistic profiles for each AI agent, the aim is to identify the Uniquely Identifiable Linguistic Patterns (UILPs) for each model and to demonstrate the effectiveness of these UILPs in identifying their respective AI agents using authorship attribution techniques. Promisingly, we are able to classify AI agents based on their original texts with a weighted F1-score of 96.94%. Further, we can attribute AI agents according to their writing style (as specified by prompts), yielding a weighted F1-score of 95.84%, which sets the baseline for this task. By employing principal component analysis (PCA) for dimensionality reduction, we achieve a weighted F1-score ranging from 89.25% to 97.83%, and an overall weighted F1-score of 96.93%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E4RnNHbOcK": {
    "title": "Learning Multilingual Idiomatic Representations by an Adaptive Contrastive Triplet Loss",
    "volume": "review",
    "abstract": "Accurately modeling idiomatic or non-compositional language has been a longstanding challenge in Natural Language Processing (NLP). This is partly because these expressions do not derive their meanings solely from their constituent words, but also due to the scarcity of relevant data resources, and their impact on the performance of downstream tasks such as machine translation and simplification. In this paper we propose an approach to model idiomaticity effectively using a triplet loss that incorporates the asymmetric contribution of components words to an idiomatic meaning for training language models by using adaptive contrastive learning and resampling miners to build an idiomatic-aware learning objective. Our proposed method is evaluated on a SemEval challenge and outperforms previous alternatives significantly in many metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LRu1IdceDMg": {
    "title": "Modeling Score Estimation for Japanese Essays with Generative Pre-trained Transformers",
    "volume": "review",
    "abstract": "This paper describes Japanese essay grading models with Generative Pre-trained Transformers (GPTs) in Japanese. Previous studies of essay grading show that neural network based models utilizing pre-trained language modelï½“ such as BERT are effective for several essay data. With the recent rapid development of downloadable GPTs, which are trained on significantly larger datasets compared to BERT, it has become feasible to employ GPTs for the task of essay grading through fine-tuning with Low-Rank Adaptation (LoRA). Most models in previous studies have been applied to English essay data and evaluated for the accuracy, but it is not clear how much prediction accuracy can be achieved for Japanese essays, where linguistic resources are limited. Thus, we apply several Japanese GPTs into Japanese essay data with 12 prompt composed of 4 themes. The experimental results show that a model pre-trained from the beginning with Japanese data has higher accuracy than a model additionally pre-trained from multilingual Llama",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IQiLxum_t2e": {
    "title": "Mitigating Open-Vocabulary Caption Hallucinations",
    "volume": "review",
    "abstract": "While recent years have seen rapid progress in image-conditioned text generation, image captioning still suffers from the fundamental issue of hallucinations, namely, the generation of spurious details that cannot be inferred from the given image. Existing methods largely use closed-vocabulary object lists to mitigate or evaluate hallucinations in image captioning, ignoring most types of hallucinations that occur in practice. To this end, we propose a framework for addressing hallucinations in image captioning in the open-vocabulary setting, including quantifying their presence and optimizing to mitigate such hallucinations. Our OpenCHAIR benchmark leverages generative foundation models to evaluate open-vocabulary caption hallucinations, surpassing the popular CHAIR benchmark in both diversity and accuracy. To mitigate open-vocabulary hallucinations on the sequence level, we propose MOCHa, an approach harnessing advancements in reinforcement learning. Our multi-objective reward function explicitly targets the trade-off between fidelity and adequacy in generations without requiring any strong supervision. MOCHa improves a large variety of image captioning models, as captured by our OpenCHAIR benchmark as well as other existing metrics. We will release our code, benchmark and models",
    "checked": true,
    "id": "9a3db70e4154ce6031006ddb0a83e6c0d112ffbb",
    "semantic_title": "mitigating open-vocabulary caption hallucinations",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=nw0DvXJ9diV": {
    "title": "Chain of Evidences and Evidence to Generate: Prompting for Context Grounded and Retrieval Augmented Reasoning",
    "volume": "review",
    "abstract": "While chain-of-thoughts (CoT) prompting has revolutionized how LLMs perform reasoning tasks, its current methods and variations (e.g, Self-consistency, ReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR) etc.,) suffer from limitations like limited context grounding, hallucination/inconsistent output generation, and iterative sluggishness. To overcome these challenges, we introduce a novel mono/dual-step prompting framework built upon two unique strategies \\textbf{Chain of Evidences (CoE)} and \\textbf{Evidence to Generate (E2G)}. Instead of unverified reasoning claims, our innovative approaches leverage the power of \"evidence for decision making\" by first focusing exclusively on the thought sequences explicitly mentioned in the context which then serve as extracted evidence, guiding the LLM's output generation process with greater precision and efficiency. This simple yet potent approach unlocks the full potential of chain-of-thoughts prompting, facilitating faster, more reliable, and contextually aware reasoning in LLMs. Our framework consistently achieves remarkable results across various knowledge-intensive reasoning and generation tasks, surpassing baseline approaches with state-of-the-art LLMs. For instance, (i) on the LogiQA benchmark using GPT-4, CoE achieves a new state-of-the-art accuracy of 53.8\\%, surpassing CoT by 18\\%, ToT by 11\\%, and CR by 9\\%; (ii) CoE with PaLM-2 outperforms the variable-shot performance of Gemini Ultra by 0.9 F1 points, achieving an F1 score of 83.3 on DROP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bp6CP0H2cD0": {
    "title": "Entity-Deduction Arena: Probing the Multi-turn Planning Capabilities of LLMs via 20 Question Games",
    "volume": "review",
    "abstract": "Large language models (LLMs) are effective at answering questions that are clearly asked. However, when faced with ambiguous queries they can act unpredictably and produce incorrect outputs. This underscores the need for the development of intelligent agents capable of asking clarification questions to resolve ambiguities effectively. This capability requires complex understanding, state tracking, reasoning and planning over multiple conversational turns. However, directly measuring this can be challenging.In this paper, we offer a surrogate problem which assesses an LLMs's capability to deduce an entity unknown to itself, but revealed to a judge, by asking the judge a series of queries. This entity-deducing game can serve as an evaluation framework to probe the conversational reasoning and planning capabilities of language models.We systematically evaluate various LLMs and discover significant differences in their performance on this task. We find that strong LLMs like GPT-4 outperform human players by a large margin. We further employ Behavior Cloning (BC) to examine whether a weaker model is capable of imitating a stronger model and generalizing to data or domains, using only the demonstrations from a stronger model. We finally propose to use Reinforcement Learning to enhance reasoning and planning capacity of Vicuna models through episodes of game playing, which lead to significant performance improvement. We hope that this problem offers insights into how autonomous agents could be trained to behave more intelligently in ambiguous circumstances",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VUvcsUUmeO": {
    "title": "FACTPICO: Factuality Evaluation for Plain Language Summarization of Medical Evidence",
    "volume": "review",
    "abstract": "Plain language summarization with LLMs can be useful for improving textual accessibility of technical content. But how factual are these summaries in a high-stakes domain like medicine? This paper presents FactPICO, a factuality benchmark for plain language summarization of medical texts describing randomized controlled trials (RCTs), which are the basis of evidence-based medicine and can directly inform patient treatment. FactPICO consists of 345 plain language summaries of RCT abstracts generated from three LLMs, with fine-grained evaluation and natural language rationales from experts. We assess the factuality of critical elements of RCTs in those summaries: Populations, Interventions, Comparators, Outcomes (PICO), as well as the reported findings concerning these. We also evaluate the correctness of the extra information (e.g., explanations) added by LLMs. Using FactPICO, we benchmark a range of existing factuality metrics, including the newly devised ones based on LLMs. We find that plain language summarization of medical evidence is still challenging, especially when balancing between simplicity and factuality, and that existing metrics correlate poorly with expert judgments on the instance level. FactPICO and our code will be made public upon publication",
    "checked": true,
    "id": "2fcddceba1710e3556e4315d13041eb029d5632c",
    "semantic_title": "factpico: factuality evaluation for plain language summarization of medical evidence",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=r09z8LQtSr": {
    "title": "Disagreeable, Slovenly, Honest and Un-named Women? Investigating Gender Bias in Educational Resources by Extending Existing Gender Bias Taxonomies",
    "volume": "review",
    "abstract": "Gender bias has been extensively studied in both the educational field and the Natural Language Processing (NLP) field, the former using human coding to identify patterns associated with and causes of gender bias in text and the latter to detect, measure and mitigate gender bias in NLP output and models. This work aims to use NLP to facilitate automatic, quantitative analysis of educational text within the framework of a gender bias taxonomy. Analyses of both educational texts and a lexical resource (WordNet) reveal patterns of bias that can inform and aid educators in updating textbooks and lexical resources and in designing assessment items",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1OGB3S-wGLV": {
    "title": "Human-AI Value Aligned Finetuning of Large Language Models with Multiagent Reinforcement Learning",
    "volume": "review",
    "abstract": "Human-AI Value Alignment has emerged as a central challenge in the rapid deployment of Artificial Intelligence (AI). In many applications like Large Language Models (LLMs), the goals and constraints for the AI model's desired behavior are known only to a (potentially very small) group of humans. The objective then is to develop mechanisms that allow humans to communicate these goals both efficiently and reliably to AI models like LLMs prior to their deployment. This requires explicitly reasoning about human strategies for shaping the behavior of AI agents, and how these strategies are derived from the humans' prior beliefs about the AI's own learning process. In this position paper, we argue that it is natural to view the alignment problem from the perspective of multiagent systems (MAS). We briefly survey open alignment challenges in the finetuning of large language models and in zero-shot learning with LLMs. We then connect these open questions to concepts developed for multiagent problems (particularly for ad hoc coordination), and discuss how these ideas may be applied to address mis-alignment in LLMs",
    "checked": false,
    "id": "8e695b876a0420344448d0d53da7f9fa1816b44a",
    "semantic_title": "superhf: supervised finetuning from human feedback",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zqB21Vk90as": {
    "title": "LASER: LLM Agent with State-Space Exploration for Web Navigation",
    "volume": "review",
    "abstract": "Large language models (LLMs) have been successfully adapted for interactive decision-making tasks like web navigation. While achieving decent performance, previous methods implicitly assume a forward-only execution mode for the model, where they only provide oracle trajectories as in-context examples to guide the model on how to reason in the environment. Consequently, the model could not handle more challenging scenarios not covered in the in-context examples, e.g., mistakes, leading to sub-optimal performance. To address this issue, we propose to model the interactive task as state space exploration, where the LLM agent transitions among a pre-defined set of states by performing actions to complete the task. This formulation enables flexible backtracking, allowing the model to recover from errors easily. We evaluate our proposed LLM Agent with State-Space ExploRation (LASER) on both the WebShop task and amazon.com. Experimental results show that LASER significantly outperforms previous methods and closes the gap with human performance on the web navigation task",
    "checked": true,
    "id": "c115984c2b28e0d17d722c55a132459a87224ccf",
    "semantic_title": "laser: llm agent with state-space exploration for web navigation",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=t64jQT7BidU": {
    "title": "Found in the middle: Calibrating Positional Attention Bias Improves Long Context Utilization",
    "volume": "review",
    "abstract": "Large language models (LLMs), even when specifically trained to process long input contexts, struggle to capture relevant information located in the middle of their input. This phenomenon has been known as the lost-in-the-middle problem. In this work, we make three contributions. First, we set out to understand the factors that cause this phenomenon. In doing so, we establish a connection between lost-in-the-middle to LLMs' intrinsic attention bias: LLMs exhibit an U-shaped attention bias where the tokens at the beginning and at the end of its input receive higher attention, regardless of their relevance. Second, we mitigate this positional bias through a calibration mechanism, found-in-the-middle, that allows the model to attend to contexts faithfully according to their relevance, even though when they are in the middle. Third, we show found-in-the-middle not only achieves better performance in locating relevant information within a long context, but also eventually leads to improved retrieval-augmented generation (RAG) performance across various tasks, outperforming existing methods by up to 10 percentage point. These findings open up future directions in understanding LLM attention bias and its potential consequences",
    "checked": true,
    "id": "9f3c17e20dff7321ddc849f8bf5194ba94370c46",
    "semantic_title": "found in the middle: calibrating positional attention bias improves long context utilization",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=1KlGVmLpy0": {
    "title": "A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia",
    "volume": "review",
    "abstract": "Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context. Yet the mechanisms underlying this contextual grounding remain unknown, especially in situations where contextual information contradicts factual knowledge stored in the parameters, which LLMs also excel at recalling. Favoring the contextual information is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify outdated or noisy stored knowledge. We present a novel method to study grounding abilities using Fakepedia, a dataset of counterfactual texts constructed to clash with a model's internal parametric knowledge. We benchmark various LLMs with Fakepedia and then we conduct a causal mediation analysis, based on our Masked Grouped Causal Tracing (MGCT), on LLM components when answering Fakepedia queries. Within this analysis, we identify distinct computational patterns between grounded and ungrounded responses. We finally demonstrate that distinguishing grounded from ungrounded responses is achievable through computational analysis alone. Our results, together with existing findings about factual recall mechanisms, provide a coherent narrative of how grounding and factual recall mechanisms interact within LLMs",
    "checked": true,
    "id": "11e56b35fa81ddb00dc96329c077c5b39c8f9e75",
    "semantic_title": "a glitch in the matrix? locating and detecting language model grounding with fakepedia",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=ssavLow3d_": {
    "title": "Rescue: Ranking LLM Responses with Partial Ordering to Improve Response Generation",
    "volume": "review",
    "abstract": "Customizing LLMs for a specific task involves distinguishing effective responses from erroneous ones. This skill can be developed using supervised fine-tuning with extensive human preference data. However, obtaining expert-annotated preference data is expensive for most tasks. In this paper, we present a novel method to optimize LLMs using ranking metrics. This method trains the model to prioritize the best responses from a pool of candidates created for a particular task. Rather than a traditional full ordering, we advocate for a partial ordering, as achieving consensus on the perfect order of candidate responses can be challenging. Our partial ordering is more robust, less sensitive to noise, and can be achieved with limited human annotations or through heuristic methods. We test our system's improved response generation ability using benchmark datasets, including the latest multi-document question answering task. We conduct ablation studies to understand crucial factors, such as how to gather candidate responses for specific tasks, determine their most suitable order, and balance supervised fine-tuning with ranking metrics. Our approach, named Rescue, suggests a promising avenue for enhancing LLMs' contextual understanding via response ranking",
    "checked": true,
    "id": "64affc9d608066c3d0a361fb583a6252444ce564",
    "semantic_title": "rescue: ranking llm responses with partial ordering to improve response generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vDcGAZyR35": {
    "title": "Which Nigerian-Pidgin does Generative AI speak?: Issues about Representativeness and Bias for Multilingual and Low Resource Languages",
    "volume": "review",
    "abstract": "Naija is the Nigerian-Pidgin spoken by approx. 120M speakers in Nigeria and it is a mixed language (e.g., English, Portuguese and Indigenous languages). Although it has mainly been a spoken language until recently, there are currently two written genres (BBC and Wikipedia) in Naija as well. Through statistical analyses and Machine Translation experiments, we prove that these two genres do not represent each other (i.e., there are linguistic differences in word order and vocabulary) and Generative AI operates only based on Naija written in the BBC genre which leads to bias in terms of representativeness",
    "checked": true,
    "id": "83b325e963976803185c1bfac1a29b337b9c68a7",
    "semantic_title": "which nigerian-pidgin does generative ai speak?: issues about representativeness and bias for multilingual and low resource languages",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zID5UkgBX-A": {
    "title": "Quantifying Generalizations: Exploring the Divide Between Human and LLMs' Sensitivity to Quantification",
    "volume": "review",
    "abstract": "Generics are expressions used to communicate abstractions about categories. While conveying general truths (e.g., \"Birds fly\"), generics have the interesting property to admit exceptions (e.g., penguins do not fly). Statements of this type help us organizing our knowledge of the world, and form the basis of how we express it (Hampton, 2012; Leslie, 2014).This study investigates how Large Language Models (LLMs) interpret generics, drawing upon psycholinguistic experimental methodologies. Understanding how LLMs interpret generic statements serves not only as a measure of their ability to abstract but also arguably plays a role in their encoding of stereotypes. Given that generics interpretation necessitates a comparison with explicitly quantified sentences, we explored i.) whether LLMs can correctly associate a quantifier with the generic structure, and ii.) whether the presence of a generic sentence as context influences the outcomes of quantifiers. We evaluated LLMs using both Surprisal distributions and prompting techniques.The findings indicate that models do not exhibit a strong sensitivity to quantification. Nevertheless, they seem to encode a meaning linked with the generic structure, which leads them to adjust their answers accordingly when a generalization is provided as context",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oYQnMDfaMr": {
    "title": "Your Co-Workers Matter: Evaluating Collaborative Capabilities of Language Models in Blocks World",
    "volume": "review",
    "abstract": "Language agents that interact with the world on their own have great potential for automating digital tasks. While large language model (LLM) agents have made progress in understanding and executing tasks such as textual games and webpage control, many real-world tasks also require collaboration with humans or other LLMs in equal roles, which involves intent understanding, task coordination, and communication. To test LLM's ability to collaborate, we design a blocks-world environment, where two agents, each having unique goals and skills, build a target structure together. To complete the goals, they can act in the world and communicate in natural language. Under this environment, we design increasingly challenging settings to evaluate different collaboration perspectives, from independent to more complex, dependent tasks. We further adopt chain-of-thought prompts that include intermediate reasoning steps to model the partner's state and identify and correct execution errors. Both human-machine and machine-machine experiments show thatLLM agents have strong grounding capacities, and our approach significantly improves the evaluation metric",
    "checked": true,
    "id": "811dd8b5367f7acbac2f2e14f6ad452db38b8bef",
    "semantic_title": "your co-workers matter: evaluating collaborative capabilities of language models in blocks world",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=_kaQ9iWikU9": {
    "title": "Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data",
    "volume": "review",
    "abstract": "Tables contrast with unstructured text data by its structure to organize the information.In this paper, we investigate the efficiency of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We pioneer in the assessment of LLMs' performance on image-based table representation. Specifically, we compare five text-based and three image-based table representations, revealing the influence of representation and prompting on LLM performance. We hope our study provides researchers insights into optimizing LLMs' application in table-related tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4WVfjIQ9G48": {
    "title": "A Simple LLM Framework for Long-Range Video Question-Answering",
    "volume": "review",
    "abstract": "We present LLoVi, a simple yet effective Language-based Long-range Video question-answering (LVQA) framework. Our method decomposes short and long-range modeling aspects of LVQA into two stages. First, we use a short-term visual captioner to generate textual descriptions of short video clips (0.5-8s in length) densely sampled from a long input video. Afterward, an LLM aggregates the densely extracted short-term captions to answer a given question. Furthermore, we propose a novel multi-round summarization prompt that asks the LLM first to summarize the noisy short-term visual captions and then answer a given input question. To analyze what makes our simple framework so effective, we thoroughly evaluate various components of our framework. Our empirical analysis reveals that the choice of the visual captioner and LLM is critical for good LVQA performance. The proposed multi-round summarization prompt also leads to a significant LVQA performance boost. Our method achieves the best-reported results on the EgoSchema dataset, best known for very long-form video question-answering. LLoVi also outperforms the previous state-of-the-art by 4.1% and 3.1% on NExT-QA and IntentQA. Finally, we extend LLoVi to grounded VideoQA which requires both QA and temporal localization, and show that it outperforms all prior methods on NExT-GQA. We will release our code",
    "checked": true,
    "id": "4641fe56cd44144b6cabea583233ed952f97f4c0",
    "semantic_title": "a simple llm framework for long-range video question-answering",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=MZASqWhAZ4V": {
    "title": "Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning",
    "volume": "review",
    "abstract": "Large language models~(LLMs) have demonstrated striking reasoning capability. Recent works have shown the benefits to LLMs from fine-tuning golden-standard Chain-of-Thought (CoT) rationales or using them as correct examples in few-shot prompting. While humans can indeed imitate correct examples, learning from our mistakes is another vital aspect of human cognition. Hence, a question naturally arises: \\textit{can LLMs learn and benefit from their mistakes, especially for their reasoning? }This study investigates this problem from both the prompting and model-tuning perspectives. We begin by introducing \\textsc{CoTErrorSet}, a new benchmark with 609,432 questions, each designed with both correct and error references, and demonstrating the types and reasons for making such mistakes. To explore the effectiveness of those mistakes, we design two methods: (1) \\textbf{Self-rethinking} prompting guides LLMs to rethink whether they have made similar previous mistakes; and (2) \\textbf{Mistake tuning} involves finetuning models in both correct and incorrect reasoning domains, rather than only tuning models to learn ground truth in traditional methodology. We conduct a series of experiments to prove LLMs can obtain benefits from mistakes in both directions. Our two methods offer potentially cost-effective strategies by leveraging errors to enhance reasoning capabilities, which costs significantly less than creating meticulously hand-crafted golden references. We ultimately make a thorough analysis of the reasons behind LLMs' errors, which provides directions that future research needs to overcome. \\textsc{CoTErrorSet} will be published soon on \\texttt{Anonymity Link}",
    "checked": true,
    "id": "094b6847434be00e41686529f45d895bd632f68a",
    "semantic_title": "can llms learn from previous mistakes? investigating llms' errors to boost for reasoning",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=HJoKs-GmMw": {
    "title": "SynText: Momentum Calibration For Multi-Document Summarization",
    "volume": "review",
    "abstract": "Multi-document summarization, a complex task in natural language processing, requires synthesizing information from multiple texts. Despite the focus on pre-training in recent research, the role of fine-tuning has been underexplored. We introduce SynText, a model that builds on the PRIMERA model for multi-document summarization through momentum calibration fine-tuning. Our results show that SynText surpasses the current state-of-the-art on the MultiNews dataset across all major ROUGE metrics. This work highlights the importance of not taking fine-tuning strategies for granted",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nBkGNZWz8CD": {
    "title": "MultiSQL: A Schema-Integrated Context-Dependent Text2SQL Dataset with Diverse SQL Operations",
    "volume": "review",
    "abstract": "Text2SQL is a task that translates natural language into SQL statements. Context-dependent Text2SQL offers a more natural database interaction by simulating dialogues between users and databases, with CoSQL and SparC as representative datasets. Yet, these datasets struggle to accurately replicate real-world situations. To address this, we introduce MultiSQL, which extends them in three key aspects: (1) Diverse SQL Operations. We incorporate diverse SQL types such as Create, Update, and Insert to broaden the scope of SQL operations. (2) Schema-Integrated Context. We integrated query context with database schema dependencies to better depict database complexity. (3) Extended Dialogues. We expand dialogue length to better simulate long conversations and complex interactions. This multi-type, schema-integrated, context-dependent Text2SQL dataset comprises nearly 800 dialogue groups and over 9,000 interaction turns across 166 complex databases, offering a better benchmark for interactive user-database dialogue.Addressing MultiSQL's challenges, we refined evaluation metrics to better capture diverse SQL types and schema dependencies. We designed a prompt framework that leverages historical data and self-refinement to accurately capture the dependency between text queries and database structures. Experiments with GPT-3.5, GPT-4, and LLaMA2-7B show both the effectiveness of our strategies and the challenges of MultiSQL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NXjm5e9erYZ": {
    "title": "VariErr NLI: Separating Annotation Error from Human Label Variation",
    "volume": "review",
    "abstract": "Human label variation arises when annotators assign different labels to the same item for valid reasons, while annotation errors occur when labels are assigned for invalid reasons. These two issues are prevalent in NLP benchmarks, yet existing research has studied them in isolation. To the best of our knowledge, there exists no prior work that focuses on teasing apart error from signal, especially in cases where signal is beyond black-and-white.To fill this gap, we introduce a systematic methodology and a new dataset, VariErr (variation versus error), focusing on the NLI task in English. We propose a 2-round annotation scheme with annotators explaining each label and subsequently judging the validity of label-explanation pairs.VariErr contains 7,574 validity judgments on 1,933 explanations for 500 re-annotated NLI items. We assess the effectiveness of various automatic error detection (AED) methods and GPTs in uncovering errors versus human label variation. We find that state-of-the-art AED methods significantly underperform compared to GPTs and humans. While GPT-4 is the best system, it still falls short of human performance by 15.2 on average precision. Our methodology is applicable beyond NLI, offering fertile ground for future research on true error versus plausible variation, which in turn can yield better and more trustworthy NLP systems",
    "checked": true,
    "id": "7301f6444241698263eda3140f64547f1dc17357",
    "semantic_title": "varierr nli: separating annotation error from human label variation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=LBgEKDdPQm": {
    "title": "Quantifying the Persona Effect in LLM Simulations",
    "volume": "review",
    "abstract": "Large language models (LLMs) have shown remarkable promise in simulating human language use and behavior. In this study, we delve into the intersection of persona variables and the capability of LLMs to simulate different perspectives. We find that persona variables can explain <10\\% variance in annotations in existing NLP datasets. Nonetheless, incorporating them via prompting in LLMs provides modest improvement. Persona prompting is most effective on data samples where disagreements among annotators are frequent yet confined to a limited range. A linear correlation exists: the more persona variables influence human response, the better LLMs predictions are using persona prompting. However, when the utility of persona variables is low (i.e., explaining <10\\% of human responses), persona prompting has little effect. Most subjective NLP datasets fall into this category, casting doubt on simulating diverse perspectives in the current NLP landscape",
    "checked": true,
    "id": "6b86326016fc7b900deac073e119c543daa0bafd",
    "semantic_title": "quantifying the persona effect in llm simulations",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=JNBPpTHrJ3C": {
    "title": "MMToM-QA: Multimodal Theory of Mind Question Answering",
    "volume": "review",
    "abstract": "Theory of Mind (ToM), the ability to understand people's minds, is an essential ingredient for developing machines with human-level social intelligence. Recent machine learning models, particularly large language models, seem to show some aspects of ToM understanding. However, existing ToM benchmarks use unimodal datasets â€“ either video or text. Human ToM, on the other hand, is more than video or text understanding. People can flexibly reason about another person's mind based on conceptual representations (e.g., goals, beliefs, plans) extracted from any available data. To address this, we introduce a multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA comprehensively evaluates machine ToM both on multimodal data and on different kinds of unimodal data about a person's activity in a household environment. To engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian Inverse Planning Accelerated by Language Models). BIP-ALM extracts unified representations from multimodal data and utilizes language models for scalable Bayesian inverse planning. We conducted a systematic comparison of human performance, BIP-ALM, and state-of-the-art models, including GPT-4. The experiments demonstrate that large language models and large multimodal models still lack robust ToM capacity. BIP-ALM, on the other hand, shows promising results, by leveraging the power of both model-based mental inference and language models",
    "checked": true,
    "id": "18155f3818e12f33d926f180b92a5c1f68e22f93",
    "semantic_title": "mmtom-qa: multimodal theory of mind question answering",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=bcfoL7QCbvC": {
    "title": "How Far can 100 Samples Go? Unlocking Zero-Shot Translation with Tiny Multi-Parallel Data",
    "volume": "review",
    "abstract": "Zero-shot translation aims to translate between language pairs not seen during training in Multilingual Machine Translation (MMT) and is largely considered an open problem. A common, albeit resource-consuming, solution is to add as many related translation directions as possible to the training corpus. In this paper, we show that the zero-shot capability of an English-centric model can be easily improved by fine-tuning with a very small amount of multi-parallel data. For example, on the EC30 dataset, we show that up to +21.7 ChrF non-English overall improvements (870 directions) can be achieved by using only 100 multi-parallel samples while preserving English-centric translation quality. When investigating the size effect of fine-tuning data and its transfer capabilities, we found to our surprise that already a small, randomly sampled set of fine-tuning directions is sufficient to achieve comparable improvements. The resulting non-English performance is close to the complete translation upper bound. We show that even in a minimal setting---fine-tuning with only one single sample---the well-known off-target issue is almost completely resolved, explaining parts--but not all---of the observed gains in translation quality",
    "checked": false,
    "id": "e302483426ef065ab1c2d99bc831d72a7a4fb22e",
    "semantic_title": "how far can 100 samples go? unlocking overall zero-shot multilingual translation via tiny multi-parallel data",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FptqZ8zP4XC": {
    "title": "Calibrating Likelihoods towards Consistency in Summarization Models",
    "volume": "review",
    "abstract": "Despite the recent advances in abstractive text summarization, current summarization models still suffer from generating factually inconsistent summaries, reducing their utility for real-world application. We argue that the main reason for such behavior is that the summarization models trained with maximum likelihood objective do not accurately rank sequences by their consistency with the document. In this work, we solve this problem by calibrating the likelihood of model generated sequences to better align with a consistency metric measured by natural language inference (NLI) models. The human evaluation study and LLM-based evaluation show that the calibrated models generate more consistent and higher-quality summaries.We also show that the models trained using our method return probabilities that are better aligned with the NLI scores, which improves reliability of summarization models",
    "checked": true,
    "id": "7464c4f54d4f9a31b96f46575d981c31b25d7622",
    "semantic_title": "calibrating likelihoods towards consistency in summarization models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=E852FH01gfy": {
    "title": "PromptAug: Informed Prompt Engineering for Data Augmentation",
    "volume": "review",
    "abstract": "Following the garbage in garbage out maxim, the quality of training data supplied to machine learning models impacts their performance. Generating these high-quality annotated training sets from unlabelled data is both expensive and unreliable. Moreover, social media platforms are increasingly limiting academic access to data, eliminating a key resource for NLP research. Consequently, researchers are shifting focus towards text data augmentation strategies to overcome these restrictions. In this work, we present an innovative data augmentation method, PromptAug, focusing on the design of distinct prompt engineering techniques for Large Language Models (LLMs). We concentrate on Instruction, Context, Example, and Definition prompt attributes, empowering LLMs to generate high-quality, class-specific data instances without requiring pre-training.We demonstrate the effectiveness of PromptAug, with improvements over the baseline dataset of 2\\% accuracy, 5\\% F1-score, 5\\% recall, and 2\\% precision. Furthermore, we evaluate PromptAug over a variety of dataset sizes, proving it's effectiveness even in extreme data scarcity scenarios. To ensure a thorough evaluation of data augmentation methods we further perform qualitative thematic analysis, identifying four problematic themes with augmented text data; Linguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and Augmented Content Misinterpretation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-D7AB3IPEJ": {
    "title": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs",
    "volume": "review",
    "abstract": "Most traditional AI safety research views models as machines and centers on algorithm-focused attacks developed by security experts. As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. Observing this, we shift the perspective, by treating LLMs as human-like communicators to examine the interplay between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then, we apply the taxonomy to automatically generate persuasive adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak risk across all risk categories: PAP consistently achieves an attack success rate of over 92\\% on Llama-2-7b-Chat, GPT-3.5, and GPT-4 in 10 trials, surpassing recent algorithm-focused attacks. On the defense side, we explore various mechanisms against PAP, find a significant gap in existing defenses, and advocate for more fundamental solutions for AI safety",
    "checked": true,
    "id": "732ce53c573475f2691a7cfc716cf4f568d17360",
    "semantic_title": "how johnny can persuade llms to jailbreak them: rethinking persuasion to challenge ai safety by humanizing llms",
    "citation_count": 99,
    "authors": []
  },
  "https://openreview.net/forum?id=ltTGAd8ynMZ": {
    "title": "The Probabilities Also Matter: A More Faithful Metric for Faithfulness of Free-Text Explanations in Large Language Models",
    "volume": "review",
    "abstract": "In order to oversee advanced AI systems, it is important to understand their reasons for generating a given output. When prompted, large language models (LLMs) can provide natural language explanations or reasoning traces that sound plausible and receive high ratings from human annotators. However, it is unclear to what extent these explanations are truly capturing the factors responsible for the model's predictions: the most ``human-like'' explanation may be different from the one that is most faithful to the model's true decision making process. In this work, we introduce the correlational counterfactual test (CCT), a faithfulness metric based on counterfactual input edits that takes into account not just the binary label change, but the total shift in the model's predicted label distribution. We evaluate the faithfulness of free-text explanations generated by few-shot-prompted LLMs from the Llama-2 family on three NLP tasks. We find that these explanations are indeed more likely to mention factors when they are impactful to the model's prediction, with the degree of association increasing with model size but varying significantly by task",
    "checked": true,
    "id": "59faaf0c63d2e4281d985e373b286c517577a933",
    "semantic_title": "the probabilities also matter: a more faithful metric for faithfulness of free-text explanations in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0nNy2xuyIB": {
    "title": "Automated analysis of the semantics in narratives by persons with Primary Progressive Aphasia",
    "volume": "review",
    "abstract": "Objective To detect differences in the semantics of the spontaneous language of persons with separate primary progressive aphasia syndromes (PPA) using automated Information Control Unit derivation. The resulting semantic clusters, that describe properties of the language of the participants, are evaluated for their use in a predictive model to identify speakers with PPA. Because language is relatively easy to acquire, its quantification may provide a low cost instrument that augments other spontaneous speech analyses in clinical practice.Materials and methodsSpeech was recorded from 15 control, 8 nonfluent variant (nfvPPA) and 8 semantic variant (svPPA) speakers, each describing the actions and figures on a given picture. Transcriptions of the recordings of controls were used to compute a prototypical description. Clustering was used to identify topics. The semantic distance between the prototype and the participant's language was used to quantify the degree to which the language of persons with PPA deviates from normal language. A classifier was used to classify individual fragments.ResultsThe vocabulary of speakers with PPA was found to be less diverse in speakers with PPA. Different clusters were identified automatically that correspond with categories of objects and actions in the image. In several clusters, speakers with PPA showed significant deviations in word usage from the prototype derived from control speakers.Group level differences were found. Compared to control speakers, nfvPPA speakers use fewer auxiliary verbs and verbs that describe figure actions, and fewer nouns that describe non-human figures. SvPPA speakers use more auxiliary verbs and fewer words that describe figures and actions. Both use more words that are semantically remote.A Random Forest classifier out-performed baseline with F1 scores of 0.80 (classes: control vs. PPA) and 0.71 (classes: control vs. nfvPPA vs. svPPA).DiscussionParticipants in both PPA groups show a profile that is distinct from that of control speakers. A surprise finding is that whereas nfvPPA is usually associated with speech motor problems, our study also finds their language deviating on the level of semantics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T2ptkhkFBgR": {
    "title": "Learning to Poison Large Language Models During Instruction Tuning",
    "volume": "review",
    "abstract": "The advent of Large Language Models (LLMs) has marked significant achievements in language processing and reasoning capabilities. Despite their advancements, LLMs face vulnerabilities to data poisoning attacks, where adversaries insert backdoor triggers into training data to manipulate outputs for malicious purposes. This work further identifies additional security risks in LLMs by designing a new data poisoning attack tailored to exploit the instruction tuning process. We propose a novel gradient-guided backdoor trigger learning approach to identify adversarial triggers efficiently, ensuring an evasion of detection by conventional defenses while maintaining content integrity. Through experimental validation across various LLMs and tasks, our strategy demonstrates a high success rate in compromising model outputs; poisoning only 1% of 4,000 instruction tuning samples leads to a Performance Drop Rate (PDR) of around 80%. Our work highlights the need for stronger defenses against data poisoning attack, offering insights into safeguarding LLMs against these more sophisticated attacks",
    "checked": true,
    "id": "44dc41803f49f7511f674ecb091d7a5c69fd5db2",
    "semantic_title": "learning to poison large language models during instruction tuning",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=C3ujNJwiWog": {
    "title": "myNLP: Natural Language Processing Library for Myanmar Language",
    "volume": "review",
    "abstract": "myNLP is a free, open-source natural language processing (NLP) library focused on the Myanmar language. The library is implemented in Python programming language and benchmarked on the available Myanmar corpora. In this paper, we provide outlines and comparisons of different approaches for each of the language processing functionalities as well as the datasets and pre-trained models. The library is constructed in a hierarchical structure including language processing functions and models for different NLP tasks. It will be publicly released and available on GitHub, with some larger models hosted on Hugging Face",
    "checked": false,
    "id": "8fde0a2b66451b7b219e9bb4dbb30ca24f1ddb8a",
    "semantic_title": "implementation of burmese language news classification system by using svm and lstm machine learning algorithm",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=voU2LgqmPU": {
    "title": "Decoding News Narratives: A Critical Analysis of Large Language Models in Framing Bias Detection",
    "volume": "review",
    "abstract": "This work contributes to the expanding research on the applicability of LLMs in social sciences by examining the performance of GPT-3.5 Turbo, GPT-4, and Flan-T5 models in detecting framing bias in news headlines through zero-shot, few-shot, and explainable prompting methods. A key insight from our evaluation is the notable efficacy of explainable prompting in enhancing the reliability of these models, highlighting the importance of explainable settings for social science research on framing bias. GPT-4, in particular, demonstrated enhanced performance in few-shot scenarios when presented with a range of relevant, in-domain examples. FLAN-T5's poor performance indicates that smaller models may require additional task-specific fine-tuning for identifying framing bias detection. Our study also found that models, particularly GPT-4, often misinterpret emotional language as an indicator of framing bias, underscoring the challenge of distinguishing between reporting genuine emotional expression and intentionally use framing bias in news headlines. We further evaluated the models on two subsets of headlines where the presence or absence of framing bias was either clear-cut or more contested, with the results suggesting that these models' can be useful in flagging potential annotation inaccuracies within existing or new datasets. Finally, the study evaluates the models in real-world conditions (\"in the wild\"), moving beyond the initial dataset focused on U.S. Gun Violence, assessing the models' performance on framed headlines covering a broad range of topics",
    "checked": true,
    "id": "c9fd2ff7ea072ce1b605e7896d5bd1d31b21b38e",
    "semantic_title": "decoding news narratives: a critical analysis of large language models in framing bias detection",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kasGnnpMNq": {
    "title": "Autonomous Workflow for Multimodal Fine-Grained Training Assistants Towards Mixed Reality",
    "volume": "review",
    "abstract": "Autonomous artificial intelligence (AI) agents have emerged as promising protocols for automatically understanding the language-based environment, particularly with the exponential development of large language models (LLMs). However, a fine-grained, comprehensive understanding of multimodal environments remains under-explored. This work designs an autonomous workflow tailored for integrating AI agents seamlessly into extended reality (XR) applications for fine-grained training. We present a demonstration of a multimodal fine-grained training assistant for LEGO brick assembly in a pilot XR environment. Specifically, we design a cerebral language agent that integrates LLM with memory, planning, and interaction with XR tools and a vision-language agent, enabling agents to decide their actions based on past experiences. Furthermore, we introduce LEGO-MRTA, a multimodal fine-grained assembly dialogue dataset synthesized automatically in the workflow served by a commercial LLM. This dataset comprises multimodal instruction manuals, conversations, XR responses, and vision question answering. Last, we present several prevailing open-resource LLMs as benchmarks, assessing their performance with and without fine-tuning on the proposed dataset. We anticipate that the broader impact of this workflow will advance the development of smarter assistants for seamless user interaction in XR environments, fostering research in both AI and HCI communities",
    "checked": true,
    "id": "68505397a2ec40a241e7b23eb06b26038ae4e0c0",
    "semantic_title": "autonomous workflow for multimodal fine-grained training assistants towards mixed reality",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b2vt_pJ1N4p": {
    "title": "MoCoKGC: Momentum Contrast Entity Encoding for Knowledge Graph Completion",
    "volume": "review",
    "abstract": "In recent years, a multitude of studies have aimed to enhance the capabilities of pretrained language models for handling Knowledge Graph Completion (KGC) tasks by integrating structural information from knowledge graphs. However, existing approaches have not effectively combined the structural attributes of knowledge graphs with textual descriptions of entities to generate entity encodings. To address this issue, we introduce MoCoKGC (Momentum Contrast Entity Encoding for Knowledge Graph Completion), incorporating three primary encoders: the entity-relation encoder, the entity encoder, and the momentum entity encoder.Through a slowly updated entity encoding mechanism, we maintain a negative sample queue and characterize the entity neighborhood.On the standard evaluation metric, Mean Reciprocal Rank (MRR), the MoCoKGC model demonstrates superior performance, achieving a 7.1\\% improvement on the WN18RR dataset and an 11\\% improvement on the Wikidata5M dataset, while also surpassing the current best model on the FB15k-237 dataset. Through a series of experiments, this paper deeply studies the role and contribution of each component and parameter of the model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5RWQPlJ6Rx9": {
    "title": "GADePo: Graph-Assisted Declarative Pooling Transformers for Document-Level Relation Extraction",
    "volume": "review",
    "abstract": "Document-level relation extraction typically relies on text-based encoders and hand-coded pooling heuristics to aggregate information learned by the encoder. In this paper, we leverage the intrinsic graph processing capabilities of the Transformer model and propose replacing hand-coded pooling methods with new tokens in the input, which are designed to aggregate information via explicit graph relations in the computation of attention weights. We introduce a joint text-graph Transformer model and a \\underline{g}raph-\\underline{a}ssisted \\underline{de}clarative \\underline{po}oling (GADePo) specification of the input, which provides explicit and high-level instructions for information aggregation. GADePo allows the pooling process to be guided by domain-specific knowledge or desired outcomes but still learned by the Transformer, leading to more flexible and customisable pooling strategies. We evaluate our method across diverse datasets and models and show that our approach yields promising results that are consistently better than those achieved by the hand-coded pooling functions",
    "checked": true,
    "id": "b367b37e58da2f0e1cab7167c884e94914e679f1",
    "semantic_title": "gadepo: graph-assisted declarative pooling transformers for document-level relation extraction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NtpubpngGDj": {
    "title": "Exploring the Influence of Label Aggregation on Minority Voices: Implications for Dataset Bias and Model Training",
    "volume": "review",
    "abstract": "Resolving disagreement in manual annotation typically consists of removing unreliable annotators and using a label aggregation strategy such as majority vote or expert opinion to resolve disagreement. These may have the side-effect of silencing or under-representing minority but equally valid opinions. In this paper, we study the impact of standard label aggregation strategies on minority opinion representation. We investigate the quality and value of minority annotations, and then examine their effect on the class distributions in gold labels, as well as how this affects behaviour of models trained on the resulting datasets. Finally, we discuss the potential bias introduced by each method and how they can be amplified by the models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=smcMzWSnb8j": {
    "title": "XrayGPT: Chest Radiographs Summarization using Large Medical Vision-Language Models",
    "volume": "review",
    "abstract": "The latest breakthroughs in large language models (LLMs) and vision-language models (VLMs) have showcased promising capabilities toward performing a wide range of tasks. Such models are typically trained on massive datasets comprising billions of image-text pairs with diverse tasks. However, their performance on task-specific domains, such as radiology, is still under-explored. While few works have recently explored LLMs-based conversational medical models, they mainly focus on text-based analysis. In this paper, we introduce XrayGPT, a conversational medical vision-language (VLMs) model that can analyze and answer open-ended questions about chest radiographs. Specifically, we align both medical visual encoder with a fine-tuned LLM to possess visual conversation abilities, grounded in an understanding of radiographs and medical knowledge. For improved alignment of chest radiograph data, we generate ~217k interactive and high-quality summaries from free-text radiology reports. Extensive experiments are conducted to validate the merits of XrayGPT. To conduct an expert evaluation, certified medical doctors evaluated the output of our XrayGPT on a test subset and the results reveal that more than 70\\% of the responses are scientifically accurate, with an average score of 4/5. We hope our simple and effective method establishes a solid baseline, facilitating future research toward automated analysis and summarization of chest radiographs. Code, models, and instruction sets will be publicly released",
    "checked": false,
    "id": "66d7d8dc54ea3dff10a11df2f29dc2104df86a57",
    "semantic_title": "xraygpt: chest radiographs summarization using medical vision-language models",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=GVR4ehJcYE1": {
    "title": "Answer is All You Need: Instruction-following Text Embedding via Answering the Question",
    "volume": "review",
    "abstract": "This work aims to build a text embedder that can capture characteristics of texts specified by user instructions clarifying the similarity criterion. While previous methods improve general task awareness by injecting the instruction information into encoding, they fail to be sensitive to clearer criteria like \"evaluate similarity based on emotion\". We instead propose a different viewpoint, which treats the instruction as a \"question\" about the input text and encodes the expected answers to obtain the representation accordingly. Intuitively, texts with the same (implicit) semantics would share similar answers following the instruction, thus leading to more similar representations. Specifically, we propose InBedder that instantiates this learning-to-answer idea by only fine-tuning language models via abstractive question answering tasks. Despite its simplicity, InBedder demonstrates significantly improved instruction-following capabilities according to our proposed instruction awareness tests and instruction robustness tests, when applied to language models with large language models (LLMs) (e.g., llama-2-7b) and smaller encoder-based LMs (e.g., roberta-large). Additionally, our qualitative analysis of clustering outcomes, achieved by applying diverse instructions to the same unlabeled corpus, demonstrates a high degree of interpretability in the clusters formed",
    "checked": true,
    "id": "217eab77e4b8d16189fa6d8e64cea489db46b56c",
    "semantic_title": "answer is all you need: instruction-following text embedding via answering the question",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=mD7iEG3ZnCW": {
    "title": "I am a Strange Dataset: Metalinguistic Tests for Language Models",
    "volume": "review",
    "abstract": "Statements involving metalinguistic self-reference (\"This paper has six sections.\") are prevalent in many domains. Can large language models (LLMs) handle such language? In this paper, we present \"I am a Strange Dataset\", a new dataset for addressing this question. There are two subtasks: generation and verification. In generation, models continue statements like \"The penultimate word in this sentence is\" (where a correct continuation is \"is\"). In verification, models judge the truth of statements like \"The penultimate word in this sentence is sentence.\" (false). We also provide minimally different metalinguistic non-self-reference examples to complement the main dataset by probing for whether models can handle metalinguistic language at all. The dataset is hand-crafted by experts and validated by non-expert annotators. We test a variety of open-source LLMs (7B to 70B parameters) as well as closed-source LLMs through APIs. All models perform close to chance across both subtasks and even on the non-self-referential metalinguistic control data, though we find some steady improvement with model scale. GPT 4 is the only model to consistently do significantly better than chance, and it is still only in the 60% range, while our untrained human annotators score well in the 89-93% range",
    "checked": true,
    "id": "867e4e0a9574d8e01fe7003179b3fd9cd809516f",
    "semantic_title": "i am a strange dataset: metalinguistic tests for language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ITlQchnmg85": {
    "title": "Label-Centric Curriculum Contrastive Learning for Zero-shot Extreme Multi-label Biomedical Document Classification",
    "volume": "review",
    "abstract": "Extreme multi-label text classification (XMC) aims to assign relevant labels to a document from a large set of candidate labels. Prior XMC research has typically concentrated on supervised learning methods. However, real-world scenarios frequently present situations where complete supervision signals, in the form of labeled and balanced datasets, are not available, highlighting the importance and relevance of zero-shot learning settings in XMC. In this paper, we study the XMC task on biomedical documents under the zero-shot setting which does not require any annotated documents in the training phase. We propose a novel label-centric curriculum contrastive learning framework for the training phase, which effectively utilizes hierarchical label information and label-metadata co-occurrence. For the inference phase, we employ a multi-stage retrieve and re-rank framework to make more accurate predictions by ruling out the irrelevant labels before ranking, rather than making direct predictions on the entire large label set. Experimental results demonstrate the effectiveness of our approach in improving the performance of XMC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wCtr7G4v58-": {
    "title": "ProAgent: From Robotic Process Automation to Agentic Process Automation",
    "volume": "review",
    "abstract": "From ancient water wheels to robotic process automation (RPA), automation technology has evolved throughout history to liberate human beings from arduous tasks. Yet, RPA struggles with tasks needing human-like intelligence, especially in elaborate design of workflow construction and dynamic decision-making in workflow execution. As Large Language Models (LLMs) have emerged human-like intelligence, this paper introduces Agentic Process Automation (APA), a groundbreaking automation paradigm using LLM-based agents for advanced automation by offloading the human labor to agents associated with construction and execution. We then instantiate ProAgent, an LLM-based agent designed to craft workflows from human instructions and make intricate decisions by coordinating specialized agents. Empirical experiments are conducted to detail its construction and execution procedure of workflow, showcasing the feasibility of APA, unveiling the possibility of a new paradigm of automation driven by agents",
    "checked": true,
    "id": "d416ebeee27c2cb6c25acd35fc65f7453398aee9",
    "semantic_title": "proagent: from robotic process automation to agentic process automation",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=OXzFOIhC0ER": {
    "title": "REANO: Optimising Retrieval-Augmented Reader Models through Knowledge Graph Generation",
    "volume": "review",
    "abstract": "Open domain question answering (ODQA) aims to answer questions with knowledge from an external corpus. Fusion-in-Decoder (FiD) is an effective retrieval-augmented reader model to address this task. Given that FiD independently encodes passages, which overlooks the semantic relationships between passages, some studies use knowledge graphs (KGs) to establish dependencies among passages. However, they only leverage knowledge triples from existing KGs, which suffer from incompleteness and may lack certain information critical for answering given questions. To this end, in order to capture the dependencies between passages while tacking the issue of incompleteness in existing KGs, we propose to enhance the retrieval-augmented reader model with a knowledge graph generation module (REANO). Specifically, REANO consists of a KG generator and an answer predictor. The KG generator aims to generate KGs from the passages and the answer predictor then generates answers based on the passages and the generated KGs. Experimental results on five ODQA datasets indicate that compared with baselines, REANO can improve the exact match score by up to 2.7% on the EntityQuestion dataset, with an average improvement of 1.8% across all the datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=StTqZlnF3cf": {
    "title": "ConstitutionalExperts: Training a Mixture of Principle-based Prompts",
    "volume": "review",
    "abstract": "Large language models (LLMs) are highly capable at a variety of tasks given the right prompt, but writing one is still a difficult and tedious process. In this work, we introduce ConstitutionalExperts, a method for learning a prompt consisting of constitutional principles (i.e. rules), given a training dataset. Unlike prior methods that optimize the prompt as a single entity, our method incrementally improves the prompt by surgically editing individual principles. We also show that we can improve overall performance by learning unique prompts for different semantic regions of the training data and using a mixture-of-experts (MoE) architecture to route inputs at inference time. We compare our method to other state of the art prompt-optimization techniques across six benchmark datasets. We also investigate whether MoE improves these other techniques. Our results suggest that ConstitutionalExperts outperforms other prompt optimization techniques by 10.9% (F1) and that mixture-of-experts improves all techniques, suggesting its broad applicability",
    "checked": true,
    "id": "b81e24e4bb1bec0fcd28521ca57c0a96340c242e",
    "semantic_title": "constitutionalexperts: training a mixture of principle-based prompts",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=EtMOrGwSUCJ": {
    "title": "Sociodemographic Bias in Language Models: A Survey and Forward Path",
    "volume": "review",
    "abstract": "This paper presents a comprehensive survey of work on sociodemographic bias in language models (LMs). Sociodemographic biases embedded within language models can have harmful effects when deployed in real-world settings. We systematically organize the existing literature into three main areas: types of bias, quantifying bias, and debiasing techniques. We also track the evolution of investigations of LM bias over the past decade. We identify current trends, limitations, and potential future directions in bias research. To guide future research towards more effective and reliable solutions, we present a checklist of open questions. We also recommend using interdisciplinary approaches to combine works on LM bias with an understanding of the potential harms",
    "checked": true,
    "id": "cfd2145fa17d2fcd7f1dba27bd713eaa4e798c1f",
    "semantic_title": "sociodemographic bias in language models: a survey and forward path",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=soWSRLO0s2q": {
    "title": "OLMo: Accelerating the Science of Language Models",
    "volume": "review",
    "abstract": "Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, we have built OLMo, a competitive, truly Open Language Model, to enable the scientific study of language models. Unlike most prior efforts that have only released model weights and inference code, we release OLMo alongside open training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation",
    "checked": true,
    "id": "ac45bbf9940512d9d686cf8cd3a95969bc313570",
    "semantic_title": "olmo: accelerating the science of language models",
    "citation_count": 100,
    "authors": []
  },
  "https://openreview.net/forum?id=K0efMEWueUI": {
    "title": "OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset",
    "volume": "review",
    "abstract": "Recent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills. Current large-scale math instruction tuning datasets such as MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed using outputs from closed-source LLMs with commercially restrictive licenses. A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on the recent progress in open-source LLMs, our proposed prompting novelty, and some brute-force scaling, we construct OpenMathInstruct-1, a math instruction tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning benchmarks, using the recently released and permissively licensed Mixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of OpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which is competitive with the best gpt-distilled models. We will release our code, models, and the OpenMathInstruct-1 dataset under a commercially permissive license",
    "checked": true,
    "id": "a3d749bc119f5c8425779e4e72e650720db2fe4b",
    "semantic_title": "openmathinstruct-1: a 1.8 million math instruction tuning dataset",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=6bOW0uk7PIy": {
    "title": "MultiPoT: Multilingual Program of Thoughts Harnesses Multiple Programming Languages",
    "volume": "review",
    "abstract": "Program of Thoughts (PoT) is an approach characterized by its executable intermediate steps, which ensure the accuracy of the numerical calculations in the reasoning process. Currently, PoT primarily uses Python. However, relying solely on a single language may result in suboptimal solutions and overlook the potential benefits of other programming languages. In this paper, we conduct comprehensive experiments on the programming languages used in PoT and find that no single language consistently delivers optimal performance across all tasks and models. The effectiveness of each language varies depending on the specific scenarios. Inspired by this, we propose a task and model agnostic approach called MultiPoT, which harnesses strength and diversity from various languages. Experimental results reveal that it significantly outperforms Python Self-Consistency. Furthermore, it achieves comparable or superior performance compared to the best monolingual PoT in almost all tasks across all models. In particular, MultiPoT achieves more than 4.6\\% improvement on average on both Starcoder and ChatGPT(gpt-3.5-turbo)",
    "checked": true,
    "id": "b6708658ec7c4dd28c2b52f1a3e40e257f217504",
    "semantic_title": "multipot: multilingual program of thoughts harnesses multiple programming languages",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=evi2gQ-3CB": {
    "title": "Learning Relational Decomposition of Queries for Question Answering from Tables",
    "volume": "review",
    "abstract": "Table Question-Answering involves both understanding the natural language query and grounding it in the context of the input table to extract the relevant information. In this context, many methods have highlighted the benefits of intermediate pre-training from SQL queries. However, while most approaches aim at generating final answers from inputs directly, we claim that there is better to do with SQL queries during training.By learning to imitate a restricted portion of SQL-like algebraic operations, we show that their execution flow provides intermediate supervision steps that allow increased generalization and structural reasoning compared with classical approaches of the field. Our study bridges the gap between semantic parsing and direct answering methods and provides useful insights regarding what types of operations should be predicted by a generative architecture or be preferably executed by an external algorithm",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RLOIphkHLD": {
    "title": "Hijacking Large Language Models via Adversarial In-Context Learning",
    "volume": "review",
    "abstract": "In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs for specific tasks by utilizing labeled examples asdemonstrations in the precondition prompts. Despite its promising performance, ICL suffers from instability with the choice and arrangement of examples. Additionally, crafted adversarial attacks pose a notable threat to the robustness of ICL. However, existing attacks are either easy to detect, rely on external models, or lack specificity towards ICL. To address these issues, this work introduces a novel transferable attack for ICL, aiming to hijack LLMs to generate the targeted response. The proposed LLM hijacking attack leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demonstrations. Extensive experimental results on various tasks and datasets demonstrate the effectiveness of our LLM hijacking attack, resulting in a distracted attention towards adversarial tokens, consequently leading to the targeted unwanted outputs",
    "checked": true,
    "id": "6d68b5c1eaf03aba857476a9825acf3e48edd840",
    "semantic_title": "hijacking large language models via adversarial in-context learning",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=nezVy_QhTxn": {
    "title": "DIALECTBENCH: An NLP Benchmark for Dialects, Varieties, and Closely-Related Languages",
    "volume": "review",
    "abstract": "Language technologies should be judged on their usefulness in real-world use cases. An often overlooked aspect in natural language processing (NLP) research and evaluation is language variation in the form of non-standard dialects or language varieties (hereafter, varieties). Most NLP benchmarks are limited to standard language varieties. To fill this gap, we propose \\benchmark{}, the first-ever large-scale benchmark for NLP on \\dialects, which aggregates an extensive set of task-varied varieties datasets (10 text-level tasks covering 281 varieties). This allows for a comprehensive evaluation of NLP system performance on different varieties. We provide substantial proof of performance disparities between standard and non-standard language varieties, and we also identify language clusters with larger performance divergence across tasks.We believe Dialectbench provides a comprehensive view of the current state of NLP for varieties and one step towards advancing it further",
    "checked": false,
    "id": "07c53cf78f0ec5b95e89dd4fc4f5774ab486c4b5",
    "semantic_title": "dialectbench: a nlp benchmark for dialects, varieties, and closely-related languages",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=2CeEZBaJsg": {
    "title": "Beyond Sparse Rewards: Enhancing Reinforcement Learning with Language Model Critique in Text Generation",
    "volume": "review",
    "abstract": "Reinforcement learning (RL) can align language models with non-differentiable reward signals, such as human preferences. However, a major challenge arises from the sparsity of these reward signals - typically, there is only a single reward for an entire output. This sparsity of rewards can lead to inefficient and unstable learning.To address this challenge, our paper introduces an novel framework that utilizes the critique capability of Large Language Models (LLMs) to produce intermediate-step rewards during RL training.Our method involves coupling a policy model with a critic language model, which is responsible for providing comprehensive feedback of each part of the output. This feedback is then translated into token or span-level rewards that can be used to guide the RL training process.We investigate this approach under two different settings: one where the policy model is smaller and is paired with a more powerful critic model, and another where a single language model fulfills both roles.We assess our approach on three text generation tasks: sentiment control, language model detoxification, and summarization. Experimental results show that incorporating artificial intrinsic rewards significantly improve both sample efficiency and the overall performance of the policy model, supported by both automatic and human evaluation",
    "checked": true,
    "id": "faae9de3d314e8731b0505607298fd826e3de1a7",
    "semantic_title": "beyond sparse rewards: enhancing reinforcement learning with language model critique in text generation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=aw1_DGbrPrv": {
    "title": "SteamPATHY: a Validated Conversational STEAM and Empathetic Dataset Harnessed Using ChatGPT",
    "volume": "review",
    "abstract": "Artificial intelligence, particularly large language models (LLMs), has made a significant impact across various domains, extending its reach into areas originally not tailored for its application, such as education and child emotional support. This expansion is driven by the LLMs' remarkable capacity to generate human-like content and engage in extended, coherent conversations. Besides, LLMs powered Chatbots can make STEAM education and empathetic virtual companions more accessible to children from different backgrounds. However, the process of fine-tuning a LLM model for this particular task faces a challenge due to the unavailability of readily accessible datasets focused on elementary school-level STEAM and empathy. Such sparsity of data can be solved by synthetically generating dataset samples using generative AI methods. While ChatGPT has been used in the past to generate datasets to finetune smaller and more computationally efficient LLMs on specific tasks, we propose in this paper to leverage ChatGPT prompting to generate SteamPATHY, student-caretaker interactions that cover STEAM and empathetic topics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qabHcQi_Njk": {
    "title": "Explaining the Hardest Errors of Contextual Embedding Based Classifiers",
    "volume": "review",
    "abstract": "We seek to explain potential causes for incorrect classification of the most challenging documents, namely, documents that no classifier using state-of-the-art, very semantically-separable contextual embedding representations managed to accurately predict. To do so, we propose a misclassification taxonomy of incorrect predictions, which we used to perform qualitative human evaluation. We posed two (research) questions, achieving a high inter-evaluator agreement of 81.7%. We worked with three sentiment analysis datasets, two in the movie reviews domain and a third one containing product reviews. We quantified answers per category in our taxonomy across all datasets and computed their proportion. Differences were observed between the product and movie review domains, such as the prevalence of ambivalence in product reviews and sarcasm in movie reviews. Our analysis also revealed an unexpectedly high rate of human mislabeling in the datasets and a significant number of model errors that we cannot yet explain. To ensure reproducibility, our documentation, code, and datasets can be accessed on GitHub",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x0AeknVYkX": {
    "title": "BloomVQA: Assessing Hierarchical Multi-modal Comprehension",
    "volume": "review",
    "abstract": "We propose a novel VQA dataset, BloomVQA, to facilitate comprehensive evaluation of large vision-language models on comprehension tasks. Unlike current benchmarks that often focus on fact-based memorization and simple reasoning tasks without theoretical grounding, we collect multiple-choice samples based on picture stories that reflect different levels of comprehension, as laid out in Bloom's Taxonomy, a classic framework for learning assessment widely adopted in education research. Our data maps to a novel hierarchical graph representation which enables automatic data augmentation and novel measures characterizing model consistency. We perform graded evaluation and reliability analysis on recent multi-modal models. In comparison to low-level tasks, we observe decreased performance on tasks requiring advanced comprehension and cognitive skills with up to 38.0% drop in VQA accuracy. In comparison to earlier models, GPT-4V demonstrates improved accuracy over all comprehension levels while also shows a tendency of bypassing visual inputs especially for higher-level tasks. Current models also show consistency patterns misaligned with human comprehension in various scenarios, demonstrating the need of improvement based on theoretically-grounded criteria. A link to the dataset will be provided",
    "checked": true,
    "id": "76ab0ebfbf2612d19f87876518496fc57625b13d",
    "semantic_title": "bloomvqa: assessing hierarchical multi-modal comprehension",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rWMR7EcC1v": {
    "title": "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?",
    "volume": "review",
    "abstract": "The adaption of multilingual pre-trained LLM into eloquent and helpful assistants is essential to facilitate their use across different language regions.In that spirit, we are the first to conduct an extensive study of the performance of multilingual models on parallel, multi-turn instruction-tuning benchmarks across a selection of the most-spoken Indo-European languages. We systematically examine the effects of language and instruction dataset size on a mid-sized, multilingual LLM by instruction-tuning it on parallel instruction-tuning datasets.Our results demonstrate that instruction-tuning on parallel instead of monolingual corpora benefits cross-lingual instruction following capabilities by up to 4.6%.Furthermore, we show that the Superficial Alignment Hypothesis does not hold in general, as the investigated multilingual 7B parameter model presents a counter-example requiring large-scale instruction-tuning datasets.Finally, we conduct a human annotation study to understand the alignment between human-based and GPT-4-based evaluation within multilingual chat scenarios",
    "checked": true,
    "id": "32b8734b4834acf22fac8399026119fa3e60de79",
    "semantic_title": "investigating multilingual instruction-tuning: do polyglot models demand for multilingual instructions?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JVRMtqZDcE": {
    "title": "Zero-Shot Video-Text Retrieval with Hallucination-Based Captioning and Structural Information Extraction",
    "volume": "review",
    "abstract": "While recent progress in video-text retrieval has been advanced by the exploration of better representation learning, in this paper, we present a novel zero-shot framework, HaliSIE, to retrieve video/text with off-the-shelf captioning methods, large language models (LLMs), and text retrieval methods. Specifically, we first map videos into captions with video captioning methods and then retrieve video captions and text using text retrieval methods, without any training or fine-tuning on models. However, due to the limited power of captioning methods, the captions often miss important content in the video, resulting in unsatisfactory retrieval performance. To translate more information into video captions, we designed a novel \\textbf{hallucination-based caption generation} method, that injects LLMs' hallucinated information into video captions. Moreover, to emphasize key information in the video, we extract key visual tokens from captions and design different templates for structuring these tokens with the proposed \\textbf{structural information extraction}, further boosting the retrieval performance. Benefiting from the hallucinated captions and structuralized information, extensive experiments on several video-text retrieval benchmarks demonstrate the superiority of our proposed zero-shot method, HaliSIE, over existing fine-tuned and pretraining methods without any data. Codes and data will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kysfZp6m7C": {
    "title": "EFSA: Towards Event-Level Financial Sentiment Analysis",
    "volume": "review",
    "abstract": "In this paper, we extend financial sentiment analysis (FSA) to event-level since events usually serve as the subject of the sentiment in financial text. Though extracting events from the financial text may be conducive to accurate sentiment predictions, it has specialized challenges due to the lengthy and discontinuity of events in a financial text. To this end, we reconceptualize the event extraction as a classification task by designing a categorization comprising coarse-grained and fine-grained event categories. Under this setting, we formulate the Event-Level Financial Sentiment Analysis(EFSA for short) task that outputs quintuples consisting of (company, industry, coarse-grained event, fine-grained event, sentiment) from financial text. A large-scale Chinese dataset containing 12,160 news articles and 13,725 quintuples is publicized as a brand new testbed for our task. A four-hop Chain-of-Thought LLM-based approach is devised for this task. Systematically investigations are conducted on our dataset, and the empirical results demonstrate the benchmarking scores of existing methods and our proposed method can reach the current state-of-the-art. Our dataset and framework implementation are available at https://anonymous.4open.science/r/EFSA-645E",
    "checked": true,
    "id": "ccb390557be8095a2de4da92380d17c84aaaf682",
    "semantic_title": "efsa: towards event-level financial sentiment analysis",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aGklVzb-qn": {
    "title": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models",
    "volume": "review",
    "abstract": "Conversation agents fueled by Large Language Models (LLMs) are providing a new way to interact with visual data. While there have been initial attempts for image-based conversation models, this work addresses the under-explored field of \\emph{video-based conversation} by introducing Video-ChatGPT. It is a multimodal model that merges a video-adapted visual encoder with an LLM. The resulting model is capable of understanding and generating detailed conversations about videos. We introduce a new dataset of 100,000 video-instruction pairs used to train Video-ChatGPT acquired via manual and semi-automated pipeline that is easily scalable and robust to label noise. We also develop a quantitative evaluation framework for video-based dialogue models to objectively analyze the strengths and weaknesses of video-based dialogue models. Our codes, models and dataset will be publicly released",
    "checked": true,
    "id": "bf7025a2e5dbb3c09deae02a1aa98a256ca559e2",
    "semantic_title": "video-chatgpt: towards detailed video understanding via large vision and language models",
    "citation_count": 237,
    "authors": []
  },
  "https://openreview.net/forum?id=OJkoC8AG69": {
    "title": "Model Editing at Scale leads to Gradual and Catastrophic Forgetting",
    "volume": "review",
    "abstract": "Editing knowledge in large language models is an attractive capability that allows us to correct incorrectly learned facts during pre-training, as well as update the model with an ever-growing list of new facts. While existing model editing techniques have shown promise, they are usually evaluated using metrics for reliability, specificity and generalization over one or few edits. We argue that for model editing to have practical utility, we must be able to make multiple edits to the same model. With this in mind, we evaluate current model editing methods at scale, focusing on two state of the art methods - ROME and MEMIT. With the lens of scalability, we evaluate model editing methods for three crucial properties - editing proficiency, fact forgetting and downstream performance. We find that as a model is edited sequentially with multiple facts, it continually becomes less editable, forgets previously edited facts and loses the ability to perform downstream tasks. For ROME and MEMIT, this \"forgetting\" happens in two phases - an initial gradual but progressive forgetting phase followed by an abrupt or catastrophic forgetting. Both gradual and catastrophic forgetting limit the usefulness of model editing methods at scale - the former makes model editing less effective as multiple edits are made to the model while the latter caps the scalability of such model editing methods. Our analysis also highlights other key limitations of ROME and MEMIT at scale. With our work, we push for better evaluation of model editing and development of model editing methods keeping scalability in mind",
    "checked": true,
    "id": "3a716fc45fe6233a29f58ad492e21d690d9054a0",
    "semantic_title": "model editing at scale leads to gradual and catastrophic forgetting",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=HfsCg8ZbZU": {
    "title": "Document-Level Machine Translation with Large-Scale Public Parallel Corpora",
    "volume": "review",
    "abstract": "Despite the fact that document-level machine translation has inherent advantages over sentence-level machine translation due to additional information available to a model from document context, most translation systems continue to operate at a sentence level. This is primarily due to the severe lack of publicly available large-scale parallel corpora at the document level. We release a large-scale open parallel corpus with document context extracted from ParaCrawl in five language pairs, along with code to compile document-level datasets for any language pair supported by ParaCrawl. We train context-aware models on these datasets and find improvements in terms of overall translation quality and targeted document-level phenomena. We also analyse how much long-range information is useful to model some of these discourse phenomena and find models are able to utilise context from several preceding sentences",
    "checked": false,
    "id": "a1374160c5ed2a6431510655af17105ce5d795d7",
    "semantic_title": "document-level neural machine translation with hierarchical modeling of global context",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=ojAq1GRqEDj": {
    "title": "$\\textit{Is the Pope Catholic? Yes, the Pope is Catholic. }\\\\$Generative Evaluation of Intent Resolution in LLMs",
    "volume": "review",
    "abstract": "Humans often express their communicative intents indirectly or non-literally, which requires their interlocutors---human or AI---to understand beyond the literal meaning of words. While most existing work has focused on discriminative evaluations, we present a new approach to generatively evaluate large language models' (LLMs') intention understanding by examining their responses to non-literal utterances. Ideally, an LLM should respond in line with the true intention of a non-literal utterance, not its literal interpretation. Our findings show that LLMs struggle to generate contextually relevant responses to non-literal language. We also find that providing oracle intentions substantially improves response appropriateness, but using chain-of-thought to make models spell out intentions before responding improves much less. These findings suggest that LLMs are not yet pragmatic interlocutors, and that explicitly modeling intention could improve LLM responses to non-literal language",
    "checked": false,
    "id": "8cffdafde24bdc852726ff29b4b2984f976d61c6",
    "semantic_title": "is the pope catholic? yes, the pope is catholic. generative evaluation of intent resolution in llms",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=PggP8DN7Qg": {
    "title": "On the effectiveness of phrase distance measures on separability and cohesion of meaning: A multilingual review",
    "volume": "review",
    "abstract": "This paper presents an automated method for evaluating phrase distancemeasures based on cohesion and diffusion measurements, eliminating theneed for direct human judgment. The evaluation involves five homegrown datasets,each consisting of 200 headlines or abstracts from news articles,subdivided into 20 sets. Two datasets are in Arabic, while others include news inFrench, German, and English. Each set contains 10 texts with sharedmeaning but different cohesion, and diffusion is modeled bydistances between articles with different meanings. The benchmark forevaluating phrase distance measures combines Silhouette Indexproperties with the mean of Pearson Correlations over distance matrixpairs. Our findings reveal that Yule distance with binary embeddingsconsistently surpasses other measures. Phrase distance performanceremains steady across languages, tokenizers and sentences' lengths",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zQK2bM_baS": {
    "title": "Speech-based Slot Filling using Large Language Models",
    "volume": "review",
    "abstract": "Recently, advancements in large language models (LLMs) have shown an unprecedented ability across various language tasks. This paper investigates the potential application of LLMs to slot filling with noisy ASR transcriptions, via both in-context learning and task-specific fine-tuning. Dedicated prompt designs and noise-robust LoRA fine-tuning are proposed to improve the robustness of LLMs for slot filling with noisy ASR transcriptions. Moreover, a linearised knowledge injection (LKI) scheme is also proposed to integrate dynamic external knowledge into LLMs. Experiments were performed on SLURP to quantify the performance of LLMs, including GPT-3.5-turbo, GPT-4, LLaMA-13B, LLaMA-2-13B and Vicuna-13B (v1.1 and v1.5) with different ASR error rates. The use of the noise-robust fine-tuning together with LKI for Vicuna-13B-v1.5 achieved 6.7% and 17.6% absolute SLU-F1 improvements compared to a fully fine-tuned Flan-T5-XL model on the limited data setup and the zero-shot setup respectively",
    "checked": true,
    "id": "435884716c37685702d61cef348ed4f2d30cbc99",
    "semantic_title": "speech-based slot filling using large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=dL5NOvPPtC": {
    "title": "RoleEval: A Bilingual Role Evaluation Benchmark for Large Language Models",
    "volume": "review",
    "abstract": "The rapid evolution of large language models necessitates effective benchmarks for evaluating their role knowledge, which is essential for establishing connections with the real world and providing more immersive interactions. This paper introduces RoleEval, a bilingual benchmark designed to assess the memorization, utilization, and reasoning capabilities of role knowledge. RoleEval comprises RoleEval-Global (including internationally recognized characters) and RoleEval-Chinese (including characters popular in China), with 6,000 Chinese-English parallel multiple-choice questions focusing on 300 influential people and fictional characters drawn from a variety of domains including celebrities, anime, comics, movies, TV series, games, and fictions. These questions cover basic knowledge and multi-hop reasoning abilities, aiming to systematically probe various aspects such as personal information, relationships, abilities, and experiences of the characters. To maintain high standards, we perform a hybrid quality check process combining both automatic and human verification, ensuring that the questions are diverse, challenging, and discriminative.Our extensive evaluations with RoleEval across various open-source and proprietary large language models, under both the zero- and few-shot settings, reveal insightful findings. Notably, while GPT-4 outperforms other models on RoleEval-Global, Chinese large language models excel on RoleEval-Chinese, highlighting significant knowledge distribution differences. We expect that RoleEval would highlight the significance of assessing role knowledge for large language models across various languages and cultural settings",
    "checked": true,
    "id": "a76630ed22310b123183e32b7d56b69d5c449d58",
    "semantic_title": "roleeval: a bilingual role evaluation benchmark for large language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=6x-JVsLU0i": {
    "title": "Partitioning-Guided K-Means: Extreme Empty Cluster Resolution for Extreme Language Model Compression",
    "volume": "review",
    "abstract": "Compactness in deep learning can be critical to a model's viability in low-resource applications, and a common approach to extreme model compression is quantization. We consider Iterative Product Quantization (iPQ) with Quant-Noise to be state-of-the-art in Quantization Aware Training (QAT), but this quantization framework suffers from preventable inference quality degradation due to prevalent empty clusters in bi-directional language modeling tasks. In this paper, we propose several novel enhancements aiming to improve the accuracy of iPQ with Quant-Noise by focusing on resolving empty clusters. Our contribution, which we call Partitioning-Guided k-means (PG k-means), is a heavily augmented k-means implementation composed of three main components. First, we propose a partitioning-based pre-assignment strategy that minimizes initial empty clusters and encourages an even weight-to-cluster distribution. Second, we propose an empirically superior empty cluster resolution heuristic executed via partitioning of large clusters. Finally, we construct an optional optimization step that consolidates intuitively dense clusters of weights to ensure shared representation. The proposed approach consistently reduces the number of empty clusters in iPQ with Quant-Noise by 100x on average and improves overall model accuracy by up to 12%, when applied to RoBERTa on a variety of tasks in the GLUE benchmark",
    "checked": false,
    "id": "3650f802699e9d04d337d0d466581683341b67e1",
    "semantic_title": "partitioning-guided k-means: extreme empty cluster resolution for extreme model compression",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CXwcQO0mwD": {
    "title": "MedExQA: Medical Question Answering Benchmark with Multiple Explanations",
    "volume": "review",
    "abstract": "This paper introduces MedExQA, a novel benchmark in medical question-answering, to evaluate large language models' (LLMs) understanding of medical knowledge through explanations. By constructing datasets across five distinct medical specialties and further incorporating multiple explanations for each question-answer pair, we address a major gap in current medical QA benchmarks which is the absence of comprehensive assessments of LLMs' ability to generate nuanced medical explanations. Our work highlights the importance of explainability in medical LLMs, proposes an effective methodology for evaluating models beyond classification accuracy, and sheds light on one specific domain, speech language pathology, where current LLMs including GPT4 lack good understanding. Our results show generation evaluation with multiple explanations aligns better with human assessment, highlighting an opportunity for a more robust automated comprehension assessment for LLMs. To diversify open-source medical LLMs (currently mostly based on Llama2), this work also proposes a new medical model, MedPhi-2, based on Phi-2 (2.7B). The model outperformed medical LLMs based on Llama2-70B in generating explanations, showing its effectiveness in the resource-constrained medical domain. We will share our benchmark datasets and the trained model",
    "checked": true,
    "id": "2cd41895b6baa722af1f27b80b12a7b373ec5bf9",
    "semantic_title": "medexqa: medical question answering benchmark with multiple explanations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=apdHPB0NKq": {
    "title": "A Systematic Literature Review of Adapter-based Approaches to Knowledge-enhanced Language Models",
    "volume": "review",
    "abstract": "Knowledge-enhanced language models (KELMs) have emerged as promising tools to bridge the gap between large-scale language models and domain-specific knowledge. KELMs can achieve higher factual accuracy and mitigate hallucinations by leveraging knowledge graphs (KGs). They are frequently combined with adapter modules to reduce the computational load and risk of catastrophic forgetting. In this paper, we conduct a systematic literature review (SLR) on adapter-based approaches to KELMs. We provide an overview of approaches in the field and explore the strengths and potential shortcomings of the multitude of discovered methods. We show that both general-knowledge and domain-specific approaches have been frequently explored along with various downstream tasks. Furthermore, we discovered that the biomedical domain is the most popular domain-specific field and that the Pfeiffer adapter is the most commonly used adapter type. We outline the main trends and propose promising future directions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NaXdl8h1vN": {
    "title": "Generating Gender Alternatives in Machine Translation",
    "volume": "review",
    "abstract": "Machine translation (MT) systems often translate terms with ambiguous gender (e.g., English term \"the nurse\") into the gendered form that is most prevalent in the systems' training data (e.g., \"enfermera\", the Spanish term for a female nurse). This often reflects and perpetuates harmful stereotypes present in society. With MT user interfaces in mind that allow for resolving gender ambiguity in a frictionless manner, we study the problem of generating all grammatically correct gendered translation alternatives. We open source train and test datasets for five language pairs and establish benchmarks for this task. Our key technical contribution is a novel semi-supervised solution for generating alternatives that integrates seamlessly with standard MT models and maintains high performance without requiring additional components or increasing inference overhead",
    "checked": true,
    "id": "11c6d69b0615a926fb557fd449bf95df99891b24",
    "semantic_title": "generating gender alternatives in machine translation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VaC18IvN0s": {
    "title": "Information Extraction from PDF Tables with Large Language Models",
    "volume": "review",
    "abstract": "Tables, found in PDF documents, contain valuable quantitative information. Unfortunately, extracting this information is difficult due to high variability in the table structure as well as content. We propose statements, a novel data-structure to self-contain quantitative facts and related information. We propose translating tables to statements as a new supervised deep-learning information extraction task. We introduce SemTabNet -- a dataset of over 100K annotated tables. Investigating a family of T5-based Statement Extraction Models, our best model predicts statements which are 82% similar to the ground-truth (F1 score of 0.97 for extracting entities). We demonstrate the advantages of representing information as statements by applying our model to over 2700 tables from ESG reports. The homogeneous nature of statements permits data-science analysis on expansive information found in large collections of tables",
    "checked": false,
    "id": "9f25d0d89443cd488bff0b80d15d3201cc51e2ca",
    "semantic_title": "statements: universal information extraction from tables with large language models for esg kpis",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yUp108pd-b": {
    "title": "Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence",
    "volume": "review",
    "abstract": "Emotional Intelligence (EI), consisting of emotion perception, emotion cognition and emotion expression, plays the critical roles in improving user interaction experience for the current large language model (LLM) based conversational general AI assistants. Previous works mainly focus on raising the emotion perception ability of them via naive fine-tuning on EI-related classification or regression tasks. However, this leads to the incomplete enhancement of EI and catastrophic forgetting of the general intelligence (GI). To this end, we first introduce \\textsc{EiBench}, a large-scale collection of EI-related tasks in the text-to-text formation with task instructions that covers all three aspects of EI, which lays a solid foundation for the comprehensive EI enhancement of LLMs. Then a novel \\underline{\\textbf{Mo}}dular \\underline{\\textbf{E}}motional \\underline{\\textbf{I}}ntelligence enhancement method (\\textbf{MoEI}), consisting of Modular Parameter Expansion and intra-inter modulation, is proposed to comprehensively enhance the EI of LLMs without compromise their GI. Extensive experiments on two representative LLM-based assistants, Flan-T5 and LLaMA-2-Chat, demonstrate the effectiveness of MoEI to improving EI while maintain GI",
    "checked": true,
    "id": "4d4d9da4f2c39089ea6c8d84e5031c195548d7b6",
    "semantic_title": "both matter: enhancing the emotional intelligence of large language models without compromising the general intelligence",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tXFOssVEY4": {
    "title": "What Changed? Converting Representational Interventions to Natural Language",
    "volume": "review",
    "abstract": "Interventions targeting the representation space of language models (LMs) have emerged as effective means to influence model behavior. These methods are employed, for example, to eliminate or alter the encoding of demographic information such as gender within the model's representations, creating a \\emph{counterfactual representation}. However, since the intervention operates within the representation space, understanding precisely which features it modifies poses a challenge. We propose a technique for converting representation-space counterfactuals into natural language counterfactuals. We demonstrate that this approach enables us to analyze the linguistic alterations corresponding to a given representation-space intervention and to interpret the features utilized for encoding a specific concept. Moreover, we demonstrate that the resulting counterfactuals can effectively mitigate bias in classification",
    "checked": false,
    "id": "a1d089feab0eaa0f1dcfe6e839e768428a99c2e7",
    "semantic_title": "speakers at the hello session",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LA65rPxQyKG": {
    "title": "Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks",
    "volume": "review",
    "abstract": "Large language models (LLMs) are proficient at generating fluent text with minimal task-specific supervision. However, their ability to generate rationales for knowledge-intensive tasks (KITs) remains under-explored. Generating rationales for KIT solutions, such as commonsense multiple-choice QA, requires external knowledge to support predictions and refute alternate options. In this work, we consider the task of generating retrieval-augmented rationalization of KIT model predictions via external knowledge-guidance within a few-shot setting. Surprisingly, crowd-workers preferred LLM-generated rationales over existing crowd-sourced rationales, generated in a similar knowledge-guided setting, on aspects such as factuality, sufficiency, and convincingness. However, follow-up fine-grained evaluation of such rationales highlight the need for further improvements in conciseness, novelty, and domain invariance. Additionally, through an expert-sourced study evaluating the reliability of the rationales, we demonstrate that humans' trust in LLM-generated rationales erode when communicated faithfully, i.e., without taking model prediction accuracy into account. We find that even instrumenting simple guardrails can be an effective for reliable rationalization",
    "checked": true,
    "id": "d3b5705d6ea1b8ec25504f46d9ff99a65a294e7c",
    "semantic_title": "characterizing large language models as rationalizers of knowledge-intensive tasks",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=HlgomUJ_9I": {
    "title": "Improving Language Models for Emotion Analysis: Insights from Cognitive Science",
    "volume": "review",
    "abstract": "We propose leveraging cognitive science research on emotions and communication to improve language models for emotion analysis. First, we present the main emotion theories in psychology and cognitive science. Then, we introduce the main methods of emotion annotation in natural language processing and their connections to psychological theories. We also present the two main types of analyses of emotional communication in cognitive pragmatics. Finally, based on the cognitive science research presented, we propose directions for improving language models for emotion analysis. We suggest that these research efforts pave the way for constructing new annotation schemes, methods, and a possible benchmark for emotional understanding, considering different facets of human emotion and communication",
    "checked": true,
    "id": "f533eb2715234053c7715f1a491c3f37c8ae11b6",
    "semantic_title": "improving language models for emotion analysis: insights from cognitive science",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HkWeIS6tCP": {
    "title": "Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models",
    "volume": "review",
    "abstract": "Prompt learning is susceptible to intrinsic bias present in pre-trained language models (LMs), resulting in sub-optimal performance of prompt-based zero/few-shot learning. In this work, we propose a null-input prompting method to calibrate intrinsic bias encoded in pre-trained LMs. Different from prior efforts that address intrinsic bias primarily for social fairness and often involve excessive computational cost, our objective is to explore enhancing LMs' performance in downstream zero/few-shot learning while emphasizing the efficiency of intrinsic bias calibration. Specifically, we leverage a diverse set of auto-selected null-meaning inputs generated from GPT-4 to prompt pre-trained LMs for intrinsic bias probing. Utilizing the bias-reflected probability distribution, we formulate a distribution disparity loss for bias calibration, where we exclusively update bias parameters ($0.1\\%$ of total parameters) of LMs towards equal probability distribution. Experimental results show that the calibration promotes an equitable starting point for LMs while preserving language modeling abilities. Across a wide range of datasets, including sentiment analysis and topic classification, our method significantly improves zero/few-shot learning performance of LMs for both in-context learning and prompt-based fine-tuning (on average $9\\%$ and $2\\%$, respectively)",
    "checked": true,
    "id": "8827b7b7e6dbba853c7e647fd06aa92e42b5f273",
    "semantic_title": "prompt-based bias calibration for better zero/few-shot learning of language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IP26pSJKR9q": {
    "title": "Intention Analysis Makes LLMs A Good Jailbreak Defender",
    "volume": "review",
    "abstract": "Aligning large language models (LLMs) with human values, particularly in the face of stealthy and complex jailbreak attacks, presents a formidable challenge. In this study, we present a simple yet highly effective defense strategy, i.e., Intention Analysis (IA). The principle behind this is to trigger LLMs' inherent self-correct and improve ability through a two-stage process: 1) essential intention analysis, and 2) policy-aligned response. Notably, IA is an inference-only method, thus could enhance the safety of LLMs without compromising their helpfulness. Extensive experiments on SAP200 and DAN benchmarks across Vicuna, ChatGLM, MPT, DeepSeek, and GPT-3.5 show that IA could consistently and significantly reduce the harmfulness in responses (averagely -46.5\\% attack success rate) and maintain the general helpfulness. Encouragingly, with the help of our IA, Vicuna-7b even outperforms GPT-3.5 in terms of attack success rate. Further analyses present some insights into how our method works. The code will be released",
    "checked": true,
    "id": "8fd29e810540c40846cddce3cbdf5060cd59fb57",
    "semantic_title": "intention analysis makes llms a good jailbreak defender",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=Q6qEQFwNABh": {
    "title": "Uncertainty Awareness of Large Language Models Under Code Distribution Shifts: A Benchmark Study",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have been widely applied in programming language analysis to enhance human productivity. Yet, their reliability can be compromised by various code distribution shifts, leading to inconsistent outputs. While probabilistic methods are known to mitigate such impact through uncertainty calibration and estimation, their efficacy in the language domain remains underexplored compared to their application in image-based tasks. In this work, we first introduce a large-scale benchmark dataset, incorporating three realistic patterns of code distribution shifts at varying intensities. Then we thoroughly investigate state-of-the-art probabilistic methods applied to LLMs using these shifted code snippets. We observe that these methods generally improve the uncertainty awareness of LLMs, with increased calibration quality and higher uncertainty estimation (UE) precision. However, our study also reveals varied performance dynamics across different criteria (e.g., calibration error vs misclassification detection) and the trade-off between efficacy and efficiency, highlighting necessary methodological selection tailored to specific contexts",
    "checked": true,
    "id": "645d8c40f2a05f0b06f9338cf7635755532d747c",
    "semantic_title": "uncertainty awareness of large language models under code distribution shifts: a benchmark study",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=C2Hqb7dUegv": {
    "title": "VISPool: Enhancing Transformer Encoders with Vector Visibility Graph Neural Networks",
    "volume": "review",
    "abstract": "The emergence of transformers has revolutionized natural language processing (NLP), as evidenced in various NLP tasks. While graph neural networks (GNNs) show recent promise in NLP, they are not standalone replacements for transformers. Rather, recent research explores combining transformers and GNNs. Existing GNN-based approaches rely on static graph construction methods requiring excessive text processing, and most of them are not scalable with the increasing document and word counts. We address these limitations by proposing a novel dynamic graph construction method for text documents based on vector visibility graphs (VVGs) generated from transformer output. Then, we introduce visibility pooler (VISPool), a scalable model architecture that seamlessly integrates VVG convolutional networks into transformer pipelines. We evaluate the proposed model on the General Language Understanding Evaluation (GLUE) benchmark datasets. VISPool outperforms the baselines with less trainable parameters, demonstrating the viability of the visibility-based graph construction method for enhancing transformers with GNNs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N81-_jppZz6": {
    "title": "ViHateT5: Enhancing Hate Speech Detection in Vietnamese With a Unified Text-to-Text Transformer Model",
    "volume": "review",
    "abstract": "Recent advancements in hate speech detection (HSD) in Vietnamese have made significant progress, primarily attributed to the emergence of transformer-based pre-trained language models, particularly those built on the BERT architecture. However, the necessity for specialized fine-tuned models has resulted in the complexity and fragmentation of developing a multitasking HSD system. Moreover, most current methodologies focus on fine-tuning general pre-trained models, primarily trained on formal textual datasets like Wikipedia, which may not accurately capture human behavior on online platforms. In this research, we introduce ViHateT5, a T5-based model pre-trained on our proposed large-scale domain-specific dataset named VOZ-HSD. By harnessing the power of a text-to-text architecture, ViHateT5 can tackle multiple tasks using a unified model and achieve state-of-the-art performance across all standard HSD benchmarks in Vietnamese. Our experiments also underscore the significance of label distribution in pre-training data on model efficacy. We provide our experimental materials for research purposes, including the VOZ-HSD dataset, pre-trained checkpoint, the unified HSD-multitask ViHateT5 model, and related source code on GitHub publicly",
    "checked": true,
    "id": "06092aeb2937b5897dd1152abe2d2881ea6b9d3f",
    "semantic_title": "vihatet5: enhancing hate speech detection in vietnamese with a unified text-to-text transformer model",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7vlPGaxUGV": {
    "title": "Cost-efficient Crowdsourcing for Span-based Sequence Labeling: Worker Selection and Data Augmentation",
    "volume": "review",
    "abstract": "This paper introduces a novel worker selection algorithm, enhancing annotation quality and reducing costs in challenging span-based sequence labeling tasks in Natural Language Processing (NLP). Unlike previous studies targeting simpler tasks, this study contends with the complexities of label interdependencies in sequence labeling tasks. The proposed algorithm utilizes a Combinatorial Multi-Armed Bandit (CMAB) approach for worker selection. The challenge of dealing with imbalanced and small-scale datasets, which hinder offline simulation of worker selection, is tackled using an innovative data augmentation method termed Shifting, Expanding, and Shrinking (SES). The SES method is designed specifically for sequence labeling tasks. Rigorous testing on CoNLL 2003 NER and Chinese OEI datasets showcased the algorithm's efficiency, with an increase in F1 score up to 100.04% of the expert-only baseline, alongside cost savings up to 65.97%. The paper also encompasses a dataset-independent test emulating annotation evaluation through a Bernoulli distribution, which still led to an impressive 97.56% F1 score of the expert baseline and 59.88% cost savings. This research addresses and overcomes numerous obstacles in worker selection for complex NLP tasks",
    "checked": true,
    "id": "bf551ea7f2a5bb2014fd6270fc8e6b92372bc2e6",
    "semantic_title": "cost-efficient crowdsourcing for span-based sequence labeling: worker selection and data augmentation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AOGSLeVvQL": {
    "title": "Third-Party Language Model Performance Prediction from Instruction",
    "volume": "review",
    "abstract": "Language model-based instruction-following systems have lately shown increasing performance on many benchmark tasks, demonstrating the capability of adapting to a broad variety of instructions. However, such systems are often not designed to be transparent about their limitations; a user may easily prompt a model with an instruction without any idea of whether the responses should be expected to be accurate, or if the system is even capable of performing the task. We propose a third party performance prediction framework, where a separate model is trained to predict the metric resulting from evaluating an instruction-following system on a task while assuming access only to its inputs and outputs at inference time. We perform this analysis with a variety of both open and closed instruction-following models as well as multiple performance predictors, and examine the effect of various factors such as model size, number of training tasks, and prompt format. Our findings indicate that third-party performance prediction is very challenging, and much work remains in developing predictors that can automatically reveal the limitations of modern instruction-following natural language processing systems",
    "checked": true,
    "id": "a1228951fa61089866579aa4671bb1fb1128df69",
    "semantic_title": "third-party language model performance prediction from instruction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZKQuNs-pjLF": {
    "title": "Beyond Recognising Entailment: Formalising Natural Language Inference from an Argumentative Perspective",
    "volume": "review",
    "abstract": "In argumentation theory, argument schemes provide a foundation that offers a characterisation of stereotypical patterns of inference. There has been little work done in providing computational approaches to identify these schemes in natural language. Moreover, advancements in recognizing textual entailment lack a standardized definition, which makes it challenging to compare methods trained on different datasets. In this work, we propose a rigorous approach to align entailment recognition with argumentation theory. Wagemans' Periodic Table of Arguments (PTA), a taxonomy of argument schemes, provides the appropriate framework to unify these two fields. To operationalise the theoretical model, we introduce a tool to assist humans in annotating arguments according to the PTA. Beyond providing insights into non-expert annotator training, we present Kialo-PTA24, the first multi-topic dataset for the PTA. We benchmark the performance of pre-trained language models on various aspects of argument analysis. Our experiments show that the task of argument canonicalisation poses a significant challenge for state-of-the-art models, suggesting an inability to represent argumentative reasoning and a direction for future investigation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ywILg4QrO-": {
    "title": "Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA",
    "volume": "review",
    "abstract": "Multipanel images, commonly seen as web screenshots, posters, etc., pervade our daily lives. These images, characterized by their composition of multiple subfigures in distinct layouts, effectively convey information to people. Toward building advanced multimodal AI applications, such as agents that understand complex scenes and navigate through webpages, the skill of multipanel visual reasoning is essential, and a comprehensive evaluation of models in this regard is important. Therefore, we introduce Multipanel Visual Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets of questions, answers, and multipanel images that specifically challenge models in comprehending multipanel images. Our evaluation shows that questions in the MultipanelVQA benchmark pose significant challenges to the state-of-the-art Large Vision Language Models (LVLMs) tested, even though humans can attain approximately 99% accuracy on these questions. Distinctively, the MultipanelVQA benchmark features synthetically generated multipanel images specifically crafted to isolate and assess the impact of various factors, such as the layout, on LVLMs' multipanel image comprehension abilities. As a result, in addition to benchmarking the capabilities of LVLMs in understanding multipanel images, we analyze the potential causes for LVLMs' performance and offer insights for enhancement with the synthetic data. Code and data will be released",
    "checked": false,
    "id": "b4839d1770f31fe0f0b3d1c46e159e4307b0963f",
    "semantic_title": "muffin or chihuahua? challenging multimodal large language models with multipanel vqa",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=QQY-Z1SvcsS": {
    "title": "Synthetic Context with LLM for Entity Linking from Scientific Tables",
    "volume": "review",
    "abstract": "Tables in scientific papers contain crucial information, such as experimental results.Entity Linking (EL) is a promising technology that analyses tables and associates them with a knowledge base.EL for table cells requires identifying the referent concept of each cell while understanding the context relevant to each cell in the paper. However, extracting the relevant context from the paper is challenging because the relevant parts are scattered in the main text and captions.This study defines a rule-based method for extracting broad context from the main text, including table captions and sentences that mention the table.Furthermore, we propose synthetic context as a more refined context generated by large language models (LLMs).In a synthetic context, contexts from the entire paper are refined by summarizing, injecting supplemental knowledge, and clarifying the referent concept.We observe this approach improves accuracy for EL by more than 10 points on the S2abEL dataset, and our qualitative analysis suggests potential future works",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qvozFP_bab": {
    "title": "Is it Really Negative? Evaluating Natural Language Video Localization Performance on Multiple Reliable Videos Pool",
    "volume": "review",
    "abstract": "With the explosion of multimedia content in recent years, Video Corpus Moment Retrieval (VCMR), which aims to detect a video moment that matches a given natural language query from multiple videos, has become a critical problem.However, existing VCMR studies have a significant limitation since they have regarded all videos not paired with a specific query as negative, neglecting the possibility of including false negatives when constructing the negative video set.In this paper, we propose an MVMR (Massive Videos Moment Retrieval) task that aims to localize video frames within a massive video set, mitigating the possibility of falsely distinguishing positive and negative videos.For this task, we suggest an automatic dataset construction framework by employing textual and visual semantic matching evaluation methods on the existing video moment search datasets and introduce three MVMR datasets.To solve MVMR task, we further propose a strong method, CroCs, which employs cross-directional contrastive learning that selectively identifies the reliable and informative negatives, enhancing the robustness of a model on MVMR task. Experimental results on the introduced datasets reveal that existing video moment search models are easily distracted by negative video frames, whereas our model shows significant performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cnRLaKBhpui": {
    "title": "ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation",
    "volume": "review",
    "abstract": "This paper introduces the ColorSwap dataset, designed to assess and improve the proficiency of multimodal models in matching objects with their colors. The dataset is comprised of 2,000 unique image-caption pairs, grouped into 1,000 examples. Each example includes a caption-image pair, along with a \"color-swapped\" pair. We follow the Winoground schema: the two captions in an example have the same words, but the color words have been rearranged to modify different objects. The dataset was created through a novel blend of automated caption and image generation with humans in the loop. We evaluate image-text matching (ITM) and visual language models (VLMs) and find that even the latest ones are still not robust at this task. GPT-4V and LLaVA score 72% and 42% on our main VLM metric, although they may improve with more advanced prompting techniques. On the main ITM metric, contrastive models such as CLIP and SigLIP perform close to chance (at 12% and 30%, respectively), although the non-contrastive BLIP ITM model is stronger (87%). We also find that finetuning on fewer than 2,000 examples yields significant performance gains on this out-of-distribution word-order understanding task",
    "checked": true,
    "id": "ee4f3f1c9bd5329d89c7c4973613e4cf01bdbe82",
    "semantic_title": "colorswap: a color and word order dataset for multimodal evaluation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=NNnOfMUqqGC": {
    "title": "Creating Corpus for Georgian Language Modelling",
    "volume": "review",
    "abstract": "The effectiveness of modern NLP methods remain contingent upon the availability of extensive and diverse high-quality training datasets. This poses a significant challenge for low-resource languages, among which Georgian stands out as not only low-resource but also remarkably under-researched. In this paper, we address one of the essential elements of this problem - the absence of the well-organized and openly accessible resources for Georgian language modeling. In particular, we introduce a software framework for collecting, cleaning, and organizing data for Georgian LLM training. We also publish an initial version of 37GB of dataset, laying the groundwork for subsequent research in this domain",
    "checked": false,
    "id": "cf8d85af3a3bd362739528832c36f31c9175008e",
    "semantic_title": "corpusnÃ³s: a massive galician corpus for training large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QSTgZL2f_aK": {
    "title": "Self-Specialization: Uncovering Latent Expertise within Large Language Models",
    "volume": "review",
    "abstract": "Recent works have demonstrated the effectiveness of self-alignment in which a large language model is aligned to follow general instructions using instructional data generated from the model itself starting from a handful of human-written seeds. Instead of general alignment, in this work, we focus on self-alignment for expert domain specialization (e.g., biomedicine, finance). As a preliminary, we quantitively show the marginal effect that generic instruction-following training has on downstream expert domains' performance. To remedy this, we propose self-specialization - allowing for effective model specialization while achieving cross-task generalization by leveraging only a few labeled seeds. Self-specialization offers a data- and parameter-efficient way of \"carving out\" an expert model out of a generalist pre-trained LLM. Exploring a variety of popular open large models as a base for specialization, our experimental results in both biomedical and financial domains show that our self-specialized models outperform their base models by a large margin, and even larger models that are generally instruction-tuned or that have been adapted to the target domain by other means",
    "checked": true,
    "id": "5efa173323e8850dde3f504a8c023cdbb6309b55",
    "semantic_title": "self-specialization: uncovering latent expertise within large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D0fEYV_Ru4": {
    "title": "Does Subword Vocabulary hold back Machine Translation?",
    "volume": "review",
    "abstract": "Subword tokenization is a heuristic to find contiguous pieces of characters that occur frequently, e.g., prefixes (dis-) and suffixes (-ing). However, natural language includes many more diverse patterns involving longer range dependencies, e.g., non-concatenative morphology in Arabic (Figure 1). A more expressive method to find such dependencies is to learn a vector-quantized codebook of tokens from raw bytes. We evaluate such learnt tokenizers on the task of machine translation across six language pairs and find that while they do not outperform subwords in general, they are more robust to misspellings and better on very short and very long sentences (by as much as 70\\%). We also demonstrate why they have a preference for representing non-concatenative morphologies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LUuxrghrIU": {
    "title": "Extensions to Interpretability Methods for Fact-Intensive Applications",
    "volume": "review",
    "abstract": "It would be advantageous if we could interpret the predictions of LMs in fact-intensive situations. Recent work has proposed several such interpretability approaches, but all are limited to idealized test situations that do not align with model behaviour in practice. We show that we can extend an interpretability method to non-ideal situations and apply it to study factual consistency. We find that consistent predictions generally correspond to the same underlying fact recall processes and identify a limitation of interpretability methods with respect to applied scenarios. Current methods cannot interpret cases for which a LM abstains from performing fact recall, something we find to usually be the case for inconsistent predictions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sOhtnR01RGo": {
    "title": "Metric-Learning Encoding Models Identify Processing Profiles of Linguistic Features in BERT's Representations",
    "volume": "review",
    "abstract": "We introduce Metric-Learning Encoding Methods (MLEMs) as a new approach to understand neural representations of sentences and their linguistic features (e.g., tense, subject person, object number). MLEMs are capable to detect both local and distributed representations.As a proof-of-concept, we apply MLEMs to neural representations extracted from BERT, and find that: (1)~there exists an order among linguistic features, which separate representations of sentences to different degrees in different layers; (2)~for some layers, neural representations are organized in a \\emph{hierarchical} way, with clusters nested within larger clusters, separated by linguistic features at different scales;(3)~in some layers (most strikingly the middle layer five of BERT), linguistic features are strongly disentangled, that is, represented within distinct clusters of selective units;(4)~MLEMs are more robust to type-I errors compared to multivariate decoding methods and are superior to univariate encoding methods in predicting neural activity.Together, this demonstrates the utility of Metric-Learning Encoding Methods for studying how linguistic features are neurally encoded in language models and the advantage of MLEMs over traditional methods. MLEMs can be extended to other domains (e.g., vision) and to other neural systems, such as the human brain",
    "checked": true,
    "id": "e270158cbc284b1862a77e4e2ef7fa6c0121f70e",
    "semantic_title": "metric-learning encoding models identify processing profiles of linguistic features in bert's representations",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=FewLiQpHHYA": {
    "title": "Deductive Closure Training of Language Models for Coherence, Accuracy, and Updatability",
    "volume": "review",
    "abstract": "While language models (LMs) can sometimes generate factually correct text and estimate truth values of individual claims, these generally do not reflect a globally coherent, manipulable model of the world. As a consequence, current LMs also generate incorrect or nonsensical content, and are difficult to edit and bring up to date. We present a method called Deductive Closure Training (DCT) that uses LMs themselves to identify implications of (and contradictions within) the text that they generate, yielding an efficient self-supervised procedure for improving LM factuality. Given a collection of seed documents, DCT prompts LMs to generate additional text implied by these documents, reason globally about the correctness of this generated text, and finally fine-tune on text inferred to be correct. Given seed documents from a trusted source, DCT provides a tool for supervised model updating; if seed documents are sampled from the LM itself, DCT enables fully unsupervised fine-tuning for improved coherence and accuracy. Across the CREAK, MQuAKE, and Reversal Curse datasets, supervised DCT improves LM fact verification and text generation accuracy by 3-26%; on CREAK, fully unsupervised DCT improves verification accuracy by 12%. These results show that LMs' reasoning capabilities during inference can be leveraged during training to improve their reliability",
    "checked": true,
    "id": "94617fc3593e87c4ff72ea0a3f5456fde5896801",
    "semantic_title": "deductive closure training of language models for coherence, accuracy, and updatability",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=61-CBL1-0Gu": {
    "title": "Designing Informative Metrics for Few-Shot Example Selection",
    "volume": "review",
    "abstract": "Pretrained language models (PLMs) have shown remarkable few-shot learning capabilities when provided with properly formatted examples. However selecting the best examples remains an open challenge. We propose a complexity based prompt selection approach for sequence tagging tasks. This approach avoids the training of a model for selection of examples and instead uses certain metrics to align the syntactico-semantic complexity of test sentences and examples. We use both sentence- and word-level metrics to match the complexity of examples to the (test) sentence being considered. Our results demonstrate that our approach extracts greater capability from PLMs: it achieves state-of-the-art performance on few-shot NER, achieving a 5% absolute improvement in F1 score on the CoNLL2003 dataset for GPT-4. We also see large gains of upto 28.85 points (F1/Acc.) in smaller models like GPT-j-6B",
    "checked": true,
    "id": "89b5f2a27fa7cb61986ac8c04bd2ea17c5b97bd8",
    "semantic_title": "designing informative metrics for few-shot example selection",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ifHCDkFxL6": {
    "title": "GraphCoder: Enhancing Repository-Level Code Completion via Code Context Graph-based Retrieval and Language Model",
    "volume": "review",
    "abstract": "The performance of repository-level code completion depends upon the effective leverage of both general and repository-specific knowledge. Despite the impressive capability of code LLMs in general code completion tasks, they often exhibit less satisfactory performance on repository-level completion due to the lack of repository-specific knowledge in these LLMs. To address this problem, we propose GraphCoder, a retrieval-augmented code completion framework that leverages LLMs' general code knowledge and the repository-specific knowledge via a graph-based retrieval-generation process. In particular, GraphCoder captures the context of completion target through code context graph (CCG) that consists of control-flow and data/control-dependence between code statements, a more structured way to capture the completion target context than the sequence-based context used in existing retrieval-augmented approaches; based on CCG, GraphCoder further employs a coarse-to-fine retrieval process to locate context-similar code snippets with the completion target from the current repository. Experimental results show that: compared with the state-of-the-art method RepoCoder, GraphCoder improves the exact match metric by 5.93% on average",
    "checked": true,
    "id": "793bbb3c70d4bac3908bd8f2ecc309f1bfdf5f52",
    "semantic_title": "graphcoder: enhancing repository-level code completion via code context graph-based retrieval and language model",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DfwLv8TIEFp": {
    "title": "Graph Representation of Narrative Context: Coherence Dependency via Retrospective Questions",
    "volume": "review",
    "abstract": "This work introduces a novel and practical paradigm for narrative comprehension, stemming from the observation that individual passages within narratives are often cohesively related than being isolated. We therefore propose to formulate a graph upon narratives dubbed NARCO that depicts a task-agnostic coherence dependency of the entire context. Especially, edges in NARCO encompass retrospective free-form questions between two context snippets reflecting high-level coherent relations, inspired by the cognitive perception of humans who constantly reinstate relevant events from prior context. Importantly, our graph is instantiated through our designed two-stage LLM prompting, thereby without reliance on human annotations. We present three unique studies on its practical utility, examining the edge efficacy via recap identification, local context augmentation via plot retrieval, and broader applications exemplified by long document QA. Experiments suggest that our approaches leveraging NARCO yield performance boost across all three tasks",
    "checked": false,
    "id": "5e490270c0fd49418e6bc5732c375867f253a20f",
    "semantic_title": "fine-grained modeling of narrative context: a coherence perspective via retrospective questions",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=BcL4wtCBNF": {
    "title": "Leveraging Entity Information for Cross-Modality Correlation Learning: The Entity-Guided Multimodal Summarization Model",
    "volume": "review",
    "abstract": "The rapid increase in multimedia data has spurred advancements in Multimodal Summarization with Multimodal Output (MSMO), which aims to produce a multimodal summary that that integrates both text and relevant images. The inherent heterogeneity of content within multimodal inputs and outputs presents a significant challenge to the execution of MSMO. Traditional approaches typically adopt a holistic perspective on coarse image-text data or individual visual objects, overlooking the essential connections between objects and the entities they represent. To integrate the fine-grained entity knowledge, we propose an Entity-Guided Multimodal Summarization model (EGMS). Our model, building on BART, utilizes dual multimodal encoders with shared weights to process text-image and entity-image information concurrently. A gating mechanism then combines visual data for enhanced textual summary generation, while image selection is refined through knowledge distillation from a pre-trained vision-language model. Extensive experiments on public MSMO dataset validate the superiority of the EGMS method, which also prove the necessity to incorporate entity information into MSMO problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zYL9YdwPryM": {
    "title": "Anchor-based Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) predominantly employ decoder-only transformer architectures, necessitating the retention of keys/values information for historical tokens to provide contextual information and avoid redundant computation. However, the substantial size and parameter volume of these LLMs require massive GPU memory. This memory demand increases with the length of the input text, leading to an urgent need for more efficient methods of information storage and processing. This study introduces Anchor-based LLMs (AnLLMs), which utilize an innovative anchor-based self-attention network (AnSAN) and also an anchor-based inference strategy. This approach enables LLMs to compress sequence information into an anchor token, reducing the keys/values cache and enhancing inference efficiency. Experiments on question-answering benchmarks reveal that AnLLMs maintain similar accuracy levels while achieving up to 99% keys/values cache reduction and up to 3.5 times faster inference. Despite a minor compromise in accuracy, the substantial enhancements of AnLLMs employing the AnSAN technique in resource utilization and computational efficiency underscore their potential for practical LLM applications",
    "checked": true,
    "id": "c152efb407e2515804fd8dac3b23b6fb98484224",
    "semantic_title": "anchor-based large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=NGzkBQXdvo": {
    "title": "\\title{\\underline{L}e\\underline{G}en: End-to-end \\underline{Leg}al Information Extraction using \\underline{Gen}erative Models}",
    "volume": "review",
    "abstract": "Despite the rapid growth in access to digital devices, the new users of the devices, especially in developing countries like India, are not able to access information on their rights and entitlements, jobs and livelihood, healthcare, education, etc. as the information is in the form of very long, complex sentences and heavy in legal parlance. Open information extraction techniques can be used to convert unstructured legal text into triples of the form \\triple{subject, relation, object} in a domain-independent manner. However, the legal text is long and complex which calls for extracting structure beyond triples, also called complex information extraction. This paper proposes a generative approach to perform complex information extraction from legal statements. We achieve this by encoding legal statements as trees to capture their complex structure and semantics. This end-to-end modelling reduces the propagation of errors across complicated pipelines. We experimented with multiple generative architectures to conclude that our proposed approach reports up to 14.7 \\% gain on an Indian Legal benchmark and is competitive on open information extraction benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VmBVLK4ik_G": {
    "title": "Improving Grammatical Error Correction via Contextual Data Augmentation",
    "volume": "review",
    "abstract": "Nowadays, data augmentation through synthetic data has been widely used in the field of Grammatical Error Correction (GEC) to alleviate the problem of data scarcity. However, these synthetic data are mainly used in the pre-training phase rather than the data-limited fine tuning phase due to inconsistent error distribution and noisy labels. In this paper, we propose a synthetic data construction method based on contextual augmentation, which can ensure an efficient augmentation of the original data with a more consistent error distribution. Specifically, we combine rule-based substitution with model-based generation, using the generation model to generate a richer context for the extracted error patterns. Besides, we also propose a relabeling-based data cleaning method to mitigate the effects of noisy labels in synthetic data. Experiments on CoNLL14 and BEA19-Test show that our proposed augmentation method consistently and substantially outperforms strong baselines and achieves the state-of-the-art level with only a few synthetic data",
    "checked": true,
    "id": "e698565e274d9da53aafb0eb7c68a4191905efad",
    "semantic_title": "improving grammatical error correction via contextual data augmentation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bg4rVgqPFSl": {
    "title": "GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) exhibit strong In-Context Learning (ICL) capabilities when prompts with demonstrations are applied to them. However, fine-tuning still remains crucial to further enhance their adaptability. Prompt-based fine-tuning proves to be an effective fine-tuning method in low-data scenarios, but high demands on computing resources limit its practicality. We address this issue by introducing a prompt-based parameter-efficient fine-tuning (PEFT) approach. GNNavi leverages insights into ICL's information flow dynamics, which indicates that label words act in prompts as anchors for information propagation. GNNavi employs a Graph Neural Network (GNN) layer to precisely guide the aggregation and distribution of information flow during the processing of prompts by hardwiring the desired information flow into the GNN. Our experiments on text classification tasks with GPT-2 and Llama2 shows GNNavi surpasses standard prompt-based fine-tuning methods in few-shot settings by updating just 0.2% to 0.5% of parameters. We compare GNNavi with prevalent PEFT approaches, such as prefix tuning, LoRA and Adapter in terms of performance and efficiency. Our analysis reveals that GNNavi enhances information flow and ensures a clear aggregation process",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8wncSaZi3p": {
    "title": "ETC$^2$: Near-Attention Ensemble of Term Classification for Effective and Efficient Text Classification",
    "volume": "review",
    "abstract": "Sequence models, particularly those leveraging transformer architectures, have dominated the Automatic Text Classification (ATC) field in the last years. These models represent words as dense contextual vectors that will compose the document (dense) representations. Though effective these models are expensive for training (fine-tuning) and at inference (prediction) time. Traditional bag-of-words approaches that directly represent a document as a single sparse vector are usually much more efficient, but are not as effective as sequence models. Both types of model commonly involve constructing a representation of the entire document before predicting its class, overlooking the importance of some individual word (co-)occurrences for the target task. This paper takes a completely different approach to the ATC task by promoting words as ``first-class'' citizens for ATC. In other words, our method called ETC$^2$, directly classifies each term of a document -- using an intricate combination of: (i) frequentist information; (ii) explicit co-occurrence and context modeling; and (iii) (near-)attention layering. It then uses these predictions to estimate the document class. The proposed approach eliminates the need for a single representation of the document, thus enormously improving model efficiency. In our experimental evaluation, ETC$^2$ was as effective as (if not better) than the best Transformer baselines in the tested datasets, being up to 17x faster at inference (prediction) time than modern Transformer-based classifiers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7ktnfjjQA7": {
    "title": "Understanding Cross-Lingual Alignmentâ€”A Survey",
    "volume": "review",
    "abstract": "Cross-lingual alignment, the meaningful similarity of representations across languages in multilingual language models, has been an active field of research in recent years. We survey the literature of techniques to improve cross-lingual alignment, providing a taxonomy of methods and summarising insights from throughout the field. We present different understandings of cross-lingual alignment and their limitations. We provide a qualitative summary of results from a number of surveyed papers. Finally, we discuss how these insights may be applied not only to encoder models, where this topic has been heavily studied, but also to encoder-decoder or even decoder-only models, and argue that an effective trade-off between language-neutral and language-specific information is key",
    "checked": false,
    "id": "c84fd9a445f0c9cbab900ec3638ad832fa4fde32",
    "semantic_title": "understanding cross-lingual alignment - a survey",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=QiSRyCCUY0": {
    "title": "Automatic Identification of Emotions in Texts: Dealing also with their Indirect Modes of Expression",
    "volume": "review",
    "abstract": "This paper presents a model that predicts whether (A) a sentence contains an emotion or not, (B) according to which mode(s) it is expressed, (C) whether the emotion is basic or complex, and (D) which emotional category it is. The originality of the paper lies in the focus on written texts (encyclopedia, novels, newspapers)---as opposed to the more widely studied conversational (sometimes multi-modal) situation---towards the analysis of text complexity in which emotions are one of the analysis factors according to certain works in psycho-linguistics. Within this particular scope, the major contribution of the paper is to introduce the identification of the modes of expression of the emotions, ranging from a direct lexical mode to the most indirect one where emotions are only suggested. The experiments are carried out on French texts for children. They show that the task is rather difficult but leading to acceptable results in comparison to what human annotators agree on. The results also seem to indicate that the task cannot be simply solved by prompting a large language model and requires a specialized model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0CPuWV9TXP": {
    "title": "Tasks That Language Models Don't Learn",
    "volume": "review",
    "abstract": "We argue that there are certain properties of language that our current large language models (LLMs) don't learn. We present an empirical investigation of visual-auditory properties of language through a series of tasks, termed H-Test. This benchmark highlights a fundamental gap between human linguistic comprehension, which naturally integrates sensory experiences, and the sensory-deprived processing capabilities of LLMs. In support of our hypothesis, 1. deliberate reasoning (Chain-of-Thought), 2. few-shot examples, or 3. stronger LLM from the same model family (LLaMA 2 13B -> LLaMA 2 70B) do not trivially bring improvements in H-Test performance. Therefore, we make a particular connection to the philosophical case of Mary, who learns about the world in a sensory-deprived environment (Jackson, 1986). Our experiments show that some of the strongest proprietary LLMs stay near random chance baseline accuracy of 50%, highlighting the limitations of knowledge acquired in the absence of sensory experience",
    "checked": false,
    "id": "f014f0ea9867c2912e625b54d57713f1937d124f",
    "semantic_title": "large language models can learn rules",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=fiaECqO3N5": {
    "title": "SPECTRUM: Speaker-Enhanced Pre-Training for Long Dialogue Summarization",
    "volume": "review",
    "abstract": "Multi-turn dialogues are characterized by their extended length and the presence of turn-taking conversations. Traditional language models often overlook the distinct features of these dialogues by treating them as regular text. In this paper, we propose a speaker-enhanced pre-training method for long dialogue summarization, which leverages the inherent structure of multiple-turn dialogues. To support our study, we curate a diverse dataset that includes transcripts from real-world scenarios, movie or TV show transcripts, and dialogues generated by a Large Language Model. We then perform a pre-training, which encompasses the detection of speaker changes, and masked utterance generation. Experimental results of fine-tuned models demonstrate that our model achieves state-of-the-art performance on downstream benchmarks with long context, surpassing baseline models and highlighting the effectiveness of our approach. Our findings highlight the importance of curating pre-training datasets that exhibit diversity and variations in length distribution to ensure effective alignment with downstream datasets",
    "checked": true,
    "id": "a37daf2fb463fe2e9c895f2e94a055c3d3a655a6",
    "semantic_title": "spectrum: speaker-enhanced pre-training for long dialogue summarization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QfpGNA5C7xM": {
    "title": "GitAgent: Facilitating Autonomous Agent with Tool Extension from GitHub",
    "volume": "review",
    "abstract": "While Large Language Models (LLMs) have demonstrated exceptional proficiency in natural language processing, their efficacy in addressing complex, multifaceted tasks remains limited. A growing area of research focuses on LLM-based agents equipped with external tools capable of performing diverse tasks. However, existing LLM-based agents only support a limited set of tools which is unable to cover a diverse range of user queries, especially for those involving expertise domains. It remains a challenge for LLM-based agents to extend their tools autonomously when confronted with various user queries. As GitHub has hosted a multitude of repositories which can be seen as a good resource for tools, a promising solution is that LLM-based agents can autonomously integrate the repositories in GitHub according to the user queries to extend their tool set. In this paper, we introduce GitAgent, an agent capable of achieving the autonomous tool extension from GitHub. GitAgent follows a four-phase procedure to incorporate repositories and it can learn human experience by resorting to GitHub Issues/PRs to solve problems encountered during the procedure. Experimental evaluation on our constructed GitBench demonstrates GitAgent's effectiveness",
    "checked": false,
    "id": "b6ca4b1bee98f4317dd25c764a8057e6ef11c89e",
    "semantic_title": "gitagent: facilitating autonomous agent with github by tool extension",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=H3oRiz3QOx": {
    "title": "Error-preserving Automatic Speech Recognition of Young English Learners' Language",
    "volume": "review",
    "abstract": "One of the central skills that language learners need to practice is speaking the language. Currently, students in school do not get enough speaking opportunities and lack conversational practice. The recent advances in speech technology and natural language processing allow the creation of novel tools to practice their speaking skills. In this work, we tackle the first component of such a pipeline, namely, the automated speech recognition module (ASR). State-of-the-art models are often trained on adult read-aloud data by native speakers and do not transfer well to young language learners' speech. Second, most ASR systems contain a powerful language model, which smooths out mistakes made by the speakers. To give corrective feedback, which is a crucial part of language learning, the ASR systems in our setting need to preserve the mistakes made by the language learners. In this work, we build an ASR system that satisfies these requirements: it works on spontaneous speech by young language learners and preserves their mistakes. For this, we collected a corpus containing around 85 hours of English audio spoken by Swiss learners from grades 4 to 6 on different language learning tasks, which we used to train an ASR model. Our experiments show that our model benefits from direct fine-tuning of children's voices and has a much higher error preservation rate",
    "checked": true,
    "id": "ef19aae24031c88e43d9358401a280f23c9b53df",
    "semantic_title": "error-preserving automatic speech recognition of young english learners' language",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TX6sN9RuKA": {
    "title": "Dynamic Data Mixing Maximizes Instruction Tuning for Mixture-of-Experts",
    "volume": "review",
    "abstract": "Mixture-of-Experts (MoE) models have shown remarkable capability in instruction tuning, especially when the number of tasks scales.However, previous methods simply merge all training tasks (e.g. creative writing, coding, and mathematics) and apply fixed sampling weights, without considering the importance of different tasks as the model training state changes. In this way, the most helpful data cannot be effectively distinguished, leading to suboptimal model performance.To reduce the potential redundancies of datasets, we make the first attempt and propose a novel dynamic data mixture for MoE instruction tuning.Specifically, inspired by MoE's token routing preference, we build dataset-level representations and then capture the subtle differences among datasets.Finally, we propose to dynamically adjust the sampling weight of datasets by their inter-redundancies, thus maximizing global performance under a limited training budget.The experimental results on two MoE models demonstrate the effectiveness of our approach on both downstream knowledge \\& reasoning tasks and open-ended queries",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bAX7OJPk5A": {
    "title": "External Knowledge-Driven Argument Mining: Leveraging Attention-Enhanced Multi-Network Model",
    "volume": "review",
    "abstract": "Argument mining involves the identification of the argument relations (AR) between pair of propositions. Often, the AR between propositions depends on contextual factors, although this context is not always overtly stated within the propositions. While humans possess the ability to infer the requisite background context for linking propositions, machines encounter challenges in doing so when such contextual cues are not explicitly provided. Establishing connections between propositions relies on maintaining a coherent flow of interconnected ideas, often centered around discussed entities, topics, or themes. This study aims to leverage external resources, including WordNet, ConceptNet, and Wikipedia, by aligning the entities within propositions with these external resources to identify the interconnected flow of ideas by analysing the chain of semantic relations linking these entities. To effectively extract relevant information from these chains, we propose integrating attention mechanisms into Multi-Network architectures. This integration allows the model to focus not only on the information presented in the propositions but also on the chain of entities, thereby enhancing the AR identification process. We evaluate various configurations of the architecture on two ontological resources (WordNet, ConceptNet) and Wikipedia, achieving a new state of the art performance with F-measure scores of 0.85, 0.84, and 0.71 respectively on three diverse datasets",
    "checked": false,
    "id": "56341b4ed36fcac1df34af651649a442ab66d81b",
    "semantic_title": "ekigcn: external knowledge injected graph convolutional networks for aspect-based sentiment analysis",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rwjm7efIO2": {
    "title": "Improving In-Context Learning with Prediction Feedback for Sentiment Analysis",
    "volume": "review",
    "abstract": "Large language models (LLMs) have achieved promising results in sentiment analysis through the in-context learning (ICL) paradigm. However, their ability to distinguish subtle sentiments still remains a challenge. Inspired by the human ability to adjust understanding via feedback, this paper enhances ICL by incorporating prior predictions and feedback, aiming to rectify sentiment misinterpretation of LLMs. Specifically, the proposed framework consists of three steps: (1) acquiring prior predictions of LLMs, (2) devising predictive feedback based on correctness, and (3) leveraging a feedback-driven prompt to refine sentiment understanding. Experimental results across nine sentiment analysis datasets demonstrate the superiority of our framework over conventional ICL methods, with an average F1 improvement of 5.95%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LCFdUXDKfO": {
    "title": "MM-Soc: A Comprehensive Benchmark for Multimodal Large Language Models in Social Media Platforms",
    "volume": "review",
    "abstract": "Social media platforms are hubs for multimodal information exchange, encompassing text, images, and videos, making it challenging for machines to comprehend the information or emotions associated with interactions in online spaces. Multimodal Large Language Models (MLLMs) have emerged as a promising solution to address these challenges, yet struggle with accurately interpreting human emotions and complex contents like misinformation. This paper introduces MM-Soc, a comprehensive benchmark designed to evaluate MLLMs' understanding of multimodal social media content. MM-Soc compiles prominent multimodal datasets and incorporates a novel large-scale YouTube tagging dataset, targeting a range of tasks from misinformation detection, hate speech detection, and social context generation. Through our exhaustive evaluation on ten size-variants of four open-source MLLMs, we have identified significant performance disparities, highlighting the need for advancements in models' social understanding capabilities. Our analysis reveals that, in a zero-shot setting, various types of MLLMs generally exhibit difficulties in handling social media tasks. However, MLLMs demonstrate performance improvements post fine-tuning, suggesting potential pathways for improvement. Our code and data are available at Anonymous GitHub (https://anonymous.4open.science/r/MLLMEval-875E)",
    "checked": false,
    "id": "3fd4a8f70f60c9cf945762d1a4c3f70b90496e7f",
    "semantic_title": "mm-soc: benchmarking multimodal large language models in social media platforms",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=lh8mgykc3L": {
    "title": "Enhanced Visual Instruction Tuning for Text-Rich Image Understanding",
    "volume": "review",
    "abstract": "Instruction tuning enhances the capability of Large Language Models (LLMs) to interact with humans. Furthermore, recent instruction-following datasets include images as visual input, collecting responses for image-based instructions. However, current visual instruction-tuned models cannot comprehend textual details within images well. This work enhances the current open-source visual instruction tuning models with text-rich images (e.g., movie posters, book covers, etc.). Specifically, we first used publicly available OCR tools to collect results on 422K text-rich images from the LAION dataset. Furthermore, we prompt text-only GPT-4 with recognized text and image captions to generate 16K conversations, each containing question-answer pairs for text-rich images. Using the above-collected data, we substantially improve (up to 20% accuracy improvement) the zero-shot capability of two open-source backbone models on seven datasets (text-based VQA, Information Extraction, ChartQA, etc.). The GPT-4-based instruction-following evaluation also demonstrates the improvement of our model on both natural images and text-rich images. We will make our code/data/models publicly available",
    "checked": false,
    "id": "a9d5d97733ccb15002ff3cfb95b0a7d8ba5236e3",
    "semantic_title": "llavar: enhanced visual instruction tuning for text-rich image understanding",
    "citation_count": 121,
    "authors": []
  },
  "https://openreview.net/forum?id=IAmq1i-CtV": {
    "title": "Plum: Prompt Learning using Metaheuristics",
    "volume": "review",
    "abstract": "Since the emergence of large language models, prompt learning has become a popular method for optimizing and customizing these models. Special prompts, such as Chain-of-Thought, have even revealed previously unknown reasoning capabilities within these models. However, the progress of discovering effective prompts has been slow, driving a desire for general prompt optimization methods. Unfortunately, few existing prompt learning methods satisfy the criteria of being truly \"general\", i.e., automatic, discrete, black-box, gradient-free, and interpretable all at once. In this paper, we introduce metaheuristics, a branch of discrete non-convex optimization methods with over 100 options, as a promising approach to prompt learning. Within our paradigm, we test six typical methods: hill climbing, simulated annealing, genetic algorithms with/without crossover, tabu search, and harmony search, demonstrating their effectiveness in white-box and black-box prompt learning. Furthermore, we show that these methods can be used to discover more human-understandable prompts that were previously unknown in both reasoning and image generation tasks, opening the door to a cornucopia of possibilities in prompt optimization",
    "checked": false,
    "id": "c874aa93efe663ed31f2ec72d45a5dd4b4cdffba",
    "semantic_title": "plum: prompt learning using metaheuristic",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=OT00knMzLY": {
    "title": "SummEQuAL: Summarization Evaluation via Question Answering using Large Language Models",
    "volume": "review",
    "abstract": "Summarization is hard to evaluate due to its diverse and abstract nature. Although N-gram-based metrics like BLEU and ROUGE are prevalent, they often do not align well with human evaluations. While model-based alternatives such as BERTScore improve, they typically require extensive labelled data. The advent of Large Language Models (LLMs) presents a promising avenue for evaluation. To this end, we introduce SummEQuAL, a novel content-based framework using LLMs for unified, reproducible summarization evaluation. SummEQuAL evaluates summaries by comparing their content with the source document, employing a question-answering approach to gauge both recall and precision. To validate SummEQuAL's effectiveness, we develop a dataset based on MultiWOZ. We conduct experiments on SummEval and our MultiWOZ-based dataset, showing that SummEQuAL largely improves the quality of summarization evaluation. Notably, SummEQuAL demonstrates a 19.7% improvement over QuestEval in terms of sample-level Pearson correlation with human assessments of consistency on the SummEval dataset. Furthermore, it exceeds the performance of the BERTScore baseline by achieving a 17.3% increase in Spearman correlation on our MultiWOZ-based dataset. Our study illuminates the potential of LLMs for a unified evaluation framework, setting a new paradigm for future summarization evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p8tc02vRzA": {
    "title": "Dependency Transformer Grammars: Integrating Dependency Structures into Transformer Language Models",
    "volume": "review",
    "abstract": "Syntactic Transformer language models aim to achieve better generalization through simultaneously modeling syntax trees and sentences. While prior work has been focusing on adding constituency-based structures to Transformers, we introduce Dependency Transformer Grammars (DTGs), a new class of Transformer language model with explicit dependency-based inductive bias. DTGs simulate dependency transition systems with constrained attention patterns by modifying attention masks, incorporate the stack information through relative positional encoding, and augment dependency arc representation with a combination of token embeddings and operation embeddings. When trained on a dataset of sentences annotated with dependency trees, DTGs achieve better generalization while maintaining comparable perplexity with Transformer language model baselines. DTGs also outperform recent constituency-based models, showing that dependency can better guide Transformer language models. Our code will be publicly available upon acceptance",
    "checked": true,
    "id": "4bd17a54c5562b3a8dff6682e3b4b86b39a17544",
    "semantic_title": "dependency transformer grammars: integrating dependency structures into transformer language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vHvdbVlMk7V": {
    "title": "Foresight v2 - A Large Language Model Medical Forecaster",
    "volume": "review",
    "abstract": "Foresight v2 (FS2) is a Large Language Model based on LLaMa v2 7B and fine-tuned on hospital data for modelling patient timelines. It is capable of understanding a patient's clinical notes and forecasting SNOMED codes for a wide range of biomedical use cases including disorder prediction, medication recommendation, risk prediction, procedure recommendation and many more. FS2 is trained on the free text portion of the MIMIC-III dataset, firstly through the extraction of biomedical concepts and then the creation of contextualised patient timelines, upon which the model is then fine-tuned. The results show significant improvement over the previous state-of-the-art for the next new biomedical concept prediction (P/R - 0.71/0.64 vs 0.52/0.32) and a similar improvement specifically for the next new disorder forecast (P/R - 0.66/0.59 vs 0.46/0.25). Finally, on the task of disorder forecast, we compare this model, to GPT-4-turbo, and show that FS2 performs significantly better on such tasks (P@5 - 0.84 vs 0.62). This highlights the need to incorporate real health data into LLMs and shows that even much smaller models when fine-tuned on high-quality specialised data outperform much larger ones",
    "checked": false,
    "id": "79c5f79827c643c1e9464c2a5ff4d1326db16f20",
    "semantic_title": "ai chatbots not yet ready for clinical use",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=IZu2pA2mXt": {
    "title": "TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles",
    "volume": "review",
    "abstract": "In light of recent advances in large language models (LLMs), the expectations for the next generation of virtual assistants include enhanced naturalness and adaptability across diverse usage scenarios.However, the creation of high-quality annotated data for Task-Oriented Dialog (TOD) is recognized to be slow and costly.To address these challenges, we introduce Task-Oriented Automatic Dialogs (TOAD), a novel and scalable TOD dataset along with its automatic generation pipeline.The TOAD dataset simulates realistic app context interaction and provide a variety of system response style options.Two aspects of system response styles are considered, verbosity level and users' expression mirroring.We benchmark TOAD on two response generation tasks and the results show that modelingmore verbose or responses without user expression mirroring is more challenging.The data and code will be released",
    "checked": true,
    "id": "bddf62f93b2200d058c08983b5bb16207be73c4d",
    "semantic_title": "toad: task-oriented automatic dialogs with diverse response styles",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=DioaoZbCsyU": {
    "title": "Enhanced Hallucination Detection in Neural Machine Translation through Simple Detector Aggregation",
    "volume": "review",
    "abstract": "Hallucinated translations pose significant threats and safety concerns when it comes to practical deployment of machine translation systems. Previous research works have identified that detectors exhibit complementary performance --- different detectors excel at detecting different types of hallucinations. In this paper, we propose to address the limitations of individual detectors by combining them and introducing a straightforward method for aggregating multiple detectors. Our results demonstrate the efficacy of our aggregated detector, providing a promising step towards evermore reliable machine translation systems",
    "checked": true,
    "id": "130c2241efd4f455f55c570a4f8ba07dd4207e9d",
    "semantic_title": "enhanced hallucination detection in neural machine translation through simple detector aggregation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Fm1LLPjwRO": {
    "title": "Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning",
    "volume": "review",
    "abstract": "Generative Commonsense Reasoning (GCR) requires a model to reason about a situation using commonsense knowledge, while generating coherent sentences. Although the quality of the generated sentences is crucial, the diversity of the generation is equally important because it reflects the model's ability to use a range of commonsense knowledge facts. Large Language Models (LLMs) have shown proficiency in enhancing the generation quality across various tasks through in-context learning (ICL) using given examples without the need for any fine-tuning. However, the diversity aspect in LLM outputs has not been systematically studied before. To address this, we propose a simple method that diversifies the LLM generations, while preserving their quality. Experimental results on three benchmark GCR datasets show that our method achieves an ideal balance between the quality and diversity. Moreover, the sentences generated by our proposed method can be used as training data to improve diversity in existing commonsense generators",
    "checked": true,
    "id": "522240c89c7ac6b1365e8308c6a88c4784adc62e",
    "semantic_title": "improving diversity of commonsense generation by large language models via in-context learning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=LnyvQpDsvXD": {
    "title": "MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling",
    "volume": "review",
    "abstract": "A major consideration in multilingual language modeling is how to best represent languages with diverse vocabularies and scripts.Although contemporary text encoding methods cover most of the world's writing systems, they exhibit bias towards the high-resource languages of the Global West.As a result, texts of underrepresented languages tend to be segmented into long sequences of linguistically meaningless units.To address the disparities, we introduce a new paradigm that encodes the same information with segments of consistent size across diverse languages.Our encoding convention (MYTE) is based on morphemes, as their inventories are more balanced across languages than characters, which are used in previous methods.We show that MYTE produces shorter encodings for all 99 analyzed languages, with the most notable improvements for non-European languages and non-Latin scripts.This, in turn, improves multilingual LM performance and diminishes the perplexity gap throughout diverse languages",
    "checked": true,
    "id": "8efaa8206874c2f7a79bba2a9bcba542e4cabf31",
    "semantic_title": "myte: morphology-driven byte encoding for better and fairer multilingual language modeling",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=xh8KBUiRgK": {
    "title": "What's the Plan? Evaluating and Developing Planning-Aware Techniques for LLMs",
    "volume": "review",
    "abstract": "Planning is a fundamental task in artificial intelligence that involves finding a sequence of actions that achieve a specified goal in a given environment.Large language models (LLMs) are increasingly used for applications that require planning capabilities, such as web or embodied agents.In line with recent studies, we demonstrate through experimentation that LLMs lack necessary skills required for planning.Based on these observations, we advocate for the potential of a hybrid approach that combines LLMs with classical planning methodology. Then, we introduce SimPlan, a novel hybrid-method, and evaluate its performance in a new challenging setup. Our extensive experiments across various planning domains demonstrate that SimPlan significantly outperforms existing LLM-based planners",
    "checked": true,
    "id": "3fd086ed4ef6b5ead31ebbac4d8bedd9faba25e6",
    "semantic_title": "what's the plan? evaluating and developing planning-aware techniques for llms",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=aReb-02mhR": {
    "title": "Mitigating Hallucinations in Large Vision-Language Models (LVLMs) via Language-Contrastive Decoding (LCD)",
    "volume": "review",
    "abstract": "Large Vision-Language Models (LVLMs) are an extension of Large Language Models (LLMs) that facilitate processing both image and text inputs, expanding AI capabilities. However, LVLMs struggle with object hallucinations due to their reliance on text cues and learned object co-occurrence biases. While most research quantifies these hallucinations, mitigation strategies are still lacking. Our study introduces a Language Contrastive Decoding (LCD) algorithm that adjusts LVLM outputs based on LLM distribution confidence levels, effectively reducing object hallucinations. We demonstrate the advantages of LCD in leading LVLMs, showing up to \\%4 improvement in POPE F1 scores and up to \\%36 reduction in CHAIR scores on the COCO validation set, while also improving captioning quality scores. Our method effectively improves LVLMs without needing complex post-processing or retraining, and is easily applicable to different models. Our findings highlight the potential of further exploration of LVLM-specific decoding algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2kQOM-K9OIv": {
    "title": "Parameter and Memory Efficient Language Model Compression using Fisher Information from Low-Rank Representations",
    "volume": "review",
    "abstract": "Modern language models demonstrate excellent performance in diverse text processing tasks. Yet, to achieve the best quality, memory and computationally demanding fine-tuning on a downstream task is required. While PEFT methods, such as LoRA enable almost no VRAM overhead for fine-tuning, the amount of memory and compute may be still prohibitive for the regular users. To compress and speed up LMs pruning techniques, such as Fisher-Weighted Singular Value Decomposition (FWSVD) (https://arxiv.org/abs/2207.00112) are therefore additionally used. Yet, FWSVD requires a downstream task fine-tuning to gather Fisher information. Our work tries to break this vicious circle of dependence on large expensive GPU showing that state-of-the-art LM compression, such as FWSVD, can be done without storing the full gradients. Namely, our approach combines the reduced number of training parameters up to $0,01\\%$ of the initial amount of parameters and the VRAM utilization up to 15\\%, for a pruning $20\\%$ of the fine-tuned model weights without any noticeable loss of accuracy. We evaluate this approach on various tasks including NLU, NER, MMLU, and summarization demonstrating the effectiveness of the method as compared to strong baselines",
    "checked": false,
    "id": "0d2f828efd1efdd57c3c4455d09413f2d3b5c11f",
    "semantic_title": "lq-lora: low-rank plus quantized matrix decomposition for efficient language model finetuning",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=dzau2BLjIl9": {
    "title": "Multi-Hop Table Retrieval for Open-Domain Text-to-SQL",
    "volume": "review",
    "abstract": "Open-domain text-to-SQL is an important task that retrieves question-relevant tables from massive databases and then generates SQL. However, existing retrieval methods that retrieve in a single hop do not pay attention to the text-to-SQL challenge of schema linking, which is aligning the entities in the question with table entities, reflected in two aspects: similar irrelevant entity and domain mismatch entity. Therefore, we propose our method, the multi-hop table retrieval with rewrite and beam search (Murre). To reduce the effect of the similar irrelevant entity, our method focuses on unretrieved entities at each hop and considers the low-ranked tables by beam search. To alleviate the limitation of domain mismatch entity, Murre rewrites the question based on retrieved tables in multiple hops, decreasing the domain gap with relevant tables. We conduct experiments on SpiderUnion and BirdUnion+, reaching new state-of-the-art results with an average improvement of 6.38%",
    "checked": true,
    "id": "df0ad2f70946034174a4a6d814423de69d1e00d8",
    "semantic_title": "multi-hop table retrieval for open-domain text-to-sql",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=7L5e91UPrqw": {
    "title": "Learning to Edit: Aligning LLMs with Knowledge Editing",
    "volume": "review",
    "abstract": "Knowledge editing techniques, aiming to efficiently modify a minor proportion of knowledge in large language models (LLMs) without negatively impacting performance across other inputs, have garnered widespread attention. However, existing methods predominantly rely on memorizing the updated knowledge, impeding LLMs from effectively combining the new knowledge with their inherent knowledge when answering questions. To this end, we propose a Learning to Edit (LTE) framework, focusing on teaching LLMs to apply updated knowledge into input questions, inspired by the philosophy of \"Teach a man to fish.\" LTE features a two-phase process: (i) the Alignment Phase, which fine-tunes LLMs on a meticulously curated parallel dataset to make reliable, in-scope edits while preserving out-of-scope information and linguistic proficiency; and (ii) the Inference Phase, which employs a retrieval-based mechanism for real-time and mass knowledge editing. By comparing our approach with seven advanced baselines across four popular knowledge editing benchmarks and two LLM architectures, we demonstrate LTE's superiority in knowledge editing performance, robustness in both batch and sequential editing, minimal interference on general tasks, and rapid editing speeds. The data and code will be publicly available",
    "checked": true,
    "id": "77744ab3d5d7570bdc2a63f17eaf3e6a1b50a1b3",
    "semantic_title": "learning to edit: aligning llms with knowledge editing",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=QCcOOJgEy7w6": {
    "title": "Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models",
    "volume": "review",
    "abstract": "Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces a lossless sparsification method named \"ProSparse\" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization with a factor smoothly increasing along sine curves in multiple stages. This can enhance activation sparsity and alleviate performance degradation by avoiding radical shifts in activation distribution. With ProSparse, we obtain high sparsity of 89.32% and 88.80% for LLaMA2-7B and LLaMA2-13B, respectively, achieving comparable performance to their original Swish-activated versions. Our inference acceleration experiments further demonstrate the practical acceleration brought by higher activation sparsity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nsecLy8prlo": {
    "title": "FANTAstic SEquences and Where to Find Them: Faithful and Efficient API Call Generation through State-tracked Constrained Decoding and Reranking",
    "volume": "review",
    "abstract": "API call generation is the cornerstone of large language models' tool-using ability that provides access to the larger world. However, existing supervised and in-context learning approaches suffer from high training costs, poor data efficiency, and generated API calls that can be unfaithful to the API documentation and the user's request. To address these limitations, we propose an output-side optimization approach called FANTASE. Two of the unique contributions of FANTASE are its State-Tracked Constrained Decoding (SCD) and Reranking components. SCD dynamically incorporates appropriate API constraints in the form of Token Search Trie for efficient and guaranteed generation faithfulness with respect to the API documentation. The Reranking component efficiently brings in the supervised signal by leveraging a lightweight model as the discriminator to rerank the beam-searched candidate generations of the large language model. We demonstrate the superior performance of FANTASE in API call generation accuracy, inference efficiency, and context efficiency with DSTC8 and API Bank datasets",
    "checked": true,
    "id": "4c3a076ba646fcb851efe42f6e3c62c3b4e9c24b",
    "semantic_title": "fantastic sequences and where to find them: faithful and efficient api call generation through state-tracked constrained decoding and reranking",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mob3Muf8y5": {
    "title": "ILogicEval: A Counterintuitive Logical Reasoning Evaluation Dataset",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have garnered significant attention worldwide due to their increasing size and improving capabilities. However, as LLMs continue to expand, traditional benchmark datasets are becoming less effective in evaluating their reasoning skills. This is primarily due to the difficulty of the tasks and issues with data contamination. Meanwhile, in the domain of logical reasoning, existing benchmarks often lack the ability to isolate specific reasoning abilities and fail to provide sufficient evidence for answer derivation. To address these issues, a novel dataset ILogicEval is proposed, which consists of sentences composed of unrelated statements, challenging LLMs to answer questions that cannot be solved based on their learned knowledge. ILogicEval is carefully designed to incorporate rich language diversity and assess the logical reasoning ability of LLMs independently of other reasoning skills, such as commonsense reasoning. To ensure a more reliable evaluation, we also introduce a new evaluation metric that mitigates the influence of bias and randomness inherent in LLMs. Through experiments, we demonstrate the extent to which logical reasoning is required to answer the questions in ILogicEval and compare the performance of different popular LLMs in conducting logical reasoning. This dataset and evaluation metric address the limitations of existing benchmarks, providing a comprehensive assessment of the logical reasoning capabilities of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R9zkeebiOQ0": {
    "title": "Towards Completeness-Oriented Tool Retrieval for Large Language Models",
    "volume": "review",
    "abstract": "Recently, enhancing the capabilities of Large Language Models (LLMs) through interaction with external tools has gathered widespread interest, where tool retrieval emerges as a crucial step. Existing tool retrieval approaches only focus on semantic matching. However, effective tool retrieval requires consideration of collaborative invocation among multiple tools rather than solely evaluating the utility of individual tools, which presents a challenge to existing tool retrieval methods. To address this, we propose a novel COllaborative Learning-based Tool Retrieval approach, COLT, which manages not only the semantic matching between user queries and tool descriptions but also takes into account the collaborative information of tools. Extensive experiments on both the open benchmark and the introduced ToolLens dataset show that COLT achieves superior performance. Notably, the performance of BERT-mini (11M) with our COLT framework outperforms BERT-large (340M), which has 30 times more parameters. Our codes and data are publicly available at https://anonymous.4open.science/r/COLT-4D13",
    "checked": false,
    "id": "493bc1ea655b9c6450a951a28d67c157e4712ce9",
    "semantic_title": "colt: towards completeness-oriented tool retrieval for large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6s-LAw3wvg2": {
    "title": "Parallel Structures in Pre-training Data Yield In-Context Learning",
    "volume": "review",
    "abstract": "Pre-trained language models (LMs) are capable of in-context learning (ICL): they can adapt to a task with only a few examples given in the prompt without any parameter update. However, it is unclear where this capability comes from as there is a stark distribution shift between pre-training text and ICL prompts. In this work, we study what patterns of the pre-training data contribute to ICL. We find that LMs' ICL ability depends on $\\textit{parallel structures}$ in the pre-training data---pairs of phrases following similar templates in the same context window. Specifically, we detect parallel structures by checking whether training on one phrase improves prediction of the other, and conduct ablation experiments to study their effect on ICL. We show that removing parallel structures in the pre-training data reduces LMs' ICL accuracy by 51% (vs 2% from random ablation). This drop persists even when excluding common patterns such as n-gram repetitions and long-range dependency, showing the diversity and generality of parallel structures. A closer look at the detected parallel structures indicates that they cover diverse linguistic tasks and span long distances in the data",
    "checked": true,
    "id": "046cbc4c77ed65b33e7b95db058284824da3b580",
    "semantic_title": "parallel structures in pre-training data yield in-context learning",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=Yj5sBs-htAV": {
    "title": "Can Large Language Models Interpret Noun-Noun Compounds? A Linguistically-Motivated Study on Lexicalized and Novel Compounds",
    "volume": "review",
    "abstract": "Noun-noun compounds interpretation is the task where a model is given one of such constructions, and it is asked to provide a paraphrase, making the semantic relation between the nouns explicit, as in carrot cake is ``a cake made of carrots.'' Such a task requires the ability to understand the implicit structured representation of the compound meaning.In this paper, we test to what extent the recent Large Language Models can interpret the semantic relation between the constituents of lexicalized English compounds and whether they can abstract from such semantic knowledge to predict the semantic relation between the constituents of similar but novel compounds (e.g., carrot dessert). We test both Surprisal metrics and prompt-based methods to see whether i.) they can correctly predict the relation between constituents, and ii.) the semantic representation of the relation is robust to paraphrasing.Using a dataset of lexicalized and annotated noun-noun compounds, we find that LLMs can infer some semantic relations better than others (with a preference for compounds involving concrete concepts). When challenged to perform abstractions and transfer their interpretations to semantically similar but novel compounds, LLMs show serious limitations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vekIBnXZWqH": {
    "title": "Dial BeInfo for Faithfulness: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning",
    "volume": "review",
    "abstract": "Factual faithfulness is a crucial requirement in information-seeking dialogue: the system should respond to the user queries so that the responses are meaningful and aligned with the knowledge provided to the system. However, most modern large language models (LLMs) suffer from hallucinations, that is, they generate responses not supported by or even contradicting the knowledge source. To mitigate the issue and increase faithfulness of information-seeking dialogue systems supported by the LLMs, we introduce BEINFO, a simple yet effective method that applies â€˜behavioural tuning' on the LLMs to aid information-seeking dialogue. Relying on three standard information seeking dialogue datasets, we show that models tuned with BEINFO become considerably more faithful to the knowledge source both for datasets and domains seen during BEINFO-tuning, as well as on unseen domains, when applied in a zero-shot manner. In addition, we present a â€˜real-life' case study on conversations with real users, showcasing that the models with 3B parameters (e.g., Flan-T5) tuned with BEINFO demonstrate strong performance on data from real â€˜production' conversations: when tuned on a limited amount of such realistic in-domain dialogues, they surpass much larger LLMs used â€˜off-the-shelf', both on automatic and human evaluation metrics",
    "checked": true,
    "id": "d2d3960bd1e4e84cbb0051945ea1419005e75e07",
    "semantic_title": "dial beinfo for faithfulness: improving factuality of information-seeking dialogue via behavioural fine-tuning",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=BmDawbt0BM": {
    "title": "Exploring the Prompt Sensitivity in LLM: A Methodological Framework and Empirical Analysis",
    "volume": "review",
    "abstract": "This paper examines LLM sensitivity to natural language prompts and proposes methods to enhance robustness. Despite their versatility, LLMs show performance volatility with prompt changes. We introduce \\lib, which featuring diverse, semantically consistent prompts mimicking human expression patterns for multiple LLM evaluations. Experiments with \\lib confirm that model size or baseline metrics do not correlate with prompt sensitivity, and subtle perturbations can impact results. We find in-context examples and diverse training instructions improve LLM resilience against different question forms. We believe this work will serve as a helpful tool in studying LLM robustness under human-like expressions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZnyqApWcrQ": {
    "title": "What Have We Achieved on Non-autoregressive Translation?",
    "volume": "review",
    "abstract": "Recent advances have made non-autoregressive (NAT) translation comparable to autoregressive methods (AT). However, their evaluation using BLEU has been shown to weakly correlate with human annotations. Limited research compares non-autoregressive translation and autoregressive translation comprehensively, leaving uncertainty about the true proximity of NAT to AT. To address this gap, we systematically evaluate four representative NAT methods across various dimensions, including human evaluation. Our empirical results demonstrate that despite narrowing the performance gap, state-of-the-art NAT still underperforms AT under more reliable evaluation metrics. Furthermore, we discover that explicitly modeling dependencies is crucial for generating natural language and generalizing to out-of-distribution sequences",
    "checked": true,
    "id": "57b75d722e62f60ba4c25e573aebe8095d5ab819",
    "semantic_title": "what have we achieved on non-autoregressive translation?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pJl_i7HIA72": {
    "title": "The Scandinavian Embedding Benchmarks: Comprehensive Assessment of Multilingual and Monolingual Text Embedding",
    "volume": "review",
    "abstract": "The evaluation of English text embeddings has transitioned from evaluating on a handful of datasets to broad coverage across many tasks through benchmarks such as MTEB. However, this is not the case for multilingual text embeddings due to a lack of available benchmarks. To address this problem, we introduce the Scandinavian Embedding Benchmark (SEB). SEB is a comprehensive framework that enables text embedding evaluation for Scandinavian languages across 24 tasks, 10 subtasks, and 4 task categories. Building on SEB, we evaluate more than 26 models, uncovering significant performance disparities between public and commercial as well as monolingual and multilingual text embedding models. We open-source SEB and integrate it with MTEB, thus bridging the text embedding evaluation gap for Scandinavian languages",
    "checked": true,
    "id": "171d74557fa5d51bb8f7b8f21544e4f306552e59",
    "semantic_title": "the scandinavian embedding benchmarks: comprehensive assessment of multilingual and monolingual text embedding",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=At6xKkmJuaJ": {
    "title": "Dictionary-Aided Translation for Handling Multi-Word Expressions in Low-Resource Languages",
    "volume": "review",
    "abstract": "Multi-word expressions (MWEs) present unique challenges in natural language processing (NLP), particularly within the context of translation systems, due to their inherent scarcity, non-compositional nature, and other distinct lexical and morphosyntactic characteristics, issues that are exacerbated in low-resource settings.In this study, we elucidate and attempt to address these challenges by leveraging a substantial corpus of human-annotated Greek MWEs. To address the complexity of translating such phrases, we propose a novel method leveraging an available out-of-context lexicon.We assess the translation capabilities of current state-of-the-art systems on this task, employing both automated metrics and human evaluators.We find that by using our method when applicable, the performance of current systems can be significantly improved, however these models are still unable to produce translations comparable to those of a human speaker",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RhiJs2jlYY": {
    "title": "Demystifying Instruction Mixing for Fine-tuning Large Language Models",
    "volume": "review",
    "abstract": "Instruction tuning significantly enhances the performance of large language models (LLMs) across various tasks. However, the procedure to optimizing the mixing of instruction datasets for LLM fine-tuning is still poorly understood. This study categorizes instructions into three primary types: NLP downstream tasks, coding, and general chat. We explore the effects of instruction tuning on different combinations of datasets on LLM performance, and find that certain instruction types are more advantageous for specific applications but can negatively impact other areas. This work provides insights into instruction mixtures, laying the foundations for future research",
    "checked": true,
    "id": "ec9414654469692d8f1de8e2401a3dcbc58ee11a",
    "semantic_title": "demystifying instruction mixing for fine-tuning large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4vRO48RwVG": {
    "title": "MathBench: Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics Benchmark",
    "volume": "review",
    "abstract": "Recent advancements in large language models (LLMs) have showcased significant improvements in mathematics. However, traditional math benchmarks like GSM8k offer a unidimensional perspective, which fall short in providing a holistic assessment of the LLMs' math capabilities. To address this gap, we introduce MathBench, a new benchmark that rigorously assesses the mathematical capabilities of large language models. MathBench spans a wide range of mathematical disciplines, offering a detailed evaluation of both theoretical understanding and practical problem-solving skills. The benchmark progresses through five distinct stages, from basic arithmetic to college mathematics, and is structured to evaluate models at various depths of knowledge. Each stage includes theoretical questions and application problems, allowing us to measure a model's mathematical proficiency and its ability to apply concepts in practical scenarios. MathBench aims to enhance the evaluation of LLMs' mathematical abilities, providing a nuanced view of their knowledge understanding levels and problem solving skills in a bilingual context",
    "checked": true,
    "id": "f957b64a4ce31dd9032570ee11b9fca942365f81",
    "semantic_title": "mathbench: evaluating the theory and application proficiency of llms with a hierarchical mathematics benchmark",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=kT_Fn0DGhe": {
    "title": "RadGraph-XL: A Large-Scale Expert-Annotated Dataset for Entity and Relation Extraction from Radiology Reports",
    "volume": "review",
    "abstract": "In order to enable extraction of structured clinical data from unstructured radiology reports, we introduce RadGraph-XL, a large-scale, expert-annotated dataset for clinical entity and relation extraction. RadGraph-XL consists of 2,300 radiology reports, which are annotated with over 410,000 entities and relations by board-certified radiologists. Whereas previous approaches focus solely on chest X-rays, RadGraph-XL includes data from four anatomy-modality pairs - chest CT, abdomen/pelvis CT, brain MR, and chest X-rays. Then, in order to automate structured information extraction, we use RadGraph-XL to train transformer-based models for clinical entity and relation extraction. Our evaluations include comprehensive ablation studies as well as an expert reader study that evaluates trained models on out-of-domain data. Results demonstrate that our model surpasses the performance of previous methods by up to 52\\% and notably outperforms GPT-4 in this domain. We release RadGraph-XL as well as our trained model to foster further innovation and research in structured clinical information extraction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pYCmHEuqMe": {
    "title": "Deciphering Multi-task Learning: Comparative Insights for Similar and Dissimilar Tasks",
    "volume": "review",
    "abstract": "Multi-Task Learning (MTL), emerged as a powerful concept in the era of machine learning, employs a shared model trained to handle multiple tasks at the same time. Numerous advantages of this novel approach inspire us to instigate the insights of various tasks with similar (Identification of Sentiment, Emotion, Sarcasm, Irony, Hate and Offensive) and dissimilar (Identification of Sentiment, Claim, Language) genres and to analyze the change in their performances with respect to long and short head approaches. We shed light on the methods employed and critical observations to promote more efficient learning paradigm across similar and dissimilar tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NjOYJLs-x7": {
    "title": "SMELLM: A Paradigm to Integrating Domain Knowledge into LLMs via the Retrieval Augmented Generation",
    "volume": "review",
    "abstract": "The utilization of large language models (LLMs) offers promising opportunities to expedite scientific discovery. However, deploying LLMs to answer scientific questions within specific interdisciplinary research domains, such as single-molecule electronics, poses various challenges that arise from the uniqueness of domain-specific data, the complexity of domain knowledge, and the uniqueness of domain objectives. To address this gap, we propose a paradigm for integrating domain knowledge from single-molecule electronics into LLMs using the retrieval-augmented generation (RAG) framework, named SMELLM. Evaluation results demonstrate that SMELLM achieves a higher SciBERT score than GPT and ChatGPT, with SMELLM-4.0 notably achieving a SciBERT score of 0.731 and a Faithfulness score of 0.916. The responses generated by SMELLM are firmly grounded in domain-specific facts, indicating significant enhancements in LLM capabilities for domain-specific natural language understanding tasks. Furthermore, SMELLM is adaptable for enhancing and evaluating proficiency in LLM across other scientific domains with low computing resource consumption",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DzCD_TDQVzK": {
    "title": "Ground-truthing perspectives on highly subjective text: basic human values perceived in song lyrics",
    "volume": "review",
    "abstract": "We present an interdisciplinary approach to gathering a dataset on a highly subjective text annotation task. The task thus requires explicit insight into broad human annotator perceptions, and conscious curation of what will be annotated. With strong inspiration from best practices in the social sciences, we add to emerging and increasing calls for greater accountability with regard to data and its quality. For our task, we choose the annotation of human values as they are perceived in song lyrics. We present our strategy to select song lyrics for annotation, draw annotators from a representative US sample, estimate number of annotators needed, and assess data quality. We obtain a dataset of 360 richly annotated lyrics, and substantiate the benefits of our approach, which can be adapted to many domains and tasks. Finally, we give a first illustration of how our data can be employed in connection to applied machine learning approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YsmnPHBbx1f": {
    "title": "How Credible Is an Answer From Retrieval-Augmented LLMs? Investigation and Improvement With Multi-Hop QA",
    "volume": "review",
    "abstract": "Retrieval-augmented Large Language Models (RaLLMs) are reshaping knowledge acquisition, offering long-form, knowledge-grounded answers through advanced reasoning and generation capabilities. Despite the emergence of impactful systems like WebGPT and New Bing, the reliability of RaLLMs, especially in complex situations, is under scrutiny. Our study tackles this concern by evaluating RaLLMs' question-answering performance using a novel benchmark focusing on Correctness and Groundedness. Correctness measures the logical soundness of the responses, and Groundedness checks for support by relevant references. We introduce an automated model-based evaluation pipeline for multi-hop question-answering tasks, revealing RaLLMs' proneness to generating inaccuracies when dealing with flawed or partial knowledge. To improve accuracy, we introduce two reasoning strategies, Self-Reflection' and Self-Completion,' enabling RaLLMs to identify and fill knowledge gaps, significantly improving answer quality without extensive model retraining",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x8UVDvruAf": {
    "title": "Multi-Granularity History and Entity Similarity Learning for Temporal Knowledge Graph Reasoning",
    "volume": "review",
    "abstract": "Temporal Knowledge Graph (TKG) reasoning, aiming to predict future unknown facts based on historical information, has attracted considerable attention due to its great practical value. Insight into history is the key to predict the future. However, most existing TKG reasoning models singly capture repetitive history, ignoring the entity's multi-hop neighbour history which can provide valuable background knowledge for TKG reasoning. In this paper, we propose \\textbf{M}ulti-\\textbf{G}ranularity History and \\textbf{E}ntity \\textbf{S}imilarity \\textbf{L}earning (MGESL) model for Temporal Knowledge Graph Reasoning, which models historical information from both coarse-grained and fine-grained history. Since similar entities tend to exhibit similar behavioural patterns, we also design a hypergraph convolution aggregator to capture the similarity between entities. Furthermore, we introduce a more realistic setting for the TKG reasoning, where candidate entities are already known at the timestamp to be predicted. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our proposed model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Opz1Il91NKg": {
    "title": "Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning",
    "volume": "review",
    "abstract": "Large language models (LLMs) often generate convincing, fluent explanations. However, different from humans, they often generate $\\textit{inconsistent}$ explanations on different inputs. For example, an LLM may generate the explanation \"$\\textit{all birds can fly}$\" when answering the question \"$\\textit{Can sparrows fly?}$\" but meanwhile answer \"$\\textit{no}$\" to the related question \"$\\textit{Can penguins fly?}$\". Explanations should be consistent across related examples so that they allow a human to simulate the LLM's decision process on multiple examples.We propose $\\textbf{explanation-consistency finetuning}$ (EC-finetuning), a method that adapts LLMs to generate more consistent natural-language explanations on related examples. EC-finetuning involves finetuning LLMs on synthetic data that is carefully constructed to contain consistent explanations. Across a variety of question-answering datasets in various domains, EC-finetuning yields a $\\textbf{10.0\\%}$ relative explanation consistency improvement on four finetuning datasets, and generalizes to seven out-of-distribution datasets not seen during finetuning ($\\textbf{+4.5\\%}$ relative). We will make our code available for reproducibility",
    "checked": true,
    "id": "8c4fc5962aa3bf6112355d1066916daf9bfbf1ce",
    "semantic_title": "towards consistent natural-language explanations via explanation-consistency finetuning",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=UhmDVokjbx": {
    "title": "Mitigating the Linguistic Gap with Phonemic Representations for Robust Multilingual Language Understanding",
    "volume": "review",
    "abstract": "Approaches to improving multilingual language understanding often require multiple languages during the training phase, rely on complicated training techniques, and---importantly---struggle with significant performance gaps between high-resource and low-resource languages. We hypothesize that the performance gaps between languages are affected by linguistic gaps between those languages and provide a novel solution for robust multilingual language modeling by employing phonemic representations (specifically, using phonemes as input tokens to LMs rather than subwords). We present quantitative evidence from three cross-lingual tasks that demonstrate the effectiveness of phonemic representation, which is further justified by a theoretical analysis of the cross-lingual performance gap",
    "checked": true,
    "id": "1cef70a14050c6fb1c773c0b48e3d3500314f14b",
    "semantic_title": "mitigating the linguistic gap with phonemic representations for robust multilingual language understanding",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u9BODP70b1b": {
    "title": "Keyword-Oriented Multimodal Modeling for Euphemism Identification",
    "volume": "review",
    "abstract": "Euphemism identification aims to identify the true meaning of a given euphemism, such as identifying \"weed\" (euphemism) as \"marijuana\" (target keyword) in illicit transactions, which is of great significance to help content moderation and combat underground market. However, existing methods only use text data to identify euphemisms, ignoring the semantic information of other modalities associated with the corresponding target keywords during the development and evolution of euphemisms. Additionally, the lack of multimodal datasets of euphemisms also hinders related research. In this paper, we regard euphemisms and their corresponding target keywords as keywords and propose improving euphemism identification quality through keyword-oriented visual and audio features. To this end, we first introduce a keyword-oriented multimodal corpus of euphemisms (KOM-Euph), involving three datasets (Drug, Weapon, and Sexuality), including text, images, and speech. Then, we propose a keyword-oriented multimodal euphemism identification method (KOM-EI), which uses cross-modal feature alignment and dynamic fusion modules to explicitly utilize the visual and audio features of the keywords for efficient euphemism identification. Extensive experiments demonstrate that our method outperforms the SOTA models and LLMs, and show the importance of our multimodal datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QvB5DoTxacN": {
    "title": "M3-Embedding: Multi-Linguality, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation",
    "volume": "review",
    "abstract": "In this paper, we introduce a new embedding model called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It provides a uniform support for the semantic retrieval of more than 100 working languages. It can simultaneously accomplish the three common retrieval functionalities: dense retrieval, multi-vector retrieval, and sparse retrieval. Besides, it is also capable of processing inputs of different granularities, spanning from short sentences to long documents of up to 8,192 tokens. The effective training of M3-Embedding presents a series of technical contributions. Notably, we propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strategy, which enables a large batch size and high training throughput to improve the discriminativeness of embeddings. M3-Embedding exhibits a superior performance in our experiment, leading to new state-of-the-art results on multilingual, cross-lingual, and long-document retrieval benchmarks",
    "checked": false,
    "id": "4d5735c186ddb2430ac9689ccf61fdcbbfc23abc",
    "semantic_title": "bge m3-embedding: multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=SS9EPqpkAO": {
    "title": "SceMQA: A Scientific College Entrance Level Multimodal Question Answering Benchmark",
    "volume": "review",
    "abstract": "The paper introduces SceMQA, a novel benchmark for scientific multimodal question answering at the college entrance level. It addresses a critical educational phase often overlooked in existing benchmarks, spanning high school to pre-college levels. SceMQA focuses on core science subjects including Mathematics, Physics, Chemistry, and Biology. It features a blend of multiple-choice and free-response formats, ensuring a comprehensive evaluation of AI models' abilities. Additionally, our benchmark provides specific knowledge points for each problem and detailed explanations for each answer. SceMQA also uniquely presents problems with identical contexts but varied questions to facilitate a more thorough and accurate assessment of reasoning capabilities. In the experiment, we evaluate both open-source and close-source state-of-the-art Multimodal Large Language Models (MLLMs), across various experimental settings. The results show that further research and development are needed in developing more capable MLLM, as highlighted by only 50% to 60% accuracy achieved by the strongest models",
    "checked": true,
    "id": "21f1a99feff322b2c2af8c4239eb86ffe64b613b",
    "semantic_title": "scemqa: a scientific college entrance level multimodal question answering benchmark",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=j1TTyjijWnzc": {
    "title": "Arabic Diacritics in the Wild: Exploiting Opportunities for Improved Diacritization",
    "volume": "review",
    "abstract": "The widespread absence of diacritical marks in Arabic text poses a significant challenge for Arabic natural language processing (NLP). This paper explores instances of naturally occurring diacritics, referred to as ``diacritics in the wild,'' to unveil patterns and latent information across six diverse genres: news articles, novels, children's books, poetry, political documents, and ChatGPT outputs. We present a new annotated dataset that maps real-world partially diacritized words to their maximal full diacritization in context. Additionally, we propose extensions to the analyze-and-disambiguate approach in Arabic NLP to leverage these diacritics, resulting in notable improvements. Our contributions encompass a thorough analysis, a valuable dataset, and an extended diacritization algorithm. We release our code and datasets as open source",
    "checked": true,
    "id": "a865f36fc1e4b12551d376a94d0867db45124ea7",
    "semantic_title": "arabic diacritics in the wild: exploiting opportunities for improved diacritization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sg0zpnNKQm": {
    "title": "ELAD: Explanation-Guided Large Language Models Active Distillation",
    "volume": "review",
    "abstract": "The deployment and application of Large Language Models (LLMs) is hindered by their memory inefficiency, computational demands, and the high costs of API inferences. Traditional distillation methods, which transfer the capabilities of LLMs to smaller models, often fail to determine whether the knowledge has been sufficiently transferred, potentially resulting in high costs or incomplete distillation. In this paper, we propose an Explanation-Guided LLMs Active Distillation (ELAD) framework that employs an active learning strategy to optimize the balance between annotation costs and model performance. To improve efficient sample selection, we introduce an explanation-guided sample selection method that identifies samples challenging its reasoning by exploiting uncertainties in explanation steps. Additionally, we present a customized LLM-annotated explanation revision technique where the teacher model detects and corrects flaws in the student model's reasoning. Our experiments across various reasoning datasets demonstrate that our framework significantly enhances the efficiency of LLM knowledge distillation",
    "checked": true,
    "id": "fd49de7395e052a18ceb60a8d8cd81e60c809d9c",
    "semantic_title": "elad: explanation-guided large language models active distillation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=_5IGldrChO": {
    "title": "CVLUE: A New Benchmark Dataset for Chinese Vision-Language Understanding Evaluation",
    "volume": "review",
    "abstract": "The abundance of vision-language (VL) understanding benchmark datasets for English, such as MS-COCO and Flickr30K, has largely facilitated the evaluation of new vision-language models (VLMs) across diverse tasks. However, despite the rapid development of Chinese VLMs, most existing Chinese VL datasets are constructed by re-annotating the images from English VL datasets, limiting the source of images to English-speaking cultures only. Some others are limited to a few fundamental tasks, like image-text retrieval. Such cultural bias and limitation of task types make these datasets unsuitable and inadequate for evaluating VLMs in Chinese culture. To remedy this issue, we present a new Chinese Vision-Language Understanding Evaluation (CVLUE) benchmark dataset, where the selection of object categories and images is entirely driven by Chinese native speakers, ensuring that the source images are representative of Chinese culture. The benchmark contains four distinct VL tasks ranging from image-text retrieval to visual question answering, visual grounding and visual dialogue, which evaluates a model's VL capability from multiple aspects. We present a detailed statistical analysis of CVLUE and provide a baseline performance analysis with several open-source multilingual VLMs on CVLUE and its English counterparts to reveal their performance gap between English and Chinese",
    "checked": true,
    "id": "910b69e67f4906a1578f1598344f917fbbdc00cc",
    "semantic_title": "cvlue: a new benchmark dataset for chinese vision-language understanding evaluation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Aa0xudIZdQr": {
    "title": "IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact",
    "volume": "review",
    "abstract": "Large language models (LLMs) excel in natural language processing but demand intensive computation. To mitigate this, various quantization methods have been explored, yet they compromise LLM performance. This paper unveils a previously overlooked type of outlier in LLMs. Such outliers are found to allocate most of the attention scores on initial tokens of input, termed as pivot tokens, which is crucial to the performance of quantized LLMs. Given that, we propose IntactKV to generate the KV cache of pivot tokens losslessly from the full-precision model. The approach is simple and easy to combine with existing quantization solutions. Besides, IntactKV can be calibrated as additional LLM parameters to boost the quantized LLMs further. Mathematical analysis also proves that IntactKV effectively reduces the upper bound of quantization error. Empirical results show that IntactKV brings consistent improvement and achieves lossless weight-only INT4 quantization on various downstream tasks, leading the new state-of-the-art for LLM quantization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jCyPNmuhaaX": {
    "title": "Estimating Agreement by Chance for Sequence Annotation",
    "volume": "review",
    "abstract": "In the field of natural language processing, correction of performance assessment for chance agreement plays a crucial role in evaluating the reliability of annotations. However, there is a notable dearth of research focusing on chance correction for assessing the reliability of sequence annotation tasks, despite their widespread prevalence in the field. To address this gap, this paper introduces a novel model for generating random annotations, which serves as the foundation for estimating chance agreement in sequence annotation tasks. Utilizing the proposed randomization model and a related comparison approach, we successfully derive the analytical form of the distribution, enabling the computation of the probable location of each annotated text segment and subsequent chance agreement estimation. Through a combination simulation and corpus-based evaluation, we successfully assess its applicability and validate its accuracy and efficacy",
    "checked": true,
    "id": "baf98d77b1de36f2b0d9c36868e96576f70bd063",
    "semantic_title": "estimating agreement by chance for sequence annotation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=njnT17FTWT": {
    "title": "Evaluate, Scale, and Credit: A Comprehensive Study on Multi-Agent Collaboration of Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models based Multi-Agent Systems (LLM-MAS) perform well in many domains, but we still lack a clear understanding of the collaboration mechanism among multiple LLM-based agents. This study aims to explore three key issues: (1) Can multi-agent outperform single-agent systems? (2) Is scaling better for multi-agent systems? (3) How to credit agents and optimize collaboration? Specifically, we design five collaboration architectures and evaluate their effectiveness across different LLMs and tasks. Our findings offer significant insights for understanding the collaboration within MAS, optimizing collaboration architectures among agents, and reducing system costs. Furthermore, our conclusion will inspire and provide new perspectives for future studies on LLM-MAS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1ITtBdUpsmm": {
    "title": "Investigating the Impact of Model Instability on Explanations and Uncertainty",
    "volume": "review",
    "abstract": "Explainable AI methods facilitate the understanding of model behaviour, yet, small, imperceptible perturbations to inputs can vastly distort explanations. As these explanations are typically evaluated holistically, before model deployment, it is difficult to assess when a particular explanation is trustworthy. Some studies have tried to create confidence estimators for explanations, but none have investigated an existing link between uncertainty and explanation quality. We artificially simulate epistemic uncertainty in text input by introducing noise at inference time. In this large-scale empirical study, we insert different levels of noise perturbations and measure the effect on the output of pre-trained language models and different uncertainty metrics. Realistic perturbations have minimal effect on performance and explanations, yet masking has a drastic effect. We find that high uncertainty doesn't necessarily imply low explanation plausibility; the correlation between the two metrics can be moderately positive when noise is exposed during the training process. This suggests that noise-augmented models may be better at identifying salient tokens when uncertain. Furthermore, when predictive and epistemic uncertainty measures are over-confident, the robustness of a saliency map to perturbation can indicate model stability issues. Integrated Gradients shows the overall greatest robustness to perturbation, while still showing model-specific patterns in performance; however, this phenomenon is limited to smaller Transformer-based language models",
    "checked": true,
    "id": "ee826e4fd84bdc401784986b38e204ee8069d4cb",
    "semantic_title": "investigating the impact of model instability on explanations and uncertainty",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=-rPKjbeWaT": {
    "title": "Spotting AI's Touch: Identifying LLM-Paraphrased Spans in Text",
    "volume": "review",
    "abstract": "AI-generated text detection has attracted increasing attention as powerful language models approach human-level generation. Limited work is devoted to detecting (partially) AI-paraphrased texts. However, AI paraphrasing is commonly employed in various application scenarios for text refinement and diversity. To this end, we propose a novel detection framework, paraphrased text span detection (PTD), aiming to identify paraphrased text spans within a text. Different from text-level detection, PTD takes in the full text and assigns each of the sentences with a score indicating the paraphrasing degree. We construct a dedicated dataset, PASTED, for paraphrased text span detection. Both in-distribution and out-of-distribution results demonstrate the effectiveness of PTD models in identifying AI-paraphrased text spans. Statistical and model analysis explains the crucial role of the surrounding context of the paraphrased text spans. Extensive experiments show that PTD models can generalize to versatile paraphrasing prompts as well as multiple paraphrased text spans",
    "checked": true,
    "id": "3b6c08cbe9b0c3d7efb8b4ba2142457168a6f198",
    "semantic_title": "spotting ai's touch: identifying llm-paraphrased spans in text",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rHB92KFC9b": {
    "title": "Simple Linguistic Inferences of Large Language Models (LLMs): Blind Spots and Blinds",
    "volume": "review",
    "abstract": "We evaluate LLMs' language understanding capacities on simple inference tasks that most humans find trivial. Specifically, we target (i) grammatically-specified entailments, (ii) premises with evidential adverbs of uncertainty, and (iii) monotonicity entailments. We design evaluation sets for these tasks and conduct experiments in both zero-shot and chain-of-thought setups, and with multiple prompts and LLMs. The models exhibit moderate to low performance on these evaluation sets. Subsequent experiments show that embedding the premise in syntactic constructions that should preserve the entailment relations (presupposition triggers) or change them (non-factives), further confuses the models, causing them to either under-predict or over-predict certain entailment labels regardless of the true relation, and often disregarding the nature of the embedding context. Overall these results suggest that, despite LLMs' celebrated language understanding capacity, even the strongest models have blindspots with respect to certain types of entailments, and certain information-packaging structures act as ``blinds'' overshadowing the semantics of the embedded premise",
    "checked": true,
    "id": "06f8c3dc7403c75bd158575fc97de20f30a91d0c",
    "semantic_title": "simple linguistic inferences of large language models (llms): blind spots and blinds",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=d_3WfKLHD2": {
    "title": "Exploring Hybrid Question Answering via Program-based Prompting",
    "volume": "review",
    "abstract": "Question answering over heterogeneous data requires reasoning over diverse sources of data, which is challenging due to the large scale of information and organic coupling of heterogeneous data. Various approaches have been proposed to address these challenges. One approach involves training specialized retrievers to select relevant information, thereby reducing the input length. Another approach is to transform diverse modalities of data into a single modality, simplifying the task difficulty and enabling more straightforward processing. In this paper, we propose HProPro, a novel program-based prompting framework for the hybrid question answering task. HProPro follows the code generation and execution paradigm. In addition, HProPro integrates various functions to tackle the hybrid reasoning scenario. Specifically, HProPro contains function declaration and function implementation to perform hybrid information-seeking over data from various sources and modalities, which enables reasoning over such data without training specialized retrievers or performing modal transformations. Experimental results on two typical hybrid question answering benchmarks HybridQA and MultiModalQA demonstrate the effectiveness of HProPro: it surpasses all baseline systems and achieves the best performances in the few-shot settings on both datasets",
    "checked": true,
    "id": "fa21fd114e3a0208c0671ccc8ba2bd2064367c10",
    "semantic_title": "exploring hybrid question answering via program-based prompting",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=KUgTUumsgWh": {
    "title": "GeNeRTe: Generating Neural Representations from Text for Classification",
    "volume": "review",
    "abstract": "Advancements in language modelling over the last decade have significantly improved downstream tasks such as automated text classification. However, deploying such systems requires high computational resources and extensive training data. Human adults can effortlessly perform such tasks with minimal computational overhead and training data which prompts research into leveraging neurocognitive signals such as Electroencephalography (EEG). We compare Large Language Models (LLMs) and EEG features captured during natural reading for text classification. Additionally, we introduce GeNeRTe, a novel state-of-the-art synthetic EEG generative model. Using only a limited amount of data, GeNeRTe learns to produce synthetic EEG features for a sentence through a neural regressor that resolves the relationship between embeddings for a sentence and its natural EEG. From our experiments, we show that GeNeRTe can effectively synthesize EEG features for unseen test sentences with just 236 sentence-EEG training pairs. Furthermore, using synthetic EEG features significantly improves text classification performance and reduces computation time. Our results emphasize the potential of synthetic EEG features, providing a viable path to create a new type of physiological embedding with lower computing requirements and improved model performance in practical applications",
    "checked": false,
    "id": "aef73e6e9646ea0c2a0e7c5e303b6fcd7a2c6034",
    "semantic_title": "bert layer weighting comparision with short text classification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4d7FmBOQuq": {
    "title": "M-RAG: Reinforcing Large Language Model Performance through Retrieval-Augmented Generation with Multiple Partitions",
    "volume": "review",
    "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by retrieving relevant memories from an external database. However, existing RAG methods typically organize all memories in a whole database, potentially limiting focus on crucial memories and introducing noise. In this paper, we introduce a multiple partition paradigm for RAG (called M-RAG), where each database partition serves as a basic unit for RAG execution. Based on this paradigm, we propose a novel framework that leverages LLMs with Multi-Agent Reinforcement Learning to optimize different language generation tasks explicitly. Through comprehensive experiments conducted on seven datasets, spanning three language generation tasks and involving three distinct language model architectures, we confirm that M-RAG consistently outperforms various baseline methods, achieving improvements of 11%, 8%, and 12% for text summarization, machine translation, and dialogue generation, respectively",
    "checked": true,
    "id": "8d0df3168870fd17b36ecd5575e406feb5a5a1b5",
    "semantic_title": "m-rag: reinforcing large language model performance through retrieval-augmented generation with multiple partitions",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=hVhNf9r20l-": {
    "title": "SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval",
    "volume": "review",
    "abstract": "Multi-modal information retrieval (MMIR) is a rapidly evolving field where significant progress has been made through advanced representation learning and cross-modality alignment research, particularly in image-text pairing.However, current benchmarks for evaluating MMIR performance on image-text pairings overlook the scientific domain, which has a notable gap with the generic data since the caption of scientific charts and tables usually describes the analysis of experimental results or scientific principles in contrast to human activity or scenery depicted in generic images.To bridge this gap, we develop a \\textbf{sci}entific domain-specific \\textbf{MMIR} benchmark (\\textbf{SciMMIR}) by leveraging open-access research paper corpora to extract data relevant to the scientific domain. This benchmark comprises \\textbf{530K} meticulously curated image-text pairs, extracted from figures and tables with detailed captions from scientific documents.We further annotate the image-text pairs with a two-level subset-subcategory hierarchy to facilitate a more comprehensive evaluation of the baselines. We conduct zero-shot and fine-tuned evaluations on prominent multi-modal image-captioning and visual language models, such as CLIP, BLIP, and BLIP-2.Our findings offer critical insights for MMIR in the scientific domain, including the impact of pre-training and fine-tuning settings and the effects of different visual and textual encoders",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RQTgBAGNoi": {
    "title": "XMC-Agent : Dynamic Navigation over Scalable Hierarchical Index for Incremental Extreme Multi-label Classification",
    "volume": "review",
    "abstract": "The eXtreme Multi-label Classification (XMC) aims at accurately assigning large-scale labels to instances, and is challenging for learning, managing, and predicting over the large-scale and rapidly growing set of labels. Traditional XMC methods, like one-vs-all and tree-based methods struggle with the growing set of labels due to their static label assumptions, and embedding-based methods struggle with the complex mapping relationships due to their late-interaction paradigm. In this paper, we propose a large language model (LLM) powered agent framework for extreme multi-label classification â€“ \\our{}, which can effectively learn, manage and predict the extremely large and dynamically increasing set of labels. Specifically, \\our{} models the extreme multi-label classification task as a dynamic navigation problem, employing a scalable hierarchical label index to effectively manage the unified label space. Additionally, we propose two algorithms to enhance the dynamic navigation capabilities of \\our{}: a self-construction algorithm for building the scalable hierarchical index, and an iterative feedback learning algorithm for adjusting the agent to specific tasks. Experiments show that \\our{} achieves the state-of-the-art performance on three standard datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TsymBdb1SL": {
    "title": "BatchEval: Towards Human-like Text Evaluation",
    "volume": "review",
    "abstract": "Significant progress has been made in automatic text evaluation with the introduction of large language models (LLMs) as evaluators. However, current sample-wise evaluation paradigm suffers from the following issues: (1) Sensitive to prompt design; (2) Poor resistance to noise; (3) Inferior ensemble performance with static reference. Inspired by the fact that humans treat both criterion definition and inter sample comparison as references for evaluation, we propose BatchEval, a paradigm that conducts batch-wise evaluation iteratively to alleviate the above problems. We explore variants under this paradigm and confirm the optimal settings are two stage procedure with heterogeneous batch composition strategy and decimal scoring format. Comprehensive experiments across 3 LLMs on 4 text evaluation tasks demonstrate that BatchEval outperforms state-of-the-art methods by 10.5% on Pearson correlations with only 64% API cost on average. Further analyses have been conducted to verify the robustness, generalization, and working mechanism of BatchEval",
    "checked": true,
    "id": "607d8df2c6c5f103f10a2d631d7364cc952cb489",
    "semantic_title": "batcheval: towards human-like text evaluation",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=kgkwsyX4aoD": {
    "title": "Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines",
    "volume": "review",
    "abstract": "Text-to-image diffusion models (T2I) use a latent representation of a text prompt to guide the image generation process. However, the process by which the encoder produces the text representation is unknown. We propose the Diffusion Lens, a method for analyzing the text encoder of T2I models by generating images from its intermediate representations. Using the Diffusion Lens, we perform an extensive analysis of two recent T2I models. Exploring compound prompts, we find that complex scenes describing multiple objects are composed progressively and more slowly compared to simple scenes; Exploring knowledge retrieval, we find that representation of uncommon concepts require further computation compared to common concepts, and that knowledge retrieval is gradual across layers. Overall, our findings provide valuable insights into the text encoder component in T2I pipelines",
    "checked": true,
    "id": "73669cafec2801cd8fa4d482782e20f86865457e",
    "semantic_title": "diffusion lens: interpreting text encoders in text-to-image pipelines",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=21YGU2l1kL": {
    "title": "SciGLM: Training Scientific Language Models with Self-Reflective Instruction Annotation and Tuning",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving advanced numerical calculations. To bridge these gaps, we introduce SciGLM, a suite of scientific language models able to conduct college-level scientific reasoning. Central to our approach is a novel self-reflective instruction annotation framework to address the data scarcity challenge in the science domain. This framework leverages existing LLMs to generate step-by-step reasoning for unlabelled scientific questions, followed by a process of self-reflective critic-and-revise. Applying this framework, we curated SciInstruct, a diverse and high-quality dataset encompassing physics, chemistry, math, and formal proofs. We fine-tuned the ChatGLM family of language models with SciInstruct, enhancing their capabilities in scientific and mathematical reasoning. Remarkably, the SciGLM consistently improves both the base model (ChatGLM3-6B-Base) and larger-scale models (32B), without sacrificing the language understanding capabilities of the base model. This makes SciGLM a suitable foundational model to facilitate diverse scientific discovery tasks. For the benefit of the wider research community, we release SciInstruct, and SciGLM, alongside a self-reflective framework and fine-tuning code",
    "checked": true,
    "id": "c6e162aedf6a5ab0135e3b991577d77ca06673f9",
    "semantic_title": "sciglm: training scientific language models with self-reflective instruction annotation and tuning",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=szJnfd_enja": {
    "title": "E-EVAL: A Comprehensive Chinese K-12 Education Evaluation Benchmark for Large Language Models",
    "volume": "review",
    "abstract": "The rapid development of Large Language Models (LLMs) has led to their increasing utilization in Chinese K-12 education. Despite the growing integration of LLMs and education, the absence of a dedicated benchmark for evaluating LLMs within this domain presents a pressing concern. Consequently, there is an urgent need for a comprehensive natural language processing benchmark to precisely assess the capabilities of various LLMs in Chinese K-12 education. In response, we introduce E-EVAL, the first comprehensive evaluation benchmark specifically tailored for Chinese K-12 education. E-EVAL comprises 4,351 multiple-choice questions spanning primary, middle, and high school levels, covering a diverse array of subjects. Through meticulous evaluation, we find that Chinese-dominant models often outperform English-dominant ones, with many exceeding GPT 4.0. However, most struggle with complex subjects like mathematics. Additionally, our analysis indicates that most Chinese-dominant LLMs do not achieve higher scores at the primary school level compared to the middle school level, highlighting the nuanced relationship between proficiency in higher-order and lower-order knowledge domains. Furthermore, experimental results highlight the effectiveness of the Chain of Thought (CoT) technique in scientific subjects and Few-shot prompting in liberal arts. Through E-EVAL, we aim to conduct a rigorous analysis delineating the strengths and limitations of LLMs in educational applications, thereby contributing significantly to the advancement of Chinese K-12 education and LLMs",
    "checked": true,
    "id": "c08e944dcbb560c5adb70eaf40c3e2982b0c1001",
    "semantic_title": "e-eval: a comprehensive chinese k-12 education evaluation benchmark for large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=2YhUJL48hPa": {
    "title": "Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have achieved remarkable performance in objective tasks such as open-domain question answering and mathematical reasoning, which can often be solved through recalling learned factual knowledge or chain-of-thought style reasoning. However, we find that the performance of LLMs in subjective tasks is still unsatisfactory, such as metaphor recognition, dark humor detection, etc. Compared to objective tasks, subjective tasks focus more on interpretation or emotional response rather than a universally accepted reasoning pathway. Based on the characteristics of the tasks and the strong dialogue-generation capabilities of LLMs, we propose RiC (Reasoning in Conversion), a method that focuses on solving subjective tasks through dialogue simulation. The motivation of RiC is to mine useful contextual information by simulating dialogues instead of supplying chain-of-thought style rationales, thereby offering potential useful knowledge behind dialogues for giving the final answers. We evaluate both API-based and open-source LLMs including GPT-4, ChatGPT, and OpenChat across twelve tasks. Experimental results show that RiC can yield significant improvement compared with various baselines",
    "checked": true,
    "id": "288338c1ec81b259af5587677fdd10d5191baf71",
    "semantic_title": "reasoning in conversation: solving subjective tasks through dialogue simulation for large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0vYrFJut08": {
    "title": "DATA-CUBE: Data Curriculum for Instruction-based Sentence Representation Learning",
    "volume": "review",
    "abstract": "Recently, multi-task instruction tuning has been utilized to improve sentence representation learning~(SRL). It endows SRL models with the capability of generating task-specific representations with the guidance of task instruction, thus exhibiting strong generalization ability on unseen tasks. However, these methods mostly neglect the potential interference problems across different tasks and instances, which may affect the training of the model.To address this issue, we propose a data curriculum method, namely \\textbf{Data-CUBE}, that arranges the order of all the multi-task data for training, to minimize the interference risks from two aspects.At the task level, we aim to find the optimal task order to minimize the total cross-task interference riskï¼Œand formulate this problem as the traveling salesman problem, which is further solved by a specially designed simulated annealing algorithm. At the instance level, we propose a measurement method to quantify the difficulty of all instances per task, and then arrange instances in an easy-to-difficult order for training.Experiments on MTEB sentence representation evaluation tasks show that our approach can boost the performance of state-of-the-art methods",
    "checked": true,
    "id": "01722c4ccc5a123df92b885bfd58c029c3c1d4a3",
    "semantic_title": "data-cube: data curriculum for instruction-based sentence representation learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D0elGobpzHF": {
    "title": "Numbers Matter! Bringing Quantity-awareness to Retrieval Systems",
    "volume": "review",
    "abstract": "Quantitative information plays a crucial role in understanding and interpreting content of documents. Many user queries contain quantities and cannot be resolved without understanding their semantics, e.g., ``car that costs less than $\\$10$k''. Yet, modern search engines apply the same ranking mechanisms for both words and quantities, overlooking magnitude and unit information. In this paper, we introduce two quantity-aware ranking techniques designed to rank both the quantity and textual content either jointly or independently. These techniques incorporate quantity information in available retrieval systems and can address queries with numerical conditions equal, greater than, and less than. To evaluate the effectiveness of our proposed models, we introduce two novel quantity-aware benchmark datasets in the domains of finance and medicine and compare our method against various lexical and neural models. The code and data are available under \\url{https://github.com/filled_in_later}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fhpfI-t4NFg": {
    "title": "Bypassing LLM Watermarks with Color-Aware Substitutions",
    "volume": "review",
    "abstract": "Watermarking approaches are proposed to identify if the text being circulated is human or large language model (LLM) generated. The state-of-the-art strategy of Kirchenbauer et al. (2023a) biases the LLM to generate specific \"green\" tokens. However, determining the robustness of this watermarking method is an open problem. Existing attack methods do not incorporate color information (if a token is green/not) and may fail to evade longer text watermark detection. We propose Self Color Testing-based Substitution (SCTS), the first \"color-aware\" attack. SCTS gets color information by strategically prompting the watermarked LLM and comparing output frequencies, using which it can determine token colors. It then substitutes green tokens with red ones. In our experiments, SCTS successfully evades watermark detection using fewer edits than related work. Additionally, we show both theoretically and empirically that SCTS can remove the watermark for arbitrarily long watermarked text",
    "checked": true,
    "id": "144399b7708c0e15b8f067f7635df173a7067905",
    "semantic_title": "bypassing llm watermarks with color-aware substitutions",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=PiH4yASndL8": {
    "title": "Examining the robustness of LLM evaluation to the distributional assumptions of benchmarks",
    "volume": "review",
    "abstract": "Benchmarks have emerged as the central approach for evaluating Large Language Models (LLMs). The research community often relies on a model's average performance across the test prompts of a benchmark to evaluate the model's performance. This is consistent with the assumption that the test prompts within a benchmark represent a random sample from some real-world distribution of interest. We note that this is generally not the case; instead, we hold that the distribution of interest varies according to the specific use case. Hence, we analyze the robustness of LLM benchmarks to their underlying distributional assumptions. We find that (1) the correlation in model performance across test prompts is non-random, (2) accounting for correlations across test prompts can change model rankings on major benchmarks, (3) explanatory factors for these correlations include semantic similarity and common LLM failure points",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5HG1V9QZG3": {
    "title": "Small LLMs Are Weak Tool Learners: A Multi-LLM Agent",
    "volume": "review",
    "abstract": "Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete various tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers accurately but also excel in task planning, tool invocation, and result summarization. While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. To overcome these challenges, we propose a novel approach that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specific capability and collaborates with others to accomplish the task. This modular framework facilitates individual updates and the potential use of smaller LLMs for building each capability.To effectively train this framework, we introduce a two-stage training paradigm. First, we fine-tune a backbone LLM on the entire dataset without discriminating sub-tasks, providing the model with a comprehensive understanding of the task. Second, the fine-tuned LLM is used to instantiate the planner, caller, and summarizer respectively, which are continually fine-tuned on respective sub-tasks. Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning",
    "checked": true,
    "id": "ff61aef2fef3a235bfaa123158a990c4f5f27d1a",
    "semantic_title": "small llms are weak tool learners: a multi-llm agent",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=LilfuMnJcO": {
    "title": "Temperature-scaling surprisal estimates improve fit to human reading times â€“ but does it do so for the \"right reasons\"?",
    "volume": "review",
    "abstract": "A wide body of evidence shows that human language processing difficulty is predicted by the information-theoretic measure surprisal, a word's negative log probability in context. However, it is still unclear how to best estimate these probabilities needed for predicting human processing difficulty -- while a long-standing belief held that models with lower perplexity would provide more accurate estimates of word predictability, and therefore lead to better reading time predictions, recent work has shown that for very large models, psycholinguistic predictive power decreases. One reason could be that language models might be more confident of their predictions than humans, because they have had exposure to several magnitudes more data. In this paper, we test what effect temperature-scaling of large language model (LLM) predictions has on surprisal estimates and their predictive power of reading times of English texts. Firstly, we show that calibration of large language models typically improves with model size, i.e.~poorer calibration cannot account for poorer fit to reading times. Secondly, we find that temperature-scaling probabilities lead to a systematically better fit to reading times (up to 89\\% improvement in delta log likelihood), across several reading time corpora. Finally, we show that this improvement in fit is chiefly driven by words that are composed of multiple subword tokens",
    "checked": false,
    "id": "746a78123035729ce0ab1103efd51b9c5b5c0e72",
    "semantic_title": "temperature-scaling surprisal estimates improve fit to human reading times -- but does it do so for the\"right reasons\"?",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Ju1l4x3UTK0": {
    "title": "Chinese Spelling Corrector Is Just Language Learner",
    "volume": "review",
    "abstract": "This paper emphasizes the Chinese spelling correction of self-supervised learning, which means there are no annotated errors within the training data. This setting is a pivotal issue that has received broad attention in the community. Our intuition is that humans are naturally good correctors with exposure to monolingual sentences, which contrasts with current unsupervised methods that strongly rely on the usage of confusion sets to produce parallel sentences. In this paper, we demonstrate that learning a spelling correction model is identical to learning a language model from monolingual data alone, with decoding it in a greater search space.We propose \\emph{Denoising Decoding Correction (D\\textsuperscript{2}C)}, which selectively imposes noise upon the source sentence to solve out the underlying correct characters. Our method largely inspires the ability of language models to perform correction, including both BERT-based models and large language models (LLMs). We show that the self-supervised learning manner generally outstrips using confusion sets in specific domains because it bypasses the need to introduce error characters to the training data which can impair the patterns in the target domains. We evaluate our methods on multi-domain datasets Syn-LEMON proposed by our work and ECSpell \\citep{Ecspell}",
    "checked": false,
    "id": "d23545c5ae9b3cc19ad0fe74848a10c13b6d3036",
    "semantic_title": "research on key technologies of chinese spelling check based on machine learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GKKsaLJf8D": {
    "title": "DiFiNet: Boundary-Aware Semantic Differentiation and Filtration Network for Nested Named Entity Recognition",
    "volume": "review",
    "abstract": "Nested Named Entity Recognition (Nested NER) entails identifying and classifying entity spans within the text, including the detection of named entities that are embedded within external entities. Prior approaches primarily employ span-based techniques, utilizing the power of exhaustive searches to address the challenge of overlapping entities. Nonetheless, these methods often grapple with the absence of explicit guidance for boundary detection, resulting insensitivity in discerning minor variations within nested spans. To this end, we propose a Boundary-aware Semantic $\\underline{Di}$fferentiation and $\\underline{Fi}$ltration $\\underline{Net}$work (DiFiNet) tailored for nested NER. Specifically, DiFiNet leverages a biaffine attention mechanism to generate a span representation matrix. This matrix undergoes further refinement through a self-adaptive semantic differentiation module, specifically engineered to discern semantic variances across spans. Furthermore, DiFiNet integrates a boundary filtration module, designed to mitigate the impact of non-entity noise by leveraging semantic relations among spans. Extensive experiments on three benchmark datasets demonstrate our model yields a new state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qdhSBPTawPc": {
    "title": "Automatic Large Language Model Evaluation via Peer Review",
    "volume": "review",
    "abstract": "The impressive performance of large language models (LLMs) has attracted considerable attention from the academic and industrial communities. Besides how to construct and train LLMs, how to effectively evaluate and compare the capacity of LLMs has also been well recognized as an important yet difficult problem. Existing paradigms rely on either human annotators or model-based evaluators to evaluate the performance of LLMs on different tasks. However, these paradigms often suffer from high cost, low generalizability, and inherited biases in practice, which make them incapable of supporting the sustainable development of LLMs in long term. In order to address these issues, inspired by the peer review systems widely used in academic publication process, we propose a novel framework that can automatically evaluate LLMs through a peer-review process. Specifically, for the evaluation of a specific task, we first construct a small qualification exam to select \"reviewers\" from a couple of powerful LLMs. Then, to actually evaluate the \"submissions\" written by different candidate LLMs, i.e., the evaluatees, we use the reviewer LLMs to rate or compare the submissions. The final ranking of evaluatee LLMs is generated based on the results provided by all reviewers. We conducted extensive experiments on both text summarization and non-factoid question answering tasks with eleven LLMs including GPT-4. The results demonstrate the existence of biasness when evaluating using a single LLM. Also, our PRE model outperforms all the baselines, illustrating the effectiveness of the peer review mechanism",
    "checked": false,
    "id": "92e9469197a53e82f42b66876b221aaa25d07292",
    "semantic_title": "a survey on large language model hallucination via a creativity perspective",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=3P5VBDO_34": {
    "title": "Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on Data-to-Text Generation",
    "volume": "review",
    "abstract": "We analyze the behaviors of open large language models (LLMs) on the task of data-to-text (D2T) generation, i.e., generating coherent and relevant text from structured data. To avoid the issue of LLM training data contamination with standard benchmarks, we design Quintd â€“ a tool for collecting novel structured data records from public APIs. Using a dataset collected with Quintd and leveraging reference-free evaluation, we analyze model behaviors on five D2T generation tasks. We find that recent open LLMs (Llama2, Mistral, and Zephyr) can generate fluent and coherent text from standard data formats in zero-shot settings. However, we also show that the semantic accuracy of the outputs is a major issue: both according to our GPT-4-based metric and human annotators, more than 80% of the outputs of open LLMs contain a semantic error. We publicly release the code, data, and model outputs",
    "checked": false,
    "id": "d3f5d760e3eeef4430705c4e7144f05b432491e9",
    "semantic_title": "beyond traditional benchmarks: analyzing behaviors of open llms on data-to-text generation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=nOnazhyUy0f": {
    "title": "Large Language Models for Propaganda Span Annotation",
    "volume": "review",
    "abstract": "The use of propagandistic techniques in online content has increased in recent years, aiming to manipulate online audiences. Although essential for more informed content consumption; very limited focus has been given to the task of extracting textual spans where propaganda techniques are used. Our study focuses on that task by investigating whether large language models (LLMs), such as GPT-4, can effectively extract these spans. We further study the potential of employing the model to collect more cost-effective annotations. Our experiments use a large-scale in-house manually annotated dataset. The results suggest that providing more annotation context to the model as prompts improves its performance compared to human annotations. Moreover, our work is the first to show the potential of utilizing LLMs to develop annotated datasets for this complex task, prompting it with annotations from human annotators with limited expertise. All annotations will be shared with the community",
    "checked": true,
    "id": "3657eff55a86a89db629e598a7ea93edd31bf59d",
    "semantic_title": "large language models for propaganda span annotation",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=nVFW80T9SM": {
    "title": "Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL",
    "volume": "review",
    "abstract": "Currently, the in-context learning method based on large language models (LLMs) has become the mainstream of text-to-SQL research. Previous works have discussed how to select demonstrations related to the user question from a human-labeled demonstration pool. However, human labeling suffers from the limitations of insufficient diversity and high labeling overhead. Therefore, in this paper, we discuss how to measure and improve the diversity of the demonstrations for text-to-SQL. We present a metric to measure the diversity of the demonstrations and analyze the insufficient of the existing labeled data by experiments. Based on the above discovery, we propose fusing iteratively for demonstrations (Fused) to build a high-diversity demonstration pool through human-free multiple-iteration synthesis, improving diversity and lowering label cost. Our method achieves an average improvement of 3.2% and 5.0% with and without human labeling on several mainstream datasets, which proves the effectiveness of Fused",
    "checked": true,
    "id": "2d125b26788e41586fa1594da60169f33fb481f0",
    "semantic_title": "improving demonstration diversity by human-free fusing for text-to-sql",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=iPmgIXrs14": {
    "title": "Harnessing Large Language Models as Post-hoc Correctors",
    "volume": "review",
    "abstract": "As Machine Learning (ML) models grow in size and demand higher-quality training data, the expenses associated with re-training and fine-tuning these models are escalating rapidly. Inspired by recent impressive achievements of Large Language Models (LLMs) in different fields, this paper delves into the question: can LLMs efficiently improve an ML's performance at a minimal cost? We show that, through our proposed training-free framework LlmCorr, an LLM can work as a post-hoc corrector to propose corrections for the predictions of an arbitrary ML model. In particular, we form a contextual knowledge database by incorporating the dataset's label information and the ML model's predictions on the validation dataset. Leveraging the in-context learning capability of LLMs, we ask the LLM to summarise the instances in which the ML model makes mistakes and the correlation between primary predictions and true labels. Following this, the LLM can transfer its acquired knowledge to suggest corrections for the ML model's predictions. Our experimental results on the challenging molecular predictions show that LlmCorr improves the performance of a number of models by up to 39%",
    "checked": true,
    "id": "9cae0b08b0b490d476e1ea75e9327da2ed1f8d78",
    "semantic_title": "harnessing large language models as post-hoc correctors",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=-_DJAbGXQ3": {
    "title": "Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs",
    "volume": "review",
    "abstract": "Reasoning about events, their relationships, and inferring implicit context are crucial abilities of event commonsense reasoning, which state-of-the-art language models still struggle to perform. However, data scarcity makes it challenging to learn systems that can generate commonsense inferences for contexts and questions involving interactions between complex events. To address this demand, we present COM$^2$ (COMplex COMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or the effect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules and Large Language.Our experiments show that Language models trained on COM$^2$ exhibit significant improvements in complex reasoning ability, resulting in enhanced zero-shot performance in both in-domain and out-of-domain tasks for question answering and generative commonsense reasoning, without expensive human annotations",
    "checked": true,
    "id": "965cc3c7c80de91c75787a51931e912bece8046a",
    "semantic_title": "complex reasoning over logical queries on commonsense knowledge graphs",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=EMdayGxZ34": {
    "title": "LongWanjuan: Towards Systematic Measurement for Long Text Quality",
    "volume": "review",
    "abstract": "The quality of training data are crucial for enhancing the long-text capabilities of foundation models. Despite existing efforts to refine data quality through heuristic rules and evaluations based on data diversity and difficulty, there's a lack of systematic approaches specifically tailored for assessing long texts. Addressing this gap, our work systematically measures the quality of long texts by evaluating three fundamental linguistic dimensions: coherence, cohesion, and complexity. Drawing inspiration from the aforementioned three dimensions, we introduce a suite of metrics designed to evaluate the quality of long texts, encompassing both statistical and pre-trained language model-based ones. Leveraging these metrics, we present LongWanjuan, a bilingual dataset specifically tailored to enhance the training of language models for long-text tasks with over 160B tokens. In LongWanjuan, we categorize long texts into holistic, aggregated, and chaotic types, enabling a detailed analysis of long-text quality. Furthermore, we devise a data mixture recipe that strategically balances different types of long texts within LongWanjuan, leading to significant improvements in model performance on long-text tasks",
    "checked": true,
    "id": "82f041ed71f8ef1cf462fa03a7e732e440259bd7",
    "semantic_title": "longwanjuan: towards systematic measurement for long text quality",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=A-w0SD3k8k": {
    "title": "Enhancing Numerical Reasoning with the Guidance of Reliable Reasoning Processes",
    "volume": "review",
    "abstract": "Numerical reasoning is an essential ability for NLP systems to handle numeric information. Recent research indicates that fine-tuning a small-scale model to learn generating reasoning processes alongside answers can significantly enhance performance. However, current methods have the limitation that most methods generate reasoning processes with large language models (LLMs), which are \"unreliable\" since such processes could contain information unrelated to the answer. To address this limitation, we introduce enhancing numerical reasoning with reliable processes (Encore), which derives the reliable reasoning process by decomposing the answer formula, ensuring which fully supports the answer. Nevertheless, models could lack enough data to learn the reasoning process generation adequately, since our method generates only one single reasoning process for one formula. To overcome this difficulty, we present a series of pre-training tasks to help models learn the reasoning process generation with synthesized data. The experiments show that Encore yields improvement on all five experimental datasets with an average of 1.8%, proving the effectiveness of our method",
    "checked": true,
    "id": "d4f43485c9f71618dfdff884a74aff8a51fc018c",
    "semantic_title": "enhancing numerical reasoning with the guidance of reliable reasoning processes",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A-qUYP17xV": {
    "title": "Enhancing Semantic Consistency of Large Language Models through Model Editing: An Interpretability-Oriented Approach",
    "volume": "review",
    "abstract": "A Large Language Model (LLM) tends to generate inconsistent and sometimes contradictory outputs when presented with a prompt that has equivalent semantics but is expressed differently from the original prompt. To achieve semantic consistency of an LLM, one of the key approaches is to finetune the model with prompt-output pairs with semantically equivalent meanings. Despite its effectiveness, a data-driven finetuning method incurs substantial computation costs in data preparation and model optimization. In this regime, an LLM is treated as a ``black box'', restricting our ability to gain deeper insights into its internal mechanism. In this paper, we are motivated to enhance the semantic consistency of LLMs through a more interpretable method (i.e., model editing) to this end. We first identify the model components (i.e., attention heads) that have a key impact on the semantic consistency of an LLM. We subsequently inject biases into the output of these model components along the semantic-consistency activation direction. It is noteworthy that these modifications are cost-effective, without reliance on mass manipulations of the original model parameters. Through comprehensive experiments on the constructed NLU and open-source NLG datasets, our method demonstrates significant improvements in the semantic consistency and task performance of LLMs. Additionally, our method exhibits promising generalization capabilities by performing well on tasks beyond the primary tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rZ-0zgVLUA": {
    "title": "Parallel Mechanism Decoders in Pretrained Language Model-based Neural Machine Translation",
    "volume": "review",
    "abstract": "Pre-trained language models (PLMs) have demonstrated their effectiveness in enhancing neural machine translation (NMT) tasks. While researchers have made numerous attempts to enhance the encoder, however, in decoder enhancement, the existing method neglects intra-layer information fusion, potentially resulting in the underutilization of encoder information. In this paper, we propose a model featuring a parallel mechanism decoder, facilitating the integration of PLM enhancements and enabling multi-granularity information fusion in the decoder. We evaluate our proposed method on the IWSLT14 De-En task and obtain significant improvements in model performance with tiny modifications",
    "checked": false,
    "id": "6c815248c431a23d2be29003dce430d6a8c2de0c",
    "semantic_title": "a comparative analysis of transformers for multilingual neural machine translation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=awYVlWNfjLe": {
    "title": "MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance",
    "volume": "review",
    "abstract": "The deployment of multimodal large language models (MLLMs) has brought forth a unique vulnerability: susceptibility to malicious attacks through visual inputs. This paper systematically investigates the novel challenge of defending MLLMs against such attacks. We discovered that images act as a \"foreign language\" that is not considered during alignment, which can make MLLMs prone to producing harmful responses. Unfortunately, unlike the discrete tokens considered in text-based LLMs, the continuous nature of image signals presents significant alignment challenges, which poses difficulty to thoroughly cover the possible scenarios. This vulnerability is exacerbated by the fact that open-source MLLMs are predominantly fine-tuned on limited image-text pairs that is much less than the extensive text-based pretraining corpus, which makes the MLLMs more prone to catastrophic forgetting of their original abilities during explicit alignment tuning. To tackle these challenges, we introduce MLLM-Protector, a plug-and-play strategy combining a lightweight harm detector and a response detoxifier. The harm detector's role is to identify potentially harmful outputs from the MLLM, while the detoxifier corrects these outputs to ensure the response stipulates to the safety standards. This approach effectively mitigates the risks posed by malicious visual inputs without compromising the model's overall performance. Our results demonstrate that MLLM-Protector offers a robust solution to a previously unaddressed aspect of MLLM security",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4FlUT4rTK_v": {
    "title": "PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLM",
    "volume": "review",
    "abstract": "Driven by the surge in code generation using large language models (LLMs), numerous benchmarks have emerged to evaluate these LLMs capabilities. We conducted a large-scale human evaluation of **HumanEval** and **MBPP**, two popular benchmarks for Python code generation, analyzing their diversity and difficulty. Our findings unveil a critical bias towards a limited set of programming concepts, neglecting most of the other concepts entirely. Furthermore, we uncover a worrying prevalence of easy tasks, potentially inflating model performance estimations. To address these limitations, we propose a novel benchmark, *PythonSaga*, featuring 185 hand-crafted prompts on a balanced representation of 38 programming concepts across diverse difficulty levels. The code and dataset are openly available to the NLP community at https://anonymous.4open.science/r/PythonSaga",
    "checked": false,
    "id": "7760bb962353b2a086b5fc3453676c3dd903946f",
    "semantic_title": "pythonsaga: redefining the benchmark to evaluate code generating llms",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=efmbt-1TOH": {
    "title": "On Incorporating Prior Knowledge Extracted from Pre-trained Language Models into Causal Discovery",
    "volume": "review",
    "abstract": "Pre-trained Language Models (PLMs) can reason about causality leveraging vast pre-trained knowledge and text descriptions of datasets, proving its effectiveness even when data is scarce. However, there are crucial limitations in the current PLM-based causal reasoning methods: i) PLM cannot utilize large datasets in prompt due to the limits of context length, and ii) the methods are not adept at comprehending the whole interconnected causal structures. On the other hand, data-driven causal discovery can discover the causal structure as a whole, although it works well only when the number of data observations is large enough. To overcome each other's limitations, we propose a new framework that integrates PLMs-based causal reasoning into data-driven causal discovery, which results in more improved and robust performance. Furthermore, our framework extends to the time-series data and exhibited superior performance",
    "checked": false,
    "id": "07e832212bf504ad65890845d6a0a30149974017",
    "semantic_title": "can we utilize pre-trained language models within causal discovery algorithms?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1c3ovLTgTXv": {
    "title": "Character-Level Chinese Dependency Parsing via Modeling Latent Intra-Word Structure",
    "volume": "review",
    "abstract": "Revealing the syntactic structure of sentences in Chinese poses significant challenges for word-level parsers due to the absence of clear word boundaries. To facilitate a transition from word-level to character-level Chinese dependency parsing, this paper proposes modeling latent internal structures within words. In this way, each word-level dependency tree is interpreted as a forest of character-level trees. A constrained Eisner algorithm is implemented to ensure the compatibility of character-level trees, guaranteeing a single root for intra-word structures and establishing inter-word dependencies between these roots. Experiments on Chinese treebanks demonstrate the superiority of our method over both the pipeline framework and previous joint models. A detailed analysis reveals that a coarse-to-fine parsing strategy empowers the model to predict more linguistically plausible intra-word structures",
    "checked": true,
    "id": "66db81e9cdc824650aa0682274d2ff36395ccd8d",
    "semantic_title": "character-level chinese dependency parsing via modeling latent intra-word structure",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5yCBT8myGU": {
    "title": "Bridging Distribution Gap via Self-Distillation Fine-Tuning on Language Models",
    "volume": "review",
    "abstract": "The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities. In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address this problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs",
    "checked": false,
    "id": "c4d43fe1b7e44c5e9929d6edf7bd11de4e6d293a",
    "semantic_title": "self-distillation bridges distribution gap in language model fine-tuning",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=DXGTX7npk6T": {
    "title": "CoLLaM: A Comprehensive Benchmark for Evaluating Large Language Models in Legal Domain",
    "volume": "review",
    "abstract": "Large language models (LLMs) have made significant progress in natural language processing tasks and have shown considerable potential in the legal domain. However, the legal applications often have high requirements on accuracy, reliability and fairness. Applying existing LLMs to legal systems without careful evaluation of their potentials and limitations could lead to significant risks in legal practice.Therefore, to facilitate the healthy development and application of LLMs in the legal domain, we propose a comprehensive benchmark CoLLaM for evaluating LLMs in legal domain.Specifically, CoLLaM is developed based on the language abilities of LLMs and the practical requirements of the legal domain. It introduces a new legal cognitive ability taxonomy (LCAT) featuring six distinctive levels: Memorization, Understanding, Logic Inference, Discrimination, Generation, and Ethic. Leveraging this taxonomy, we collected 13,650 questions across 23 tasks and evaluated them against 38 open-source and commercial LLMs. Our experimental results led to interesting findings and indicate that applying LLMs in the legal domain still has a long way to go. The details of CoLLaM can be found on the anonymous website \\url{https://anonymous.4open.science/r/CoLLaM-31F2}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lLLm605_kP": {
    "title": "Landmark Embedding For Long-Context Retrieval Augmentation",
    "volume": "review",
    "abstract": "Retrieval augmentation is a promising approach to handle long-context language modeling. However, the existing retrieval methods usually work with the chunked context, which is prone to inferior quality of semantic representation and incomplete retrieval of useful information. In this work, we propose a new method for the retrieval augmentation of long-context language modeling, called \\textbf{Landmark Embedding}. Our method is characterized by threefold technical contributions. Firstly, we introduce a \\textit{chunking-free architecture}, which keeps the long context coherent such that high-quality embeddings can be generated for the fine-grained units within the context. Secondly, we present a \\textit{position-aware objective function}, which prioritizes the ultimate boundary for a consecutive span of information. By learning to discriminate such a special position, the useful information can be comprehensively retrieved for the query. Thirdly, we design a novel \\textit{multi-stage learning algorithm}, which makes the best use of readily available data and synthetic data for cost-effective training of the landmark embedding. In our experimental study, landmark embedding is able to substantially improve the performance for both LLaMA-2 and ChatGPT in a variety of long-context tasks; meanwhile, it also outperforms the existing retrieval methods with a notable advantage. Our model and source code will be made publicly available",
    "checked": false,
    "id": "ba233fd52bb840d138c5a2aa46c715ed858cda9d",
    "semantic_title": "bge landmark embedding: a chunking-free embedding method for retrieval augmented long-context large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=XRcVKe9SwMw": {
    "title": "Multi-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models",
    "volume": "review",
    "abstract": "We present Multi-expert Prompting, an enhanced extension of ExpertPrompting (Xu et al., 2023), which guides a large language model (LLM) to fulfill the input instruction as multiple experts, composes a combined response from experts' responses, and selects the best among individual experts and combined responses. Our evaluations demonstrate Multi-expert Prompting surpasses ExpertPrompting and comparable baselines significantly in enhancing the truthfulness, factuality, informativeness, and usefulness, and reducing the toxicity and hurtfulness of LLMs, achieving state-of-the-art truthfulness. Moreover, it is highly adaptable to diverse scenarios, eliminating the need for manual prompt construction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9gWcfurWjeL": {
    "title": "Neurons in Large Language Models: Dead, N-gram, Positional",
    "volume": "review",
    "abstract": "We analyze a family of large language models in such a lightweight manner that can be done on a single GPU. Specifically, we focus on the OPT family of models ranging from 125m to 66b parameters and rely only on whether an FFN neuron is activated or not. First, we find that the early part of the network is sparse and represents many discrete features. Here, many neurons (more than in some layers of the 66b model) are \"dead\", i.e. they never activate on a large collection of diverse data. At the same time, many of the alive neurons are reserved for discrete features and act as token and n-gram detectors. Interestingly, their corresponding FFN updates not only promote next token candidates as could be expected, but also explicitly focus on removing the information about triggering them tokens, i.e., current input. To the best of our knowledge, this is the first example of mechanisms specialized at removing (rather than adding) information from the residual stream. With scale, models become more sparse in a sense that they have more dead neurons and token detectors. Finally, some neurons are positional: them being activated or not depends largely (or solely) on position and less so (or not at all) on textual data. We find that smaller models have sets of neurons acting as position range indicators while larger models operate in a less explicit manner",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=50D6zAYZ51": {
    "title": "EDIT: Towards Enhancing Dialogue Response Generation for Large Language Models by Asking Questions to Detect User's Intentions",
    "volume": "review",
    "abstract": "Large Language Models (LLMs), such as ChatGPT, have recently been applied to various NLP tasks due to its open-domain generation capabilities. However, during the dialogue process, users may have implicit intentions that might be overlooked by LLMs.Besides, it is unlikely for LLMs to encompass all fields comprehensively and LLMs cannot update the latest knowledge in real-time.To tackle these two issues, we propose a framework~\\emph{using LLM to \\textbf{E}nhance dialogue response generation by asking questions to \\textbf{D}etect user's \\textbf{I}mplicit in\\textbf{T}entions} (\\textbf{EDIT}). Firstly, we construct a \\textit{Context-Open-Question} (COQ) dataset to train a question generator (QG) and generate open questions related to the dialogue context as the potential user's intention; Then, EDIT answers those questions by interacting with LLMs and retrieving domain-specific knowledge bases respectively; Finally, EDIT generates response by integrating those answers. To evaluate generated responses, we have specifically designed two metrics, \\textit{Information Content} (IC) and \\textit{Context Coherence} (CC), respectively.The results demonstrated significant improvements after combining current mainstream LLMs with EDIT on two task-oriented dialogue dataset (Wizard of Wikipedia and Holl-E)",
    "checked": false,
    "id": "f8fa697302ef9d759b72404f5a8aa918865abdf4",
    "semantic_title": "a new dialogue response generation agent for large language models by asking questions to detect user's intentions",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=2F8ETP1b80x": {
    "title": "Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint",
    "volume": "review",
    "abstract": "Reinforcement learning (RL) has been widely used in training large language models (LLMs) for preventing unexpected outputs, e.g., reducing harmfulness and errors. However, existing RL methods mainly adopt instance-level reward, which cannot provide fine-grained supervision for complex reasoning tasks. As a result, the RL training cannot be fully aware of the specific part or step that actually leads to the incorrectness in model response. To address it, we propose a new RL method named RLMEC that incorporates a generative model as the reward model, which is trained by the erroneous solution rewriting task under the minimum editing constraint, which can produce token-level supervision for RL training. Based 0on the generative reward model, we design the token-level RL objective for training and an imitation-based regularization for stabilizing RL process. And these two objectives focus on the revision of the key tokens for the erroneous solution, reducing the effect of other unimportant tokens. Experiment results on 8 tasks have demonstrated the effectiveness of our approach. Our code and data will be publicly released",
    "checked": true,
    "id": "59084df7203c6be33838ba3e3854eb9bda053ed2",
    "semantic_title": "improving large language models via fine-grained reinforcement learning with minimum editing constraint",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=Q7MqlDZRa6": {
    "title": "Unlocking Understanding: A Novel Pipeline for Automated Concept Map Extraction from Text",
    "volume": "review",
    "abstract": "Concept maps are graphs of entities and their relations that can foster students' understanding of texts. However, manually constructing them is a challenging task. To overcome this, automatic concept map extraction methods have emerged, typically using a pipeline approach to extract entities and their relations. Yet, existing methods face limitations in scalability, scarcity of data and non open-access architectures. To bridge these gaps, we introduce a novel, modularized and open-source pipeline for concept map extraction, using semantic and sub-symbolic techniques. To address scalability, we integrate a summarization step over the input documents and an importance ranking step to make relation extraction more efficient. To tackle data scarcity, we fine-tune a sequence-to-sequence neural model with limited annotated examples. Our approach achieves state-of-the-art performance on METEOR metrics, particularly crucial for concept maps, given the focus on semantic similarity of this metric, and state-of-the-art precision for ROUGE-2. This contribution advances automated concept map extraction, opening doors to wider applications supporting learning and knowledge access",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OQjGKvPBOO": {
    "title": "BadActs: A Universal Backdoor Defense in the Activation Space",
    "volume": "review",
    "abstract": "Backdoor attacks pose an increasingly severe security threat to Deep Neural Networks (DNNs) during their development stage. In response, backdoor sample purification has emerged as a promising defense mechanism, aiming to eliminate backdoor triggers while preserving the integrity of the clean content in the samples. However, existing approaches have been predominantly focused on the word space, which are ineffective against feature-space triggers and significantly impair performance on clean data. To address this, we introduce a universal backdoor defense that purifies backdoor samples in the activation space by drawing abnormal activations towards optimized minimum clean activation distribution intervals. The advantages of our approach are twofold: (1) By operating in the activation space, our method captures from surface-level information like words to higher-level semantic concepts such as syntax, thus counteracting diverse triggers; (2) the fine-grained continuous nature of the activation space allows for more precise preservation of clean content while removing triggers. Furthermore, we propose a detection module based on statistical information of abnormal activations, to achieve a better trade-off between clean accuracy and defending performance. Extensive experiments on diverse datasets and against diverse attacks (including syntax and style attacks) demonstrate that our defense achieves state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qJBNdeTJkBk": {
    "title": "Learning Personalized Alignment for Evaluating Open-ended Text Generation",
    "volume": "review",
    "abstract": "With rapid progress made in language qualities such as fluency and consistency via large language models (LLMs), there has been increasing interest in assessing alignment with diverse human preferences. Traditional metrics heavily rely on lexical similarity with human-written references and have been observed to suffer from a poor correlation with human evaluation. Furthermore, they ignore the diverse preferences of humans, a key aspect in evaluating open-ended tasks like story generation. Inspired by these challenges, we introduce a personalized evaluation framework PERSE to provide an interpretable evaluation from an individual perspective. PERSE first deduces the specific preference from several annotated data and then measures the quality based on this preference. Moreover, it offers an interpretable explanation for its evaluation, such as the score of different aspects. Through instruction-tuning on 10k data, our 13B LLaMA-2-based PERSE shows a 15.8\\% increase in Kendall correlation and a 13.7\\% rise in accuracy on zero-shot reviewers compared to GPT-4",
    "checked": true,
    "id": "c81fd487a418268d42dce8613236297a9dc127fc",
    "semantic_title": "learning personalized alignment for evaluating open-ended text generation",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=QqGCEHSA21": {
    "title": "Extending LLMs' Context Window with 100 Samples",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) are known to have limited extrapolation ability beyond their pre-trained context window, constraining their application in downstream tasks with lengthy inputs. Recent studies have sought to extend LLMs' context window by modifying rotary position embedding (RoPE), a popular position encoding method adopted by well-known LLMs such as LLaMA, PaLM, and GPT-NeoX. However, prior works like Position Interpolation (PI) and YaRN are resource-intensive and lack comprehensive experiments to assess their applicability. In this work, we identify the inherent need for LLMs' attention entropy (i.e. the information entropy of attention scores) to maintain stability and introduce a novel extension to RoPE which combines adjusting RoPE's base frequency and scaling the attention logits to help LLMs efficiently adapt to a larger context window. We validate the superiority of our method in both fine-tuning performance and robustness across different context window sizes on various context-demanding tasks. Notably, our method extends the context window of LLaMA-2-7B-Chat to 16,384 with only 100 samples and 6 training steps, showcasing extraordinary efficiency. Finally, we also explore how data compositions and training curricula affect context window extension for specific downstream tasks, suggesting fine-tuning LLMs with lengthy conversations as a good starting point. We release our code and SFT data at https://anonymous.4open.science/r/Entropy-ABF-0084/README.md",
    "checked": true,
    "id": "0595dac8260443365dfbe4821787419736baaa66",
    "semantic_title": "extending llms' context window with 100 samples",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=whp2iJ21rmW": {
    "title": "Mirror: Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning",
    "volume": "review",
    "abstract": "While Large language models (LLMs) have the capability to iteratively reflect on their own outputs, recent studies have observed their struggles with knowledge-rich problems without access to external resources. In addition to the inefficiency of LLMs in self-assessment, we also observe that LLMs struggle to revisit their predictions despite receiving explicit negative feedback. Therefore, We propose Mirror, a Multiple-perspective self-reflection method for knowledge-rich reasoning, to avoid getting stuck at a particular reflection iteration. Mirror enables LLMs to reflect from multiple-perspective clues, achieved through a heuristic interaction between a Navigator and a Reasoner. It guides agents toward diverse yet plausibly reliable reasoning trajectory without access to ground truth by encouraging (1) diversity of directions generated by Navigator and (2) agreement among strategically induced perturbations in responses generated by the Reasoner. The experiments on five reasoning datasets demonstrate that Mirror's superiority over several contemporary self-reflection approaches. Additionally, the ablation study studies clearly indicate that our strategies alleviate the aforementioned challenges",
    "checked": false,
    "id": "ab15a0fc78532b2cae1007e5315f838ae23d0a4a",
    "semantic_title": "mirror: a multiple-perspective self-reflection method for knowledge-rich reasoning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=s9Bg_00-Qc": {
    "title": "AS-ES Learning: Towards efficient CoT learning in small models",
    "volume": "review",
    "abstract": "Chain-of-Thought (CoT) serves as a critical emerging ability in LLMs, especially when it comes to logical reasoning. Attempts have been made to induce such ability in small models as well by distilling from the data with CoT generated by Large Language Models (LLMs). However, existing methods often simply generate and incorporate more data from LLMs and fail to note the importance of efficiently utilizing existing CoT data. We here propose a new training paradigm AS-ES (Abstractive Segments - Extractive Segments) learning, which exploits the inherent information in CoT for iterative generation. Experiments show that our methods surpass the direct seq2seq training on CoT-extensive tasks like MWP and PET summarization, without data augmentation or altering the model itself. Furthermore, we explore the reason behind the inefficiency of small models in learning CoT and provide an explanation of why AS-ES learning works, giving insights into the underlying mechanism of CoT",
    "checked": true,
    "id": "f6a89478d137b647f1291e552957d4cf147f5909",
    "semantic_title": "as-es learning: towards efficient cot learning in small models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9mK0vffxz3": {
    "title": "Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint",
    "volume": "review",
    "abstract": "Large language models (LLMs) internalize enormous \\textit{parametric knowledge} during pre-training. Concurrently, realistic applications necessitate external \\textit{contextual knowledge} to aid models on the underlying tasks. This raises a crucial dilemma known as \\textit{knowledge conflicts}, where the contextual knowledge clashes with the parametric knowledge. However, existing decoding works are specialized in resolving knowledge conflicts and could inadvertently deteriorate performance in absence of conflicts. In this paper, we propose an adaptive decoding method, termed as contextual information-entropy constraint decoding (COIECD), to discern whether the knowledge conflicts occur and resolve them. It can improve the model's faithfulness to conflicting context, and simultaneously maintain high performance among non-conflicting context. Our experiments show that COIECD exhibits strong performance and robustness over knowledge conflicts in realistic datasets. Code is available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vaJI0ZQyor": {
    "title": "Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!",
    "volume": "review",
    "abstract": "Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans. However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation. This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without any training. Our experiments with ED across three datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin. Crucially, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment",
    "checked": true,
    "id": "3283764bbbd7084c9e6995e20953dbf36b25a226",
    "semantic_title": "emulated disalignment: safety alignment for large language models may backfire!",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=IvFIl-0r_T": {
    "title": "When Only Time Will Tell: Interpreting How Transformers Process Local Ambiguities Through the Lens of Restart-Incrementality",
    "volume": "review",
    "abstract": "Incremental models that process sentences one token at a time will sometimes encounter points where more than one interpretation is possible. Causal models are forced to output one interpretation and continue, whereas models that can revise may edit their previous output as the ambiguity is resolved. In this work, we look at how restart-incremental Transformers build and update internal states, in an effort to shed light on what processes cause revisions not viable in autoregressive models. We propose an interpretable way to analyse the incremental states, showing that their sequential structure encodes information on the garden path effect and its resolution. Our method brings insights on various bidirectional encoders for contextualised meaning representation and dependency parsing, contributing to show their advantage over causal models when it comes to revisions",
    "checked": true,
    "id": "eed750b46aba9c942aa91cc25974f32a0f8375f2",
    "semantic_title": "when only time will tell: interpreting how transformers process local ambiguities through the lens of restart-incrementality",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=GYOGbvpOu7p": {
    "title": "Forward-Backward Reasoning in Large Language Models for Mathematical Verification",
    "volume": "review",
    "abstract": "Self-Consistency samples diverse reasoning chains with answers and chooses the final answer by majority voting. It is based on forward reasoning and cannot further improve performance by sampling more reasoning chains when saturated. To further boost performance, we introduce backward reasoning to verify candidate answers. Specifically, for mathematical tasks, we mask a number in the question and ask the LLM to answer a backward question created by a simple template, i.e., to predict the masked number when a candidate answer is provided. Instead of using forward or backward reasoning alone, we propose FOBAR to combine FOrward and BAckward Reasoning for verification. Extensive experiments on six standard mathematical data sets and three LLMs show that FOBAR achieves state-of-the-art performance. In particular, FOBAR outperforms Self-Consistency, which uses forward reasoning alone, demonstrating that combining forward and forward reasoning is better. In addition, FOBAR performs better than existing verification methods, showing the effectiveness of the simple template used in backward reasoning and the proposed combination. Extensions to non-mathematical problems are also discussed and validated empirically",
    "checked": true,
    "id": "801d7ba75fc833aa76ce4863dc1f79e30ee0c23f",
    "semantic_title": "forward-backward reasoning in large language models for mathematical verification",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=gZ_hSxwd8d": {
    "title": "SirLLM: Streaming Infinite Retentive LLM",
    "volume": "review",
    "abstract": "As Large Language Models (LLMs) become increasingly prevalent in various domains, their ability to process inputs of any length and maintain a degree of memory becomes essential. However, the one-off input of overly long texts is limited, as studies have shown that when input lengths exceed the LLMs' pre-trained text length, there is a dramatic decline in text generation capabilities. Moreover, simply extending the length of pre-training texts is impractical due to the difficulty in obtaining long text data and the substantial memory consumption costs this would entail for LLMs. Recent efforts have employed streaming inputs to alleviate the pressure of excessively long text inputs, but this approach can significantly impair the model's long-term memory capabilities.Motivated by this challenge, we introduce Streaming Infinite Retentive LLM (SirLLM), which allows LLMs to maintain longer memory during infinite-length dialogues without the need for fine-tuning. SirLLM utilizes the Token Entropy metric and a memory decay mechanism to filter key phrases, endowing LLMs with both long-lasting and flexible memory. We designed three distinct tasks and constructed three datasets to measure the effectiveness of SirLLM from various angles: (1) DailyDialog; (2) Grocery Shopping; (3) Rock-Paper-Scissors. Our experimental results robustly demonstrate that SirLLM can achieve stable and significant improvements across different LLMs and tasks, compellingly proving its effectiveness. When having a coversation, \"A sir could forget himself,\" but SirLLM never does!",
    "checked": true,
    "id": "677b6888a24c0f241b200e5f2b8e0ae6d347ba0b",
    "semantic_title": "sirllm: streaming infinite retentive llm",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GvQuUcLdMPa": {
    "title": "Tree-Planted Transformers: Large Language Models with Implicit Syntactic Supervision",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have achieved remarkable success thanks to scalability on large text corpora, but have some drawback in training efficiency. In contrast, Syntactic Language Models (SLMs) can be trained efficiently to reach relatively high performance thanks to syntactic supervision, but have trouble with scalability. Thus, given these complementary advantages of LLMs and SLMs, it is necessary to develop an architecture that integrates the scalability of LLMs with the training efficiency of SLMs, namely Syntactic Large Language Models (SLLM). In this paper, we propose a novel method dubbed tree-planting: implicitly \"plant\" trees into attention weights of Transformer LMs to reflect syntactic structures of natural language. Specifically, Transformer LMs trained with tree-planting will be called Tree-Planted Transformers (TPT), which learn syntax on small treebanks via tree-planting and then scale on large text corpora via continual learning with syntactic scaffolding. Targeted syntactic evaluations on the SyntaxGym benchmark demonstrated that TPTs, despite the lack of explicit syntactic supervision, significantly outperformed various SLMs with explicit syntactic supervision that generate hundreds of syntactic structures in parallel, suggesting that tree-planting and TPTs are the promising foundation for SLLMs",
    "checked": true,
    "id": "d1d5bbfaa1d58493711d40fa39bdd345560c7821",
    "semantic_title": "tree-planted transformers: large language models with implicit syntactic supervision",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=dpG_ilbqYL": {
    "title": "ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases",
    "volume": "review",
    "abstract": "Enabling large language models to utilize real-world tools effectively is crucial for achieving embodied intelligence. Existing approaches to tool learning have either primarily relied on extremely large language models, such as GPT-4, to attain generalized tool-use abilities in a zero-shot manner, or utilized supervised learning to train limited scopes of tools on compact models. However, it remains uncertain whether smaller language models can achieve generalized tool-use abilities without tool-specific training. To address this question, this paper introduces ToolAlpaca, a novel framework designed to automatically generate a diverse tool-use corpus and learn generalized tool-use abilities on compact language models with minimal human intervention. Specifically, ToolAlpaca first automatically creates a highly diversified tool-use corpus by building a multi-agent simulation environment. The corpus contains 3.9k tool-use instances from more than 400 real-world tool APIs spanning 50 distinct categories. Subsequently, the constructed corpus is employed to fine-tune compact language models, resulting in two models, namely ToolAlpaca-7B and ToolAlpaca-13B, respectively. Finally, we evaluate the ability of these models to utilize previously unseen tools without specific training. Experimental results demonstrate that ToolAlpaca achieves effective generalized tool-use capabilities comparable to those of extremely large language models like GPT-3.5, demonstrating that learning generalized tool-use ability is feasible for compact language models",
    "checked": true,
    "id": "455866ca838f356b53a7e3e5b344834f9e93dbbc",
    "semantic_title": "toolalpaca: generalized tool learning for language models with 3000 simulated cases",
    "citation_count": 97,
    "authors": []
  },
  "https://openreview.net/forum?id=oBlOMS_Lga": {
    "title": "Bridging Language Gaps: Enhancing Few-Shot Language Adaptation",
    "volume": "review",
    "abstract": "The disparity in language resources presents a significant challenge in multilingual NLP. High-resource languages have the advantage of extensive data and advanced models, whereas low-resource languages often lack the data necessary for effective training. Our research aims to bridge this gap by utilizing the task-specific knowledge from multilingual models trained on high-resource languages and efficiently transferring it to lower-resource languages. Our focus is on task-specific fine-tuning, the stage where acquiring labeled data is costly and where methods that optimize data use are most valuable. The primary advantage of our approach is its data efficiency, enabling rapid adaptation to new languages in a task-specific few-shot setting and reducing the need for large labeled datasets. Our Contrastive Language Alignment with Prompting (CoLAP) method integrates contrastive learning with cross-lingual representations, facilitating knowledge transfer from high-resource to lower-resource languages. This effectively narrows the cross-lingual performance gap, contributing to the development of more efficient multilingual NLP techniques",
    "checked": false,
    "id": "64a257d1e5d8a18a0fbef5b25fe19413d87ef4d9",
    "semantic_title": "distilled feature fields enable few-shot language-guided manipulation",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=jzDk45U07U": {
    "title": "Transformers Can Model Human Hyperprediction in Buzzer Quiz",
    "volume": "review",
    "abstract": "Humans are thought to predict the next words during sentence comprehension, but under unique circumstances, they demonstrate an ability for longer coherent word sequence prediction. This study investigates whether language models can model such hyperprediction observed in humans during sentence processing, specifically in the context of buzzer quizzes. We conducted eye-tracking experiments where participants read the first half of buzzer quiz questions and predicted the second half, while we modeled their reading time using language models. The results showed that the pre-trained language model can partially capture human hyperprediction. When the language model was fine-tuned with quiz questions, the perplexity value decreased. Lower perplexity corresponded to higher psychometric predictive power; however, excessive data for fine-tuning led to a decrease in perplexity and the fine-tuned model exhibited a low psychometric predictive power",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c4Y7fDO-36": {
    "title": "Efficient Knowledge Infusion via KG-LLM Alignment",
    "volume": "review",
    "abstract": "To tackle the problem of domain-specific knowledge scarcity within large language models(LLMs), knowledge graph-retrieval-augmented method has been proven to be an effective and efficient technique for knowledge infusion. However, existing approaches face two primary challenges: knowledge mismatch between public available knowledge graphs and the specific domain of the task at hand, and poor information compliance of LLM with knowledge graphs. In this paper, we leverage a small set of labeled samples and a large-scale corpus to efficiently construct domain-specific knowledge graphs by LLM, addressing the issue of knowledge mismatch. Additionally, we propose a three-stage KG-LLM alignment strategy to enhance the LLM's capability to utilize information from knowledge graphs. We conduct experiments with a limited-sample setting on two biomedical question-answering datasets, and the results demonstrate that our approach outperforms existing baselines",
    "checked": true,
    "id": "9ec9ec38f7057b123f297f9a66d544974194f5d0",
    "semantic_title": "efficient knowledge infusion via kg-llm alignment",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=vn21nDvnt1": {
    "title": "In-Context Learning with Iterative Demonstration Selection",
    "volume": "review",
    "abstract": "Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Leveraging the merits of both dimensions, we propose Iterative Demonstration Selection (IDS). Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is then used to choose demonstrations that are prepended to the test sample for inference. The generated answer is followed by its corresponding reasoning path for extracting a new set of demonstrations in the next iteration. After several iterations, IDS adopts majority voting to obtain the final result. Through extensive experiments on tasks including reasoning, question answering, topic classification, and sentiment analysis, we demonstrate that IDS can consistently outperform existing ICL demonstration selection methods",
    "checked": true,
    "id": "b217b6bc340af9a10bebbf8acc36ea30871769bd",
    "semantic_title": "in-context learning with iterative demonstration selection",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=E8r0jBzfoH": {
    "title": "Discover rather than Memorize: A Novel Benchmark for Relational Triple Extraction",
    "volume": "review",
    "abstract": "Relational Triple Extraction (RTE), one of the crucial components of information extraction, has experienced rapid development in recent years. However, due to the triple duplication problem in existing datasets, previous methods often yield highly competitive results by simply memorizing the duplicated triples rather than discovering the new triples from raw text. Specifically, In the two most widely-used datasets (NYT and WebNLG), more than 80% of the triples from the test set are direct duplicates of triples already present in their training set. In response to this, we propose a new dataset, named ENT, to evaluate the model's ability to Extract New Triples, which aligns more coherently with the objectives of the RTE task. Specifically, based on the Wikidata knowledge graph slices and Large Language Model Prompting, we design an RTE dataset construction pipeline. It consists of four steps, including: 1) Preprocess, 2) Paragraph Generation, 3) Rule-based Check and 4) Semantic Check. ENT comprises 300k+ unique triples with all the test set samples containing at least one new triple. We conduct a re-evaluation of nine existing state-of-the-art methods and observe a generalized 10%+ and 7.5%+ decrease in extraction accuracy on ENT compared to NYT and WebNLG respectively. This demonstrates that ENT is a more challenging and meaningful benchmark, and we hope it will lead to new directions in the study of the RTE task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vUeZEfCrzYG": {
    "title": "P4: Plug-and-Play Discrete Prompting for Large Language Models Personalization",
    "volume": "review",
    "abstract": "Empowering Large Language Models (LLMs) with distinct human-like personality traits has become an innovative task for developing advanced dialog systems. Although LLMs demonstrate impressive capabilities in following instructions, directly prompting them to exhibit certain personalities through manually crafted instructions may result in sub-optimal performance. In this paper, we propose a plug-and-play prompting method to manipulate the LLMs' personality traits. Specifically, we append discrete personalized suffixes, automatically generated through an aggregated gradient-based search method, to the user query or dialog histories and induce LLMs to respond with target personalities. In addition, due to the high redundancy of the search space, we adopt a reward-based strategy to prune the vocabulary and focus exclusively on influential tokens. Experiment results on four models ranging from $1.1$B to $13$B show that our method achieves $79.9$\\% accuracy in customizing LLMs' personalities, significantly outperforming other prompting methods ($65.5\\%$) and model editing methods. Our method also excels in generation fluency and quality with the lowest generation perplexity and the highest GPT-4 evaluation scores",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qT9x0XaaQrV": {
    "title": "Fine-tuning Large Language Models for Automated Diagnostic Screening Summaries",
    "volume": "review",
    "abstract": "Improving mental health support in developing countries is a pressing need. One potential solution is the development of scalable, automated systems to conduct diagnostic screenings, which could help alleviate the burden on mental health professionals. In this work, we evaluate several state-of-the-art Large Language Models (LLMs), with and without fine-tuning, on our custom dataset for generating concise summaries from mental state examinations. We rigorously evaluate four different models for summary generation using established ROUGE metrics and input from human evaluators. The results highlight that our top-performing fine-tuned model outperforms existing models, achieving ROUGE-1 and ROUGE-L values of 0.810 and 0.764, respectively. Furthermore, we assessed the fine-tuned model's generalizability on a publicly available D4 dataset, and the outcomes were promising, indicating its potential applicability beyond our custom dataset",
    "checked": true,
    "id": "cf7246215466006213d82bdfaa7263914e56abef",
    "semantic_title": "fine-tuning large language models for automated diagnostic screening summaries",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ISntmJqGxk": {
    "title": "Generating Zero-shot Abstractive Explanations for Rumour Verification",
    "volume": "review",
    "abstract": "The task of rumour verification in social media concerns assessing the veracity of a claim on the basis of conversation threads that result from it. While previous work has focused on predicting a veracity label, here we reformulate the task to generate model-centric free-text explanations of a rumour's veracity. The approach is model agnostic in that it generalises to any model. Here we propose a novel GNN-based rumour verification model. We follow a zero-shot approach by first applying post-hoc explainability methods to score the most important posts within a thread and then we use these posts to generate informative explanations using opinion-guided summarisation. To evaluate the informativeness of the explanatory summaries, we exploit the few-shot learning capabilities of a large language model (LLM). Our experiments show that LLMs can have similar agreement to humans in evaluating summaries. Importantly, we show explanatory abstractive summaries are more informative and better reflect the predicted rumour veracity than just using the highest ranking posts in the thread",
    "checked": true,
    "id": "94056fbd7063f6808819e42ddd40de761b7a018e",
    "semantic_title": "generating zero-shot abstractive explanations for rumour verification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SAiAqI_ywE8": {
    "title": "CodeComplex: Dataset for Worst-Case Time Complexity Prediction",
    "volume": "review",
    "abstract": "Analyzing the worst-case time complexity of a code is a crucial task in computer science and software engineering for ensuring the efficiency, reliability, and robustness of software systems. However, it is well-known that the problem of determining the worst-case time complexity of a program is theoretically undecidable. In response to this challenge, we introduce CodeComplex, a novel source code dataset where each code is manually annotated with a corresponding worst-case time complexity. CodeComplex comprises 4,900 Java codes and an equivalent number of Python codes, all sourced from programming competitions and annotated with complexity labels by a panel of algorithmic experts. To the best of our knowledge, CodeComplex stands as the most extensive code dataset tailored for predicting complexity. Subsequently, we present the outcomes of our experiments employing various baseline models, leveraging state-of-the-art neural models in code comprehension like CodeBERT, GraphCodeBERT, UniXcoder, PLBART, CodeT5, CodeT5+, and ChatGPT. We analyze how the dataset impacts the model's learning in predicting time complexity. We release our dataset and baseline models publicly to encourage the relevant (NLP, SE, and PL) communities to participate in this research",
    "checked": false,
    "id": "054420a9c624c9f70143e41571a2dc9756f40521",
    "semantic_title": "codecomplex: a time-complexity dataset for bilingual source codes",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uWeadjNEVDT": {
    "title": "GroundingGPT: Language Enhanced Multi-modal Grounding Model",
    "volume": "review",
    "abstract": "Multi-modal large language models (MLLMs) have demonstrated remarkable performance across various tasks. However, these models often prioritize capturing global information and overlook the importance of perceiving local information. This limitation hinders their ability to effectively understand fine-grained details and handle grounding tasks that necessitate nuanced comprehension. Although some recent works have made strides in this, they have primarily focused on single-modality inputs. Therefore, we propose GroundingGPT, an end-to-end language enhanced multi-modal grounding model. It is designed to perform fine-grained grounding tasks for three modalities: image, video and audio. To enhance the model's performance, we adopt a coarse-to-fine training strategy, utilizing a three-stage training approach to progressively enhance the model's semantic awareness and fine-grained understanding capabilities. Additionally, we employ a diversified stage-specific dataset construction pipeline, developing a multi-modal, multi-granularity dataset tailored for training the model in different stages. Extensive experiments conducted on multiple multi-modal benchmarks demonstrate that our model achieves an impressive fine-grained understanding of multi-modal inputs on grounding tasks while maintaining or improving its global comprehension capabilities. We will make the code, dataset, and model publicly available to facilitate further research in this area",
    "checked": false,
    "id": "090294139835018d2357a7b638fb03b4c7dd5df3",
    "semantic_title": "groundinggpt:language enhanced multi-modal grounding model",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=Eh-Y5VXGPDk": {
    "title": "Unsupervised Sign Language Translation and Generation",
    "volume": "review",
    "abstract": "Motivated by the success of unsupervised neural machine translation (UNMT), we introduce an unsupervised sign language translation and generation network (USLNet), which learns from abundant single-modality (text and video) data without parallel sign language data. USLNet comprises two main components: single-modality reconstruction modules (text and video) that rebuild the input from its noisy version in the same modality and cross-modality back-translation modules (text-video-text and video-text-video) that reconstruct the input from its noisy version in the different modality using back-translation procedure. Unlike the single-modality back-translation procedure in text-based UNMT, USLNet faces the cross-modality discrepancy in feature representation, in which the length and the feature dimension mismatch between text and video sequences. We propose a sliding window method to address the issues of aligning variable-length text with video sequences.To our knowledge, USLNet is the first unsupervised sign language translation and generation model capable of generating both natural language text and sign language video in a unified manner. Experimental results on the BBC-Oxford Sign Language dataset (BOBSL) and Open-Domain American Sign Language dataset (OpenASL) reveal that USLNet achieves competitive results compared to supervised baseline models, indicating its effectiveness in sign language translation and generation",
    "checked": true,
    "id": "d8155a5e0730a5d3aab22b6582f7a862f9065f2a",
    "semantic_title": "unsupervised sign language translation and generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sl4r1L6Vhv": {
    "title": "Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) are powerful tools for natural language processing, enabling novel applications and user experiences. However, to achieve optimal performance, LLMs often require adaptation with private data, which poses privacy and security challenges. Several techniques have been proposed to adapt LLMs with private data, such as Low-Rank Adaptation (LoRA), Soft Prompt Tuning (SPT), and In-Context Learning (ICL), but their comparative privacy and security properties have not been systematically investigated. In this work, we fill this gap by evaluating the robustness of LoRA, SPT, and ICL against three types of well-established attacks: membership inference, which exposes data leakage (privacy); backdoor, which injects malicious behavior (security); and model stealing, which can violate intellectual property (privacy and security). Our results show that there is no silver bullet for privacy and security in LLM adaptation and each technique has different strengths and weaknesses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LX7iCQR6Dfd": {
    "title": "LLaST: Improved End-to-end Speech Translation System Leveraged by Large Language Models",
    "volume": "review",
    "abstract": "We introduces LLaST, a framework for building high-performance Large Language model based Speech-to-text Translation systems. We address the limitations of end-to-end speech translation (E2E ST) models by exploring model architecture design and optimization techniques tailored for LLMs. Our approach includes LLM-based speech translation architecture design, ASR-augmented training, multilingual data augmentation, and dual-LoRA optimization. Our approach demonstrates superior performance on the CoVoST-2 benchmark and showcases exceptional scaling capabilities powered by LLMs.We believe this effective method will serve as a strong baseline for speech translation and provide insights for futureimprovements of the LLM-based speech translation framework",
    "checked": true,
    "id": "f4259f5d0b63f55859795161859b830e155318af",
    "semantic_title": "llast: improved end-to-end speech translation system leveraged by large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=7zByVZHkSY": {
    "title": "Enhancing Large Language Model with Self-Controlled Memory Framework",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) are constrained by their inability to process lengthy inputs, resulting in the loss of critical historical information. To address this limitation, in this paper, we propose the Self-Controlled Memory (SCM) framework to enhance the ability of LLMs to maintain long-term memory and recall relevant information. Our SCM framework comprises three key components: an LLM-based agent serving as the backbone of the framework, a memory stream storing agent memories, and a memory controller updating memories and determining when and how to utilize memories from memory stream. Additionally, the proposed SCM is able to process ultra-long texts without any modification or fine-tuning, which can integrate with any instruction following LLMs in a plug-and-play paradigm. Furthermore, we annotate a dataset to evaluate the effectiveness of SCM for handling lengthy inputs. The annotated dataset covers three tasks: long-term dialogues, book summarization, and meeting summarization. Experimental results demonstrate that our method achieves better retrieval recall and generates more informative responses compared to competitive baselines in long-term dialogues",
    "checked": true,
    "id": "f711aae062ae30c0888910b2bdcc5be6c1d1c340",
    "semantic_title": "enhancing large language model with self-controlled memory framework",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=rCUw2Mq_E_cI": {
    "title": "Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data",
    "volume": "review",
    "abstract": "Quantitative reasoning is a critical skill to analyze data, yet the assessment of such ability remains limited. To address this gap, we introduce the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate Large Language Models' capability in statistical and causal reasoning with real-world data. The benchmark comprises a carefully constructed dataset of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers. To compare models' quantitative reasoning abilities on data and text, we enrich the benchmark with an auxiliary set of 290 text-only questions, namely QRText. We evaluate natural language reasoning, program-based reasoning, and agent reasoning methods including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models. The strongest model GPT-4 achieves an accuracy of 58%, which has a large room for improvement. Among open-source models, Deepseek-coder-instruct, a code LLM pretrained on 2T tokens, gets the highest accuracy of 37%. Analysis reveals that models encounter difficulties in data analysis and causal reasoning, and struggle in using causal knowledge and provided data simultaneously",
    "checked": true,
    "id": "83ed094e60c16abae2f1eb6a1be25934dc83648d",
    "semantic_title": "are llms capable of data-based statistical and causal reasoning? benchmarking advanced quantitative reasoning with data",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=nMDyxG_bP6": {
    "title": "Topic-XICL: Demonstration Selection with Topic Inference for Cross-lingual In-context Learning",
    "volume": "review",
    "abstract": "Cross-lingual in-context learning (XICL) shows promise for adapting large language models (LLMs) to low-resource languages. Previous methods rely on off-the-shelf retrievers or task-specific retrievers based on feedback signals from LLM for demonstration selection, they often overlook important factors beyond semantic similarity or can be resource-costly. To address these challenges, we propose a novel approach called Topic-XICL, which leverages a latent topic model to select demonstrations across languages. We assume that latent topic variables incorporate additional information beyond semantics, such as syntax and task structure. By training this topic model on rich-resource language data with a small parameter LLM, we obtain more informative demonstrations by topic inference and utilize them for in-context learning across various LLMs. Our method is tested on three multilingual tasks (XNLI, XCOPA, and TydiQA-GoldP) using three different-size BLOOMZ models and three models with approximately 7 billion parameters (BLOOM, XGLM, and Llama2). Comparative evaluations against random selection, semantic similarity selection, and clustering-based selection baselines show consistent improvements in multilingual average performance with our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zCYPgrw9Cj8": {
    "title": "Measuring Bias and Agreement in Large Language Model Presupposition Judgments",
    "volume": "review",
    "abstract": "Identifying linguistic bias in text requires understanding what is said and what is meant. This requires going beyond what is being asserted directly, and determining what is presupposed. Large language models (LLMs) represent a potential automatic approach for identifying presupposed content, but it is unknown how well LLM judgments correspond to human judgments. Further, LLMs may exhibit their own biases in determining what is presupposed. To study this empirically, we prompt multiple LLMs to make presupposition judgments for texts of varying domains from three different human-labeled datasets. We calculate the agreement between LLMs and human raters, and find that variations in text domain, verb factivity, context window size, and the type of presupposition trigger result in changes to human-model agreement scores. We also observe discrepancies in agreement scores that indicate potential biases from LLMs. The gender of the subject appears to impact agreement, as female pronouns are associated with lower agreement than male pronouns. Across multiple dimensions, differences in political ideology appear to correspond to differences in agreement",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=keGqYL62JG": {
    "title": "When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection",
    "volume": "review",
    "abstract": "Depression is a critical concern in global mental health, prompting extensive research into AI-based detection methods. Among various AI technologies, Large Language Models (LLMs) stand out for their versatility in healthcare applications. However, the application of LLMs in the identification and analysis of depressive states remains relatively unexplored, presenting an intriguing avenue for future research. In this paper, we present an innovative approach to employ an LLM in the realm of depression detection, integrating acoustic speech information into the LLM framework for this specific application. We investigate an efficient method for automatic depression detection by integrating speech signals into LLMs utilizing Acoustic Landmarks. This approach is not only valuable for the detection of depression but also represents a new perspective in enhancing the ability of LLMs to comprehend and process speech signals. By incorporating acoustic landmarks, which are specific to the pronunciation of spoken words, our method adds critical dimensions to text transcripts. This integration also provides insights into the unique speech patterns of individuals, revealing the potential mental states of individuals. By encoding acoustic landmarks information into LLMs, evaluations of the proposed approach on the DAIC-WOZ dataset reveal state-of-the-art results when compared with existing Audio-Text baselines",
    "checked": true,
    "id": "7b75b3d9f08aea9498baef8426f954106c3b5802",
    "semantic_title": "when llms meets acoustic landmarks: an efficient approach to integrate speech into large language models for depression detection",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=DiiK7Zq7F57": {
    "title": "MM-LLMs: Recent Advances in MultiModal Large Language Models",
    "volume": "review",
    "abstract": "In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Initially, we outline general design formulations for model architecture and training pipeline. Subsequently, we introduce a taxonomy encompassing $122$ MM-LLMs, each characterized by its specific formulations. Furthermore, we review the performance of selected MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Finally, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field. We hope that this survey contributes to the ongoing advancement of the MM-LLMs domain",
    "checked": true,
    "id": "a050c9b0c321839e4427ab9defa3463be7825ac4",
    "semantic_title": "mm-llms: recent advances in multimodal large language models",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=5j8ATqsxB_": {
    "title": "FAME: Factual Multi-task Model Editing Benchmark",
    "volume": "review",
    "abstract": "Large language models (LLMs) possess the capability to retain a wide range of knowledge, albeit they also show tendencies for factual inaccuracies. To rectify such inaccuracies without the necessity for costly model retraining, a variety of model editing approaches have been proposed, aiming to correct these inaccuracies in a more cost-efficient way. To evaluate these model editing methods, previous work introduced a series of datasets. However, most of these datasets use fabricated data, rendering them incapable of evaluating or improving the capabilities of models. Additionally, they only included a single task, preventing them from comprehensively simulating the real world. To resolve these challenges and effectively enhance the capabilities of LLMs, we present FAME (FActual Multi-task model Editing), an authentic, comprehensive, and multi-task dataset, which is designed to amplify the practicality of model editing. We then propose SKEME (Structured Knowledge retrieved by Exact Matching and reranking Editing), a model editing technique predicated on structured knowledge retrieval. The experiments demonstrate that our method performs excellently across various tasks and scenarios, confirming its practicality",
    "checked": false,
    "id": "8efecf2a2192c81be47fb0214c14b76aa9472ced",
    "semantic_title": "mc-mke: a fine-grained multimodal knowledge editing benchmark emphasizing modality consistency",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DRoRF5KGgIe": {
    "title": "GKT: A Novel Guidance-Based Knowledge Transfer Framework For Efficient Cloud-edge Collaboration LLM Deployment",
    "volume": "review",
    "abstract": "The burgeoning size of Large Language Models (LLMs) has led to enhanced capabilities in generating responses, albeit at the expense of increased inference times and elevated resource demands. Existing methods of acceleration, predominantly hinged on knowledge distillation, generally necessitate fine-tuning of considerably large models, such as Llama-7B, posing a challenge for average users. Furthermore, present techniques for expediting inference and reducing costs operate independently. To address these issues, we introduce a novel and intuitive Guidance-based Knowledge Transfer (GKT) framework. This approach leverages a larger LLM as a ``teacher'' to create guidance prompts, paired with a smaller ``student'' model to finalize responses. Remarkably, GKT requires no fine-tuning and doesn't necessitate the teacher and student models to have the same vocabulary, allowing for extensive batch generation to accelerate the process while ensuring user customization. GKT can be seamlessly integrated into cloud-edge collaboration architectures, and is versatile enough for plug-and-play application across various models. It excels in both efficiency and affordability, epitomizing a ``cheap and cheerful'' solution. GKT achieves a maximum accuracy improvement of 14.18%, along with a 10.72 times speed-up on GSM8K and an accuracy improvement of 14.00 % along with a 7.73 times speed-up in CSQA. When utilizing ChatGPT as teacher model and Llama2-70B as the student model, we can achieve 95.00% of ChatGPT's performance at 52% of the cost. The results highlight substantial enhancements in accuracy and processing speed on the GSM8K and CSQA datasets, surpassing the performance of using either the student or teacher models in isolation",
    "checked": true,
    "id": "07d46677c31a450fecbecd3e96b096fe250031b0",
    "semantic_title": "gkt: a novel guidance-based knowledge transfer framework for efficient cloud-edge collaboration llm deployment",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iX8tO1U5iwa": {
    "title": "Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding",
    "volume": "review",
    "abstract": "Contemporary translation engines based on the encoder-decoder framework have made significant strides in development. However, the emergence of Large Language Models (LLMs) has disrupted their position by presenting the potential for achieving superior translation quality. To uncover the circumstances in which LLMs excel and explore how their strengths can be harnessed to enhance translation quality, we first conduct a comprehensive analysis to assess the strengths and limitations of various commercial NMT systems and MT-oriented LLMs. Our findings indicate that neither NMT nor MT-oriented LLMs alone can effectively address all the translation issues, but MT-oriented LLMs show promise as a complementary solution to NMT systems. Building upon these insights, we propose Cooperative Decoding (CoDec), which treats NMT systems as a pretranslation model and MT-oriented LLMs as a supplemental solution to handle complex scenarios beyond the capability of NMT alone. Experimental results on the WMT22 test sets and a newly collected test set WebCrawl demonstrate the effectiveness and efficiency of CoDec, highlighting its potential as a robust solution for combining NMT systems with MT-oriented LLMs in the field of machine translation",
    "checked": true,
    "id": "c2bc469112a3b55e29b5fb7fd28b5f24d897269a",
    "semantic_title": "improving machine translation with large language models: a preliminary study with cooperative decoding",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=HyILHI4F7d": {
    "title": "DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators",
    "volume": "review",
    "abstract": "Generally, the decoder-only large language models (LLMs) are adapted to context-aware neural machine translation (NMT) in a concatenating way, where LLMs take the concatenation of the source sentence (i.e., intra-sentence context) and the inter-sentence context as the input, and then to generate the target tokens sequentially. This adaptation strategy, i.e., concatenation mode, considers intra-sentence and inter-sentence contexts with the same priority, despite an apparent difference between the two kinds of contexts. In this paper, we propose an alternative adaptation approach, named Decoding-enhanced Multi-phase Prompt Tuning (DeMPT), to make LLMs discriminately model and utilize the inter- and intra-sentence context and more effectively adapt LLMs to context-aware NMT. First, DeMPT divides the context-aware NMT process into three separate phases. During each phase, different continuous prompts are introduced to make LLMs discriminately model various information. Second, DeMPT employs a heuristic way to further discriminately enhance the utilization of the source-side inter- and intra-sentence information at the final decoding phase. Experiments show that our approach significantly outperforms the concatenation method, and further improves the performance of LLMs in discourse modeling",
    "checked": true,
    "id": "7b0fd65d2f77f6501fb9fd723c3bbe42b62d2276",
    "semantic_title": "dempt: decoding-enhanced multi-phase prompt tuning for making llms be better context-aware translators",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Wz2H5e6rvLy": {
    "title": "Multi-Aspect Controllable Text Generation with Disentangled Counterfactual Augmentation",
    "volume": "review",
    "abstract": "Multi-aspect controllable text generation aims to control the generated texts in attributes from multiple aspects (e.g., \"positive\" from sentiment and \"sport'' from topic). Existing works neglect attribute correlations formed by the intertwining of different attributes. Particularly, the stereotype formed by imbalanced attribute correlations significantly affects multi-aspect control. In this paper, we propose MAGIC, a new multi-aspect controllable text generation method with disentangled counterfactual augmentation. We alleviate the issue of imbalanced attribute correlations during training using counterfactual feature vectors in the attribute latent space by disentanglement. During inference, we enhance attribute correlations by target-guided counterfactual augmentation to further improve multi-aspect control. Experiments show that MAGIC outperforms state-of-the-art baselines in both imbalanced and balanced attribute correlation scenarios",
    "checked": true,
    "id": "f3530618e10bb20930b3a2782d8594922e5a27bd",
    "semantic_title": "multi-aspect controllable text generation with disentangled counterfactual augmentation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7BPA47jgoJv": {
    "title": "Retrieval-Augmented Multilingual Knowledge Editing",
    "volume": "review",
    "abstract": "Knowledge represented in Large Language Models (LLMs) is quite often incorrect and can also become obsolete over time. Updating knowledge via fine-tuning is computationally resource-hungry and not reliable, and so knowledge editing (KE) has developed as an effective and economical alternative to inject new knowledge or to fix factual errors in LLMs. Although there has been considerable interest in this area, current KE research exclusively focuses on monolingual settings, typically in English. However, what happens if the new knowledge is supplied in one language, but we would like to query an LLM in a different language? To address the problem of multilingual knowledge editing, we propose Retrieval-Augmented Multilingual Knowledge Editor (ReMaKE) to update knowledge in LLMs. ReMaKE can be used to perform model-agnostic knowledge editing in a multilingual setting. ReMaKE concatenates the new knowledge retrieved from a multilingual knowledge base with users' prompts before querying an LLM. Our experimental results show that ReMaKE outperforms baseline knowledge editing methods by a significant margin and is scalable to real-word application scenarios. Our multilingual knowledge editing dataset (MzsRE) in 12 languages, the code, and additional project information will be made available",
    "checked": true,
    "id": "5e2ce535acf3a2ce8fd36d0f97e36c7777f65504",
    "semantic_title": "retrieval-augmented multilingual knowledge editing",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=fxM13uraI1": {
    "title": "Two Heads Are Better Than One: Exploiting Both Sequence and Graph Models in AMR-To-Text Generation",
    "volume": "review",
    "abstract": "Abstract meaning representation (AMR) is a special semantic representation language that captures sentences' meaning with syntax-irrelevant graphs. AMR-to-text generation aims to generate text according to a given AMR graph and is helpful in various downstream NLP tasks. Existing AMR-to-text generation methods roughly fall into two categories, each with pros and cons. The sequence-to-sequence models, especially pretrained language models (PLMs), have good text generation ability but cannot cope well with the structural information of AMR graphs. The graph-to-sequence models utilize graph neural networks (GNNs), showcasing complementary strengths and limitations. Combining both methods could harness their strengths; yet, merging a GNN with a PLM is non-trivial. In this paper, we propose DualGen, a dual encoder-decoder model that integrates a specially designed GNN into a sequence-to-sequence PLM. We conduct extensive experiments, human evaluation, and a case study, finding that DualGen achieves the desired effect and yields state-of-the-art performance in the AMR-to-text generation task. We also show it outperforms the most potent general-purpose PLMs, LLaMA and GPT-4",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fTefxGHzhki": {
    "title": "GenDec: A robust generative Question-decomposition method for Multi-hop reasoning",
    "volume": "review",
    "abstract": "Multi-hop QA (MHQA) involves step-by-step reasoning to answer complex questions and find multiple relevant supporting facts. However, Existing large language models'(LLMs) reasoning ability in multi-hop question answering remains exploration, which is inadequate in answering multi-hop questions. Moreover, it is unclear whether LLMs follow a desired reasoning chain to reach the right final answer.In this paper, we propose a \\textbf{gen}erative question \\textbf{dec}omposition method (GenDec) from the perspective of explainable QA by generating independent and complete sub-questions based on incorporating additional extracted evidence for enhancing LLMs' reasoning ability in RAG.To demonstrate the impact, generalization, and robustness of Gendec, we conduct two experiments, the first is combining GenDec with small QA systems on paragraph retrieval and QA tasks. We secondly examine the reasoning capabilities of various state-of-the-art LLMs including GPT-4 and GPT-3.5 combined with GenDec.We experiment on the HotpotQA, 2WikihopMultiHopQA, MuSiQue, and PokeMQA datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xgRlonUe-I": {
    "title": "Large Language Model-based Human-Agent Collaboration for Complex Task Solving",
    "volume": "review",
    "abstract": "In recent developments within the research community, the integration of Large Language Models (LLMs) in creating fully autonomous agents has garnered significant interest. Despite this, LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs. In this work, we introduce the problem of LLM-based human-agent collaboration for complex task-solving, exploring their synergistic potential. In addition, we propose a \\underline{\\textbf{Re}}inforcement Learning-based \\underline{\\textbf{H}}uman-\\underline{\\textbf{A}}gent \\underline{\\textbf{C}}ollaboration method, \\underline{\\textbf{ReHAC}}. This approach includes a policy model designed to determine the most opportune stages for human intervention within the task-solving process. We construct a human-agent collaboration dataset to train this policy model in an offline reinforcement learning environment. Our validation tests confirm the model's effectiveness. The results demonstrate that the synergistic efforts of humans and LLM-based agents significantly improve performance in complex tasks, primarily through well-planned, limited human intervention. Datasets and code are available at: \\url{https://anonymous.4open.science/r/ReHAC}",
    "checked": true,
    "id": "a70f0f9b9b9dc7d5caadcb23a551ea4213727548",
    "semantic_title": "large language model-based human-agent collaboration for complex task solving",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=kEGi4gao41": {
    "title": "Prompt-Based Length Controlled Generation with Multiple Control Types",
    "volume": "review",
    "abstract": "Large language models (LLMs) have attracted great attention given their strong performance on a wide range of NLP tasks. In practice, users often expect generated texts to fall within a specific length range, making length controlled generation an important topic, especially for GPT-style models. Existing length control methods mostly focus on a simple control type of \"equal to\" a target length. Different from them, we propose a prompt-based method to achieve length controlled generation under different control types with high accuracy. In particular, we adopt reinforcement learning (RL) and sample filtering with the reward signal given by rule-based reward models, which enhances the length control ability of models by rewarding outputs that follow certain control instructions. In addition, we introduce a standard prompt extractor to parse arbitrary users' input into standard control instructions. Experiments show that our method significantly improves the accuracy of prompt-based length control on popular summarization datasets like CNNDM and NYT under multiple control types. Moreover, both the standard prompt extractor and RL-tuned model show strong generalization to unseen control prompt templates",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=av0W-nes_5": {
    "title": "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Generative LLM Inference",
    "volume": "review",
    "abstract": "The development of state-of-the-art generative large language models (LLMs) disproportionately relies on English-centric tokenizers, vocabulary and pre-training data. Despite the fact that some LLMs have multilingual capabilities, recent studies have shown that their inference efficiency deteriorates when generating text in languages other than English. This results in increased inference time and costs. Cross-lingual vocabulary adaptation methods have been proposed for adapting models to a target language aiming to improve downstream performance. However, the effectiveness of these methods on increasing inference efficiency of generative LLMs has yet to be explored. In this paper, we perform an empirical study of various cross-lingual vocabulary adaptation methods on five generative LLMs (including monolingual and multilingual models) across four typologically-diverse languages and four natural language understanding tasks. We find that cross-lingual vocabulary adaptation substantially contributes to LLM inference speedups of up to 271.5%. We also show that adapting LLMs that have been pre-trained on more balanced multilingual data results in downstream performance comparable to the original models",
    "checked": true,
    "id": "441d9c239ba5fee0be1ac122330052c7b6bf822e",
    "semantic_title": "an empirical study on cross-lingual vocabulary adaptation for efficient generative llm inference",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=vWT2RMjNgI": {
    "title": "Document-level Claim Extraction and Decontextualisation for Fact-Checking",
    "volume": "review",
    "abstract": "Selecting which claims to check is a time consuming task for human fact-checkers, especially from documents consisting of multiple sentences and containing multiple claims. However, existing claim extraction approaches focus more on identifying and extracting claims from individual sentences, e.g., identifying whether a sentence contains a claim or the exact boundaries of the claim within a sentence. In this paper, we propose a method for \\textit{document-level} claim extraction for fact-checking, which aims to extract check-worthy claims from documents and decontextualise them so that they can be understood out of context. Specifically, we first recast claim extraction as extractive summarization in order to identify central sentences from documents, then rewrite them to include necessary context from the originating document through sentence decontextualisation. Evaluation with both automatic metrics and a fact-checking professional shows that our method is able to extract check-worthy claims from documents at a higher rate than previous work, while also improving evidence retrieval",
    "checked": true,
    "id": "97ef1dcac9f0891c4a53fd4092279666976ce9dc",
    "semantic_title": "document-level claim extraction and decontextualisation for fact-checking",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V9x0onEda3": {
    "title": "TPEval: A Novel Truth-Preserving Evaluation Method for Probing LLMs Professional Factual Knowledge Mastery",
    "volume": "review",
    "abstract": "Using large language models (LLMs) to solve problems in professional fields (e.g., medicine) is emerging as a research hotspot, requiring LLMs to master sufficient domain-specific factual knowledge. Recently, several LLMs achieved notable performance on multiple professional-field evaluation benchmarks. However, current benchmarks generally leverage common and fixed question formulations, allowing LLMs to provide correct answers based on surface-level patterns in questions without mastering the underlying knowledge. In this paper, we focus on this problem. We propose a general \\textbf{t}ruth-\\textbf{p}reserving \\textbf{eval}uation framework (\\textbf{TPEval}) to precisely probe LLMs' mastery of factual knowledge in professional fields through distinct representations of the same knowledge. Specifically, for each piece of knowledge, we convert its original expression into multiple truth-preserving statements with logical transformations, presenting the knowledge in diverse ways. By leveraging these statements, the proposed framework can more precisely estimate LLMs' mastery of the specified knowledge. Given the wealth of factual knowledge in medicine, we validate the effectiveness of our framework in the medical domain. We curate 6,000+ clinical facts and generate eight statements for each fact using the proposed method, evaluating the mastery of LLMs. Experimental results indicate a notable decline in LLMs' performance as the number of statements per fact increases, suggesting insufficient knowledge mastery of LLMs. Our method can serve as an effective solution for probing LLMs' knowledge mastery in professional fields",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DxmJfaDBFT": {
    "title": "Bilingual Rhetorical Structure Parsing with Large Parallel Annotations",
    "volume": "review",
    "abstract": "Discourse parsing is a crucial task in natural language processing that aims to reveal the higher-level semantic relations in a text. Despite growing interest in cross-lingual discourse parsing, challenges persist due to limited parallel data and inconsistencies in the Rhetorical Structure Theory (RST) application across languages and corpora. To address this, we introduce a parallel Russian annotation for the large and diverse English GUM RST corpus. Leveraging recent advances, our end-to-end RST parser achieves state-of-the-art results on both English and Russian corpora. It demonstrates effectiveness in both monolingual and bilingual settings, successfully transferring even with limited second-language annotation. To the best of our knowledge, this work is the first to evaluate the potential of cross-lingual end-to-end RST parsing on a manually annotated parallel corpus",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ppdwpvvWxK": {
    "title": "Book2Dial: Generating Teacher Student Interactions from Textbooks for Cost-Effective Development of Educational Chatbots",
    "volume": "review",
    "abstract": "Educational chatbots are a promising tool for assisting student learning. However, the development of effective chatbots in education has been challenging, as high-quality data is seldom available in this domain. In this paper, we propose a framework for generating synthetic teacher-student interactions grounded in a set of textbooks. Our approaches capture a key aspect of learning interactions where curious students with partial knowledge interactively ask teachers questions about the material in the textbook. We highlight various quality criteria that such dialogues must fulfill and compare several approaches relying on either prompting or finetuning large language models according to these criteria. We use the synthetic dialogues to train educational chatbots and show the benefits of further fine-tuning in educational domains. However, careful human evaluation shows that our best data synthesis method still suffers from hallucinations and tends to reiterate information from previous conversations. Our findings offer insights for future efforts in synthesizing conversational data that strikes a balance between size and quality. We will open-source our data and code",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=07VjRLjDcS": {
    "title": "Piecing Together Clues: A Benchmark for Evaluating the Detective Skills of Large Language Models",
    "volume": "review",
    "abstract": "Detectives frequently engage in information detection and reasoning simultaneously when making decisions across various cases, especially when confronted with a vast amount of information. With the rapid development of large language models~(LLMs), evaluating how these models identify key information and reason to solve questions becomes increasingly relevant. We introduces the DetectBench, a reading comprehension dataset designed to assess a model's ability to jointly ability in key information detection and multi-hop reasoning when facing complex and implicit information. The DetectBench comprises 3,928 questions, each paired with a paragraph averaging 190 tokens in length. To enhance model's detective skills, we propose the Self-Question Framework. These methods encourage models to identify all possible clues within the context before reasoning. Our experiments reveal that existing models perform poorly in both information detection and multi-hop reasoning. However, the Self-Question Framework approach alleviates this issue",
    "checked": true,
    "id": "687c9a4956a7ba94e7f7d85951b18c7abdbaf883",
    "semantic_title": "piecing together clues: a benchmark for evaluating the detective skills of large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZMDcnWKeir": {
    "title": "Exploring Forgetting in LLM Pre-Training",
    "volume": "review",
    "abstract": "In large language model (LLM), the challenge of catastrophic forgetting remains a formidable obstacle to building an omniscient model. Despite the pioneer research on task-level forgetting in LLM fine-tuning, there is a dearth of studies focusing on finer-grained forgetting at the sample level during pre-training. This paper delves into the intricacies of forgetting during the pre-training phase, where models are typically trained on massive diverse corpus for only one epoch. We systematically explore the existence, essence, and measurement of forgetting in LLM pre-training. Specifically, we investigate the limitations of traditional metrics such as perplexity (ppl) in accurately measuring the forgetting in pre-training, and propose three new metrics evaluating LLM's ability of assessing related memories of entities, which is viewed as the key reflection of whether forgetting happens in pre-training. Extensive evaluations and insights on forgetting of pre-training facilitate future researches on LLMs",
    "checked": false,
    "id": "f352a10baaf1b23b020c95f9a64a808bce5342e2",
    "semantic_title": "examining forgetting in continual pre-training of aligned large language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=6A6jOXOhEC": {
    "title": "WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers. We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs. Specifically, we incorporates past-only quantization to improve the computation of attention. Additionally, we introduce two-dimensional quantization strategy to handle the distribution of KV cache, along with a cross-block reconstruction regularization for parameter optimization. Experiments show that WKVQuant achieves almost comparable memory savings to weight-activation quantization, while also approaching the performance of weight-only quantization",
    "checked": true,
    "id": "f608011b0f50a14bb2949c186a7c632a099aa75b",
    "semantic_title": "wkvquant: quantizing weight and key/value cache for large language models gains more",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=OgM5mBq1bt": {
    "title": "Retrieval-Augmented Multi-modal Chain-of-Thoughts Reasoning for Large Language Models",
    "volume": "review",
    "abstract": "The advancement of Large Language Models~(LLMs) has brought substantial attention to the Chain of Thought~(CoT) approach, primarily due to its ability to enhance the capability of LLMs on tasks requiring complex reasoning. Moreover, the significance of CoT approaches extends to the application of LLMs for multi-modal tasks. However, the selection of optimal CoT demonstration examples in multi-modal reasoning for LLMs remains less explored for LLMs due to the inherent complexity of multi-modal examples. In this paper, we introduce a novel approach that addresses this challenge by using retrieval mechanisms to dynamically and automatically select demonstration examples based on cross-modal and intra-modal similarities. Furthermore, we employ a stratified sampling method categorising demonstration examples into groups based on their types and retrieving examples from different groups respectively to promote the diversity of demonstration examples. Through a series of experiments on two popular benchmark datasets - ScienceQA and MathVista, we demonstrate that our approach significantly improves the performance of LLMs by more than 2.5\\%, achieving state-of-the-art results in multi-modal reasoning tasks",
    "checked": true,
    "id": "34802b1f153d436d5ddb428642b8ae415485269e",
    "semantic_title": "retrieval-augmented multi-modal chain-of-thoughts reasoning for large language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=_BkTCedtAb": {
    "title": "Don't Say No: Jailbreaking LLM by Suppressing Refusal",
    "volume": "review",
    "abstract": "Ensuring the safety alignment of Large Language Models (LLMs) is crucial to generating responses consistent with human values. Despite their ability to recognize and avoid harmful queries, LLMs are vulnerable to \"jailbreaking\" attacks, where carefully crafted prompts elicit them to produce toxic content. One category of jailbreak attacks is reformulating the task as adversarial attacks by eliciting the LLM to generate an affirmative response. However, the typical attack in this category GCG has very limited attack success rate. In this study, to better study the jailbreak attack, we introduce the \\textit{DSN} (Don't Say No) attack, which prompts LLMs to not only generate affirmative responses but also novelly enhance the objective to suppress refusals. In addition, another challenge lies in jailbreak attacks is the evaluation, as it is difficult to directly and accurately assess the harmfulness of the attack. The existing evaluation such as refusal keyword matching has its own limitation as it reveals numerous false positive and false negative instances. To overcome this challenge, we propose an ensemble evaluation pipeline incorporating Natural Language Inference (NLI) contradiction assessment, external LLM evaluation, as well as refusal matching. Extensive experiments demonstrate the potency of the \\textit{DSN} and the effectiveness of ensemble evaluation compared to baseline methods",
    "checked": true,
    "id": "bcb694c1fbf55ce884cbe89ff7c489f933586ef7",
    "semantic_title": "don't say no: jailbreaking llm by suppressing refusal",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=ClTvYbgLo9": {
    "title": "Deciphering the lmpact of Pretraining Data on Large Language Models through Machine Unlearning",
    "volume": "review",
    "abstract": "Through pretraining on a corpus with various sources, Large Language Models (LLMs) have gained impressive performance. However, the impact of each component of the pretraining corpus remains opaque. As a result, the organization of the pretraining corpus is still empirical and may deviate from the optimal. To address this issue, we systematically analyze the impact of 48 datasets from 5 major categories of pretraining data of LLMs and measure their impacts on LLMs using benchmarks about nine major categories of model capabilities. Our analyses provide empirical results about the contribution of multiple corpora on the performances of LLMs, along with their joint impact patterns, including complementary, orthogonal, and correlational relationships. We also identify a set of ``high-impact data'' such as Books that is significantly related to a set of model capabilities. These findings provide insights into the organization of data to support more efficient pretraining of LLMs",
    "checked": true,
    "id": "7ff1fb0eac5a72301e33ecf8344c4a03f430dc67",
    "semantic_title": "deciphering the lmpact of pretraining data on large language models through machine unlearning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Ao9CMORhoD": {
    "title": "WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing",
    "volume": "review",
    "abstract": "Knowledge editing aims to rectify inaccuracies in large language models (LLMs) without costly retraining for outdated or erroneous knowledge. However, current knowledge editing methods primarily focus on single editing, failing to meet the requirements for lifelong editing\\footnote{In this paper, lifelong editing is synonymous with lifelong knowledge editing.}. This study reveals a performance degradation encountered by knowledge editing in lifelong editing, characterized by toxicity buildup and toxicity flash, with the primary cause identified as pattern unmatch. We introduce a knowledge editing approach named WilKE, which selects editing layer based on the pattern matching degree of editing knowledge across different layers. Experimental results demonstrate that, in lifelong editing, WilKE exhibits an average improvement of 46.2\\% and 67.8\\% on editing GPT2-XL and GPT-J relative to state-of-the-art knowledge editing methods",
    "checked": true,
    "id": "dfbbc9373a575fb3031cd195c7aa616e183f0b60",
    "semantic_title": "wilke: wise-layer knowledge editor for lifelong knowledge editing",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=i4a9-TY70A": {
    "title": "CODIS: Benchmarking Context-dependent Visual Comprehension for Multimodal Large Language Models",
    "volume": "review",
    "abstract": "Multimodal large language models (MLLMs) have demonstrated promising results in a variety of tasks that combine vision and language. As these models become more integral to research and applications, conducting comprehensive evaluations of their capabilities has grown increasingly important. However, most existing benchmarks fail to consider that, in certain situations, images need to be interpreted within a broader context. In this work, we introduce a new benchmark, named as CODIS, designed to assess the ability of models to use context provided in free-form text to enhance visual comprehension. Our findings indicate that MLLMs consistently fall short of human performance on this benchmark. Further analysis confirms that these models struggle to effectively extract and utilize contextual information to improve their understanding of images. This underscores the pressing need to enhance the ability of MLLMs to comprehend visuals in a context-dependent manner",
    "checked": true,
    "id": "2081bd89dd425e8a4ed0f6a424a02c4e5eb57ef6",
    "semantic_title": "codis: benchmarking context-dependent visual comprehension for multimodal large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jCrBtcQshE": {
    "title": "Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural Knowledge",
    "volume": "review",
    "abstract": "Recent studies have highlighted the presence of cultural biases in Large Language Models (LLMs), yet often lack a robust methodology to dissect these phenomena comprehensively. Our work aims to bridge this gap by delving into the \\textsc{Food} domainâ€”a universally relevant yet culturally diverse aspect of human life. We introduce \\textsc{FmLAMA}, a multilingual dataset centered on food-related cultural facts and variations in food practices. We analyze LLMs across various architectures and configurations, evaluating their performance in both monolingual and multilingual settings. By leveraging templates in six different languages, we investigate how LLMs interact with language-specific and cultural knowledge. Our findings reveal that (1) LLMs demonstrate a pronounced bias towards food knowledge prevalent in the United States; (2) Incorporating relevant cultural context significantly improves LLMs' ability to access cultural knowledge; (3) The efficacy of LLMs in capturing cultural nuances is highly dependent on the interplay between the probing language, the specific model architecture, and the cultural context in question. This research underscores the complexity of integrating cultural understanding into LLMs and emphasizes the importance of culturally diverse datasets to mitigate biases and enhance model performance across different cultural domains",
    "checked": true,
    "id": "e8fc7d4ab83dc28ff4628bbc62188c3438e5ea41",
    "semantic_title": "does mapo tofu contain coffee? probing llms for food-related cultural knowledge",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=9F3Gez_jKO8": {
    "title": "Speaking in Wavelet Domain: A Simple and Efficient Approach to Speed up Speech Diffusion Model",
    "volume": "review",
    "abstract": "Recently, Denoising Diffusion Probabilistic Models (DDPMs) have attained leading performances across a diverse range of generative tasks. However, in the field of speech synthesis, although DDPMs exhibit impressive performance, their prolonged training duration and substantial inference costs hinder practical deployment. Existing approaches primarily focus on enhancing inference speed, while approaches to accelerate trainingâ€”a key factor in the costs associated with adding or customizing voicesâ€”often necessitate complex modifications to the model, compromising their universal applicability. To address the aforementioned challenges, we propose an inquiry: is it possible to enhance the training/inference speed and performance of DDPMs by modifying the speech signal itself? In this paper, we double the training and inference speed of Speech DDPMs by simply redirecting the generative target to the wavelet domain. This method not only achieves comparable or superior performance to the original model in speech synthesis tasks but also demonstrates its versatility. By investigating and utilizing different wavelet bases, our approach proves effective not just in speech synthesis, but also in speech enhancement",
    "checked": true,
    "id": "510278b81907bcc0f247eda5f69e99fbf8b3ff82",
    "semantic_title": "speaking in wavelet domain: a simple and efficient approach to speed up speech diffusion model",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nAv_JuDRqeT": {
    "title": "Counter-intuitive: Large Language Models Can Better Understand Knowledge Graphs Than We Thought",
    "volume": "review",
    "abstract": "Although the method of enhancing large language models' (LLMs') reasoning ability and reducing their hallucinations through the use of knowledge graphs (KGs) has received widespread attention, the exploration of how to enable LLMs to integrate the structured knowledge in KGs on-the-fly remains inadequate. Researchers often co-train KG embeddings and LLM parameters to equip LLMs with the ability of comprehending KG knowledge. However, this resource-hungry training paradigm significantly increases the model learning cost and is also unsuitable for non-open-source, black-box LLMs. In this paper, we employ complex question answering (CQA) as a task to assess the LLM's ability of comprehending KG knowledge. We conducted a comprehensive comparison of KG knowledge injection methods (from triples to natural language text), aiming to explore the optimal prompting method for supplying KG knowledge to LLMs, thereby enhancing their comprehension of KG. Contrary to our initial expectations, our analysis revealed that LLMs effectively handle messy, noisy, and linearized KG knowledge, outperforming methods that employ well-designed natural language (NL) textual prompts. This counter-intuitive finding provides substantial insights for future research on LLMs' comprehension of structured knowledge",
    "checked": false,
    "id": "5ef8b2b65f0f2b9b216c29164c8246e363c693ce",
    "semantic_title": "large language models can better understand knowledge graphs than we thought",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gD4XDRbmMg": {
    "title": "FedMCP: Parameter-Efficient Federated Learning with Model-Contrastive Personalization",
    "volume": "review",
    "abstract": "Given the growing concerns over data privacy and security, fine-tuning pre-trained language models (PLMs) in federated learning (FL) has become the standard practice. However, this process faces two primary challenges. Firstly, the utilization of large-scale PLMs introduces excessive communication overheads. Secondly, the data heterogeneity across FL clients presents a major obstacle in achieving the desired fine-tuning performance. To address these challenges, we present a parameter-efficient fine-tuning (PEFT) method with Model-Contrastive Personalization (FedMCP). This approach introduces two adapter modules to the frozen PLM and only aggregates the global adapter in the federated aggregate phase while the private adapter stays in clients. The model-contrastive regularization term and aggregation strategy encourage the global adapter to learn universal knowledge from all clients and the private adapter to capture idiosyncratic knowledge for each individual client. Verified across a highly heterogeneous cross-silo dataset, the empirical evaluation shows considerable performance improvement achieved by FedMCP over state-of-the-art approaches",
    "checked": false,
    "id": "2510bd1b8bc09d7b8f08cf888297753c97c5ef44",
    "semantic_title": "federated learning from pre-trained models: a contrastive learning approach",
    "citation_count": 100,
    "authors": []
  },
  "https://openreview.net/forum?id=P1Ue9SV-BS": {
    "title": "Growing Trees on Sounds: Assessing Strategies for End-to-End Dependency Parsing of Speech",
    "volume": "review",
    "abstract": "Direct dependency parsing of the speech signal --as opposed to parsing speech transcriptions-- has recently been proposed as a task $\\citep{pupier22_interspeech}$, as a way of incorporating prosodic information in the parsing system and bypassing the limitations of a pipeline approach that would consist of using first an Automatic Speech Recognition (ASR) system and then a syntactic parser.In this article, we report on a set of experiments aiming at assessing the performance of two parsing paradigms (graph-based parsing and sequence labeling based parsing) on speech parsing.We perform this evaluation on a large treebank of spoken French, featuringrealistic spontaneous conversations.Our findings show that (i) the graph based approach obtain better results across the board (ii) parsing directly from speech outperforms a pipeline approach, despite having 30% fewer parameters",
    "checked": true,
    "id": "444ea5a28ccce4e02a13de901064b2d4f9a21913",
    "semantic_title": "growing trees on sounds: assessing strategies for end-to-end dependency parsing of speech",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AN98oq93vY4": {
    "title": "X-ACE: Explainable and Multi-factor Audio Captioning Evaluation",
    "volume": "review",
    "abstract": "Automated audio captioning (AAC) aims to generate descriptions based on audio input, attracting exploration of emerging audio language models (ALMs). However, current evaluation metrics only provide a single score to assess the overall quality of captions without characterizing the nuanced difference by systematically going through an evaluation checklist. To this end, we propose the explainable and multi-factor audio captioning evaluation (X-ACE) paradigm. X-ACE identifies four main factors that constitute the majority of audio features, specifically sound event, source, attribute and relation. To assess a given caption from an ALM, it is firstly transformed into an audio graph, where each node denotes an entity in the caption and corresponds to a factor. On the one hand, graph matching is conducted from part to whole for a holistic assessment. On the other hand, the nodes contained within each factor are aggregated to measure the factor-level performance. The pros and cons of an ALM can be explicitly and clearly demonstrated through X-ACE, pointing out the direction for further improvements. Experiments show that X-ACE exhibits better correlation with human perception and can detect mismatches sensitively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c8690RuigbE": {
    "title": "End-to-end Learning of Logical Rules for Enhancing Document-level Relation Extraction",
    "volume": "review",
    "abstract": "Document-level relation extraction (DocRE) aims to extract relations between entities in a whole document. One of the pivotal challenges of DocRE is to capture the intricate interdependencies between relations of entity pairs. Previous methods have shown that logical rules are able to explicitly help capture such interdependencies. These methods either learn logical rules to refine the output of a trained DocRE model, or first learn logical rules from annotated data and then inject the learnt rules to a DocRE model using auxiliary training objective. In this paper, we argue that these learning pipelines may suffer from the issue of error propagation. To mitigate this issue, we propose \\emph{Joint Modeling Relation extraction and Logical rules} or \\emph{JMRL} for short, a novel rule-based framework that jointly learns both a DocRE model and logical rules in an end-to-end fashion. Specifically, we parameterize a rule reasoning module in JMRL to simulate the inference of logical rules, thereby explicitly modeling the reasoning process. We also introduce an auxiliary loss and a residual connection mechanism in JMRL to better reconcile the DocRE model and the rule reasoning module. Experimental results on \\textcolor{red}{four} benchmark datasets demonstrate that the proposed JMRL framework is consistently superior to existing rule-based frameworks on all datasets, improving five baseline models for DocRE by a significant margin",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rqAvGqiIngz": {
    "title": "Ask One More Time: Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios",
    "volume": "review",
    "abstract": "Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local optimality. To address this shortcoming, ensemble-optimization tries to obtain multiple reasoning paths to get the final answer assembly. However, current ensemble-optimization methods either simply employ rule-based post-processing such as self-consistency, or train an additional model based on several task-related human annotations to select the best one among multiple reasoning paths, yet fail to generalize to realistic settings where the type of input questions is unknown or the answer format of reasoning paths is unknown. To avoid their limitations, we propose Self-Agreement, a generalizable ensemble-optimization method applying in almost all scenarios where the type of input questions and the answer format of reasoning paths may be known or unknown. Self-agreement firstly samples from language model's decoder to generate a diverse set of reasoning paths, and subsequently prompts the language model one more time to determine the optimal answer by selecting the most agreed answer among the sampled reasoning paths. Self-agreement simultaneously achieves remarkable performance on six public reasoning benchmarks and superior generalization capabilities",
    "checked": false,
    "id": "63e0abd9fe80aa362e880a030d147f66edc5db92",
    "semantic_title": "just ask one more time! self-agreement improves reasoning of language models in (almost) all scenarios",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=p3IDS0zMyXN": {
    "title": "Sibyl: Sensible Empathetic Dialogue Generation with Visionary Commonsense Knowledge",
    "volume": "review",
    "abstract": "Recently, there has been a heightened interest in building chatbots based on Large Language Models (LLMs) to emulate human-like qualities in dialogues, including expressing empathy and offering emotional support. Despite having access to commonsense knowledge to better understand the psychological aspects and causality of dialogue context, even these powerful LLMs struggle to achieve the goals of empathy and emotional support. As current approaches do not adequately anticipate dialogue future, they may mislead language models to ignore complex dialogue goals of empathy and emotional support, resulting in unsupportive responses lacking empathy.To address this issue, we present an innovative framework named \\underline{S}ens\\underline{ib}le Empathetic Dialogue Generation with zhenVisionar\\underline{y} Commonsense Know\\underline{l}edge (\\textit{Sibyl}). Designed to concentrate on the imminent dialogue future, this paradigm directs LLMs toward the implicit requirements of the conversation, aiming to provide more sensible responses. Experimental results demonstrate that incorporating our paradigm for acquiring commonsense knowledge into LLMs comprehensively enhances the quality of their responses",
    "checked": true,
    "id": "2f790a5215e486a083310632332c955bdc1e89e5",
    "semantic_title": "sibyl: sensible empathetic dialogue generation with visionary commonsense knowledge",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=N4g9UlbmhMO": {
    "title": "A Large Scale Synthetic Dataset for MULTImodaL hATE \"MULTILATE\" with Text and Images and Adversarial Samples",
    "volume": "review",
    "abstract": "Nowadays, one of the main problems our society struggles with is fighting online hate. In other words, as social media explodes with multimodal hate speech content, we require scalable multimodal hate speech detection sys- tems. Thus, we present MULTILATE, a MULTImodaL hATE 2.6 million sample dataset for cross-modal hate speech classification and additional ex- planation through 3W Question Answering. Key features of the dataset include (1) textual utterances, (2) synthesized pictures produced by Stable Diffusion, (3) pixel-level temperature maps meant for explaining a picture- text interface, (4) question-answer triples addressing \"who\", \"what\", and \"why\" components of the statement,(5) Adversarial examples of both text and images. MULTILATE is aimed at creating and assessing interpretable multimodal hate speech classifiers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=514YdrN_2Vh": {
    "title": "DPPM: Data Pyramid Pre-trained Model for Few-Shot Text Summarization",
    "volume": "review",
    "abstract": "Text summarization models are often trained with standard datasets like CNN/DailyMail and BBC XSum. However, recent studies reveal poor annotation qualities of these datasets. Fine-tuning and evaluating models on such data yield conflicting conclusions between N-gram metrics and human evaluation. On the other hand, annotating a large number of high-quality data is infeasible due to the time-consuming property. To address this challenge, we propose a novel Data Pyramid Pre-trained Model (DPPM) for few-shot text summarization. Data Pyramid includes three parts: extractive data, abstractive data, and user-specific data. The first two types of data serve as the foundation, comprising the majority of the training data, thereby bolstering the DPPM's ability to generate domain-general summaries, and the user-specific data can align our model with human preferences. Experimental results on re-annotated CNN/DailyMail and BBC XSum datasets show that our DPPM (based on BART-Large) surpasses the performance of 175B GPT-3 and achieves the single-model SOTA",
    "checked": false,
    "id": "546a91d0957a2c34e083a9d9c3d8784aeac980ae",
    "semantic_title": "psp: pre-trained soft prompts for few-shot abstractive summarization",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=xWMsP9iicOS": {
    "title": "UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation",
    "volume": "review",
    "abstract": "Large language models (LLMs) produce hallucinated text, compromising their practical utility in professional contexts. To assess the reliability of LLMs, numerous initiatives have developed benchmark evaluations for hallucination phenomena. However, they often employ constrained generation techniques to produce the evaluation dataset due to cost and time limitations. For instance, this may involve employing directed hallucination induction or deliberately modifying authentic text to generate hallucinations. These are not congruent with the unrestricted text generation demanded by real-world applications. Furthermore, a well-established Chinese-language dataset dedicated to the evaluation of hallucinations is presently lacking. Consequently, we have developed an Unconstrained Hallucination Generation Evaluation (UHGEval) benchmark, containing hallucinations generated by LLMs with minimal restrictions. Concurrently, we have established a comprehensive benchmark evaluation framework to aid subsequent researchers in undertaking scalable and reproducible experiments. We have also evaluated prominent Chinese LLMs and the GPT series models to derive insights regarding hallucination",
    "checked": true,
    "id": "1146d40d3d01427a008a20530269667b8989750c",
    "semantic_title": "uhgeval: benchmarking the hallucination of chinese large language models via unconstrained generation",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=z23vkc2ifyT": {
    "title": "ETA: Enriching Typos Automatically from Real-World Corpora for Few-Shot Learning",
    "volume": "review",
    "abstract": "Spell checking is the task of rectifying errors in a sentence resulting from various factors, and despite continuous research in this field, research often focused on widely known specific languages. In this study, we focus on the Korean language and its linguistic characteristics, particularly the propensity for a single character can be incorrect in diverse ways. Therefore, we categorize spelling errors from real-world corpora and automatically construct an error corpus based on their statistical patterns. When we employed them to leverage the impact of a pre-trained large language model (LLM), we confirm that utilizing the introduced spelling errors as samples for few-shot learning can be helpful in error correction tasks. We hope that this study contributes to the automatic construction of error corpora and prompt-based approaches for other low-resource languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3f9Ex4wosVX": {
    "title": "Achilles-Bench: A Challenging Benchmark for Low-Resource Evaluation",
    "volume": "review",
    "abstract": "With promising yet saturated results in high-resource settings, low-resource datasets have gradually become crucial benchmarks (e.g., BigBench Hard, superGLUE) for evaluating the learning ability of advanced neural networks. In this work, we find that there exists a set of ``hard examples'' in low-resource settings that challenge neural networks but are not well evaluated, which causes over-estimated performance. We first give a theoretical analysis on which factors bring the difficulty of low-resource learning. It then motivates us to propose a challenging benchmark Achilles-Bench to better evaluate the learning ability, which covers 11 datasets, including 8 natural language process (NLP) datasets and 3 computer vision (CV) datasets. Experiments on a wide range of models show that neural networks, even pre-trained language models, have sharp performance drops on our benchmark, demonstrating the effectiveness of evaluating the weaknesses of neural networks. On NLP tasks, we surprisingly find that despite better results on traditional low-resource benchmarks, pre-trained networks, does not show performance improvements on our benchmarks. there is still a large robustness gap between existing models and human-level performance, highlighting the need for robust low-resource learning models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5CDOyW9N-7K": {
    "title": "Large Language Models Need Consultants for Reasoning: Becoming an Expert in a Complex Human System Through Behavior Simulation",
    "volume": "review",
    "abstract": "Large language models (LLMs), in conjunction with various reasoning reinforcement methodologies, have demonstrated remarkable capabilities comparable to humans in fields such as mathematics, law, coding, common sense, and world knowledge. In this paper, we delve into the reasoning abilities of LLMs within complex human systems. We propose a novel reasoning framework, termed \"Mosaic Expert Observation Wall\" (MEOW) exploiting generative-agents-based simulation technique. In the MEOW framework, simulated data are utilized to train an expert model concentrating ``experience'' about a specific task in each independent time of simulation. It is the accumulated ``experience'' through simulation that makes for an expert on a task in a complex human system. We conduct experiments within a communication game that mirrors real-world security scenarios. The results indicate that our proposed methodology can cooperate with existing methodologies to enhance the reasoning abilities of LLMs in complex human systems",
    "checked": true,
    "id": "dd40699c4a42cf5f61087cd2b068442750375b1b",
    "semantic_title": "large language models need consultants for reasoning: becoming an expert in a complex human system through behavior simulation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=HnjrmpSrvw": {
    "title": "Evaluating Cross-lingual Consistency of Factual Knowledge in Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs), exemplified by the likes of ChatGPT, have marked significant strides in the field of Natural Language Processing, earning widespread acclaim for their multitasking prowess. However, as the demand for cross-lingual applications escalates, the issue of response consistency in different linguistic contexts within LLMs becomes increasingly apparent, particularly in terms of knowledge-based queries. This study is committed to a profound evaluation of cross-lingual consistency in the knowledge embedded within LLMs. Existing research on knowledge-based cross-lingual consistency is notably scarce and suffers from conspicuous limitations. To address these shortcomings, we have constructed a factual knowledge dataset based on Wikidata, spanning five domains and twelve languages. Furthermore, we propose a novel set of metrics for evaluating cross-lingual consistency of knowledge, incorporating cross-lingual semantic consistency, cross-lingual accuracy consistency, and cross-lingual timeliness consistency. Leveraging this newly constructed dataset and evaluation metrics, we have undertaken a comprehensive evaluation and analysis of six representative open-source and closed-source models. The source code will be made publicly available for further research",
    "checked": false,
    "id": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
    "semantic_title": "cross-lingual consistency of factual knowledge in multilingual language models",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=ZLJgAtMnnj": {
    "title": "SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents",
    "volume": "review",
    "abstract": "Graphical User Interface (GUI) agents are designed to automate complex tasks on digital devices, such as smartphones and desktops. Most existing GUI agents interact with the environment through extracted structured data, which can be notably lengthy (e.g., HTML) and occasionally inaccessible (e.g., on desktops). To alleviate this issue, we propose a novel visual GUI agent -- SeeClick, which only relies on screenshots for task automation. In our preliminary study, we have discovered a key challenge in developing visual GUI agents: GUI grounding -- the capacity to accurately locate screen elements based on instructions. To tackle this challenge, we propose to enhance SeeClick with GUI grounding pre-training and devise a method to automate the curation of GUI grounding data. Along with the efforts above, we have also created ScreenSpot, the first realistic GUI grounding benchmark that encompasses mobile, desktop, and web environments. After pre-training, SeeClick demonstrates significant improvement in ScreenSpot over various baselines. Moreover, comprehensive evaluations on three widely used benchmarks consistently support our finding that advancements in GUI grounding directly correlate with enhanced performance in downstream GUI agent tasks. The model, data and code will be open-sourced",
    "checked": true,
    "id": "f9b39a6a7e40986b46f7796f3a805d70d7e3931a",
    "semantic_title": "seeclick: harnessing gui grounding for advanced visual gui agents",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=M_qFbtRigzS": {
    "title": "Improving Molecular Property Prediction via Topology-Enhanced Chemical Language Model",
    "volume": "review",
    "abstract": "Pre-trained chemical language models (CLMs) excel in the field of molecular property predictions, utilizing string-based molecular descriptors such as SMILES for learning universal representations. However, the one-dimensional format of SMILES can impede the effectiveness of the model because it lacks the topological information necessary for accurate property predictions. In this work, we introduce HINT, a novel framework to enhance the understanding of molecular structures within CLMs with topological fingerprints. HINT enhances molecular representations of CLMs through a molecular substructure prediction task and fingerprint-based contrastive learning. Experimental results on various tasks verify that HINT significantly improves the molecular property prediction performance of CLMs",
    "checked": false,
    "id": "54403b68e2203fff54af9d23eaf49b72a278db02",
    "semantic_title": "improving molecular properties prediction through latent space fusion",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tJeQfskV9j8": {
    "title": "Self-Modifying State Modeling for Simultaneous Machine Translation",
    "volume": "review",
    "abstract": "Simultaneous Machine Translation (SiMT) generates target outputs while receiving stream source inputs and requires a read/write policy to decide whether to wait for the next source token or generate a new target token, whose decisions form a \\textit{decision path}. Existing SiMT methods, which learn the policy by exploring various decision paths in training, face inherent limitations. These methods not only fail to precisely optimize the policy due to the inability to accurately assess the individual impact of each decision on SiMT performance, but also cannot sufficiently explore all potential paths because of their vast number. Besides, building decision paths requires unidirectional encoders to simulate streaming source inputs, which impairs the translation quality of SiMT models. To solve these issues, we propose \\textbf{S}elf-\\textbf{M}odifying \\textbf{S}tate \\textbf{M}odeling (SM$^2$), a novel training paradigm for SiMT task. Without building decision paths, SM$^2$ individually optimizes decisions at each state during training. To precisely optimize the policy, SM$^2$ introduces Self-Modifying process to independently assess and adjust decisions at each state. For sufficient exploration, SM$^2$ proposes Prefix Sampling to efficiently traverse all potential states. Moreover, SM$^2$ ensures compatibility with bidirectional encoders, thus achieving higher translation quality. Experiments show that SM$^2$ outperforms strong baselines. Furthermore, SM$^2$ allows offline machine translation models to acquire SiMT ability with fine-tuning",
    "checked": true,
    "id": "7cad4e7d1a43052a98680be0236d19e66631610d",
    "semantic_title": "self-modifying state modeling for simultaneous machine translation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=clAbfsg8tPQ": {
    "title": "Improving Language Understanding from Screenshots",
    "volume": "review",
    "abstract": "An emerging family of language models (LMs), capable of processing both text and images within a single visual view, has the promise to unlock complex tasks such as chart understanding and UI navigation. We name them as screenshot language models. Despite their appeal, existing screenshot LMs substantially lag behind text-only models on language understanding. To close this gap, we focus on a simplified setting where screenshots are rendered from plain text. We propose a novel Patch-and-Text Prediction (PTP) objective where we mask and recover both image patches of screenshots and text within screenshots. We also conduct careful ablation studies in masking rates, patch sizes, and designs for training stability. Our pre-trained model, while solely taking visual inputs, achieves comparable performance with BERT (within 2%) on 6 out of 8 GLUE tasks and improves up to 8% on specific datasets over prior work. Additionally, we extend PTP to train autoregressive screenshot LMs and demonstrate its effectiveness---our models can significantly reduce perplexity by utilizing the screenshot context. Together, we hope our findings can inspire future research on developing powerful screenshot LMs, and extending their reach to broader applications",
    "checked": true,
    "id": "59f9b5c6167ff40327cc1abc75aa22711872c545",
    "semantic_title": "improving language understanding from screenshots",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=1q7NZ6VXlQ2": {
    "title": "DocEE-zh: A Fine-grained Benchmark for Chinese Document-level Event Extraction",
    "volume": "review",
    "abstract": "Event extraction aims to identify events and then extract the arguments involved in those events. In recent years, there has been a gradual shift from sentence-level event extraction to document-level event extraction research. Despite the significant success achieved in English domain event extraction research, event extraction in Chinese still remains largely unexplored. However, a major obstacle to promoting Chinese document-level event extraction is the lack of fine-grained, wide domain coverage datasets for model training and evaluation. In this paper, we propose DocEE-zh, a new chinese document-level event extraction dataset comprising over 36,000 events and more than 210,000 arguments. We highlight two features: focus on high-interest event types and fine-grained argument types. Experimental results indicate that state-of-the-art models still fail to achieve satisfactory performance (F1 score of 68%), revealing that Chinese DocEE remains an unresolved challenge",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m3K0Ot_l_01": {
    "title": "Expanding Horizons or Hitting Walls? Limits and Potentials of LLMs in Augmenting Lexical Knowledge Bases",
    "volume": "review",
    "abstract": "This paper investigates the potential of Large Language Models (LLMs) to augment lexical knowledge bases (KBs) and to address their common limitations, such as static nature, limited coverage, and labor-intensive creation and maintenance. We propose a methodology that leverages LLMs to accurately reconstruct information from a source KB and generate new knowledge. Then, we evaluate this methodology using various LLMs and prompting techniques across three separate KBs. The results suggest that LLMs can accurately provide information when given ample contextual cues and when dealing with high-specificity concepts. However, they are prone to errors and inconsistencies when asked for rare or generic knowledge. The findings also indicate that LLMs can contribute to KB management by reducing the need for manual intervention. This study highlights the potential and limitations of LLMs in lexical semantics and emphasizes the importance of novel approaches to KB creation, maintenance, and integration",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SOyF4HlpKC7": {
    "title": "Combating Label Sparsity in Short Text Topic Modeling via Nearest Neighbor Augmentation",
    "volume": "review",
    "abstract": "Extracting semantic topics from short texts presents a significant challenge in the field of data mining. While efforts have been made to mitigate data sparsity issue, the limited length of short documents also results in the absence of semantically relevant words, causing biased evidence lower bound and incomplete labels for likelihood maximization. We refer to this issue as the label sparsity problem. To combat this problem, we propose kNNTM, a neural short text topic model that incorporates a $k$-Nearest-Neighbor-based label completion algorithm by augmenting the reconstruction label with $k$ nearest documents to complement these relevant but unobserved words. Furthermore, seeking a precise reflection of distances between documents, we propose a fused multi-view distances metric that takes both local word similarities and global topic semantics into consideration. Extensive experiments on multiple public short-text datasets show that kNNTM model outperforms the state-of-the-art baseline models and can derive both high-quality topics and document representations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZmhvgOdqxO_": {
    "title": "SuperPos-Prompt: Enhancing Soft Prompt Tuning of Language Models with Superposition of Multi Token Embeddings",
    "volume": "review",
    "abstract": "Soft prompt tuning techniques have recently gained traction as an effective strategy for the parameter-efficient tuning of pretrained language models, particularly minimizing the required adjustment of model parameters. Despite their growing use, achieving optimal tuning with soft prompts, especially with smaller datasets, remains a substantial challenge. This study makes two contributions in this domain: (i) we introduce \\textsc{SuperPos-Prompt}, a new reparameterization technique employing the superposition of multiple pretrained vocabulary embeddings to improve the learning of soft prompts. Our experiments across several GLUE and SuperGLUE benchmarks consistently highlight \\textsc{SuperPos-Prompt}'s superiority over \\textit{Residual Prompt} tuning, exhibiting an average score increase of $+6.4$ in \\textit{T5-Small} and $+5.0$ in \\textit{T5-Base} along with a faster convergence. Remarkably, \\textsc{SuperPos-Prompt} occasionally outperforms even full fine-tuning methods. (ii) Additionally, we demonstrate enhanced performance and rapid convergence by omitting dropout from the frozen network, yielding consistent improvements across various scenarios and tuning methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q-qlLV8BlWfB": {
    "title": "When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have been found to have difficulty knowing they do not possess certain knowledge and tend to provide specious answers in such cases. Retrieval Augmentation (RA) has been extensively studied to mitigate LLMs' hallucinations. However, due to the extra overhead and unassured quality of retrieval, it may not be optimal to conduct RA all the time. A straightforward idea is to only conduct retrieval when LLMs are uncertain about a question. This motivates us to enhance the LLMs' ability to perceive their knowledge boundaries to help RA. In this paper, we first quantitatively measure LLMs' such ability and confirm their overconfidence. Then, we study how LLMs' certainty about a question correlates with their dependence on external retrieved information. We propose several methods to enhance LLMs' perception of knowledge boundaries and show that they are effective in reducing overconfidence. Additionally, equipped with these methods, LLMs can achieve comparable or even better performance of RA with much fewer retrieval calls",
    "checked": true,
    "id": "138dcb9ff90f5e710d256bef5dedb28cdae9b764",
    "semantic_title": "when do llms need retrieval augmentation? mitigating llms' overconfidence helps retrieval augmentation",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=oR6eW2PaVkp": {
    "title": "Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various applications, fundamentally reshaping the landscape of natural language processing (NLP) research. However, recent evaluation frameworks often rely on the output probabilities of LLMs for predictions, primarily due to computational constraints, diverging from real-world LLM usage scenarios. While widely employed, the efficacy of these probability-based evaluation strategies remains an open research question. This study aims to scrutinize the validity of such probability-based evaluation methods within the context of using LLMs for Multiple Choice Questions~(MCQs), highlighting their inherent limitations. Our empirical investigation reveals that the prevalent probability-based evaluation method inadequately aligns with generation-based prediction. Furthermore, current evaluation frameworks typically assess LLMs through predictive tasks based on output probabilities rather than directly generating responses, owing to computational limitations. We illustrate that these probability-based approaches do not effectively correspond with generative predictions. The outcomes of our study can enhance the understanding of LLM evaluation methodologies and provide insights for future research in this domain",
    "checked": true,
    "id": "c290633698501ea83144d61d001eb7ac7a42d853",
    "semantic_title": "beyond probabilities: unveiling the misalignment in evaluating large language models",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=8Fy91lvkwsW": {
    "title": "DebugBench: Evaluating Debugging Capability of Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and three open-source models in a zero-shot scenario. We find that (1) while closed-source models like GPT-4 exhibit inferior debugging performance compared to humans, open-source models such as Code Llama fail to attain any pass rate scores; (2) the complexity of debugging notably fluctuates depending on the bug category; (3) incorporating runtime feedback has a clear impact on debugging performance which is not always helpful. As an extension, we also compare LLM debugging and code generation, revealing a strong correlation between them for closed-source models. These findings will benefit the development of LLMs in debugging",
    "checked": true,
    "id": "bb39d2b273d4c75f8e489f1f66dec42a7e14c5fd",
    "semantic_title": "debugbench: evaluating debugging capability of large language models",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=4Qd1gdEmoTq": {
    "title": "VisDiaHalBench: A Visual Dialogue Benchmark For Diagnosing Hallucination in Large vision-Language Models",
    "volume": "review",
    "abstract": "Despite the significant success of large vision-language models(LVLMs), some studies have revealed that LVLMs suffer from the hallucination problem, where the LVLMs' response contains descriptions of non-exits objects. Although various benchmarks have been proposed to investigate this problem, they mostly focus on single-turn evaluation and overlook the hallucination raised by textual inputs. To examine the combined effects of textual and visual inputs, we propose a novel visual dialogue hallucination evaluation benchmark VisDiaHalBench. The benchmark consists of samples with five-turn questions about an edited image and its original version. VisDiaHalBench differs from previous hallucination benchmarks in the following three points: 1) The questions and answers are unambiguously grounded by annotated scene graphs. 2) The images are uncommonly edited to inspect the visual model and common-object hallucination in LLMs. 3) The carefully designed dialogue refers a same object in different turns to assess the image consistency and influence of history for LVLMs. The detailed analysis of several state-of-the-art LVLMs across image consistency, visual understanding, history influence, and other dimensions reveals their substantial performance gap with single-turn VQA tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0HP_VjOS6tm": {
    "title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving",
    "volume": "review",
    "abstract": "Most existing chain-of-thought (CoT) prompting methods suffer from the issues of generalizability and consistency, as they often rely on instance-specific solutions that may not be applicable to other cases and lack task-level consistency in their reasoning steps. To address these limitations, we propose a comprehensive framework, StrategyLLM, harnessing the capabilities of LLMs to construct generalizable and consistent few-shot prompts for various tasks automatically. To this end, StrategyLLM employs four LLM-based agents: strategy generator, executor, optimizer, and evaluator, working together to generate, evaluate, and select promising strategies for a given task.The experimental results demonstrate that StrategyLLM outperforms the competitive baseline CoT-SC that requires human-annotated solutions on 13 datasets across 4 challenging tasks without human involvement, including math reasoning (34.21% $\\rightarrow$ 38.79%), commonsense reasoning (70.3% $\\rightarrow$ 72.5%), algorithmic reasoning (51.7% $\\rightarrow$ 62.0%), and symbolic reasoning (30.0% $\\rightarrow$ 79.2%)",
    "checked": true,
    "id": "a1cbe3e24cdcb49541ef5da4f82d9ebc45bb6261",
    "semantic_title": "strategyllm: large language models as strategy generators, executors, optimizers, and evaluators for problem solving",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=e0aH7nnRkHA": {
    "title": "Large language models are not probabilistic reasoners",
    "volume": "review",
    "abstract": "Advances in the general capabilities of large language models (LLMs) have led to the possibility of incorporating them into automated decision systems. A faithful representation of probabilistic reasoning in these models can be essential to ensure the reasoning of the automated decision systems incorporating them is trustworthy and explainable. Despite previous work suggesting that LLMs can perform complex reasoning and well-calibrated uncertainty quantification, we find that current versions of this class of model lack the ability to provide consistent and coherent probability estimates. We then suggest possible directions that future research can take to alleviate this weakness",
    "checked": false,
    "id": "08f154143909564e40fec03cfa7960ea80f739b3",
    "semantic_title": "towards logically consistent language models via probabilistic reasoning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zlV6w1O4Ap5": {
    "title": "LangBridge: Multilingual Reasoning Without Multilingual Supervision",
    "volume": "review",
    "abstract": "We introduce LangBridge, a zero-shot approach to adapt language models for multilingual reasoning tasks without multilingual supervision. LangBridge operates by bridging two models, each specialized in different aspects: (1) one specialized in understanding multiple languages (e.g., mT5 encoder) and (2) one specialized in reasoning (e.g., MetaMath). LangBridge connects the two models by introducing minimal trainable parameters between them. Despite utilizing only English data for training, LangBridge considerably enhances the performance of language models on low-resource languages across mathematical reasoning, coding, and logical reasoning. Our analysis suggests that the efficacy of LangBridge stems from the language-agnostic characteristics of multilingual representations. We publicly release our code and models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S3Lh24DheFl": {
    "title": "Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives",
    "volume": "review",
    "abstract": "The reflection capacity of Large Language Model (LLM) has garnered extensive attention. A post-hoc prompting strategy, e.g., reflexion and self-refine, refines LLM's response based on self-evaluated or external feedback. However, recent research indicates without external feedback, LLM's intrinsic reflection is unstable. Our investigation unveils that the key bottleneck is the quality of the self-evaluated feedback. We find LLMs often exhibit overconfidence or high randomness when self-evaluate, offering stubborn or inconsistent feedback, which causes poor reflection. To remedy this, we advocate Self-Contrast: It adaptively explores diverse solving perspectives tailored to the request, contrasts the differences, and summarizes these discrepancies into a checklist which could be used to re-examine and eliminate discrepancies. Our method endows LLM with diverse perspectives to alleviate stubborn biases. Moreover, their discrepancies indicate potential errors or inherent uncertainties that LLM often overlooks. Reflecting upon these can catalyze more accurate and stable reflection. Experiments conducted on a series of reasoning and translation tasks with different LLMs serve to underscore the effectiveness and generality of our strategy",
    "checked": true,
    "id": "4bebe389dfa85423e5cc089edf20b2c3f572f38c",
    "semantic_title": "self-contrast: better reflection through inconsistent solving perspectives",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=PYhr3zcvwN1": {
    "title": "BIBench: Benchmarking Data Analysis Knowledge of Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of tasks. However, their proficiency and reliability in the specialized domain of Data Analysis, particularly with a focus on data-driven thinking, remain uncertain. To bridge this gap, we introduce BIBench, a comprehensive benchmark designed to evaluate the data analysis capabilities of LLMs within the context of Business Intelligence (BI). BIBench assesses LLMs across three dimensions: 1) BI foundational knowledge, evaluating the models' numerical reasoning and familiarity with financial concepts; 2) BI knowledge application, determining the models' ability to quickly comprehend textual information and generate analysis questions from multiple views; and 3) BI technical skills, examining the models' use of technical knowledge to address real-world data analysis challenges. BIBench comprises 11 sub-tasks, spanning three categories of task types: classification, extraction, and generation. Additionally, we've developed BIChat, a domain-specific dataset with over a million data points, to fine-tune LLMs. We will release BIBenchmark, BIChat, and the evaluation scripts at \\url{https://github.com/xxx}. This benchmark aims to provide a measure for in-depth analysis of LLM abilities and foster the advancement of LLMs in the field of data analysis",
    "checked": false,
    "id": "82e6b69cb50123a8b05fa3c562fcd91eca0cd454",
    "semantic_title": "findabench: benchmarking financial data analysis ability of large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ubJz2G5rWxX": {
    "title": "The #COP28 deal is yet another historic failure.\" Multilingual Sentiment Term Extraction on Environmental Sustainability",
    "volume": "review",
    "abstract": "This paper introduces EnviS, a corpus of 5k tweets annotated with sentiment terms in three languages (Italian, English, and Indonesian) for investigating the debate on environmental sustainability in Social Media. We present a framework for the automatic aggregation of span-level annotations that preserves the annotators' perspective, avoiding additional manual intervention, reducing costs, and preserving the quality of the annotations. Furthermore, we ran a battery of baseline experiments using six open-source instruction/chat-based LLMs in zero-shot and few-shot settings, showing the limits of these models in following instructions and providing correct answers for the extraction and classification of sentiment terms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mz2JYufbg4g": {
    "title": "Bootstrapping LLM Agents via Verification",
    "volume": "review",
    "abstract": "We present a self-training method that allows language model-based agents to improve performance without distilling proprietary models. Existing self-verification methods struggle to validate function signatures defined in agent prompts. A common failure is the verifier hallucinating non-existent constraints on function calls due to interference between model knowledge and examples in prompts. To address this, we devise a neural-symbolic verification system that prioritizes language models for validating solution completeness and pertinence while delegating fact checks to a symbolic system. We propose bootstrap-by-verification learning which combines massive agent trajectory sampling with our verification for self-training. Experiments on spreadsheet and web browsing benchmarks show the method's effectiveness",
    "checked": false,
    "id": "8223f81e8cc126b83d2774fe2da19ead290c144d",
    "semantic_title": "bootstrapping llm-based task-oriented dialogue agents via self-talk",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=6y-N9mBo8aF": {
    "title": "API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access",
    "volume": "review",
    "abstract": "This study aims to address the pervasive challenge of quantifying uncertainty in large language models (LLMs) with black-box API access. Conformal Prediction (CP), known for its model-agnostic and distribution-free features, is a desired approach for various LLMs and data distributions. However, existing CP methods for LLMs typically assume access to the logits, which are unavailable for some API-only LLMs. In addition, logits are known to be miscalibrated, potentially leading to degraded CP performance. To tackle these challenges, we introduce a novel CP method that (1) is tailored for API-only LLMs without logit-access; (2) minimizes the size of prediction sets; and (3) ensures a statistical guarantee of the user-defined coverage. The core idea of this approach is to formulate nonconformity measures using both coarse-grained (i.e., sample frequency) and fine-grained uncertainty notions (e.g., semantic similarity). Experimental results on both close-ended and open-ended Question Answering tasks show our approach can mostly outperform the logit-based CP baselines",
    "checked": true,
    "id": "56a4fb8bf5bac348e2efd5f8628d52a409102100",
    "semantic_title": "api is enough: conformal prediction for large language models without logit-access",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=WDVXPhkwYhB": {
    "title": "Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models",
    "volume": "review",
    "abstract": "Object hallucination has been an Achilles' heel which hinders the broader applications of large vision-language models (LVLMs). Object hallucination refers to the phenomenon that the LVLMs claim non-existent objects in the image. To mitigate the object hallucinations, instruction tuning and external model-based detection methods have been proposed, which either require large-scare computational resources or depend on the detection result of external models. However, there remains an under-explored field to utilize the LVLM itself to alleviate object hallucinations. In this work, we adopt the intuition that the LVLM tends to respond logically consistently for existent objects but inconsistently for hallucinated objects. Therefore, we propose a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation, namely \\textbf{LogicCheckGPT}. In specific, we devise logical consistency probing to raise questions with logical correlations, inquiring about attributes from objects and vice versa. Whether their responses can form a logical closed loop serves as an indicator of object hallucination. As a plug-and-play method, it can be seamlessly applied to all existing LVLMs. Comprehensive experiments conducted on three benchmarks across four LVLMs have demonstrated significant improvements brought by our method, indicating its effectiveness and generality",
    "checked": true,
    "id": "80248c8c7cbb5bb1d2a508001108f3f15bb60430",
    "semantic_title": "logical closed loop: uncovering object hallucinations in large vision-language models",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=O8v-eHEZiav": {
    "title": "NovelQA: A Benchmark for Long-Range Novel Question Answering",
    "volume": "review",
    "abstract": "The rapid advancement of Large Language Models (LLMs) has introduced a new frontier in natural language processing, particularly in understanding and processing long-context information. However, the evaluation of these models' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark specifically designed to test the capabilities of LLMs with extended texts. Constructed from English novels, NovelQA offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper presents the design and construction of NovelQA, highlighting its manual annotation, and diverse question types. Our evaluation of Long-context LLMs on NovelQA reveals significant insights into the models' performance, particularly emphasizing the challenges they face with multi-hop reasoning, detail-oriented questions, and extremely long input with more than 100,000 tokens. The results underscore the necessity for further advancements in LLMs to improve their long-context comprehension and computational literary studies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zby4Ade9CCF": {
    "title": "Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation",
    "volume": "review",
    "abstract": "In this paper, we introduce a novel Face-to-Face spoken dialogue model. It processes audio-visual speech from user input and generates audio-visual speech as the response, marking the initial step towards creating an avatar chatbot system without relying on intermediate text. To this end, we newly introduce MultiDialog, the first large-scale multimodal (\\ie, audio and visual) spoken dialogue corpus containing 387 hours of approximately 10,000 dialogues, recorded based on the open domain dialogue dataset, TopicalChat. The MultiDialog contains parallel audio-visual recordings of conversation partners acting according to the given script with emotion annotations, which we expect to open up research opportunities in multimodal synthesis. Our Face-to-Face spoken dialogue model incorporates a textually pretrained large language model and adapts it into the audio-visual spoken dialogue domain by incorporating speech-text joint pretraining. Through extensive experiments, we validate the effectiveness of our model in facilitating a face-to-face conversation. All the data will be open-sourced",
    "checked": true,
    "id": "d030f4520d7fe1b74ac09042fbe6458fd4ed4f79",
    "semantic_title": "let's go real talk: spoken dialogue model for face-to-face conversation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BF1jtfsSXJU": {
    "title": "Evaluation, Analysis, and Mitigation of Shortcut Learning for Large Language Models in In-Context Learning",
    "volume": "review",
    "abstract": "Recent studies have confirmed that Pre-trained Language Models (PLMs) have a tendency to shortcut learning, thus producing a sharp drop in performance under distribution shift. However, most existing approaches focus only on shortcut learning in fine-tuned lightweight PLMs and cannot bridge the gap with Large Language Models (LLMs). In addition, how to evaluate LLMs dependence on shortcuts and how to alleviate the dependence on shortcuts still need extensive and in-depth research. Therefore, motivated by the above challenges, this paper proposes a benchmark containing two common text classification tasks to analyze and quantify the impact of shortcuts on LLMs in In-Context Learning (ICL). Then, we explain the shortcut learning of LLMs from the perspective of information flow: LLMs tend to make one-sided inferences by using the association between repeated shortcuts and labels in context. Finally, we evaluate several prompt-based shortcut mitigation strategies that lead to more robust predictions from the LLMs. Our work establishes a set of LLMs' shortcut research processes from assessment to analysis to mitigation, and provides new insights into LLMs shortcut learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zbNqSx2nzoL": {
    "title": "DParT: Transferring knowledge between languages changing a few weights",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have significantly impacted both research and business domains, automating tasks previously unattainable by artificial intelligence. However, the primary focus on English and European languages presents a barrier in adapting and applying LLMs to other languages due to the challenges involved in data collection, pre-processing, and model training. To overcome this issue, we propose a Double Partial Tuning DParT strategy. It involves modifying the structure of the training data in the first stage and employing low rank adapters LoRA in the second stage, leading to knowledge transfer between languages and low computational efforts in terms of trainable parameters and data quantity. Tests on Arabic and Russian languages demonstrate the superiority of DParT over other training methods, potentially expanding the application of LLMs in various languages and further revolutionizing research and business fields.We selected Arabic and Russian languages, as they originate from distinct language families and utilize two different non-Latin scripts, in order to demonstrate the effectiveness of the proposed approach. Code and datasets will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E42GF8r8XS9": {
    "title": "InfoLossQA: Characterizing and Recovering Information Loss in Text Simplification",
    "volume": "review",
    "abstract": "Text simplification aims to make technical texts more accessible to laypeople but often results in deletion of information and vagueness. This work proposes InfoLossQA, a framework to characterize and recover simplification-induced information loss in form of question-and-answer (QA) pairs. Building on the theory of Questions Under Discussion, the QA pairs are designed to help readers deepen their knowledge of a text. First, we collect a dataset of 1,000 linguist-curated QA pairs derived from 104 LLM simplifications of English medical study abstracts. Our analyses of this data reveal that information loss occurs frequently, and that the QA pairs give a high-level overview of what information was lost. Second, we devise two methods for this task: end-to-end prompting of open-source and commercial language models, and a natural language inference pipeline. With a novel evaluation framework considering the correctness of QA pairs and their linguistic suitability, our expert evaluation reveals that models struggle to reliably identify information loss and applying similar standards as humans at what constitutes information loss",
    "checked": true,
    "id": "e086c452000a58b4695383e9774dfbaeed912065",
    "semantic_title": "infolossqa: characterizing and recovering information loss in text simplification",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=_UH8Oeh2Hrg": {
    "title": "ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering",
    "volume": "review",
    "abstract": "The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies. The code of ELLA-V will be open-sourced after cleanups. Audio samples are availableat https://anonymous.4open.science/w/ELLAV_anonymous/",
    "checked": true,
    "id": "7f8e4e1e8a264ce8909e4482136835396dbbb662",
    "semantic_title": "ella-v: stable neural codec language modeling with alignment-guided sequence reordering",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=T9fHaLMR_Vo": {
    "title": "Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark",
    "volume": "review",
    "abstract": "The summarization capabilities of pretrained and large language models (LLMs) have been widely validated in general areas, but their use in scientific corpus, which involves complex sentences and specialized knowledge, has been less assessed. This paper presents conceptual and experimental analyses of scientific summarization, highlighting the inadequacies of traditional evaluation methods, such as $n$-gram, embedding comparison, and QA, particularly in providing explanations, grasping scientific concepts, or identifying key content. Subsequently, we introduce the Facet-aware Metric (FM), employing LLMs for advanced semantic matching to evaluate summaries based on different aspects. This facet-aware approach offers a thorough evaluation of abstracts by decomposing the evaluation task into simpler subtasks. Recognizing the absence of an evaluation benchmark in this domain, we curate a Facet-based scientific summarization Dataset (FD) with facet-level annotations. Our findings confirm that FM offers a more logical approach to evaluating scientific summaries. In addition, fine-tuned smaller models can compete with LLMs in scientific contexts, while LLMs have limitations in learning from in-context information in scientific domains. This suggests an area for future enhancement of LLM",
    "checked": true,
    "id": "d4e54af88d0aa32fc5ac0c8e83dafb083c9c8a83",
    "semantic_title": "rethinking scientific summarization evaluation: grounding explainable metrics on facet-aware benchmark",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tQ0ZzJozjBF": {
    "title": "Reformatted Alignment",
    "volume": "review",
    "abstract": "The quality of finetuning data is crucial for aligning large language models (LLMs) with human values.Current methods to improve data quality are either labor-intensive or prone to factual errors caused by LLM hallucinations.This paper explores elevating the quality of existing instruction data to better align with human values, introducing a simple and effective approach named ReAlign, which \\textit{reformats} the responses of instruction data into a format that better aligns with pre-established criteria and the collated evidence.This approach minimizes human annotation, hallucination, and the difficulty in scaling, remaining orthogonal to existing alignment techniques.Experimentally, ReAlign significantly boosts the general alignment ability, math reasoning, factuality, and readability of the LLMs.Encouragingly, \\emph{without} introducing any additional data or advanced training techniques, and merely by reformatting the response, LLaMA-2-13B's mathematical reasoning ability on \\texttt{GSM8K} can be improved \\textbf{from 46.77\\% to 56.63\\%} in accuracy.Additionally, a mere 5\\% of \\modelname data yields a 67\\% boost in general alignment ability measured by the Alpaca dataset. This work highlights the need for further research into the \\emph{science} and \\emph{mechanistic interpretability} of LLMs. We have made the associated code and data publicly accessible to support future studies at https://anonymous.4open.science/r/ReAlign-9B3D",
    "checked": true,
    "id": "a7d047dd9f41d5f3e7eaa39e5ba8c97cccc7276d",
    "semantic_title": "reformatted alignment",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=tAkmvpBysjD": {
    "title": "Leveraging Large Language Models for Biomedical Terminology Normalization",
    "volume": "review",
    "abstract": "Biomedical Terminology Normalization aims at finding the standard term in a given termbase for non-standardized mentions coming from social media or clinical texts, and the mainstream approaches adopted with the ``Recall and Re-rank'' framework. Instead of the traditional pretraining-finetuning paradigm, we would like to explore the possibility of accomplishing this task through a training-free paradigm using the powerful large language models (LLMs). Hoping to address the costs of re-training due to discrepancies of both standard termbases and annotation protocols. Another major obstacle in this task is that both mentions and terms are short texts. Short texts contain an insufficient amount of information that can introduce ambiguity, especially in a biomedical context. Therefore, besides using the advanced embedding model, we distill knowledge from LLM to expand the short text for a more informative description, enabling a superior unsupervised retrieval approach. Furthermore, we introduce an innovative training-free biomedical terminology normalization framework. By leveraging the reasoning capabilities of the LLM, in combination with supervised data and domain-specific expertise, to conduct more sophisticated ranking and re-ranking processes. Experimental results across multiple datasets indicate that both our unsupervised and supervised approaches achieve state-of-the-art",
    "checked": false,
    "id": "d28d082520a274b3a845e4ce1007df0327b14223",
    "semantic_title": "leveraging large language models for clinical abbreviation disambiguation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D4e2zZf_IEq": {
    "title": "MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL",
    "volume": "review",
    "abstract": "Recent LLM-based Text-to-SQL methods usually suffer from significant performance degradation on ``huge\" databases and complex user questions that require multi-step reasoning. Moreover, most existing methods neglect the crucial significance of LLMs utilizing external tools and model collaboration. To address these challenges, we introduce \\textsc{MAC-SQL}, a novel LLM-based multi-agent collaborative framework. Our framework comprises a core decomposer agent for Text-to-SQL generation with few-shot chain-of-thought reasoning, accompanied by two auxiliary agents that utilize external tools or models to acquire smaller sub-databases and refine erroneous SQL queries. The decomposer agent collaborates with auxiliary agents, which are activated as needed and can be expanded to accommodate new features or tools for effective Text-to-SQL parsing. In our framework, We initially leverage GPT-4 as the strong backbone LLM for all agent tasks to determine the upper bound of our framework. We then fine-tune an open-sourced instruction-followed model, SQL-Llama, by leveraging Code Llama 7B, to accomplish all tasks as GPT-4 does. Experiments show that SQL-Llama achieves a comparable execution accuracy of 43.94, compared to the baseline accuracy of 46.35 for vanilla GPT-4. At the time of writing, MAC-SQL+GPT-4 achieves an execution accuracy of 59.59 when evaluated on the BIRD benchmark, establishing a new state-of-the-art (SOTA) on its holdout test set",
    "checked": true,
    "id": "43f0b39bc787311b22e0a8659780743cb54a67be",
    "semantic_title": "mac-sql: a multi-agent collaborative framework for text-to-sql",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=Np94QTqUu8n": {
    "title": "Refining Corpora from a Model Calibration Perspective for Chinese Spelling Correction",
    "volume": "review",
    "abstract": "Chinese Spelling Correction (CSC) commonly lacks large-scale high-quality corpora, due to the labor-intensive labeling of spelling errors in real-life human writing or typing scenarios. Two data augmentation methods are widely adopted: (1) \\textit{Random Replacement} with the guidance of confusion sets and (2) \\textit{OCR/ASR-based Generation} that simulates character misusing. However, both methods inevitably introduce noisy data (e.g., false spelling errors), potentially leading to over-correction. By carefully analyzing the two types of corpora, we find that though the latter achieves more robust generalization performance, the former yields better-calibrated CSC models. We then provide a theoretical analysis of this empirical observation, based on which a corpus refining strategy is proposed. Specifically, OCR/ASR-based data samples are fed into a well-calibrated CSC model trained on random replacement-based corpora and then filtered based on prediction confidence. By learning a simple BERT-based model on the refined OCR/ASR-based corpus, we set up impressive state-of-the-art performance on three widely-used benchmarks, while significantly alleviating over-correction (e.g., lowering false positive predictions)",
    "checked": true,
    "id": "8768e14ba68a50d8672ce96a16e6ba67966eeaaa",
    "semantic_title": "refining corpora from a model calibration perspective for chinese spelling correction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cFsKfHiNk_": {
    "title": "XMoE: Sparse Models with Fine-grained and Adaptive Expert Selection",
    "volume": "review",
    "abstract": "Sparse models, including sparse Mixture-of-Experts (MoE) models, have emerged as an effective approach for scaling Transformer models. However, they often suffer from computational inefficiency since a significant number of parameters are unnecessarily involved in computations by multiplying values by zero or low activation values. To address this issue, we present XMoE, a novel MoE designed to enhance both the efficacy and efficiency of sparse MoE models. XMoE leverages small experts and a threshold-based router to enable tokens to selectively engage only essential parameters. Our extensive experiments on language modeling and machine translation tasks demonstrate that \\tool enhances model performance and can decrease the computation load at MoE layers by over 50\\% without sacrificing performance. Furthermore, we present the versatility of \\tool by applying it to dense models, enabling sparse computation during inference. We provide a comprehensive analysis and make our code available at \\url{https://anonymous.4open.science/r/XMoE}",
    "checked": true,
    "id": "dede394d2bd54fda63c36e48064264e16e0c6b65",
    "semantic_title": "xmoe: sparse models with fine-grained and adaptive expert selection",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qTUOgHVSwZr": {
    "title": "Should Cross-Lingual AMR Parsing go Meta? An Empirical Assessment of Meta-Learning and Joint Learning AMR Parsing",
    "volume": "review",
    "abstract": "Cross-lingual AMR parsing is the task of predicting AMR graphs in a target language when training data is available only in a source language. Due to the small size of AMR training data and evaluation data, cross-lingual AMR parsing has only been explored in a small set of languages such as English, Spanish, German, Chinese, and Italian. Taking inspiration from Langedijk et al. (2022), who apply meta-learning to tackle cross-lingual syntactic parsing, we investigate the use of meta-learning for cross-lingual AMR parsing. We evaluate our models in both zero-shot and few-shot scenarios and assess their effectiveness in Croatian, Farsi, Korean, Chinese, and French. Notably, Korean and Croatian test sets are developed as part of our work, based on the existing The Little Prince English AMR corpus, and made publicly available. We empirically study this approach by comparing it to a classical joint learning method. Our findings suggest that while the meta-learning model performs similarly to a jointly trained model on average smatch score, it exhibits inconsistency and unstable performance across settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6WI0lVIDdyZ": {
    "title": "Learning to Filter Context for Retrieval-Augmented Generation",
    "volume": "review",
    "abstract": "On-the-fly retrieval of relevant knowledge has proven an essential element of reliable systems for tasks such as open-domain question answering and fact verification. However, because retrieval systems are not perfect, generation models are required to generate outputs given partially or entirely irrelevant passages. This can cause over- or under-reliance on context, and result in problems in the generated output such as hallucinations. To alleviate these problems, we propose FILCO, a method that improves the quality of the context provided to the generator by (1) identifying useful context based on lexical and information-theoretic approaches, and (2) training context filtering models that can filter retrieved contexts at test time. We experiment on six knowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that our method outperforms existing approaches on extractive question answering (QA), complex multi-hop and long-form QA, fact verification, and dialog generation tasks",
    "checked": true,
    "id": "7848d4b4e6ba0897a85cebb6467e94eb0b60d583",
    "semantic_title": "learning to filter context for retrieval-augmented generation",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=4FZLEymzcw": {
    "title": "LLM-based Related Work Section Generation Framework Incorporating Perspectives Researchers Value",
    "volume": "review",
    "abstract": "This paper proposes a Large Language Model (LLM)-based framework to generate paper's related work section, incorporating perspectives valued by researchers. While LLMs excel at summarization, ambiguous instructions limit the clarity of a generated related work section for researchers. Through the surveys, we identified the preferred perspectives for a related work section: \"categorization'', \"comparison'', and \"pointing out problems''. We incorporate these perspectives into a prompt with few-shot examples. Furthermore, to provide the framework with explainability and aid in the fact-checking, we have the LLM select salient sentences from cited papers to extract evidences. Experimental results with human evaluation demonstrate that the generated related work section tends to be preferred over human-written ones and has fewer hallucinations. Our codes and the dataset we collected are available at https://anonymous.4open.science/r/anony_rwg/",
    "checked": false,
    "id": "c6cf3e64787178b87bc40fde557a04e71c821ee9",
    "semantic_title": "imagine sustainability: integrated inner-outer transformation in research, education and practice",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=_j0Qjz9sWOe": {
    "title": "FreeCtrl: Constructing Control Centers with Feedforward Layers for Learning-Free Controllable Text Generation",
    "volume": "review",
    "abstract": "Controllable text generation (CTG) seeks to craft texts adhering to specific attributes, traditionally employing learning-based techniques such as training, fine-tuning, or prefix-tuning with attribute-specific datasets. These approaches, while effective, demand extensive computational and data resources. In contrast, some proposed learning-free alternatives circumvent learning but often yield inferior results, exemplifying the fundamental machine learning trade-off between computational expense and model efficacy. To overcome these limitations, we propose FreeCtrl, a learning-free method that dynamically modulates the weights of selected feedforward neural network (FFN) vectors to increase the likelihood of generating sentences with desired attribute-related keywords. Specifically, we first identify the key characteristics and challenges of using FFN layers for CTG and then introduce a structured workflow to build and adaptively activate control centers constructed by FFN vectors to regulate the language model outputs on desirable attributes. Extensive experiments on single- and multi-attribute control reveal that the proposed learning-free FreeCtrl outperforms other learning-free and learning-based methods, successfully resolving the dilemma between learning costs and model performance",
    "checked": true,
    "id": "88217a7a800cbfdb9f3e8bfa3c1c2c33d59a00c8",
    "semantic_title": "freectrl: constructing control centers with feedforward layers for learning-free controllable text generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j_4nj0CGoU": {
    "title": "Make Large Language Model a Better Ranker",
    "volume": "review",
    "abstract": "The evolution of Large Language Models (LLMs) has significantly enhanced capabilities across various fields, leading to a paradigm shift in how Recommender Systems (RSs) are conceptualized and developed. However, existing research primarily focuses on point-wise and pair-wise recommendation paradigms. These approaches prove inefficient in LLM-based recommenders due to the high computational cost of utilizing Large Language Models. While some studies have delved into list-wise approaches, they fall short in ranking tasks. This shortfall is attributed to the misalignment between the objectives of ranking and language generation. To this end, this paper introduces the Language Model Framework with Aligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap between the capabilities of LLMs and the nuanced requirements of ranking tasks within recommender systems. A key feature of ALRO is the introduction of soft lambda loss, an adaptation of lambda loss tailored to suit language generation tasks. Additionally, ALRO incorporates a permutation-sensitive learning mechanism that addresses position bias, a prevalent issue in generative models, without imposing additional computational burdens during inference. Our evaluative studies reveal that ALRO outperforms existing embedding-based recommendation methods and the existing LLM-based recommendation baselines, highlighting its efficacy",
    "checked": true,
    "id": "1eb3d80c68d45e6ff09965a3069787e41f10a090",
    "semantic_title": "make large language model a better ranker",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=qMT9ExoRKE2": {
    "title": "EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs",
    "volume": "review",
    "abstract": "Existing methods for fine-tuning sparse LLMs often suffer from resource-intensive requirements and high retraining costs. Additionally, many fine-tuning methods often rely on approximations or heuristic optimization strategies, which may lead to suboptimal solutions. To address these issues, we propose an efficient and fast framework for fine-tuning sparse LLMs based on minimizing reconstruction error. Our approach involves sampling a small dataset for calibration and utilizing backpropagation to iteratively optimize block-wise reconstruction error, on a block-by-block basis, aiming for optimal solutions. Extensive experiments on various benchmarks consistently demonstrate the superiority of our method over other baselines. For instance, on the Wikitext2 dataset with LlamaV1-7B at 70% sparsity, our proposed EBFT achieves a perplexity of 16.88, surpassing the state-of-the-art DSnoT with a perplexity of 75.14. Moreover, with a structured sparsity ratio of 26\\%, EBFT achieves a perplexity of 16.27, outperforming LoRA (perplexity 16.44). Furthermore, the fine-tuning process of EBFT for LlamaV1-7B only takes approximately 30 minutes, and the entire framework can be executed on a single 16GB GPU. The source code is available at https://github.com/anonymousACL2024/EBFT",
    "checked": true,
    "id": "0c41cfc99c77c0cca5eff8767ac60128dafa124a",
    "semantic_title": "ebft: effective and block-wise fine-tuning for sparse llms",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=SbORi6hlDg": {
    "title": "Quantifying Atomic Knowledge in Self-Diagnosis for Chinese Medical LLMs",
    "volume": "review",
    "abstract": "The booming development of medical large-scale language models (LLMs) enables users to complete preliminary medical consultations (self-diagnosis) in their daily lives. Recent evaluations of medical LLMs mainly focus on their ability to complete medical tasks, pass medical examinations, or obtain a favorable GPT-4 rating. There are still challenges in using them to provide directions for improving medical LLMs, including misalignment with practical use, lack of depth in exploration, and over-reliance on GPT-4. To address the above issues, we construct a fact-checking style Self-Diagnostic Atomic Knowledge (SDAK) benchmark. Through atomic knowledge that is close to real usage scenarios, it can more accurately, reliably, and fundamentally evaluate the memorization ability of medical LLMs for medical knowledge. The experimental results show that Chinese medical LLMs still have much room for improvement in self-diagnostic atomic knowledge. We further explore different types of data commonly adopted for fine-tuning medical LLMs and find that distilled data enhances medical knowledge retention more effectively than real-world doctor-patient conversations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gnUsp9Zfrp2": {
    "title": "Translation Deserves Better: Analyzing Translation Artifacts in Cross-lingual Visual Question Answering",
    "volume": "review",
    "abstract": "Building a reliable visual question answering (VQA) system across different languages is a challenging problem, primarily due to the lack of abundant samples for training. To address this challenge, recent studies have employed machine translation systems for the cross-lingual VQA task. This involves translating the evaluation samples into a source language (usually English) and using monolingual models (i.e., translate-test). However, our analysis reveals that translated texts contain unique characteristics distinct from human-written ones, referred to as translation artifacts. We find that these artifacts can significantly affect the models, confirmed by extensive experiments across diverse models, languages, and translation processes. In light of this, we present a simple data augmentation strategy that can alleviate the adverse impacts of translation artifacts",
    "checked": true,
    "id": "c65ba6328d1a7663839876c5d25deb11066f15bb",
    "semantic_title": "translation deserves better: analyzing translation artifacts in cross-lingual visual question answering",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TrEgMzhvgXU": {
    "title": "Flexible Scaling of LLM's Context With Extensible Embedding",
    "volume": "review",
    "abstract": "Large language models (LLMs) call for extension of context to handle many critical applications. However, the existing approaches are prone to expensive costs and inferior quality of context extension. In this work, we propose Extensible Embedding, which realizes high-quality extension of LLM's context with strong flexibility and cost-effectiveness. Extensible embedding stand as an enhancement of typical token embedding, which represents the information for an extensible scope of context instead of a single token. By leveraging such compact input units of higher information density, the LLM can access to a vast scope of context even with a small context window. Extensible embedding is systematically optimized in architecture and training method, which leads to multiple advantages. 1) High flexibility of context extension, which flexibly supports ad-hoc extension of diverse context lengths. 2) Strong sample efficiency of training, which enables the embedding model to be learned in a cost-effective way. 3) Superior compatibility with the existing LLMs, where the extensible embedding can be seamlessly introduced as a plug-in component. Comprehensive evaluations on long-context language modeling and understanding tasks verify extensible embedding as an effective, efficient, flexible, and compatible method to extend the LLM's context",
    "checked": false,
    "id": "82ba78d1520fbdca6f6e23fca1f555f25c8f86d4",
    "semantic_title": "flexibly scaling large language models contexts through extensible tokenization",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=wt63SYp0sk1": {
    "title": "HGT: Leveraging Heterogeneous Graph-enhanced Large Language Models for Few-shot Complex Table Understanding",
    "volume": "review",
    "abstract": "Table understanding (TU) has achieved promising advancements, but it faces the challenges of the scarcity of manually labeled tables and the presence of complex table structures. To address these challenges, we propose HGT, a framework with a heterogeneous graph (HG)-enhanced large language model (LLM) to tackle few-shot TU tasks. It leverages the LLM by aligning the table semantics with the LLM's parametric knowledge through soft prompts and instruction turning and deals with complex tables by a multi-task pre-training scheme involving three novel multi-granularity self-supervised HG pre-training objectives. We empirically demonstrate the effectiveness of HGT, showing that it outperforms the SOTA for few-shot complex TU on several benchmarks",
    "checked": true,
    "id": "4c455b8689f96c452a62671ec16a4c2cef153908",
    "semantic_title": "hgt: leveraging heterogeneous graph-enhanced large language models for few-shot complex table understanding",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ybYXfk3KVN": {
    "title": "PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have shown remarkable comprehension abilities but face challenges in GPU memory usage during inference, hindering their scalability for real-time applications like chatbots. To accelerate inference, we store computed keys and values (KV cache) in the GPU memory. Existing methods study the KV cache compression to reduce memory by pruning the pre-computed KV cache. However, they neglect the inter-layer dependency between layers and huge memory consumption in pre-computation. To explore these deficiencies, we find that the number of crucial keys and values that influence future generations decreases layer by layer and we can extract them by the consistency in attention weights. Based on the findings, we propose PyramidInfer, a method that compresses the KV cache by layer-wise retaining crucial context. PyramidInfer saves significant memory by computing fewer keys and values without sacrificing performance. Experimental results show PyramidInfer improves 2.2x throughput compared to Accelerate with over 54% GPU memory reduction in KV cache",
    "checked": true,
    "id": "2f96229c404a5cbd0c0d06492f5ea3d6bfcf50d2",
    "semantic_title": "pyramidinfer: pyramid kv cache compression for high-throughput llm inference",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=6vJsZjRiQXT": {
    "title": "Enhancing Temporal Knowledge Graph Forecasting with Large Language Models via Chain-of-History Reasoning",
    "volume": "review",
    "abstract": "Temporal Knowledge Graph (TKG) forecasting aims to predict future facts based on given histories. Most recent graph-based models excel at capturing structural information within TKGs but lack semantic comprehension abilities. Nowadays, with the surge of LLMs, the LLM-based TKG prediction model has emerged. However, the existing LLM-based model exhibits three shortcomings: (1) It only focuses on the first-order history for prediction while ignoring high-order historical information, resulting in the provided information for LLMs being extremely limited. (2) LLMs struggle with optimal reasoning performance under heavy historical information loads. (3) For TKG prediction, the temporal reasoning capability of LLM alone is limited. To address the first two challenges, we propose Chain-of-History (CoH) reasoning which explores high-order histories step-by-step, achieving effective utilization of high-order historical information for LLMs on TKG prediction. To address the third issue, we design CoH as a paly-and-plug module to enhance the performance of graph-based models for TKG prediction. Extensive experiments on three datasets and backbones demonstrate the effectiveness of CoH",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yOUHod2NCr": {
    "title": "CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models",
    "volume": "review",
    "abstract": "Open large language models (LLMs) have significantly advanced the field of natural language processing, showcasing impressive performance across various tasks.Despite the significant advancements in LLMs, their effective operation still relies heavily on human input to accurately guide the dialogue flow, with agent tuning being a crucial optimization technique that involves human adjustments to the model for better response to such guidance.Addressing this dependency, our work introduces the TinyAgent model, trained on a meticulously curated high-quality dataset. We also present the Collaborative Multi-Agent Tuning (CMAT) framework, an innovative system designed to augment language agent capabilities through adaptive weight updates based on environmental feedback. This framework fosters collaborative learning and real-time adaptation among multiple intelligent agents, enhancing their context-awareness and long-term memory. In this research, we propose a new communication agent framework that integrates multi-agent systems with environmental feedback mechanisms, offering a scalable method to explore cooperative behaviors. Notably, our TinyAgent-7B model exhibits performance on par with GPT-3.5, despite having fewer parameters, signifying a substantial improvement in the efficiency and effectiveness of LLMs",
    "checked": true,
    "id": "cae8f8a5d4038256cbad514eda0ef5821e466dd0",
    "semantic_title": "cmat: a multi-agent collaboration tuning framework for enhancing small language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rMf5MQgYlcE": {
    "title": "VE-KD: Vocabulary-Expansion Knowledge-Distillation for Training Smaller Domain-Specific Language Models",
    "volume": "review",
    "abstract": "We propose VE-KD, a novel method that balances knowledge distillation and vocabulary expansionwith the aim of training efficient domain-specific language models.Compared with traditional pre-training approaches,VE-KD exhibits competitive performance in downstream taskswhile reducing model size and using fewer computational resources.Additionally, VE-KD refrains from overfitting in domain adaptation.Our experiments with different biomedical domain tasks demonstratethat VE-KD performs well compared with models such as BioBERT (+1% at HoC) and PubMedBERT (+1% at PubMedQA),with about 96% less training time. Furthermore, it outperforms DistilBERT and Adapt-and-Distill,showing a significant improvement in document-level tasks. Investigation of vocabulary size and tolerance,which are hyperparameters of our method,provides insights for further model optimization. The fact that VE-KD consistently maintains its advantages,even when the corpus size is small,suggests that it is a practical approach for domain-specific language tasksand is transferrable to different domains for broader applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K9_IGLPOwE": {
    "title": "Seeing the Unseen: Visual Metaphor Captioning for Videos",
    "volume": "review",
    "abstract": "Metaphors are a common communication tool used in our day-to-day life. The detection and generation of metaphors in textual form have been studied extensively but metaphors in other forms have been under-explored. Recent studies have shown that Vision-Language (VL) models cannot understand visual metaphors in memes and adverts. As of now, no probing studies have been done that involve complex language phenomena like metaphors with videos. Hence, we introduce a new VL task of describing the metaphors present in the videos in our work. To facilitate this novel task, we construct and release two datasets- a manually created dataset with 705 videos and 2115 human-written captions and a synthetic dataset of 90886 MSCOCO images with synthetically generated metaphor captions. We propose a novel video metaphor captioning system: GIT-LLaVA, which uses a frozen video captioning model augmented by a Large Language Model (LLM) for generating metaphors, as a strong baseline. We perform a comprehensive analysis of SOTA video language models on this task. We publish our datasets and benchmark results for our novel task to enable further research",
    "checked": true,
    "id": "38558608dedc495dd748ea3381b08b0d2ffccf56",
    "semantic_title": "seeing the unseen: visual metaphor captioning for videos",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Omb9A7LFvav": {
    "title": "A Two-Agent Game for Zero-shot Relation Triplet Extraction",
    "volume": "review",
    "abstract": "Relation triplet extraction is a fundamental task in natural language processing that aims to identify semantic relationships between entities in text. It is particularly challenging in the zero-shot setting, i.e., zero-shot relation triplet extraction (ZeroRTE), where the relation sets between training and test are disjoint. Existing methods deal with this task by integrating relations into prompts, which may lack sufficient understanding of the unseen relations. To address these limitations, this paper presents a novel Two-Agent Game (TAG) approach to deliberate and debate the semantics of unseen relations. TAG consists of two agents, a generator and an extractor. They iteratively interact in three key steps: attempting, criticizing, and rectifying. This enables the agents to fully debate and understand the unseen relations. Experimental results demonstrate consistent improvement over ALBERT-Large, BART, and GPT3.5, without incurring additional inference costs in all cases. Remarkably, our method outperforms strong baselines by a significant margin, achieving an impressive 6%-16% increase in F1 scores, particularly when dealing with FewRel with five unseen relations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=as4QVcBtHz": {
    "title": "My Answer is C\": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models",
    "volume": "review",
    "abstract": "The open-ended nature of language generation makes the evaluation of autoregressive large language models (LLMs) challenging. One common evaluation approach uses multiple-choice questions to limit the response space. The model is then evaluated by ranking the candidate answers by the log probability of the first token prediction. However, first-tokens may not consistently reflect the final response output, due to model's diverse response styles such as starting with \"Sure\" or refusing to answer. Consequently, first-token evaluation is not indicative of model behaviour when interacting with users. But by how much? We evaluate how aligned first-token evaluation is with the text output along several dimensions, namely final option choice, refusal rate, choice distribution and robustness under prompt perturbation. Our results show that the two approaches are severely misaligned \\emph{on all dimensions}, reaching mismatch rates over 60\\%. Models heavily fine-tuned on conversational or safety data are especially impacted. Crucially, models remain misaligned even when we increasingly constrain prompts, i.e., force them to start with an option letter or example template. Our findings i) underscore the importance of inspecting the text output, too and ii) caution against relying solely on first-token evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LZwQraQzWjW": {
    "title": "PST-Bench: Tracing and Benchmarking the Source of Publications",
    "volume": "review",
    "abstract": "Tracing the source of research papers is a fundamental yet challenging task for researchers. The billion-scale citation relations between papers hinder researchers from understanding the evolution of science efficiently. To date, there is still a lack of an accurate and scalable dataset constructed by professional researchers to identify the direct source of their studied papers, based on which automatic algorithms can be developed to expand the evolutionary knowledge of science. In this paper, we study the problem of paper source tracing (PST) and construct a high-quality and ever-increasing dataset PST-Bench in computer science. Based on PST-Bench, we reveal several intriguing discoveries, such as the differing evolution patterns across various topics. An exploration of various methods underscores the hardness of PST-Bench, pinpointing potential directions on this topic. The dataset and codes have been available",
    "checked": true,
    "id": "a23e45e11fe7f6f667cd411817a18af4d5f966a9",
    "semantic_title": "pst-bench: tracing and benchmarking the source of publications",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=q1hK39mMn9": {
    "title": "Exploring Multilingual Human Value Concepts in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?",
    "volume": "review",
    "abstract": "Prior research in representation engineering has revealed that LLMs encode concepts within their representation spaces, predominantly centered around English. In this study, we extend this philosophy to a multilingual scenario, delving into multilingual human value concepts in LLMs. Through our comprehensive exploration covering 7 types of human values, 16 languages and 3 LLM series with distinct multilinguality, we empirically substantiate the existence of multilingual human values in LLMs. Further cross-lingual analysis on these concepts discloses 3 traits arising from language resource disparities: cross-lingual inconsistency, distorted linguistic relationships, and unidirectional cross-lingual transfer between high- and low-resource languages, all in terms of human value concepts. Additionally, we prove the feasibility of cross-lingual control over value alignment capabilities of LLMs, leveraging the dominant language as a source language. Drawing from our findings on multilingual value alignment, we prudently provide suggestions on the composition of multilingual data for LLMs pre-training: including a limited number of dominant languages for cross-lingual alignment transfer while avoiding their excessive prevalence, and keeping a more balanced distribution of non-dominant languages. We aspire that our findings will contribute to enhancing the safety and utility of multilingual AI",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U0Euy_qlkgV": {
    "title": "Data-Efficiently Learn Large Language Model for Universal 3D Scene Perception",
    "volume": "review",
    "abstract": "3D scene understanding has gained significant attention due to its wide range of applications. However, existing methods for 3D scene understanding are limited to specific downstream tasks, which hinders their practicality in real-world applications. This paper presents Chat-3D, which combines the 3D visual perceptual ability of pre-trained 3D representations and the impressive reasoning and conversation capabilities of advanced LLMs to achieve the first universal dialogue systems for 3D scenes. Specifically, we align 3D representations into the feature space of LLMs, thus enabling LLMs to perceive the 3D world. Given the scarcity of 3D scene-text data, we propose a three-stage training strategy to efficiently utilize the available data for better alignment. To enhance the reasoning ability and develop a user-friendly interaction scheme, we further construct a high-quality object-centric 3D instruction dataset and design an associated object-centric prompt. With limited data, Chat-3D achieves a 82.2% relative score compared with GPT-4 on the constructed instruction dataset, and comparable performance to state-of-the-art LLM-based methods",
    "checked": false,
    "id": "30cfc4e7174211aa48c965826d51db773f0d37c7",
    "semantic_title": "chat-3d: data-efficiently tuning large language model for universal dialogue of 3d scenes",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=MkmEjdUiHnXZ": {
    "title": "Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies",
    "volume": "review",
    "abstract": "Ten years ago a single metric, BLEU, governed progress in machine translation research.For better or worse, there is no such consensus today, and consequently it is difficult for researchers to develop and retain the kinds of heuristic intuitions about metric deltas that drove earlier research and deployment decisions.This paper investigates the ``dynamic range'' of a number of modern metrics in an effort to provide a collective understanding of the meaning of differences in scores both within and among metrics; in other words, we ask `what point difference x in metric y is required between two systems for humans to notice?'We conduct our evaluation on a new large dataset, ToShip23, using it to discover deltas at which metrics achieve system-level differences that are meaningful to humans, which we measure by pairwise system accuracy.We additionally show that this method of establishing delta-accuracy is more stable than the standard use of statistical p-values in regards to testset size.Where data size permits, we also explore the effect of metric deltas and accuracy across finer-grained features such as translation direction, domain, and system closeness",
    "checked": true,
    "id": "e4fd996d7caf2f1bef540172cc27b170967828c0",
    "semantic_title": "navigating the metrics maze: reconciling score magnitudes and accuracies",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=5aAiNO8Bn-2": {
    "title": "Relay Decoding: Concatenating Large Language Models for Machine Translation",
    "volume": "review",
    "abstract": "Leveraging large language models for machine translation has demonstrated promising results. However, it does require the large language models to possess the capability of handling both the source and target languages in machine translation. In cases where obtaining a suitable specific large language model is not feasible, resorting to continuous learning methods becomes a costly endeavor. To mitigate these expenses, we propose an innovative approach called RD (Relay Decoding}), which entails concatenating two distinct large models that individually support the source and target languages. By incorporating a simple mapping layer to facilitate the connection between these two models and utilizing a limited amount of parallel data for training, we successfully achieve superior results in the machine translation task. Experimental results conducted on the Multi30k and WikiMatrix datasets validate the effectiveness of our proposed method",
    "checked": true,
    "id": "58996d32408c644b51f882e5e6c2df072f84bad8",
    "semantic_title": "relay decoding: concatenating large language models for machine translation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MesJjx1aFA": {
    "title": "S-Agent: an Agent Collaborative Framework Inspired by the Scientific Methodology",
    "volume": "review",
    "abstract": "An increasing number of advancements have been accomplished in agents empowered by Large Language Models (LLM), particularly in resolving simple dialogue tasks. However, existing agents still face intractable robustness issues for solving complex tasks, encountering the cascading hallucinations induced by multi-step invocations of LLM. Certain recent studies utilize multi-step reasoning, planning strategies, and domain workflows to improve the success rate of complex tasks, yet they neglect the scientific methodology that encompasses the accumulated wisdom derived from centuries of scientific inquiry. Drawing inspiration from the scientific methodology, we propose the S-Agent - an agent collaborative framework meticulously designed to actively experiment and refine theories based on the analysis of experimental results, thereby enhancing the deductive capabilities of LLMs and complementing their inductive and communicative strengths. Additionally, we introduce an innovative parallel planning methodology, wherein agents with identical roles collaborate to simultaneously address the same inquiry. Extensive experiments demonstrate the effectiveness and efficiency of our approach. Notably, we achieve a new state-of-the-art 96.3% pass@1 accuracy on the HumanEval coding benchmark with GPT-4",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O8VQMt0KIqZ": {
    "title": "CNTLS: A Benchmark Dataset for Abstractive and Extractive Chinese Timeline Summarization",
    "volume": "review",
    "abstract": "Timeline summarization (TLS) involves creating summaries of long-running events by amalgamating dated summaries from multiple news articles. However, the scarcity of available data has considerably hindered the advancement of timeline summarization. In this paper, we introduce the CNTLS dataset, an open resource for Chinese timeline summarization. CNTLS comprises 77 real-life topics, each containing 2524 documents, and achieves an average compression of nearly 60\\% of the duration of all topics.We meticulously analyze the corpus using established metrics, focusing on the style of the summaries and the complexity of the summarization task. We rigorously assess the performance of various classic extraction TLS systems and substantiate the applicability of the large model approach for generative TLS systems on the CNTLS corpus, thereby furnishing benchmarks and fostering further research. To the best of our knowledge, CNTLS marks the inception of the first Chinese timeline summarization dataset. The dataset and source code are released~\\footnote{Code and data available at: \\emph{{Accompanied ARR submission}}.}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a1qvg-l7yI": {
    "title": "Authorship Style Transfer with Inverse Transfer Data Augmentation",
    "volume": "review",
    "abstract": "Authorship style transfer aims to modify the style of neutral text to match the unique speaking or writing style of a particular individual. While Large Language Models (LLMs) present promising solutions, their effectiveness is limited by the small number of in-context learning demonstrations, particularly for authorship styles not frequently seen during pre-training. In response, this paper proposes an inverse transfer data augmentation (\\model) method, leveraging LLMs to create (neutral text, stylized text) pairs. This method involves removing the existing styles from stylized texts, a process made more feasible due to the prevalence of neutral texts in pre-training. We use this augmented dataset to train a compact model that is efficient for deployment and adept at replicating the targeted style. Our experimental results, conducted across four datasets with distinct authorship styles, establish the effectiveness of \\smodel over traditional style transfer methods and forward transfer using GPT-3.5. For further research and application, our dataset and code are openly accessible at https://github.com/AnonymousRole/ITDA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gPIGDFX6Dzs": {
    "title": "Large Language Models as Test Case Generators: Performance Evaluation and Enhancement",
    "volume": "review",
    "abstract": "Code generation with Large Language Models (LLMs) has been extensively studied and achieved remarkable progress. As a complementary aspect to code generation, test case generation is of crucial importance in ensuring the quality and reliability of code. However, using LLMs as test case generators has been much less explored. Current research along this line primarily focuses on enhancing code generation with assistance from test cases generated by LLMs, while the performance of LLMs in test case generation alone has not been comprehensively examined. To bridge this gap, we conduct extensive experiments to study how well LLMs can generate high-quality test cases. We find that as the problem difficulty increases, state-of-the-art LLMs struggle to generate correct test cases, largely due to their inherent limitations in computation and reasoning. To mitigate this issue, we further propose a multi-agent framework called TestChain that decouples the generation of test inputs and test outputs. Notably, TestChain uses a ReAct format conversation chain for LLMs to interact with a Python interpreter in order to provide more accurate test outputs. Our results indicate that TestChain outperforms the baseline by a large margin. Particularly, in terms of the accuracy of test cases, TestChain using GPT-4 as the backbone achieves a 13.84% improvement over the baseline on the LeetCode-hard dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nU1q_uc3I-": {
    "title": "When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards",
    "volume": "review",
    "abstract": "Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value â€” we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple-choice question benchmarks (e.g., MMLU), minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a *hybrid* scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust evaluation schemes on the existing benchmarks",
    "checked": true,
    "id": "5508ecfdd6f432d8f9932060bedf1d742ab5aac8",
    "semantic_title": "when benchmarks are targets: revealing the sensitivity of large language model leaderboards",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=8DUtvSsQa6V": {
    "title": "Are LLMs Aware that Some Questions are not Open-ended?",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have shown the impressive capability of answering questions in a wide range of scenarios. However, when LLMs face different types of questions, it is worth exploring whether LLMs are aware that some questions have limited answers and have to respond more deterministically but some do not. We refer to the ability as question awareness that LLMs know to adjust the determinacy of the answers according to the questions. The lack of question awareness leads to two contradictory issues: (1) Too casual to answer non-open-ended questions. (2) Too boring to answer open-ended questions. In this paper, we first evaluate the question awareness of LLMs. The experimental results show that LLMs have the above issues of lacking the awareness of questions in certain domains, e.g. factual knowledge. To mitigate these issues, we propose a method called Question Awareness Temperature Sampling (QuATS). This method enhances the question awareness of LLMs by dynamically adjusting the output distributions based on question features. The automatic adjustment in QuATS eliminates the need for manual temperature tuning in text generation and improves model performance in various benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NegTKhPNFJ": {
    "title": "CTL-Prompt: Contrastive Topic-Length Prompt Learning for Dialogue Summarization",
    "volume": "review",
    "abstract": "The prevalence of online meetings, such as Zoom and Microsoft Teams, has highlighted the necessity for an effective dialogue summary. This study proposes Contrastive Topic-Length Prompt Learning (CTL-Prompt), a simple method that generates topic-based summaries. First, we used topic prompts to direct our dialogue summarization in order to steer the summary towards a particular topic in light of recent success with prompts in guiding aspects in general summarization. Nevertheless, our preliminary experiment revealed that depending solely on the topic prompt frequently leads to mostly identical summaries across topics. We further added a length control prompt that controls the length of the generated summaries based on the length of the reference summaries for each topic. While it was able to generate a more concise summary, the summaries across topics remained similar. To promote the model to produce concise yet diverse summaries across topics, we propose the use of contrastive learning on topic-length prompts, which make use of positive and negative pairs to enforce the models to learn the similarities and differences of different topics. Experimental results showed that our model outperformed other baseline models in the ROUGE and BERT scores on the DialogSum dataset. This result was reproduced in the MACSum dataset, and similar results were found. Our work is available at [anonymized]",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iSqdv1arH-": {
    "title": "Alignment for Honesty",
    "volume": "review",
    "abstract": "Recent research has made significant strides in aligning large language models (LLMs) with helpfulness and harmlessness. In this paper, we argue for the importance of alignment for \\emph{honesty}, ensuring that LLMs proactively refuse to answer questions when they lack knowledge, while still not being overly conservative. However, a pivotal aspect of alignment for honesty involves discerning an LLM's knowledge boundaries, which demands comprehensive solutions in terms of metric development, benchmark creation, and training methodologies. We address these challenges by first establishing a precise problem definition and defining ``honesty'' inspired by the Analects of Confucius. This serves as a cornerstone for developing metrics that effectively measure an LLM's honesty by quantifying its progress post-alignment. Furthermore, we introduce a flexible training framework which is further instantiated by several efficient fine-tuning techniques that emphasize honesty without sacrificing performance on other tasks. Our extensive experiments reveal that these aligned models show a marked increase in honesty, as indicated by our proposed metrics. We open-source all relevant resources to facilitate future research at \\url{https://anonymous.4open.science/r/alignment-for-honesty}",
    "checked": true,
    "id": "6cfbbf7604adda1df65932e3c4d157770a2df000",
    "semantic_title": "alignment for honesty",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=aZk8QtYanxL": {
    "title": "Training Language Models to Generate Text with Citations via Fine-grained Rewards",
    "volume": "review",
    "abstract": "While recent Large Language Models (LLMs) have proven useful in answering user queries, they are prone to hallucination, and their responses often lack credibility due to missing references to reliable sources. An intuitive solution to these issues would be to include in-text citations referring to external documents as evidence. While previous works have directly prompted LLMs to generate in-text citations, their performances are far from satisfactory, especially when it comes to smaller LLMs. In this work, we propose an effective training framework using fine-grained rewards to teach LLMs to generate highly supportive and relevant citations, while ensuring the correctness of their responses. We also conduct a systematic analysis of applying these fine-grained rewards to common LLM training strategies, demonstrating its advantage over conventional practices. We conduct extensive experiments on Question Answering (QA) datasets taken from the ALCE benchmark and validate the model's generalizability using EXPERTQA. On LLaMA-2-7B, the incorporation of fine-grained rewards achieves the best performance among the baselines, even surpassing that of GPT-3.5-turbo",
    "checked": true,
    "id": "fabaabcbca3c518c0b4fff2fea5dee8546181388",
    "semantic_title": "training language models to generate text with citations via fine-grained rewards",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=IJf8Jc67yls": {
    "title": "Bridging Theory and Practice in Multimodal Deep Learning: A Comprehensive Review in the Large Language Model Era",
    "volume": "review",
    "abstract": "In the past few years, the realm of deep learning has captivated widespread interest, with multimodal deep learning (MMDL) rising as an exceptionally promising area. MMDL specializes in processing and amalgamating data from varied communication channels, including text, speech, vision, and spatial indicators. This article delivers an exhaustive exploration of MMDL methodologies and their expansive applications. Furthermore, we delve into a detailed examination of diverse MMDL techniques, encapsulating the progression of model architectures, advancements in data augmentation, refresh methods, and optimization tactics. The main goal of this review is to tackle the pressing challenges and delineate the trajectory for future research in the dynamic field of deep learning, especially focusing on the era of Large Language Models (LLMs). We believe that this comprehensive review will greatly enhance the comprehension of MMDL and act as a crucial tool for researchers aiming to delve into new and promising research paths",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XzyQhEGXgKs": {
    "title": "Generative Models for Automatic Medical Decision Rule Extraction from Text",
    "volume": "review",
    "abstract": "Medical decision rules play a key role in many clinical decision support systems (CDSS). However, these rules are conventionally constructed by medical experts, which is expensive and hard to scale up. In this study, we explore the automatic extraction of medical decision rules from text, leading to a solution to construct large-scale medical decision rules. We adopt a formulation of medical decision rules as binary trees consisting of condition/decision nodes. Such trees are referred to as medical decision trees and we introduce several generative models extract them from text. The proposed models inherit the merit of two categories of successful natural language generation frameworks, i.e., sequence-to-sequence generation and autoregressive generation. To unleash the potential of pretrained language models, we design three styles of linearization (natural language, augmented natural language and JSON code), acting as the target sequence for our models. Our final system achieves 67% tree accuracy on a comprehensive benchmark, outperforming state-of-the-art discriminative baseline by 12% absolute value. This demonstrates the effectiveness of generative models on explicitly modeling structural decision-making roadmaps and boosts the development of CDSS as well as explainable AI",
    "checked": false,
    "id": "86afaa6b579297ffb61c6c6ee3b2bac2c5c4f561",
    "semantic_title": "2378. use of natural language processing to extract published real world data on a covid vaccine and antiviral treatment",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=BsZFQ-henXV": {
    "title": "Extending the Massive Text Embedding Benchmark to French",
    "volume": "review",
    "abstract": "In recent years, numerous embedding models have been made available and widely used for various NLP tasks. Choosing a model that performs well for several tasks in English has been largely simplified by the Massive Text Embedding Benchmark (MTEB), but extensions to other languages remain challenging.This is why we expand MTEB to propose the first massive benchmark of sentence embeddings for French.Not only we gather 22 existing datasets in an easy-to-use interface, but we also create three new French datasets for a global evaluation over 8 different tasks.We perform a large scale comparison with 46 carefully selected embedding models, conduct comprehensive statistical tests, and analyze the correlation between model performance and many of their characteristics.We find out that even if no model is the best on all tasks, large multilingual models pre-trained on sentence similarity perform particularly well. Our work comes with open-source code, new datasets and a public leaderboard",
    "checked": true,
    "id": "4d3f6696736887275550fc9fc34af1cc9291067d",
    "semantic_title": "extending the massive text embedding benchmark to french",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MAYuSQmCWjL": {
    "title": "Propagation and Pitfalls: Reasoning-based Assessment of Knowledge Editing through Counterfactual Tasks",
    "volume": "review",
    "abstract": "Current approaches of knowledge editing struggle to effectively propagate updates to interconnected facts. In this work, we delve into the barriers that hinder the appropriate propagation of updated knowledge within these models for accurate reasoning. To support our analysis, we introduce a novel reasoning-based benchmark --ReCoE (Reasoning-based Counterfactual Editing dataset) -- which covers six common reasoning schemes in real world. We conduct a thorough analysis of existing knowledge editing techniques, including input-augmentation, finetuning, and locate-and-edit. We found that all model editing methods show notably low performance on this dataset, especially in certain reasoning schemes. Our analysis over the chain-of-thought generation of edited models further uncover key reasons behind the inadequacy of existing knowledge editing methods from a reasoning standpoint, involving aspects on fact-wise editing, fact recall ability, and coherence in generation. We will make our benchmark publicly available",
    "checked": true,
    "id": "465f76343f5ca61a088d97623f5fe73e5ca7e630",
    "semantic_title": "propagation and pitfalls: reasoning-based assessment of knowledge editing through counterfactual tasks",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=hGjqiNeTRJP": {
    "title": "Large Language Models for Generative Information Extraction: A Survey",
    "volume": "review",
    "abstract": "Information extraction (IE) aims to extract structural knowledge from plain natural language texts. Recently, generative Large Language Models (LLMs) have demonstrated remarkable capabilities in text understanding and generation. As a result, numerous works have been proposed to integrate LLMs for IE tasks based on a generative paradigm. To conduct a comprehensive systematic review and exploration of LLM efforts for IE tasks, in this study, we survey the most recent advancements in this field. We first present an extensive overview by categorizing these works in terms of various IE subtasks and learning paradigms, and then we empirically analyze the most advanced methods and discover the emerging trend of IE tasks with LLMs. Based on a thorough review conducted, we identify several insights in technique and promising research directions that deserve further exploration in future studies. We will maintain a public repository and consistently update related resources",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4lMwEJ-49yg": {
    "title": "A Modular Approach for Multimodal Summarization of TV Shows",
    "volume": "review",
    "abstract": "In this paper we address the task of summarizing television shows which touches key areas in AI research: complex reasoning, multiple modalities, and long narratives. We present a modular approach where separate components perform specialized sub-tasks which we argue affords greater flexibility compared to end-to-end methods. Our modules involve detecting scene boundaries, reordering scenes so as to minimize the number of cuts between different events, converting visual information to text, summarizing the dialogue in each scene, and fusing the scene summaries into a final summary for the entire episode. We also present a new metric, PREFS (\\textbf{P}recision and \\textbf{R}ecall \\textbf{E}valuation of Summary \\textbf{F}act\\textbf{s}), to measure both precision and recall of generated summaries, which we decompose into atomic facts. Tested on the recently released SummScreen3D dataset (Papalampidi and Lapata, 2023), our method produces higher quality summaries than comparison models, as measured with ROUGE and our new fact-based metric",
    "checked": true,
    "id": "5c13fda8ea2252be7087162a03462149bbba55a9",
    "semantic_title": "a modular approach for multimodal summarization of tv shows",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ihrVaGlJbej": {
    "title": "Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation",
    "volume": "review",
    "abstract": "We introduce Bonito, an open-source model for conditional task generation: the task of converting unannotated text into task-specific training datasets for instruction tuning. Our goal is to enable zero-shot task adaptation of large language models on users' specialized, private data. We train Bonito on a new large-scale dataset with 1.65M examples created by remixing existing instruction tuning datasets into meta-templates. The meta-templates for a dataset produce training examples where the input is the unannotated text and the task attribute and the output consists of the instruction and the response. We use Bonito to generate synthetic tasks for seven datasets from specialized domains across three task types---yes-no question answering, extractive question answering, and natural language inference---and adapt language models. We show that Bonito significantly improves the average performance of pretrained and instruction tuned models over the de facto self supervised baseline. For example, adapting Mistral-Instruct-v2 and instruction tuned variants of Mistral and Llama2 with Bonito improves the strong zero-shot performance by 22.1 F1 points whereas the next word prediction objective undoes some of the benefits of instruction tuning and reduces the average performance by 0.8 F1 points. We conduct additional experiments with Bonito to understand the effects of the domain, the size of the training set, and the choice of alternative synthetic task generators. Overall, we show that learning with synthetic instruction tuning datasets is an effective way to adapt language models to new domains",
    "checked": true,
    "id": "0ec88f8071d4a55e62a1b85661c1f11a01489047",
    "semantic_title": "learning to generate instruction tuning datasets for zero-shot task adaptation",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=XU3g_lTAOYu": {
    "title": "PARADE: Parameter-Efficient Fine-tuning with Prompt Aware Representation ADjustmEnt",
    "volume": "review",
    "abstract": "Despite the presence of many competitive PEFT methods like LoRA, we still need a PEFT method that is efficient under the single-backbone multi-tenant setting while performing competitively in the downstream tasks. In this work, we propose a novel PEFT method, \\underline{P}rompt \\underline{A}ware \\underline{R}epresentation \\underline{AD}justm\\underline{E}nt (PARADE). First, we propose to install a lightweight vector generator at each Transformer layer to generate vectors that will modify the hidden states in the multi-head self-attention (MHSA) and position-wise feed-forward (FFN) modules and, as a result, modulate the behaviors of the pre-trained backbone. Second, the vector generators are modules with a bottleneck architecture consisting of a pooling operation, two linear projections, and an activation function. To enhance the downstream performance of vector generators, we propose an attention-based capsule network as the pooling operation, which can effectively summarize the semantic information in the input instructions. We have conducted experiments on various tasks, and the experimental results demonstrate that: (a) our PARADE method can outperform the recent baselines with comparable tunable parameters. (b) Our PARADE method is more efficient than LoRA under the single-backbone multi-tenant setting.\\footnote{Codes and fine-tuned models will be open-sourced to facilitate future research. }",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YiI9p5VQQiD": {
    "title": "Competition-Level Problems are Effective LLM Evaluators",
    "volume": "review",
    "abstract": "Large language models (LLMs) have demonstrated impressive reasoning capabilities, yet there is ongoing debate about these abilities and the potential data contamination problem recently. This paper aims to evaluate the reasoning capacities of LLMs, specifically in solving recent competition-level programming problems in Codeforces, which are expert-crafted and unique, requiring deep understanding and robust reasoning skills. We first provide a comprehensive evaluation of GPT-4's perceived zero-shot performance on this task, considering various aspects such as problems' release time, difficulties, and types of errors encountered. Surprisingly, the perceived performance of GPT-4 has experienced a cliff like decline in problems after September 2021 consistently across all the difficulties and types of problems, which shows the potential data contamination, as well as the challenges for any existing LLM to solve unseen complex reasoning problems. We further explore various approaches such as fine-tuning, Chain-of-Thought prompting and problem description simplification, unfortunately none of them is able to consistently mitigate the challenges. Through our work, we emphasize the importance of this excellent data source for assessing the genuine reasoning capabilities of LLMs, and foster the development of LLMs with stronger reasoning abilities and better generalization in the future",
    "checked": true,
    "id": "73f02529d1878977d24ee0a906ad2b48a5ba022f",
    "semantic_title": "competition-level problems are effective llm evaluators",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=INYvSbkGImw": {
    "title": "InstructERC: Reforming Emotion Recognition in Conversation with a Multi-task Retrieval-based LLMs Framework",
    "volume": "review",
    "abstract": "The field of emotion recognition of conversation (ERC) has been focusing on separating sentence feature encoding and context modeling, lacking exploration in generative paradigms based on unified designs.In this study, we propose a novel approach, \\textbf{InstructERC}, to reformulate the ERC task from a discriminative framework to a generative framework based on Large Language Models (LLMs). InstructERC makes three significant contributions: (1) it introduces a simple yet effective retrieval template module, which helps the model explicitly integrate multi-granularity dialogue supervision information.(2) We introduce two additional emotion alignment tasks, namely speaker identification and emotion prediction tasks, to implicitly model the dialogue role relationships and future emotional tendencies in conversations.(3) Pioneeringly, we unify emotion labels across benchmarks through the feeling wheel to fit real application scenarios. InstructERC still perform impressively on this unified dataset.Our LLM-based plugin framework significantly outperforms all previous models and achieves comprehensive SOTA on three commonly used ERC datasets. Extensive analysis of parameter-efficient and data-scaling experiments provides empirical guidance for applying it in practical scenarios.Our code and aligned unified dataset (UIME) can be found in the supplementary material",
    "checked": false,
    "id": "3506b3be3b2472adb748e5c2cc57c200d403d7b5",
    "semantic_title": "instructerc: reforming emotion recognition in conversation with a retrieval multi-task llms framework",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=PeAvVEkVz1R": {
    "title": "Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated a powerful ability for text generation. However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired behaviors such as toxicity or hallucinations can manifest. While much larger models (e.g., ChatGPT) may demonstrate strength in mitigating these issues, there is still no guarantee of complete prevention. In this work, we propose formalizing text generation as a future-constrained generation problem to minimize undesirable behaviors and enforce faithfulness to instructions. The estimation of future constraint satisfaction, accomplished using LLMs, guides the text generation process. Our extensive experiments demonstrate the effectiveness of the proposed approach across three distinct text generation tasks: keyword-constrained generation (Lin et al., 2020), toxicity reduction (Gehman et al., 2020), and factual correctness in question-answering (Gao et al., 2023)",
    "checked": true,
    "id": "daa171a25956b537b222a564c1488b2b6cfbb6bb",
    "semantic_title": "unlocking anticipatory text generation: a constrained approach for large language models decoding",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E34F0m7sggm": {
    "title": "Enhancing Legal Case Retrieval via Scaling High-quality Asymmetric Query-Candidate Pairs",
    "volume": "review",
    "abstract": "Legal case retrieval (LCR) aims to provide similar cases as references for a given fact description. This task is crucial for promoting consistent judgments in similar cases, effectively enhancing judicial fairness and improving work efficiency for judges. However, existing works face two main challenges for real-world applications: existing works mainly focus on case-to-case retrieval using lengthy queries, which does not match real-world scenarios; and the limited data scale, with current datasets containing only hundreds of queries, is insufficient to satisfy the training requirements of existing data-hungry neural models. To address these issues, we introduce an automated method to construct asymmetrically query-candidate pairs and construct the largest LCR dataset to date, LEAD, which is hundreds of times larger than existing datasets. This dataset can provide ample training signals for LCR models. Experimental results demonstrate that models training with LEAD can achieve state-of-the-art results on two widely used LCR benchmarks. Besides, the construction method can be also applied to civil cases and achieve promising results. The code and dataset used in this paper will be released to promote the development of LCR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GkdATXOAi9l": {
    "title": "Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models",
    "volume": "review",
    "abstract": "Model editing aims to precisely modify the behaviours of large language models (LLMs) on specific knowledge while keeping irrelevant knowledge unchanged. It has been proven effective in resolving hallucination and out-of-date issues in LLMs. As a result, it can boost the application of LLMs in many critical domains (e.g., medical domain), where the hallucination is not tolerable. In this paper, we propose two model editing studies and validate them in the medical domain: (1) directly editing the factual medical knowledge and (2) editing the explanations to facts. Meanwhile, we observed that current model editing methods struggle with the specialization and complexity of medical knowledge. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. It employs causal tracing to identify the precise location of knowledge in neurons and then introduces scalable adapters into the dense layers of LLMs. These adapters are assigned scaling values based on the corresponding specific knowledge. To evaluate the editing impact, we build two benchmark datasets and introduce a series of challenging and comprehensive metrics. Extensive experiments on medical LLMs demonstrate the editing efficiency of MedLaSA, without affecting irrelevant knowledge that is not edited",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WwiGfB-ZssP": {
    "title": "Auditing Counterfire: Evaluating Advanced Counterargument Generation with Evidence and Style",
    "volume": "review",
    "abstract": "We audited counter-arguments generated by large language models (LLMs), focusing on their ability to generate evidence-based and stylistic counter-arguments to posts from the Reddit ChangeMyView dataset. Our evaluation is based on Counterfire: a new dataset of 32,000 counter-arguments generated from large language models (LLMs): GPT-3.5 Turbo and Koala and their fine-tuned variants, and PaLM 2, with varying prompts for evidence use and argumentative style. GPT-3.5 Turbo ranked highest in argument quality with strong paraphrasing and style adherence, particularly in `reciprocity' style arguments. However, the `No Style' counter-arguments proved most persuasive on average. The findings suggest that a balance between evidentiality and stylistic elements is key to an effective counter-argument. We close with a discussion of future research directions and implications for fine-tuning LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r07wJJzMrD1": {
    "title": "Stealthy Attack on Large Language Model based Recommendation",
    "volume": "review",
    "abstract": "Recently, the powerful large language models (LLMs) have been instrumental in propelling the progress of recommender systems (RS). However, while these systems have flourished, their susceptibility to security threats has been largely overlooked. In this work, we reveal that the introduction of LLMs into recommendation models presents new security vulnerabilities due to their emphasis on the textual content of items. We demonstrate that attackers can significantly boost an item's exposure by merely altering its textual content during the testing phase, without requiring direct interference with the model's training process. Additionally, the attack is notably stealthy, as it does not affect the overall recommendation performance and the modifications to the text are subtle, making it difficult for users and platforms to detect. Our comprehensive experiments across four mainstream LLM-based recommendation models demonstrate the superior efficacy and stealthiness of our approach. Our work unveils a significant security gap in LLM-based recommendation systems and paves the way for future research on protecting these systems",
    "checked": true,
    "id": "47e221157a7d6f76504e5e8efa07062550c2b734",
    "semantic_title": "stealthy attack on large language model based recommendation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=2RHBEH-dhJ_": {
    "title": "Explanation for Machine Translation Errors: Generation and Evaluation",
    "volume": "review",
    "abstract": "The fine-grained annotations of translation errors have been widely applied in machine translation researches such as translation quality estimation, designing automatic evaluation metrics, but these annotations only contain information such as error type, location, and severity, the reasons of the errors are not annotated. Since explaining why an annotated text span is erroneous is important for building the trustworthy machine translation models, we manually build the first resource for evaluating the quality of the explanation for the errors. We tested large language models (LLMs) on this evaluation resource, and found that LLMs failed to deliver trustworthy explanations for the machine translation errors. So, we propose a hard chain-of-thought (H-CoT) approach that induces the explanation for the errors step-by-step via hard chains. Experiments on the evaluation resource show that H-CoT greatly improves the explanation quality over LLMs without H-CoT",
    "checked": false,
    "id": "228ac408943b38cb732ac87023f415100110c9e6",
    "semantic_title": "xtower: a multilingual llm for explaining and correcting translation errors",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4iqsfXxvFm": {
    "title": "$\\texttt{DARA}$: Decomposition-Alignment-Reasoning Autonomous Language Agent for Question Answering over Knowledge Graphs",
    "volume": "review",
    "abstract": "Answering Questions over Knowledge Graphs (KGQA) is key to well-functioning autonomous language agents in various real-life applications. To improve the neural-symbolic reasoning capabilities of language agents powered by Large Language Models (LLMs) in KGQA, we propose the Decomposition-Alignment-Reasoning Agent (DARA) framework. DARA effectively parses questions into formal queries through a dual mechanism: high-level iterative task decomposition and low-level task grounding. Importantly, DARA can be efficiently trained with a small number of high-quality reasoning trajectories. Our experimental results demonstrate that DARA fine-tuned on LLMs (e.g. Llama-2-7B, Mistral) outperforms both in-context learning-based agents with GPT-4 and alternative fine-tuned agents, across different benchmarks, making such models more accessible for real-life applications. We also show that DARA attains performance comparable to state-of-the-art enumerating-and-ranking-based methods for KGQA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oM2EorbKk1": {
    "title": "Style-Unified Meta-In-Context Learning: Improving In-Context Learning Ability by Learning to Unify Output Styles",
    "volume": "review",
    "abstract": "This paper proposes a style-unified meta-in-context learning that enhances In-Context Learning (ICL) ability for language models by learning to unify the output styles. Meta-training for ICL (MetaICL), a method that learns ICL ability for enhancing to follow a few in-context examples, has been proposed. However, the language models trained with MetaICL might not be able to consider information obtained from in-context examples at inference because it is reported that the performance is unaffected when random or flipped outputs are used in a few in-context examples. Our key idea for using in-context information is explicitly giving a relationship between outputs in context and a target output by unifying the output style. Specifically, arbitrary symbols (e.g., integer or word) are inserted into the outputs in context, and we expect the model to focus on examples by learning to output the same symbols at the same positions. To evaluate the proposed method, we create a Japanese dataset containing multiple examples per task. Experiments using a 0.6B Japanese language model demonstrate that the proposed method outperforms the conventional method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=khQScJjF-E": {
    "title": "SCALE: Synergized Collaboration of Asymmetric Language Translation Engines",
    "volume": "review",
    "abstract": "In this paper, we introduce SCALE, a collaborative framework that connects a compact Specialized Translation Model (STM) and a general-purpose Large Language Model (LLM) as one unified translation engine. By introducing translation from STM into the triplet in-context demonstrations, SCALE unlocks refinement and pivoting ability of LLM, thus 1) mitigating language bias of LLMs and parallel data bias of STMs, 2) enhancing LLM speciality without sacrificing generality, and 3) facilitating continual learning in a LLM-tuning-free way.Our comprehensive experiments show that SCALE significantly outperforms both LLMs (GPT-4, GPT-3.5) and supervised models (NLLB, M2M) in either high-resource or challenging low-resource settings. Moreover SCALE shows great scalability by only updating the lightweight STM and witness consistent system improvement, an averaged 4 BLEURT score across 4 languages without tuning LLM. Interestingly, SCALE could also effectively exploit the existing language bias of LLMs by using an English-centric STM as a pivot to conduct translation between any language pairs, outperforming GPT-4 by an average of 6 COMET points across eight translation directions. Furthermore we provide an in-depth analysis of SCALE's robustness, translation characteristics, latency costs and inherent language bias, providing solid foundation for future studies exploring the potential synergy between LLMs and more specialized models",
    "checked": true,
    "id": "e6fd045dc7fdb79dcef2f71e38c0edeb1b11ee0d",
    "semantic_title": "scale: synergized collaboration of asymmetric language translation engines",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=voMLbH82JA": {
    "title": "Synergetic Event Understanding: A Collaborative Approach to Cross-Document Event Coreference Resolution with Large Language Models",
    "volume": "review",
    "abstract": "Cross-document event coreference resolution (CDECR) involves clustering event mentions across multiple documents that refer to the same real-world events. Existing approaches utilize fine-tuning of small language models (SLMs) like BERT to address the compatibility among the contexts of event mentions. However, due to the complexity and diversity of contexts, these models are prone to learning simple co-occurrences. Recently, large language models (LLMs) like ChatGPT have demonstrated impressive contextual understanding, yet they encounter challenges in adapting to specific information extraction (IE) tasks. In this paper, we propose a collaborative approach for CDECR, leveraging the capabilities of both a universally capable LLM and a task-specific SLM. The collaborative strategy begins with the LLM accurately and comprehensively summarizing events through prompting. Then, the SLM refines its learning of event representations based on these insights during fine-tuning. Experimental results demonstrate that our approach surpasses the performance of both the large and small language models individually, forming a complementary advantage. Across various datasets, our approach achieves state-of-the-art performance, underscoring its effectiveness in diverse scenarios",
    "checked": true,
    "id": "c3a469cced443c061f939597f2b44591c7e39a00",
    "semantic_title": "synergetic event understanding: a collaborative approach to cross-document event coreference resolution with large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NHqIicndg7": {
    "title": "PACE: Improving Prompt with Actor-Critic Editing for Large Language Model",
    "volume": "review",
    "abstract": "Large language models (LLMs) have showcased remarkable potential across various tasks by conditioning on prompts. However, the quality of different human-written prompts leads to substantial discrepancies in LLMs' performance, and improving prompts usually necessitates considerable human effort and expertise. To this end, this paper proposes Prompt with Actor-Critic Editing (PACE) for LLMs to enable automatic prompt editing. Drawing inspiration from the actor-critic algorithm in reinforcement learning, PACE leverages LLMs as the dual roles of actors and critics, conceptualizing prompt as a type of policy. PACE refines prompt, taking into account the feedback from both actors performing prompt and critics criticizing response. This process helps LLMs better align prompt to a specific task, thanks to real responses and thinking from LLMs.We conduct extensive experiments on 24 instruction induction tasks and 21 big-bench tasks. Experimental results indicate that PACE elevates the relative performance of medium/low-quality human-written prompts by up to 98\\%, which has comparable performance to high-quality human-written prompts. Moreover, PACE also exhibits notable efficacy for prompt generation",
    "checked": true,
    "id": "d8ecaac57de593207c85461ccfc7d10c8807d78d",
    "semantic_title": "pace: improving prompt with actor-critic editing for large language model",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=R11UZI9fXX": {
    "title": "Improving Complex Reasoning over Knowledge Graph with Logic-Aware Curriculum Tuning",
    "volume": "review",
    "abstract": "Answering complex logical queries over incomplete knowledge graphs (KGs) is challenging. Most previous works have focused on learning entity/relation embeddings and simulating first-order logic operators with various neural networks. However, they are bottlenecked by the inability to share world knowledge to improve logical reasoning, thus resulting in suboptimal performance. In this paper, we propose a complex logical reasoning schema over knowledge graphs upon large language models (LLMs), containing a curriculum-based logical-aware instruction tuning framework, named LACT. Specifically, we augment the arbitrary first-order logical queries via binary tree decomposition, to stimulate the reasoning capability of LLMs. To address the difficulty gap among different types of complex queries, we design a simple and flexible logic-aware curriculum learning framework. Experiments across widely used datasets demonstrate that LACT has substantial improvements (brings an average +5.5% MRR score) over advanced methods, achieving the new state-of-the-art",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H5gRi4RvAQ": {
    "title": "Video-Language Understanding: A Survey from Model Architecture, Model Training, and Data Perspectives",
    "volume": "review",
    "abstract": "Humans use multiple senses to comprehend the environment. Vision and language are two of the most vital senses since they allow us to easily communicate our thoughts and perceive the world around us. There has been a lot of interest in creating video-language understanding systems with human-like senses since a video-language pair can mimic both our linguistic medium and visual environment with temporal dynamics. In this survey, we review the key tasks of these systems and highlight the associated challenges. Based on the challenges, we summarize their methods from model architecture, model training, and data perspectives. We also conduct performance comparison among the methods, and discuss promising directions for future research",
    "checked": true,
    "id": "ef6d3fbd97ba38974d31d0a8789a159772c73ed1",
    "semantic_title": "video-language understanding: a survey from model architecture, model training, and data perspectives",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=OXdXMkDtfa": {
    "title": "MUSE: Mutual Learning with Distilling Lightweight Students for Semi-Supervised Text Mining",
    "volume": "review",
    "abstract": "The semi-supervised learning strategy in lightweight models requires reducing annotated samples and facilitating cost-effective inference. However, the constraint on model parameters, imposed by the scarcity of training labels, introduces a trade-off that can potentially limit the inferential performance of the model. In this paper, we introduce MUSE, a mutual learning framework tailored for semi-supervised text mining with lightweight cohorts. MUSE incorporates an online distillation procedure to obtain lightweight student cohorts by mimicking the Teacher. Furthermore, MUSE encompasses an ensemble of lightweight student cohorts that collaborate and reciprocally instruct each other, thereby refining the optimization trajectories for enhanced robustness. Our MUSE equipped with a 2-layer distilled BERT, exhibits a substantial average performance improvement over SOTA lightweight SSL frameworks of FLiText and DisCo by 13.25% and 9.56% in text classification tasks with extremely limited labeled data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7BhjqMfwTm": {
    "title": "Small Empowering Large: Leverage Performance and Efficiency by Exploring the Cooperation of LLMs",
    "volume": "review",
    "abstract": "The combined use of Large Language Models (LLMs) and Information Retrieval (IR) has made significant progress in solving the multi-hop QA problem. However, achieving high performance requires increasingly complex and interactive integration of IR and 'large' LLMs, which poses challenges to efficiency and domain specialization capabilities. A specifically fine-tuned 'small' LLM, such as LlaMa-7B, presents a viable solution to this challenge.Nevertheless, addressing the challenges entails considering two aspects: 1) Where Problem: identifying the phases in which employing a 'small' LLMs is most beneficial is essential. 2) How Problem: devising effective strategies for combining 'small' and 'large' LLMs is necessary.A lightweight approach is proposed where the 'large' LLMs service and a specifically fine-tuned 'small' LLMs cooperate to answer the multi-hop questions. Our research reveals that the 'large' LLMs service primarily handles top-level planning, while the fine-tuned 'small' LLMs is tasked with generating answers and rectifying any inconsistencies with the retrieved information. Experimental results on the HotPotQA dataset demonstrate that our proposed method achieves comparable accuracies with significantly reduced costs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=deTWMJE8QU": {
    "title": "Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages",
    "volume": "review",
    "abstract": "While large language models (LLMs) have been pre-trained on multilingual corpora, their performance still lags behind in most languages compared to a few resource-rich languages. One common approach to mitigate this issue is to translate training data from resource-rich languages into other languages and then continue training. However, using the data obtained solely relying on translation while ignoring the original capabilities of LLMs across languages is not always effective, which we show will limit the performance of cross-lingual knowledge transfer. In this work, we propose SDRRL, a method based on Self-Distillation from Resource-Rich Languages that effectively improve multilingual performance by leveraging the internal capabilities of LLMs on resource-rich languages. We evaluate on different LLMs (LLaMA-2 and SeaLLM) and source languages (English and French) across various comprehension and generation tasks, experimental results demonstrate that SDRRL can significantly enhance multilingual capabilities while minimizing the impact on original performance in resource-rich languages",
    "checked": true,
    "id": "5e5f04a5ab1d22ffad0e96585469e269369ec676",
    "semantic_title": "enhancing multilingual capabilities of large language models through self-distillation from resource-rich languages",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oGwG9RKpZg": {
    "title": "Towards A Unified View of Answer Calibration for Multi-Step Reasoning",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have broadened the scope for improving multi-step reasoning capabilities. We generally divide multi-step reasoning into two phases: *path generation* to generate the reasoning path(s); and *answer calibration* post-processing the reasoning path(s) to obtain a final answer. However, the existing literature lacks systematic analysis on different answer calibration approaches. In this paper, we summarize the taxonomy of recent answer calibration techniques and break them down into step-level and path-level strategies. We then conduct a thorough evaluation on these strategies from a unified view, systematically scrutinizing step-level and path-level answer calibration across multiple paths. Experimental results reveal that integrating the dominance of both strategies tends to derive optimal outcomes. Our study holds the potential to illuminate key insights for optimizing multi-step reasoning with answer calibration",
    "checked": true,
    "id": "03f3801956fc4cc026860568670f9f65ed29b192",
    "semantic_title": "towards a unified view of answer calibration for multi-step reasoning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=arYneJTtHg8": {
    "title": "SpeechAgents: Human-Communication Simulation with Multi-Modal Multi-Agent Systems",
    "volume": "review",
    "abstract": "Human communication is a complex and diverse process that not only involves multiplefactors such as language, commonsense, andcultural backgrounds but also requires the participation of multimodal information, such asspeech. Large Language Model (LLM)-basedmulti-agent systems have demonstrated promising performance in simulating human society.Can we leverage LLM-based multi-agent systems to simulate human communication? However, current LLM-based multi-agent systemsmainly rely on text as the primary medium.In this paper, we propose SpeechAgents, amulti-modal LLM based multi-agent systemdesigned for simulating human communication. SpeechAgents utilizes multi-modal LLMas the control center for individual agent andemployes multi-modal signals as the mediumfor exchanged messages among agents. Additionally, we propose Multi-Agent Tuning toenhance the multi-agent capabilities of LLMwithout compromising general abilities. Tostrengthen and evaluate the effectiveness ofhuman communication simulation, we buildthe Human-Communication Simulation Benchmark. Experimental results demonstrate thatSpeechAgents can simulate human communication dialogues with consistent content, authentic rhythm, and rich emotions and demonstrateexcellent scalability even with up to 25 agents,which can apply to tasks such as drama creationand audio novels generation. Demos are available at https://speechagents.github.io/",
    "checked": true,
    "id": "f04e58782380383f4edaaa899fd85bfb3d20c2e1",
    "semantic_title": "speechagents: human-communication simulation with multi-modal multi-agent systems",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mbanOyQszUB": {
    "title": "Faithful Chart Summarization with ChaTS-Pi",
    "volume": "review",
    "abstract": "Chart-to-summary generation can help explore data, communicate insights, and help the visually impaired people. Multi-modal generative models have been used to produce fluent summaries, but they can suffer from factual and perceptual errors. In this work we present CHATS-CRITIC, a reference-free chart summarization metric for scoring faithfulness. CHATS-CRITIC is composed of an image-to-text model to recover the table from a chart, and a tabular entailment model applied to score the summary sentence by sentence. We find that CHATS-CRITIC evaluates the summary quality according to human ratings better than reference-based metrics, either learned or n-gram based, and can be further used to fix candidate summaries by removing not supported sentences. We then introduce CHATS-PI, a chart-to-summary pipeline that leverages CHATS-CRITIC during inference to fix and rank sampled candidates from any chart-summarization model. We evaluate CHATS-PI and CHATS-CRITIC using human raters, establishing state-of-the-art results on two popular chart-to-summary datasets",
    "checked": true,
    "id": "8bf2f1b03d657256679863e2772fadfed34067aa",
    "semantic_title": "faithful chart summarization with chats-pi",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3pxR0P2FTh": {
    "title": "TextGenSHAP: Scalable Post-hoc Explanations in Text Generation with Long Documents",
    "volume": "review",
    "abstract": "Large language models (LLMs) have attracted great interest in many real-world applications given their increasingly accurate responses and coherent reasoning abilities. On the other hand, because of their complex and black-box nature, the demand for scalable and faithful explanations for LLM-generated outputs continues to grow. Explainability methods for deep learning, especially the well-respected Shapley value, have matured significantly over the past decade. However, there are major challenges in extending Shapley values to LLMs, particularly when dealing with long input contexts (containing thousands of tokens) and considering autoregressive generation of output sequences. In this paper, we introduce TextGenSHAP, an efficient post-hoc explanation method incorporating LLM-specific techniques. We demonstrate that this leads to significant runtime improvements compared to conventional Shapley value computations, reducing runtime from hours to minutes for token-level explanations, and to just seconds for document-level explanations. We then demonstrate how such explanations can improve end-to-end performance of retrieval augmented generation, localizing important words from within long documents, and reranking passages collected by retrieval systems. In open-domain question answering on NQ Open and MIRACL, TextGenSHAP improves the recall of document retrieval systems by multiple points and closes the accuracy gap of open-domain question answering with a 5-10\\% point improvement",
    "checked": true,
    "id": "c3ec2a49c5a09308a496ea21424dcd3eb812a754",
    "semantic_title": "textgenshap: scalable post-hoc explanations in text generation with long documents",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=AOl05vk787R": {
    "title": "Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering",
    "volume": "review",
    "abstract": "Mitigating the hallucinations of Large Language Models (LLMs) and enhancing them is a crucial task. Although some existing methods employ model self-enhancement techniques, they fall short of effectively addressing unknown factual hallucinations. Using Knowledge Graph (KG) enhancement approaches fails to address the generalization across different KG sources and the enhancement of open-ended answer questions simultaneously. To tackle these limitations, there is a framework that combines \\textbf{Pseudo-Graph Generation} and \\textbf{Atomic Knowledge Verification} proposed. The enhancement of LLM using KG in an open-ended question-answering setting is implemented by leveraging the Pseudo-Graph Generation. Atomic Knowledge Verification utilizes atomic-level knowledge querying and verification to achieve generalizability under different KG sources. Compared to the baseline, this approach yields a minimum improvement of 11.5 in the ROUGE-L score for open-ended questions. For precise questions, we observe a minimum accuracy improvement of 7.5. Moreover, there is also demonstration that this framework exhibits generalizability across different KG sources. In summary, our results pave the way for enhancing LLMs by incorporating Pseudo- and Multisource-KGs, particularly in the context of open-ended questions",
    "checked": true,
    "id": "c54675e2fbec984c18f7e61de83bca919aa811d6",
    "semantic_title": "enhancing large language models with pseudo- and multisource- knowledge graphs for open-ended question answering",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=KCHcCWP2iE": {
    "title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
    "volume": "review",
    "abstract": "Recent advances in large language models (LLMs) have revolutionized the landscape of reasoning tasks. To enhance the capabilities of LLMs to emulate human reasoning, prior studies have focused on modeling reasoning steps using various thought structures like chains, trees, or graphs. However, LLM-based reasoning still encounters the following challenges: (1) Limited adaptability of preset structures to diverse tasks; (2) Insufficient precision in exploiting known conditions to derive new ones; and (3) Inadequate consideration of historical reasoning experiences for subsequent reasoning steps. To this end, we propose DetermLR, a novel perspective that rethinks the reasoning process as an evolution from indeterminacy to determinacy. First, we categorize known conditions into two types: determinate and indeterminate premises, facilitating the transformation process. Subsequently, we leverage quantitative measurements to prioritize more relevant premises to explore new insights. Furthermore, we automate the storage and extraction of available premises and reasoning paths with reasoning memory, preserving historical reasoning details for subsequent reasoning steps. Comprehensive experimental results demonstrate that DetermLR surpasses all baselines on various logical reasoning benchmarks: LogiQA, ProofWriter, FOLIO, PrOntoQA, and LogicalDeduction. Compared to previous multi-step reasoning methods, DetermLR achieves higher accuracy with fewer reasoning steps, highlighting its superior efficiency and effectiveness in solving logical reasoning tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UBc-puh1EnQ": {
    "title": "Text2MDT: Extracting Decision Trees from Medical Texts Using Large Language Models",
    "volume": "review",
    "abstract": "Knowledge of the medical decision process, which can be modeled as medical decision trees (MDTs), is critical to building clinical decision support systems. However, the current MDT construction methods rely heavily on time-consuming and laborious manual annotation. In this work, we propose a novel task, Text2MDT, to explore the automatic extraction of MDTs from medical texts such as medical guidelines and textbooks. We normalized the form of the MDT and created an annotated Text2MDT dataset in Chinese with the participation of medical experts. We investigate two different methods for the Text2MDT tasks: (a) an end-to-end framework that only relies on a GPT style large language models (LLM) instruction tuning to generate all the node information and tree structures. (b) The pipeline framework decomposes the Text2MDT task into three subtasks. Experiments on our Text2MDT dataset demonstrate that (a) the end-to-end method based on LLMs (7B parameters or larger) shows promising results and successfully outperforms the pipeline methods. (b) The chain-of-thought (COT) prompting method \\cite{Wei2022ChainOT} can improve the performance of the fine-tuned LLMs on the Text2MDT test set. (c) the lightweight pipelined method based on encoder-based pre-trained models also performs well with LLMs with model complexity two magnitudes smaller.\\footnote{Our Text2MDT dataset and the source codes are open-sourced, and we will make the dataset and the source codes openly available upon acceptance. }",
    "checked": false,
    "id": "8358271ab8f04a3432880e33997174a1abf028a9",
    "semantic_title": "causality extraction from medical text using large language models (llms)",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cPbrk4lwJX": {
    "title": "Human Simulacra: A Step toward the Personification of Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) are recognized as systems that closely mimic aspects of human intelligence. This capability has attracted attention from the social science community, who see the potential in leveraging LLMs to replace human participants in experiments, thereby reducing research costs and complexity. In this paper, we introduce a framework for large language models personification, including a strategy for constructing virtual characters' life stories from the ground up, a Multi-Agent Cognitive Mechanism capable of simulating human cognitive processes, and a psychology-guided evaluation method to assess human simulations from both self and observational perspectives. Experimental results demonstrate that our constructed simulacra can produce personified responses that align with their target characters. Our work is a preliminary exploration which offers great potential in practical applications. All the code and datasets will be released, with the hope of inspiring further investigations",
    "checked": true,
    "id": "17e0de89f6e8cacfc13f543da22b0f3cb611a2c7",
    "semantic_title": "human simulacra: a step toward the personification of large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bl84oQzHg22": {
    "title": "Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of Machine Cognition",
    "volume": "review",
    "abstract": "Recent advances in LLMs have sparked a debate on whether they understand text. In this position paper, we argue that opponents in this debate hold different definitions for understanding, and particularly differ in their view on the role of consciousness. To substantiate this claim, we propose a thought experiment involving an open-source chatbot Z which excels on every possible benchmark, seemingly without subjective experience. We ask whether Z is capable of understanding, and show that different schools of thought within seminal AI research seem to answer this question differently, uncovering their terminological disagreement. Moving forward, we propose two distinct working definitions for understanding which explicitly acknowledge the question of consciousness, and draw connections with a rich literature in philosophy, psychology and neuroscience",
    "checked": true,
    "id": "7b76201b2c4730aba93c845d335247e36d1d9577",
    "semantic_title": "do zombies understand? a choose-your-own-adventure exploration of machine cognition",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=zVR5JN2IPYD": {
    "title": "LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization",
    "volume": "review",
    "abstract": "Pretrained language models (PLMs) have become remarkably adept at task and language generalization. Nonetheless, they often fail dramatically when faced with unseen languages, posing a significant problem for diversity and equal access to PLM technology. In this work, we present LinguAlchemy, a regularization technique that incorporates various aspects of languages covering typological, geographical, and phylogenetic constraining the resulting representation of PLMs to better characterize the corresponding linguistics constraints. LinguAlchemy significantly improves the accuracy performance of mBERT and XLM-R on unseen languages by ~18% and ~2%, respectively compared to fully fine-tuned models and displaying a high degree of unseen language generalization. We further introduce AlchemyScale and AlchemyTune, extension of LinguAlchemy which adjusts the linguistic regularization weights automatically, alleviating the need for hyperparameter search. LinguAlchemy enables better cross-lingual generalization to unseen languages which is vital for better inclusivity and accessibility of PLMs",
    "checked": true,
    "id": "e397cac3f38aba116cc623bdf1a6d638537f5e29",
    "semantic_title": "lingualchemy: fusing typological and geographical elements for unseen language generalization",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=jCtZYTMGYY": {
    "title": "Learning to Plan and Generate Text with Citations",
    "volume": "review",
    "abstract": "The increasing demand for the deployment of LLMs in information-seeking scenarios has spurred efforts in creating verifiable systems, which generate responses to queries along with supporting evidence. In this paper, we explore the \\emph{attribution} capabilities of plan-based models which have been recently shown to improve the faithfulness, grounding, and controllability of generated text. We conceptualize plans as a sequence of questions which serve as \\emph{blueprints} of the generated content and its organization. We propose two attribution models that utilize different variants of blueprints, an \\textit{abstractive} model where questions are generated from scratch, and an \\textit{extractive} model where questions are copied from the input. Experiments on long-form question-answering show that planning consistently improves attribution quality. Moreover, the citations generated by blueprint models are more accurate compared to those obtained from LLM-based pipelines lacking a planning component",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r9nkcR2BmXUV": {
    "title": "Abstract Meaning Representation-Based Logic-Driven Data Augmentation for Logical Reasoning",
    "volume": "review",
    "abstract": "Combining large language models with logical reasoning enhances their capacity to address problems in a robust and reliable manner. Nevertheless, the intricate nature of logical reasoning poses challenges to gathering reliable data from the web for building comprehensive training datasets, subsequently affecting the performance on downstream tasks. To address this, we introduce a novel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the original text into an Abstract Meaning Representation (AMR) graph, a structured semantic representation that encapsulates the logic structure of the sentence, upon which operations are performed to generate logically modified AMR graphs. The modified AMR graphs are subsequently converted back into text to create augmented data. Notably, our methodology is architecture-agnostic and enhances both generative large language models, such as GPT-3.5 and GPT-4, through prompt augmentation, and discriminative large language models through contrastive learning with logic-driven data augmentation. Empirical evidence underscores the efficacy of our proposed method with improvement in performance across seven downstream tasks, such as reading comprehension requiring logical reasoning, textual entailment, and natural language inference. Furthermore, our method leads on the ReClor leaderboard\\footnote{\\url{https://eval.ai/web/challenges/challenge-page/503/leaderboard/1347}}. The source code and data are publicly available\\footnote{\\url{https://bit.ly/3SuiRVi}}",
    "checked": true,
    "id": "1c5c2a10ece56682c8acf98e90091f0e1851cdc7",
    "semantic_title": "abstract meaning representation-based logic-driven data augmentation for logical reasoning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=jRZwLMxO2GB": {
    "title": "Finding and Editing Multi-Modal Neurons in Pre-Trained Transformers",
    "volume": "review",
    "abstract": "Understanding the internal mechanism of multi-modal large language models (LLMs) is becoming increasingly critical for continuous improvements in both academia and industry. In this paper, we propose a novel method to identify key neurons for interpretability --- how multi-modal LLMs bridge visual and textual concepts for captioning. Our method improves conventional works upon efficiency and applied range by removing needs of costly gradient computation. Based on those identified neurons, we further design a multi-modal knowledge editing method, beneficial to mitigate sensitive words or hallucination. For rationale of our design, we provide theoretical assumption. For empirical evaluation, we have conducted extensive quantitative and qualitative experiments. The results not only validate the effectiveness of our methods, but also offer insightful findings that highlight three key properties of multi-modal neurons: sensitivity, specificity and causal-effect, to shed light for future research. We will release code upon acceptance",
    "checked": false,
    "id": "38449fadc86ae2a469fecdddf1351c82907078db",
    "semantic_title": "finding and editing multi-modal neurons in pre-trained transformer",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=u5SGCPrqxGa": {
    "title": "Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View",
    "volume": "review",
    "abstract": "As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: *Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)?* This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique 'societies' comprised of LLM agents, where each agent is characterized by a specific 'trait' (easy-going or overconfident) and engages in collaboration with a distinct 'thinking pattern' (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches, but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents manifest human-like social behaviors, such as conformity and consensus reaching, mirroring foundational social psychology theories. In conclusion, we integrate insights from social psychology to contextualize the collaboration of LLM agents, inspiring further investigations into the collaboration mechanism for LLMs. We commit to sharing our code and datasets\\footnote{\\url{https://anonymous.4open.science/r/MachineSoM-3178}.}, hoping to catalyze further research in this promising avenue",
    "checked": true,
    "id": "9fcdbfdf28245010c875ce85502351fe05c04b49",
    "semantic_title": "exploring collaboration mechanisms for llm agents: a social psychology view",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=yEZAzwj1yf": {
    "title": "An Empirical Study of LLM for Code Analysis: Understanding Syntax and Semantics",
    "volume": "review",
    "abstract": "Large language models~(LLMs) demonstrate significant potential to revolutionize software engineering (SE). However, the high reliability and risk control requirements in software engineering raise concerns about the need for interpretability of LLMs. To address this concern, we conducted a study to evaluate the capabilities of LLMs and their limitations for code analysis in SE. Code analysis is essential in software development. It identifies bugs, security, and compliance problems and evaluates code quality and performance. We break down the abilities needed for LLMs to address SE tasks related to code analysis into three categories: 1) syntax understanding, 2) static behaviour understanding, and 3) dynamic behaviour understanding. We used four foundational models and assessed the performance of LLMs on multiple-language tasks. We found that, while LLMs are good at understanding code syntax, they struggle with comprehending code semantics, particularly dynamic semantics. Furthermore, our study highlights that LLMs are susceptible to hallucinations when interpreting code semantic structures. It is necessary to explore methods to verify the correctness of LLM's output to ensure its dependability in SE. More importantly, our study provides an initial answer to why the codes generated by LLM are usually syntax-correct but are possibly vulnerable",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GxgcY7umKqg": {
    "title": "Applying Intrinsic Debiasing on Downstream Tasks: Challenges and Considerations for Machine Translation",
    "volume": "review",
    "abstract": "Most works on gender bias focus on intrinsic bias --- removing traces of information about a protected group from the model's internal representation. However, these works are often disconnected from the impact of such debiasing on downstream applications, which is the main motivation for debiasing in the first place. In this work, we systematically test how methods for intrinsic debiasing affect neural machine translation models, by measuring the extrinsic bias of such systems under different design choices. We highlight three challenges and mismatches between the debiasing techniques and their end-goal usage, including the choice of embeddings to debias, the mismatch between words and sub-word tokens debiasing, and the effect on different target languages. We find that these considerations have a significant impact on downstream performance and the success of debiasing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yexhote3v_": {
    "title": "Can Large Language Models Recall Reference Location Like Humans?",
    "volume": "review",
    "abstract": "When completing knowledge-intensive tasks, humans sometimes need not just an answer but also a corresponding reference passage for auxiliary reading. Previous methods required obtaining pre-segmented article chunks through additional retrieval models. This paper explores leveraging the parameterized knowledge stored during the pre-training phase of large language models (LLMs) to independently recall reference passage from any starting position. We propose a two-stage framework that simulates the scenario of humans recalling easily forgotten references. Initially, the LLM is prompted to recall document title identifiers to obtain a coarse-grained document set. Then, based on the acquired coarse-grained document set, it recalls fine-grained passage. In the two-stage recall process, we use constrained decoding to ensure that content outside of the stored documents is not generated. To increase speed, we only recall a short prefix in the second stage, then locate its position to retrieve a complete passage. Experiments on KILT knowledge-sensitive tasks have verified that LLMs can independently recall reference passage location in various task forms, and the obtained reference significantly assist downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9ADRjfO3BP": {
    "title": "General Collaborative Framework between Large Language Model and Experts for Universal Information Extraction",
    "volume": "review",
    "abstract": "Recently, unified information extraction have been widely concerned NLP community, which aims at using a unified paradigm to perform various information extraction tasks. However, they inevitably suffering from some thorny problems such as noise interference, abstract label semantics, and diverse span granularity. In this paper, First of all, we start by presenting three problematic assumptions that exist in previous research works from a unified information extraction perspective. These problems severely hinder the development of information extraction models. Furthermore, to solve these problems, we propose the General Collaborative Information Extraction framework for universal information extraction. Specifically, GCIE consists of a general Recognizer for identifying predefined types and multiple task-specific Experts for extracting spans. The Recognizer is a large language model, while the Expert is a series of smaller language models, and they collaborate in a pipeline to achieve unified or task-specific information extraction. Empirical experiments on 6 IE tasks and 13 datasets, under supervised and few-shot settings, validate the effectiveness and generality of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cKtl8uDd90": {
    "title": "Who Wrote this Code? Watermarking for Code Generation",
    "volume": "review",
    "abstract": "Since the remarkable generation performance of large language models raised ethical and legal concerns, approaches to detect machine-generated text by embedding watermarks are being developed. However, we discover that the existing works fail to function appropriately in code generation tasks due to the task's nature of having low entropy. Extending a logit-modifying watermark method, we propose \\textbf{S}elective \\textbf{W}at\\textbf{E}rmarking via \\textbf{E}ntropy \\textbf{T}hresholding (SWEET), which enhances detection ability and mitigates code quality degeneration by removing low-entropy segments at generating and detecting watermarks. Our experiments show that SWEET significantly improves code quality preservation while outperforming all baselines, including post-hoc detection methods, in detecting machine-generated code text. Our code is available in the supplementary materials",
    "checked": true,
    "id": "90213971da84c974bc7502b1112a0ee8a0a33601",
    "semantic_title": "who wrote this code? watermarking for code generation",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=Y3eP_JEPqf": {
    "title": "ReWIRED: Instructional Explanations in Teacher-Student Dialogues",
    "volume": "review",
    "abstract": "How to assess the quality of teaching in instructional explanation dialogues is a recurring point of debate in didactics research. For the NLP community, this is a challenging topic thus far, even with the use of LLMs. To address the matter, we create a new annotation scheme of teaching acts aligned with contemporary didactic teaching models. On this basis, we extend an existing dataset of conversational explanations about communicating scientific understanding in teacher-student settings on five levels of the explainee's expertise, with the proposed teaching annotation: explanation and dialogue acts. For better granularity, we reframe the task from a dialogue turn classification to a span labeling task. We then evaluate language models on the labeling of such acts and find that the broad range and structure of the proposed labels is hard to model for LLMs such as GPT-3.5/-4 via prompting, but a fine-tuned BERT can perform both act classification and span labeling well. Finally, we operationalize a series of quality metrics for instructional explanations in the form of a test suite. We find that they match the five expertise levels well and that experts in our data often stick to best practices in teaching",
    "checked": false,
    "id": "2e009f90204602874a29b31aa86e48ebcd8cf17d",
    "semantic_title": "student teachers' professional identity construction through famous education quotes",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=19S4snMgpR": {
    "title": "Leveraging Large Language Models for NLG Evaluation: A Survey",
    "volume": "review",
    "abstract": "In the rapidly evolving domain of Natural Language Generation (NLG) evaluation, introducing Large Language Models (LLMs) has opened new avenues for assessing generated content quality, e.g., coherence, creativity, and context relevance. This survey aims to provide a thorough overview of leveraging LLMs for NLG evaluation, a burgeoning area that lacks a systematic analysis. We propose a coherent taxonomy for organizing existing LLM-based evaluation metrics, offering a structured framework to understand and compare these methods. Our detailed exploration includes critically assessing various LLM-based methodologies, as well as comparing their strengths and limitations in evaluating NLG outputs. By discussing unresolved challenges, including bias, robustness, domain-specificity, and unified evaluation, this survey seeks to offer insights to researchers and advocate for fairer and more advanced NLG evaluation techniques",
    "checked": true,
    "id": "4b8df079495cbec21ae90d60ab84e8dd813ca7e6",
    "semantic_title": "leveraging large language models for nlg evaluation: a survey",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=_HvNltOPJq": {
    "title": "Better Semantic Representation: A Low-Shot Relation Extraction Method Based on Token-Generated Contributions",
    "volume": "review",
    "abstract": "In light of the era of information explosion, traditional relation extraction methods are in a bottleneck due to data limitations in the face of the constant emergence of new relation categories. Therefore the study of low-shot relation extraction in real scenarios is crucial. In the few-shot scenario, it is necessary to build up the model's ability to summarize the semantics of instances. In the zero-shot scenario, it is necessary to establish the label matching ability of the model. Although they need to establish different basic abilities of the model, the common point is that they all need to build excellent semantic representations in the end, which is ignored by the existing methods. In this paper, we propose a method (TGCRE) based on token-generated contribution to unify low-shot relation extraction by generating better semantic representations. Further, we propose a multi-level spatial semantic matching scheme in zero-shot scenarios, aiming to solve the problem that existing methods cannot fully utilize feature information and are susceptible to irrelevant contexts. Experimental results show that our method outperforms previous robust baselines and achieves state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VdQY6901Y4": {
    "title": "Do Llamas Work in English? On the Latent Language of Multilingual Transformers",
    "volume": "review",
    "abstract": "We ask whether multilingual language models trained on unbalanced, English-dominated corpora use English as an internal pivot languageâ€“â€“a question of key importance for understanding how language models function and the origins of linguistic bias.Focusing on the Llama-2 family of transformer models, our study is based on carefully constructed non-English prompts with a unique correct single-token continuation. From layer to layer, transformers gradually map an input embedding of the final prompt token to an output embedding from which next-token probabilities are computed. Tracking intermediate embeddings through their high-dimensional space reveals three distinct phases, whereby intermediate embeddings (1) start far away from output token embeddings; (2) already in middle layers allow for decoding a semantically correct next token, but giving higher probability to its version in English than in the input language; (3) move into an input-language-specific region of the embedding space. We cast these results into a conceptual model where the three phases operate in ''input space'', ''concept space'', and ''output space'', respectively. Crucially, our evidence suggests that the abstract ''concept space'' lies closer to English than to other input languages, which may have important consequences regarding the biases embodied by multilingual language models",
    "checked": true,
    "id": "e88efdfc05d0d8832b2ded7af9c8ececdd03e58d",
    "semantic_title": "do llamas work in english? on the latent language of multilingual transformers",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=fWk9CcDPZJ": {
    "title": "Text-to-Song: Towards Controllable Music Generation Incorporating Vocal and Accompaniment",
    "volume": "review",
    "abstract": "\\begin{abstract}A song is a combination of singing voice and accompaniment. However, existing works focus on singing voice synthesis and music generation independently. Little attention was paid to explore song synthesis. In this work, we propose a novel task called text-to-song synthesis which incorporating both vocals and accompaniments generation. We develop Melodist, a two-stage text-to-song method that consists of singing voice synthesis (SVS) and vocal-to-accompaniment (V2A) synthesis. Melodist leverages tri-tower contrastive pretraining to learn more effective text representation for controllable V2A synthesis. A Chinese song dataset mined from a music website is built up to alleviate data scarcity for our research. The evaluation results on our dataset demonstrate that Melodist can synthesize songs with comparable quality and style consistency. Audio samples can be found in \\url{https://text2songMelodist.github.io/Sample/}.\\end{abstract}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2xzWfVtGuM": {
    "title": "BeamAggR: Beam Aggregation Reasoning over Multi-source Knowledge for Multi-hop Question Answering",
    "volume": "review",
    "abstract": "Large language models (LLMs) have demonstrated strong reasoning capabilities. Nevertheless, they still suffer from factual errors when tackling knowledge-intensive tasks. Retrieval-augmented reasoning represents a promising approach. However, significant challenges still persist, including inaccurate and insufficient retrieval for complex questions, as well as difficulty in integrating multi-source knowledge. To address this, we propose Beam Aggregation Reasoning (BeamAggR), a reasoning framework for knowledge-intensive multi-hop QA. BeamAggR explores and prioritizes promising answers at each hop of question. Concretely, we parse the complex questions into trees, which include atom and composite questions, followed by bottom-up reasoning. For atomic questions, the LLM conducts reasoning on multi-source knowledge to get answer candidates. For composite questions, the LLM combines beam candidates, explores multiple reasoning paths through probabilistic aggregation, and prioritizes the most promising trajectory. Extensive experiments on four open-domain multi-hop reasoning datasets show that our method significantly outperforms SOTA methods by 8.5%. Moreover, our analysis reveals that BeamAggR elicits better knowledge collaboration and answer aggregation",
    "checked": true,
    "id": "cbbe1ae3983f8e1215f34db5c5b24b2a70555411",
    "semantic_title": "beamaggr: beam aggregation reasoning over multi-source knowledge for multi-hop question answering",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9RZtJGbILi": {
    "title": "Hyperparameter-Free Approach for Faster Minimum Bayes Risk Decoding",
    "volume": "review",
    "abstract": "Minimum Bayes-Risk (MBR) decoding is shown to be a powerful alternative to beam search decoding for a wide range of text generation tasks.However, MBR requires a huge amount of time for inference to compute the MBR objective, which makes the method infeasible in many situations where response time is critical.Confidence-based pruning (CBP) (Cheng and Vlachos, 2023) has recently been proposed to reduce the inference time in machine translation tasks. Although it is shown to significantly reduce the amount of computation, it requires hyperparameter tuning using a development set to be effective.To this end, we propose Adaptive Minimum Bayes-Risk (AMBR) decoding, a hyperparameter-free method to run MBR decoding efficiently. AMBR is derived from the observation that the problem of computing the sample-based MBR objective is the \\textit{medoid identification problem}.AMBR uses the Correlated Sequential Halving (CSH) algorithm (Baharav and Tse, 2019), the algorithm with the best performance guarantee to date for the medoid identification problem, to compute the sample-based MBR objective.We evaluate AMBR on machine translation, text summarization, and image captioning tasks. The results show that AMBR achieves on par with CBP, with CBP selecting hyperparameters through an Oracle for each given computation budget",
    "checked": true,
    "id": "60afc279e2c6d1e82b5ef5399548e18270aa8f75",
    "semantic_title": "hyperparameter-free approach for faster minimum bayes risk decoding",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=kZKcUOjv0N": {
    "title": "Learning Adverbs with Spectral Mixture Kernels",
    "volume": "review",
    "abstract": "For humans and robots to collaborate more in the real world, robots need to understand human intentions from the different manner of their behaviors.In our study, we focus on the meaning of adverbs which describe human motions. We propose a topic model, Hierarchical Dirichlet Process-Spectral Mixture Latent Dirichlet Allocation, which concurrently learns the relationship between those human motions and those adverbs by capturing the frequency kernels that represent motion characteristics and the shared topics of adverbs that depict such motions.We trained the model on datasets we made from movies about \"walking\" and \"dancing\", and found that our model outperforms representative neural network models in terms of perplexity score.We also demonstrate our model's ability to determine the adverbs for a given motion and confirmed that the model predicts more appropriate adverbs",
    "checked": true,
    "id": "841045e5a62dbeea4323e61a00841024b3971a3e",
    "semantic_title": "learning adverbs with spectral mixture kernels",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b3AoAk60mL": {
    "title": "How Do Moral Emotions Shape Political Participation? A Cross-Cultural Analysis of Online Petitions Using Language Models",
    "volume": "review",
    "abstract": "Understanding the interplay between emotions in language and user behaviors is critical. We study how moral emotions shape political participation of users based on cross-cultural online petition data. To quantify moral emotions, we employ a context-aware NLP model that is designed to capture the subtle nuances of emotions across cultures. For model training, we construct and share a moral emotion dataset comprising 50,000 petition sentences in Korean and English along with emotion labels annotated by a fine-tuned LLM. We examine two distinct types of user participation: general support (i.e., registered signatures of petitions) and active support (i.e., sharing petitions on social media). We discover that moral emotions like other-suffering increase both forms of participation and help petitions go viral, while self-conscious have the opposite effect. The most prominent moral emotion, other-condemning, led to polarizing responses among the audience. In contrast, other-praising was perceived differently by culture; it led to a rise in active support in Korea but a decline in the UK. Our findings suggest that both moral emotions embedded in language and cultural perceptions are critical in engaging the public in political discourse",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jyG14i5vQ5": {
    "title": "Trial and Error: Exploration-Based Trajectory Optimization of LLM Agents",
    "volume": "review",
    "abstract": "Large language models (LLMs) have emerged as the core controller for various autonomous agent systems. In this work, we introduce ETO, a method aimed at enhancing the capabilities of open-source LLM agents. Unlike previous work that solely trains on success expert trajectories, our approach enables agents to learn from exploration failures, leading to improved performance through an iterative exploration-training framework. During the exploration phase, the agent explores the environment, collecting failure trajectories to construct contrastive trajectory pairs. In the training phase, the agent leverages the trajectory contrastive information to update its policy. This iterative process of exploration and training facilitates further improvement for the agents. Experiments on three agent datasets show our method consistently outperforms baselines by more than 5% in final rewards. Moreover, analysis on task-solving efficiency, and the potential in scenarios without expert trajectory further highlight the effectiveness of our method",
    "checked": false,
    "id": "f95da5b7be2fac2381eb5dfe26dc7dc5bc2d9a90",
    "semantic_title": "trial and error: exploration-based trajectory optimization for llm agents",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=piWQ1Pw7eS": {
    "title": "The Knowledge Alignment Problem: Bridging Human and External Knowledge for Large Language Models",
    "volume": "review",
    "abstract": "Large language models often necessitate grounding on external knowledge to generate faithful and reliable answers. Yet even with the correct groundings in the reference, they can ignore them and rely on wrong groundings or their inherent biases to hallucinate when users, being largely unaware of the specifics of the stored information, pose questions that might not directly correlate with the retrieved groundings. In this work, we formulate this knowledge alignment problem and introduce MixAlign, a framework that interacts with both the human user and the knowledge base to obtain and integrate clarifications on how the user question relates to the stored information. MixAlign employs a language model to achieve automatic knowledge alignment and, if necessary, further enhances this alignment through human user clarifications. Experimental results highlight the crucial role of knowledge alignment in boosting model performance and mitigating hallucination, with improvements noted up to 22.2% and 27.1% respectively. We also demonstrate the effectiveness of MixAlign in improving knowledge alignment by producing high-quality, user-centered clarifications",
    "checked": true,
    "id": "c3b16cc964277616f78eb9e7a4ccf363484b8da3",
    "semantic_title": "the knowledge alignment problem: bridging human and external knowledge for large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=shQK5l22Xn": {
    "title": "Adaptive Decoding for Efficient Automatic Speech Recognition",
    "volume": "review",
    "abstract": "The latency and computational demand of End-to-end (E2E) automatic speech recognition (ASR) models hinder their deployment on lightweight devices. We find that, although these models can be tuned for efficiency concerns, the computational burden of large vocabularies remains a challenge. In this paper, we propose an adaptive decoding method (ADD) to speed up E2E ASR systems. It segments the vocabulary based on the inherent characteristics of speech, enabling the models to predict each word with a much smaller vocabulary. Our method significantly reduces the FLOPs required for calculations. We also find that the unit-based methods, developed through self-supervised learning, capture acoustic features well and achieve performance comparable to the phone-based methods",
    "checked": false,
    "id": "7f464b2fcb56a159b178fe2013b215fe2256a7ba",
    "semantic_title": "continuously learning new words in automatic speech recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HZnVY_PrFN": {
    "title": "Unleashing the Power of Large Language Models in Zero-shot Relation Extraction via Self-Prompting",
    "volume": "review",
    "abstract": "Recent research in zero-shot Relation Extraction (RE) has concentrated on employing Large Language Models (LLMs) as extractors, owing to their notable zero-shot capabilities. By directly prompting the LLM or transforming the task into a Question Answering (QA) problem, the LLM can efficiently extract relations from a given sample. However, current methods often exhibit suboptimal performance, primarily due to the lack of detailed and context-specific prompts necessary for effectively understanding the variety of sentences and relations. To bridge this gap, we introduce the Self-Prompting framework, a novel method designed to fully harness the embedded RE knowledge within LLMs. Specifically, our framework employs a three-stage diversity approach to prompt LLMs, generating multiple synthetic samples that encapsulate specific relations from scratch. These generated samples act as in-context learning samples, offering explicit and context-specific guidance to efficiently prompt LLMs for RE. Experimental evaluations conducted on benchmark datasets have demonstrated the superiority of our approach over existing LLM-based zero-shot RE methods. Furthermore, our experiments highlight the effectiveness of our generation pipeline in producing high-quality synthetic data that significantly enhances performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mx3YBrqnzh": {
    "title": "DAPrompt: Deterministic Assumption Prompt Learning for Event Causality Identification",
    "volume": "review",
    "abstract": "Event Causality Identification (ECI) aims at determining whether there is a causal relation between two event mentions. Conventional prompt learning designs a prompt template to first predict an answer word and then maps it to the final decision. Unlike conventional prompts, we argue that predicting an answer word may not be a necessary prerequisite for the ECI task. Instead, we can first make a deterministic assumption on the existence of causal relation between two events and then evaluate its rationality to either accept or reject the assumption. The design motivation is to try the most utilization of the encyclopedia-like knowledge embedded in a pre-trained language model. In light of such considerations, we propose a deterministic assumption prompt learning model, called DAPrompt, for the ECI task. In particular, we design a simple deterministic assumption template concatenating with the input event pair, which includes two masks as predicted events' tokens. We use the probabilities of predicted events to evaluate the assumption rationality for the final event causality decision. Experiments on the EventStoryLine corpus and Causal-TimeBank corpus validate our design objective in terms of significant performance improvements over the state-of-the-art algorithms",
    "checked": true,
    "id": "e92f4ff44def2273d9fcb02921b257dcbe3c9626",
    "semantic_title": "daprompt: deterministic assumption prompt learning for event causality identification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ekBMZjRiQ3J": {
    "title": "On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference",
    "volume": "review",
    "abstract": "Large language Models~(LLMs) are notably cost-prohibitive to deploy in resource-constrained environments due to their excessive memory and computational demands. In addition to model parameters, the key-value cache is also stored in GPU memory, growing linearly with batch size and sequence length. As a remedy, recent works have proposed various eviction policies for maintaining the overhead of key-value cache under a given budget. This paper embarks on the efficacy of existing eviction policies in terms of \\textit{importance score calculation} and \\textit{eviction scope construction}. We identify the deficiency of prior policies in these two aspects and introduce RoCo, a \\underline{r}\\underline{o}bust \\underline{c}ache \\underline{o}mission policy based on local attention scores and robustness measures. Extensive experimentation spanning prefilling and auto-regressive decoding stages validates the superiority of RoCo. Finally, we release EasyKV, a versatile software package dedicated to user-friendly key-value constrained generative inference. Code available at \\url{https://anonymous.4open.science/r/EasyKV-9088/}",
    "checked": true,
    "id": "f395f022548d1d1f11e231f42d4852dbff5e9376",
    "semantic_title": "on the efficacy of eviction policy for key-value constrained generative language model inference",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=KQvn7ZtH_s": {
    "title": "Don't Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation",
    "volume": "review",
    "abstract": "Neural machine translation systems estimate probabilities of target sentences given source sentences, yet these estimates may not align with human preferences. This work introduces QE-fusion, a method that synthesizes translations using a quality estimation metric (QE), which correlates better with human judgments. QE-fusion leverages a pool of candidates sampled from a model, combining spans from different candidates using a QE metric such as CometKiwi. We compare QE-fusion against beam search and recent reranking techniques, such as Minimum Bayes Risk decoding or QE-reranking. Our method consistently improves translation quality in terms of COMET and BLEURT scores when applied to large language models (LLMs) used for translation (PolyLM, XGLM, Llama2, Mistral, ALMA and Tower) and to multilingual translation models (NLLB), over five language pairs. Notably, QE-fusion exhibits larger improvements for LLMs due to their ability to generate diverse outputs. We demonstrate that our approach generates novel translations in over half of the cases and consistently outperforms other methods across varying numbers of candidates (5â€“200). Furthermore, we empirically establish that QE-fusion scales linearly with the number of candidates in the pool",
    "checked": true,
    "id": "e21e5e9f87c3ac0c281264f4e8d94d15924591d6",
    "semantic_title": "don't rank, combine! combining machine translation hypotheses using quality estimation",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=7fVII_jwoQ": {
    "title": "What Would Happen Next? Predicting Consequences from An Event Causality Graph",
    "volume": "review",
    "abstract": "Predicting the consequences based on some past events has a huge potential in various Natural Language Processing applications. However, existing work faces two shortcomings: (1) Simple modeling scenarios, such as Script Event Prediction task, which predict subsequent events only based on an event chain; (2) Interpolation scenarios, such as Event Knowledge Graph Completion task, where the predicted event has already occurred in known events. In this paper, we propose a new task named Event Causality Graph Prediction, which forecast the consequence event based on an event causality graph constructed from a document describing complex event scenarios. To that end, we propose two corresponding datasets and an \\textbf{G}raph \\textbf{C}ontrastive \\textbf{P}rompt \\textbf{L}earning model(GCPL), which utilize the benefits of graph prompt learning and introduce the Dual Encoder to integrate node text and graph structure information. We conduct extensive experiments on two datasets and our GCPL achieves state-of-the-art performance among all competitors",
    "checked": false,
    "id": "e8d2bd4a6efa3475ab1d9ba45fb0382da17b634d",
    "semantic_title": "sudden event prediction based on event knowledge graph",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=vRcJ0oWXGy": {
    "title": "Bayesian Example Selection Improves In-Context Learning for Speech, Text, and Visual Modalities",
    "volume": "review",
    "abstract": "Large language models (LLMs) can adapt to new tasks through in-context learning (ICL) based on a few examples presented in dialogue history without any model parameter update. Despite such convenience, the performance of ICL heavily depends on the quality of the in-context examples presented, which makes the in-context example selection approach a critical choice. This paper proposes a novel eBayesian in-Context example Selection method (ByCS) for ICL. Extending the inference probability conditioned on in-context examples based on Bayes' theorem, ByCS focuses on the inverse inference conditioned on test input. Following the assumption that accurate inverse inference probability (likelihood) will result in accurate inference probability (posterior), in-context examples are selected based on their inverse inference results. Diverse and extensive cross-tasking and cross-modality experiments are performed with speech, text, and image examples. Experimental results show the efficacy and robustness of our ByCS method on various models, tasks and modalities",
    "checked": true,
    "id": "729c6421c049d47350e04e32256a14d6fb74f9f1",
    "semantic_title": "bayesian example selection improves in-context learning for speech, text, and visual modalities",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=7BUgadP8EA": {
    "title": "TaskBench: Benchmarking Large Language Models for Task Automation",
    "volume": "review",
    "abstract": "Recently, the incredible progress of large language models (LLMs) has ignited the spark of task automation, which decomposes the complex tasks described by user instructions into sub-tasks, and invokes external tools to execute them, and plays a central role in autonomous agents. However, there lacks a systematic and standardized benchmark to foster the development of LLMs in task automation. To this end, we introduce TaskBench to evaluate the capability of LLMs in task automation. Specifically, task automation can be formulated into three critical stages: task decomposition, tool invocation, and parameter prediction to fulfill user intent. This complexity makes data collection and evaluation more challenging compared to common NLP tasks. To generate high-quality evaluation datasets, we introduce Tool Graph to represent the decomposed tasks in user intent, and adopt Back-Instruct to simulate user instruction and annotations. Furthermore, we propose TaskEval to evaluate the capability of LLMs from different aspects, including task decomposition, tool invocation, and parameter prediction. Our experimental findings reveal that TaskBench effectively measures LLMs' task automation proficiency. Benefiting from the mixture of automated data construction and human verification, TaskBench ensures high consistency with human evaluation, establishing it as a reliable and comprehensive benchmark for LLM-based agents",
    "checked": true,
    "id": "017f1c28c7d4fb65c6fff7d3c2fff1687597e252",
    "semantic_title": "taskbench: benchmarking large language models for task automation",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=Bg3DzXLZ2X": {
    "title": "Benchmarking and Improving Compositional Generalization of Multi-aspect Controllable Text Generation",
    "volume": "review",
    "abstract": "Compositional generalization, representing the model's ability to generate text with new attribute combinations obtained by recombining single attributes from the training data, is a crucial property for multi-aspect controllable text generation (MCTG) methods. Nonetheless, a comprehensive compositional generalization evaluation benchmark of MCTG is still lacking. We propose CompMCTG, a benchmark encompassing diverse multi-aspect labeled datasets and a crafted three-dimensional evaluation protocol, to holistically evaluate the compositional generalization of MCTG approaches. We observe that existing MCTG works generally confront a noticeable performance drop in compositional testing. To mitigate this issue, we introduce Meta-MCTG, a training framework incorporating meta-learning, where we enable models to learn how to generalize by simulating compositional generalization scenarios in the training phase. We demonstrate the effectiveness of Meta-MCTG through achieving obvious improvement (by at most 3.64%) for compositional testing performance in 94.4% cases",
    "checked": true,
    "id": "b518654542b6ad9e27d9fc7eaf7ce9f15190dfee",
    "semantic_title": "benchmarking and improving compositional generalization of multi-aspect controllable text generation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=N5-lY6mE5T": {
    "title": "Prompt Optimization via Adversarial In-Context Learning",
    "volume": "review",
    "abstract": "We propose a new method, Adversarial In-Context Learning (adv-ICL), to optimize prompts for in-context learning (ICL). Inspired by adversarial learning, adv-ICL is implemented as a two-player game between a generator and discriminator, with LLMs acting as both. In each round, given an input prefixed by task instructions and several exemplars, the generator produces an output. The discriminator then classifies the generator's input-output pair as model-generated or real data. Based on the discriminator's loss, a prompt modifier LLM proposes possible edits to the generator and discriminator prompts, and the edits that most improve the adversarial loss are selected. We show that applying adv-ICL results in significant improvements over state-of-the-art prompt optimization techniques for both open and closed-source models on $13$ generation and classification tasks including summarization, arithmetic reasoning, machine translation, data-to-text generation, and the MMLU and big-bench hard benchmarks. In addition, our method is computationally efficient, easily extensible to other LLMs and tasks, and effective in low-resource settings",
    "checked": true,
    "id": "48cfb2e5d32dcfedd9acb8eb38f499d2b7202369",
    "semantic_title": "prompt optimization via adversarial in-context learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=sc5i7q6DQO": {
    "title": "CriticBench: Benchmarking LLMs for Critique-Correct Reasoning",
    "volume": "review",
    "abstract": "The ability of Large Language Models (LLMs) to critique and refine their reasoning is crucial for their application in evaluation, feedback provision, and self-improvement. This paper introduces CriticBench, a comprehensive benchmark designed to assess LLMs' abilities to critique and rectify their reasoning across a variety of tasks. CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic. It compiles 15 datasets and incorporates responses from three LLM families. Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in generation, critique, and correction reasoning, i.e., GQC reasoning. Our findings reveal: (1) a linear relationship in GQC capabilities, with critique-focused training markedly enhancing performance; (2) a task-dependent variation in correction effectiveness, with logic-oriented tasks being more amenable to correction; (3) GQC knowledge inconsistencies that decrease as model size increases; and (4) an intriguing inter-model critiquing dynamic, where stronger models are better at critiquing weaker ones, while weaker models can surprisingly surpass stronger ones in their self-critique. We hope these insights into the nuanced critique-correct reasoning of LLMs will foster further research in LLM critique and self-improvement",
    "checked": true,
    "id": "6f24e0782300dca8a4cefcb5a3ccba94bfbb1395",
    "semantic_title": "criticbench: benchmarking llms for critique-correct reasoning",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=xIzmcQUs19": {
    "title": "Analyze, Generate and Refine: Query Expansion with LLMs for Zero-Shot Open-Domain QA",
    "volume": "review",
    "abstract": "Query expansion (QE) is a critical component in the open-domain question answering (OpenQA) pipeline, enhancing the retrieval performance by broadening the scope of queries with additional relevant texts. However, existing methods like GAR and EAR rely heavily on supervised training and often struggle to maintain effectiveness across domains and datasets. Meanwhile, although large language models (LLMs) have demonstrated QE capability for information retrieval (IR) tasks, their application in OpenQA is hindered by the inadequate analysis of query's informational needs and the lack of quality control for generated QEs, failing to meet the unique requirements of OpenQA. To bridge this gap, we propose a novel LLM-based QE approach named AGR for the OpenQA task, leveraging a three-step prompting strategy. AGR begins with an analysis of the query, followed by the generation of answer-oriented expansions, and culminates with a refinement process for better query formulation. Extensive experiments on four OpenQA datasets reveal that AGR not only rivals in-domain supervised methods in retrieval accuracy, but also outperforms state-of-the-art baselines in out-domain zero-shot scenarios. Moreover, it exhibits enhanced performance in end-to-end QA evaluations, underscoring the superiority of AGR for OpenQA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VFEUNY689v": {
    "title": "Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) have unveiled remarkable reasoning capabilities by exploiting chain-of-thought (CoT) prompting, which generates intermediate reasoning chains to serve as the rationale for deriving the answer. However, current CoT methods either simply employ general prompts such as Let's think step by step, or heavily rely on pre-defined task-specific demonstrations to attain preferable performances, thereby engendering an inescapable gap between performance and generalization. To bridge this gap, we propose GeM-CoT, a Generalizable CoT prompting mechanism in Mixed-task scenarios where the type of input questions is unknown. GeM-CoT first categorizes the question type and subsequently samples or constructs demonstrations from the corresponding data pool in an automatic pattern. With this technical design, GeM-CoT simultaneously enjoys superior generalization capabilities and remarkable performances on 10 public reasoning tasks and 23 BBH tasks",
    "checked": false,
    "id": "4e3e064e6613e2c40c1fa5dc2bd8fd934b410ccb",
    "semantic_title": "meta-cot: generalizable chain-of-thought prompting in mixed-task scenarios with large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ncdKl1xdPq9": {
    "title": "Direct Large Model Alignment Through Self-Rewarding Contrastive Prompt Distillation",
    "volume": "review",
    "abstract": "Aligning large language models (LLMs) with human expectations without human-annotated preference data is an important problem. In this paper, we propose a method to evaluate the response preference by using the output probabilities of response pairs under contrastive prompt pairs, which could achieve better performance on LLaMA2-7B and LLaMA2-13B compared to RLAIF. Based on this, we propose an automatic alignment method, Direct Large Model Alignment (DLMA). First, we use contrastive prompt pairs to automatically generate preference data. Then, we continue to evaluate the generated preference data using contrastive prompt pairs and calculate a self-rewarding score. Finally, we use the DPO algorithm to effectively align LLMs by combining this self-rewarding score. In the experimental stage, our DLMA method could surpass the RLHF method without relying on human-annotated preference data",
    "checked": false,
    "id": "de6ddb30b07f192f2be142062c4c6c817e508d96",
    "semantic_title": "direct large language model alignment through self-rewarding contrastive prompt distillation",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=mlCxXIJ3gi9": {
    "title": "SimSCR: A Simple Supervised Contrastive Learning Framework for Response Selection of Dialogue Systems",
    "volume": "review",
    "abstract": "Supervised contrastive learning has shown impressive performance across multiple NLP tasks, enhancing model generalization by shortening the distance between semantic representations of samples in the same category and increasing the distance between those of different categories. For the task of response selection, directly calculating the similarity between context and response may lead to suboptimal model performance due to insufficient attention mechanism interaction, as compared to traditional full attention methods. To address this issue, we propose an innovative interactive supervised contrastive learning framework that transforms the problem of response selection from classification into a matching issue by introducing a special response named anchor response during training, effectively applying contrastive learning to this task. This framework not only combines the advantages of deep context interaction found in traditional methods but also leverages the strong generalization capability of contrastive learning. Additionally, we introduce a heuristic method for hard negative responses sampling, which significantly reduces the need for large numbers of negative samples in contrastive learning. Applying our method, the results obtained on three publicly available response selection datasets have reached the current state-of-the-art level",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xWq8cxkoj3N": {
    "title": "Evaluating Intention Detection Capability of Large Language Models in Persuasive Dialogues",
    "volume": "review",
    "abstract": "We investigate intention detection in persuasive multi-turn dialogs employing the largest available Large Language Models (LLMs).Much of the prior research measures the intention detection capability of machine learning models without considering the conversational history.To evaluate LLMs' intention detection capability in conversation, we modified the existing datasets of persuasive conversation and created datasets using a multiple-choice paradigm.It is crucial to consider others' perspectives through their utterances when engaging in a persuasive conversation, especially when making a request or reply that is inconvenient for others.This feature makes the persuasive dialogue suitable for the dataset of measuring intention detection capability.We incorporate the concept of `face acts,' which categorize how utterances affect mental states.This approach enables us to measure intention detection capability by focusing on crucial intentions and to conduct comprehensible analysis according to intention types",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z9Rk2PNBZA_a": {
    "title": "F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods",
    "volume": "review",
    "abstract": "Large language models (LLMs) garner significant attention for their unprecedented performance, leading to an increasing number of researches evaluating LLMs. However, these evaluation benchmarks are limited to assessing the instruction-following capabilities, overlooking the fundamental abilities that emerge during the pre-training stage. Previous subjective evaluation methods mainly reply on scoring by API models. However, in the absence of references, large models have shown limited ability to discern subtle differences. To bridge the gap, we propose F-Eval, a bilingual evaluation benchmark to evaluate the fundamental abilities, including expression, commonsense and logic. The tasks in F-Eval include multi-choice objective tasks, open-ended objective tasks, reference-based subjective tasks and reference-free subjective tasks. For reference-free subjective tasks, we devise new evaluation methods, serving as alternatives to scoring by API models. We conduct evaluations on 13 advanced LLMs. Results show that our evaluation methods show higher correlation coefficients and larger distinction than other evaluators. Additionally, we discuss the influence of different model sizes, dimensions, and normalization methods. We anticipate that F-Eval will facilitate the study of LLMs' fundamental abilities",
    "checked": true,
    "id": "b5111ea632fea5d0f90d7b240b31490401c02d6b",
    "semantic_title": "f-eval: asssessing fundamental abilities with refined evaluation methods",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MN-X2VWMGtz": {
    "title": "Learning to Memorize: Scalable Continual Learning in Semiparametric Language Models with Mixture-of-Neighbors Induction Memory",
    "volume": "review",
    "abstract": "Semiparametric language models (LMs) have shown promise in NLP tasks. However, the non-parametric memory in semiparametric LMs has traditionally been treated as static storage unable to learn and separated from the internal flow of the parametric models to compress information, limiting scalability and efficiency. Based on recent interpretability theories of LMs, we claim that the non-parametric memory represented by $k$NN-LM is actually learnable, and conceptualize it as Mixture-of-Neighbors Induction Memory (MoNIM), which synergizes the induction capabilities of MHSA layers with the memorization strength of FFN layers. We integrate MoNIM into Transformer architecture as a pluggable FFN-like layer, positioned parallel to the final FFN layer, to show that it learns (or compresses data) as part of parametric models does. Extensive experiments demonstrate that MoNIM is a scalable continual learner in both data- and model-wise, enhancing the scalability and continual learning performance of semiparametric LMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4KhOXgU9G7K": {
    "title": "Courtroom-LLM: An Innovative Courtroom-Analogous Framework for Text Classification",
    "volume": "review",
    "abstract": "In this research, we introduce the Courtroom-LLM framework, a novel multi-LLM structure inspired by legal courtroom processes, aiming to enhance decision-making in ambiguous text classification scenarios. Our approach simulates a courtroom setting within LLMs, assigning roles similar to those of prosecutors, defense attorneys, and judges, to facilitate comprehensive analysis of complex textual cases. We demonstrate that this structured multi-LLM setup can significantly improve decision-making accuracy, particularly in ambiguous situations, by harnessing the synergistic effects of diverse LLM arguments. Our results from thorough evaluations on various NLP tasks show that the Courtroom-LLM framework surpasses both conventional single LLM classifiers and basic structured multi-LLM systems, underscoring the benefits of our legal proceedings-inspired model in enhancing NLP decision-making",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qux4_lQBDly": {
    "title": "Simpson's Paradox and the Accuracy-Fluency Tradeoff in Translation",
    "volume": "review",
    "abstract": "A good translation should be faithful to the source and should respect the norms of the target language. We address a theoretical puzzle about the relationship between these objectives. On one hand, intuition and some prior work suggest that accuracy and fluency should trade off against each other, and that capturing every detail of the source can only be achieved at the cost of fluency. On the other hand, quality assessment researchers often suggest that accuracy and fluency are highly correlated and difficult for human raters to distinguish (Callison-Burch et al., 2007). We show that the tension between these views is an instance of Simpson's paradox, and that accuracy and fluency are positively correlated at the level of the corpus but trade off at the level of individual source sentences. We further suggest that the relationship between accuracy and fluency is best evaluated at the sentence level, and that the trade off between these dimensions has implications both for assessing translation quality and developing improved machine translation systems",
    "checked": true,
    "id": "3238350bfec1f70400ef3f1a59f7c7079f32191d",
    "semantic_title": "simpson's paradox and the accuracy-fluency tradeoff in translation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=mkadcX-Cf8L": {
    "title": "AdvisorQA: A Benchmark for Advice-seeking Question Answering with Collective Intelligence",
    "volume": "review",
    "abstract": "As the integration of large language models into daily life is on the rise, there is a clear gap in benchmarks for advising on subjective and personal dilemmas. To address this, we introduce AdvisorQA, the first benchmark developed to assess LLMs' capability in offering advice for deeply personalized concerns, utilizing the LifeProTips subreddit forum. This forum features a dynamic interaction where users post advice-seeking questions, receiving an average of 8.9 advice per query, with 164.2 upvotes from hundreds of users, embodying a collective intelligence framework. Therefore, we've completed a benchmark encompassing daily life questions, diverse corresponding responses, and majority vote ranking to train our helpfulness metric. Baseline experiments validate the efficacy of AdvisorQA through our helpfulness metric, GPT-4, and human evaluation, analyzing phenomena beyond the trade-off between helpfulness and harmlessness. AdvisorQA marks a significant leap in enhancing QA systems for providing personalized, empathetic advice, showcasing LLMs' improved understanding of human subjectivity",
    "checked": false,
    "id": "4c10b7de3262cb68e5146f385e6a0a36acb0aba1",
    "semantic_title": "advisorqa: towards helpful and harmless advice-seeking question answering with collective intelligence",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HHeDtTQibwg": {
    "title": "Corrective Retrieval Augmented Generation",
    "volume": "review",
    "abstract": "Large language models (LLMs) inevitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heavily on the relevance of retrieved documents, raising concerns about how the model behaves if retrieval goes wrong. To this end, we propose the Corrective Retrieval Augmented Generation (CRAG) to improve the robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different knowledge retrieval actions can be triggered. Since retrieval from static and limited corpora can only return sub-optimal documents, large-scale web searches are utilized as an extension for augmenting the retrieval results. Besides, a decompose-then-recompose algorithm is designed for retrieved documents to selectively focus on key information and filter out irrelevant information in them. CRAG is plug-and-play and can be seamlessly coupled with various RAG-based approaches. Experiments on four datasets covering short- and long-form generation tasks show that CRAG can significantly improve the performance of RAG-based approaches",
    "checked": true,
    "id": "5bbc2b5aa6c63c6a2cfccf095d6020b063ad47ac",
    "semantic_title": "corrective retrieval augmented generation",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=qlPyalLAiaA": {
    "title": "No-Skim: Towards Efficiency Robustness Evaluation on Skimming-based Language Models",
    "volume": "review",
    "abstract": "To reduce the computation cost and the energy consumption in large language models (LLM), skimming-based acceleration dynamically drops unimportant tokens of the input sequence progressively along layers of the LLM while preserving the tokens of semantic importance. However, our work for the first time reveals the acceleration may be vulnerable to \\textit{Denial-of-Service} (DoS) attacks. In this paper, we propose \\textit{No-Skim}, a general framework to help the owners of skimming-based LLM to understand and measure the efficiency robustness of their acceleration scheme. Specifically, our framework searches minimal and unnoticeable perturbations to generate adversarial inputs that sufficiently increase the remaining token ratio, thus increasing the computation cost and energy consumption. With no direct access to the model internals, we further devise a time-based approximation algorithm to infer the remaining token ratio as the loss oracle. We systematically evaluate the vulnerability of the skimming acceleration in various LLM architectures including BERT and RoBERTa on the GLUE benchmark. In the worst case, the perturbation found by \\textit{No-Skim} substantially increases the running cost of LLM by over 106\\% on average",
    "checked": true,
    "id": "86cbbc77aa623e5647fec585d5efbaf8089d4c13",
    "semantic_title": "no-skim: towards efficiency robustness evaluation on skimming-based language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gqBWLjEvclj": {
    "title": "Enhancing Multilingual Causal Commonsense Reasoning in LLMs: A Novel Assessment Approach and Strategy",
    "volume": "review",
    "abstract": "Commonsense reasoning is crucial for connecting premises to hypotheses by leveraging implicit world knowledge. The XCopa dataset, spanning 11 languages, serves as a benchmark for evaluating cross-lingual transfer capabilities in commonsense reasoning and emphasizes the importance of tapping into implicit knowledge for effective communication in diverse linguistic contexts. Recent advancements in Large Language Models (LLMs), such as Llama2, have made remarkable progress in Causal Commonsense Reasoning, setting new benchmarks. However, multilingual LLMs like XGLM and PolyLM face challenges due to smaller training datasets compared to English-centric LLMs. This work introduces a novel evaluation strategy, G-Evaluation, in the XCopa dataset. While this strategy resulted in decreased accuracy metrics across models, Llama2 showed improved performance, highlighting its adaptability. Despite efforts, multilingual XCopa models still fall behind their English counterparts in accuracy. Models like Llama2 exhibit performance variations across languages, underscoring the need for bridging this gap with Machine Translation (MT). To address this, we propose XTools, a strategy that combines Machine Translation and Automatic Post-Editing tools. By implementing XTools, multilingual accuracy can be elevated to 89.6\\%, aligning with English performance. Our contributions include redefining the evaluation method with G-Evaluation, introducing XTools for enhancing multilingual capabilities, validating Automatic Post-Editing Tool integration, and showcasing the potential of lightweight models in improving overall performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OaqajmP_xgJ": {
    "title": "FactoScalpel: Enhancing the Factual Consistency of Abstractive Summarization through Knowledge Injection",
    "volume": "review",
    "abstract": "Recently, abstractive summarization has problems with factual inconsistencies in generated summaries. Inspired by the related work on knowledge storage in Transformer, we firstly explore the relationship between factual errors and Feed-Forward Networks (FFNs) in Transformer, and propose factual errors attribution method. Based on the results, we inject knowledge to the decoder for the first time, propose a fact-aware summarization model FactoScalpel which integrates a Knowledge Bank and router-controlled mechanism into FFNs. By introducing facts through Knowledge Bank, balancing the original FNN with the newly added Knowledge Bank module through router-controlled mechanism, FactoScalpel achieves factual improvement of the decoder end through fine surgery. We compare FactoScalpel with multiple fact-aware summarization models using multiple factual consistency metrics based on the XSum, our method achieves state-of-the-art results in most experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CIK41gaxRwH": {
    "title": "Multi-Cultural Norm Base: Frame-based Norm Discovery in Multi-Cultural Settings",
    "volume": "review",
    "abstract": "Sociocultural norms serve as guiding principles for personal conduct in social interactions within a particular society or culture. The study of norm discovery has seen significant development over the last few years, with various interesting approaches. However, it is difficult to adopt these approaches to discover norms in a new culture, as they rely either on human annotations or real-world dialogue contents. This paper presents a robust automatic norm discovery pipeline, which utilizes the cultural knowledge of GPT-3.5 Turbo (ChatGPT) along with several social factors. By using these social factors and ChatGPT, our pipeline avoids the use of human dialogues that tend to be limited to specific scenarios, as well as the use of human annotations that make it difficult and costly to enlarge the dataset. The resulting database - Multi-cultural Norm Base (MNB) - covers 6 distinct cultures, with over 150k sociocultural norm statements in total. A state-of-the-art Large Language Model (LLM), Llama 2, fine-tuned with our proposed dataset, shows remarkable results on various downstream tasks, outperforming models fine-tuned on other datasets significantly",
    "checked": false,
    "id": "60cc0b1a573e75732d54ac688b84b7bd4a021b89",
    "semantic_title": "normsage: multi-lingual multi-cultural norm discovery from conversations on-the-fly",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=kE5iGPDc-V": {
    "title": "I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning with Style-Aligned Response Adjustments",
    "volume": "review",
    "abstract": "Fine-tuning large language models (LLMs) with a small data set for particular tasks is a widely encountered yet complex challenge. The potential for overfitting on a limited number of examples can negatively impact the model's ability to generalize and retain its original skills. Our research explores the impact of the style of ground-truth responses during the fine-tuning process. We found that matching the ground-truth response style with the LLM's inherent style results in better learning outcomes. Building on this insight, we developed a method that minimally alters the LLM's pre-existing responses to correct errors, using these adjusted responses as training targets. This technique enables precise corrections in line with the model's native response style, safeguarding the model's core capabilities and thus avoid overfiting. Our findings show that this approach not only improves the LLM's task-specific accuracy but also crucially maintains its original competencies and effectiveness",
    "checked": true,
    "id": "47ac1fe29e81d2737a595af2cab39687b75f33b3",
    "semantic_title": "i learn better if you speak my language: enhancing large language model fine-tuning with style-aligned response adjustments",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ArYhSA_DRC": {
    "title": "Unraveling the Complexities of Offensive Language: A Detailed Analytical Framework for Understanding Offensive Communication Dynamics",
    "volume": "review",
    "abstract": "Offensive online content can marginalize and cause harm to groups and individuals. To prevent harm while ensuring speech rights, fair and accurate detection is required. However, current models and data struggle to distinguish offensive language from acceptable, non-toxic language variations related to culture or subjective interpretation. This study presents a comprehensive toxicity assessment with two annotated datasets focusing on nuances of human interpretation with objective evaluation. The substantial increase in inter-annotator agreement indicates the effectiveness of structured guidelines at controlling subjective variability and strengthening result consistency. Additionally, we explore the effectiveness of in-context learning with few-shot examples to improve toxicity detection from large language models (LLMs), GPTs specifically, finding that explicit assessment criteria significantly improve agreement between automated and human evaluations of offensive content. The feasibility of criteria-based automatic annotations is evidenced by the better performance of smaller models fine-tuned on 10 times less auto-annotated data with multi-language variations. The findings demonstrate notable efficiency in combining contextual understanding of LLMs with criterion-guided in-context learning with limited data size and heterogeneous language types.Content Warning: This article only analyzes offensive language for academic purposes. Discretion is advised",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=reddQnmur6": {
    "title": "PolCLIP: A Unified Image-Text Word Sense Disambiguation Model via Generating Multimodal Complementary Representations",
    "volume": "review",
    "abstract": "Word sense disambiguation (WSD) is divided into two subtasks: textual word sense disambiguation (Textual-WSD) and visual word sense disambiguation (Visual-WSD). They aim to identify the most semantically relevant senses or images to a given context containing ambiguous target words. However, existing WSD models seldom address these two subtasks jointly due to lack of images in Textual-WSD datasets or lack of senses in Visual-WSD datasets. To bridge this gap, we propose PolCLIP, a unified image-text WSD model. By employing an image-text complementarity strategy, it simulates stable diffusion to generate implicit visual representations for senses and imitates image captioning to provide implicit textual representations for images. Additionally, a disambiguation-oriented image-sense dataset is constructed for the training objective of learning multimodal polysemy representations. To the best of our knowledge, PolCLIP is the first model that can cope with both Textual-WSD and Visual-WSD. Extensive experimental results on benchmarks demonstrate the effectiveness of our method, achieving a 2.53% F1-score increase over the state-of-the-art models on Textual-WSD and a 2.22% HR@1 improvement on Visual-WSD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TSJ3kYS2QJ": {
    "title": "Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question Answering Benchmark",
    "volume": "review",
    "abstract": "How to better evaluate the capabilities of Large Language Models (LLMs) is the focal point and hot topic in current LLMs research.Previous work has noted that due to the extremely high cost of iterative updates of LLMs, they are often unable to answer the latest dynamic questions well. To promote the improvement of Chinese LLMs' ability to answer dynamic questions, in this paper, we introduce $\\textbf{CDQA}$, a $\\textbf{C}$hinese $\\textbf{D}$ynamic $\\textbf{QA}$ benchmark containing question-answer pairs related to the latest news on the Chinese Internet. We obtain high-quality data through a pipeline that combines humans and models, and carefully classify the samples according to the frequency of answer changes to facilitate a more fine-grained observation of LLMs' capabilities. We have also evaluated and analyzed mainstream and advanced Chinese LLMs on $\\textit{CDQA}$. Extensive experiments and valuable insights suggest that our proposed $\\textit{CDQA}$ is challenging and worthy of more further study. We believe that the benchmark we provide will become the key data resource for improving LLMs' Chinese question-answering ability in the future",
    "checked": true,
    "id": "5c761ee41bc70fe2c65f9cedb06ebd7ec6705b43",
    "semantic_title": "let llms take on the latest challenges! a chinese dynamic question answering benchmark",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=wW0Q-zw7Wt": {
    "title": "Data Contamination Issues in Brain-to-Text Decoding",
    "volume": "review",
    "abstract": "Decoding non-invasive cognitive signals to natural language has long been the goal of building practical brain-computer interfaces (BCIs). Recent major milestones have successfully decoded cognitive signals like functional Magnetic Resonance Imaging (fMRI) and electroencephalogram (EEG) into text under open vocabulary setting. However, how to split the datasets for training, validating, and testing in brain-to-text decoding still remains controversial. Additionally, the issue of data contamination observed in prior research persists. In this study, we undertake a comprehensive analysis on current dataset splitting strategies and discover that data contamination significantly overstates the performance of models. Specifically, first we find the leakage of test subjects' cognitive signals corrupts the training of a robust encoder. Second, we prove the leakage of text stimuli causes the auto-regressive decoder to memorize seen information in test set. To eliminate the influence of data contamination and fairly evaluate different models' generalization ability, we propose a new splitting method for different types of cognitive dataset (e.g. fMRI, EEG). We also evaluate the performance of SOTA brain-to-text decoding models under the proposed dataset splitting paradigm as baselines for further research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DjvrDE49XvD": {
    "title": "PesTest: A Comprehensive Benchmark for Psychological Emotional Support Capability of Large Language Models",
    "volume": "review",
    "abstract": "Large language models with good psychological emotional support capabilities can provide users with effective psychological comfort and help users maintain a good psychological environment. However, there is currently a lack of evaluation datasets with a comprehensive psychological system for the psychological emotional support capabilities of large language models. In this paper, we propose PesTest, a large language model psychological emotional support capability assessment benchmark with comprehensive topics and rich task types. PesTest has a comprehensive psychological system, specifically including 7 major categories and 40 sub-categories of topics. We use PesTest to evaluate the performance of existing large language models on psychological emotional support tasks and discover their deficiencies on certain topics, making up for the shortcomings in comprehensiveness of previous evaluations. Furthermore, we fine-tune the model using PesTest's training set and achieve better results than the original model on the test set, which proves the effect of PesTest on improving the psychological emotional support capabilities of large language models and provides a reference for future research. We will make our benchmark publicly available at Anonymous_Link",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=urZOcvj6Qmu": {
    "title": "Author Name Disambiguation via Graph-Enhanced Language Model Fine-Tuning",
    "volume": "review",
    "abstract": "Author name disambiguation (AND) serves as a core component of modern academic search systems to curate author profiles and bibliometrics. Recently, language models (LMs) and graph neural networks (GNNs) have significantly pushed the frontier of modeling textual and relational information. However, their representation powers are not fully exploited to improve the accuracy of AND. In this work, we propose a unified model -- graph-enhanced language model (i.e., GAND) that enables joint modeling of the text information and relations between documents. Compared to the traditional contrastive loss, we develop a multi-task fine-tuning objective. This not only mitigates potential distribution shifts in testing data but also improves the efficiency of fine-tuning language models for AND.Experiments on two real datasets for name disambiguation demonstrate the superior performance of our approach over embedding-based approaches, fine-tuning LMs and OpenAI's text embeddings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sfvWL7sX0ri": {
    "title": "Multimodal Entity Linking with Dynamic Modality Selection and Interactive Prompt Learning",
    "volume": "review",
    "abstract": "Recent advances in Multimodal Entity Linking (MEL) utilize multimodal information to link target mentions to corresponding entities. However, existing methods uniformly adopt a \"one-size-fits-all\" approach, ignoring individual sample needs and modality-induced noise. Also, the commonly used separate large-scale visual and text pre-trained models for feature extraction do not address inter-modal heterogeneity and the high computational cost of fine-tuning. To resolve these two issues, this paper introduces a novel approach named Multimodal Entity Linking with Dynamic Modality Selection and Interactive Prompt Learning (DSMIP). First, we design three expert networks that utilize different subsets of modalities to tackle the task and train them individually. In particular, for the multimodal expert network, we extract multimodal features of entities and mentions by updating multimodal prompts and set up a coupling function to realize the interaction of prompts between modalities. Subsequently, to select the best-suited expert network for each specific sample, we devise a Modality Selection Gating Network to gain the optimal one-hot selection vector by applying a specialized reparameterization technique and a two-stage training. Experimental results on three public benchmark datasets demonstrate that our solution outperforms the majority of state-of-the-art baselines and surpasses all baselines in settings with low training resources",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eQSSGjwPS4d": {
    "title": "ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages",
    "volume": "review",
    "abstract": "Tool learning is widely acknowledged as a foundational approach or deploying large language models (LLMs) in real-world scenarios. While current research primarily emphasizes leveraging tools to augment LLMs, it frequently neglects emerging safety considerations tied to their application. To fill this gap, we present $ToolSword$, a comprehensive framework dedicated to meticulously investigating safety issues linked to LLMs in tool learning. Specifically, ToolSword delineates six safety scenarios for LLMs in tool learning, encompassing $malicious$ $queries$ and $jailbreak$ $attacks$ in the input stage, $noisy$ $misdirection$ and $risky$ $cues$ in the execution stage, and $harmful$ $feedback$ and $error$ $conflicts$ in the output stage. Experiments conducted on 11 open-source and closed-source LLMs reveal enduring safety challenges in tool learning, such as handling harmful queries, employing risky tools, and delivering detrimental feedback, which even GPT-4 is susceptible to. Moreover, we conduct further studies with the aim of fostering research on tool learning safety. The data will be released upon acceptance of the paper",
    "checked": true,
    "id": "9f25324c89686495afb697cdb79f98d79092b843",
    "semantic_title": "toolsword: unveiling safety issues of large language models in tool learning across three stages",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=R9k0o7rDP7x": {
    "title": "NoisyICL: A Little Noise in Model Parameters Calibrates In-context Learning",
    "volume": "review",
    "abstract": "In-Context Learning (ICL) is suffering from unsatisfactory performance and under-calibration due to high prior bias and unfaithful confidence. Some previous works fine-tuned language models for better ICL performance with enormous datasets and computing costs. In this paper, we propose NoisyICL, simply perturbing the model parameters by random noises to strive for better performance and calibration. Our experiments on two models and 12 downstream datasets show that NoisyICL can help ICL produce more accurate predictions. Our further analysis indicates that NoisyICL enables the model to provide more fair predictions, and also with more faithful confidence. Therefore, we believe that NoisyICL is an effective calibration of ICL. Our experimental code is uploaded to Github",
    "checked": true,
    "id": "2ce8670954b430ad1a9368f80bda48f7ba363ddd",
    "semantic_title": "noisyicl: a little noise in model parameters calibrates in-context learning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=HP8s_R-j3X2": {
    "title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning",
    "volume": "review",
    "abstract": "Recently, numerous new benchmarks have been established to evaluate the performance of large language models (LLMs) via either computing a holistic score or employing another LLM as a judge. However, these approaches suffer from data leakage due to the open access of the benchmark and inflexible evaluation process. To address this issue, we introduce **TreeEval**, a benchmark-free evaluation method for LLMs that let a high-performance LLM host an irreproducible evaluation session and essentially avoids the data leakage. Moreover, this LLM performs as an examiner to raise up a series of questions under a topic with a tree planing strategy, which considers the current evaluation status to decide the next question generation and ensures the completeness and efficiency of the evaluation process. We evaluate $6$ models of different parameter sizes, including $7$B, $13$B, and $33$B, and ultimately achieved the highest correlation coefficient with AlpacaEval2.0 using only around $45$ questions. We also conduct more analysis to show the robustness and reliability of TreeEval. Our code can be accessed via the provided [URL](Anonymous github repository)",
    "checked": true,
    "id": "da3c41164c7502709cca508710127336e138621e",
    "semantic_title": "treeeval: benchmark-free evaluation of large language models through tree planning",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=j5VG0lvMAHh": {
    "title": "Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues",
    "volume": "review",
    "abstract": "A successful negotiation demands a deep comprehension of the conversation context, Theory-of-Mind (ToM) skills to infer the partner's motives, as well as strategic reasoning and effective communication, making it challenging for automated systems. Given the remarkable performance of LLMs across a variety of NLP tasks, in this work, we aim to understand how LLMs can advance different aspects of negotiation research, ranging from designing dialogue systems to providing pedagogical feedback and scaling up data collection practices. To this end, we devise a methodology to analyze the multifaceted capabilities of LLMs across diverse dialogue scenarios covering all the time stages of a typical negotiation interaction. Our analysis adds to the increasing evidence for the superiority of GPT-4 across various tasks while also providing insights into specific tasks that remain difficult for LLMs. For instance, the models correlate poorly with human players when making subjective assessments about the negotiation dialogues and often struggle to generate responses that are contextually appropriate as well as strategically advantageous",
    "checked": true,
    "id": "4e61fa433eb1a905309f5955a75235e74f7eb6b2",
    "semantic_title": "are llms effective negotiators? systematic evaluation of the multifaceted capabilities of llms in negotiation dialogues",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=_Gei3ULJ3cQ": {
    "title": "MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception",
    "volume": "review",
    "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in visual perception and understanding. However, these models also suffer from hallucinations, which limit their reliability as AI systems. We believe that these hallucinations are partially due to the models' struggle with understanding what they can and cannot perceive from images, a capability we refer to as self-awareness in perception. Despite its importance, this aspect of MLLMs has been largely unexplored in prior studies. The study in this paper aims to define and evaluate the self-awareness of MLLMs in perception. To do this, we first introduce the knowledge quadrant in perception, which helps define what MLLMs know and do not know about images. Using this framework, we propose a novel benchmark, the Self-Awareness in Perception for MLLMs (MM-SAP), specifically designed to assess this capability. We apply MM-SAP to a variety of popular MLLMs, offering a comprehensive analysis of their self-awareness and providing detailed insights. The experiment results reveal that current MLLMs possess limited self-awareness capabilities, pointing to a crucial area for future advancement in the development of reliable MLLMs",
    "checked": true,
    "id": "f48b95576bd50a69937d8bbff0cc42bdb20e49f5",
    "semantic_title": "mm-sap: a comprehensive benchmark for assessing self-awareness of multimodal large language models in perception",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=UmiAkx80Pzd": {
    "title": "Large Language Model and Knowledge Graph Entangled Logical Reasoning",
    "volume": "review",
    "abstract": "Large language models (LLMs) and knowledge graphs (KGs) have complementary strengths and weaknesses for logical reasoning. LLMs exhibit strong semantic reasoning capacities, but they lack world knowledge and structured reasoning abilities. KGs contain extensive factual knowledge but have limited language understanding and reasoning flexibility. In this paper, we propose a framework LKLR that entangles LLMs and KGs for synergistic reasoning. A key technique is transforming the LLM's implicit reasoning chain into a grounded logical query over the KG, enabling seamless integration. Traversing this query grounds each inference step in KG facts while maintaining reasoning flow, combining the robust knowledge of KGs with the semantic reasoning of LLMs. Our approach synergistically integrates neural and symbolic reasoning to achieve hybrid reasoning capabilities. Experimental results on several QA benchmarks show that our proposed framework achieves state-of-the-art performance and provides transparent and reliable reasoning",
    "checked": false,
    "id": "4362edfd3907204cf1b7ec8e3c16c56db5cd14cf",
    "semantic_title": "large language models-guided dynamic adaptation for temporal knowledge graph reasoning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=A3P33ZCY-j-": {
    "title": "Advancing Parameter Efficiency in Fine-tuning via Representation Editing",
    "volume": "review",
    "abstract": "Parameter Efficient Fine-Tuning (PEFT) has gained significant attention for its ability to achieve competitive results while updating only a small subset of trainable parameters. Despite the promising performance of current PEFT methods, they present challenges in hyperparameter selection, such as determining the rank of LoRA or Adapter, or specifying the length of soft prompts. In addressing these challenges, we propose a novel approach to fine-tuning neural models, termed Representation EDiting (RED), which scales and biases the representation produced at each layer. RED substantially reduces the number of trainable parameters by a factor of $25,700$ compared to full parameter fine-tuning, and by a factor of $32$ compared to LoRA. Remarkably, RED achieves comparable or superior results to full parameter fine-tuning and other PEFT methods.Extensive experiments were conducted across models of varying architectures and scales, including RoBERTa, GPT-2, T5, and Llama-2, and the results demonstrate the efficiency and efficacy of RED, positioning it as a promising PEFT approach for large neural models",
    "checked": true,
    "id": "2bcd4c8e0935662344c2bab479ef8b6b5fffcc50",
    "semantic_title": "advancing parameter efficiency in fine-tuning via representation editing",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=xr_ueWk3R54": {
    "title": "Enforcing Paraphrase Generation via Controllable Latent Diffusion",
    "volume": "review",
    "abstract": "Paraphrase generation aims to produce high-quality and diverse utterances of a given text. Though state-of-the-art generation via the diffusion model reconciles generation quality and diversity, textual diffusion suffers from a truncation issue that hinders efficiency and quality control. In this work, we propose \\textit{L}atent \\textit{D}iffusion \\textit{P}araphraser~(LDP), a novel paraphrase generation by modeling a controllable diffusion process given a learned latent space. LDP achieves superior generation efficiency compared to its diffusion counterparts. It facilitates only input segments to enforce paraphrase semantics, which further improves the results without external features. Experiments show that LDP achieves improved and diverse paraphrase generation compared to baselines. Further analysis shows that our method is also helpful to other similar text generations and domain adaptations. Our code and data are available at https://anonymous.4open.science/r/8F72",
    "checked": true,
    "id": "dc8f6ce0181f268e19579a981431540f21c0a27d",
    "semantic_title": "enforcing paraphrase generation via controllable latent diffusion",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t9B8fjF7Vbk": {
    "title": "BERT fine-tuning for Japanese Author Attribution using Stylometric Features",
    "volume": "review",
    "abstract": "Authorship Attribution (AA) is the task of identifying the author of a text. It marks a novel effort in applying deep learning techniques for AA in Japanese, a field where such approaches have been limited. Historically, AA studies in Japanese have predominantly employed Random Forests and SVMs, focusing on small author groups and facing a scarcity of datasets for author identification. Our work diverges by fine-tuning a pre-trained BERT model to assess its efficacy in both text-only scenarios and when incorporating Japanese-specific stylistic features. Key findings reveal an 84% accuracy rate for identifying five authors using only text data, and a notable 82% accuracy with an expanded set of 80 authors, highlighting the potential of deep learning for managing larger author pools. However, the addition of stylistic features for a set of 25 authors resulted in reduced accuracy (53%). The study further achieved 97% accuracy in distinguishing Japanese speakers and 61% in nationality prediction. These outcomes emphasize the viability of deep learning-based AA in Japanese, presenting a significant advancement in the domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=de6HitL-BDt": {
    "title": "A Unified Generative Framework for Bilingual Euphemism Detection and Identification",
    "volume": "review",
    "abstract": "Various euphemisms are emerging in social networks, attracting widespread attention from the natural language processing community. However, existing euphemism datasets are only domain-specific or language-specific. In addition, existing approaches to the study of euphemisms are one-sided. Either only the euphemism detection task or only the euphemism identification task is accomplished, lacking a unified framework. To this end, we construct a large-scale Bilingual Multi-category dataset of Euphemisms named BME, which covers a total of 12 categories for two languages, English and Chinese. Then, we first propose a unified generative model to Jointly conduct the tasks of bilingual Euphemism Detection and Identification named JointEDI. By comparing with LLMs and human evaluation, we demonstrate the effectiveness of the proposed JointEDI and the feasibility of unifying euphemism detection and euphemism identification tasks. Moreover, the BME dataset also provides a new reference standard for euphemism detection and euphemism identification",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tAQ3-YmqgfY": {
    "title": "Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack",
    "volume": "review",
    "abstract": "Recent developments in balancing the usefulness and safety of Large Language Models (LLMs) have raised a critical question: Are mainstream NLP tasks adequately aligned with safety consideration? Our study, focusing on safety-sensitive documents obtained through adversarial attacks, reveals significant disparities in the safety alignment of various NLP tasks. For instance, LLMs can effectively summarize malicious long documents but often refuse to translate them. This discrepancy highlights a previously unidentified vulnerability: attacks exploiting tasks with weaker safety alignment, like summarization, can potentially compromise the integrity of tasks traditionally deemed more robust, such as translation and question-answering (QA). Moreover, the concurrent use of multiple NLP tasks with lesser safety alignment increases the risk of LLMs inadvertently processing harmful content. We demonstrate these vulnerabilities in various safety-aligned LLMs, particularly Llama2 models, Gemini and GPT-4, indicating an urgent need for strengthening safety alignments across a broad spectrum of NLP tasks",
    "checked": true,
    "id": "9763c526f667de1f853fdcf59b29f1ac5ddd33f1",
    "semantic_title": "safety alignment in nlp tasks: weakly aligned summarization as an in-context attack",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=R-9vMMbF3uY": {
    "title": "HyperCL: A Contrastive Learning Framework for Hyper-Relational Knowledge Graph Embedding with Hierarchical Ontology",
    "volume": "review",
    "abstract": "Knowledge Graph (KG) embeddings are essential for link prediction over KGs. Compared to triplets, hyper-relational facts consisting of a base triplet and an arbitrary number of key-value pairs, can better characterize real-world facts and have aroused various hyper-relational embedding techniques recently. Nevertheless, existing works seldom consider the ontology of KGs, which is beneficial to link prediction tasks. A few studies attempt to incorporate the ontology information, by either utilizing the ontology as constraints on entity representations or jointly learning from hyper-relational facts and the ontology. However, existing approaches mostly overlook the ontology hierarchy and suffer from the dominance issue of facts over ontology, resulting in suboptimal performance. Against this background, we propose a universal contrastive learning framework for hyper-relational KG embeddings ($\\textbf{HyperCL}$), which is flexible to integrate different hyper-relational KG embedding methods and effectively boost their link prediction performance. HyperCL designs relation-aware Graph Attention Networks to capture the hierarchical ontology and a concept-aware contrastive loss to alleviate the dominance issue. We evaluate HyperCL on three real-world datasets in different link prediction tasks. Experimental results show that HyperCL consistently boosts the performance of state-of-the-art baselines with an average improvement of 3.1-7.4% across the three datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ktcAF6gQpF": {
    "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models",
    "volume": "review",
    "abstract": "Several post-training quantization methods have been applied to large language models (LLMs), and have been shown to perform well down to 8-bits. We find that these methods break down at lower bit precision, and investigate quantization aware training for LLMs (LLM-QAT) to push quantization levels even further. We propose a data-free distillation method that leverages generations produced by the pre-trained model, which better preserves the original output distribution and allows quantizing any generative model independent of its training data, similar to post-training quantization methods. In addition to quantizing weights and activations, we also quantize the KV cache, which is critical for increasing throughput and support long sequence dependencies at current model sizes. We experiment with LLaMA models of sizes 7B, 13B, and 30B, at quantization levels down to 4-bits. We observe large improvements over training-free methods, especially in the low-bit settings",
    "checked": true,
    "id": "6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2",
    "semantic_title": "llm-qat: data-free quantization aware training for large language models",
    "citation_count": 95,
    "authors": []
  },
  "https://openreview.net/forum?id=FtAxkSyElq": {
    "title": "Which Words Matter? Understanding How Large Language Models Comprehend Arguments",
    "volume": "review",
    "abstract": "Pioneering developments in large-scale language models (LLMs) have marked a substantial stride in their ability to comprehend multifaceted debate topics and to construct argumentative narratives. Despite this progress, there remains a notable lack of scholarly understanding of the processes by which LLMs engage with and analyze computational arguments. Classical studies have delved into the linguistic frameworks of arguments, encapsulating their essence within the realms of structural organization and logical coherence. Yet, it remains unclear whether LLMs utilize these recognized frameworks in addressing argument-related tasks. In an effort to illuminate this research void, our study introduces three hypotheses centered on the dynamics of claim, evidence and stance identification in argument mining tasks: 1) Omitting specific logical connectors in an argument does not change the implicit logical relationship, and LLMs can learn it from the modified context. 2) The importance of words or phrases in an argument is determined by the extent of implicit information they encapsulate, regardless of their individual components within the structure of the argument. 3) Removing crucial words or phrases from an argument alters the implicit logical relationship, making it impossible for LLMs to learn the original logic from the modified text.Through comprehensive assessments on the standard IAM dataset, it is revealed that information contained in the phrases within the argument has a greater impact on the understanding of the argument by large models, and the experiment results validate our hypothesis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Y97ULf0q-": {
    "title": "The Gaps between Pre-train and Downstream Settings in \\\\Bias Evaluation and Debiasing",
    "volume": "review",
    "abstract": "The output tendencies of PLMs vary markedly before and after FT due to the updates to the model parameters.These divergences in output tendencies result in a gap in the social biases of PLMs.For example, there exists a low correlation between the intrinsic bias scores of a PLM and its extrinsic bias scores under FT-based debiasing methods.Additionally, applying FT-based debiasing methods to a PLM leads to a decline in performance in downstream tasks.On the other hand, PLMs trained on large datasets can learn without parameter updates via ICL using prompts.ICL induces smaller changes to PLMs compared to FT-based debiasing methods.Therefore, we hypothesize that the gap observed in pre-trained and FT models does not hold true for debiasing methods that use ICL.In this study, we demonstrate that ICL-based debiasing methods show a higher correlation between intrinsic and extrinsic bias scores compared to FT-based methods.Moreover, the performance degradation due to debiasing is also lower in the ICL case compared to that in the FT case",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e6jPnUqOg4": {
    "title": "Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models",
    "volume": "review",
    "abstract": "Temporal knowledge graph question answering (TKGQA) poses a significant challenge task, due to the temporal constraints hidden in questions and the answers sought from dynamic structured knowledge. Although large language models (LLMs) have made considerable progress in their reasoning ability over structured data, their application to the TKGQA task is a relatively unexplored area. This paper first proposes a novel generative temporal knowledge graph question answering framework, GenTKGQA, which guides LLMs to answer temporal questions through two phases: Subgraph Retrieval and Answer Generation. First, we exploit LLM's intrinsic knowledge to mine temporal constraints and structural links in the questions without extra training, thus narrowing down the subgraph search space in both temporal and structural dimensions. Next, we design virtual knowledge indicators to fuse the graph neural network signals of the subgraph and the text representations of the LLM in a non-shallow way, which helps the open-source LLM deeply understand the temporal order and structural dependencies among the retrieved facts through instruction tuning. Experimental results demonstrate that our model outperforms state-of-the-art baselines, even achieving 100% on the metrics for the simple question type",
    "checked": true,
    "id": "5309e53a67834dcb2db3ddd75ce5a1128da97d40",
    "semantic_title": "two-stage generative question answering on temporal knowledge graph using large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=UJMvlJ5a7x": {
    "title": "PDF-to-Tree: Parsing PDF Text Blocks into a Tree",
    "volume": "review",
    "abstract": "In PDF documents, the reading order of text blocks is missing, which can hinder machine understanding of the document's content.Existing works try to extract one universal reading order for a PDF file.However, applications, like Retrieval Augmented Generation (RAG), require breaking long articles into sections and subsections for better indexing.For this reason, this paper introduces a new task and dataset, PDF-to-Tree, which organizes the text blocks of a PDF into a tree structure.Since a PDF may contain thousands of text blocks, far exceeding the number of words in a sentence, this paper proposes a transition-based parser that uses a greedy strategy to build the tree structure.Compared to parser for plain text, we also use multi-modal features to encode the parser state.Experiments show that our approach achieves an accuracy of 93.93%, surpassing the performance of baseline methods by an improvement of 12.22%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ryRLknhke0": {
    "title": "MARVEL: Unlocking the Multi-Modal Capability of Dense Retrieval via Visual Module Plugin",
    "volume": "review",
    "abstract": "This paper proposes Multi-modAl Retrieval model via Visual modulE pLugin (MARVEL), which learns an embedding space for queries and multi-modal documents to conduct retrieval. MARVEL encodes queries and multi-modal documents with a unified encoder model, which helps to alleviate the modality gap between images and texts. Specifically, we enable the image understanding ability of the well-trained dense retriever, T5-ANCE, by incorporating the visual module's encoded image features as its inputs. To facilitate the multi-modal retrieval tasks, we build the ClueWeb22-MM dataset based on the ClueWeb22 dataset, which regards anchor texts as queries, and exacts the related text and image documents from anchor-linked web pages. Our experiments show that MARVEL significantly outperforms the state-of-the-art methods on the multi-modal retrieval dataset WebQA and ClueWeb22-MM. MARVEL provides an opportunity to broaden the advantages of text retrieval to the multi-modal scenario. Besides, we also illustrate that the language model has the ability to extract image semantics and partly map the image features to the input word embedding space. All source codes will be released via GitHub",
    "checked": true,
    "id": "4f407e3baaa12da7470de49303e8b6bc129317af",
    "semantic_title": "marvel: unlocking the multi-modal capability of dense retrieval via visual module plugin",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NsrEQuY4Tu6": {
    "title": "Deterministic Reversible Data Augmentation for Neural Machine Translation",
    "volume": "review",
    "abstract": "Data augmentation is an effective way to diversify corpora in machine translation, but previous methods may introduce semantic inconsistency between original and augmented data because of irreversible operations and random subword sampling procedures. To generate both symbolically diverse and semantically consistent augmentation data, we propose Deterministic Reversible Data Augmentation (DRDA), a simple but effective data augmentation method for neural machine translation. DRDA adopts deterministic segmentations and reversible operations to generate multi-granularity subword representations and pulls them closer together with multi-view techniques. With no extra corpora or model changes required, DRDA outperforms strong baselines on several translation tasks with a clear margin (up to 4.3 BLEU gain over Transformer) and exhibits good robustness in noisy, low-resource, and cross-domain datasets. The code will be released at Anonymous Link",
    "checked": true,
    "id": "c848ba9db8b4a2aa08f28c29dd8f612ccd43d24f",
    "semantic_title": "deterministic reversible data augmentation for neural machine translation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PBZnUMCjfVc": {
    "title": "LLM Factoscope: Uncovering LLMs' Factual Discernment through Measuring Inner States",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have revolutionized various domains with extensive knowledge and creative capabilities. However, a critical issue with LLMs is their tendency to produce outputs that diverge from factual reality. This phenomenon is particularly concerning in sensitive applications such as medical consultation and legal advice, where accuracy is paramount. Inspired by human lie detectors using physiological responses, we introduce the LLM Factoscope, a novel Siamese network-based model that leverages the inner states of LLMs for factual detection. Our investigation reveals distinguishable patterns in LLMs' inner states when generating factual versus non-factual content. We demonstrate its effectiveness across various architectures, achieving over 96% accuracy on our custom-collected factual detection dataset. Our work opens a new avenue for utilizing LLMs' inner states for factual detection and encourages further exploration into LLMs' inner workings for enhanced reliability and transparency",
    "checked": false,
    "id": "b9ee59e296dc2d7d4ae3f3346a1f0ba7ea881570",
    "semantic_title": "llm factoscope: uncovering llms' factual discernment through inner states analysis",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WDuTN-RA8VE": {
    "title": "Towards Semantic Consistency Data Augmentation for Bio-Relation Extraction via Biomedical Notion Infusion",
    "volume": "review",
    "abstract": "Biomedical Relation Extraction (Bio-RE) aims to recognize and classify the potential relations between various molecules and biomolecules. The main obstacle in Bio-RE is the scarcity of annotations especially in low-resource relation labels, thus the models cannot fully understand the connection between chemicals and diseases or drug-drug interactions. Existing works usually adopted data augmentation approaches to generate pseudo-annotated instances to alleviate the scarcity of annotations. However, the generated sentences largely ignore the semantic consistency of the biomedical domain and the logical coherence between biomolecules and diseases, causing a fatal phenomenon that the generated sentences introduce counterfactual information when learning the interactions between the drugs or diseases. To this end, this paper proposes a bio-notion-dedicated data augmentation approach that is able to measure intersections between biomedical relation notions and tokens of each instance to generate augmented data with semantic consistency. Experimental results demonstrate that our proposed method could bring 5.61% F1 improvement over SoTA baseline methods on three benchmark Bio-RE datasets in terms of BLURB",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OOCRYJIAMS7": {
    "title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems",
    "volume": "review",
    "abstract": "Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,952 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of 17.23\\% on OlympiadBench, with a mere 11.28\\% in physics, highlighting the benchmark rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies. We hope that our challenging benchmark can serve as a valuable resource for helping future AGI research endeavors",
    "checked": true,
    "id": "bcf2c7e3f4ed64c8294c35a59220a26dd4f40060",
    "semantic_title": "olympiadbench: a challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=W7MdC0WdQB": {
    "title": "ManiTweet: A New Benchmark for Identifying Manipulation of News on Social Media",
    "volume": "review",
    "abstract": "Considerable advancements have been made to tackle the misrepresentation of information derived from reference articles in the domains of fact-checking and faithful summarization. However, an unaddressed aspect remains - the identification of social media posts that manipulate information presented within associated news articles. This task presents a significant challenge, primarily due to the prevalence of personal opinions in such posts. We present a novel task, identifying manipulation of news on social media, which aims to detect manipulation in social media posts. To study this task, we have proposed a data collection schema and curated a dataset called ManiTweet, consisting of 3.6K pairs of tweets and corresponding articles. Our analysis demonstrates that this task is highly challenging, with large language models (LLMs) yielding unsatisfactory performance. Additionally, we have developed a simple yet effective framework that outperforms LLMs significantly on the ManiTweet dataset. Finally, we have conducted an exploratory analysis of human-written tweets, unveiling intriguing connections between manipulation and factuality of news articles",
    "checked": true,
    "id": "122dbf4c968a448fb2dde6c5c4a10f2697364531",
    "semantic_title": "manitweet: a new benchmark for identifying manipulation of news on social media",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Csoe6yqxgYv": {
    "title": "MARIO: MAth Reasoning with code Interpreter Output - A Reproducible Pipeline",
    "volume": "review",
    "abstract": "Large language models (LLMs) have significantly improved in understanding natural language but still lack in mathematical reasoning, a hurdle on the path to true artificial general intelligence. The training of large language models, based on next-token prediction, struggles to capture the precise nature of mathematical reasoning, presenting both practical and theoretical challenges. In this paper, we address this challenge by enriching the data landscape and introducing a reasonable data format, enhanced the text analysis of the LLM with a capability to utilize a Python code interpreter. This dataset is derived from GSM8K and MATH and has been further refined through a combination of GPT annotations, human review, and self-training processes. Additionally, we propose a tentative, easily replicable protocol for the fine-tuning of math-specific LLMs, which has led to a significant improvement in the performance of a 7B-parameter LLM on the GSM8K and MATH datasets. We are committed to advancing the field of mathematical reasoning in LLMs and, to that end, we will make the source code and checkpoints publicly available",
    "checked": true,
    "id": "675629ff78cef09665a1135fece66195ed80a640",
    "semantic_title": "mario: math reasoning with code interpreter output - a reproducible pipeline",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=t6TBZkSc8C": {
    "title": "ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios",
    "volume": "review",
    "abstract": "Existing evaluations of tool learning primarily focus on validating the alignment of selected tools for large language models (LLMs) with expected outcomes. However, these approaches rely on a limited set of scenarios where answers can be pre-determined. Furthermore, a $sole$ emphasis on outcomes disregards the intricate capabilities essential for LLMs to effectively utilize tools. To tackle this issue, we propose $ToolEyes$, a fine-grained system tailored for the evaluation of the LLMs' tool learning capabilities in authentic scenarios. The system meticulously examines seven real-world scenarios, analyzing five dimensions crucial to LLMs in tool learning: $format$ $alignment$, $intent$ $comprehension$, $behavior$ $planning$, $tool$ $selection$, and $answer$ $organization$. Additionally, ToolEyes incorporates a tool library boasting approximately 600 tools, serving as an intermediary between LLMs and the physical world. Evaluations involving ten LLMs across three categories reveal a preference for specific scenarios and limited cognitive abilities in tool learning. Intriguingly, expanding the model size even exacerbates the hindrance to tool learning. These findings offer instructive insights aimed at advancing the field of tool learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WNSy6e1EwB": {
    "title": "NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models",
    "volume": "review",
    "abstract": "The considerable size of Large Language Models (LLMs) presents notable deployment challenges, particularly on resource-constrained hardware. Structured pruning, offers an effective means to compress LLMs, thereby reducing storage costs and enhancing inference speed for more efficient utilization. In this work, we study data-efficient and resource-efficient structure pruning methods to obtain smaller yet still powerful models. Knowledge Distillation is well-suited for pruning, as the intact model can serve as an excellent teacher for pruned students. However, it becomes challenging in the context of LLMs due to memory constraints. To address this, we propose an efficient progressive Numerous-teacher pruning method (NutePrune). NutePrune mitigates excessive memory costs by loading only one intact model and integrating it with various masks and LoRA modules, enabling it to seamlessly switch between teacher and student roles. This approach allows us to leverage numerous teachers with varying capacities to progressively guide the pruned model, enhancing overall performance. Extensive experiments across various tasks demonstrate the effectiveness of NutePrune. In LLaMA-7B zero-shot experiments, NutePrune retains 97.17% of the performance of the original model at 20% sparsity and 95.07% at 25% sparsity",
    "checked": true,
    "id": "9495f0f50ef9a3225e8e230746a8f139d27ce8cc",
    "semantic_title": "nuteprune: efficient progressive pruning with numerous teachers for large language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=PXqSZ4DyZY": {
    "title": "DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows",
    "volume": "review",
    "abstract": "Large language models (LLMs) have become a dominant and important tool for NLP researchers in a wide range of tasks. Today, many researchers use LLMs in synthetic data generation, task evaluation, fine-tuning, distillation, and other model-in-the-loop research workflows. However, challenges arise when using these models that stem from their scale, their closed source nature, and the lack of standardized tooling for these new and emerging workflows. The rapid rise to prominence of these models and these unique challenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them. In this ACL 2024 theme track paper, we introduce DataDreamer, an open source Python library that allows researchers to write simple code to implement powerful LLM workflows. DataDreamer also helps researchers adhere to best practices that we propose to encourage open science and reproducibility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=exr9PEGRkq": {
    "title": "RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning",
    "volume": "review",
    "abstract": "Tool learning has generated widespread interest as a vital means of interaction between Large Language Models (LLMs) and the physical world. Current research predominantly emphasizes LLMs' capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world. To bridge this gap, we introduce $RoTBench$, a multi-level benchmark for evaluating the robustness of LLMs in tool learning. Specifically, we establish five external environments, each featuring varying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union), providing an in-depth analysis of the model's resilience across three critical phases: tool selection, parameter identification, and content filling. Experiments involving six widely-used models underscore the urgent necessity for enhancing the robustness of LLMs in tool learning. For instance, the performance of GPT-4 even drops significantly from 80.00 to 58.10 when there is no substantial change in manual accuracy. More surprisingly, the noise correction capability inherent in the GPT family paradoxically impedes its adaptability in the face of mild noise. In light of these findings, we propose RoTTuning, a strategy that enriches the diversity of training environments to bolster the robustness of LLMs in tool learning",
    "checked": true,
    "id": "d6beb9cc394f1e2046371678737346f05270ca91",
    "semantic_title": "rotbench: a multi-level benchmark for evaluating the robustness of large language models in tool learning",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=ddl6ifBONyT": {
    "title": "Latent Learningscape Guided In-context Learning",
    "volume": "review",
    "abstract": "The growing interest in leveraging large language models is driven by their exceptional imitation and reasoning capabilities. In-context learning (ICL), a streamlined method, has shown potential in boosting these models' performance without modifying their underlying parameters, especially when supplied with suitable demonstrations. However, existing methods mainly choose demonstrations by comparing surface-level semantic similarities (e.g., based on embedding) and fall short of identifying the most fitting ones. This paper introduces the concept of a ``latent learningscape'', a more nuanced representation that describes the characteristic of the demonstrations. Building on this concept, we develop a results-driven approach to characterize the latent learningscape features of demonstrations, which then inform the creation of more effective prompts. Through comprehensive testing across datasets in arithmetic, commonsense, and symbolic reasoning tasks, our approach outperforms leading models, showing an average increase in scores by 7.4 percentage points",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hb5FWWlhVgZ": {
    "title": "Weak Supervision Text Classification using Cosine Similarity and SVM for Hardware Constrained Systems",
    "volume": "review",
    "abstract": "Weakly supervised text classification is the ability to classify large, diverse types of unstructured text data while requiring only a small amount of manual guidance. With open-source pre-trained language models becoming widely available in the last couple of years, the weak supervision text classification domain has received renewed interest due to the potential for transfer learning. Recent weak supervision methods proposed using pre-trained language models have performed well against the popular WRENCH benchmark datasets (Zhang et al., 2021), demonstrating the capability of transfer learning. However, these methods use pre-trained language models that are computationally expensive to perform inference with and are unfeasible to finetune without specialized accelerated hardware. Methods that don't require fine-tuning often require repeated inference or large storage needs to achieve their results. In this paper, an alternative solution is proposed that uses a single inference step, has minimal storage and memory requirements, doesn't require accelerated hardware, and can provide competitive results to much more hardware-intensive methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j0cFHHJT-p": {
    "title": "Mitigating Social Biases in Language Models through Unlearning",
    "volume": "review",
    "abstract": "Mitigating bias in language models (LMs) has become a critical problem due to the widespread deployment of LMs. Numerous approaches revolve around data pre-processing and fine-tuning of language models, tasks that can be both time-consuming and computationally demanding. Consequently, there is a growing interest in machine unlearning techniques given their capacity to induce the forgetting of undesired behaviors of the existing pre-trained or fine-tuned models with lower computational cost. In this work, we explore two unlearning methods, (1) Partitioned Contrastive Gradient Unlearning (PCGU) applied on decoder models and (2) Negation via Task Vector, to reduce social biases in state-of-the-art and open-source LMs such as LLaMA-2 and OPT. We also implement distributed PCGU for large models. It is empirically shown, through quantitative and qualitative analyses, that negation via Task Vector method outperforms PCGU in debiasing with minimum deterioration in performance and perplexity of the models. On LLaMA-2 7B, negation via Task Vector reduces the bias score by 11.8%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zzjMtnWLQ9": {
    "title": "On the Multi-turn Instruction Following for Conversational Web Agents",
    "volume": "review",
    "abstract": "Web agents powered by Large Language Models (LLMs) have demonstrated remarkable abilities in planning and executing multi-step interactions within complex web-based environments, fulfilling a wide range of web navigation tasks. Despite these advancements, the potential for LLM-powered agents to effectively engage with sequential user instructions in real-world scenarios has not been fully explored. In this work, we introduce a new task of Conversational Web Navigation, which necessitates sophisticated interactions that span multiple turns with both the users and the environment, supported by a specially developed dataset named Multi-Turn Mind2Web (MT-Mind2Web). To tackle the limited context length of LLMs and the context-dependency issue of the conversational tasks, we further propose a novel framework, named self-reflective memory-augmented planning (Self-MAP), which employs memory utilization and self-reflection techniques. Extensive experiments are conducted to benchmark the MT-Mind2Web dataset, and validate the effectiveness of the proposed method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3HZWRLiO0G": {
    "title": "LoraMap: Harnessing the Power of LoRA Connections",
    "volume": "review",
    "abstract": "The advancement of Large Language Models (LLMs) benefit from fact-checking to mitigate hallucination and parameter-efficient techniques such as Low-rank adaptations (LoRA) to overcome enormous computational overhead. While some studies have explored the parallel integration of multiple LoRAs, these approaches need attention to the connections between them. This paper investigates methods to establish connections among multiple LoRAs inspired by the information processing behavior of the human brain. We create three reasoning datasets tailored to fact-checking and fine-tune individual LoRAs, allowing them to view and reason from diverse perspectives. Then, we explore strategies for allocating these reasoning LoRAs and introduce LoraMap, an approach to map connections between them. The results on the fact-checking task demonstrate the superior performance of LoraMap over LoraHub, an existing LoRA composition method. LoraMap also achieves higher performance with significantly fewer parameters than LoraConcat, which concatenates LoRAs and further fine-tunes them",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SNfzwtjV_-": {
    "title": "Enhancing Relation Extraction via Supervised Rationale Verification and Feedback",
    "volume": "review",
    "abstract": "Despite the rapid progress that existing automated feedback methods have made in correcting the output of large language models (LLMs), these methods cannot be well applied to the relation extraction (RE) task due to their designated feedback objectives and correction manner. To address this problem, we propose a novel automated feedback framework for RE, which presents a rationale supervisor to verify the rationale and provide re-selected demonstrations as feedback to correct the initial prediction. Specifically, we first design a causal intervention and observation method for collecting data to train the rationale supervisor. Then, we present a verification-feedback-correction procedure to iteratively enhance LLMs' capability of handling the RE task. Extensive experiments prove that our proposed framework significantly outperforms existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1pyh5y_pAh": {
    "title": "Addressing Format Faithfulness Challenges in Language Models",
    "volume": "review",
    "abstract": "The zero- and few-shot prompting paradigms in large language models (LLMs) have significantly improved the accessibility and flexibility in language-related tasks, where the need for task-specific architecture design or supervision is eliminated. Along with the convenience, these paradigms also introduce the requirements of output format specifications, thereby mandating users to devise an output format and include it in the prompt as a request for LLMs to faithfully adhere to. To study the ability of LLMs to comply with format specifications, we identify the concept of format faithfulness. Based on the formal definition and the detailed taxonomization of format faithfulness, we present FormatBench, a benchmark that covers full categories of format faithfulness in our taxonomy and a wide range of LLM application scenarios. Extensive experiments on FormatBench reveal that state-of-the-art LLMs can still have difficulties in generating basic structured output as instructed. To improve the format faithfulness of LLMs, we design and implement three adaptation approaches, namely format regulation, format tuning, and format refinement. Detailed analyses of these approaches validate their effectiveness in improving format faithfulness rate by up to 9.8%. Our codes and datasets are publicly available at Anonymous Link",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jXkQaxIt-G": {
    "title": "ExSiM: Towards Explainable Automated Evaluation of NLG Systems",
    "volume": "review",
    "abstract": "Automated evaluation of Natural Language Generation (NLG) systems is hard. The common practice of evaluating NLG systems involves computing the similarity between a collection of automatically generated documents and their corresponding (human-written) golden reference documents. Unfortunately, existing document similarity metrics are black boxes and, thus, hard to interpret and explain, making robust evaluation of NLG systems even more challenging. To address this issue, this paper introduces a new evaluation metric called ExSiM that provides a vector of scores instead of a single similarity score, where each component of the vector describes a particular property of the similarity metric, thus providing a natural way of explanation. Our experimental results with Wikipedia article triplets and a custom-created narrative dataset demonstrate that the proposed ExSiM vector can perform comparably to traditional metrics like BERTScore and ROUGE in terms of undirected similarity assessment while providing useful explanations and yielding a higher human-machine agreement in directed similarity assessment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TX0RVNL38z": {
    "title": "Hybrid Multi-stage Decoding for Few-shot NER with Entity-aware Contrastive Learning",
    "volume": "review",
    "abstract": "Few-shot named entity recognition can identify new types of named entities based on a few labeled examples. Previous methods employing token-level or span-level metric learning suffer from the computational burden and a large number of negative sample entities. In this paper, we propose the Hybrid Multi-stage Decoding for Few-shot NER with Entity-aware Contrastive Learning (MsFNER). Specifically, we first detect named entities without type and then classify entity types by the entity classification module. We divide MsFNER into 3 stages: training stage, finetuning stage, and inference stage. In the training stage, we employ a contrastive learning module to enhance entity representations and train the modules on the source domain. During finetuning, we finetune the model on the target support domain. In the inference stage, we replace the contrastive learning module with a KNN module and the final entity type inference is jointly determined by KNN and entity classification module. We conduct experiments on the open FewNERD dataset and the results demonstrate the advance of MsFNER",
    "checked": true,
    "id": "53ae0df5747a7289f2b02df72d62b85833737b22",
    "semantic_title": "hybrid multi-stage decoding for few-shot ner with entity-aware contrastive learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJei1E0Y03": {
    "title": "ChatKBQA: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models",
    "volume": "review",
    "abstract": "Knowledge Base Question Answering (KBQA) aims to answer natural language questions over large-scale knowledge bases (KBs), which can be summarized into two crucial steps: knowledge retrieval and semantic parsing. However, three core challenges remain: inefficient knowledge retrieval, mistakes of retrieval adversely impacting semantic parsing, and the complexity of previous KBQA methods. To tackle these challenges, we introduce ChatKBQA, a novel and simple generate-then-retrieve KBQA framework, which proposes first generating the logical form with fine-tuned LLMs, then retrieving and replacing entities and relations with an unsupervised retrieval method, to improve both generation and retrieval more directly. Experimental results show that ChatKBQA achieves new state-of-the-art performance on standard KBQA datasets, WebQSP, and CWQ. This work can also be regarded as a new paradigm for combining LLMs with knowledge graphs (KGs) for interpretable and knowledge-required question answering",
    "checked": true,
    "id": "4941b37136cdf1836b78ddb6cee65a28c3ce45f0",
    "semantic_title": "chatkbqa: a generate-then-retrieve framework for knowledge base question answering with fine-tuned large language models",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=SR3szASIqy": {
    "title": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers",
    "volume": "review",
    "abstract": "Recently, there has been a growing trend of utilizing Large Language Model (LLM) to evaluate the quality of other LLMs. Many studies have employed proprietary close-source models, especially GPT4, as the evaluator. Alternatively, other works have fine-tuned judge models based on open-source LLMs as the evaluator. In this study, we conduct an empirical study of different judge models on their evaluation capability. Our findings indicate that although the fine-tuned judge models achieve high accuracy on in-domain test sets, even surpassing GPT4, they are inherently task-specific classifiers, and their generalizability and fairness severely underperform GPT4",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6C_QCC5MSn": {
    "title": "Improving Speaker Diarization Using Semantic Information: Joint Pairwise Constraints Propagation",
    "volume": "review",
    "abstract": "Speaker diarization, an important task in speech processing, has been predominantly relied on acoustic signal analysis to differentiate speakers. This reliance on acoustic features often overlooks the wealth of semantic content within speech that can provide additional clues regarding speaker identities. Addressing this gap, our study introduces a semantically enriched diarization approach that extends beyond the acoustic domain, tapping into the nuances of linguistic content. We present a novel method that employs advanced language understanding to extract semantic cues, which are integral to discerning speaker contributions within conversations. Our approach utilizes these cues to formulate pairwise constraints, introducing a multi-modal clustering process to segment and classify speakers and their spoken contents. By integrating these semantically derived constraints into the diarization pipeline, we achieve substantial improvements in accuracy. Extensive evaluations on public dataset illustrate that our method consistently outstrips acoustic-only systems, offering a more holistic perspective on speaker diarization by fully embracing the semantic information of speech",
    "checked": true,
    "id": "a1d1f3823f95faa3f6352d26e64837210d001081",
    "semantic_title": "improving speaker diarization using semantic information: joint pairwise constraints propagation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=CijUELoCds": {
    "title": "Query Augmentation by Decoding Semantics from Brain Signals",
    "volume": "review",
    "abstract": "Query augmentation is a crucial technique for refining semantically imprecise queries. Traditionally, query augmentation relies on extracting information from initially retrieved, potentially relevant documents. If the quality of the initially retrieved documents is low, then the effectiveness of query augmentation would be limited as well. We propose Brain-Aug, which enhances a query by incorporating semantic information decoded from brain signals within a query context. Brain-Aug generates the continuation of the original query with a prompt constructed with brain signal information and a ranking-oriented inference approach. Experimental results on fMRI (functional magnetic resonance imaging) datasets show that BrainAug can reformulate semantically imprecise queries, leading to improved document ranking performance. Further analysis shows that the improvement brought by brain signals is particularly notable for ambiguous queries",
    "checked": true,
    "id": "4ddce69eb9f77d2c1cf8a4411ca203761f54c3e0",
    "semantic_title": "query augmentation by decoding semantics from brain signals",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aEH6_0yIVF": {
    "title": "LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History",
    "volume": "review",
    "abstract": "With the recent emergence of powerful instruction-tuned large language models (LLMs), various helpful conversational Artificial Intelligence (AI) systems have been deployed across many applications. When prompted by users, these AI systems successfully perform a wide range of tasks as part of a conversation. To provide some sort of memory and context, such approaches typically condition their output on the entire conversational history. Although this sensitivity to the conversational history can often lead to improved performance on subsequent tasks, we find that performance can in fact also be negatively impacted, if there is a \\emph{task-switch}. To the best of our knowledge, our work makes the first attempt to formalize the study of such vulnerabilities and interference of tasks in conversational LLMs caused by task-switches in the conversational history. Our experiments across 5 datasets with 15 task switches using popular LLMs reveal that many of the task-switches can lead to significant performance degradation",
    "checked": true,
    "id": "4ab03200801816b27d1363373e9c55c115c4b09b",
    "semantic_title": "llm task interference: an initial study on the impact of task-switch in conversational history",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=thCsuWWQ00": {
    "title": "Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models",
    "volume": "review",
    "abstract": "The burgeoning interest in Multimodal Large Language Models (MLLMs), such as OpenAI's GPT-4V(ision), has significantly impacted both academic and industrial realms. These models enhance Large Language Models (LLMs) with advanced visual understanding capabilities, facilitating their application in a variety of multimodal tasks. Recently, Google introduced Gemini, a cutting-edge MLLM designed specifically for multimodal integration. Despite its advancements, preliminary benchmarks indicate that Gemini lags behind GPT models in commonsense reasoning tasks. However, this assessment, based on a limited dataset (i.e., HellaSWAG), does not fully capture Gemini's authentic commonsense reasoning potential. To address this gap, our study undertakes a thorough evaluation of Gemini's performance in complex reasoning tasks that necessitate the integration of commonsense knowledge across modalities. We carry out a comprehensive analysis of 12 commonsense reasoning datasets, ranging from general to domain-specific tasks. This includes 11 datasets focused solely on language, as well as one that incorporates multimodal elements. Our experiments across four LLMs and two MLLMs demonstrate Gemini's competitive commonsense reasoning capabilities. We also highlight common challenges faced by current LLMs and MLLMs in commonsense reasoning, emphasizing the need for further advancements",
    "checked": true,
    "id": "557182159154a0478b50f19838767ebb1749db0d",
    "semantic_title": "gemini in reasoning: unveiling commonsense in multimodal large language models",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=1CPqEXvVZrk": {
    "title": "COSMIC: Mutual Information for Task-Agnostic Summarization Evaluation",
    "volume": "review",
    "abstract": "Assessing the quality of summarizers poses significant challenges. In response, we propose a novel task-oriented evaluation approach that assesses summarizers based on their capacity to produce summaries that are useful for downstream tasks, while preserving task outcomes. We theoretically establish a direct relationship between the resulting error probability of these tasks and the mutual information between source texts and generated summaries. We introduce COSMIC as a practical implementation of this metric, demonstrating its strong correlation with human judgment-based metrics and its effectiveness in predicting downstream task performance. Comparative analyses against established metrics like BERTScore and ROUGE highlight the competitive performance of COSMIC",
    "checked": true,
    "id": "83f1a581cb63b6aa64d0d448573ef5cba8344a0e",
    "semantic_title": "cosmic: mutual information for task-agnostic summarization evaluation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zT_smwSmfe": {
    "title": "Attributed Question Answering by Faithful Fact Decomposition and Evidence Aggregation",
    "volume": "review",
    "abstract": "Attribution is required for explanation in question answering (QA) using large language models (LLMs), but has not been sufficiently investigated. In this paper, we investigate the limitations of SOTA approaches to attributed QA and address such limitations by proposing a framework called FIDES (faithful fact decomposition and evidence aggregation) for attributed QA. Our experiments on three datasets demonstrate that FIDES significantly outperforms the SOTA methods by over 14% with both GPT-3.5-turbo and Gemini",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tZOnvqm7swW": {
    "title": "Towards Better Understanding of Contrastive Sentence Representation Learning: A Unified Paradigm for Gradient",
    "volume": "review",
    "abstract": "Sentence Representation Learning (SRL) is a crucial task in Natural Language Processing (NLP), where contrastive Self-Supervised Learning (SSL) is currently a mainstream approach. However, the reasons behind its remarkable effectiveness remain unclear. Specifically, in other research fields, contrastive SSL shares similarities in both theory and practical performance with non-contrastive SSL (e.g., alignment & uniformity, Barlow Twins, and VICReg). However, in SRL, contrastive SSL outperforms non-contrastive SSL significantly. Therefore, two questions arise: First, what commonalities enable various contrastive losses to achieve superior performance in SRL? Second, how can we make non-contrastive SSL, which is similar to contrastive SSL but ineffective in SRL, effective? To address these questions, we start from the perspective of gradients and discover that four effective contrastive losses can be integrated into a unified paradigm, which depends on three components: the Gradient Dissipation, the Weight, and the Ratio. Then, we conduct an in-depth analysis of the roles these components play in optimization and experimentally demonstrate their significance for model performance. Finally, by adjusting these components, we enable non-contrastive SSL to achieve outstanding performance in SRL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NJn9FUNfG8X": {
    "title": "Mitigating Privacy Seesaw in Large Language Models: Augmented Privacy Neuron Editing via Activation Patching",
    "volume": "review",
    "abstract": "Protecting privacy leakage in large language models remains a paramount challenge. In this paper, we reveal Privacy Seesaw in LLM privacy safeguarding, a phenomenon where measures to secure specific private information inadvertently heighten exposure risks for other privacy. Through comprehensive analysis, we identify the amount of targeted privacy data and the volume of edited privacy neurons as the two central triggers to this issue. To mitigate privacy seesaw, we propose Augmented Privacy Neuron Editing via Activation Patching (APNEAP), a novel framework designed to well balance model performance with privacy protection. The proposed APNEAP augments collected private data by automatically synthesizing new private data, which deactivates the first trigger to the privacy seesaw issue. Additionally, it adapts activation patching to privacy neuron editing for switching off the second trigger to the privacy seesaw problem. Experimental results show that the proposed APNEAP is capable of alleviating the privacy seesaw phenomenon and offers a more stable and reliable approach to privacy protection in LLMs than previous methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T8HXjSXnHYQ": {
    "title": "ICDPO: Effectively Borrowing Alignment Capability of Others via In-context Direct Preference Optimization",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) rely on Human Preference Alignment (HPA) to ensure the generation of safe content. Due to the heavy cost associated with fine-tuning, fine-tuning-free methods have emerged, typically modifying LLM decoding with external auxiliary methods. However, these methods do not essentially enhance the LLM itself. In this paper, we rethink the derivation procedures of DPO, based on which we conversely build an instant scorer using the states of the LLM before and after In-context Learning (ICL). Accordingly, we propose a novel approach called In-Context Direct Preference Optimization (ICDPO). It enables LLMs to borrow the HPA capabilities from superior LLMs with ICL, generating well-aligned responses as estimated by the aforementioned instant scorer, thereby enhancing the final performance. ICDPO can be further enhanced with a two-stage retriever and an upgraded scorer, both offering benefits. Extensive experiments show its effectiveness, particularly in outperforming two fine-tuning-free baselines, and it exhibits competitiveness with SFT + LoRA. We also conduct detailed analyses to offer comprehensive insights into ICDPO",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pN2U669_fUf": {
    "title": "Data-centric Learning is All LLMs Need",
    "volume": "review",
    "abstract": "In the past year, a multitude of open-source large language models (LLMs) have emerged, demonstrating increasingly robust performance. However, numerous issues surrounding these LLMs remain unresolved. In this work, we focus on how data influences LLMs. We propose a new data processing pipeline to obtain high-quality data. Subsequently, we introduce an efficient pre-training strategy, continuing pre-training LLaMA2 to obtain the Ziya2 model. Ziya2 significantly outperforms LLaMA2 on all selected benchmarks and meets or exceeds the performance of representative open-source models of the same size. Upon analyzing the intermediate checkpoints, we find that different types of data vary in their efficiency in enhancing the performance of Ziya2. Thus, we define three data attributes and establish new data-centric scaling laws to illustrate how different data impacts LLMs. Finally, we arrive at two conclusions that improving the `Coherence' and `Readability' of the data provides a larger boost to the LLMs than improving the `Similarity' to the downstream tasks",
    "checked": false,
    "id": "490680ecfd920daed8147401e77c1ba074b7c8fb",
    "semantic_title": "ziya2: data-centric learning is all llms need",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=GWjJmg85up": {
    "title": "ADAM: Dense Retrieval Distillation with Adaptive Dark Examples",
    "volume": "review",
    "abstract": "To improve the performance of the dual-encoder retriever, one effective approach is knowledge distillation from the cross-encoder ranker. Existing works prepare training instances by pairing each query with one positive and a batch of negatives. However, most hard negatives mined by advanced dense retrieval methods are still too trivial for the teacher to distinguish, preventing the teacher from transferring abundant {dark knowledge} to the student through its soft label.To alleviate this issue, we propose {Adam}, a knowledge distillation framework that can better transfer the dark knowledge held in the teacher with {a}daptive {d}ark ex{am}ples. Different from previous works that only rely on one positive and hard negatives as candidate passages, we create dark examples that all have moderate relevance to the query by strengthening negatives and masking positives in the discrete space. Furthermore, as the quality of knowledge held in different training instances varies as measured by the teacher's confidence score, we propose a self-paced distillation strategy that adaptively concentrates on a subset of high-quality instances to conduct our dark-example-based knowledge distillation to help the student learn better. We conduct experiments on two widely-used benchmarks and verify the effectiveness of our method",
    "checked": true,
    "id": "929ecaf5e6f8f706c736e64222e06a552aea0934",
    "semantic_title": "adam: dense retrieval distillation with adaptive dark examples",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=6C6gt69ObQfY": {
    "title": "MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge Editing",
    "volume": "review",
    "abstract": "Multimodal knowledge editing represents a critical advancement in enhancing the capabilities of Multimodal Large Language Models (MLLMs). Despite its potential, current benchmarks predominantly focus on coarse-grained knowledge, leaving the intricacies of fine-grained (FG) multimodal entity knowledge largely unexplored. This gap presents a notable challenge, as FG entity recognition is pivotal for the practical deployment and effectiveness of MLLMs in diverse real-world scenarios. To bridge this gap, we introduce MIKE, a comprehensive benchmark and dataset specifically designed for the FG multimodal entity knowledge editing. MIKE encompasses a suite of tasks tailored to assess different perspectives, including Vanilla Name Answering, Entity-Level Caption, and Complex-Scenario Recognition. In addition, a new form of knowledge editing, Multi-step Editing, is introduced to evaluate the editing efficiency. Through our extensive evaluations, we demonstrate that the current state-of-the-art methods face significant challenges in tackling our proposed benchmark, underscoring the complexity of FG knowledge editing in MLLMs. Our findings spotlight the urgent need for novel approaches in this domain, setting a clear agenda for future research and development efforts within the community",
    "checked": true,
    "id": "1ecc1583a38fed4fe232b2d76d5c60086f9fa36a",
    "semantic_title": "mike: a new benchmark for fine-grained multimodal entity knowledge editing",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=xZVSJ3QcGC": {
    "title": "Adapting Large Language Models for Document-Level Machine Translation",
    "volume": "review",
    "abstract": "Large language models (LLMs) have made significant strides in various natural language processing (NLP) tasks. Recent research shows that the moderately-sized LLMs often outperform their larger counterparts after task-specific fine-tuning. In this work, we delve into the process of adapting LLMs to specialize in document-level machine translation (DocMT) for a specific language pair. Firstly, we explore how prompt strategies affect downstream translation performance. Then, we conduct extensive experiments with two fine-tuning methods, three LLM backbones, and 18 translation tasks across nine language pairs. Our findings indicate that in some cases, these specialized models even surpass GPT-4 in translation performance, while they still significantly suffer from the off-target translation issue in others, even if they are exclusively fine-tuned on bilingual parallel documents. Furthermore, we provide an in-depth analysis of these LLMs tailored for DocMT, exploring aspects such as translation errors, discourse phenomena, training strategy, the scaling law of parallel documents, additional evaluation on recent test sets, and zero-shot crosslingual transfer. Our findings not only shed light on the strengths and limitations of LLM-based DocMT models but also provide a foundation for future research",
    "checked": true,
    "id": "f7c89f1f83595257d6e2bc306d4deee4cf77f573",
    "semantic_title": "adapting large language models for document-level machine translation",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=sPNV8inXgfr": {
    "title": "PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering",
    "volume": "review",
    "abstract": "Long-term memory plays a critical role in personal interaction, considering long-term memory can better leverage world knowledge, historical information, and preferences in dialogues. Our research introduces PerLTQA, an innovative QA dataset that combines semantic and episodic memories, including world knowledge, profiles, social relationships, events, and dialogues. This dataset is collected to investigate the use of personalized memories, focusing on social interactions and events in the QA task. PerLTQA features two types of memory and a comprehensive benchmark of 8,593 questions for 30 characters, facilitating the exploration and application of personalized memories in Large Language Models (LLMs). Based on PerLTQA, we propose a novel framework for memory integration and generation, consisting of three main components: memory classification, memory retrieval, and memory synthesis. We evaluate this framework using five LLMs and three retrievers. Experimental results demonstrate that BERT-based classification models significantly outperform LLMs such as ChatGLM3 and ChatGPT in the memory classification task. Furthermore, our study highlights the importance of effective memory integration in the QA task",
    "checked": true,
    "id": "dbb9312f9d4d773e92b99030eeed055a63eaf1bf",
    "semantic_title": "perltqa: a personal long-term memory dataset for memory classification, retrieval, and synthesis in question answering",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9b6dTZPgU1j": {
    "title": "Turn Waste into Worth: Rectifying Top-$k$ Router of MoE",
    "volume": "review",
    "abstract": "Sparse Mixture of Experts (MoE) models are popular for training large language models due to their computational efficiency. However, the commonly used top-$k$ routing mechanism suffers from redundancy computation and memory costs due to the unbalanced routing. Some experts are overflow, where the exceeding tokens are dropped. While some experts are vacant, which are padded with zeros, negatively impacting model performance. To address the dropped tokens and padding, we propose the Rectify-Router, comprising the Intra-GPU Rectification and the Fill-in Rectification. The Intra-GPU Rectification handles dropped tokens, efficiently routing them to experts within the GPU where they are located to avoid inter-GPU communication. The Fill-in Rectification addresses padding by replacing padding tokens with the tokens that have high routing scores. Our experimental results demonstrate that the Intra-GPU Rectification and the Fill-in Rectification effectively handle dropped tokens and padding, respectively. Furthermore, the combination of them achieves superior performance, surpassing the accuracy of the vanilla top-1 router by 4.7\\%",
    "checked": false,
    "id": "656f4a76bcbc1a3a032ef5cf284909ef1bb58156",
    "semantic_title": "turn waste into worth: rectifying top-k router of moe",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=DfNSiamPEz": {
    "title": "Thoughts to Target: Enhance Planning for Target-driven Conversation",
    "volume": "review",
    "abstract": "In conversational AI, large-scale models excel in various tasks but struggle with target-driven conversation planning. Current methods, such as chain-of-thought reasoning and tree-search policy learning techniques, either neglect plan rationality or require extensive human simulation procedures. Addressing this, we propose a novel two-stage framework, named EnPL, to improve the LLMs' capability in planning conversations towards designated targets, including (1) distilling natural language plans from target-driven conversation corpus and (2) generating new plans with demonstration-guided in-context learning. Specifically, we first propose a filter approach to distill a high-quality plan dataset, ConvPlan(Resources of this paper can be found at https://anony- mous.4open.science/r/ConvPlan-2023). With the aid of corresponding conversational data and support from relevant knowledge bases, we validate the quality and rationality of these plans. Then, these plans are leveraged to help guide LLMs to further plan for new targets. Empirical results demonstrate that our method significantly improves the planning ability of LLMs, especially in target-driven conversations. Furthermore, EnPL is demonstrated to be quite effective in creating large-scale target-driven conversation datasets, paving the way for constructing extensive target-driven conversational models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=izYeJ1q_Doq": {
    "title": "Modality-Aware Integration with Large Language Models for Knowledge-Based Visual Question Answering",
    "volume": "review",
    "abstract": "Knowledge-based visual question answering (KVQA) has been extensively studied to answer visual questions with external knowledge, e.g., knowledge graphs (KGs). While several attempts have been proposed to leverage large language models (LLMs) as an implicit knowledge source, it remains challenging since LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g., images, KGs and LLMs, cannot be readily aligned for complex scenarios. To tackle these, we present a novel modality-aware integration with LLMs for KVQA ($\\texttt{MAIL}$). It carefully leverages multimodal knowledge for both image understanding and knowledge reasoning. Specifically, \\((i)\\) we propose a two-stage prompting strategy with LLMs to densely embody the image into a $\\textit{scene graph}$ with detailed visual features; \\((ii)\\) We construct a coupled $\\textit{concept graph}$ by linking the mentioned entities with external facts. \\((iii)\\) A tailored pseudo-siamese graph medium fusion is designed for sufficient multimodal fusion. We utilize the shared mentioned entities in two graphs as mediums to bridge a tight inter-modal exchange, while maximally preserving insightful intra-modal learning by constraining the fusion within mediums. Extensive experiments on two benchmark datasets show the superiority of $\\texttt{MAIL}$ with 24$\\times$ less resources",
    "checked": true,
    "id": "b6b5ffb9a1faee7b488eb46f2d68f62beaecb8c7",
    "semantic_title": "modality-aware integration with large language models for knowledge-based visual question answering",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=tKGLNJ2x8t": {
    "title": "Voice Attribute Editing with Text Prompt",
    "volume": "review",
    "abstract": "Despite recent advancements in speech generation with text prompt providing control over speech style, voice attributes in synthesized speech remain elusive and challenging to control. This paper introduces a novel task: voice attribute editing with text prompt, with the goal of making relative modifications to voice attributes according to the actions described in the text prompt. To solve this task, VoxEditor, an end-to-end generative model, is proposed. In VoxEditor, addressing the insufficiency of text prompt, a Residual Memory (ResMem) block is designed, that efficiently maps voice attributes and these descriptors into the shared feature space. Additionally, the ResMem block is enhanced with a voice attribute degree prediction (VADP) block to align voice attributes with corresponding descriptors, addressing the imprecision of text prompt caused by non-quantitative descriptions of voice attributes. We also establish the open-source VCTK-RVA dataset, which leads the way in manual annotations detailing voice characteristic differences among different speakers. Extensive experiments demonstrate the effectiveness and generalizability of our proposed method in terms of both objective and subjective metrics. The dataset and audio samples are available on the website",
    "checked": true,
    "id": "3612748e6e9003bff4051ea3997e14b7d0c842b1",
    "semantic_title": "voice attribute editing with text prompt",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=leVaMiNcwHE": {
    "title": "Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding",
    "volume": "review",
    "abstract": "To mitigate the high inference latency stemming from autoregressive decoding in Large Language Models (LLMs), Speculative Decoding has emerged as a novel decoding paradigm for LLM inference. In each decoding step, this method first drafts several future tokens efficiently and then verifies them in parallel. Unlike autoregressive decoding, Speculative Decoding facilitates the simultaneous decoding of multiple tokens per step, thereby accelerating inference. This paper presents a comprehensive overview and analysis of this promising decoding paradigm. We begin by providing a formal definition and formulation of Speculative Decoding. Then, we organize in-depth discussions on its key facets, such as drafter selection and verification strategies. Furthermore, we present a comparative analysis of leading methods under third-party testing environments. We aim for this work to serve as a catalyst for further research on Speculative Decoding, ultimately contributing to more efficient LLM inference",
    "checked": true,
    "id": "0cee098244c9978032702862a43a09f468f691a4",
    "semantic_title": "unlocking efficiency in large language model inference: a comprehensive survey of speculative decoding",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=AjMH-cmMpjW": {
    "title": "Amanda: Adaptively Modality-Balanced Domain Adaptation for Multimodal Emotion Recognition",
    "volume": "review",
    "abstract": "This paper investigates unsupervised multimodal domain adaptation for multimodal emotion recognition, which is a solution for data scarcity yet remains under studied. Due to the varying distribution discrepancies of different modalities between source and target domains, the primary challenge lies in how to balance the domain alignment across modalities to guarantee they are all well aligned. To achieve this, we first develop our model based on the information bottleneck theory to learn optimal representation for each modality independently. Then, we align the domains via matching the label distributions and the representations. In order to balance the representation alignment, we propose to minimize a surrogate of the alignment losses, which is equivalent to adaptively adjusting the weights of the modalities throughout training, thus achieving balanced domain alignment across modalities. Overall, the proposed approach features \\textbf{A}daptively \\textbf{m}odality-bal\\textbf{an}ced \\textbf{d}omain \\textbf{a}daptation, dubbed as \\textbf{Amanda}, for multimodal emotion recognition. Extensive empirical results on commonly used benchmark datasets demonstrate that Amanda significantly outperforms competing approaches. The code is submitted as supplementary material, and the extracted features of the datasets will be made publicly available upon the publication of the paper",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SsAHMcacaQ": {
    "title": "Speechworthy Instruction-tuned Language Models",
    "volume": "review",
    "abstract": "Text and spoken language are two major modes of human communication, each with distinct constraints and preferences.However, current instruction-tuned language models are exclusively trained with textual preference annotations, which result in models that are not optimized for generating text bound for speech.We collect preference data for speech by asking users to \\textit{listen} to response pairs, revealing nuanced preferences for simple and colloquial responses that are also informative.To optimize for speech-suitability, we compile system prompts applying guidelines established by audio-based media as well as curate a spoken-preference dataset of 11K voice-suitable input prompts with 20K annotated response pairs.We use these resources to adapt Falcon-Instruct 7B to the speech domain via both prompting and reinforcement learning with human feedback (RLHF).Human and automatic evaluation show that both approaches can improve speech-suitability, with RLHF on speech-based preferences in particular producing a model whose responses are preferred 4.8\\% more on average to those of the base model and its prompted counterpart",
    "checked": false,
    "id": "a938fcc877d181e3d9a64e5de0a1421ef461bb58",
    "semantic_title": "unified text structuralization with instruction-tuned language models",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=kvWfOHp13Ym": {
    "title": "DOP: Diagnostic-Oriented Prompting for Large Language Models in Mathematical Correction",
    "volume": "review",
    "abstract": "Math world problems correction(MWPC) is a novel task dedicated to rectifying reasoning errors in the process of solving mathematical problems. In this paper, leveraging the advancements in large language models (LLMs), we address two key objectives:(1) Distinguishing between mathematical reasoning and error correction; (2) Exploring strategies to enhance the error correction capabilities of LLMs in mathematics to solve MWPC task. We noticed that, in real-time education,assisting students in recognizing their mistakes is more crucial than simply providing correct answers. However, current research tends to prioritize obtaining accurate solutions to math problems rather than correcting potentially incorrect ones. Therefore, we modify the research paradigm, demonstrating that improving mathematical reasoning abilities does not equate to mastery in error correction. Meanwhile, we propose a novel method called diagnostic-oriented promping(DOP) aimed at facilitating LLMs to excel in error correction. In experiments, DOP has shown outstanding performance, highlighting its significant impact. We argue that in mathematical education, the demand for outstanding correctors surpasses that for proficient reasoners",
    "checked": true,
    "id": "490cd30b2e0bd83cdc62d28ffdd4279931522b9c",
    "semantic_title": "dop: diagnostic-oriented prompting for large language models in mathematical correction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8tYrPBqjped": {
    "title": "ConFit: Improving Resume-Job Matching using Data Augmentation and Contrastive Learning",
    "volume": "review",
    "abstract": "A reliable resume-job matching system helps a company find suitable candidates from a pool of resumes, and helps a job seeker find relevant jobs from a list of job posts. However, since job seekers apply only to a few jobs, interaction records in resume-job datasets are sparse. Different from many prior work that use complex modeling techniques, we tackle this sparsity problem using data augmentations and a simple contrastive learning approach. ConFit first creates an augmented resume-job dataset by paraphrasing specific sections in a resume or a job post. Then, ConFit uses contrastive learning to further increase training samples from $B$ pairs per batch to $O(B^2)$ per batch. We evaluate ConFit on two real-world datasets and find it outperforms prior methods (including BM25 and OpenAI text-ada-002) by up to 19% and 31% absolute in nDCG@10 for ranking jobs and ranking resumes, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-PumjO4BxKG": {
    "title": "Navigating the Shadows: Unveiling Effective Disturbances for Modern AI Content Detectors",
    "volume": "review",
    "abstract": "With the launch of ChatGPT, large language models (LLMs) have attracted global attention. In the realm of article writing, LLMs have witnessed extensive utilization, giving rise to concerns related to intellectual property protection, personal privacy, and academic integrity. In response, AI-text detection has emerged to distinguish between human and machine-generated content. However, recent research indicates that these detection systems often lack robustness and struggle to effectively differentiate perturbed texts. Currently, there is a lack of systematic evaluations regarding detection performance in real-world applications, and a comprehensive examination of perturbation techniques and detector robustness is also absent. To bridge this gap, our work simulates real-world scenarios in both informal and professional writing, exploring the out-of-the-box performance of current detectors. Additionally, we have constructed 12 black-box text perturbation methods to assess the robustness of current detection models across various perturbation granularities. Furthermore, through adversarial learning experiments, we investigate the impact of perturbation data augmentation on the robustness of AI-text detectors. After the review process, we will publicly release all our code and data",
    "checked": true,
    "id": "dd4d62e2d927f610996bca5c9abd6824ede91adc",
    "semantic_title": "navigating the shadows: unveiling effective disturbances for modern ai content detectors",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uXnVcZdD0W": {
    "title": "LDB: A Large Language Model Debugger via Verifying Runtime Execution Step by Step",
    "volume": "review",
    "abstract": "Large language models (LLMs) are leading significant progress in code generation. Beyond one-pass code generation, recent works further integrate unit tests and program verifiers into LLMs to iteratively refine the generated programs. However, these works consider the generated programs as an indivisible entity, which falls short for LLMs in debugging the programs, especially when the programs contain complex logic flows and data operations. In contrast, when human developers debug programs, they typically set breakpoints and selectively examine runtime execution information. The execution flow and the intermediate variables play a crucial role in the debugging process, yet they are underutilized in the existing literature on code generation. In this study, we introduce Large Language Model Debugger (LDB), a novel debugging framework that enables LLMs to refine their generated programs with the runtime execution information. Specifically, LDB segments the programs into basic blocks and tracks the values of intermediate variables after each block throughout the runtime execution. This allows LLMs to concentrate on simpler code units within the overall execution flow, verify their correctness against the task description block by block, and efficiently pinpoint any potential errors. Experiments demonstrate that LDB consistently enhances the baseline performance by up to 9.8% across the HumanEval, MBPP, and TransCoder benchmarks, archiving new state-of-the-art performance in code debugging for various LLM selections",
    "checked": false,
    "id": "26353aca5f6e4d6b938f861959ad70a4493cf372",
    "semantic_title": "ldb: a large language model debugger via verifying runtime execution step-by-step",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=hUEVREdfB7": {
    "title": "Context Repair with Large Language Models for Document-level Neural Machine Translation",
    "volume": "review",
    "abstract": "Current neural machine translation models generate translation output sentence-by-sentence, where each translation procedure is carried out independently. This sentence-level decoding strategy results in an inherent issue of incoherence. Consequently, considerable effort has been dedicated to document-level machine translation to mitigate the incoherence problem. In this work, we propose a simple and effective technique to repair document context by leveraging the power of large language models. The document-level translation task is decomposed into a sentence-level translation task and a contextual information repair task. we first employ a conventional sentence-level translation model to generate sentence-level translation outputs. Then, we pair these outputs with their corresponding translation references to create few-shot examples. Finally, we utilize a large language model along with these few-shot examples to perform context repair for the test sentences. Experimental results on the Bilingual Web Books test set demonstrate the effectiveness of the proposed approach in document-context translation. Besides, the approach also works with other methods. Further analysis and human evaluation results indicate that the proposed approach outperforms the baseline model in terms of human preference",
    "checked": false,
    "id": "840fb73f48ab761b48b041ea2b9fe4d92d0eca57",
    "semantic_title": "contextual refinement of translations: large language models for sentence and document-level post-editing",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=6ysNeLLf8i": {
    "title": "Literary performance of pre-trained large language models: a case study of Pharmako-AI",
    "volume": "review",
    "abstract": "This study investigates the literary performance of hybrid text co-created by a professional author and a large language model (LLM), exemplified by the novel Pharmako-AI by K Allado-Mcdowell (2020). While pre-trained LLMs have gained particular popularity for their literary output such as poetry, recent studies suggest potential shortcomings in LLMs' literary performance. Benchmarking and assessing LLMs are important, because they improve LLMs' interpretability and transparency in various aspects, including literary performance. Computational linguistic assessments for hybrid text are relatively understudied, which motivates this study. From NLP perspective, we aim to characterize and evaluate the collaboration between a human writer and an LLM. Drawing on previous investigations of LLMs' creative capabilities, our study utilizes sentiment and similarity analyses to analyze Pharmako-AI's literary aspects, comparing human and LLMs generated text. Findings reveal nuanced sentiment patterns and higher sophistication in human-generated text. This indicates the distinctive nature of the hybrid text relative to a traditional work of fiction. Our study contributes to understanding the interplay between human and LLM contributions in literary collaboration",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tJ1eXxXjct": {
    "title": "Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have become increasingly popular due to their ability to process and generate natural language. However, as they are trained on massive datasets of text, LLMs can also inherit harmful biases and produce outputs that are not aligned with human values. This paper explores two main approaches to LLM alignment: Reinforcement Learning with Human Feedback (RLHF) and contrastive learning-based methods like Direct Preference Optimization (DPO). We discuss the advantages and disadvantages of each approach, highlighting the complexity and instability of RLHF compared to the simpler but potentially less robust DPO. With that in mind, we propose MPO (Mixed Preference Optimization), a novel method that combines the strengths of both approaches. Specifically, we introduce a data selection method that utilizes the score difference of a reward model and divides the data into two parts: an easy set and a hard set. We then propose a two-stage training procedure: first train DPO on the easy dataset, and then train PPO on the difficult set with DPO model being the reference model. Experiments are conducted on two public alignment datasets, namely HH-RLHF and TLDR, demonstrating the effectiveness of MPO, both in terms of GPT4 evaluation and human evaluation",
    "checked": true,
    "id": "d2be1c0f537adc9ffec42510bc84d2f478ee2402",
    "semantic_title": "mixed preference optimization: reinforcement learning with data selection and better reference model",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=GGXg_3ulv7": {
    "title": "Who is the Writer?Identifying the Generative Model by Writing Style",
    "volume": "review",
    "abstract": "Texts generated by generative models closely resemble high-quality human-written texts, raising concerns about their potential misuse in spreading misinformation and engaging in academic dishonesty.Identifying human and model-generated texts presents a significant challenge.To address this, we present the Identify the Writer by Writing Style (IWWS) model, a novel approach designed to identify the writing styles of human and generative model. To establish a robust foundation for research in distinguishing texts generated by human and generative model, we also propose a comprehensive dataset, HumanGenTextify,to establish a robust foundation for research in identifying texts.Experimental results demonstrate the superiority of the IWWS model over existing methods. It not only achieves high accuracy in text source identification but also provides insights into the distinctive writing styles that characterize human and model-generated texts. Our work lays the groundwork for future explorations into automated text classification and opens new avenues for research into the authenticity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qqsl9Eikkk": {
    "title": "Slice-Specific Few-Shot Recalibration of Language Models",
    "volume": "review",
    "abstract": "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), in which the model's confidence score reflects its prediction accuracy. However, while an LM may be well-calibrated on multiple domains combined, it can be significantly miscalibrated within each domain (e.g., overconfidence in math balances out underconfidence in history). In order to attain well-calibrated confidence estimates for each slice of the distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from a given slice and predicts the slice-specific precision scores at various confidence thresholds. Our trained model can recalibrate for new slices, without using any labeled data from that slice. This helps us identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain. Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling",
    "checked": false,
    "id": "e9ab14dbbad99c2b31dce690816d53c21320239e",
    "semantic_title": "few-shot recalibration of language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=tw2VDmCVTL": {
    "title": "Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty",
    "volume": "review",
    "abstract": "As natural language becomes the default interface for human-AI interaction, there is a critical need for LMs to appropriately communicate uncertainties in downstream applications. In this work, we investigate how LMs incorporate confidence about their responses via natural language and how downstream users behave in response to LM-articulated uncertainties. We examine publicly deployed models and find that LMs are reluctant to express uncertainties when answering questions even when they produce incorrect responses. LMs can be explicitly prompted to express confidences, but tend to be overconfident, resulting in high error rates (an average of 47%) among confident responses. We test the risks of LM overconfidence by conducting human experiments and show that users rely heavily on LM generations, whether or not they are marked by certainty. Lastly, we investigate the preference-annotated datasets used in post training alignment and find that humans have a bias against texts with uncertainty. Our work highlights a new set of safety harms facing human-LM interactions and proposes design recommendations and mitigating strategies moving forward",
    "checked": true,
    "id": "9a8d0f0ace05d3795a9a58f94675710b89006941",
    "semantic_title": "relying on the unreliable: the impact of language models' reluctance to express uncertainty",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=Quy2oKeyF1": {
    "title": "Simul-LLM: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) with billions of parameters and pretrained on massive amounts of data are now capable of near or better than state-of-the-art performance in a variety of downstream natural language processing tasks. Neural machine translation (NMT) is one such task that LLMs have been applied to with great success. However, little research has focused on applying LLMs to the more difficult subset of NMT called simultaneous translation (SimulMT), where translation begins before the entire source context is available to the model. In this paper, we address key challenges facing LLMs fine-tuned for SimulMT, validate classical SimulMT concepts and practices in the context of LLMs, explore adapting LLMs that are fine-tuned for NMT to the task of SimulMT, and introduce Simul-LLM, the first open-source fine-tuning and evaluation pipeline development framework for LLMs focused on SimulMT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i7cz1LMA-1": {
    "title": "Analysing The Impact of Sequence Composition on Language Model Pre-Training",
    "volume": "review",
    "abstract": "Most language model pre-training frameworks concatenate multiple documents into fixed-length sequences and use \\emph{causal masking} to compute the likelihood of each token given its context; this strategy is widely adopted due to its simplicity and efficiency. However, to this day, the influence of the pre-training sequence composition strategy on the generalisation properties of the model remains under-explored. In this work, we find that applying causal masking can lead to the inclusion of distracting information from previous documents during pre-training, which negatively impacts the performance of the models on language modelling and downstream tasks. In \\emph{intra-document causal masking}, the likelihood of each token is only conditioned on the previous tokens in the same document, which eliminates potential distracting information from previous documents and significantly improves the performance. Furthermore, we find that concatenating related documents can reduce some potential distractions during pre-training, and our proposed efficient retrieval-based sequence construction method, \\textsc{Bm25}Chunk, can improve in-context learning (+11.6\\%), knowledge memorisation (+9.8\\%), and context utilisation (+7.2\\%) abilities of language models without sacrificing efficiency",
    "checked": true,
    "id": "c96b17f3362c2d795964d07427a9396a09db6b2c",
    "semantic_title": "analysing the impact of sequence composition on language model pre-training",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=D3BgnqqTss": {
    "title": "Time is Encoded in the Weights of Finetuned Language Models",
    "volume": "review",
    "abstract": "We present time vectors, a simple tool to customize language models to new time periods. Time vectors are created by finetuning a language model on data from a single time (e.g., a year or month), and then subtracting the weights of the original pretrained model. This vector specifies a direction in weight space that, as our experiments show, improves performance on text from that time period. Time vectors specialized to adjacent time periods appear to be positioned closer together in a manifold. Using this structure, we interpolate between time vectors to induce new models that perform better on intervening and future time periods, without any additional training. We demonstrate the consistency of our findings across different tasks, domains, model sizes, and time scales. Our results suggest that time is encoded in the weight space of finetuned models",
    "checked": true,
    "id": "3f081ee658a08d0b3ccc6358c85e618d05f74eb3",
    "semantic_title": "time is encoded in the weights of finetuned language models",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=Yl3nnoA_Is": {
    "title": "Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models",
    "volume": "review",
    "abstract": "This paper investigates the capabilities of Large Language Models (LLMs) in understanding their knowledge and uncertainty over questions. Specifically, we focus on addressing known-unknown questions, characterized by high uncertainty due to the absence of definitive answers. To facilitate our study, we collect a new dataset with Known-Unknown Questions (KUQ) and establish a categorization framework to clarify the origins of uncertainty in such queries. Subsequently, we examine the performance of open-source LLMs, fine-tuned using this dataset, in distinguishing between known and unknown queries within open-ended question-answering scenarios. The fine-tuned models demonstrated a significant improvement, achieving a considerable increase in F1-score relative to their pre-fine-tuning state. Through a comprehensive analysis, we reveal insights into the models' improved uncertainty articulation and their consequent efficacy in multi-agent debates. These findings help us understand how LLMs can be trained to identify and express uncertainty, improving our knowledge of how they understand and express complex or unclear information",
    "checked": true,
    "id": "c22bfecc684be370bc22611deb8737d65466a390",
    "semantic_title": "knowledge of knowledge: exploring known-unknowns uncertainty with large language models",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=KWb9CsI9m_": {
    "title": "Steering Llama 2 via Contrastive Activation Addition",
    "volume": "review",
    "abstract": "We introduce Contrastive Activation Addition (CAA), an innovative method for steering language models by modifying their activations during forward passes. CAA computes ``steering vectors'' by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user's prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights into CAA's mechanisms by employing various activation space interpretation methods. CAA accurately steers model outputs and sheds light on how high-level concepts are represented in Large Language Models (LLMs)",
    "checked": true,
    "id": "edd1dc1e8d7989f36c0e54f69f4aeb5e597edc8b",
    "semantic_title": "steering llama 2 via contrastive activation addition",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=a3waBEaOx1": {
    "title": "FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability",
    "volume": "review",
    "abstract": "This paper presents FoFo, a pioneering benchmark for evaluating large language models' (LLMs) ability to follow complex, domain-specific formats, a crucial yet underexamined capability for their application as AI agents. Despite LLMs' advancements, existing benchmarks fail to assess their format-following proficiency adequately. FoFo fills this gap with a diverse range of real-world formats and instructions, developed through an AI-Human collaborative method. Our evaluation across both open-source (e.g., Llama 2, WizardLM) and closed-source (e.g., GPT-4, PALM2, Gemini) LLMs highlights three key findings: open-source models significantly lag behind closed-source ones in format adherence; LLMs' format-following performance is independent of their content generation quality; and LLMs' format proficiency varies across different domains. These insights suggest the need for specialized tuning for format-following skills and highlight FoFo's role in guiding the selection of domain-specific AI agents. FoFo will be publicly released, contributing a critical tool for advancing LLM evaluation and application",
    "checked": true,
    "id": "5ec2aa6c84e4b7ee51c6cc3e4cd74ed3b21c2df1",
    "semantic_title": "fofo: a benchmark to evaluate llms' format-following capability",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=VTxdoi-cka": {
    "title": "AI 'News' Content Farms Are Easy to Make and Hard to Detect: A Case Study in Italian",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) are increasingly used as 'content farm' Models (CFMs), to generate synthetic text that could pass for real news articles. This is already happening even for languages that do not have high-quality monolingual LLMs. We show that fine-tuning Llama LLM, mostly trained on English, on as little as 40K Italian news articles, is sufficient for producing news-like texts that native speakers of Italian struggle to identify as synthetic. We investigate three LLMs and three methods of detecting synthetic texts (log-likelihood, DetectGPT, and supervised classification), finding that they all perform better than human raters, but they are all impractical in the real world (requiring either access to token likelihood information or a large dataset of CFM texts). We also explore the possibility of creating a proxy CFM: an LLM fine-tuned on a similar dataset to one used by the real 'content farm'. We find that even a small amount of fine-tuning data suffices for creating a successful detector, but we need to know which base LLM is used, which is a major challenge. Our results suggest that there are currently no practical methods for detecting synthetic news 'in the wild', while generating them is too easy. We highlight the urgency of more NLP research on this problem",
    "checked": false,
    "id": "77dc09e14d0faa39998d9988e2b46731e859ea2f",
    "semantic_title": "ai \"news\" content farms are easy to make and hard to detect: a case study in italian",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n8HBSZrRNS": {
    "title": "Bio-RFX: Refining Biomedical Extraction via Advanced Relation Classification and Structural Constraints",
    "volume": "review",
    "abstract": "The ever-growing biomedical publications magnify the challenge of extracting structured data from unstructured texts. This task involves two components: biomedical entity identification (Named Entity Recognition, NER) and their interrelation determination (Relation Extraction, RE). However, existing methods often neglect unique features of the biomedical literature, such as ambiguous entities, nested proper nouns, and overlapping relation triplets, and underutilize prior knowledge, leading to an intolerable performance decline in the biomedical domain, especially with limited annotated training data. In this paper, we propose the Biomedical Relation-First eXtraction (Bio-RFX) model by leveraging sentence-level relation classification before entity extraction to tackle entity ambiguity. Moreover, we exploit structural constraints between entities and relations to guide the model's hypothesis space, enhancing extraction performance across different training scenarios. Comprehensive experimental results on biomedical datasets show that Bio-RFX achieves significant improvements on both NER and RE tasks. Even under the low-resource training scenarios, it outperforms all baselines in NER and has highly competitive performance compared to the state-of-the-art fine-tuned baselines in RE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KW6QhKLMex": {
    "title": "Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts",
    "volume": "review",
    "abstract": "Weight-sharing supernets are crucial for performance estimation in cutting-edge neural architecture search (NAS) frameworks. Despite their ability to generate diverse subnetworks without retraining, the quality of these subnetworks is not guaranteed due to weight sharing. In NLP tasks like machine translation and pre-trained language modeling, there is a significant performance gap between supernet and training from scratch for the same model architecture, necessitating retraining post optimal architecture identification.This study introduces a solution called mixture-of-supernets, a generalized supernet formulation leveraging mixture-of-experts (MoE) to enhance supernet model expressiveness with minimal training overhead. Unlike conventional supernets, this method employs an architecture-based routing mechanism, enabling indirect sharing of model weights among subnetworks. This customization of weights for specific architectures, learned through gradient descent, minimizes retraining time, significantly enhancing training efficiency in NLP. The proposed method attains state-of-the-art (SoTA) performance in NAS for fast machine translation models, exhibiting a superior latency-BLEU tradeoff compared to HAT, the SoTA NAS framework for machine translation. Furthermore, it excels in NAS for building memory-efficient task-agnostic BERT models, surpassing NAS-BERT and AutoDistil across various model sizes",
    "checked": true,
    "id": "85bc4cf4d623b5984ec0be338575dd09cc823140",
    "semantic_title": "mixture-of-supernets: improving weight-sharing supernet training with architecture-routed mixture-of-experts",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=RbMtDpHFFi": {
    "title": "Multi-Modal Retrieval For Large Language Model Based Speech Recognition",
    "volume": "review",
    "abstract": "Retrieval is a widely adopted approach for improving language models leveraging external information. As the field moves towards multi-modal large language models, it is important to extend the pure text based methods to incorporate other modalities in retrieval as well for applications across the wide spectrum of machine learning tasks and data types. In this work, we propose multi-modal retrieval with two approaches: kNN-LM and cross-attention techniques. We demonstrate the effectiveness of our retrieval approaches empirically by applying them to automatic speech recognition tasks with access to external information. Under this setting, we show that speech-based multi-modal retrieval outperforms text based retrieval, and yields up to $~50\\,\\%$ improvement in word error rate over the multi-modal language model baseline. Furthermore, we achieve state-of-the-art recognition results on the Spoken-Squad question answering dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d1N11r_C_Q": {
    "title": "Reference-based Metrics Disprove Themselves in Question Generation",
    "volume": "review",
    "abstract": "Reference-based metrics such as BLEU and BERTScore are widely used to evaluate question generation (QG). In this study, on QG benchmarks such as SQuAD and HotpotQA, we find that using human-written references cannot guarantee the effectiveness of the reference-based metrics. Most QG benchmarks have only one reference; we replicated the annotation process and collect another reference. A good metric was expected to grade a human-validated question no worse than generated questions. However, the results of reference-based metrics on our newly collected reference disproved the metrics themselves. We propose a reference-free metric consisted of multi-dimensional criteria such as naturalness, answerability, and complexity, utilizing large language models. These criteria are not constrained to the syntactic or semantic of a single reference question. And the metric does not require a diverse set of references. Experiments reveal that our metric accurately distinguishes between high-quality questions and flawed ones, and achieves state-of-the-art alignment with human judgment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bd0K4ohvhN": {
    "title": "TRAM: Benchmarking Temporal Reasoning for Large Language Models",
    "volume": "review",
    "abstract": "Reasoning about time is essential for understanding the nuances of events described in natural language. Previous research on this topic has been limited in scope, characterized by a lack of standardized benchmarks that would allow for consistent evaluations across different studies. In this paper, we introduce TRAM, a temporal reasoning benchmark composed of ten datasets, encompassing various temporal aspects of events such as order, arithmetic, frequency, and duration, designed to facilitate a comprehensive evaluation of the TeR capabilities of large language models (LLMs). We evaluate popular LLMs like GPT-4 and Llama2 in zero-shot and few-shot scenarios, and establish baselines with BERT-based and domain-specific models. Our findings indicate that the best-performing model lags significantly behind human performance. It is our aspiration that TRAM will spur further progress in enhancing the TeR capabilities of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2_VlD1BkMM": {
    "title": "MEANT: Multimodal Encoder for Antecedent Information",
    "volume": "review",
    "abstract": "The stock market provides a rich well of information that can be split across modalities, which makes it an ideal candidate for multimodal evaluation. Multimodal data plays an increasingly important role in the development of machine learning and has shown to positively impact performance. But information can do more than exist across modes--- it can exist across time. How should we attend to temporal data that consists of multiple information types? This work introduces (i) the MEANT model, a Multimodal Encoder for Antecedent information and (ii) a new dataset called TempStock. TempStock consists of price, Tweets, and graphical data with over a million Tweets from all of the companies in the S\\&P 500 Index. We find that MEANT improves performance on existing baselines by over 15\\%, and that the textual information affects performance far more than the visual information on our time-dependent task from our ablation study",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AvDm8W0iGN": {
    "title": "EditCoT: A Novel Multi-Intent Text Revision Modeling Framework",
    "volume": "review",
    "abstract": "Multi-intent text revision is a complex process aiming to fix all potential text defects. Inspired by Chain-of-Thought, this study introduces a multi-step edit reasoning framework (EditCoT) to model multi-intent text revision tasks using large language models (LLMs). EditCoT decomposes the text revision task into multiple rewrite reasoning steps and fixes the corresponding text defects in each reasoning step. EditCoT enhances the reasoning ability of LLMs in text editing and enables multi-intent text revision by resolving each edit intent step-by-step. We investigate the performance of EditCoT on multi-/single-intent text revision tasks. The results show that EditCoT can achieve the best performance in multi-intent text revision and present a competitive performance compared to specifically fine-tuned single-intent models. Additionally, EditCoT also exhibits good transferability to unseen edit intents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lQXmiXaiPs": {
    "title": "LLM Performance Predictors are good initializers for Architecture Search",
    "volume": "review",
    "abstract": "In this work, we utilize Large Language Models (LLMs) for a novel use case: constructing Performance Predictors (PP) that estimate the performance of specific deep neural network architectures on downstream tasks. We create PP prompts for LLMs, comprising (i) role descriptions, (ii) instructions for the LLM, (iii) hyperparameter definitions, and (iv) demonstrations presenting sample architectures with efficiency metrics and 'training from scratch' performance. In machine translation (MT) tasks, GPT-4 with our PP prompts (LLM-PP) achieves a SoTA mean absolute error and a slight degradation in rank correlation coefficient compared to baseline predictors. Additionally, we demonstrate that predictions from LLM-PP can be distilled to a compact regression model (LLM-Distill-PP), which surprisingly retains much of the performance of LLM-PP. This presents a cost-effective alternative for resource-intensive performance estimation. Specifically, for Neural Architecture Search (NAS), we introduce a Hybrid-Search algorithm (HS-NAS) employing LLM-Distill-PP for the initial search stages and reverting to the baseline predictor later. HS-NAS performs similarly to SoTA NAS, reducing search hours by approximately 50%, and in some cases, improving latency, GFLOPs, and model size",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ewQlC1ZpWi": {
    "title": "Symmetric Dot-Product Attention for Efficient Training of BERT Language Models",
    "volume": "review",
    "abstract": "Initially introduced as a machine translation model, the Transformer architecture has now become the foundation for modern deep learning architecture, with applications in a wide range of fields, from computer vision to natural language processing. Nowadays, to tackle increasingly more complex tasks, Transformer-based models are stretched to enormous sizes, requiring increasingly larger training datasets, and unsustainable amount of compute resources. The ubiquitous nature of the Transformer and its core component, the attention mechanism, are thus prime targets for efficiency research. In this work, we propose an alternative compatibility function for the self-attention mechanism introduced by the Transformer architecture. This compatibility function exploits an overlap in the learned representation of the traditional scaled dot-product attention, leading to a symmetric with pairwise coefficient dot-product attention. When applied to the pre-training of BERT-like models, this new symmetric attention mechanism reaches a score of 79.36 on the GLUE benchmark against 78.74 for the traditional implementation, leads to a reduction of 6% in the number of trainable parameters, and reduces the number of training steps required before convergence by half",
    "checked": true,
    "id": "0430f7e17a51c43bd90822751ab32fb3fd542bd6",
    "semantic_title": "symmetric dot-product attention for efficient training of bert language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c4_CyulDqK": {
    "title": "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction",
    "volume": "review",
    "abstract": "Large language models (LLMs) have demonstrated impressive abilities in generating unstructured natural language according to instructions. However, their performance can be inconsistent when tasked with producing text that adheres to specific structured formats, which is crucial in applications like named entity recognition (NER) or relation extraction (RE). To address this issue, this paper introduces an efficient method, G&O, to enhance their structured text generation capabilities. It breaks the generation into a two-step pipeline: initially, LLMs generate answers in natural language as intermediate responses. Subsequently, LLMs are asked to organize the output into the desired structure, using the intermediate responses as context. G&O effectively separates the generation of content from the structuring process, reducing the pressure of completing two orthogonal tasks simultaneously. Tested on zero-shot NER and RE, the results indicate a significant improvement in LLM performance with minimal additional efforts. This straightforward and adaptable prompting technique can also be combined with other strategies, like self-consistency, to further elevate LLM capabilities in various structured text generation tasks",
    "checked": true,
    "id": "79ff4eb495094e3b47468515846d507144135ae8",
    "semantic_title": "a simple but effective approach to improve structured language model output for information extraction",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=chco4W_Djf": {
    "title": "LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning",
    "volume": "review",
    "abstract": "Chain-of-thought (CoT) prompting has emerged as a popular in-context learning (ICL) approach for large language models (LLMs), especially when tackling complex reasoning tasks. Traditional ICL approaches construct prompts using examples that contain questions similar to the input question. However, CoT prompting, which includes crucial intermediate reasoning steps (rationales) within its examples, necessitates selecting examples based on these rationales rather than the questions themselves. Existing methods require human experts or pre-trained LLMs to describe the skill, a high-level abstraction of rationales, to guide the selection. These methods, however, are often costly and difficult to scale. Instead, this paper introduces a new approach named Latent Reasoning Skills (LaRS) that employs unsupervised learning to create a latent space representation of rationales, with a latent variable called a reasoning skill. Concurrently, LaRS learns a reasoning policy to determine the required reasoning skill for a given question. Then the ICL examples are selected by aligning the reasoning skills between past examples and the question. Our approach is theoretically grounded and sample-efficient, eliminating the need for helper LLM inference or manual prompt design. Empirically, LaRS achieves performance comparable to SOTA rationale-based selection methods, saving thousands of LLM inferences and significantly reducing the time required to process the example bank",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pXKlTtXxRB8": {
    "title": "Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems",
    "volume": "review",
    "abstract": "Retrieval-Augmented Generation (RAG) improves Language Models (LMs) by incorporating external knowledge at test time to enable customized adaptation. We study the risk of datastore leakage in Retrieval-In-Context based RAG systems. We show that an adversary can exploit LMs' instruction-following capabilities to easily extract text data verbatim from the datastore of RAG systems built with instruction-tuned LMs via prompt injection. The vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up. Extending our study to production RAG models GPTs, we design an attack that can cause datastore leakage with a 100% success rate on 25 randomly selected customized GPTs with at most 2 queries, and we extract text data verbatim at a rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,000 words by prompting the GPTs with only 100 queries generated by themselves",
    "checked": true,
    "id": "7b00cb1fe1773f964d123dab6d4812c7bc63de06",
    "semantic_title": "follow my instruction and spill the beans: scalable data extraction from retrieval-augmented generation systems",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=nUYFsO7EJv8": {
    "title": "Beyond Hate Speech: NLP's Challenges and Opportunities in Uncovering Dehumanizing Language",
    "volume": "review",
    "abstract": "Dehumanization, characterized as a subtle yet harmful manifestation of hate speech, involves denying individuals of their human qualities and often results in violence against marginalized groups. Despite significant progress in Natural Language Processing across various domains, its application in detecting dehumanizing language is limited, largely due to the scarcity of publicly available annotated data for this domain. This paper evaluates the performance of cutting-edge NLP models, including GPT-4, GPT-3.5, and LLAMA-2, in identifying dehumanizing language. Our findings reveal that while these models demonstrate potential, achieving a 70\\% accuracy rate in distinguishing dehumanizing language from broader hate speech, they also display biases. They are over-sensitive in classifying other forms of hate speech as dehumanization for a specific subset of target groups, while more frequently failing to identify clear cases of dehumanization for other target groups. Moreover, leveraging one of the best-performing models, we automatically annotated a larger dataset for training more accessible models. However, our findings indicate that these models currently do not meet the high-quality data generation threshold necessary for this task",
    "checked": true,
    "id": "d32cc4a0db26c4c508337d51c2fa0c50da32f702",
    "semantic_title": "beyond hate speech: nlp's challenges and opportunities in uncovering dehumanizing language",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lNUu2_DRdH": {
    "title": "Workflow-Guided Response Generation for Task-Oriented Dialogue",
    "volume": "review",
    "abstract": "Task-oriented dialogue (TOD) systems aim to achieve specific goals through interactive dialogue. Such tasks usually involve following specific workflows, i.e. executing a sequence of actions in a particular order. While prior work has focused on supervised learning methods to condition on past actions, they do not explicitly optimize for compliance to a desired workflow. In this paper, we propose a novel framework based on reinforcement learning (RL) to generate dialogue responses that are aligned with a given workflow. Our framework consists of ComplianceReward, a metric designed to evaluate how well a generated response executes the specified action, combined with an RL optimization process that utilizes an interactive sampling technique. We evaluate our approach on two TOD datasets, Action-Based Conversations Dataset (ABCD) (Chen et al., 2021) and MultiWOZ 2.2 (Zang et al., 2020) on a range of automated and human evaluation metrics. Our findings indicate that our RL-based framework outperforms baselines and is effective at generating responses that both comply with the intended workflows while being expressed in a natural and fluent manner",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qU1YwNQibYK": {
    "title": "LLMs cannot find reasoning errors, but can correct them!",
    "volume": "review",
    "abstract": "While self-correction has shown promise in improving LLM outputs in terms of style and quality (e.g. Chen et al., 2023; Madaan et al., 2023), recent attempts to self-correct logical or reasoning errors often cause correct answers to become incorrect, resulting in worse performances overall (Huang et al., 2023). In this paper, we break down the self-correction process into two core components: mistake finding and output correction. For mistake finding, we release BIG-Bench Mistake, a dataset of logical mistakes in Chain-of-Thought reasoning traces. We provide benchmark numbers for several state-of-the-art LLMs, and demonstrate that LLMs generally struggle with finding logical mistakes. For output correction, we propose a backtracking method which provides large improvements when given information on mistake location. We construe backtracking as a lightweight alternative to reinforcement learning methods, and show that it remains effective with a reward model at 60-70% accuracy",
    "checked": true,
    "id": "2cc5a2e8e5e739dbc22fce6eb0242bda3acd7998",
    "semantic_title": "llms cannot find reasoning errors, but can correct them!",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=h_YKwWlRq4-": {
    "title": "On the Generalization of Training-based ChatGPT Detection Method",
    "volume": "review",
    "abstract": "Large language models, such as ChatGPT, achieve amazing performance on various language processing tasks. However, they can also be exploited for improper purposes such as plagiarism or misinformation dissemination. Thus, there is an urgent need to detect the texts generated by LLMs. One type of most studied methods trains classification models to distinguish LLM texts from human texts. However, existing studies demonstrate the trained models may suffer from distribution shifts (during test), i.e., they are ineffective to predict the generated texts from unseen language tasks or topics. In this work, we focus on ChatGPT as a representative model, and we conduct a comprehensive investigation on these methods' generalization behaviors under distribution shift caused by a wide range of factors, including prompts, text lengths, topics, and language tasks. To achieve this goal, we first collect a new dataset with human and ChatGPT texts, and then we conduct extensive studies on the collected dataset. Our studies unveil insightful findings that provide guidance for future methodologies and data collection strategies for LLM detection",
    "checked": false,
    "id": "feb457c12242d4980b5df2eefc4ee6f5855428e2",
    "semantic_title": "on the generalization of training-based chatgpt detection methods",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=Fmz30OCQcjN": {
    "title": "QASE Enhanced PLMs: Improved Control in Text Generation for MRC",
    "volume": "review",
    "abstract": "To address the challenges of out-of-control generation in generative models for machine reading comprehension (MRC), we introduce the Question-Attended Span Extraction (QASE) module. Integrated during the fine-tuning of pre-trained generative language models (PLMs), QASE enables these PLMs to match SOTA extractive methods and outperform leading LLMs like GPT-4 in MRC tasks, without significant increases in computational costs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YKNtAXae4z": {
    "title": "TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification",
    "volume": "review",
    "abstract": "Large Language Model (LLM) services and models often come with legal rules on who can use them and how they must use them. Assessing the compliance of the released LLMs is crucial, as these rules protect the interests of the LLM contributor and prevent misuse. In this context, we describe the novel problem of Black-box Identity Verification (BBIV). The goal is to determine whether a third-party application uses a certain LLM through its chat function. We propose a method called Targeted Random Adversarial Prompt (TRAP) that identifies the specific LLM in use. We repurpose adversarial suffixes, originally proposed for jailbreaking, to get a pre-defined answer from the target LLM, while other models give random answers. TRAP detects the target LLMs with over 95% true positive rate at under 0.2% false positive rate even after a single interaction. TRAP remains effective even if the LLM has minor changes that do not significantly alter the original function",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k7_G9OJQSd": {
    "title": "ULTRA: Unleash LLMs' Potential for Event Argument Extraction through Hierarchical Modeling and Pair-wise Refinement",
    "volume": "review",
    "abstract": "Structural extraction of events within discourse is critical since it avails a deeper understanding of communication patterns and behavior trends. Event argument extraction (EAE), at the core of event-centric understanding, is the task of identifying role-specific text spans (i.e., arguments) for a given event. Document-level EAE (DocEAE) focuses on arguments that are scattered across an entire document. In this work, we explore open-source Large Language Models (LLMs) for DocEAE, and propose ULTRA, a hierarchical framework that extracts event arguments more cost-effectively. Further, it alleviates the positional bias issue intrinsic to LLMs. ULTRA sequentially reads text chunks of a document to generate a candidate argument set, upon which non-pertinent candidates are dropped through self-refinement. We introduce LEAFER to address the challenge LLMs face in locating the exact boundary of an argument. ULTRA outperforms strong baselines, including strong supervised models and ChatGPT, by 9.8% when evaluated by Exact Match (EM)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z1hZRBCZYPv": {
    "title": "STRUCTSUM Generation for Faster Text Comprehension",
    "volume": "review",
    "abstract": "We consider the task of generating structured representations of text using large language models (LLMs). We focus on tables and mind maps as representative modalities. Tables are more organized way of representing data, while mind maps provide a visually dynamic and flexible approach, particularly suitable for sparse content. Despite the effectiveness of LLMs on different tasks, we show that current models struggle with generating structured outputs. In response, we present effective prompting strategies for both of these tasks. We introduce a taxonomy of problems around factuality, global and local structure, common to both modalities and propose a set of critiques to tackle these issues resulting in an absolute improvement in accuracy of $+37$pp ($79\\%$) for mind maps and $+15$pp ($78\\%$) for tables. To evaluate semantic coverage of generated structured representations we propose \\textsc{Auto-QA}, and we verify the adequacy of \\textsc{Auto-QA} using SQuAD dataset. We further evaluate the usefulness of structured representations via a text comprehension user study. The results show a significant reduction in comprehension time compared to text when using table ($42.9\\%$) and mind map ($31.9\\%$), without loss in accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ftWc6BW77Iy": {
    "title": "Qibo: A Large Language Model for Traditional Chinese Medicine",
    "volume": "review",
    "abstract": "In the field of Artificial Intelligence, Large Language Models (LLMs) have demonstrated significant advances in user intent understanding and response in a number of specialized domains, including medicine, law, and finance. However, in the unique domain of traditional Chinese medicine (TCM), the performance enhancement of LLMs is challenged by the essential differences between its theories and modern medicine, as well as the lack of specialized corpus resources. In this paper, we aim to construct and organize a professional corpus in the field of TCM, to endow the large model with professional knowledge that is characteristic of TCM theory, and to successfully develop the Qibo model based on LLaMA, which is the first LLM in the field of TCM to undergo a complete training process from pre-training to Supervised Instruction Fine-Tuning (SFT). Furthermore, we develop the Qibo-benchmark, a specialized tool for evaluating the performance of LLMs, which is a specialized tool for evaluating the performance of LLMs in the TCM domain. This tool will provide an important basis for quantifying and comparing the understanding and application capabilities of different models in the field of traditional Chinese medicine, and provide guidance for future research directions and practical applications of intelligent assistants for traditional Chinese medicine. Finally, we conducted sufficient experiments to prove that Qibo has good performance in the field of traditional Chinese medicine",
    "checked": true,
    "id": "6457474106e7af2c5f67769fbf915432b7d1b513",
    "semantic_title": "qibo: a large language model for traditional chinese medicine",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=k1iDNO-F9n": {
    "title": "InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment",
    "volume": "review",
    "abstract": "Do current large language models (LLMs) better solve graph reasoning and generation tasks with parameter updates? In this paper, we propose \\textbf{InstructGraph}, a framework that empowers LLMs with the abilities of graph reasoning and generation by instruction tuning and preference alignment. Specifically, we first propose a structured format verbalizer to unify all graph data into a universal code-like format, which can simply represent the graph without any external graph-specific encoders. Furthermore, a graph instruction tuning stage is introduced to guide LLMs in solving graph reasoning and generation tasks. Finally, we identify potential hallucination problems in graph tasks and sample negative instances for preference alignment, the target of which is to enhance the output's reliability of the model. Extensive experiments across multiple graph-centric tasks exhibit that InstructGraph can achieve the best performance and outperform GPT-4 and LLaMA2 by more than 13\\% and 38\\%, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QdE6PMD9ha": {
    "title": "Unity in Diversity: Collaborative Pre-training Across Multimodal Medical Sources",
    "volume": "review",
    "abstract": "Although pre-training has become a prevalent approach for addressing various biomedical tasks, the current efficacy of pre-trained models is hindered by their reliance on a limited scope of medical sources. This limitation results in data scarcity during pre-training and restricts the range of applicable downstream tasks. In response to these challenges, we develop MedCSP, a new pre-training strategy designed to bridge the gap between multimodal medical sources. MedCSP employs modality-level aggregation to unify patient data within individual sources. Additionally, leveraging temporal information and diagnosis history, MedCSP effectively captures explicit and implicit correlations between patients across different sources. To evaluate the proposed strategy, we conduct comprehensive experiments, where the experiments are based on 6 modalities from 2 real-world medical data sources, and MedCSP is evaluated on 4 tasks against 19 baselines, marking an initial yet essential step towards cross-source modeling in the medical domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3gAMIOB3KKQ": {
    "title": "Self-Augmented In-Context Learning for Unsupervised Word Translation",
    "volume": "review",
    "abstract": "Recent work has shown that, while large language models (LLMs) demonstrate strong word translation or bilingual lexicon induction (BLI) capabilities in few-shot setups, they still cannot match the performance of 'traditional' mapping-based approaches in the unsupervised scenario where no seed translation pairs are available, especially for lower-resource languages. To address this challenge with LLMs, we propose self-augmented in-context learning (SAIL) for unsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces a set of high-confidence word translation pairs for in-context learning (ICL) from an LLM, which it then reapplies to the same LLM in the ICL fashion. Our method shows substantial gains over zero-shot prompting of LLMs on two established BLI benchmarks spanning a wide range of language pairs, also outperforming mapping-based baselines across the board. In addition to achieving state-of-the-art unsupervised BLI performance, we also conduct comprehensive analyses on SAIL and discuss its limitations",
    "checked": true,
    "id": "382125e1416fbec2d40d7b0d207e620693c76ecb",
    "semantic_title": "self-augmented in-context learning for unsupervised word translation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z84nU8l0m0": {
    "title": "E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity",
    "volume": "review",
    "abstract": "Traditional pruning methods are known to be challenging to work in Large Language Models for Generative AI because of their unaffordable training process and large computational demands. For the first time, we introduce the information entropy of hidden state features into a pruning metric design, namely E-Sparse, to improve the accuracy of N:M sparsity on LLM. E-Sparse employs the information richness to leverage the channel importance, and further incorporates several novel techniques to put it into effect: (1) it introduces information entropy to enhance the significance of parameter weights and input feature norms as a novel pruning metric, and performs N:M sparsity without modifying the remaining weights. (2) it designs global naive shuffle and local block shuffle to quickly optimize the information distribution and adequately cope with the impact of N:M sparsity on LLMs' accuracy. E-Sparse is implemented as a Sparse-GEMM on FasterTransformer and runs on NVIDIA Ampere GPUs. Extensive experiments on the LLaMA family and OPT models show that E-Sparse can significantly speed up the model inference over the dense model (up to 1.53 ) and obtain significant memory saving (up to 43.52%), with acceptable accuracy loss",
    "checked": false,
    "id": "9df4bfd06331557e1c0edf7937d20bd755369ed0",
    "semantic_title": "e-sparse: boosting the large language model inference through entropy-based n: m sparsity",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=MG-85xt8x7": {
    "title": "Reconstruct Your Previous Conversations! Comprehensively Investigating Privacy Leakage Risks in Conversations with GPT Models",
    "volume": "review",
    "abstract": "Significant advancements have been made in the field of large language models recently, represented by GPT models. Users frequently have multi-round private conversations with cloud-hosted GPT models for task optimization. Yet, this operational paradigm introduces additional attack surfaces, particularly in custom GPTs and hijacked chat sessions. In this paper, we introduce a straightforward yet potent Conversation Reconstruction Attack, that employs malicious prompts to query GPT models to leak previous conversations. Our comprehensive examination of privacy risks during GPT interactions under this attack reveals GPT-4's considerable resilience. We present two advanced attacks targeting improved reconstruction of past conversations, demonstrating significant privacy leakage across all models under these advanced techniques. Evaluating various defense mechanisms, we find them ineffective against these attacks. Our findings highlight the ease with which privacy can be compromised in interactions with GPT models, urging the community to safeguard against potential abuses of these models' capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j1kpuYzWgr": {
    "title": "From Zero to Hero: Generalized Cold-Start Anomaly Detection",
    "volume": "review",
    "abstract": "When first deploying an anomaly detection system, e.g., to detect out-of-scope queries in chatbots, there are no observed data, making data-driven approaches ineffective. Zero-shot anomaly detection methods offer a solution to such \"cold-start\" cases, but unfortunately they are often not accurate enough. This paper studies the realistic but underexplored \\textit{generalized cold-start} setting where an anomaly detection model is initialized using zero-shot guidance, but subsequently receives a small number of contaminated observations (namely, that may include anomalies). The goal is to make efficient use of both the zero-shot guidance and the observations. We propose ColdFusion, a method that effectively adapts the zero-shot anomaly detector to contaminated observations. To support future development of this new setting, we propose an evaluation suite consisting of evaluation protocols and metrics",
    "checked": false,
    "id": "f0583ae00368c8fded7ab703d114e696f695f807",
    "semantic_title": "from zero to hero: cold-start anomaly detection",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qOQ04Y58_F": {
    "title": "Distilling Knowledge from Text-to-Image Generative Models Improves Visio-Linguistic Reasoning in CLIP",
    "volume": "review",
    "abstract": "Image-text contrastive models like CLIP have wide applications in zero-shot classification, image-text retrieval, and transfer learning. However, they often struggle on compositional visio-linguistic tasks (e.g., attribute-binding or object-relationships) where their performance is no better than random chance. To address this, we introduce SDS-CLIP, a lightweight and sample-efficient distillation method to enhance CLIP's compositional visio-linguistic reasoning. Our approach fine-tunes CLIP using a distillation objective borrowed from large text-to-image generative models like Stable-Diffusion, which are known for their strong visio-linguistic reasoning abilities. On the challenging Winoground benchmark, SDS-CLIP improves the visio-linguistic performance of various CLIP models by up to 7%, while on the ARO dataset, it boosts performance by up to 3%. This work underscores the potential of well-designed distillation objectives from generative models to enhance contrastive image-text models with improved visio-linguistic reasoning capabilities",
    "checked": true,
    "id": "6ad2776e368dade4da2ce71e4250087e0ca85868",
    "semantic_title": "distilling knowledge from text-to-image generative models improves visio-linguistic reasoning in clip",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=LnrYBm96YY": {
    "title": "Enhancing Portuguese Varieties Identification with Cross-Domain Approaches",
    "volume": "review",
    "abstract": "Recent advances in natural language processing (NLP) have significantly raised expectations for generative models to produce coherent text across diverse languages varieties. In the particular case of the Portuguese language, a predominance of Brazilian Portuguese corpora online induces linguistics traces on those models, limiting its adoption outside Brazil. To address this gap and promote the creation of European Portuguese resources, we developed a cross-domain language variety identifier (LVI) to discriminate between European and Brazilian Portuguese. The findings of the literature review process motivated us to compile PtBrVarId, a cross-domain LVI corpus, and to study how transformer-based LVI classifiers can be optimised to perform in a cross-domain scenario. Our most effective model, a PtBrVarId fine-tuned version of BERT, sets a new state-of-the-art result of $0.84$ $F_1$-Score on the DSL-TL corpus, the LVI reference benchmark. This result was obtained while maintaining state-of-the-art (SOTA) results (above 0.90 $F_1$-Score) in the cross-domain scenario. Although this research is focused on two Portuguese varieties, its ideas can be extended to other varieties and languages. We open-source the code, corpus, and models to foster further research in this task",
    "checked": false,
    "id": "daae64c1f1d159da39ac91254589e17d70673b6a",
    "semantic_title": "synopsis of a consultative workshop at the ocean sciences meeting 2022 on the ceos ocean variables enabling research and applications for geo (coverage) initiative",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=r0XZg_YPWV": {
    "title": "In Dialogues We Learn\": Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning",
    "volume": "review",
    "abstract": "Personalized dialogue systems have gained significant attention in recent years for their ability to generate responses that align with a specific persona. However, most existing approaches rely on pre-defined personal profiles, which are not only time-consuming and labor-intensive to create but also lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning framework that enhances the ability of pre-trained large language models to leverage dialogue history effectively for completing personalized dialogue generation tasks without the need of pre-defined profiles. Our experiments across three datasets demonstrate that IDL can lead to substantial improvements, with BLEU and ROUGE scores increasing by up to $200\\%$ and $247\\%$, respectively. Additionally, the results of human evaluations further validate the efficacy of our method",
    "checked": true,
    "id": "5678a1ee9d5542785115555b856e51a1dd9eb0e9",
    "semantic_title": "in dialogues we learn\": towards personalized dialogue without pre-defined profiles through in-dialogue learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=RvLT9lSpkv": {
    "title": "Annotation Assistance for Thematic Analysis using Transfer Learning",
    "volume": "review",
    "abstract": "Manual annotation of qualitative research data is costly and time-consuming. Recently, machine learning approaches have been introduced to assist such tasks. However, it remains challenging for a machine learning model to incorporate context, data scarcity, data imbalance and other aspects in thematic analysis. We employed transfer learning, combining the pre-trained models BERT and ResNet to propose an annotation assistance model and evaluated its accuracy and efficiency for semi-automatic annotation. We experimented on a dataset of focus group discussions between researchers and participants on perception towards robots in public spaces. We tested various training methods, including few-shot learning, data augmentation, and the use of different data modalities, to evaluate the impact of dataset size, data balance, and data modality on the proposed annotation assistance model's performance. The best-performing model achieved an average balanced accuracy of 59.89% for predicting thematic labels in researcher sentences and 48.67% for participant sentences",
    "checked": false,
    "id": "b30d15f2cce2f51a71d8632ce34dd37a9c7dc362",
    "semantic_title": "improving phiri performance and scalability: working within egi-ace",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=2gtCJ7hVfQ": {
    "title": "Learning Representation for Earnings Call Transcript via Structure-Aware Key Insight Extraction",
    "volume": "review",
    "abstract": "Learning representations for earnings call transcripts encounter significant challenges, such as the unreliability of the knowledge encoding process and specific domain-specific requirements in the financial context. To address these challenges, this work proposes a self-supervised transcript representation learning approach that utilizes structural information within transcripts to provide supervision signals. Additionally, it offers concise explanations for each decision made by the neural networks through a redundancy-aware key sentence extractor. Extensive experiments across various downstream tasks, such as risk prediction, information retrieval, and firm similarity analysis, demonstrate the effectiveness of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YVsaB3Fnac": {
    "title": "Asymmetric Bias in Text-to-Image Generation with Adversarial Attacks",
    "volume": "review",
    "abstract": "The widespread use of Text-to-Image (T2I) models in content generation requires careful examination of their safety, including their robustness to adversarial attacks. Despite extensive research on adversarial attacks, the reasons for their effectiveness remain underexplored. This paper presents an empirical study on adversarial attacks against T2I models, focusing on analyzing factors associated with attack success rates (ASR). We introduce a new attack objective - entity swapping using adversarial suffixes and two gradient-based attack algorithms. Human and automatic evaluations reveal the asymmetric nature of ASRs on entity swap: for example, it is easier to replace \"human\" with \"robot\" in the prompt \"a human dancing in the rain.\" with an adversarial suffix, but the reverse replacement is significantly harder. We further propose probing metrics to establish indicative signals from the model's beliefs to the adversarial ASR. We identify conditions that result in a success probability of 60% for adversarial attacks and others where this likelihood drops below 5%",
    "checked": true,
    "id": "49cd807031bb658e13df21fc05a79c77f0c6b6d3",
    "semantic_title": "asymmetric bias in text-to-image generation with adversarial attacks",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=xkHkfXXW7U": {
    "title": "EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge",
    "volume": "review",
    "abstract": "Despite the remarkable strides of Large Language Models (LLMs) in various fields, the wide applications of LLMs on edge devices are limited due to their massive parameters and computations. To address this, quantization is commonly adopted to generate lightweight LLMs with efficient computations and fast inference. However, Post-Training Quantization (PTQ) methods dramatically degrade in quality when quantizing weights, activations, and KV cache together to below 8 bits. Besides, many Quantization-Aware Training (QAT) works quantize model weights, leaving the activations untouched, which do not fully exploit the potential of quantization for inference acceleration on the edge. In this paper, we propose EdgeQAT, the Entropy and Distribution Guided QAT for the optimization of lightweight LLMs to achieve inference acceleration on Edge devices. We first identify that the performance drop of quantization primarily stems from the information distortion in quantized attention maps, demonstrated by the different distributions in quantized query and key of the self-attention mechanism. Then, the entropy and distribution guided QAT is proposed to mitigate the information distortion. Moreover, we design a token importance-aware adaptive method to dynamically quantize the tokens with different bit widths for further optimization and acceleration. Our extensive experiments verify the substantial improvements with our framework across various datasets. Furthermore, we achieve an on-device speedup of up to 2.37x compared with its FP16 counterparts across multiple edge devices, signaling a groundbreaking advancement",
    "checked": true,
    "id": "38be82da178efaa903ebf5b57520c64195c7354e",
    "semantic_title": "edgeqat: entropy and distribution guided quantization-aware training for the acceleration of lightweight llms on the edge",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=E8eER_qJc2": {
    "title": "VoiceTuner: Self-Supervised Pre-training and Efficient Fine-tuing For Voice Generation",
    "volume": "review",
    "abstract": "Voice large language models (LLMs) cast voice synthesis as a language modeling task in a discrete space, and have demonstrated significant progress to date. Despite the recent success, the current development of voice LLMs in low-resource applications is hampered by data scarcity and high computational cost. In this work, we propose VoiceTuner, with a self-supervised pre-training and efficient fine-tuning approach for low-resource voice generation. Specifically, 1) to mitigate data scarcity, we leverage large-scale unlabeled dataset and pre-train VoiceTuner-SSL without pre-defined applications, which can be fine-tuned in downstream tasks; 2) to further reduce the high training cost in complete fine-tuning, we introduce a multiscale adapter to effectively update around only 1% parameters as a plug-and-play module; and 3) to alleviate the challenges of modeling long audio tokens inherited from inefficient attention mechanism, we introduce VoiceTuner-Mamba with multiscale state space models in place of transformers. Experimental results demonstrate that VoiceTuner-SSL presents strong acoustic continuations. VoiceTuner exhibits superior quality and style similarity in three low-resource (1h, 10h, 30h) generation tasks. Audio samples are available at https://VoiceTuner.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8rAzPVCsyf": {
    "title": "Towards Explainable Chinese Native Learner Essay Fluency Assessment: Dataset, Tasks, and Method",
    "volume": "review",
    "abstract": "Grammatical Error Correction (GEC) is a crucial technique in Automated Essay Scoring (AES) for evaluating the fluency of essays. However, in Chinese, existing GEC datasets often fail to consider the importance of specific grammatical error types within compositional scenarios, lack research on data collected from native Chinese speakers, and largely overlook cross-sentence grammatical errors. Furthermore, the measurement of the overall fluency of an essay is often overlooked. To address these issues, we present CEFA (Chinese Essay Fluency Assessment), an extensive corpus that is derived from essays authored by native Chinese-speaking primary and secondary students and encapsulates essay fluency scores along with both coarse and fine-grained grammatical error types and corrections. Experiments employing various benchmark models on CEFA substantiate the challenging nature of our dataset. Our findings further highlight the significance of fine-grained annotations in fluency assessment and the mutually beneficial relationship between error types and corrections. We will make the corpus and related codes available for research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VznVl88VzL": {
    "title": "TETA: Temporal-Enhanced Text-to-Audio Generation",
    "volume": "review",
    "abstract": "Large diffusion models have been successful in text-to-audio (T2A) synthesis tasks, but they often suffer from common issues such as semantic misalignment and poor temporal consistency due to limited natural language understanding and data scarcity. Additionally, 2D spatial structures widely used in T2A works lead to unsatisfactory audio quality when generating variable-length audio samples since they ignore the time-frequency structure in the mel-spectrogram. To address these challenges, we propose TETA, a latent diffusion-based T2A method. Our approach includes several techniques to improve semantic alignment and temporal consistency: Firstly, we use pre-trained large language models (LLMs) to parse the text into structured <event \\& order> pairs for better temporal information capture. We also introduce another structured-text encoder to aid in learning semantic alignment during the diffusion denoising process. To improve the performance of variable length generation and enhance the temporal information extraction, we design a feed-forward Transformer-based diffusion denoiser. Finally, we use LLMs to augment and transform a large amount of audio-label data into audio-text datasets to alleviate the problem of scarcity of temporal data. Extensive experiments show that our method outperforms baseline models in both objective and subjective metrics, and achieves significant gains in temporal information understanding, semantic consistency, and sound quality. Our demos are available at \\url{https://teta2023.github.io}",
    "checked": false,
    "id": "83d4b22d803ae856cf6b308482bd504fa151d39e",
    "semantic_title": "make-an-audio 2: temporal-enhanced text-to-audio generation",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=NmiraNcR-H": {
    "title": "Evade ChatGPT Detectors via A Single Space",
    "volume": "review",
    "abstract": "ChatGPT brings significant social value but also raises concerns about the misuse of AI-generated text. Consequently, an important problem is how to detect whether texts are generated by ChatGPT or by human. Although automated detection methods have been proposed, we find that these detectors do not effectively discriminate the semantic and stylistic gaps between human-generated and AI-generated text. Instead, the ``subtle differences'', such as {\\it an extra space}, become crucial for detection. Based on this discovery, we propose the SpaceInfi strategy to evade detection. Experiments demonstrate the effectiveness of this strategy across multiple benchmarks and detectors. And we empirically show that a phenomenon called {\\it token mutation} causes the evasion for language model-based detectors",
    "checked": true,
    "id": "ae39f3c1c8c1bce030d3b32847519dec0405d9aa",
    "semantic_title": "evade chatgpt detectors via a single space",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=AaaxPMyPyu": {
    "title": "Can You Learn Semantics Through Next-Word Prediction? The Case of Entailment",
    "volume": "review",
    "abstract": "Do LMs infer the semantics of text from co-occurrence patterns in their training data? Merrill et al. (2022) argue that, in theory, probabilities predicted by an optimal LM encode semantic information about entailment relations, but it is unclear whether neural LMs trained on corpora learn entailment in this way because of strong idealizing assumptions made by Merrill et al. In this work, we investigate whether their theory can be used to decode entailment judgments from neural LMs. We find that a test similar to theirs can decode entailment relations between natural sentences, well above random chance, though not perfectly, across many datasets and LMs. This suggests LMs implicitly model aspects of semantics to predict semantic effects on sentence co-occurrence patterns. However, we find the test that predicts entailment in practice works in the opposite direction to the theoretical test. We thus revisit the assumptions underlying the original test, finding its derivation did not adequately account for redundancy in human-written text. We argue that correctly accounting for redundancy related to *explanations* might derive the observed flipped test and, more generally, improve linguistic theories of human speakers",
    "checked": true,
    "id": "596e341f34a9726f6400377eecf848c8f5351086",
    "semantic_title": "can you learn semantics through next-word prediction? the case of entailment",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=epFJoGD8wkg": {
    "title": "Controllable Text Summarization: Unraveling Challenges, Approaches, and Prospects - A Survey",
    "volume": "review",
    "abstract": "Generic text summarization approaches often fail to address the specific intent and needs of individual users. Recently, scholarly attention has turned to the development of summarization methods that are more closely tailored and controlled to align with specific objectives and user needs. Despite a growing corpus of controllable summarization research, there is no comprehensive survey available that thoroughly explores the diverse controllable attributes employed in this context, delves into the associated challenges, and investigates the existing solutions. In this survey, we formalize the Controllable Text Summarization (CTS) task, categorize controllable attributes according to their shared characteristics and objectives, and present a thorough examination of existing datasets and methods within each category. Moreover, based on our findings, we uncover limitations and research gaps, while also exploring potential solutions and future directions for CTS",
    "checked": true,
    "id": "7fe7ceda8d30b7eadbd49e61e32beebf513e1d12",
    "semantic_title": "controllable text summarization: unraveling challenges, approaches, and prospects - a survey",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=mA28B_PCL9": {
    "title": "Direct Preference Optimization with an Offset",
    "volume": "review",
    "abstract": "Direct preference optimization (DPO) is a successful fine-tuning strategy for aligning large language models with human preferences without the need to train a reward model or employ reinforcement learning. DPO, as originally formulated, relies on binary preference data and fine-tunes a language model to increase the likelihood of a preferred response over a dispreferred response. However, not all preference pairs are equal: while in some cases the preferred response is only slightly better than the dispreferred response, there can be a stronger preference for one response when, for example, the other response includes harmful or toxic content. In this paper, we propose a generalization of DPO, termed DPO with an offset (ODPO), that does not treat every preference pair equally during fine-tuning. Intuitively, ODPO requires the difference between the likelihood of the preferred and dispreferred response to be greater than an offset value. The offset is determined based on the extent to which one response is preferred over another. Our experiments on various tasks suggest that ODPO significantly outperforms DPO in aligning language models, especially when the number of preference pairs is limited",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zsFx_kRfCJ3": {
    "title": "Label Augmentation for Zero-Shot Hierarchical Text Classification",
    "volume": "review",
    "abstract": "Hierarchical Text Classification poses the difficult challenge of classifying documents into multiple labels organized in a hierarchy. The vast majority of works aimed to address this problem relies on supervised methods which are difficult to implement due to the scarcity of labeled data in many real world applications. This paper focuses on strict Zero-Shot Classification, the setting in which the system lacks both labeled instances and training data.We propose a novel approach that uses a Large Language Model to augment the deepest layer of the labels hierarchy in order to enhance its specificity. We achieve this by generating semantically relevant labels as children connected to the existing branches, creating a deeper taxonomy that better overlaps with the input texts. We leverage the enriched hierarchy to perform Zero-Shot Hierarchical Classification by using the Upward score Propagation technique. We test our method on four public datasets, obtaining new state-of-the art results on three of them. We introduce two cosine similarity-based metrics to quantify the density and granularity of a label taxonomy and we show a strong correlation between the metric values and the classification performance of our method on the datasets",
    "checked": false,
    "id": "6083c840b4f9d5edbae0d60c7d330a76cd965a6a",
    "semantic_title": "teleclass: taxonomy enrichment and llm-enhanced hierarchical text classification with minimal supervision",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=GPGT8FB4T8T": {
    "title": "XCodeEval: An Execution-based Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval",
    "volume": "review",
    "abstract": "Recently, pre-trained large language models (LLMs) have shown impressive abilities in generating codes from natural language descriptions, repairing buggy codes, translating codes between languages, and retrieving relevant code segments. However, the evaluation of these models has often been performed in a scattered way on only one or two specific tasks, in a few languages, at a partial granularity (e.g., function) level, and in many cases without proper training data. Even more concerning is that in most cases the evaluation of generated codes has been done in terms of mere lexical overlap with a reference code rather than actual execution. We introduce *xCodeEval*, the largest executable multilingual multitask benchmark to date consisting of $25$M document-level coding examples ($16.5$B tokens) from about $7.5$K unique problems covering up to $11$ programming languages with execution-level parallelism. It features a total of $7$ tasks involving code understanding, generation, translation and retrieval. *xCodeEval* adopts an execution-based evaluation and offers a multilingual code execution engine, \\texttt{ExecEval} that supports unit test based execution in all the $11$ languages. To address the challenge of balancing the distributions of text-code samples over multiple attributes in validation/test sets, we propose a novel data splitting and a data selection schema based on the geometric mean and graph-theoretic principle. Our experiments with OpenAI's LLMs (zero-shot) and open-LLMs (zero-shot and fine-tuned) on the tasks and languages demonstrate to be quite challenging as per the current advancements in language models",
    "checked": false,
    "id": "1012d2a3281dbb40c22e25652b57fc532180f59d",
    "semantic_title": "xcodeeval: a large scale multilingual multitask benchmark for code understanding, generation, translation and retrieval",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=4lmwGKcj7F-": {
    "title": "Cascaded Chain-of-Thoughts Distillation: Distilling Reasoning Capabilities from Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities at increased scales, spurring efforts to distill such capabilities into smaller, compact models via teacher-student learning. Previous works directly fine-tune student models on teachers' generated Chain-of-Thoughts (CoTs) data or learn it in a multi-task framework. However, these methods struggle with CoTs generalization due to spurious correlations between questions and answers, as well as inconsistencies in the logic connecting the rationales to the answers. In this paper, we propose \\textbf{Cas}caded \\textbf{Co}Ts \\textbf{D}istillation (CasCoD), a straightforward but effective method to address these issues. Specifically, we decompose the full CoTs distillation into two comprehensive tasks and learn it in a cascade way by sharing the input prefix. By separating and cascading the tasks, CasCoD not only enables the student model to concentrate on reasoning without the distraction of answers but ensures faithful reasoning in students, thus enhancing the generalizability of CoTs. Extensive experiments and further analysis demonstrate the effectiveness of CasCoD on both in-domain and out-of-domain benchmark reasoning datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-eXNq4LYR7S": {
    "title": "Can Large Language Models Mine Interpretable Financial Factors More Effectively? A Neural-Symbolic Factor Mining Agent Model",
    "volume": "review",
    "abstract": "Finding interpretable factors for stock returns is the most vital issue in the empirical asset pricing domain. As data-driven methods, existing factor mining models can be categorized into symbol-based and neural-based models. Symbol-based models are interpretable but inefficient, while neural-based approaches are efficient but lack interpretability. Hence, mining interpretable factors effectively presents a significant challenge. Inspired by the success of Large Language Models (LLMs) in various tasks, we propose a FActor Mining Agent (FAMA) model that enables LLMs to integrate the strengths of both neural and symbolic models for factor mining. In this paper, FAMA consists of two main components:Cross-Sample Selection (CSS) and Chain-of-Experience (CoE). CSS addresses the homogeneity challenges in LLMs during factor mining by assimilating diverse factors as in-context samples, whereas CoE enables LLMs leverage past successful mining experiences, expediting the mining of effective factors. Experimental evaluations on real-world stock market data demonstrate the effectiveness of our approach by surpassing the SOTA RankIC by 0.006 and RankICIR by 0.105 in predicting S&P 500 returns. Furthermore, the investment simulation shows that our model can achieve superior performance with an annualized return of 39.0% and a Sharpe ratio of 667.6%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oma500esY8": {
    "title": "Language-Informed Beam Search Decoding for Multilingual Machine Translation",
    "volume": "review",
    "abstract": "Beam search decoding is the de-facto method for decoding auto-regressive Neural Machine Translation (NMT) models, including multilingual NMT where the target language is specified as an input. However, decoding multilingual NMT models commonly produces off-target translations -- yielding translation outputs not in the intended language.In this paper, we first conduct an error analysis of off-target translations for a strong multilingual NMT model and identify how these decodings are produced during beam search. We then propose Language-informed Beam Search (LiBS), a general decoding algorithm incorporating an off-the-shelf Language Identification (LiD) model into beam search decoding to reduce off-target translations. LiBS is an inference-time procedure that is NMT-model agnostic and does not require any additional parallel data. Results show that our proposed LiBS algorithm on average improves +1.1 BLEU and +0.9 BLEU on WMT and OPUS datasets, and reduces off-target rates from 22.9% to 7.7% and 65.8% to 25.3% respectively",
    "checked": false,
    "id": "e8e6d462d0a7656a9fefec21075159fd2138a780",
    "semantic_title": "gentranslate: large language models are generative multilingual speech and machine translators",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=PC9tBA9z6OP": {
    "title": "VulLibGen: Generating Names of Vulnerability-Affected Packages via a Large Language Model",
    "volume": "review",
    "abstract": "To avoid potential risks posed by vulnerabilities in third-party packages, security practitioners maintain vulnerability reports in vulnerability databases (e.g., GitHub Advisory) to help developers realize and deploy vulnerability patches.However, existing work shows that in more than half of the vulnerability reports, the field of vulnerability-affected packages is missing or incorrect. To help reduce the manual efforts in completing and validating the affected-package field, existing work proposes to automatically identify this information. However, all existing work suffers from low accuracy, relying on relatively small models such as logistic regression and BERT due to linear time cost to the number of packages under consideration. To address these limitations, we propose the first work, a framework named VulLibGen, to explore the use of a large language model (LLM) for directly generating the names of affected packages. VulLibGen conducts supervised fine-tuning (SFT) and retrieval augmented generation (RAG) to supply domain knowledge to the LLM, and a local search technique to ensure that the generated name of an affected package is among the names of the packages under consideration. Our evaluation results show that VulLibGen has an average accuracy of 0.806 for identifying vulnerable packages in the four most popular ecosystems in GitHub Advisory (Java, JS, Python, Go) while the best SOTA ranking approaches achieve only 0.721. Additionally, VulLibGen has provided high value to security practice: we have submitted 28 pairs of <vulnerability, affected package> to GitHub Advisory, and 22 of them have been accepted and merged",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QAPSha9FGDL": {
    "title": "Absorbing Commonsense Knowledge from LLMs for Improving Social Fairness in Pre-trained Language Models",
    "volume": "review",
    "abstract": "Pre-trained Language models (PLMs) are trained on inherently socially biased sources, inevitably causing undesirable application impacts. Current debiasing paradigm involves identifying bias from external corpora, which have limited quality, diversity, or equivalence among different groups, potentially impacting bias location and debiasing effectiveness. In light of this, we advance fairness in PLMs by absorbing coherent, balanced, and semantically informative social \\underline{Commonsense \\underline{K}nowledge (CK-Debias) automatically generated from large language models (LLMs). Our study addresses the demographic CK generation from LLM and explores strategies to optimize CK utilization. This is achieved by employing causal analysis to align knowledge for estimating bias space and identifying the most biased prompts to enhance bias avoidance capability. Experiment results on public datasets and intrinsic and extrinsic metrics show that \\M~can significantly reduce multiple social biases across various PLMs while keeping their language expressiveness intact",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SucQt9LzUof": {
    "title": "Rethinking the Roles of Large Language Models in Chinese Grammatical Error Correction",
    "volume": "review",
    "abstract": "Recently, Large Language Models (LLMs) have been widely studied by researchers for their roles in various downstream NLP tasks. As a fundamental task in the NLP field, Chinese Grammatical Error Correction (CGEC) aims to correct all potential grammatical errors in the input sentences. Previous studies have shown that LLMs' performance as correctors on CGEC remains unsatisfactory due to its challenging task focus. To promote the CGEC field to better adapt to the era of LLMs, we rethink the roles of LLMs in the CGEC task so that they can be better utilized and explored in CGEC. Considering the rich grammatical knowledge stored in LLMs and their powerful semantic understanding capabilities, we utilize LLMs as explainers to provide explanation information for the CGEC small models during error correction to enhance performance. We also use LLMs as evaluators to bring more reasonable CGEC evaluations, thus alleviating the troubles caused by the subjectivity of the CGEC task. In particular, our work is also an active exploration of how LLMs and small models better collaborate in downstream tasks. Extensive experiments and detailed analyses on widely used datasets verify the effectiveness of our thinking intuition and the proposed methods",
    "checked": true,
    "id": "b1fe92f5992b141a3587fa5c180997d4b59d5811",
    "semantic_title": "rethinking the roles of large language models in chinese grammatical error correction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=28hCL5s_cSj": {
    "title": "Mistake-assisted Distillation: Enhancing Student's CoT Capabilities by Identifying Key Reasoning Steps",
    "volume": "review",
    "abstract": "With the scaling up of model parameters, powerful reasoning capabilities have emerged in Large Language Models (LLMs). However, resource constraints in practical applications pose challenges to the deployment of such models, which prompted a lot of attention to distilling the capabilities into smaller, compact language models. Prior distillation works simply fine-tune student models on Chain-of-Thoughts (CoTs) data generated by teacher LLMs, resulting in the student merely imitating the teacher's reasoning style without capturing the key in reasoning. In this paper, we propose a novel distillation method called \\textbf{Mis}take-\\textbf{A}ss\\textbf{i}sted \\textbf{D}istillation (MisAiD) to help students identify the key reasoning steps and learn the thinking way in reasoning. Specifically, we first retain all CoT data annotated by teacher LLMs, irrespective of correctness. Then, we design specific prompts to rectify teachers' wrong CoTs and mistake the correct CoTs, respectively, forming the dual CoTs data that have similar reasoning steps but divergent conclusions. Finally, we identify the key reasoning steps in dual CoTs and employ a fine-grained loss function to guide student learning. Extensive experiments and comprehensive analyses demonstrate the effectiveness of MisAiD on both in-domain and out-of-domain benchmark reasoning datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1yc_pfxOdmY": {
    "title": "MP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs",
    "volume": "review",
    "abstract": "Despite advancements in on-topic dialogue systems, effectively managing topic shifts within dialogues remains a persistent challenge, largely attributed to the limited availability of training datasets. To address this issue, we propose Multi-Passage to Dialogue (MP2D), a data generation framework that automatically creates conversational question-answering datasets with natural topic transitions. By leveraging the relationships between entities in a knowledge graph, MP2D maps the flow of topics within a dialogue, effectively mirroring the dynamics of human conversation. It retrieves relevant passages corresponding to the topics and transforms them into dialogues through the passage-to-dialogue method. Through quantitative and qualitative experiments, we demonstrate MP2D's efficacy in generating dialogue with natural topic shifts. Furthermore, this study introduces a novel benchmark for topic shift dialogues, termed TS-WikiDialog. Utilizing the dataset, we demonstrate that even Large Language Models (LLMs) struggle to handle topic shifts in dialogue effectively, and we showcase the performance improvements of models trained on datasets generated by MP2D across diverse topic shift dialogue tasks",
    "checked": true,
    "id": "6da55d9743eaf091750bcbbecaec6cd62ba8824b",
    "semantic_title": "mp2d: an automated topic shift dialogue generation framework leveraging knowledge graphs",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tj2nckSejZV": {
    "title": "Basic Reading Distillation",
    "volume": "review",
    "abstract": "Large language models (LLMs) have demonstrated remarkable abilities in various natural language processing areas, but they demand high computation resources which limits their deployment in realworld. Distillation is one technique to solve this problem through either knowledge distillation or task distillation. Both distillation approaches train small models to imitate specific features of LLMs, but they all neglect basic reading education for small models on generic texts that are unrelated to downstream tasks. In this paper, we propose basic reading distillation (BRD) which educates a small model to imitate LLMs basic reading behaviors, such as named entity recognition, question raising and answering, on each sentence. After such basic education, we apply the small model on various tasks including language inference benchmarks and BIG-Bench-Hard tasks. It shows that the small model can outperform or perform comparable to over 20x bigger LLMs. Probing analysis reveals that our small model gains strengthened abilities layerwisely, leading to better performances across various tasks",
    "checked": false,
    "id": "929569c161813cd33e537afe80cdf864a1d08914",
    "semantic_title": "new materials and phenomena in membrane distillation",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=15gWQKbv56A": {
    "title": "STSPL-SSC: Semi-Supervised Few-Shot Short Text Clustering with Semantic text similarity Optimized Pseudo-Labels",
    "volume": "review",
    "abstract": "This study introduces the Semantic Textual Similarity Pseudo-Label Semi-Supervised Clustering (STSPL-SSC) framework. The STSPL-SSC framework is designed to tackle the prevalent issue of scarce labeled data by combining a Semantic Textual Similarity Pseudo-Label Generation process with a Robust Contrastive Learning module. The process begins with employing k-means clustering on embeddings for initial pseudo-Label allocation. Then we use a Semantic Text Similarity-enhanced module to supervise the secondary clustering of pseudo-labels using labeled data to better align with the real clustering centers. Subsequently, an Adaptive Optimal Transport (AOT) approach fine-tunes the pseudo-labels. Finally, a Robust Contrastive Learning module is employed to foster the learning of classification and instance-level distinctions, aiding clusters to better separate. Experiments conducted on multiple real-world datasets demonstrate that with just one label per class, clustering performance can be significantly improved, outperforming state-of-the-art models with an increase of 1-6% in both accuracy and normalized mutual information, approaching the results of fully-labeled classification",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ka1W6kOnbS0": {
    "title": "From Information to Insight: Leveraging LLMs for Open Aspect-Based Educational Summarization",
    "volume": "review",
    "abstract": "This paper addresses the challenge of aspect-based summarization in education by introducing Reflective ASPect-based summarization (ReflectASP), a dataset that summarizes student reflections on STEM lectures. Despite the promising performance of large language models in general summarization, their application to nuanced, aspect-specific summaries in educational texts remains under-explored. ReflectASP eases the exploration of open-aspect-based summarization (OABS), overcoming the limitations of current datasets and annotation complexities. We leverage GPT-4 for generating reference summaries and propose a self-refine framework to enhance summary quality. Our work benchmarks the capabilities of different language models in this novel context, contributing a unique dataset and insights into effective summarization strategies for educational content. We will make our model, dataset, and all human evaluation results available at {url annonymized_for_review}",
    "checked": false,
    "id": "6fee80fb0dd8a566c09357fcb015e60ce92a6673",
    "semantic_title": "leveraging the power of llms: a fine-tuning approach for high-quality aspect-based summarization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4MSkoJ-okmJ": {
    "title": "RePALM: Popular Quote Tweet Generation via Auto-Response Augmentation",
    "volume": "review",
    "abstract": "A quote tweet enables users to share others' content while adding their own commentary. In order to enhance public engagement through quote tweets, we investigate the task of generating popular quote tweets. This task aims to produce quote tweets that garner higher popularity, as indicated by increased likes, replies, and retweets. Despite the impressive language generation capabilities of large language models (LLMs), there has been limited research on how LLMs can effectively learn the popularity of text to better engage the public. Therefore, we introduce a novel approach called Response-augmented Popularity-Aligned Language Model (RePALM), which aligns language generation with popularity by leveraging insights from augmented auto-responses provided by readers. We utilize the Proximal Policy Optimization framework with a dual-reward mechanism to jointly optimize for the popularity of the quote tweet and its consistency with the auto-responses. In our experiments, we collected two datasets consisting of quote tweets containing external links and those referencing others' tweets. Extensive results demonstrate the superiority of RePALM over advanced language models that do not incorporate response augmentation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WiwxKXJvsIg": {
    "title": "ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget",
    "volume": "review",
    "abstract": "Entity Linking (EL) and Relation Extraction (RE) are fundamental tasks in Natural Language Processing, serving as critical components in a wide range of applications.In this paper, we propose ReLiK, a Retriever-Reader architecture for both EL and RE, where, given an input text, the Retriever module undertakes the identification of candidate entities or relations that could potentially appear within the text. Subsequently, the Reader module is tasked to discern the pertinent retrieved entities or relations and establish their alignment with the corresponding textual spans.Notably, we put forward an innovative input representation that incorporates the candidate entities or relations alongside the text, making it possible to link entities or extract relations in a single forward pass and to fully leverage pre-trained language models contextualization capabilities, in contrast with previous Retriever-Reader-based methods, which necessitate a forward pass for each candidate.Our formulation of EL and RE achieves state-of-the-art performance in both in-domain and out-of-domain benchmarks while using academic budget training and with up to 40x inference speed with respect to other competitors.Finally, we show how our architecture can be seamlessly used for Information Extraction (cIE), i.e. EL + RE, and setting a new state of the art by employing a shared Reader that simultaneously extracts entities and relations",
    "checked": true,
    "id": "9681d495ad4b6538f9949a24b11bea84caffaf4e",
    "semantic_title": "relik: retrieve and link, fast and accurate entity linking and relation extraction on an academic budget",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s-4Gp3mEb38": {
    "title": "Mitigating Catastrophic Forgetting in Multi-domain Chinese Spelling Correction by Multi-stage Knowledge Transfer Framework",
    "volume": "review",
    "abstract": "Chinese Spelling Correction (CSC) aims to detect and correct spelling errors in given sentences. Recently, multi-domain CSC has gradually attracted the attention of researchers because it is more practicable. In this paper, we focus on the key flaw of the CSC model when adapting to multi-domain scenarios: the tendency to forget previously acquired knowledge upon learning new domain-specific knowledge (i.e., $\\textbf{catastrophic forgetting}$). To address this, we propose a novel model-agnostic $\\textbf{M}$ulti-stage $\\textbf{K}$nowledge $\\textbf{T}$ransfer ($\\textbf{MKT}$) framework, which utilizes a continuously evolving teacher model for knowledge transfer in each domain, rather than focusing solely on new domain knowledge. It deserves to be mentioned that we are the first to apply continual learning methods to the multi-domain CSC task. Experiments prove the effectiveness of our proposed method, and further analyses demonstrate the importance of overcoming catastrophic forgetting for improving the model performance",
    "checked": true,
    "id": "86eee1d440bda13abe6ae9f17935a676b616c5ab",
    "semantic_title": "mitigating catastrophic forgetting in multi-domain chinese spelling correction by multi-stage knowledge transfer framework",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=5y8tPhUiNJm": {
    "title": "EnrichMath: Enriching Idea and Solution Elicit Mathematical Reasoning in Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) have witnessed remarkable advancements across a spectrum of language tasks. Despite great progress, mathematical problem-solving is still a particularly formidable challenge. Previous studies have tried to address this problem by augmenting questions and found that the performance is saturated with more training data. To further enhance the complex mathematical reasoning capabilities of LLMs, we propose EnrichMath, which is fine-tuned on our EnrichMathQA dataset. EnrichMathQA is constructed by enhancing answers in MATH and GSM8K to have a leading summary and reducing the thought jumping with our proposed Enrich Reasoning Idea(ERI) and Enrich Reasoning Solution(ERS) strategies. EnrichMath achieves state-of-the-art performance among current open-source mathematical models. Our EnrichMath-70B achieves 32.5% accuracy on the MATH benchmark, outperforming MetaMath by 2.7%. Furthermore, EnrichMath-70B achieves an accuracy of 84.1% on GSM8K, which is comparable to the methods that use external calculation tools",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JoKAnlCDnyh": {
    "title": "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models",
    "volume": "review",
    "abstract": "Since large language models (LLMs) achieve significant success in recent years, the hallucination issue remains a challenge, numerous benchmarks are proposed to detect the hallucination. Nevertheless, some of these benchmarks are not naturally generated by LLMs but are intentionally induced. Also, many merely focus on the factuality hallucination while ignoring the faithfulness hallucination. Additionally, although dialogue pattern is more widely utilized in the era of LLMs, current benchmarks only concentrate on sentence-level and passage-level hallucination. In this study, we propose DiaHalu, the first dialogue-level hallucination evaluation benchmark to our knowledge. Initially, we integrate the collected topics into system prompts and facilitate a dialogue between two ChatGPT3.5. Subsequently, we manually modify the contents that do not adhere to human language conventions and then have LLMs re-generate, simulating authentic human-machine interaction scenarios. Finally, professional scholars annotate all the samples in the dataset. DiaHalu covers four common multi-turn dialogue domains and five hallucination subtypes, extended from factuality and faithfulness hallucination. Experiments through some well-known LLMs and detection methods on the dataset show that DiaHalu is a challenging benchmark, holding significant value for further research",
    "checked": true,
    "id": "83d81e31f5c32f6989d98be1133adfc08db094ce",
    "semantic_title": "diahalu: a dialogue-level hallucination evaluation benchmark for large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OlOvWkLpked": {
    "title": "Aligning Large Language Models with Human Preferences through Representation Engineering",
    "volume": "review",
    "abstract": "Aligning large language models (LLMs) with human preferences is crucial for enhancing their utility in terms of helpfulness, truthfulness, safety, harmlessness, and interestingness. Existing methods for achieving this alignment often involve employing reinforcement learning from human feedback (RLHF) to fine-tune LLMs based on human labels assessing the relative quality of model responses. Nevertheless, RLHF is susceptible to instability during fine-tuning and presents challenges in implementation. Drawing inspiration from the emerging field of representation engineering (RepE), this study aims to identify relevant representations for high-level human preferences embedded in patterns of activity within an LLM and achieve precise control of model behavior by transforming its representations. This novel approach, denoted as Representation Alignment from Human Feedback (RAHF), proves to be effective, computationally efficient, and easy to implement. Extensive experiments demonstrate the efficacy of RAHF in not only capturing but also manipulating representations to align with a broad spectrum of human preferences or values, rather than being confined to a singular concept or function (e.g. honesty or bias). RAHF's versatility in accommodating diverse human preferences shows its potential for advancing LLM performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nBR-HOmrHf8": {
    "title": "Scaling Sentence Embeddings with Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have recently garnered significant interest. With in-context learning, they achieve impressive results in various natural language tasks. However, the application of LLMs to sentence embeddings remains an area of ongoing research. In this work, we introduce a prompt-based method, PromptEOL, designed to improve LLMs performance on sentence embeddings with explicit one word limitation. We further integrate in-context learning to refine sentence embeddings. Our extensive experiments demonstrate that in-context learning allows LLMs to generate superior sentence embeddings without any fine-tuning, enabling them to perform comparably to current contrastive learning methods. We also investigate the integration of contrastive learning with PromptEOL. Notably, the 2.7B OPT model, when combined our method, surpasses the previous state-of-the-art method with 4.8B parameters. In addition, we propose a novel method based on Direct Performance Optimization (DPO) to better align the embeddings. With our methods, we successfully achieve an 86.76 Spearman correlation on STS tasks, a 1.8 improvement over the previous methods",
    "checked": true,
    "id": "f7ccf8ecd508e0b2d423169588dd1c1a82dd3b4d",
    "semantic_title": "scaling sentence embeddings with large language models",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=UJAEa-MsePL": {
    "title": "Re-Reading Improves Reasoning in Large Language Models",
    "volume": "review",
    "abstract": "To enhance the reasoning capabilities of off-the-shelf Large Language Models (LLMs), we introduce a simple, yet general and effective prompting method, RE2, i.e., \\textbf{Re}-\\textbf{Re}ading the question as input. Unlike most thought-eliciting prompting methods, such as Chain-of-Thought (CoT), which aim to elicit the reasoning process in the output, RE2 shifts the focus to the input by processing questions twice, thereby enhancing the understanding process. Consequently, RE2 demonstrates strong generality and compatibility with most thought-eliciting prompting methods, including CoT. Crucially, RE2 facilitates a \"bidirectional\" encoding in unidirectional decoder-only LLMs because the first pass could provide global information for the second pass. We begin with a preliminary empirical study as the foundation of RE2, illustrating its potential to enable \"bidirectional\" attention mechanisms. We then evaluate RE2 on extensive reasoning benchmarks across 14 datasets, spanning 112 experiments, to validate its effectiveness and generality. Our findings indicate that, with the exception of a few scenarios on vanilla ChatGPT, RE2 consistently enhances the reasoning performance of LLMs through a simple re-reading strategy. Further analyses reveal RE2's adaptability, showing how it can be effectively integrated with different LLMs, thought-eliciting prompting, and ensemble strategies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uE4PP5bveX7": {
    "title": "NeuroTrialNER: An Annotated Corpus for Neurological Diseases and Therapies in Clinical Trial Registries",
    "volume": "review",
    "abstract": "Extracting and aggregating information from clinical trial registries could provide invaluable insights into the drug development landscape and advance the treatment of neurologic diseases. However, achieving this at scale is hampered by the volume of available data and the lack of an annotated corpus to assist in the development of automation tools. Thus, we introduce NeuroTrialNER, a new and fully open corpus for named entity recognition (NER). It comprises 893 clinical trial summaries sourced from ClinicalTrials.gov, annotated for neurological diseases, interventions, and control treatments. We describe our data collection process and the corpus in detail. We demonstrate its utility for NER using large language models and achieve a close-to-human performance. By bridging the gap in data resources, we hope to foster the development of text processing applications that help researchers navigate clinical trials data more easily, efficiently, and comprehensively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OFeKQn8K2Bk": {
    "title": "Is Cognition and Action Consistent or Not: Investigating Large Language Model's Personality",
    "volume": "review",
    "abstract": "In this study, we investigate the reliability of Large Language Models (LLMs) in professing human-like personality traits through responses to personality questionnaires. Our goal is to evaluate the consistency between LLMs' professed personality inclinations and their actual \"behavior\", examining the extent to which these models can emulate human-like personality patterns. Through a comprehensive analysis of LLM outputs against established human benchmarks, we seek to understand the cognition-action divergence in LLMs and propose hypotheses for the observed results based on psychological theories and metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1SOGBr5QFBC": {
    "title": "MSPN: Multiple Semantics Perception Network for Remote Sensing Change Captioning",
    "volume": "review",
    "abstract": "Remote sensing images usually cover a large surface area, so the change information is usually difficult to be precisely localized. Especially, some changes are easy to be overlooked due to their inconspicuous locations and fuzzy shapes. In addition, unlike the natural image change description task, the remote sensing image change description task aims to capture the most significant changes without various influencing factors, such as light, seasonal influences and complex land cover. To address the above challenges, in this paper, we propose a multiple semantic perception network (MSPN) model to extract more accurate feature representations to guide the decoder in generating high-quality change descriptions. In the visual encoder stage, the global efficient semantic awareness module is designed for global feature embedding, the self-semantic awareness module digs deep into the internal connections between features, and the change semantic interaction module effectively distinguishes semantic changes from irrelevant ones. In the description generation phase, the Transformer-based decoder is designed to guide the change description generation. Extensive experiments on the LEVIR-CC dataset demonstrate the superiority of the MSPN model over many state-of-the-art techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6DiulKVxhY": {
    "title": "GeoAgent: To Empower LLMs using Geospatial Tools for Address Standardization",
    "volume": "review",
    "abstract": "This paper presents a novel solution to tackle the challenges that posed by the abundance of non-standard addresses, which input by users in modern applications such as navigation maps, ride-hailing apps, food delivery platforms, and logistics services. These manually entered addresses often contain irregularities, such as missing information, spelling errors, colloquial descriptions, and directional offsets, which hinder address-related tasks like address matching and linking. To tackle these challenges, we propose GeoAgent, a new framework comprising two main components: a large language model (LLM) and a suite of geographical tools. By harnessing the semantic understanding capabilities of the LLM and integrating specific geospatial tools, GeoAgent incorporates spatial knowledge into address texts and achieves efficient address standardization. Further, to verify the effectiveness and practicality of our approach, we construct a comprehensive dataset of complex non-standard addresses, which fills the gaps in existing datasets and proves invaluable for training and evaluating the performance of address standardization models in this community. Experimental results demonstrate the efficacy of GeoAgent, showcasing substantial improvements in the performance of address-related models across various downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CqCzgUqjPms": {
    "title": "When LLMs Meet Cunning Questions: A Fallacy Understanding Benchmark for Large Language Models",
    "volume": "review",
    "abstract": "Recently, Large Language Models (LLMs) have made remarkable evolutions in language understanding and generation. Following this, various benchmarks for measuring all kinds of capabilities of LLMs have sprung up. In this paper, we challenge the reasoning and understanding abilities of LLMs by proposing a $\\textbf{F}$a$\\textbf{L}$lacy $\\textbf{U}$nderstanding $\\textbf{B}$enchmark ($\\textbf{FLUB}$) containing cunning questions that are easy for humans to understand but difficult for models to grasp. Specifically, the cunning questions that FLUB focuses on mainly consist of the tricky, humorous, and misleading questions collected from the real internet environment. And we design three tasks with increasing difficulty in the FLUB benchmark to evaluate the fallacy understanding ability of LLMs. Based on FLUB, we investigate the performance of multiple representative and advanced LLMs, reflecting our FLUB is challenging and worthy of more future study. Interesting discoveries and valuable insights are achieved in our extensive experiments and detailed analyses. We hope that our benchmark can encourage the community to improve LLMs' ability to understand fallacies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BqttonajSN": {
    "title": "TimeArena: Shaping Efficient Multitasking Language Agents in a Time-Aware Simulation",
    "volume": "review",
    "abstract": "Despite remarkable advancements in emulating human-like behavior through Large Language Models (LLMs), current textual simulations do not adequately address the notion of time. To this end, we introduce TimeArena, a novel textual simulated environment that incorporates complex temporal dynamics and constraints that better reflect real-life planning scenarios. In TimeArena, agents are asked to complete multiple tasks as soon as possible, allowing for parallel processing to save time. We implement the dependency between actions, the time duration for each action, and the occupancy of the agent and the objects in the environment. TimeArena grounds to 30 real-world tasks in cooking, household activity, and laboratory work. We conduct extensive experiments with various LLMs using TimeArena. Our findings reveal that even the most powerful models, e.g., GPT-4, still lag behind humans in effective multitasking, underscoring the need for enhanced temporal awareness in the development of language agents",
    "checked": true,
    "id": "3246160ec123069e81ed06e3c3898c4cf7cd9ee0",
    "semantic_title": "timearena: shaping efficient multitasking language agents in a time-aware simulation",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=bClVeTj7S4": {
    "title": "Knowledge Graph Enhanced Large Language Model Editing",
    "volume": "review",
    "abstract": "Large language models (LLMs) are pivotal in advancing natural language processing (NLP) tasks, yet their efficacy is hampered by inaccuracies and outdated knowledge. Model editing emerges as a promising solution to address these challenges. However, existing editing methods struggle to track and incorporate changes in knowledge associated with edits, which limits the generalization ability of postedit LLMs in processing edited knowledge. To tackle these problems, we propose a novel model editing method that leverages knowledge graphs for enhancing LLM editing, namely GLAME. Specifically, we first utilize a knowledge graph augmentation module to uncover associated knowledge that has changed due to editing, obtaining its internal representations within LLMs. This approach allows knowledge alterations within LLMs to be reflected through an external graph structure. Subsequently, we design a graph-based knowledge edit module to integrate structured knowledge into the model editing. This ensures that the updated parameters reflect not only the modifications of the edited knowledge but also the changes in other associated knowledge resulting from the editing process. Comprehensive experiments conducted on GPT-J and GPT-2 XL demonstrate that GLAME significantly improves the generalization capabilities of post-edit LLMs in employing edited knowledge",
    "checked": true,
    "id": "9acb2df43927a2df64a4b9d39d68ab0a482d05f5",
    "semantic_title": "knowledge graph enhanced large language model editing",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=DGsGaMit3MN": {
    "title": "Enhancing Emotional Generation Capability of Large Language Models via Emotional Chain-of-Thought",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have shown remarkable performance in various emotion recognition tasks, thereby piquing the research community's curiosity for exploring their potential in emotional intelligence. However, several issues in the field of emotional generation tasks remain unresolved, including human preference alignment and emotional generation assessment. In this paper, we propose the Emotional Chain-of-Thought (ECoT), a plug-and-play prompting method that enhances the performance of LLMs on various emotional generation tasks by aligning with human emotional intelligence guidelines. To assess the reliability of ECoT, we propose an automated model-based evaluation method called Emotional Generation Score (EGS). EGS incorporates Goleman's Emotional Intelligence Theory as a consensus of human experts, providing a new perspective on the evaluation of emotional generation tasks. Extensive experimental results demonstrate the effectiveness of ECoT and EGS. Further, we discuss the promise of LLMs in the field of emotional intelligence and present key insights into the LLMs with the ECoT in emotional generation tasks",
    "checked": true,
    "id": "d13dc7f94991e4cf7fba70c340b1d9d36346f238",
    "semantic_title": "enhancing emotional generation capability of large language models via emotional chain-of-thought",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=PoXJRRDdHT": {
    "title": "Split and Merge: Aligning Position Biases in LLM-based Evaluators",
    "volume": "review",
    "abstract": "Large language models (LLMs) have shown promise as automated evaluators for assessing the quality of answers generated by AI systems. However, LLM-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content. To address this limitation, we propose PORTIA, an alignment-based systemdesigned to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner. Specifically, PORTIA splits the answers into multiple segments, taking into account both length and semantics, and merges them back into a single prompt for evaluation by LLMs. Extensive experiments with six LLMs on 11,520 answer pairs demonstrate that PORTIA markedly enhances the consistency rates for all models and forms of comparison tested, achieving an average relative improvement of 47.46\\%. It also enables GPT-3.5 to achieve performance comparable to GPT-4 and elevates GPT-4's consistency rate up to 98\\%. Subsequent human evaluations indicate that the PORTIA-enhanced GPT-3.5 model can even surpass standalone GPT-4 in terms of alignment with human evaluators, highlighting PORTIA's ability to correct position bias, improve LLM consistency, and boost performance while keeping cost efficiency",
    "checked": false,
    "id": "2e0f66e626a69da131e486da451218bbabfe7d3b",
    "semantic_title": "split and merge: aligning position biases in large language model based evaluators",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=mjXem22BY6": {
    "title": "Knowledge Verification to Nip Hallucination in the Bud",
    "volume": "review",
    "abstract": "While large language models (LLMs) have demonstrated exceptional performance across various tasks following human alignment, they may still generate responses that sound plausible but contradict factual knowledge, a phenomenon known as \\emph{hallucination}. In this paper, we demonstrate the feasibility of mitigating hallucinations by verifying and minimizing the inconsistency between external knowledge present in the alignment data and the intrinsic knowledge embedded within foundation LLMs. Specifically, we propose a novel approach called Knowledge Consistent Alignment (KCA), which employs a well-aligned LLM to automatically formulate assessments based on external knowledge to evaluate the knowledge boundaries of foundation LLMs. To address knowledge inconsistencies in the alignment data, KCA implements several specific strategies to deal with these data instances. We demonstrate the superior efficacy of KCA in reducing hallucinations across six benchmarks, utilizing foundation LLMs of varying backbones and scales. This confirms the effectiveness of mitigating hallucinations by reducing knowledge inconsistency. Our code, model weights, and data will be publicly accessible",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pz5uBkUyano": {
    "title": "GSAC: Improving Multi-Document Summarization with Graph Structure-Aware Encoder",
    "volume": "review",
    "abstract": "Sequence-to-sequence neural networks have achieved remarkable success in abstractive text summarization. However, current models may not be directly adaptable to the task of multi-document summarization (MDS). In this paper, we propose a neural summarization framework that can effectively process lengthy texts and multiple input documents. We propose a method to seamlessly integrate graph representations into the encoder-decoder model. Additionally, we introduce an extra training objective aimed at maximizing the similarity between the compressed graph text and the ground-truth summary at the node level. Our approach utilizes an innovative method for constructing text graphs to tackle the challenges of applying graph structures in multi-document scenarios. With a base PRIMERA model, our method shows superior performance compared to previous state-of-the-art models on the Multi-news, Multi-XScience and Wikisum datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=84z1mIvPnUp": {
    "title": "EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models",
    "volume": "review",
    "abstract": "This study introduces EventRL, a reinforcement learning approach that significantly enhances the event extraction capabilities of large language models (LLMs). EventRL addresses the challenges of instruction following and hallucination by introducing outcome supervision, which provides direct feedback on the accuracy of event extraction. The method employs specialized reward functionsâ€”Argument-F1, Average-F1, and Product-F1â€”to guide the model's training and improve its understanding of event structures. Our experiments on the ACE05 dataset, which includes both Held-in Test (for seen event types) and Held-out Test (for unseen event types), demonstrate that EventRL outperforms Supervised Fine-Tuning (SFT) and Few-Shot Prompting (FSP) (based on GPT4) methods for event extraction. The results further show that EventRL is particularly effective in handling unseen event types, and that the choice of reward function and the inclusion of code data can significantly improve event extraction performance",
    "checked": true,
    "id": "ded732209b0ba8a6704cc62ab8197a898b57f833",
    "semantic_title": "eventrl: enhancing event extraction with outcome supervision for large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=AhK9g8fSkrE": {
    "title": "Alleviating Hallucinations of Large Language Models through Induced Hallucinations",
    "volume": "review",
    "abstract": "Despite their impressive capabilities, large language models (LLMs) have been observed to generate responses that include inaccurate or fabricated information, a phenomenon commonly known as ``hallucination''. In this work, we propose a simple \\textit{Induce-then-Contrast} Decoding (ICD) strategy to alleviate hallucinations. We first construct a factually weak LLM by inducing hallucinations from the original LLMs. Then, we penalize these induced hallucinations during decoding to enhance the factuality of the generated content. Concretely, we determine the final next-token predictions by amplifying the predictions from the original model and downplaying the induced untruthful predictions via contrastive decoding. Experimental results on both discrimination-based and generation-based hallucination evaluation benchmarks, such as TruthfulQA and \\textsc{FActScore}, demonstrate that our proposed ICD methods can effectively enhance the factuality of LLMs across various model sizes and families. For example, when equipped with ICD, Llama2-7B-Chat and Mistral-7B-Instruct achieve performance comparable to ChatGPT and GPT4 on TruthfulQA, respectively",
    "checked": true,
    "id": "3f915aab835cbfe69e7b2ea1c73b74ac8a2d384e",
    "semantic_title": "alleviating hallucinations of large language models through induced hallucinations",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=wve4bdsQcXj": {
    "title": "Chain of History: Learning and Forecasting with LLMs for Temporal Knowledge Graph Completion",
    "volume": "review",
    "abstract": "Temporal Knowledge Graph Completion (TKGC) is a complex task involving the prediction of missing event links at future timestamps by leveraging established temporal structural knowledge. This paper aims to provide a comprehensive perspective on harnessing the advantages of Large Language Models (LLMs) for reasoning in temporal knowledge graphs, presenting an easily transferable pipeline. In terms of graph modality, we underscore the LLM's prowess in discerning the structural information of pivotal nodes within the historical chain. As for the generation mode of the LLMs utilized for inference, we conduct an exhaustive exploration into the variances induced by a range of inherent factors in LLMs, with particular attention to the challenges in comprehending reverse logic. We adopt a parameter-efficient fine-tuning strategy to harmonize the LLMs with the task requirements, facilitating the learning of the key knowledge highlighted earlier. Comprehensive experiments are undertaken on several widely recognized datasets, revealing that our framework exceeds or parallels existing methods across numerous popular metrics. Additionally, we execute a substantial range of ablation experiments and draw comparisons with several advanced commercial LLMs, to investigate the crucial factors influencing LLMs' performance in structured temporal knowledge inference tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tQsdOqoYiuB": {
    "title": "Unveiling Imitation Learning: Exploring the impact of Data Falsity to Large Language Model",
    "volume": "review",
    "abstract": "Many recent studies endeavor to improve open-sourced language models through imitation learning, re-training on the synthetic instruction data from state-of-the-art proprietary models like ChatGPT and GPT-4.However, the innate nature of synthetic data inherently contains noisy data, giving rise to a substantial presence of low-quality data replete with misleading queries, erroneous responses, and flawed reasoning.Although we intuitively grasp the potential harm of noisy data, we lack a quantitative understanding of its impact.To this end, this paper explores correlation between the degree of noise and its impact on language models through instruction tuning.We first introduce the Falsity-Controllable (\\noco) dataset, which comprises pairs of true answers and corresponding reasoning, as well as false pairs to manually control the factuality ratio of the dataset.Through our extensive experiments, we found multiple intriguing findings of the correlation between factuality and instruction tuning. Specifically, factuality can significantly impact various benchmark characteristics especially when benchmarks are related to knowledge domain, and initial data quality plays a critical role, whereas the number of learning steps has a lesser impact.Additionally, we noted that once the language model is trained with a dataset contaminated by noise, restoring its original performance becomes exceptionally challenging, verging on irreversible",
    "checked": true,
    "id": "d536e9f2219a882306f0fe0415bc758223e3bdfd",
    "semantic_title": "unveiling imitation learning: exploring the impact of data falsity to large language model",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X2WREjKT2I": {
    "title": "How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts",
    "volume": "review",
    "abstract": "The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling deceptive information in prompts, thus producing hallucinated responses under such conditions. To quantitatively assess this vulnerability, we present MAD-Bench, a carefully curated benchmark that contains 850 test samples divided into 6 categories, such as non-existent objects, count of objects, spatial relationship, and visual confusion. We provide a comprehensive analysis of popular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such as LLaVA-1.5 and CogVLM. Empirically, we observe significant performance gaps between GPT-4V and other models; and previous robust instruction-tuned models, such as LRV-Instruction and LLaVA-RLHF, are not effective on this new benchmark. While GPT-4V achieves 75.02% accuracy on MAD-Bench, accuracy of any other model in our experiments ranges from 5% to 35%. We further propose a remedy that adds an additional paragraph to the deceptive prompts to encourage models to think twice before answering the question. Surprisingly, this simple method can even double the accuracy; however, the absolute numbers are still too low to be satisfactory. We hope MAD-Bench can serve as a valuable benchmark to stimulate further research to enhance models' resilience against deceptive prompts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=owB1ANajN1": {
    "title": "Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning",
    "volume": "review",
    "abstract": "Instruction tuning for large language models (LLMs) can drive them to produce results consistent with human goals in specific downstream tasks. However, the process of continual instruction tuning (CIT) for LLMs may bring about the catastrophic forgetting (CF) problem, where previously learned abilities are degraded. Recent methods try to alleviate the CF problem by modifying models or replaying data, which may only remember the surface-level pattern of instructions and get confused on held-out tasks. In this paper, we propose a novel continual instruction tuning method based on Key-part Information Gain (KPIG). Our method computes the information gain on masked parts to dynamically replay data and refine the training objective, which enables LLMs to capture task-aware information relevant to the correct response and alleviate overfitting to general descriptions in instructions. In addition, we propose two metrics, P-score and V-score, to measure the generalization and instruction-following abilities of LLMs. Experiments demonstrate our method achieves superior performance on both seen and held-out tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=djCR9CNv9jj": {
    "title": "Self-DC: When to retrieve and When to generate? Self Divide-and-Conquer for Compositional Unknown Questions",
    "volume": "review",
    "abstract": "Retrieve-then-read and generate-then-read are two typical solutions to handle unknown and known questions in open-domain question-answering, while the former retrieves necessary external knowledge and the later prompt the large language models to generate internal known knowledge encoded in the parameters. However, few of previous works consider the compositional unknown questions, which consist of several known or unknown sub-questions. Thus, simple binary classification (known or unknown) becomes sub-optimal and inefficient since it will call external retrieval excessively for each compositional unknown question. To this end, we propose the first Compositional unknown Question-Answering dataset (CuQA), and introduce a Self Divide-and-Conquer (Self-DC) framework to empower LLMs to adaptively call different methods on-demand, resulting in better performance and efficiency. Experimental results on two datasets (CuQA and FreshQA) demonstrate that Self-DC can achieve comparable or even better performance with much more less retrieval times compared with several strong baselines",
    "checked": true,
    "id": "1539bb565a86c764aae690b77a6a2e78f476ae4f",
    "semantic_title": "self-dc: when to retrieve and when to generate? self divide-and-conquer for compositional unknown questions",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=57hJvTxaZ2g": {
    "title": "A Simple Framework to Accelerate Multilingual Language Model for Monolingual Text Generation",
    "volume": "review",
    "abstract": "Recent advancements in large language models (LLMs) have remarkably enhanced performances on a variety of tasks in multiple languages. However, tokenizers in LLMs trained primarily on English-centric corpora often overly fragment a text into character or Unicode-level tokens in non-Roman alphabetic languages, leading to inefficient text generation. We introduce a simple yet effective framework to accelerate text generation in such languages. Our approach involves employing a new language model head with a vocabulary set tailored to a specific target language for a pre-trained LLM. This is followed by fine-tuning the new head while incorporating a verification step to ensure the model's performance is preserved. We show that this targeted fine-tuning, while freezing other model parameters, effectively reduces token fragmentation for the target language. Our extensive experiments demonstrate that the proposed framework increases the generation speed by a factor of 1.7 while maintaining the performance of pre-trained multilingual models on target monolingual tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=angtV8KPO7L": {
    "title": "Text2Model: Text-based Model Induction for Zero-shot Image Classification",
    "volume": "review",
    "abstract": "We address the challenge of building task-agnostic classifiers using only text descriptions, demonstrating a unified approach to image classification, 3D point cloud classification, and action recognition from scenes.Unlike approaches that learn a fixed representation of the output classes, we generate at inference time a model tailored to a query classification task. To generate task-based zero-shot classifiers, we train a hypernetwork that receives class descriptions and outputs a multi-class model. The hypernetwork is designed to be equivariant with respect to the set of descriptions and the classification layer, thus obeying the symmetries of the problem and improving generalization. Our approach generates non-linear classifiers and can handle rich textual descriptions. We evaluate this approach in a series of zero-shot classification tasks, for image, point-cloud, and action recognition, using a range of text descriptions: From single words to rich descriptions. Our results demonstrate strong improvements over previous approaches, showing that zero-shot learning can be applied with little training data.Furthermore, we conduct an analysis with foundational vision and language models, demonstrating that they struggle to generalize when describing what attributes the class lacks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ljn5cQ-1MkD": {
    "title": "RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models",
    "volume": "review",
    "abstract": "Retrieval-augmented generation (RAG) has become a main technique for alleviating hallucinations in large language models (LLMs). Despite the integration of RAG, LLMs may still present unsupported or contradictory claims to the retrieved contents. In order to develop effective hallucination prevention strategies under RAG, it is important to create benchmark datasets that can measure the extent of hallucination. This paper presents RAGTruth, a corpus tailored for analyzing word-level hallucinations in various domains and tasks within the standard RAG frameworks for LLM applications. RAGTruth comprises nearly 18,000 naturally generated responses from diverse LLMs using RAG. These responses have undergone meticulous manual annotations at both the individual case and word levels, incorporating evaluations of hallucination intensity. We not only benchmark hallucination frequencies across different LLMs, but also critically assess the effectiveness of several existing hallucination detection methodologies. We show that using a high-quality dataset such as RAGTruth, it is possible to finetune a relatively small LLM and achieve a competitive hallucination detection performance when compared to the existing prompt-based approaches using state-of-the-art LLMs such as GPT-4. Furthermore, the finetuned model can effectively mitigate hallucination in LLM responses",
    "checked": true,
    "id": "cfce709a65f90312d2bdc1a6cf0380c19becf694",
    "semantic_title": "ragtruth: a hallucination corpus for developing trustworthy retrieval-augmented language models",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=8aAEfAkYZ9": {
    "title": "DPP: A parameter-efficient Cross-style Transfer method with a Dual-level Prompt Pool",
    "volume": "review",
    "abstract": "Text style transfer is a crucial area in natural language processing. Previous research primarily focused on simple styles such as emotion and formality, while the valuable cross-style transfer has been overlooked. This paper introduces a novel framework named DPP, aimed at exploring the potential of cross-style transfer. DPP is a prompt-based transfer learning framework designed to capture style features through dual-level prompts. Utilizing an adaptive attention mechanism, DPP fuses learned role-level prompts for transfer learning. Additionally, the cross-style transfer task faces several challenges, including the scarcity of training corpora and the limitations of evaluation metrics. We introduce a pipeline using Large Language Models (LLMs) to generate corpora in cross-style, alleviating the issue of data scarcity. To overcome the difficulties in automatic evaluation, we propose a set of evaluation methods based on LLMs. Experimental results show that DPP outperforms existing strong benchmarks in terms of comprehensive performance in style accuracy and content preservation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tGkPUHEqyod": {
    "title": "Fine-Tuning Pre-Trained Language Models with Gaze Supervision",
    "volume": "review",
    "abstract": "Human gaze data provide cognitive information that reflect human language comprehension and has been effectively integrated into a variety of natural language processing (NLP) tasks, demonstrating improved performance over corresponding plain text-based models. In this work, we propose to integrate a gaze module into pre-trained language models (PLMs) at the fine-tuning stage to improve their capabilities to learn representations that are grounded in human language processing. This is done by extending the conventional purely text-based fine-tuning objective with an auxiliary loss to exploit cognitive signals. The gaze module is only included during training, retaining compatibility with existing PLM-based pipelines. We evaluate the proposed approach using two distinct PLMs on the GLUE benchmark and observe that the proposed model improves performance compared to both standard fine-tuning and traditional text augmentation baselines. All code is available on \\url{anonymous_git}",
    "checked": false,
    "id": "045ccef36eabb0230428a343558e773dc6423491",
    "semantic_title": "fine-tuning pre-trained language models with noise stability regularization",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=W4okPX9fRID": {
    "title": "Large Language Models are Parallel Multilingual Learners",
    "volume": "review",
    "abstract": "In this study, we reveal an in-context learning (ICL) capability of multilingual large language models (LLMs): by translating the input to several languages, we provide $\\textbf{P}$arallel $\\textbf{I}$nput in $\\textbf{M}$ultiple Languages ($\\textbf{PIM}$) to LLMs, which significantly enhances their comprehension abilities. To test this capability, we design extensive experiments encompassing 8 typical datasets, 7 languages and 8 state-of-the-art multilingual LLMs. Experimental results show that (1) incorporating more languages help $\\textbf{PIM}$ surpass the conventional ICL further; (2) even combining with the translations that are inferior to baseline performance can also help. Moreover, by examining the activated neurons in LLMs, we discover a counterintuitive but interesting phenomenon. Contrary to the common thought that \\textsc{PIM} would activate more neurons than monolingual input to leverage knowledge learned from diverse languages, $\\textbf{PIM}$ actually inhibits neurons and promotes more precise neuron activation especially when more languages are added. This phenomenon aligns with the neuroscience insight about synaptic pruning, which removes less used neural connections, strengthens remainders, and then enhances brain intelligence",
    "checked": true,
    "id": "56318a5ded349c27dbb1358ce1a19a013ec56724",
    "semantic_title": "large language models are parallel multilingual learners",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=bOxGAShjSim": {
    "title": "Self-Augmentation via Self-Reweighting: Unlocking Intrinsic Potential of Language Models for Cross-Encoded Conditional Semantic Textual Similarity Measurement",
    "volume": "review",
    "abstract": "Conditional Semantic Textual Similarity (C-STS) introduces specific limiting conditions to the traditional Semantic Textual Similarity (STS) task, posing challenges for various mainstream models. Language models employing cross-encoding demonstrate satisfactory performance in STS, yet their effectiveness significantly diminishes in C-STS. In this work, we argue that the failure of cross-encoding language models in C-STS is not due to their inability to extract effective features, but rather because they extract an excessive number of features, thereby diluting the impact of condition-relevant features. To alleviate this, we propose Self-Augmentation via Self-Reweighting, which does not require the introduction of any external auxiliary information. Instead, it amplifies the impact of condition-relevant features and suppresses condition-irrelevant features through model's intrinsic information. The self-reweighted outputs are used as a self-augmentation signal to enhance the model's original outputs. On the C-STS test set, our proposed method consistently improves the performance of all fine-tuning baseline models (up to around 3 points). Remarkably, it even enables models with smaller parameter scales to surpass the performance of zero-shot and few-shot prompted large language models (such as GPT-4) with substantially larger parameter scales",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y4It3Ct6oY": {
    "title": "Hybrid Alignment Training for Large Language Models",
    "volume": "review",
    "abstract": "Alignment training is crucial for enabling large language models (LLMs) to cater to human intentions and preferences. It is typically performed based on two stages with different objectives: instruction-following alignment and human-preference alignment. However, aligning LLMs with these objectives in sequence suffers from an inherent problem: the objectives may conflict, and the LLMs cannot guarantee to simultaneously align with the instructions and human preferences well. To response to these, in this work, we propose a Hybrid Alignment Training (HBAT) approach, based on alternating alignment and modified elastic weight consolidation methods. The basic idea is to alternate between different objectives during alignment training, so that better collaboration can be achieved between the two alignment tasks. We experiment with HBAT on summarization and dialogue tasks. Experimental results show that HBAT can outperform all baselines. Notably, HBAT yields consistent performance gains over the traditional two-stage alignment training when using both proximal policy optimization and direct preference optimization",
    "checked": true,
    "id": "74f1b67fa18bc9c4033a9e8fd4e11ed38b178a37",
    "semantic_title": "hybrid alignment training for large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BBYIAwFzzhb": {
    "title": "AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators",
    "volume": "review",
    "abstract": "With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist experts in annotating factual claims and training high-quality classifiers, and can work with or without expert supervision. Our analyses also result in PoliClaim, a comprehensive claim detection dataset spanning diverse political topics",
    "checked": true,
    "id": "0e314ddbf28514d92f2405b73941242c162ae0ba",
    "semantic_title": "afacta: assisting the annotation of factual claim detection with reliable llm annotators",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=oFbZBPWq23J": {
    "title": "Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space",
    "volume": "review",
    "abstract": "Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks. Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios. In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis. Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping. To alleviate this dilemma, we propose \\textbf{Mu}lti-\\textbf{Sc}a\\textbf{le} \\textbf{Lo}w-\\textbf{R}ank \\textbf{A}daptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters. Through downscaling in the frequency space, MuScleLoRA encourages the model to prioritize the learning of relatively high-frequency clean mapping, consequently mitigating backdoor learning. Experimental results demonstrate that MuScleLoRA outperforms baselines significantly. Notably, MuScleLoRA reduces the average success rate of diverse backdoor attacks to below 15\\% across multiple datasets and generalizes to various backbone LMs, including BERT, RoBERTa, and Llama2. The codes are publicly available at Anonymous",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=US2NcVBrwy": {
    "title": "Distantly-Supervised Named Entity Recognition with Uncertainty-Aware Teacher Learning and Student-Student Collaborative Learning",
    "volume": "review",
    "abstract": "Distantly-Supervised Named Entity Recognition (DS-NER) effectively alleviates the burden of annotation, but meanwhile suffers from the label noise. Recent works attempt to adopt the teacher-student framework to gradually refine the training labels and improve the overall robustness. However, we argue that these teacher-student methods achieve limited performance because the poor calibration of the teacher network produces incorrectly pseudo-labeled samples, leading to error propagation. Therefore, we attempt to mitigate this issue by proposing: (1) Uncertainty-Aware Teacher Learning that leverages the prediction uncertainty to reduce the number of incorrect pseudo labels in the self-training stage; (2) Student-Student Collaborative Learning that allows the transfer of reliable labels between two student networks instead of indiscriminately relying on all pseudo labels from its teacher. This approach further enables a full exploration of mislabeled samples rather than simply filtering unreliable pseudo-labeled samples. We evaluate our proposed method on five DS-NER datasets, demonstrating that our method is superior to the state-of-the-art DS-NER denoising methods",
    "checked": false,
    "id": "b284969315b0ff055a1225cfe27f5b3021ce6f98",
    "semantic_title": "improving the robustness of distantly-supervised named entity recognition via uncertainty-aware teacher learning and student-student collaborative learning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=BnccBapcs24": {
    "title": "A Template Is All You Meme",
    "volume": "review",
    "abstract": "A templatic meme possesses a base semantics that can be tailored by whomever posts it on social media. Machine learning systems that treat memes as just images with text struggle to be performant, which is likely due to such systems having insufficient context. There can be more to memes than the obvious image and text. To aid understanding of memes, we release a knowledge base of memes, composed of more than 5,200 meme templates, detailed information about each one, and 54,000 examples of template instances (templatic memes). To demonstrate the semantic signal of meme templates, we formulate a majority-based, non-parametric classifier that leverages our knowledge base. Our method outperforms more expensive techniques but exposes an underlying issue with meme datasets, where template information is leaked from the training data and models can exploit this knowledge in a way we may not want them to. To control the impact of this template awareness, we reorganize datasets to account for the influence of meme templates. Our re-split datasets discourage undesirable shortcuts to meme understanding, resulting in increased model robustness. This work sets the state-of-the-art for five of the six tasks that we consider",
    "checked": true,
    "id": "5d269d0548777b20f8d249490c9178c8974630dd",
    "semantic_title": "a template is all you meme",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=R1Ligvqxpg": {
    "title": "Uni-Dubbing: Zero-Shot Speech Synthesis from Visual Articulation",
    "volume": "review",
    "abstract": "In the field of speech synthesis, there is a growing emphasis on employing multimodal speech to enhance robustness. A key challenge in this area is the scarcity of datasets that pair audio with corresponding video. We employ a methodology that incorporates modality alignment during the pre-training phase on multimodal datasets, uniquely facilitating zero-shot generalization through the process of freezing the video modality feature extraction component and the encoder module within the pretrained weights, thereby enabling effective cross-modal and cross-lingual transfer. We have named this method `Uni-Dubbing'. Our method finely tunes with both multimodal and single-modality audio data. In multimodal scenarios, it achieves a reduced word error rate (WER) of 31.73\\%, surpassing the previous best of 33.9\\%. It also excels in metrics like tone quality and synchronization. With single-modality audio, it achieves a WER of 36.08\\%, demonstrating adaptability to limited data. Its domain generalization capabilities are proven across various language tasks in video translation and audio generation. Trained on 433 hours of audio data, it surpasses techniques using 200 hours of audiovisual data. The code and demo are available at \\href{https://diracer.github.io/unidubbing}{https://diracer.github.io/unidubbing}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dbBbtapDjbU": {
    "title": "Private Language Models via Truncated Laplacian Mechanism",
    "volume": "review",
    "abstract": "Recently it has been shown that deep learning models for NLP tasks are prone to attacks that can even reconstruct the verbatim training texts. To prevent privacy leakage, researchers have investigated word-level perturbations, relying on the formal guarantees of differential privacy (DP) in the embedding space. However, many existing approaches either achieve unsatisfactory performance in the high privacy regime when using the Laplacian or Gaussian mechanism, or resort to weaker relaxations of DP that are inferior to the canonical DP in terms of privacy strength. This raises the question of whether a new method for private word embedding can be designed to overcome these limitations. In this paper, we propose a novel private embedding method called the high dimensional truncated Laplacian mechanism. Specifically, we introduce a non-trivial extension of the truncated Laplacian mechanism, which was previously only investigated in one-dimensional space cases. Theoretically, we show that our method has a lower variance compared to the previous private word embedding methods. To further validate its effectiveness, we conduct comprehensive experiments on private embedding and downstream tasks using three datasets. Remarkably, even in the high privacy regime, our approach only incurs a slight decrease in utility compared to the non-private scenario",
    "checked": false,
    "id": "c3bacc93d8c0c6ed31763d0c0fc7951f0ab82ed8",
    "semantic_title": "differentially private low-rank adaptation of large language model using federated learning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=tSfZNBvE-t": {
    "title": "Evaluating and Mitigating Number Hallucinations in Large Vision-Language Models: A Consistency Perspective",
    "volume": "review",
    "abstract": "Large vision language models have demonstrated remarkable efficacy in addressing challenges related to both textual and visual content. Nevertheless, akin to their linguistic counterparts, these models are susceptible to various hallucinations. In this paper, we focus on a new form of hallucination, specifically termed as number hallucination, which denotes instances where models fail to accurately identify the quantity of objects in an image. We establish a dataset and employ evaluation metrics to assess number hallucination, revealing a pronounced prevalence of this issue across mainstream large vision language models (LVLMs). Additionally, we delve into a thorough analysis of number hallucination, examining inner and outer inconsistency problem from two related perspectives. We assert that this inconsistency is one cause of number hallucination and propose a consistency training method as a means to alleviate such hallucination",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n_oWm6JXWYa": {
    "title": "CLOMO: Counterfactual Logical Modification with Large Language Models",
    "volume": "review",
    "abstract": "In this study, we delve into the realm of counterfactual reasoning capabilities of large language models (LLMs). Our primary objective is to cultivate the counterfactual thought processes within LLMs and rigorously assess these processes for their validity. Specifically, we introduce a novel task, Counterfactual Logical Modification (CLOMO), and a high-quality human-annotated benchmark. In this task, LLMs must adeptly alter a given argumentative text to uphold a predetermined logical relationship. To effectively evaluate a generation model's counterfactual capabilities, we propose an innovative evaluation metric, the decomposed Self-Evaluation Score (SES) to directly evaluate the natural language output of LLMs instead of modeling the task as a multiple-choice problem. Analysis shows that the proposed automatic metric aligns well with human preference. Our experimental results show that while LLMs demonstrate a notable capacity for logical counterfactual thinking, there remains a discernible gap between their current abilities and human performance. We will release the data and code after publication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OTIbbJ-WUWD": {
    "title": "Evolving to the Future: Unseen Event Adaptive Fake News Detection on Social Media",
    "volume": "review",
    "abstract": "With the rapid development of social media, the wide dissemination of fake news on social media is increasingly threatening both individuals and society. In the dynamic landscape of social media, fake news detection aims to develop a model trained on news reporting past events. The objective is to predict and identify fake news about future events, which often relate to subjects entirely different from those in the past. However, existing fake detection methods exhibit a lack of robustness and cannot generalize to unseen events. To address this, we introduce Future ADaptive Event-based Fake news Detection (FADE) framework. Specifically, we train a target predictor through an adaptive augmentation strategy and graph contrastive learning to make more robust overall predictions. Simultaneously, we independently train an event-only predictor to obtain biased predictions. Then we further mitigate event bias by obtaining the final prediction by subtracting the output of the event-only predictor from the output of the target predictor. Encouraging results from experiments designed to emulate real-world social media conditions validate the effectiveness of our method in comparison to existing state-of-the-art approaches",
    "checked": true,
    "id": "360e300c5e499c42f016065ad3c3a6f22b86fcb9",
    "semantic_title": "evolving to the future: unseen event adaptive fake news detection on social media",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8XwaLBc2cb": {
    "title": "Balanced Data Sampling for Language Model Training with Clustering",
    "volume": "review",
    "abstract": "Data plays a fundamental role in the training of Large Language Models (LLMs). While attention has been paid to the collection and composition of datasets, determining the data sampling strategy in training remains an open question. Most LLMs are trained with a simple strategy, random sampling. However, this sampling strategy ignores the unbalanced nature of training data distribution, which can be sub-optimal. In this paper, we propose ClusterClip Sampling to balance the text distribution of training data for better model training. Specifically, ClusterClip Sampling utilizes data clustering to reflect the data distribution of the training set and balances the common samples and rare samples during training based on the cluster results. A repetition clip operation is introduced to mitigate the overfitting issue led by samples from certain clusters.Extensive experiments validate the effectiveness of ClusterClip Sampling, which outperforms random sampling and other cluster-based sampling variants under various training datasets and large language models",
    "checked": true,
    "id": "7b4005e0c624750252260bcac1ef67c9b804ec97",
    "semantic_title": "balanced data sampling for language model training with clustering",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=masq0lfC3s": {
    "title": "DEBATE: Devil's Advocate-Based Assessment and Text Evaluation",
    "volume": "review",
    "abstract": "As natural language generation (NLG) models have become prevalent, systematically assessing the quality of machine-generated texts has become increasingly important. Recent studies introduce LLM-based evaluators that operate as reference-free metrics, demonstrating their capability to adeptly handle novel tasks. However, these models generally rely on a single-agent approach, which, we argue, introduces an inherent limit to their performance. This is because there exist biases in LLM agent's responses, including preferences for certain text structure or content. In this work, we propose DEBATE, an NLG evaluation framework based on multi-agent scoring system augmented with a concept of Devil's Advocate. Within the framework, one agent is instructed to criticize other agents' arguments, potentially resolving the bias in LLM agent's answers. DEBATE substantially outperforms the previous state-of-the-art methods in two meta-evaluation benchmarks in NLG evaluation, SummEval and TopicalChat. We also show that the extensiveness of debates among agents and the persona of an agent can influence the performance of evaluators",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cl246qWjOJV": {
    "title": "Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for Chinese Mental Health Text Analysis",
    "volume": "review",
    "abstract": "In the current environment, psychological issues are prevalent and widespread, with social media serving as a key outlet for individuals to share their feelings. This results in the generation of vast quantities of data daily, where negative emotions have the potential to precipitate crisis situations. There is a recognized need for models capable of efficient analysis. While pre-trained language models have demonstrated their effectiveness broadly, there's a noticeable gap in pre-trained models tailored for specialized domains like psychology. To address this, we have collected a huge dataset from Chinese social media platforms and enriched it with publicly available datasets to create a comprehensive database encompassing 3.36 million text entries. To enhance the model's applicability to psychological text analysis, we integrated psychological lexicons into the pre-training masking mechanism. Building on an existing Chinese language model, we performed adaptive training to develop a model specialized for the psychological domain. We assessed our model's effectiveness across four public benchmarks, where it not only surpassed the performance of standard pre-trained models but also showed a inclination for making psychologically relevant predictions. Due to concerns regarding data privacy, the dataset will not be made publicly available. However, we have made the pre-trained models and codes publicly accessible to the community via: https://anonymous.4open.science/r/Chinese-MentalBERT-0893",
    "checked": true,
    "id": "15aad9b72ad278397a467a2f7a2576fff6557c5f",
    "semantic_title": "chinese mentalbert: domain-adaptive pre-training on social media for chinese mental health text analysis",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=dEOpksiqiQi": {
    "title": "Enhancing Depression-Diagnosis-Oriented Chat with Psychological State Tracking",
    "volume": "review",
    "abstract": "Depression-diagnosis-oriented chat aims to guide patients in self-expression to collect key symptoms for depression detection. Recent work focuses on combining task-oriented dialogue and chitchat to simulate the interview-based depression diagnosis. Whereas, these methods can not well capture the changing information, feelings, or symptoms of the patient during dialogues. Moreover, no explicit framework has been explored to guide the dialogue, which results in some useless communications that affect the experience. In this paper, we propose to integrate Psychological State Tracking (POST) within the large language model (LLM) to explicitly guide depression-diagnosis-oriented chat. Specifically, the state is adapted from a psychological theoretical model, which consists of four components, namely Stage, Information, Summary and Next. We fine-tune an LLM model to generate the dynamic psychological state, which is further used to assist response generation at each turn to simulate the psychiatrist. Experimental results on the existing benchmark show that our proposed method boosts the performance of all subtasks in depression-diagnosis-oriented chat",
    "checked": true,
    "id": "9b7f1f71185c26b7ab6c4a34c0cf67c5bceb83cf",
    "semantic_title": "enhancing depression-diagnosis-oriented chat with psychological state tracking",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tKDavZ5pnbK": {
    "title": "ValueBench: Towards Comprehensively Evaluating Value Orientations and Understanding of Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) are transforming diverse fields and gaining increasing influence as human proxies. This development underscores the urgent need for evaluating value orientations and understanding of LLMs to ensure their responsible integration into public-facing applications. This work introduces ValueBench, the first comprehensive psychometric benchmark for evaluating value orientations and understanding in LLMs. ValueBench collects data from 44 established psychometric inventories, encompassing 453 multifaceted value dimensions. We propose an evaluation pipeline grounded in realistic human-AI interactions to probe value orientations, along with novel tasks for evaluating value understanding in an open-ended value space. With extensive experiments conducted on six representative LLMs, we unveil their shared and distinctive value orientations and exhibit their ability to approximate expert conclusions in value-related extraction and generation tasks",
    "checked": true,
    "id": "d0d229d4308eeeb4b67af6c43893bbdfb7191a80",
    "semantic_title": "valuebench: towards comprehensively evaluating value orientations and understanding of large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f5DvxTHoIpK": {
    "title": "Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models",
    "volume": "review",
    "abstract": "Recent work has showcased the powerful capability of large language models (LLMs) in recalling knowledge and reasoning. However, the reliability of LLMs in combining these two capabilities into reasoning through multi-hop facts has not been widely explored. This paper systematically investigates the possibilities for LLMs to utilize shortcuts based on direct connections between the initial and terminal entities of multi-hop knowledge. We first explore the existence of factual shortcuts through Knowledge Neurons, revealing that: (i) the strength of factual shortcuts is highly correlated with the frequency of co-occurrence of initial and terminal entities in the pre-training corpora; (ii) few-shot prompting leverage more shortcuts in answering multi-hop questions compared to chain-of-thought prompting. Then, we analyze the risks posed by factual shortcuts from the perspective of multi-hop knowledge editing. Analysis shows that approximately 20% of the failures are attributed to shortcuts, and the initial and terminal entities in these failure instances usually have higher co-occurrences in the pre-training corpus. Finally, we propose erasing shortcut neurons to mitigate the associated risks and find that this approach significantly reduces failures in multiple-hop knowledge editing caused by shortcuts. Code is publicly available at Anonymous",
    "checked": true,
    "id": "2bd09677fc5d39d8ace60a704b80ecffdb2a0bfe",
    "semantic_title": "investigating multi-hop factual shortcuts in knowledge editing of large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=CunieKS5Oay": {
    "title": "KnowTuning: Knowledge-aware Fine-tuning for Large Language Models",
    "volume": "review",
    "abstract": "Despite their success at many natural language processing (NLP) tasks, large language models (LLMs) still struggle to effectively leverage knowledge for knowledge-intensive tasks, manifesting limitations such as generating incomplete, non-factual, or illogical answers. These limitations stem from inadequate knowledge awareness of LLMs during vanilla fine-tuning. To address these problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to explicitly and implicitly improve the knowledge awareness of LLMs. We devise an explicit knowledge-aware generation stage to train LLMs to explicitly identify knowledge triples in answers. We also propose an implicit knowledge-aware comparison stage to train LLMs to implicitly distinguish between reliable and unreliable knowledge, in three aspects: completeness, factuality, and logicality. Extensive experiments on both generic and medical question answering (QA) datasets confirm the effectiveness of KnowTuning, through automatic and human evaluations, across various sizes of LLMs. Finally, we demonstrate that the improvements of KnowTuning generalize to unseen QA datasets",
    "checked": true,
    "id": "08436b3ddafd2edc798753ebc87f6ceffed6e8df",
    "semantic_title": "knowtuning: knowledge-aware fine-tuning for large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h4ZiktL5X36": {
    "title": "Metacognitive Symbolic Distillation Framework for Multi-choice Machine Reading Comprehension",
    "volume": "review",
    "abstract": "Previous research often utilizes symbolic distillation to transfer the reasoning abilities of large teacher models to smaller student models. However, when it comes to multi-choice machine reading comprehension (MMRC), solely learning from the rationales generated by the teacher model for correct options overlooks educational significance of understanding the reasons behind incorrect options. In education, metacognition requires individuals to actively identify errors when reading to deepen their understanding. To this end, we propose a novel framework for achieving metacognitive symbolic distillation. Initially, we prompt the teacher large language model (LLM) to generate rationales for each option in the MMRC dataset. Subsequently, the student model could be fine-tuned based on the MMRC data equipped with these rationales. Our experiments on two MMRC datasets demonstrate that our approach effectively enhances the performance of the small model compared with standard fine-tuned models and symbolic distilled models. Moreover, when the student model is large enough, upgrading the teacher model can lead to further improvements. We will make our code and data publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OrYUWrlk8rU": {
    "title": "ITER: Iterative Transformer-based Entity Recognition and Relation Extraction",
    "volume": "review",
    "abstract": "Entity Recognition and Relation Extraction are essential components in extracting structured information from text. Recent advances for both tasks generate a structured representation of the information in an autoregressive fashion, a time-intensive and computationally expensive approach. This raises the natural question whether autoregressive methods are necessary in order to achieve comparable results. In this work, we propose ITER, a functionally more expressive, non-autoregressive model, that unifies several improvements to a recent language modeling approach: ITER improves inference throughput by up to 23x, is capable of handling nested entities and effectively halves the number of required parameters in comparison. Furthermore, we achieve a SOTA result of 84.30 F1 for the relation extraction dataset ADE and demonstrate competitive performances for both named entity recognition with GENIA and CoNLL03 as well as for relation extraction with CoNLL04 and NYT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZhYI_XXkDz9": {
    "title": "Label-Efficient Model Selection for Text Generation",
    "volume": "review",
    "abstract": "Model selection for a given target task can be costly, as it may entail extensive annotation of the quality of outputs of different models. We introduce DiffUse, an efficient method to make an informed decision between candidate text generation models based on preference annotations. DiffUse reduces the required amount of annotations, thus saving valuable time and resources in performing evaluation. DiffUse intelligently selects instances by clustering embeddings that represent the semantic differences between model outputs. Thus, it is able to identify a subset of examples that are more informative for preference decisions. Our method is model-agnostic, and can be applied to any text generation model. Moreover, we propose a practical iterative approach for dynamically determining how many instances to annotate. In a series of experiments over hundreds of model pairs, we demonstrate that DiffUse can dramatically reduce the required number of annotations -- by up to 75% -- while maintaining high evaluation reliability",
    "checked": true,
    "id": "388fae4a7298f68c8bb7b04e8aa917446058568b",
    "semantic_title": "label-efficient model selection for text generation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=QOHEN_IZzfo": {
    "title": "InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance",
    "volume": "review",
    "abstract": "With the rapid development of large language models (LLMs), they are not only used as general-purpose AI assistants but are also customized through further fine-tuning to meet the requirements of different applications. A pivotal factor in the success of LLMs is the alignment process. Current alignment methods, such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), focus on training-time alignment and are often complex and cumbersome to implement. Therefore, we develop InferAligner, a simple yet effective inference-time alignment method that utilizes cross-model guidance for harmlessness alignment. InferAligner utilizes safety steering vectors extracted from safety-aligned model to modify the activations of the target model when responding to harmful inputs, thereby guiding the target model to provide harmless responses. Experimental results show that our method can be very effectively applied to domain-specific models in finance, medicine, and mathematics, as well as to multimodal large language models (MLLMs) such as LLaVA. It significantly diminishes the Attack Success Rate (ASR) of both harmful instructions and jailbreak instructions, while maintaining almost unchanged performance in downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n7L4v5pSAA6": {
    "title": "Demonstration Augmentation for Zero-shot In-context Learning",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated an impressive capability known as In-context Learning (ICL), which enables them to acquire knowledge from textual demonstrations without the need for parameter updates. However, many studies have highlighted that the model's performance is sensitive to the choice of demonstrations, presenting a significant challenge for practical applications where we lack prior knowledge of user queries. Consequently, we need to construct an extensive demonstration pool and incorporate external databases to assist the model, leading to considerable time and financial costs. In light of this, some recent research has shifted focus towards zero-shot ICL, aiming to reduce the model's reliance on external information by leveraging their inherent generative capabilities. Despite the effectiveness of these approaches, the content generated by the model may be unreliable, and the generation process is time-consuming. To address these issues, we propose Demonstration Augmentation for In-context Learning (DAIL), which employs the model's previously predicted historical samples as demonstrations for subsequent ones. DAIL brings no additional inference cost and does not rely on the model's generative capabilities. Our experiments reveal that DAIL can significantly improve the model's performance over direct zero-shot inference and can even outperform few-shot ICL without any external information",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g-LPFWsB9qC": {
    "title": "SmellDetector: Code Smell Detection and Refactoring with Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in many tasks such as code generation and automated program repair. However, code LLMs have ignored another important task in programmers' daily development work, which is to improve the maintainability, readability, and scalability of the program. All of these characteristics are related to code smells and we study how to improve them by detecting and removing code smells. Most works on code smells still rely on using measures formulated by experts as features, but lack of use of the rich prior knowledge contained in code LLMs. In this paper, we propose SmellDetector, a comprehensive model for both code smell detection and refactoring opportunities detection in Java. We train the model with the designed prompt which contains both code smells of class-level and method-level in the same code snippet, including more than 20 types. We achieve state-of-the-art performance on the code smell detection task and change the basic paradigm of code smell detection from binary classification problem to multi-label classification. Finally, it has been verified through experiments that good code smell detection helps to detect refactoring opportunities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9kaCbk4gD8y": {
    "title": "Beyond English-Centric Machine Translation by Multilingual Instruction Tuning Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance on Machine Translation (MT) among various natural languages. However, many LLMs are English-dominant and only support some high-resource languages, they will fail on the non-English-Centric translation task. In this work, we propose a Multilingual Instruction Tuning (MIT) method to improve the LLMs on non-English-Centric translation. We design a multilingual instruction method which leverage the English sentence as reference to help LLMs understand the source sentence. In order to solve the problem of difficulty in obtaining multilingual parallel corpora of low-resource languages, we train a to-English LLM to generate English reference so that our MIT method only needs bilingual data. We experiment on BLOOM and LLaMA2 foundations and extensive experiments show that MIT outperforms the baselines and some large-scale language models like ChatGPT and Google Translate. We further demonstrate the importance of English reference in both training and inference processes",
    "checked": false,
    "id": "341103678d9def1add23ad12a28e31a985ea50cd",
    "semantic_title": "croissantllm: a truly bilingual french-english language model",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=W0YdfUayfLk": {
    "title": "PIXAR: Auto-Regressive Language Modeling in Pixel Space",
    "volume": "review",
    "abstract": "Recent work showed the possibility of building open-vocabulary large language models (LLMs) that directly operate on pixel representations. These models are implemented as autoencoders that reconstruct masked patches of rendered text.However, these pixel-based LLMs are limited to discriminative tasks (e.g., classification) and, similar to BERT, cannot be used to generate text.Therefore, they cannot be used for generative tasks such as free-form question answering. In this work, we introduce PIXAR, the first pixel-based autoregressive LLM that performs text generation. Consisting of only a decoder, PIXAR can perform free-form generative tasks while keeping the number of parameters on par with previous encoder-decoder models.Furthermore, we highlight the challenges of generating text as non-noisy images and show this is due to using a maximum likelihood objective. To overcome this problem, we propose an adversarial pretraining stage that improves the readability and accuracy of PIXAR by 8.1 on LAMBADA and 8.5 on bAbI--- making it comparable to GPT-2 on text generation tasks.This paves the way to build open-vocabulary LLMs that operate on perceptual input only and calls into question the necessity of the usual symbolic input representation, i.e., text as (sub)tokens",
    "checked": true,
    "id": "e4d31c8a94acc557bf21c8aa0936b3c3593ba123",
    "semantic_title": "pixar: auto-regressive language modeling in pixel space",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=weqZ6YOPv7C": {
    "title": "InstructEd: Soft-Instruction Tuning for Model Editing with Hops",
    "volume": "review",
    "abstract": "The task of model editing becomes popular for correcting inaccurate or outdated parametric knowledge in Large Language Models (LLMs). However, there are major limitations of state of the art (SOTA) model editing methods, including the excessive memorization issue caused by the direct editing methods, as well as the error propagation and knowledge conflict issues from the memory enhancement methods, resulting in hindering models' \\textit{portability}, e.g., the ability to transfer the new knowledge to related one-hop or multi-hop content. To address these issues, we propose the InstructEd method, the idea of which is to insert soft instructions into the attention module so as to facilitate interactions between instructions and questions and to understand and utilize new facts. Our main findings are: (i) InstructEd has achieved SOTA performance on three datasets for one-hop/multi-hop evaluation with LLaMAs and GPT2, achieving 10\\% (5\\%) improvement in one-hop (multi-hop) model editing.(ii) Different from earlier methods on editing parameters in FFN, we show that editing attention can also help. (iii) Model editing is highly related to retrieval augmented methods, which can help improve the locality of model editing while slightly decrease the editing performance with hops",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dgDrpGtC3-r": {
    "title": "IAPT: Instance-Aware Prompt Tuning for Large Language Models",
    "volume": "review",
    "abstract": "Soft prompt tuning is a widely studied parameter-efficient fine-tuning method. However, it has a clear drawback: many soft tokens must be inserted into the input sequences to guarantee downstream performance. As a result, soft prompt tuning is less considered than Low-rank adaptation (LoRA) in the large language modeling (LLM) era. In this work, we propose a novel prompt tuning method, \\underline{I}nstruction-\\underline{A}ware \\underline{P}rompt \\underline{T}uning (IAPT), that requires only four soft tokens. First, we install a parameter-efficient soft prompt generator at each Transformer layer to generate idiosyncratic soft prompts for each input instruction. The generated soft prompts can be seen as a semantic summary of the input instructions and can effectively guide the output generation. Second, the soft prompt generators are modules with a bottleneck architecture consisting of a self-attention pooling operation, two linear projections, and an activation function. Pilot experiments show that prompt generators at different Transformer layers require different activation functions. Thus, we propose to learn the idiosyncratic activation functions for prompt generators automatically with the help of rational functions. We have conducted experiments on various tasks, and the experimental results demonstrate that (a) our IAPT method can outperform the recent baselines with comparable tunable parameters. (b) Our IAPT method is more efficient than LoRA under the single-backbone multi-tenant setting.\\footnote{Codes and fine-tuned models will be open-sourced to facilitate future research. }",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L1jDrChZkZ": {
    "title": "Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering",
    "volume": "review",
    "abstract": "Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. Furthermore, we show that data quality, which can be drastically improved by proposed quality filters, matters more than quantity in improving Evidence-Based QA",
    "checked": true,
    "id": "23f83ffd91bd9d678661193c84ca85f1dc743c88",
    "semantic_title": "towards faithful and robust llm specialists for evidence-based question-answering",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=svi_9EGD6j_": {
    "title": "EX-FEVER: A Dataset for Multi-hop Explainable Fact Verification",
    "volume": "review",
    "abstract": "Fact verification aims to automatically probe the veracity of a claim based on several pieces of evidence. Existing works are always engaging in accuracy improvement, let alone explainability, a critical capability of fact verification systems.Constructing an explainable fact verification system in a complex multi-hop scenario is consistently impeded by the absence of a relevant, high-quality dataset. Previous datasets either suffer from excessive simplification or fail to incorporate essential considerations for explainability. To address this, we present EX-FEVER, a pioneering dataset for multi-hop explainable fact verification. With over 60,000 claims involving 2-hop and 3-hop reasoning, each is created by summarizing and modifying information from hyperlinked Wikipedia documents. Each instance is accompanied by a veracity label and an explanation that outlines the reasoning path supporting the veracity classification. Additionally, we demonstrate a novel baseline system on our EX-FEVER dataset, showcasing document retrieval, explanation generation, and claim verification, and validate the significance of our dataset. Furthermore, we highlight the potential of utilizing Large Language Models in the fact verification task. We hope our dataset could make a significant contribution by providing ample opportunities to explore the integration of natural language explanations in the domain of fact verification",
    "checked": true,
    "id": "89c0806acdb92b939cf461b9a7d80dc94d827976",
    "semantic_title": "ex-fever: a dataset for multi-hop explainable fact verification",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=w_L20yATD2": {
    "title": "An Eye Opener Regarding Task-Based Text Gradient Saliency",
    "volume": "review",
    "abstract": "Eye movements in reading reveal humans' cognitive processes during language understanding. As such, the time a reader's eyes dwell on a token has been utilized as a measure for the visual attention paid to that word, or the importance of that word to the reader. This study investigates the alignment of the importance attributed to input tokens by language models (LMs) on the one hand and humans, in the form of fixation durations, on the other hand. While previous research on the internal processes of LMs have employed the models' attention weights, recent studies have argued in favor of gradient-based methods. Moreover, previous approaches to interpret LMs' internals with human gaze have neglected the tasks readers performed during reading, even though psycholinguistic research underlines that reading patterns are task-dependent. We thus introduce a novel approach that employs a gradient-based saliency method designed to emulate task-specific human reading strategies to align model and human importance, and we find that task specificity plays a crucial role in this alignment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kCMxGGvGfKV": {
    "title": "GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-Trick",
    "volume": "review",
    "abstract": "Large language models (LLMs) excellently generate human-like text, but also raise concerns about misuse in fake news and academic dishonesty. Decoding-based watermark, particularly the watermark based on the GumbelMax trick (GM watermark), is a standout solution for safeguarding machine-generated texts due to its notable detectability. However, GM watermark encounters a major challenge with generation diversity, always yielding identical outputs for the same prompt, negatively impacting generation diversity and user experience. To overcome this limitation, we introduce a new type of GM watermark, the Logits-Addition watermark, as well as three variants that aim to enhance diversity, particularly the GumbelSoft watermark (i.e., the softmax variant of the Logits-Addition watermark). When assessed for detectability in high diversity settings, our Gumbelsoft demonstrates superior performance, with its AUROC score exceeding those of the two alternative variants by a margin of 0.1 to 0.3 and outperforming other decoding-based watermarking methods by a minimum of 0.1",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FN9s18gsXLp": {
    "title": "Semantic Information: A difference that makes a difference",
    "volume": "review",
    "abstract": "In this study based on an English fairytale corpus, we interpret Semantic information (SemI) in natural language as the difference of information between an informed and an uninformed system. Only an informed system contains SemI and its amount is the information difference between an informed and an uninformed system. This difference we were able to show",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EHGJKljxbPd": {
    "title": "CoTBal: Comprehensive Task Balancing for Multi-Task Visual Instruction Tuning",
    "volume": "review",
    "abstract": "Visual instruction tuning is a key training stage of large multimodal models (LMMs). Nevertheless, the common practice of indiscriminately mixing instruction-following data from various tasks may result in suboptimal overall performance due to different instruction formats and knowledge domains across tasks. To mitigate this issue, we propose a novel Comprehensive Task Balancing (CoTBal) algorithm for multi-task visual instruction tuning of LMMs. To our knowledge, this is the first work that explores multi-task optimization in visual instruction tuning. Specifically, we consider two key dimensions for task balancing: (1) Inter-Task Contribution, the phenomenon where learning one task potentially enhances the performance in other tasks, attributable to the overlapping knowledge domains, and (2) Intra-Task Difficulty, which refers to the learning difficulty within a single task. By quantifying these two dimensions with performance-based metrics, task balancing is thus enabled by assigning more weights to tasks that offer substantial contributions to others, receive minimal contributions from others, and also have great intra-task difficulties. Experiments show that our CoTBal leads to superior overall performance in multi-task visual instruction tuning",
    "checked": true,
    "id": "9a257f5a2c3ab73ec123c3980c2a29e7d8258fbe",
    "semantic_title": "cotbal: comprehensive task balancing for multi-task visual instruction tuning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qJHMh5cGufu": {
    "title": "Selene: Pioneering Automated Proof in Software Verification",
    "volume": "review",
    "abstract": "Ensuring correctness is a pivotal aspect of software engineering. Among the various strategies available, software verification offers a definitive assurance of correctness. Nevertheless, writing verification proofs is resource-intensive and manpower-consuming, and there is a great need to automate this process. We introduce Selene in this paper, which is the first project-level automated proof benchmark constructed based on the real-world industrial-level operating system microkernel, seL4. Selene provides a comprehensive framework for end-to-end proof generation and a lightweight verification environment. Our experimental results with advanced large language models (LLMs), such as GPT-3.5-turbo and GPT-4, highlight the capabilities of LLMs in the domain of automated proof generation. Additionally, our further proposed augmentations indicate that the challenges presented by Selene can be mitigated in future research endeavors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e3DpLd44DOw": {
    "title": "ProLex: A Benchmark for Language Proficiency-oriented Lexical Substitution",
    "volume": "review",
    "abstract": "Lexical Substitution discovers appropriate substitutes for a given target word in a context sentence. However, the task fails to consider substitutes that are of equal or higher proficiency than the target, an aspect that could be beneficial for language learners looking to improve their writing. To bridge this gap, we propose a new task --- language proficiency-oriented lexical substitution. We also introduce ProLex, a novel benchmark designed to assess systems' ability to generate not only appropriate substitutes but also substitutes that demonstrate better language proficiency. Besides the benchmark, we propose models that can automatically perform the new task. We show that our best model, a Llama2-13B model fine-tuned with task-specific synthetic data, outperforms ChatGPT by an average of 3.2% in F-score and achieves comparable results with GPT-4 on ProLex",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AcpUs7X1x5C": {
    "title": "SciAgent: Tool-augmented Language Models for Scientific Reasoning",
    "volume": "review",
    "abstract": "Scientific reasoning poses an excessive challenge for even the most advanced Large Language Models (LLMs). To make this task more practical and solvable for LLMs, we introduce a new task setting named tool-augmented scientific reasoning. This setting supplements LLMs with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user. To facilitate the research of such setting, we construct a tool-augmented training corpus named MathFunc which encompasses over 30,000 samples and roughly 6,000 tools. Building on MathFunc, we develop SciAgent to retrieve, understand and, if necessary, use tools for scientific problem solving. Additionally, we craft a benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs' abilities with tool assistance. Extensive experiments on SciToolBench confirm the effectiveness of SciAgent. Notably, SciAgent-Mistral-7B surpasses other LLMs with the same size by more than $13\\%$ in absolute accuracy. Furthermore, SciAgent-DeepMath-7B shows much superior performance than ChatGPT",
    "checked": true,
    "id": "60126292c0b31dfc8628d99001e057b9f8355000",
    "semantic_title": "sciagent: tool-augmented language models for scientific reasoning",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=0aHK5lZSl6Y": {
    "title": "The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants",
    "volume": "review",
    "abstract": "We present Belebele, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the FLORES-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and findings, notably that despite significant cross-lingual transfer in English-centric LLMs, much smaller MLMs pretrained on balanced multilingual data still understand far more languages. Overall, Belebele opens up new avenues for evaluating and analyzing the multilingual capabilities of NLP systems",
    "checked": true,
    "id": "fe6670cfc0d0dfe184afc8e003df51333d3a750e",
    "semantic_title": "the belebele benchmark: a parallel reading comprehension dataset in 122 language variants",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=NxXZlYCK2LO": {
    "title": "HeSum: a Novel Dataset for Abstractive Text Summarization in Hebrew",
    "volume": "review",
    "abstract": "While large language models (LLMs) excel in various natural language tasks for English, their performance in low-resource languages like Hebrew, especially for complex tasks like abstractive summarization, remains unclear. Hebrew's morphological richness adds further challenges due to ambiguity in sentence structure and word meaning. In this paper, we address this gap by introducing HeSum, a novel benchmark dataset specifically designed for Hebrew abstractive text summarization. HeSum comprises 10,000 article-summary pairs sourced from Hebrew news websites and written by professionals. Linguistic analysis confirms HeSum's high abstractness and unique morphological challenges. We show that HeSum presents distinct difficulties even for state-of-the-art LLMs, establishing it as a valuable testbed for advancing generative language in MRLs such as Hebrew",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7KTGLTk407E": {
    "title": "Detection-Correction Structure via General Language Model for Grammatical Error Correction",
    "volume": "review",
    "abstract": "Grammatical error correction (GEC) is a task dedicated to rectifying texts with minimal edits, which can be decoupled into two components: detection and correction. However, previous works have predominantly focused on direct correction, with no prior efforts to integrate both into a single model. Moreover, the exploration of the detection-correction paradigm by large language models (LLMs) remains underdeveloped. This paper introduces an integrated detection-correction structure, named DeCoGLM, based on the General Language Model (GLM). The detection phase employs a fault-tolerant detection template, while the correction phase leverages autoregressive mask infilling for localized error correction. Through the strategic organization of input tokens and modification of attention masks, we facilitate multi-task learning within a single model. Our model demonstrates competitive performance against the state-of-the-art models on English and Chinese GEC datasets. Further experiments present the effectiveness of the detection-correction structure in LLMs, suggesting a promising direction for GEC",
    "checked": true,
    "id": "9072b1e6477de4bc7c7b5f280025df50f9a7d817",
    "semantic_title": "detection-correction structure via general language model for grammatical error correction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UPG_Qihwgax": {
    "title": "Marathon: A Race Through the Realm of Long Context with Large Language Models",
    "volume": "review",
    "abstract": "With the advancement of large language models (LLMs) and the expansion of their context windows, existing long-context benchmarks fall short in effectively evaluating the models' comprehension and reasoning abilities in extended texts. Moreover, conventional benchmarks relying on F1 metrics often inaccurately score responses: they may undervalue correct answers that differ from the reference responses and overvalue incorrect ones that resemble the reference texts. In response to these limitations, we introduce Marathon, a novel evaluation benchmark that adopts a multiple-choice question format. It is specifically designed to overcome the constraints of previous benchmarks and provide a rapid, precise, and unbiased appraisal of the long-context comprehension skills of large language models. We conducted comprehensive evaluations on the Marathon benchmark with a range of state-of-the-art LLMs and assessed the effectiveness of various optimization strategies tailored for long-context generation. We anticipate that the Marathon benchmark and its associated leaderboard will enable a more precise and equitable evaluation of LLMs' capabilities in understanding and reasoning over extended contexts",
    "checked": true,
    "id": "1f5cb502775398d9aef749d5373ff336002ed372",
    "semantic_title": "marathon: a race through the realm of long context with large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=8eaftIyOdQZ": {
    "title": "Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have revolutionized open-domain dialogue agents but encounter challenges in multi-character role-playing (MCRP) scenarios. To address the issue, we present \\textbf{Neeko}, an innovative framework designed for efficient multiple characters imitation. Unlike existing methods, Neeko employs a dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to diverse characters. Our framework breaks down the role-playing process into agent pre-training, multiple characters playing, and character incremental learning, effectively handling both seen and unseen roles.This dynamic approach, coupled with distinct LoRA blocks for each character, enhances Neeko's adaptability to unique attributes, personalities, and speaking patterns. As a result, Neeko demonstrates superior performance in MCRP over most existing methods, offering more engaging and versatile user interaction experiences",
    "checked": true,
    "id": "789c5dd71fb6085161931955339de2172539cdc8",
    "semantic_title": "neeko: leveraging dynamic lora for efficient multi-character role-playing agent",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=5Jm3W2tmama": {
    "title": "Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs",
    "volume": "review",
    "abstract": "Large language models (LLMs) have achieved impressive human-like performance across various reasoning tasks. However, their mastery of underlying inferential rules still falls short of human capabilities. To investigate this, we propose a logic scaffolding inferential rule generation framework, to construct an inferential rule base, ULogic, comprising both primitive and compositional rules across five domains. Our analysis of GPT-series models over a rule subset reveals significant gaps in LLMs' logic understanding compared to human performance, especially in compositional and structural complex rules with certain bias patterns. We further distill these rules into a smaller-scale inference engine for flexible rule generation and enhancing downstream reasoning. Through a multi-judger evaluation, our inference engine proves effective in generating accurate, complex and abstract conclusions and premises, and improve various commonsense reasoning tasks. Overall, our work sheds light on LLMs' limitations in grasping inferential rule and suggests ways to enhance their logical reasoning abilities~\\footnote{All code and data have been uploaded and will be published upon acceptance.}",
    "checked": true,
    "id": "49ed697a1e5cc5be831e33f2efb7dc61c77d28e7",
    "semantic_title": "can llms reason with rules? logic scaffolding for stress-testing and improving llms",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=9Fl6z0fLez3": {
    "title": "Quantum-Inspired Sentence Representation: Rethinking Word-Based Density Matrices",
    "volume": "review",
    "abstract": "This paper proposes a novel approach to enhance traditional quantum-inspired models. We introduce a Quantum-Inspired Sentence Representation model (QISR), which transforms word density matrices into representations of entire sentences, improving computational resource efficiency. Compared with traditional quantum-inspired models, the QISR method works at the density matrix layer and has better effects on the overall model as the embedding dimension increases. Even the QPDN model with a word embedding of 768 dimensions only requires 1736MB. This optimization has potential benefits for the overall model architecture, particularly when dealing with large word embedding dimensions. Furthermore, this approach reduces computing resource consumption while maintaining high computational accuracy, highlighting its potential benefits in processing complex language tasks. This research provides a novel approach to sentence representation in quantum-inspired language models and highlights the potential value of improved computational methods in a quantum-inspired context. Our research results are expected to provide modeling support and practical application guidance for future text processing endeavors",
    "checked": false,
    "id": "bc79b56b33908e1518185bdeefba9166a12210c8",
    "semantic_title": "complex-valued neural network-based quantum language models",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=B3qT4gVHwqY": {
    "title": "Speculative Decoding via Early-exiting for Faster LLM Inference with Thompson Sampling Control Mechanism",
    "volume": "review",
    "abstract": "The recent advancements in large language models (LLMs) have been extraordinary, yet the escalating inference costs associated with them present challenges in real-world applications. To address these challenges, we propose a novel approach called Early-exiting Speculative Decoding (EESD) with lossless acceleration. Specifically, EESD utilizes a segment of the LLM to generate draft tokens, incorporating Early-exiting structures after the first N layers. To enhance the quality of draft tokens, a self-distillation method is integrated. This early-exiting design not only reduces deployment and training costs but also significantly accelerates the token generation speed. Moreover, we introduce a novel sampling mechanism that leverages Thompson Sampling to regulate the generation processes, automatically determining the quantity of draft tokens in each round. The original LLM is then employed to validate these draft tokens through a single forward pass, and thus guarantees that the final output text maintains a distribution consistent with vanilla auto-regressive decoding. The experimental results on both 13B and 70B models demonstrate that our approach decodes tokens at a markedly accelerated rate compared to prior methods, showing the effectiveness of our approach",
    "checked": true,
    "id": "7e206ea4c8b42f67d66c5205891b06d2af612667",
    "semantic_title": "speculative decoding via early-exiting for faster llm inference with thompson sampling control mechanism",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pDdbi68yzgr": {
    "title": "Emergent Word Order Universals from Cognitively-Motivated Language Models",
    "volume": "review",
    "abstract": "The world's languages exhibit certain so-called typological or implicational universals; for example, Subject-Object-Verb (SOV) word order typically employs postpositions. Explaining the source of such biases is a key goal in linguistics.We study the word-order universals through a computational simulation with language models (LMs).Our experiments show that typologically typical word orders tend to have lower perplexity estimated by LMs with cognitively plausible biases: syntactic biases, specific parsing strategies, and memory limitations. This suggests that the interplay of these cognitive biases and predictability (perplexity) can explain many aspects of word-order universals.This also showcases the advantage of cognitively-motivated LMs, which are typically employed in cognitive modeling, in the computational simulation of language universals",
    "checked": true,
    "id": "c2f1750b81561b55d27db5410da02cb42f17e34b",
    "semantic_title": "emergent word order universals from cognitively-motivated language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=dS6caJbi69": {
    "title": "Spiral of Silence: How is Large Language Model Killing Information Retrieval?â€”A Case Study on Open Domain Question Answering",
    "volume": "review",
    "abstract": "The practice of Retrieval-Augmented Generation (RAG), which integrates Large Language Models (LLMs) with retrieval systems, has become increasingly prevalent. However, the repercussions of LLM-derived content infiltrating the web and influencing the retrieval-generation feedback loop are largely uncharted territories. In this study, we construct and iteratively run a simulation pipeline to deeply investigate the short-term and long-term effects of LLM text on RAG systems. Taking the trending Open Domain Question Answering (ODQA) task as a point of entry, our findings reveal a potential digital \"Spiral of Silence\" effect, with LLM-generated text consistently outperforming human-authored content in search rankings, thereby diminishing the presence and impact of human contributions online. This trend risks creating an imbalanced information ecosystem, where the unchecked proliferation of erroneous LLM-generated content may result in the marginalization of accurate information. We urge the academic community to take heed of this potential issue, ensuring a diverse and authentic digital information landscape",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OYlThSvQEnc": {
    "title": "Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation",
    "volume": "review",
    "abstract": "With the rapid development of large language models, Retrieval-Augmented Generation (RAG) that incorporates external knowledge has become a widely adopted approach to help large language models alleviate knowledge bottlenecks and mitigate hallucinations. However, the existing RAG paradigm inevitably suffers from the impact of flawed information introduced during the retrieval, thereby diminishing the reliability and correctness of the generated outcomes. In this paper, we propose Credibility-Aware Generation (CAG), a universally applicable framework designed to address the issue of flawed information in RAG. At its core, CAG aims to equip models with the ability to discern and process information based on its credibility. To this end, we propose an innovative data transformation framework that generates data based on credibility, thereby effectively endowing models with the capability of CAG. To effectively assess models' capabilities of CAG, we construct a comprehensive benchmark encompassing three critical real-world scenarios. Experimental results demonstrate that our models can understand and utilize credibility, significantly outperform other models with retrieval augmentation, and effectively resist the impact of noise documents, maintaining robust performance",
    "checked": true,
    "id": "e876f221c3eaf542d0d79f5f1f5c2f51ad2d0a48",
    "semantic_title": "not all contexts are equal: teaching llms credibility-aware generation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=ARHk8E2eTRc": {
    "title": "Can Neuron Activation be Predicted? A New Lens of Neuron Predictability for Analyzing Transformer-based Large Language Models",
    "volume": "review",
    "abstract": "Transformer-based large language models (LLMs) play a vital role in various NLP tasks, but the internal neurons are rather unpredictable, functioning in a black box style. Thus, in this work, we introduce the \\textit{Neuron Predictability Lens (NPL)}, an analytical framework that focuses on the way how neurons work through feed-forward networks (FFNs), to understand and analyze transformer-based LLMs. Based on the proposed framework, we conduct extensive experiments on LLaMA-2 and GPT-J. Firstly, we show that neuron activations are predictable and for the first time we introduce the concept of \\textit{Neuron Predictability}. Secondly, we apply NPL to both global and local analysis. For global analysis, we investigate how FFNs contribute to model behaviors explicitly and implicitly with the aid of NPL. For local analysis, we explore the connection between neuron predictability and neuron interpretability. We examine various functional neurons under NPL and uncover the existence of ``background neurons.'' With the findings mentioned above, we demonstrate the value of NPL as a novel analytical tool and shed light on its future application on model efficiency and/or effectiveness for improved language modeling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qDTIXE_iJFr": {
    "title": "Advancement in Graph Understanding: A Multimodal Benchmark and Fine-Tuning of Vision-Language Models",
    "volume": "review",
    "abstract": "Graph data organizes complex relationships and interactions between objects, facilitating advanced analysis and decision-making across different fields.In this paper, we propose a new paradigm for interactive and instructional graph data understanding and reasoning.Instead of adopting complex graph neural models or heuristic graph-to-text instruction design, we leverage Vision-Language Models (VLMs) to encode the graph images with varying structures across different domains.This paper first evaluates the capabilities of public VLMs in graph learning from multiple aspects.Then it introduces a novel instruction-following dataset for multimodal graph understanding and reasoning in English and Chinese. Besides, by fine-tuning MiniGPT-4 and LLaVA on our dataset, we achieved an accuracy increase of 5\\%-15\\% compared to baseline models, with the best-performing model attaining scores comparable to Gemini in GPT-asissted Evaluation . This research not only showcases the potential of integrating VLMs with graph data but also opens new avenues for advancements in graph data understanding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=45aMpeVvXB": {
    "title": "RE-RAG: Improving Open-Domain QA Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation",
    "volume": "review",
    "abstract": "Retrieval-augmented generation (RAG) frame work is showing state-of-the-art performance on open-domain question answering tasks by referencing external knowledge. However, the RAG system faces challenges with performance degradation when it is fed contexts of low relevance or when the relative relevance among the input contexts is inaccurately assessed. In this work, we propose a RE-RAG framework that injects an explicit context relevance estimator (RE) into the RAG system. RE-RAG re-evaluates the retrieved contexts with the proposed context RE and passes the more relevant contexts along with their measure importance to the generator. To train context RE, we propose an unsupervised learning method, which does not utilize any labeled document ranking data to train the context RE. To examine the efficacy of RE-RAG, we examine its performance on Natural Questions and TriviaQA datasets. RE-RAG achieves on-par performance compared to the FiD variants while utilizing fewer contexts (0.25x). We show that the proposed context RE, which was trained with the T5 model, is also applicable to RAG with LLMs(ChatGPT) by improving the performance on NQ (+6.4EM) and TQA (+2.8EM), respecitvely. Lastly, we display that RE can add interpretability to RAG framework as RE score highly correlates with the RE-RAG accuracy. Consequently, RE can be utilized to filter out unanswerable scenarios where context does not contain answers with 38.9%-51.3% accuracy just by examining a set of retrieved contexts",
    "checked": true,
    "id": "755bf404a7e8a81df53172abd3fdb5ff8e31ecf3",
    "semantic_title": "re-rag: improving open-domain qa performance and interpretability with relevance estimator in retrieval-augmented generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ja7GCfkX1p": {
    "title": "SEER: Facilitating Structured Reasoning and Explanation via Reinforcement Learning",
    "volume": "review",
    "abstract": "Elucidating the reasoning process with structured explanations from question to answer is crucial, as it significantly enhances the interpretability, traceability, and trustworthiness of question-answering (QA) systems. However, structured explanations demand models to perform intricately structured reasoning, which poses great challenges. Most existing methods focus on single-step reasoning through supervised learning, ignoring logical dependencies between steps. Moreover, existing reinforcement learning (RL) based methods overlook the structured relationships, underutilizing the potential of RL in structured reasoning. In this paper, we propose SEER, a novel method that maximizes a structure-based return to facilitate structured reasoning and explanation. Our proposed structure-based return precisely describes the hierarchical and branching structure inherent in structured reasoning, effectively capturing the intricate relationships between different reasoning steps. In addition, we introduce a fine-grained reward function to meticulously delineate diverse reasoning steps. Extensive experiments show that SEER significantly outperforms state-of-the-art methods, achieving an absolute improvement of 6.9% over RL-based methods on EntailmentBank, a 4.4% average improvement on STREET benchmark, and exhibiting outstanding efficiency and cross-dataset generalization performance",
    "checked": true,
    "id": "caa6a66bf49952295fa21bb95eb55ae6f845c8b6",
    "semantic_title": "seer: facilitating structured reasoning and explanation via reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V9RzGS1ayiK": {
    "title": "GATE X-E : A Challenge Set for Gender-Fair Translations from Weakly-Gendered Languages",
    "volume": "review",
    "abstract": "Neural Machine Translation (NMT) continues to improve in quality and adoption, yet the in advertent perpetuation of gender bias remainsa significant concern. Despite numerous studies on gender bias in translations into English from weakly gendered-languages, there are no benchmarks for evaluating this phenomenon or for assessing mitigation strategies. To address this gap, we introduce GATE X-E, an extension to the GATE (Rarrick et al., 2023) corpus, that consists of human translations from Turkish, Hungarian, Finnish, and Persian into English. Each translation is accompanied by feminine, masculine, and neutral variants. The dataset, which contains between 1250 and 1850 instances for each of the four language pairs, features natural sentences with a wide range of sentence lengths and domains, challenging translation rewriters on various linguistic phenomena. Additionally, we present a translation gender rewriting solution built with GPT-4 and use GATE X-E to evaluate it. We open source our contributions to encourage further research on gender debiasing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x8mQ_CjOtQ": {
    "title": "Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement Method",
    "volume": "review",
    "abstract": "Bargaining is an important and unique part of negotiation between humans. As LLM-driven agents learn to negotiate and act like real humans, how to evaluate agents' bargaining abilities remains an open problem.For the first time, we formally described the Bargaining task as an asymmetric incomplete information game, defining the gains of the Buyer and Seller in multiple bargaining processes. It allows us to quantitatively assess an agent's performance in the Bargain task.We collected a real product price dataset, AmazonHistoryPrice, and conducted evaluations of various LLM agents' bargaining abilities. We find that playing a Buyer is much harder than a Seller, and increasing model size can not effectively improve the Buyer's performance.To address the challenge, we propose a novel approach called OG-Narrator that integrates a deterministic Offer Generator to control the price range of Buyer's offers, and an LLM Narrator to create natural language sentences for generated offers.Experimental results show that OG-Narrator improves the buyer's deal rates from 26.67% to 88.88% and brings a ten times of multiplication of profits on all baselines, even a model that has not been aligned",
    "checked": true,
    "id": "f85ec64b14494216702d5218f58e725224ad80aa",
    "semantic_title": "measuring bargaining abilities of llms: a benchmark and a buyer-enhancement method",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=nfcKFJw0K91": {
    "title": "FaithScore: Fine-grained Evaluations of Hallucinations in Large Vision-Language Models",
    "volume": "review",
    "abstract": "We introduce FAITHSCORE (Faithfulness to Atomic Image Facts Score), a reference-free and fine-grained evaluation metric that measures the faithfulness of the generated free-form answers from large vision-language models (LVLMs). The FAITHSCORE evaluation first identifies sub-sentences containing descriptive statements that need to be verified, then extracts a comprehensive list of atomic facts from these sub-sentences, and finally conducts consistency verification between fine-grained atomic facts and the input image. Meta-evaluation demon strates that our metric highly correlates with human judgments of faithfulness. We collect two benchmark datasets (i.e. LLaVA-1k and MSCOCO-Cap) for evaluating LVLMs instruction-following hallucinations. We measure hallucinations in state-of-the-art LVLMs with FAITHSCORE on the datasets. Results reveal that current systems are prone to generate hallucinated content unfaithful to the image, which leaves room for future improvements. We hope our metric FAITHSCORE can help evaluate future LVLMs in terms of faithfulness and provide insightful advice for enhancing LVLMs' faithfulness",
    "checked": false,
    "id": "fbae34c21a6a0cbf3f9e2710b7fce0e011aec72c",
    "semantic_title": "faithscore: evaluating hallucinations in large vision-language models",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=2iMGukg8Js": {
    "title": "Large Language Models are Few-shot Generators: Proposing Hybrid Prompt Algorithm To Generate Webshell Escape Samples",
    "volume": "review",
    "abstract": "The frequent occurrence of cyber-attacks has made webshell attacks and defense gradually become a research hotspot in the field of network security. However, the lack of publicly available benchmark datasets and the over-reliance on manually defined rules for webshell escape sample generation have slowed down the progress of research related to webshell escape sample generation and artificial intelligence-based webshell detection. To address the drawbacks of weak webshell sample escape capabilities, the lack of webshell datasets with complex malicious features, and to promote the development of webshell detection, we propose the Hybrid Prompt algorithm for webshell escape sample generation with the help of large language models. As a prompt algorithm specifically developed for webshell sample generation, the Hybrid Prompt algorithm not only combines various prompt ideas including Chain of Thought, Tree of Thought, but also incorporates various components such as webshell hierarchical module and few-shot example to facilitate the LLM in learning and reasoning webshell escape strategies. Experimental results show that the Hybrid Prompt algorithm can work with multiple LLMs with excellent code reasoning ability to generate high-quality webshell samples with high Escape Rate ($88.61\\%$ with GPT-4 model on VIRUSTOTAL detection engine) and Survival Rate ($54.98\\%$ with GPT-4 model)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gza89F3uRoR": {
    "title": "Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning",
    "volume": "review",
    "abstract": "As new research on Large Language Models (LLMs) continues, it is difficult to keep up with new research and models. To help researchers synthesize the new research many have written survey papers, but even those have become numerous. In this paper, we develop a method to automatically assign survey papers to a taxonomy. We collect the metadata of 144 LLM survey papers and explore three paradigms to classify papers within the taxonomy. Our work indicates that leveraging graph structure information on co-category graphs can significantly outperform the language models in two paradigms; pre-trained language models' fine-tuning and zero-shot/few-shot classifications using LLMs. We find that our model surpasses an average human recognition level and that fine-tuning LLMs using weak labels generated by a smaller model, such as the GCN in this study, can be more effective than using ground-truth labels, revealing the potential of weak-to-strong generalization in the taxonomy classification task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KTfzucncYLa": {
    "title": "ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs",
    "volume": "review",
    "abstract": "The integration of Large Language Models (LLMs) and knowledge graphs (KGs) has achieved remarkable success in various natural language processing tasks. However, existing methodologies that integrate LLMs and KGs often navigate the task-solving process solely based on the LLM's analysis of the question, overlooking the rich cognitive potential inherent in the vast knowledge encapsulated in KGs. To address this, we introduce Observation-Driven Agent (ODA), a novel AI agent framework tailored for tasks involving KGs. ODA incorporates KG reasoning abilities via global observation that enhances reasoning capabilities through a cyclical paradigm of observation, action, and reflection. Confronting the exponential explosion of knowledge during observation, we innovatively design a recursive observation mechanism. Subsequently, we integrate the observed knowledge into the action and reflection modules. Through extensive experiments, ODA demonstrates state-of-the-art performance on several datasets, notably achieving accuracy improvements of 12.87% and 8.9%",
    "checked": true,
    "id": "a42818ecc9a187862301fdad1e9c24adb6d985cb",
    "semantic_title": "oda: observation-driven agent for integrating llms and knowledge graphs",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=vecjG9Mh06J": {
    "title": "PRP-Graph: Pairwise Ranking Prompting to LLMs with Graph Aggregation for Effective Text Re-ranking",
    "volume": "review",
    "abstract": "Pairwise Ranking Prompting (PRP) demonstrates impressive effectiveness in zero-shot document re-ranking tasks with large language models (LLMs). However, in the existing methods, PRP only outputs the same label for the comparison results of different confidence intervals without considering the uncertainty of pairwise comparison, which implies an underutilization of the generation probability information of LLMs. To bridge this gap, we propose PRP-Graph, a novel pairwise re-ranking approach, based on a refined scoring PRP unit that exploits the output probabilities of target labels to capture the degree of certainty of the comparison results. Specifically, the PRP-Graph consists of two stages, namely ranking graph construction and ranking graph aggregation. Extensive experiments conducted on the BEIR benchmark demonstrate the superiority of our approach over existing PRP-based methods. Comprehensive analysis reveals that the PRP-Graph displays strong robustness towards the initial ranking order and delivers exceptional re-ranking results with acceptable efficiency. Our code and data will be available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HwzOLkLCUpa": {
    "title": "Mitigating Mention Surface Bias for Entity Disambiguation",
    "volume": "review",
    "abstract": "Entity disambiguation (ED) is a foundational task in NLP for question-answering and information extraction applications. One of the main challenges of ED in real-world settings is the handling of overshadowed entities, i.e., the entities that share mention surfaces with common entities. The current approach for handling overshadowed entities relies on the coherence of entities in a given text, which is not always available and requires additional computing resources. In this paper, we formulated a causal graph for ED and found that the mention surfaces can act as a shortcut, misleading the ED models to be biased towards common entities. We propose a simple yet effective debiasing method that mitigates the effect of the mention surfaces on model predictions. Experimental results demonstrate that our method yields the best results against overshadowed entities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bKhSH0odBZA": {
    "title": "InfiMM-Eval: Complex Open-ended Reasoning Evaluation for Multi-modal Large Language Models",
    "volume": "review",
    "abstract": "Multi-modal Large Language Models are increasingly prominent due to their superior reasoning abilities to excel at complex tasks. Prevailing benchmarks related to multi-modal reasoning attempt to assess MLLMs through yes/no or multi-choice questions, which by design can introduce position bias and overlook the intermediate reasoning process, thereby rendering the results less convincing. To this end, we systematically categorize the reasoning tasks into deductive, abductive and analogical reasoning, and introduce InfiMM-Eval, a manually curate benchmark featuring 279 diverse and nuanced reasoning questions across these categories. The questions are designed to be fully open-ended to better represent the characteristics of generative models. To mitigate the challenge of answering complex reasoning questions, we encourage models to generate intermediate reasoning steps. These steps are incorporated into the evaluation protocol to reduce bias towards plausible guesses or responses that lack definitive answers, while facilitating the assessment of more nuanced reasoning skills. This evaluation scheme closely resembles the method by which humans evaluate exams in real-world settings, enabling a more reliable assessment. We evaluate a large selection of trending MLLMs to reveal the discrepancies in reasoning abilities between open-source and proprietary MLLMs. Additionally, we conduct a comprehensive analysis of three reasoning related factors, highlighting potential directions for further research in elevating MLLMs in reasoning tasks",
    "checked": true,
    "id": "0c7ca22dfbe1ce02cb0f0658292499457db8ec6e",
    "semantic_title": "infimm-eval: complex open-ended reasoning evaluation for multi-modal large language models",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=W3k985oeLfL": {
    "title": "A Grounded Preference Model for LLM Alignment",
    "volume": "review",
    "abstract": "Despite LLMs' recent advancements, they still suffer from factual inconsistency and hallucination. An often-opted remedy is retrieval-augmented generation -- however, there is no guarantee that the model will strictly adhere to retrieved grounding. Fundamentally, LLMs need to be aligned to be more faithful to grounding, which will require high-quality preference annotations. This paper investigates whether we can create high-quality grounded preference data for model alignment without using annotations from humans or large proprietary models. We experimented with existing entailment data and proposed approaches to generate synthetic grounded preference data, with which we train a Grounded Preference Model(GPM). We demonstrate through Proximal Policy Optimization(PPO) training of Mistral-7B-Instruct that our GPM model can successfully align powerful LLMs to generate much better grounded responses as judged by GPT4. Moreover, we show that our GPM is also a great faithfulness classifier, achieving SoTA in dialogue sub-tasks of the TRUE faithfulness Benchmark. We will release our GPM under the Apache 2.0 license",
    "checked": false,
    "id": "eae3d22ba3e504eb8c1d6ba7b50eef5c1dea2bce",
    "semantic_title": "aligning large language models with self-generated preference data",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NKnQa49afXS": {
    "title": "Large Language Models as Zero-shot Dialogue State Tracker through Function Calling",
    "volume": "review",
    "abstract": "Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT's performance beating the SOTA by 5.6% Avg. JGA. Individual model results for GPT-3.5 and GPT-4 are boosted by 4.8% and 14%, respectively. We also show that by fine-tuning on a small collection of diverse task-oriented dialogues, we can equip modestly sized models, specifically a 13B parameter LLaMA2-Chat model, with function-calling capabilities and DST performance comparable to ChatGPT while maintaining their chat capabilities. We plan to open-source experimental code and model",
    "checked": true,
    "id": "723a6340ee641e190b22bb47455d05e3b4237179",
    "semantic_title": "large language models as zero-shot dialogue state tracker through function calling",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yNK-hTpj0ty": {
    "title": "SSDAU: Structured Semantic Data Augmentation for Joint Entity and Relation Extraction",
    "volume": "review",
    "abstract": "Joint entity and relation extraction (JERE) are highly susceptible to weak-generalization due to low-quality text on various natural language processing (NLP) tasks. Data augmentation is a common approach to enhance text quality. However, traditional data augmentation methods tend to compromise intrinsic textual logic, making it challenging to design effective augmentation techniques. In this paper, we propose a novel paradigm called Structured Semantic Data Augmentation (SSDAU) that that preserves the structured semantics of augmented text. SSDAU applies context-awareness to encode semantic entities in the text, then designs a Tarjan algorithm to complete the neighborhood semantic analysis of related entity, and finally completes the data augmentation by entity reconstruction of neighboring text with a structured semantic model. We compare different baselines under different JERE scenarios and evaluate the performance and efficiency of SSDAU. The results demonstrate that SSDAU can generate high-quality data, which greatly improves the performance of the JERE model and outperforms the state of the art",
    "checked": false,
    "id": "ee62a25c9d251d7bdf15014196a9359fb64cc1c5",
    "semantic_title": "centre: a paragraph-level chinese dataset for relation extraction among enterprises",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=igLtCMVACqs": {
    "title": "TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data",
    "volume": "review",
    "abstract": "In this work, we address question answering (QA) over a hybrid of tabular and textual data, involving a variety of common content in reality like SEC filings, where discrete reasoning is often required. We consider harnessing the multi-step reasoning capabilities of large language models (LLMs) to tackle this problem, which have recently achieved remarkable success in many natural language tasks. To do this, we first abstract a Step-wise Pipeline for tabular and textual QA to help LLMs better execute multi-step inference, containing three key steps of Extractor, Reasoner and Executor. We initially design an instruction to validate the pipeline on GPT-4, demonstrating promising results. However, utilizing an online LLM like GPT-4 holds various challenges in terms of cost, latency, and data security risk, which motivates us to specialize smaller LLMs in this task. We then develop a TAT-LLM model by fine-tuning LLaMA 2 with the training data generated automatically from existing datasets following the Step-wise Pipeline. The experimental results have verified that our TAT-LLM model can outperform all compared models, including prior best fine-tuned models and very large-scale LLMs like GPT-4 on FinQA, TAT-QA and TAT-DQA benchmarks",
    "checked": true,
    "id": "ecc3415b74717b3f786760e12934a31b37d98312",
    "semantic_title": "tat-llm: a specialized language model for discrete reasoning over tabular and textual data",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=F-F2OJWVZ1t": {
    "title": "Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations",
    "volume": "review",
    "abstract": "We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs' reasoning ability, such as Chain-of-Thought. Our findings indicate that the LLM's language orientation and the task's domain influence the effectiveness of the prompt strategy, which enriches previous research findings. We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance. We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors. Our study precisely identified the LLMs' strengths and weaknesses, providing the clear direction for optimization. It can also serve as a reference for studies in other fields. We will release CHARM on GitHub",
    "checked": true,
    "id": "333b7c8e9af8b49271663039edf62a9da01ad16a",
    "semantic_title": "benchmarking chinese commonsense reasoning of llms: from chinese-specifics to reasoning-memorization correlations",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=UVxaErOReaS": {
    "title": "Data-Efficient Alignment via Learning from Collective Feedback in Social Media",
    "volume": "review",
    "abstract": "Aligning large language models (LLMs) with human feedback becomes a critical area due to LLMs' potential for acquiring undesirable abilities from unsupervised corpora. Traditionally, LLMs' alignment involves extensive human preference data collection, which is time-consuming and labor-intensive. To address this issue, in this paper, we explore LLM alignment via learning from collective feedback (LCF) contained in social media. Social media users often provide diverse feedback on content, reflecting a broad spectrum of human preferences, which can provide abundant training signals for alignment. We thoroughly investigate the training strategies for incorporating collective feedback and examine the effectiveness of LCF on widely-used direct preference optimization algorithm. The experimental results show that LCF can significantly reduce the need for human annotation, achieving comparable performance with only 20% of annotated data. Additionally, LLMs with LCF exhibit improved generalization across out-of-domain instructions. The code and data used in our paper will be released to promote the development of learning from collective feedback",
    "checked": false,
    "id": "c245cf72c6268cc579be458cd96c2869b9391b08",
    "semantic_title": "sentiment data analysis for detecting social sense after covid-19 using hybrid optimization method",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=YOg1HqFX5su": {
    "title": "Decomposing Co-occurrence Matrices into Interpretable Components as Formal Concepts",
    "volume": "review",
    "abstract": "This study addresses the interpretability of word representations through an investigation of a count-based co-occurrence matrix. Employing the mathematical methodology of Formal Concept Analysis, we reveal an underlying structure that is amenable to human interpretation. Furthermore, we unveil the emergence of hierarchical and geometrical structures within word vectors as consequences of word usage. Our experiments on the PPMI matrix demonstrate that the formal concepts we identified align with interpretable categories, as shown in the category completion task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q88y4Vbo-2Z": {
    "title": "Sparse KD: Knowledge Distillation for Sparse Models in Constrained Fine-tuning Scenarios",
    "volume": "review",
    "abstract": "Large language models (LLMs) have achieved tremendous success in various domains, but their massive parameter sizes pose challenges for fine-tuning and inference. Recently, the common model compression process involves obtaining a sparse LLM through pruning, followed by LoRA-finetuning. However, these methods often suffer from significant performance degradation. We attempted to address this by introducing additional teacher distillation, but found limited improvements due to the gap between the teacher and student models and constrained training iterations. To overcome these challenges, we propose Sparse KD, the first distillation framework specifically designed for sparse models in constrained fine-tuning scenarios. Our framework includes dynamic temperature, knowledge alignment, and Bayesian distillation optimization strategies. Dynamic temperature can adaptively align the strength of the teacher's knowledge, and the Knowledge Alignment Module can bridge the gap by projecting teacher-student knowledge to the same interval. Applying Bayesian optimization swiftly finds optimal settings based on these strategies, thereby improving model performance. Comprehensive experiments across diverse task types have demonstrated that this combination can be applied to LLMs with effective and stable results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XSzMcfJq78M": {
    "title": "Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark for Deception Reasoning",
    "volume": "review",
    "abstract": "Deception detection has attracted increasing attention due to its importance in many practical scenarios. Currently, data scarcity harms the development of this field. On the one hand, it is costly to hire participants to simulate deception scenarios. On the other hand, it is difficult to collect videos containing deceptive behaviors on the Internet. To address data scarcity, this paper proposes a new data collection pipeline. Specifically, we use GPT-4 to simulate a role-play between a suspect and a police officer. During interrogation, the suspect lies to the police officer to evade responsibility for the crime, while the police officer uncovers the truth and gathers evidence. Compared with previous datasets, this strategy reduces data collection costs, providing a promising way to increase the dataset size. Meanwhile, we extend the traditional deception detection task to deception reasoning, further providing evidence for deceptive parts. This dataset can also be used to evaluate the complex reasoning capability of current large language models and serve as a reasoning benchmark for further research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8r0T-kDNICF": {
    "title": "History-Aware Conversational Dense Retrieval",
    "volume": "review",
    "abstract": "Conversational search facilitates complex information retrieval by enabling multi-turn interactions between users and the system. Supporting such interactions requires a comprehensive understanding of the conversational inputs to formulate a good search query based on historical information. In particular, the search query should include the relevant information from the previous conversation turns.However, current approaches for conversational dense retrieval primarily rely on fine-tuning a pre-trained ad-hoc retriever using the whole conversational search session, which can be lengthy and noisy. Moreover, existing approaches are limited by the amount of manual supervision signals in the existing datasets. To address the aforementioned issues, we propose a History-Aware Conversational Dense Retrieval (HAConvDR) system, which incorporates two ideas: context-denoised query reformulation and automatic mining of supervision signals based on the actual impact of historical turns. Experiments on two public conversational search datasets demonstrate the improved history modeling capability of HAConvDR, in particular for long conversations with topic shifts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uuC7HZUPy8R": {
    "title": "Collaboration or Corporate Capture? Quantifying NLP's Reliance on Industry Artifacts and Contributions",
    "volume": "review",
    "abstract": "Impressive performance of pre-trained models has garnered public attention and made news headlines in recent years. Almost always, these models are produced by or in collaboration with industry. Using them is critical for competing on natural language processing (NLP) bench-marks and correspondingly to stay relevant in NLP research. We surveyed 100 papers published at EMNLP 2022 to determine to what degree researchers rely on industry models, other artifacts, and contributions to publish in prestigious NLP venues and found that the ratio of their citation is at least three times greater than what would be expected. We discuss two possible perspectives regarding NLP's increasing reliance on industry: 1) Is collaboration withindustry still collaboration in the absence of an alternative? Or 2) has NLP inquiry been captured by the motivations and research direction of private corporations?",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rnf-BeHwPoq": {
    "title": "SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs",
    "volume": "review",
    "abstract": "Large language models hold significant potential for integrating various data types, such as text documents and database records, for advanced analytics. However, blending text and numerical data presents substantial challenges. LLMs need to process and cross-reference entities and numbers, handle data inconsistencies and redundancies, and develop planning capabilities such as building a working memory for managing complex data queries. In this paper, we introduce four novel tasks centered around sports data analytics to evaluate the numerical reasoning and information fusion capabilities of LLMs. These tasks involve providing LLMs with detailed, play-by-play sports game descriptions, then challenging them with adversarial scenarios such as new game rules, longer durations, scrambled narratives, and analyzing key statistics in game summaries. We conduct extensive experiments on NBA and NFL games to assess the performance of LLMs on these tasks. Our benchmark, SportsMetrics, introduces a new mechanism for assessing LLMs' numerical reasoning and fusion skills",
    "checked": true,
    "id": "4a3e0c90baa440d7e327f4b156bca3a40cefbd40",
    "semantic_title": "sportsmetrics: blending text and numerical data to understand information fusion in llms",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=j3sZGxC_-DA": {
    "title": "Unlocking Markets: A Multilingual Benchmark to Cross-Market Question Answering",
    "volume": "review",
    "abstract": "Users post numerous product-related questions on e-commerce platforms, affecting their purchase decisions. Product-related question answering (PQA) entails utilizing product-related resources to provide precise responses to users. We propose a novel task of Multilingual Cross-market Product-based Question Answering (MCPQA) and define the task as providing answers to product-related questions in a main marketplace by utilizing information from another resource-rich auxiliary marketplace in a multilingual context. We introduce a large-scale dataset comprising over 7 million questions from 17 marketplaces across 11 languages. We then perform automatic translation on the Electronics category of our dataset, naming it as McMarket. We focus on two subtasks: review-based answer generation and product-related question ranking. For each subtask, we label a subset of McMarket using an LLM and further evaluate the quality of the annotations via human assessment. We then conduct experiments to benchmark our dataset, using models ranging from traditional lexical models to LLMs in both single-market and cross-market scenarios across McMarket and the corresponding LLM subset. Results show that incorporating cross-market information significantly enhances performance in both tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=glGkVk8kX6": {
    "title": "Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models",
    "volume": "review",
    "abstract": "The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the \"confidence\" of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction in LLMs. To address the over-criticism issue, we introduce an If-or-Else (IoE) prompting principle, which guides LLMs to judge themselves with intrinsic ``confidence'', enabling effective self-correction without relying on external feedback or human-annotated examples",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k4aMbVAyiN": {
    "title": "MlingConf: A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models",
    "volume": "review",
    "abstract": "The tendency of Large Language Models (LLMs) to generate hallucinations and exhibit overconfidence in predictions raises concerns regarding their reliability. Precious confidence or uncertainty estimations indicating the extent of trustworthiness of a model's response are essential to developing reliable AI systems. Current research primarily focuses on LLM confidence estimations in English, remaining a void for other widely used languages and impeding the global development of reliable AI applications. This paper introduces a comprehensive framework of multilingual confidence estimation on LLMs named M LING C ONF. First, M LING C ONF introduces an elaborated and expert-checked multilingual QA dataset. Second, we delve into the performance of confidence estimations and examine how these confidence scores can enhance LLM performance through self-refinement across diverse languages. Finally, we propose a cross-lingual confidence estimation method to achieve more precise confidence scores. The experimental results showcase the performance of various confidence estimation methods across different languages as well as present that our proposed cross-lingual confidence estimation technique significantly enhances confidence estimation and outperforms several baseline methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5DMm_f1Wp6": {
    "title": "Open Grounded Planning: Challenges and Benchmark Construction",
    "volume": "review",
    "abstract": "The emergence of large language models (LLMs) has increasingly drawn attention to the use of LLMs for human-like planning. Existing work on LLM-based planning either focuses on leveraging the inherent language generation capabilities of LLMs to produce free-style plans, or employs reinforcement learning approaches to learn decision-making for a limited set of actions within restricted environments. However, both approaches exhibit significant discrepancies from the open and executable requirements in real-world planning. In this paper, we propose a new planning task--open grounded planning. The primary objective of open grounded planning is to ask the model to generate an executable plan based on a variable action set, thereby ensuring the executability of the produced plan. To this end, we establishes a benchmark for open grounded planning spanning a wide range of domains. Then we test current state-of-the-art LLMs along with five planning approaches, revealing that existing LLMs and methods still struggle to address the challenges posed by grounded planning in open domains. The outcomes of this paper define and establish a foundational dataset for open grounded planning, and shed light on the potential challenges and future directions of LLM-based planning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XxY04_8COV": {
    "title": "Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability",
    "volume": "review",
    "abstract": "As large language models (LLMs) grow more powerful, concerns around potential harms like toxicity, unfairness, and hallucination threaten user trust. Ensuring beneficial alignment of LLMs with human values through model alignment is thus critical yet challenging, requiring a deeper understanding of LLM behaviors and mechanisms. We propose opening the black box of LLMs through a framework of holistic interpretability encompassing complementary bottom-up and top-down perspectives. The bottom-up view, enabled by mechanistic interpretability, focuses on component functionalities and training dynamics. The top-down view utilizes representation engineering to analyze behaviors through hidden representations. In this paper, we review the landscape around mechanistic interpretability and representation engineering, summarizing approaches, discussing limitations and applications, and outlining future challenges in using these techniques to achieve ethical, honest, and reliable reasoning aligned with human values",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VgGXH0zIlw": {
    "title": "Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models",
    "volume": "review",
    "abstract": "Neural network pruning has become increasingly crucial due to the complexity of neural network models and their widespread use in various fields. Existing pruning algorithms often suffer from limitations such as architecture specificity, excessive complexity and reliance on intricate calculations, rendering them impractical for real-world applications.In this paper, we propose KEN (Kernel density Estimator for Neural network compression): a straightforward, universal and unstructured pruning algorithm based on Kernel Density Estimation (KDE). KEN aims to construct optimized transformer models by selectively preserving the most significant parameters while restoring others to their pre-training state. This approach maintains model performance while allowing storage of only the optimized subnetwork, leading to significant memory savings.Extensive evaluations on seven transformer models demonstrate that KEN achieves equal or better performance than the original models with a minimum parameter reduction of 25%. In-depth comparisons against other pruning and PEFT algorithms confirm KEN effectiveness. Furthermore, we introduce $KEN_{viz}$, an explainable tool that visualizes the optimized model composition and the subnetwork selected by KEN",
    "checked": true,
    "id": "d3eeea8b1826b9799f960397857ba00e5045208a",
    "semantic_title": "less is ken: a universal and simple non-parametric pruning algorithm for large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=vQVlMtunC4": {
    "title": "One-Shot Learning as Instruction Data Prospector for Large Language Models",
    "volume": "review",
    "abstract": "Contemporary practices in instruction tuning often hinge on enlarging data scaling without a clear strategy for ensuring data quality, inadvertently introducing noise that may compromise model performance. To address this challenge, we introduce Nuggets, a novel and efficient methodology that leverages one-shot learning to discern and select high-quality instruction data from extensive datasets. Nuggets assesses the potential of individual instruction examples to act as effective one-shot learning instances, thereby identifying those that can significantly improve performance across diverse tasks. Nuggets utilizes a scoring system based on the impact of candidate examples on the perplexity of a diverse anchor set, facilitating the selection of the most advantageous data for instruction tuning. Through rigorous evaluations on two benchmarks, namely MT-Bench and Alpaca-Eval, our study illustrates that instruction tuning with the top 1\\% of examples curated by Nuggets substantially outperforms conventional methods employing the entire dataset. For reproducibility, we will release our code and data upon acceptance",
    "checked": false,
    "id": "7a31971b0af439dec6fc484cca20df57f440b644",
    "semantic_title": "one shot learning as instruction data prospector for large language models",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=fmNsIxLLox": {
    "title": "Distributional Inclusion Hypothesis and Quantifications: Probing for Hypernymy in Functional Distributional Semantics",
    "volume": "review",
    "abstract": "Functional Distributional Semantics (FDS) models the meaning of words by truth-conditional functions. This provides a natural representation for hypernymy but no guarantee that it can be learnt when FDS models are trained on a corpus. In this paper, we probe into FDS models and study the representations learnt, drawing connections between quantifications, the Distributional Inclusion Hypothesis (DIH), and the variational-autoencoding objective of FDS model training. Using synthetic data sets, we reveal that FDS models learn hypernymy on a restricted class of corpus that strictly follows the DIH. We further introduce a training objective that both enables hypernymy learning under the reverse of the DIH and improves hypernymy detection from real corpora",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X5QouIwU8-": {
    "title": "Controlling Summarization Length Through EOS Token Weighting",
    "volume": "review",
    "abstract": "Controlling the length of generated text can be crucial in various text generation tasks including summarization. Existing methods often require complex model alterations, limiting compatibility with pre-trained models. We address these limitations by developing a simple approach for controlling the length of automatic text summaries by increasing the importance of correctly predicting the EOS token in the cross entropy loss computation. The proposed methodology is agnostic to architecture and decoding algorithm and orthogonal to other inference-time techniques for controlling generation length, allowing for powerful hybrid combinations. We test it with encoder-decoder and modern GPT-style LLMs. We show that our method can consistently control generation length without affecting the quality of the summary",
    "checked": false,
    "id": "3c8e764df31034c9b4416c47876879647afb7c00",
    "semantic_title": "llmeffichecker:understanding and testing efficiency degradation of large language models",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=gkorQDBbuL": {
    "title": "MERA: A Comprehensive LLM Evaluation in Russian",
    "volume": "review",
    "abstract": "Over the past few years, one of the most notable advancements in AI research has been in foundation models (FMs), headlined by the rise of language models (LMs). However, despite researchers' attention and the rapid growth in LM application, the capabilities, limitations, and associated risks still need to be better understood. To address these issues, we introduce a new instruction benchmark, MERA, for evaluating foundation models oriented towards the Russian language. The benchmark encompasses 21 evaluation tasks for generative models covering 10 skills and is designed as a black-box test to ensure the exclusion of data leakage. The paper introduces a methodology to evaluate FMs and LMs in fixed zero- and few-shot instruction settings that can be extended to other modalities. We propose an evaluation methodology, an open-source code base for the MERA assessment, and a leaderboard with a submission system. We evaluate open LMs as baselines and find they are still far behind the human level. We publicly release MERA to guide forthcoming research, anticipate groundbreaking model features, standardize the evaluation procedure, and address potential ethical concerns and drawbacks",
    "checked": true,
    "id": "18b3b96ffb28c785452081aa367cbc02a1cf7567",
    "semantic_title": "mera: a comprehensive llm evaluation in russian",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=YByJvxsJ_v": {
    "title": "Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models",
    "volume": "review",
    "abstract": "Incremental Learning (IL) has been a long-standing problem in both vision and Natural Language Processing (NLP) communities.In recent years, as Pre-trained Language Models (PLMs) have achieved remarkable progress in various NLP downstream tasks, utilizing PLMs as backbones has become a common practice in recent research of IL in NLP.Most assume that catastrophic forgetting is the biggest obstacle to achieving superior IL performance and propose various techniques to overcome this issue.However, we find that this assumption is problematic.Specifically, we revisit more than 20 methods on four classification tasks (Text Classification, Intent Classification, Relation Extraction, and Named Entity Recognition) under the two most popular IL settings (Class-Incremental and Task-Incremental) and reveal that most of them severely underestimate the inherent anti-forgetting ability of PLMs.Based on the observation, we propose a frustratingly easy method called SEQ* for IL with PLMs.The results show that SEQ* has competitive or superior performance compared with state-of-the-art (SOTA) IL methods yet requires considerably less trainable parameters and training time.These findings urge us to revisit the IL with PLMs and encourage future studies to have a fundamental understanding of the catastrophic forgetting in PLMs.The data, code and scripts are in the supplementray material and will be publicly available",
    "checked": true,
    "id": "9e2a811a6f5d1c5352ce19ac24303810eb1867f7",
    "semantic_title": "learn or recall? revisiting incremental learning with pre-trained language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=KopsMVJPRO": {
    "title": "Efficient Long Sequence Modeling and Beyond",
    "volume": "review",
    "abstract": "Transformers have been successful in almost all modalities and achieved remarkable performance. However, their quadratic complexity limits their application on long sequences. To address the issue, researchers proposed many efficient sequence models, such as efficient Transformers, and state space models. Intending to assist researchers in navigating this vast landscape, this paper presents a detailed survey of a wide range of recent long-sequence models from their shared nature of token mixing and provides a clear taxonomy. It offers a comprehensive overview of existing work, ultimately providing valuable insights for future design",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zpAW_13xm0q": {
    "title": "On Context Utilization in Summarization with Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) excel in abstractive summarization tasks, delivering fluent and pertinent summaries. Recent advancements have extended their capabilities to handle long-input contexts, exceeding 100k tokens. However, in question answering, language models exhibit uneven utilization of their input context. They tend to favor the initial and final segments, resulting in a U-shaped performance pattern concerning where the answer is located within the input. This bias raises concerns, particularly in summarization where crucial content may be dispersed throughout the source document(s). Besides, in summarization, mapping facts from the source to the summary is not trivial as salient content is usually re-phrased. In this paper, we conduct the first comprehensive study on context utilization and position bias in summarization. Our analysis encompasses 5 LLMs, 10 datasets, and 5 evaluation metrics. We introduce a new evaluation benchmark called MiddleSum on the which we benchmark two alternative inference methods to alleviate position bias: hierarchical summarization and incremental summarization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MCeFkwJkYE": {
    "title": "Robust Singing Voice Transcription Serves Synthesis",
    "volume": "review",
    "abstract": "Note-level Automatic Singing Voice Transcription (AST) converts singing recordings into note sequences, facilitating the automatic annotation of singing datasets for Singing Voice Synthesis (SVS) applications. Current AST methods, however, struggle with accuracy and robustness when used for practical annotation. This paper presents ROSVOT, the first robust AST model that serves SVS, incorporating a multi-scale framework that effectively captures coarse-grained note information and ensures fine-grained frame-level segmentation, coupled with an attention-based pitch decoder for reliable pitch prediction. We also established a comprehensive annotation-and-training pipeline for SVS to test the model in real-world settings. Experimental findings reveal that the proposed model achieves state-of-the-art transcription accuracy with either clean or noisy inputs. Moreover, when trained on enlarged, automatically annotated datasets, the SVS model outperforms its baseline, affirming the capability for practical application. Audio samples are available at https://rosvot.github.io",
    "checked": true,
    "id": "c465b911fa4bdcd0c773b6a3861fcd2d43ce6091",
    "semantic_title": "robust singing voice transcription serves synthesis",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Af3xgkrfif": {
    "title": "Re3: A Holistic Framework and Dataset for Modeling Collaborative Document Revision",
    "volume": "review",
    "abstract": "Collaborative review and revision of textual documents is the core of knowledge work and a promising target for empirical analysis and NLP assistance. Yet, a holistic framework that would allow modeling complex relationships between document revisions, reviews and author responses is lacking. To address this gap, we introduce Re3, a framework for joint analysis of collaborative document revision. We instantiate this framework in the scholarly domain, and present Re3-Sci, a large corpus of aligned scientific paper revisions manually labeled according to their action and intent, and supplemented with the respective peer reviews and human-written edit summaries. We use the new data to provide first empirical insights into collaborative document revision in the academic domain, and to assess the capabilities of state-of-the-art LLMs at automating edit analysis and facilitating text-based collaboration. We make our annotation environment and protocols, the resulting data and experimental code publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sd4JrLNPRK": {
    "title": "Dual Prompt Tuning based Contrastive Learning for Hierarchical Text Classification",
    "volume": "review",
    "abstract": "Hierarchical text classification aims at categorizing texts into multi-tiered tree-like label hierarchy. Existing methods pay more attention to capture hierarchy-aware text feature by exploiting explicit parent-child relationships, while interactions between peer labels are rarely taken into account, resulting in severe label confusions within each layer. In this work, we propose a novel Dual Prompt Tuning (DPT) method, which emphasizes to identify discrimination among peer labels by performing contrastive learning on each hierarchical layer. We design an innovative hand-crafted prompt containing slots for both positive and negative label predictions to cooperate with contrastive learning. In addition, we introduce a label hierarchy self-sensing auxiliary task to ensure cross-layer label consistency. Extensive experiments demonstrate that DPT achieves significant improvements and outperforms the current state-of-the-art methods on BGC and RCV1-V2 benchmark datasets",
    "checked": false,
    "id": "0add4d30f175dae9ce030e2ac15415cc6a92602b",
    "semantic_title": "cophtc: contrastive learning with prompt tuning for hierarchical text classification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0swSI-kO2d": {
    "title": "Concept-1K: A Novel Benchmark for Instance Incremental Learning",
    "volume": "review",
    "abstract": "Incremental learning (IL) is essential to realize the human-level intelligence in the neural network.However, existing IL scenarios and datasets are unqualified for assessing forgetting in PLMs, giving an illusion that PLMs do not suffer from catastrophic forgetting.To this end, we propose a challenging IL scenario called instance-incremental learning (IIL) and a novel dataset called Concept-1K, which supports an order of magnitude larger IL steps. Based on the experiments on Concept-1K, we reveal that billion-parameter PLMs still suffer from catastrophic forgetting, and the forgetting is affected by both model scale, pretraining, and buffer size.Furthermore, existing IL methods and a popular finetuning technique, LoRA, fail to achieve satisfactory performance.Our study provides a novel scenario for future studies to explore the catastrophic forgetting of PLMs and encourage more powerful techniques to be designed for alleviating the forgetting in PLMs.The data, code and scripts are in the supplementary material and will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2YPHP28KgU": {
    "title": "Navigating the OverKill in Large Language Models",
    "volume": "review",
    "abstract": "Large language models are meticulously aligned to be both helpful and harmless. However, recent research points to a potential overkill which means models may refuse to answer benign queries. In this paper, we investigate the factors for overkill by exploring how models handle and determine the safety of queries. Our findings reveal the presence of shortcuts within models, leading to excessive attention to harmful words like 'kill' and prompts emphasizing safety will exacerbate overkill. Based on these insights, we introduce Self-Contrastive Decoding (Self-CD), a training-free and model-agnostic strategy, to alleviate this phenomenon. We first extract such excessive attention by amplifying the difference in the model's output distributions when responding to system prompts that either include or omit an emphasis on safety. Then we determine the final next-token predictions by downplaying the excessive attention via contrastive decoding. Empirical results have indicated that our method has achieved an average reduction of the refusal rate by 20 % while having almost no impact on safety",
    "checked": true,
    "id": "8b8f3bf616bd355b3abb4546de38c3e4595bee96",
    "semantic_title": "navigating the overkill in large language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=OCj6Yf5dEj": {
    "title": "Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL translation",
    "volume": "review",
    "abstract": "Understanding textual description to generate code seems to be an achieved capability of instruction-following Large Language Models (LLMs) in zero-shot scenario. However, there is a severe possibility that this translation ability may be influenced by having seen target textual descriptions and the related code. This effect is known as Data Contamination. In this study, we investigate the impact of Data Contamination on the performance of GPT-3.5 in the Text-to-SQL code-generating tasks. Hence, we introduce a novel method to detect Data Contamination in GPTs and examine GPT-3.5's Text-to-SQL performances using the known Spider Dataset and our new unfamiliar dataset Termite. Furthermore, we analyze GPT-3.5's efficacy on databases with modified information via an adversarial table disconnection (ATD) approach, complicating Text-to-SQL tasks by removing structural pieces of information from the database. Our results indicate a significant performance drop in GPT-3.5 on the unfamiliar Termite dataset, even with ATD modifications, highlighting the effect of Data Contamination on LLMs in Text-to-SQL translation tasks",
    "checked": true,
    "id": "2837a485b5895efea2eb63a707db2196be1d4a2f",
    "semantic_title": "investigating the impact of data contamination of large language models in text-to-sql translation",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=ekIf7sb_La": {
    "title": "GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning",
    "volume": "review",
    "abstract": "Large language models have shown impressive results for multi-hop mathematical reasoning when the input question is only textual. Many mathematical reasoning problems, however, contain both text and image. With the ever-increasing adoption of vision language models (VLMs), understanding their reasoning abilities for such problems is crucial. In this paper, we evaluate the reasoning capabilities of VLMs along various axes through the lens of geometry problems. We procedurally create a synthetic dataset of geometry questions with controllable difficulty levels along multiple axes, thus enabling a systematic evaluation. The empirical results obtained using our benchmark for state-of-the-art VLMs indicate that these models are not as capable in subjects like geometry (and, by generalization, other topics requiring similar reasoning) as suggested by previous benchmarks. This is made especially clear by the construction of our benchmark at various depth levels, since solving higher-depth problems requires long chains of reasoning rather than additional memorized knowledge",
    "checked": true,
    "id": "608a2b333fd8262e8c918f36c5700bafd3ea3cdd",
    "semantic_title": "geomverse: a systematic evaluation of large models for geometric reasoning",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=MIb0-CUfkv9": {
    "title": "ToolCoder: Enabling Code Generation Models to Use Unknown APIs with API Search Tools",
    "volume": "review",
    "abstract": "Automatically generating source code from natural language descriptions has been a growing field of research in recent years. Invoking correct APIs is crucial to code generation. However, existing code generation models struggle in handling unknown APIs (e.g., user-private projects and libraries) , often generating erroneous or even non-existent APIs. Inspired by the process of human developers using code search tools to learn unknown APIs, we propose ToolCoder, a novel approach that integrates API search tools with existing models to assist code generation and API selection. ToolCoder automatically invokes API search tools to retrieve relevant APIs and learns API usages from retrieved results. Our experimental results demonstrate that ToolCoder exhibits excellent performance and generalization ability across five public and private library code generation benchmarks, with at least 6.21% improvement on average pass@1 metrics and 9.64% improvement on average pass@10 metrics compared to state-of-the-art methods. Furthermore, we show that our relatively small ToolCoder model is comparable to one of the current best models, i.e.,, GPT-3.5, highlighting the potential of incorporating programming tools into the code generation process",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EWwZRzwXi3d": {
    "title": "FinAgent: Benchmarking Financial Analysis with Stock Agent Framework",
    "volume": "review",
    "abstract": "Constructing stock agents that facilitate investment analysis is an important research direction in finance. The key technology is the agent's capability to automatically identify user queries and integrate multimodal data for analysis by large language models (LLMs). Currently, LLMs have made some progress, primarily in retrieving text and time-series data from knowledge bases based on user queries and providing these data in a basic combination to LLMs. However, they have not efficiently integrated these data to enhance the performance of the LLMs. Also, they do not fully exploit image information and depend on extensive knowledge bases that require real-time updates. To overcome these limitations, we propose the FinAgent dataset, which encompasses research datasets, financial Q\\&A, stock charts, and handwritten chain-of-thought (CoT) data. Moreover, we innovate a Stock-Agent that efficiently discerns user intent and retrieves necessary information via APIs and knowledge bases to tackle financial tasks. Additionally, we propose an efficient multimodal information fusion method that enhances data sorting and organization, thereby improving LLMs' analytical quality. We conduct extensive experiments to demonstrate the effectiveness of our framework in financial analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T7qW9bLOQv": {
    "title": "LANS: A Layout-Aware Neural Solver for Plane Geometry Problem",
    "volume": "review",
    "abstract": "Geometry problem solving (GPS) is a challenging mathematical reasoning task requiring multi-modal understanding, fusion, and reasoning. Existing neural solvers take GPS as a vision-language task but are short in the representation of geometry diagrams that carry rich and complex layout information. In this paper, we propose a layout-aware neural solver named LANS, integrated with two new modules: multimodal layout-aware pre-trained language model (MLA-PLM) and layout-aware fusion attention (LA-FA). MLA-PLM adopts structural and semantic pre-training (SSP) to implement global relationship modeling, and point matching pre-training (PMP) to achieve alignment between visual points and textual points. LA-FA employs a layout-aware attention mask to realize point-guided cross-modal fusion for further boosting layout awareness of LANS. Extensive experiments on datasets Geometry3K and PGPS9K validate the effectiveness of the layout-aware modules and superior problem-solving performance of our LANS solver, over existing symbolic and neural solvers. The code will be made public available soon",
    "checked": true,
    "id": "e99de66608a3b060d54548b9e9a7c39961872cd7",
    "semantic_title": "lans: a layout-aware neural solver for plane geometry problem",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=IAIRi3uM95": {
    "title": "LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation",
    "volume": "review",
    "abstract": "Low-Rank Adaptation (LoRA) introduces auxiliary parameters for each layer to fine-tune the pre-trained model under limited computing resources. But it still faces challenges of resource consumption when scaling up to larger models. Previous studies employ pruning techniques by evaluating the importance of LoRA parameters for different layers to address the problem. However, these efforts only analyzed parameter features to evaluate their importance. Indeed, the output of LoRA related to the parameters and data is the factor that directly impacts the frozen model. To this end, we propose LoRA-drop which evaluates the importance of the parameters by analyzing the LoRA output. We retain LoRA for important layers and the LoRA of the other layers share the same parameters. Abundant experiments on NLU and NLG tasks demonstrate the effectiveness of LoRA-drop",
    "checked": true,
    "id": "6cd1a41a8cc8feadff889d5f9de4c2cf0f6e3bf3",
    "semantic_title": "lora-drop: efficient lora parameter pruning based on output evaluation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Wsr2eunEE3r": {
    "title": "Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning",
    "volume": "review",
    "abstract": "Deductive reasoning plays a pivotal role in the formulation of sound and cohesive arguments. It allows individuals to draw conclusions that logically follow, given the truth value of the information provided. Recent progress in the domain of large language models (LLMs) has showcased their capability in executing deductive reasoning tasks. Nonetheless, a significant portion of research primarily assesses the accuracy of LLMs in solving such tasks, often overlooking a deeper analysis of their reasoning behavior. In this study, we draw upon principles from cognitive psychology to examine inferential strategies employed by LLMs, through a detailed evaluation of their responses to propositional logic problems. Our findings indicate that LLMs display reasoning patterns akin to those observed in humans, including strategies like $\\textit{supposition following}$ or $\\textit{chain construction}$. Moreover, our research demonstrates that the architecture and scale of the model significantly affect its preferred method of reasoning, with more advanced models tending to adopt strategies more frequently than less sophisticated ones. Importantly, we assert that a model's accuracy, that is the correctness of its final conclusion, does not necessarily reflect the validity of its reasoning process. This distinction underscores the necessity for more nuanced evaluation procedures in the field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xloLa6Tdksr": {
    "title": "Learning Disentangled Semantic Spaces of Explanations via Invertible Neural Networks",
    "volume": "review",
    "abstract": "Disentangled latent spaces usually have better semantic separability and geometrical properties, which leads to better interpretability and more controllable data generation. While this has been well investigated in Computer Vision, in tasks such as image disentanglement, in the NLP domain, sentence disentanglement is still comparatively under-investigated. Most previous work have concentrated on disentangling task-specific generative factors, such as sentiment, within the context of style transfer. In this work, we focus on a more general form of sentence disentanglement, targeting the localised modification and control of more general sentence semantic features. To achieve this, we contribute to a novel notion of sentence semantic disentanglement and introduce a flow-based invertible neural network (INN) mechanism integrated with a transformer-based language Autoencoder (AE) in order to deliver latent spaces with better separability properties. Experimental results demonstrate that the model can conform the distributed latent space into a better semantically disentangled sentence space, leading to improved language interpretability and controlled generation when compared to the recent state-of-the-art language VAE models",
    "checked": true,
    "id": "4ff756807bf2011342e5909d3bc3744031025b9e",
    "semantic_title": "learning disentangled semantic spaces of explanations via invertible neural networks",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=O8jmCw5puG": {
    "title": "CIBench: Evaluating Your LLMs with a Code Interpreter Plugin",
    "volume": "review",
    "abstract": "While LLM-Based agents, which use external tools to solve complex problems, have made significant progress, benchmarking their ability is challenging, hindering a clear understanding of their limitations. In this paper, we propose an evaluation framework, named CIBench, to comprehensively assess LLMs' ability to utilize code interpreters for data science tasks. Our evaluation framework includes an evaluation dataset and two evaluation modes. The evaluation dataset is constructed using an LLM-human cooperative approach and simulates an authentic workflow by leveraging consecutive and interactive IPython sessions. The two evaluation modes assess LLMs' ability with and without human assistance. We conduct extensive experiments to analyze the ability of 19 LLMs on CIBench and provide valuable insights for future LLMs in data science tasks",
    "checked": true,
    "id": "d23d845a48d9c414e166123fc7651eb56e4d575a",
    "semantic_title": "cibench: evaluating your llms with a code interpreter plugin",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=knSmblNuAQ": {
    "title": "DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing",
    "volume": "review",
    "abstract": "Automated essay scoring (AES) is a useful tool in English as a Foreign Language (EFL) writing education, offering real-time essay scores for students and instructors. However, previous AES models were trained on essays and scores irrelevant to the practical scenarios of EFL writing education and usually provided a single holistic score due to the lack of appropriate datasets. In this paper, we release DREsS, a large-scale, standard dataset for rubric-based automated essay scoring. DREsS comprises three sub-datasets: DREsS_New, DREsS_Std., and DREsSCASE. We collect DREsS_New, a real-classroom dataset with 1.7K essays authored by EFL undergraduate students and scored by English education experts. We also standardize existing rubric-based essay scoring datasets as DREsS_Std. We suggest CASE, a corruption-based augmentation strategy for essays, which generates 20K synthetic samples of DREsS_CASE and improves the baseline results by 45.44%. DREsS will enable further research to provide a more accurate and practical AES system for EFL writing education",
    "checked": true,
    "id": "99eec76a1d90c7deaf49112e7853112a9f5c061a",
    "semantic_title": "dress: dataset for rubric-based essay scoring on efl writing",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=_M_6zwm9Yf": {
    "title": "SoFA: Shielded On-the-fly Alignment via Priority Rule Following",
    "volume": "review",
    "abstract": "The alignment problem in Large Language Models (LLMs) involves adapting them to the broad spectrum of human values. This requirement challenges existing alignment methods due to diversity of preferences and regulatory standards. This paper introduces a novel alignment paradigm, priority rule following, which defines rules as the primary control mechanism in each dialog, prioritizing them over user instructions. Our preliminary analysis reveals that even the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding and prioritizing the rules. Therefore, we present PriorityDistill, a semi-automated approach for distilling priority following signals from LLM simulations to ensure robust rule integration and adherence. Our experiments show that this method not only effectively minimizes misalignments utilizing only one general rule but also adapts smoothly to various unseen rules, ensuring they are shielded from hijacking and that the model responds appropriately",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IAv1cEayAj": {
    "title": "PG-Video-LLaVA: Pixel Grounding Large Video-Language Models",
    "volume": "review",
    "abstract": "Extending image-based Large Multimodal Models (LMMs) to videos is challenging due to the inherent complexity of video data. The recent approaches extending image-based LMMs to videos either lack the grounding capabilities (e.g., VideoChat, Video-ChatGPT, Video-LLaMA) or do not utilize the audio-signals for better video understanding (e.g., Video-ChatGPT). Addressing these gaps, we propose PG-Video-LLaVA, the first LMM with pixel-level grounding capability, integrating audio cues by transcribing them into text to enrich video-context understanding. Our framework uses an off-the-shelf tracker and a novel grounding module, enabling it to spatially localize objects in videos following user instructions. We evaluate PG-Video-LLaVA using video-based generative and question-answering benchmarks and introduce new benchmarks specifically designed to measure prompt-based object grounding performance in videos. Further, we propose using open-source Vicuna LLM for video-based conversation benchmarking, as opposed to GPT-3.5 utilized in Video-ChatGPT, ensuring reproducibility of results which is a concern with the proprietary nature of GPT-3.5. Our framework builds on SoTA image-based LLaVA model and extends its advantages to the video domain, delivering promising gains on video-based conversation and grounding tasks. Our codes, pretrained models, and interactive demos will be made publicly available",
    "checked": true,
    "id": "4edbb942c2d20a6f5a4e3caa763a9761be953231",
    "semantic_title": "pg-video-llava: pixel grounding large video-language models",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=S0Le0xDSIx": {
    "title": "A Multi-Task Embedder For Retrieval Augmented LLMs",
    "volume": "review",
    "abstract": "LLMs confront inherent limitations in terms of its knowledge, memory, and action. The retrieval augmentation stands as a vital mechanism to address these limitations, which brings in useful information from external sources to augment the LLM. However, existing retrieval methods encounter two pressing issues. On one hand, the general retrievers are not properly optimized for retrieval augmentation hence exhibit limited effectiveness; on the other hand, the task-specific retrievers excel in the targeted retrieval augmentation scenario, while lack the versatility to handle diverse scenarios. In this work, we propose LLM-Embedder for the unified support of diverse retrieval augmentation scenarios. Our method presents three technical contributions. Firstly, we introduce a new reward formulation, namely rank-aware reward. It exploits the ranking position of the desired output among N sampled outputs from the LLM, which leads to fine-grained and robust computation of reward from the LLM's feedback. Secondly, we design a novel distillation objective, called graded distillation. It incorporates both the absolute value and the relative order of the reward for more sufficient utilization of the LLM's feedback. Thirdly, we systematically optimize the multi-task learning, which effectively unifies the multiple retrieval functionalities into one model. In our experiment, LLM-Embedder substantially improves the LLM's performances in various downstream tasks, while introducing superior retrieval augmentation's effect over both general and task-specifc retrievers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ijrUpgTpje": {
    "title": "Mitigating Reversal Curse via Semantic-aware Permutation Training",
    "volume": "review",
    "abstract": "While large language models (LLMs) have achieved impressive performance across diverse tasks, recent studies showcase that causal LLMs suffer from the \"reversal curse\". It is a typical example that the model knows \"A's father is B\", but is unable to reason \"B's child is A\". This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models' ability to comprehend and apply bidirectional reasoning. In this paper, we first conduct substantial evaluation and identify that the root cause of the reversal curse lies in the different word order between the training and inference stage, namely, the poor ability of causal language models to predict antecedent words within the training data. Accordingly, permutation on the training data is considered as a potential solution, since this can make the model predict antecedent words or tokens. However, previous permutation methods may disrupt complete phrases or entities, thereby posing challenges for the model to comprehend and learn from training data. To address this issue, we propose Semantic-aware Permutation Training (SPT), which addresses this issue by segmenting the training sentences into semantic units (i.e., entities or phrases) with an assistant language model and permuting these units before feeding into the model. Extensive experiments demonstrate that SPT effectively mitigates the reversal curse since the performance on reversed questions approximates that on the forward ones, and significantly advances the performance of existing works",
    "checked": false,
    "id": "f3d9668c859c0307b87bc415b9b9fd3c1de65f44",
    "semantic_title": "mitigating reversal curse in large language models via semantic-aware permutation training",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CqLaEfAqrA": {
    "title": "Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated exceptional proficiency in instruction-following, becoming increasingly crucial across various applications. However, this capability brings with it the risk of prompt injection attacks, where attackers inject instructions into LLMs' input to elicit undesirable actions or content. Understanding the robustness of LLMs against such attacks is vital for their safe implementation. In this work, we establish a benchmark to evaluate the robustness of instruction-following LLMs against prompt injection attacks. Our objective is to determine (1) the extent to which LLMs can be influenced by injected instructions and (2) their ability to differentiate between these injected and original target instructions. Through extensive experiments with leading instruction-following LLMs, we uncover significant vulnerabilities in their robustness to such attacks. Our results indicate that some models are overly tuned to follow any embedded instructions in the prompt, overly focusing on the latter parts of the prompt without fully grasping the entire context. By contrast, models with a better grasp of the context and instruction-following capabilities will potentially be more susceptible to compromise by injected instructions. This underscores the need to shift the focus from merely enhancing LLMs' instruction-following capabilities to improving their overall comprehension of prompts and discernment of instructions that are appropriate to follow. We hope our in-depth analysis offers insights into the underlying causes of these vulnerabilities, aiding in the development of future solutions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M3AXjbrIOF": {
    "title": "The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs",
    "volume": "review",
    "abstract": "Large language models (LLMs) have recently experienced remarkable progress, where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities, leading to impressive performances in various multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs. In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from hallucination. To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the hallucination level given spurious images. This benchmark contains 7,308 text-image pairs across 13 categories. Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation results aid in better assessments of the MLLMs' robustness in the presence of misleading images",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tSdMUT6IPPj": {
    "title": "Efficient Sparse Attention needs Adaptive Token Release",
    "volume": "review",
    "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide array of text-centric tasks. However, their `large' scale introduces significant computational and storage challenges, particularly in managing the key-value states of the transformer, which limits their wider applicability.Therefore, we propose to adaptively release resources from caching and rebuild the necessary key-value states. Particularly, we accomplish this by a lighting controller module to approximate an ideal top-$K$ sparse attention. This module retains the tokens with the highest top-$K$ attention weights and simultaneously rebuilds the discarded tokens, which may become essential for future decoding.Comprehensive experiments in natural language generation and natural language modeling task reveal that our method is not only competitive with full attention in terms of performance but also achieves a significant throughput improvement of up to $\\textbf{221.8\\%}$.The code for replication is available on the https://anonymous.4open.science/r/ADORE-5384",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ITPxIvQebs": {
    "title": "Multilingual Instruction Tuning With Just a Pinch of Multilinguality",
    "volume": "review",
    "abstract": "As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial. In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages from the pre-training corpus. We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning. Furthermore, we find that only 40 multilingual examples integrated in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning. In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in multiple languages compared to monolingually tuned models, despite training on 10x fewer examples in those languages. Finally, we find that diversifying the instruction tuning set with even just 2-4 languages significantly improves cross-lingual generalization. Our results suggest that building massively multilingual instruction-tuned models can be done with only a very small set of multilingual instruction-responses",
    "checked": true,
    "id": "23c6706bc8ec8eae162ed8565e5b9ebd57ae00f0",
    "semantic_title": "multilingual instruction tuning with just a pinch of multilinguality",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=lRzwpVbFil": {
    "title": "CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language Models",
    "volume": "review",
    "abstract": "Cognitive dynamics, which refer to the evolution in human cognitive processes, are pivotal to advance human understanding of the world. Recent advancements in large language models (LLMs) highlight their potential for cognitive simulation. However, these LLM-based cognitive studies primarily focus on replicating human cognition in specific contexts, overlooking the inherently dynamic nature of cognition. To bridge this gap, we explore the cognitive dynamics of LLMs and present a corresponding task inspired by longitudinal studies. Toward the task, we develop CogBench, a novel benchmark to assess the cognitive dynamics of LLMs and validate it through participant surveys. We also design two evaluation metrics for CogBench, including Authenticity and Rationality. Recognizing the inherent static nature of LLMs, we further introduce CogGPT for the task, which features an innovative iterative cognitive mechanism to develop lifelong cognitive dynamics. Empirical results demonstrate the superiority of CogGPT over several existing methods, particularly in its ability to facilitate role-specific cognitive dynamics under continuous information flows. We will release the code and data to enable further research",
    "checked": true,
    "id": "c2eeef03f0c0d85237fe64b8da3a44d6170dbf32",
    "semantic_title": "coggpt: unleashing the power of cognitive dynamics on large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=c99y34NvWS": {
    "title": "LongAlign: A Recipe for Long Context Alignment of Large Language Models",
    "volume": "review",
    "abstract": "Extending large language models to effectively handle long contexts requires instruction fine-tuning on input sequences of similar length. To address this, we present LongAlign---a recipe of the instruction data, training, and evaluation for long context alignment. First, we construct a long instruction-following dataset using Self-Instruct. To ensure the data diversity, it covers a broad range of tasks from various long context sources. Second, we adopt the packing and sorted batching strategies to speed up supervised fine-tuning on data with varied length distributions. Additionally, we develop a loss weighting method to balance the contribution to the loss across different sequences during packing training. Third, we introduce the LongBench-Chat benchmark for evaluating instruction-following capabilities on queries of 10k-100k in length. Experiments show that LongAlign outperforms existing recipes for LLMs in long context tasks by up to 30%, while also maintaining their proficiency in handling short, generic tasks",
    "checked": true,
    "id": "ec9203f6c25a353325dd23ed38e5036b79d9e79b",
    "semantic_title": "longalign: a recipe for long context alignment of large language models",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=V8qau7Y2jL": {
    "title": "A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains",
    "volume": "review",
    "abstract": "Prompting language models to provide step-by-step answers (e.g., \"Chain-of-Thought\") is the prominent approach for complex reasoning tasks, where more accurate reasoning chains typically improve downstream task performance. Recent literature discusses automatic methods to verify reasoning to evaluate and improve their correctness. However, no fine-grained step-level datasets are available to enable thorough evaluation of such verification methods, hindering progress in this direction. We introduce REVEAL: Reasoning Verification Evaluation, a dataset to benchmark automatic verifiers of complex Chain-of-Thought reasoning in open-domain question-answering settings. REVEAL includes comprehensive labels for the relevance, attribution to evidence passages, and logical correctness of each reasoning step in a language model's answer, across a variety of datasets and state-of-the-art language models. Evaluation on REVEAL shows that verifiers struggle at verifying reasoning chains - in particular, verifying logical correctness and detecting contradictions",
    "checked": true,
    "id": "0f51d47871d99cda3e9eaf4ae1c9c7025ae76325",
    "semantic_title": "a chain-of-thought is as strong as its weakest link: a benchmark for verifiers of reasoning chains",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=OJ_Wsb5gKk": {
    "title": "Unified Interpretation of Smoothing Methods for Negative Sampling Loss Functions in Knowledge Graph Embedding",
    "volume": "review",
    "abstract": "Knowledge Graphs (KGs) are fundamental resources in knowledge-intensive tasks in NLP. Due to the limitation of manually creating KGs, KG Completion (KGC) has an important role in automatically completing KGs by scoring their links with KG Embedding (KGE). To handle many entities in training, KGE relies on Negative Sampling (NS) loss that can reduce the computational cost by sampling. Since the appearance frequencies for each link are at most one in KGs, sparsity is an essential and inevitable problem. The NS loss is no exception. As a solution, the NS loss in KGE relies on smoothing methods like Self-Adversarial Negative Sampling (SANS) and subsampling. However, it is uncertain what kind of smoothing method is suitable for this purpose due to the lack of theoretical understanding. This paper provides theoretical interpretations of the smoothing methods for the NS loss in KGE and induces a new NS loss, Triplet Adaptive Negative Sampling (TANS), that can cover the characteristics of the conventional smoothing methods. Experimental results of TransE, DistMult, ComplEx, RotatE, HAKE, and HousE on FB15k-237, WN18RR, and YAGO3-10 datasets and their sparser subsets show the soundness of our interpretation and performance improvement by our TANS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LpB-nBam73": {
    "title": "PLEM: Prototype Learning with Evidence Match for Improving Few-Shot Document-Level Relation Extraction",
    "volume": "review",
    "abstract": "Few-shot document-level relation extraction (FSDLRE) aims to develop a model with the ability to generalize to new categories in the context of document-level relation extraction, using a small number of support samples. Among others, metric based meta-learning methods are widely used in FSDLRE, which involve constructing class prototypes using the contextual representation of the entire document and the representation of entity pairs for relation classification. However, in relation classification, only a subset of sentences in a document, known as evidence, is required to determine the relationship category of entity pairs. In this paper, we propose a prototype learning method with evidence match (PLEM). By introducing an evidence matching auxiliary task in the process of relation prototype construction, the model is guided to focus more on the semantics of evidence sentences when building prototypes, thereby enhancing the relation prototypes. We further design task-specific evidence prototypes, enabling the model to adapt to the evidence semantic space of different relation categories. Extensive experimental results demonstrate that PLEM outperforms the state-of-the-art methods, achieving an average improvement of 1.23% in Macro F1 across various settings of two FSDLRE benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yshGZsuM0_": {
    "title": "OneNet: A Fine-Tuning Free Framework for Few-Shot Entity Linking via Large Language Model Prompting",
    "volume": "review",
    "abstract": "Entity Linking (EL) is the process of associating ambiguous textual mentions to specific entities in a knowledge base. Traditional EL methods heavily rely on large datasets to enhance their performance, a dependency that becomes problematic in the context of few-shot entity linking, where only a limited number of examples are available for training. To address this challenge, we present OneNet, an innovative framework that utilizes the few-shot learning capabilities of Large Language Models (LLMs) without the need for fine-tuning. To the best of our knowledge, this marks a pioneering approach to applying LLMs to few-shot entity linking tasks. OneNet is structured around three key components prompted by LLMs: (1) an entity reduction processor that simplifies inputs by summarizing and filtering out irrelevant entities, (2) a dual-perspective entity linker that combines contextual cues and prior knowledge for precise entity linking, and (3) an entity consensus judger that employs a unique consistency algorithm to alleviate the hallucination in the entity linking reasoning. Comprehensive evaluations across six benchmark datasets reveal that OneNet outperforms current state-of-the-art entity linking methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f7r8pMQ9Up": {
    "title": "You Only Look at Screens: Multimodal Chain-of-Action Agents",
    "volume": "review",
    "abstract": "Autonomous graphical user interface (GUI) agents aim to facilitate task automation by interacting with the user interface without manual intervention. Recent studies have investigated eliciting the capabilities of large language models (LLMs) for effective engagement in diverse environments. To align with the input-output requirement of LLMs, most existing approaches are developed under a sandbox setting where they rely on external tools and application-specific APIs to parse the environment into textual elements and interpret the predicted actions. Consequently, those approaches often grapple with inference inefficiency and error propagation risks. To mitigate the challenges, we introduce Auto-GUI, a multimodal solution that directly interacts with the interface, bypassing the need for environment parsing or reliance on application-dependent APIs. Moreover, we propose a chain-of-action technique---leveraging a series of intermediate previous action histories and future action plans---to help the agent decide what action to execute. We evaluate our approach on a new device-control benchmark AITW with 30$K$ unique instructions, spanning multi-step tasks such as application operation, web searching, and web shopping. Experimental results show that Auto-GUI achieves state-of-the-art performance with an action type prediction accuracy of 90\\% and an overall action success rate of 74\\%. Code is publicly available at \\texttt{Anonymous}",
    "checked": true,
    "id": "6ab33b17cd45e7cbd2cb9b0c5a2d56e5eac1c814",
    "semantic_title": "you only look at screens: multimodal chain-of-action agents",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=IFQWebjZ2Su": {
    "title": "Span-based Named Entity Recognition using Attention-based Vector Aggregation",
    "volume": "review",
    "abstract": "Named entity recognition (NER) is a fundamental task in natural language processing and is widely implemented in natural language processing systems. Conventionally, the most common method for NER has been sequential labelling utilizing conditional random fields (CRFs). In recent years, span-based methods that input a group of multiple words and determine whether the input group is an NE have also been used. In this paper, we propose a span-based NER method that utilizes attention-based aggregation to combine information from multiple words into a single vector. Experimental results demonstrate that our method achieves a state-of-the-art performance for the WNUT-16 and WNUT-17 datasets commonly used in NER",
    "checked": false,
    "id": "bf5ed12e89b9fd8f56bd28bc8f3ddc68ed3df4e0",
    "semantic_title": "cross-lingual named entity recognition based on attention and adversarial training",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=hOqN0RFzA1": {
    "title": "DetectiveNN: Imitating Human Emotional Reasoning with a Recall-Detect-Predict Framework for Emotion Recognition in Conversations",
    "volume": "review",
    "abstract": "Recognizing emotions in conversations involves an internal cognitive process that interprets emotional cues by using a collection of past emotional experiences. However, many existing methods struggle to decipher emotional cues in dialogues due to their models' lack of capacity for cognitive reasoning. In this work, we introduce an innovative Detective Network (DetectiveNN), a novel model that is grounded in the cognitive theory of emotion and utilizes a \"recall-detect-predict\" framework to imitate human emotional reasoning. This process begins by `recalling' past interactions of a specific speaker to collect emotional cues. It then `detects' relevant emotional patterns by interpreting these cues in the context of the ongoing conversation. Finally, it `predicts' the speaker's emotional state in the next moment. Tested on three benchmark datasets, our approach outperforms existing methods. This highlights the advantages of incorporating cognitive factors into deep learning, enhancing task efficiency and prediction accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FItO0tAQIa": {
    "title": "Incremental Sequence Labeling: A Tale of Two Shifts",
    "volume": "review",
    "abstract": "The incremental sequence labeling task involves continuously learning new classes over time while retaining knowledge of the previous ones. Our investigation identifies two significant semantic shifts: E2O (where the model mislabels an old entity as a non-entity) and O2E (where the model labels a non-entity or old entity as a new entity). Previous research has predominantly focused on addressing the E2O problem, neglecting the O2E issue. This negligence results in a model bias towards classifying new data samples as belonging to the new class during the learning process. To address these challenges, we propose a novel framework, Incremental Sequential Labeling without Semantic Shifts (IS3). Motivated by the identified semantic shifts (E2O and O2E), IS3 aims to mitigate catastrophic forgetting in models. As for the E2O problem, we use knowledge distillation to maintain the model's discriminative ability for old entities. Simultaneously, to tackle the O2E problem, we alleviate the model's bias towards new entities through debiased loss and optimization levels.Our experimental evaluation, conducted on three datasets with various incremental settings, demonstrates the superior performance of IS3 compared to the previous state-of-the-art method by a significant margin",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AgWI_4Eq2k": {
    "title": "An Information Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented Generation",
    "volume": "review",
    "abstract": "Retrieval-augmented generation integrates the capabilities of large language models with relevant information retrieved from an extensive corpus, yet encounters challenges when confronted with real-world noisy data. One recent solution is to train a filter module to find relevant content but only achieve suboptimal noise compression. In this paper, we propose to introduce the information bottleneck theory into retrieval-augmented generation. Our approach involves the filtration of noise by simultaneously maximizing the mutual information between compression and ground output, while minimizing the mutual information between compression and retrieved passage. In addition, we derive the formula of information bottleneck to facilitate its application in novel comprehensive evaluations, the selection of supervised fine-tuning data, and the construction of reinforcement learning rewards. Experimental results demonstrate that our approach achieves significant improvements across various question answering datasets, not only in terms of the correctness of answer generation but also in the conciseness with $2.5\\%$ compression rate",
    "checked": true,
    "id": "3529f8c04273367260aa3292a6cef779d821fe14",
    "semantic_title": "an information bottleneck perspective for effective noise filtering on retrieval-augmented generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mkWyOlOkst": {
    "title": "$Phonotactic probabilities in Mandarin syllables$",
    "volume": "review",
    "abstract": "In this study, a spoken data sample is computed via frequency-based probabilistic phonotactics in Mandarin syllables by categorizing the spoken dataset into 12 syllable structure types. Phonotactic probabilities are measured by the bigram or biphone frequencies with which phonological segments and phone sequences occur in word types in Mandarin. Spoken data drawn from 2,384,567 lexical items show that correlations between speech production measures and phonological/articulatory complexity are not found. Instead, phonotactic probabilities influence speech production processes in Mandarin speakers independent of phonological complexity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sWWEW1mj2w": {
    "title": "DocLLM: A Layout-Aware Generative Language Model for Multimodal Document Understanding",
    "volume": "review",
    "abstract": "Enterprise documents such as forms, receipts, reports, and other such records, often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper, we present DocLLM, a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents, taking into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and spatial modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices. Furthermore, we devise a pre-training objective that learns to infill text segments. This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents. The pre-trained model is fine-tuned using a large-scale instruction dataset, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks, and generalizes well to 4 out of 5 previously unseen datasets",
    "checked": true,
    "id": "575f403261d5f99526f0b4dfc8644352d6c4467a",
    "semantic_title": "docllm: a layout-aware generative language model for multimodal document understanding",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=VjXjUL7HyM": {
    "title": "Can Large Language Models Detect Rumors on Social Media?",
    "volume": "review",
    "abstract": "In this work, we investigate to use Large Language Models (LLMs) for rumor detection on social media. However, it is challenging for LLMs to reason over the entire propagation information on social media, which contains news contents and numerous comments, due to LLMs may not concentrate on key clues in the complex propagation information, and have trouble in reasoning when facing massive and redundant information. Accordingly, we propose an LLM-empowered Rumor Detection (LeRuD) approach, in which we design prompts to teach LLMs to reason over important clues in news and comments, and divide the entire propagation information into a Chain-of-Propagation for reducing LLMs' burden. We conduct extensive experiments on the Twitter and Weibo datasets, and LeRuD outperforms several state-of-the-art rumor detection models by 3.2% to 7.7%. Meanwhile, by applying LLMs, LeRuD requires no data for training, and thus shows more promising rumor detection ability in few-shot or zero-shot scenarios",
    "checked": true,
    "id": "6ae2a428d149a71e0d7bf747dee97aad21eb1551",
    "semantic_title": "can large language models detect rumors on social media?",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=cI_k6IbEYu": {
    "title": "Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural Language Understanding",
    "volume": "review",
    "abstract": "Recent strides in large language models (LLMs) have yielded remarkable performance, leveraging reinforcement learning from human feedback (RLHF) to significantly enhance generation and alignment capabilities. However, RLHF encounters numerous challenges, including the objective mismatch issue, leading to suboptimal performance in Natural Language Understanding (NLU) tasks.To address this limitation, we propose a novel Reinforcement Learning framework enhanced with Label-sensitive Reward (RLLR) to amplify the performance of LLMs in NLU tasks. By incorporating label-sensitive pairs into reinforcement learning, our method aims to adeptly capture nuanced label-sensitive semantic features during RL, thereby enhancing natural language understanding.Experiments conducted on five diverse foundation models across eight tasks showcase promising results. In comparison to Supervised Fine-tuning models (SFT), RLLR demonstrates an average performance improvement of 1.54%. Compared with RLHF models, the improvement averages at 0.69%. These results reveal the effectiveness of our method for LLMs in NLU tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wxnxnfIwAt": {
    "title": "MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering",
    "volume": "review",
    "abstract": "Recent advances in few-shot question answering (QA) mostly rely on the power of pre-trained large language models (LLMs) and fine-tuning in specific settings. Although the pre-training stage has already equipped LLMs with powerful reasoning capabilities, LLMs still need to be fine-tuned to adapt to specific domains to achieve the best results. In this paper, we propose to select the most informative data for fine-tuning, thereby improving the efficiency of the fine-tuning process with comparative or even better accuracy on the open-domain QA task. We present MinPrompt, a minimal data augmentation framework for open-domain QA based on an approximate graph algorithm and unsupervised question generation. We transform the raw text into a graph structure to build connections between different factual sentences, then apply graph algorithms to identify the minimal set of sentences needed to cover the most information in the raw text. We then generate QA pairs based on the identified sentence subset and train the model on the selected sentences to obtain the final model. Empirical results on several benchmark datasets and theoretical analysis show that MinPrompt is able to achieve comparable or better results than baselines with a high degree of efficiency, bringing consistent improvements in F-1 scores",
    "checked": true,
    "id": "39a9beda08f4bf33d800fa6bf2e9fdde12d0a118",
    "semantic_title": "minprompt: graph-based minimal prompt data augmentation for few-shot question answering",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=fDv3k-xEZr": {
    "title": "SimTeG: A Frustratingly Simple Approach Improves Textual Graph Learning",
    "volume": "review",
    "abstract": "Textual graphs (TGs) are graphs whose nodes correspond to text (sentences or documents), which are widely prevalent. The representation learning of TGs involves two stages: \\((i)\\) \\textit{unsupervised feature extraction} and \\((ii)\\) \\textit{supervised graph representation learning}. In recent years, extensive efforts have been devoted to the latter stage, where Graph Neural Networks (GNNs) have dominated. However, the former stage for most existing graph benchmarks still relies on traditional feature engineering techniques. This motivates us to investigate the outcomes of enhancing only the text embeddings in benchmark models. While it is anticipated that advanced text embeddings will boost GNN performance, key questions remain underexplored: the extent of this improvement, particularly how advanced text features can enhance a \\textit{rudimentary} GNN architecture.} Therefore, in this work, we investigate the impact of enhancing benchmark text embeddings exclusively and evaluate it on two fundamental graph representation learning tasks: \\textit{node classification} and \\textit{link prediction}. Through extensive experiments, we show that better text embeddings significantly improves the performance of various GNNs \\textit{especially basic GNN baselines}, on multiple graph benchmarks. Remarkably, when additional supporting text provided by large language models (LLMs) is included, \\textit{a simple two-layer GraphSAGE} trained on an ensemble of text embeddings achieves an accuracy of 77.48\\% on \\texttt{OGBN-Arxiv}, comparable to state-of-the-art (SOTA) performance obtained from far more complicated GNN architectures. We will release our code and generated node features soon",
    "checked": true,
    "id": "303b7d0a81395562e3a46578a89d6821ce564a8b",
    "semantic_title": "simteg: a frustratingly simple approach improves textual graph learning",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=xJq-ssDSB1": {
    "title": "On the Robustness of Document-Level Relation Extraction Models to Entity Name Variations",
    "volume": "review",
    "abstract": "Driven by the demand for cross-sentence and large-scale relation extraction, document-level relation extraction (DocRE) has attracted increasing research interest. Despite the continuous improvement in performance, we find that existing DocRE models which initially perform well may make more mistakes when merely changing the entity names in the document, hindering the generalization to novel entity names. To this end, we systematically investigate the robustness of DocRE models to entity name variations in this work. We first propose a principled pipeline to generate entity-renamed documents by replacing the original entity names with names from Wikidata. By applying the pipeline to DocRED and Re-DocRED datasets, we construct two novel benchmarks named Env-DocRED and Env-Re-DocRED for robustness evaluation. Experimental results show that both three representative DocRE models and two LLM-based in-context learning methods consistently lack sufficient robustness to entity name variations. Finally, we propose an entity variation robust training method which not only effectively improves the robustness of DocRE models but also enhances their understanding and reasoning capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0Nw5GEmQUIf": {
    "title": "Strengthened Symbol Binding Makes Large Language Models Reliable Multiple-Choice Selectors",
    "volume": "review",
    "abstract": "Multiple-Choice Questions (MCQs) constitute a critical area of research in the study of Large Language Models (LLMs). Previous works have investigated the selection bias problem in MCQs within few-shot scenarios, in which the LLM's performance may be influenced by the presentation of answer choices, leaving the selection bias during Supervised Fine-Tuning (SFT) unexplored. In this paper, we reveal that selection bias persists in the SFT phase of LLMs, primarily due to the LLM's inadequate Multiple Choice Symbol Binding (MCSB) capability. This limitation implies that the model struggles to associate the answer options with their corresponding symbols effectively. To enhance the model's MCSB capability, we first incorporate option contents into the loss function and subsequently adjust the weights of the option symbols and contents, guiding the model to understand the option content of the current symbol. Based on this, we introduce an efficient SFT algorithm for MCQs, termed Point-wise Intelligent Feedback (PIF). PIF constructs negative instances by randomly combining the incorrect option contents with all candidate option symbols, and proposes a point-wise loss to provide feedback on these negative samples into LLMs. Our experimental results demonstrate that PIF significantly reduces the model's selection bias by improving its MCSB capability. Remarkably, PIF exhibits a substantial enhancement in the accuracy for MCQs",
    "checked": true,
    "id": "a2494949a7061e8fe8343af44bc1edb00973726f",
    "semantic_title": "strengthened symbol binding makes large language models reliable multiple-choice selectors",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=m83Vcel5Sez": {
    "title": "X-Instruction: Aligning Language Model in Low-resource Languages with Self-curated Cross-lingual Instructions",
    "volume": "review",
    "abstract": "Large language models respond well in high-resource languages like English but struggle in low-resource languages. It may arise from the lack of high-quality instruction following data in these languages. Directly translating English samples into these languages can be a solution but unreliable, leading to responses with translation errors and lacking language-specific or cultural knowledge. To address this issue, we propose a novel method to construct cross-lingual instruction following samples with instruction in English and response in low-resource languages. Specifically, the language model first learns to generate appropriate English instructions according to the natural web texts in other languages as responses. The candidate cross-lingual instruction tuning samples are further refined and diversified. We have employed this method to build a large-scale cross-lingual instruction tuning dataset on 10 languages, namely X-Instruction. The instruction data built using our method incorporate more language-specific knowledge compared with the naive translation method. Experimental results have shown that the response quality of the model tuned on X-Instruction greatly exceeds the model distilled from a powerful teacher model, reaching or even surpassing the ones of ChatGPT. In addition, we find that models tuned on cross-lingual instruction following samples can follow the instruction in the output language without further tuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pMzOlw7NxH": {
    "title": "Developing A Novel Bidirectional Sparse Graph Attention Adaptor for Evidence-Based Fact-Checking",
    "volume": "review",
    "abstract": "Evidence-based Fact-checking aims to verify or debunk a claim with evidence given and has benefited from Large-Language-Model (LLM) advancements in text understanding. However, autoregressive LLMs suffer from their unidirectional nature, known as ``Reversal Curse'', causing their performance to be unsatisfactory. Therefore, in this paper, we propose to utilize bidirectional attention as an external adapter for two-way information aggregation. Further, we leverage hierarchical sparse graphs to reduce the noise impact of attention and an efficient feature-compression mechanism to reduce the number of adaptor parameters. Experimental results on both English and Chinese datasets demonstrate the significant improvements achieved by our proposed approach and its state-of-the-art performance in the Evidence-based Fact-checking task. The code will be available on GitHub",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NDLcUDNnOkR": {
    "title": "BvSP: Broad-view Soft Prompting for Few-Shot Aspect Sentiment Quad Prediction",
    "volume": "review",
    "abstract": "Aspect sentiment quad prediction (ASQP) aims to predict four aspect-based elements, including aspect term, opinion term, aspect category, and sentiment polarity. In practice, unseen aspects, due to distinct data distribution, impose many challenges for a trained neural model. Motivated by this, this work formulates ASQP into the few-shot scenario, which aims for fast adaptation in real applications. Therefore, we first construct a few-shot ASQP dataset (FSQP) that contains richer categories and is more balanced for the few-shot study. Moreover, recent methods extract quads through a generation paradigm, which involves converting the input sentence into a templated target sequence. However, they primarily focus on the utilization of a single template or the consideration of different template orders, thereby overlooking the correlations among various templates. To tackle this issue, we further propose a Broad-view Soft Prompting (BvSP) method that aggregates multiple templates with a broader view by taking into account the correlation between the different templates. Specifically, BvSP uses the pre-trained language model to select the most relevant k templates with Jensenâ€“Shannon divergence. BvSP further introduces soft prompts to guide the pre-trained language model using the selected templates. Then, we aggregate the results of multi-templates by voting mechanism. Empirical results demonstrate that BvSP significantly outperforms the state-of-the-art methods under four few-shot settings and other public datasets. Our code and dataset are available at https://anonymous.4open.science/r/BvSP-2E11/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BsbnA7AZhth": {
    "title": "Dataflow-Guided Retrieval Augmentation for Repository-Level Code Completion",
    "volume": "review",
    "abstract": "Recent years have witnessed the deployment of code language models (LMs) in various code intelligence tasks such as code completion. Yet, it is challenging for pre-trained LMs to generate correct completions in private repositories. Previous studies retrieve cross-file context based on import relations or text similarity, which is insufficiently relevant to completion targets. In this paper, we propose a dataflow-guided retrieval augmentation approach, called DraCo, for repository-level code completion. DraCo parses a private repository into code entities and establishes their relations through an extended dataflow analysis, forming a repo-specific context graph. Whenever triggering code completion, DraCo precisely retrieves relevant background knowledge from the repo-specific context graph and generates well-formed prompts to query code LMs. Furthermore, we construct a large Python dataset, ReccEval, with more diverse completion targets. Our experiments demonstrate the superior accuracy and applicable efficiency of DraCo, improving code exact match by 3.43% and identifier F1-score by 3.27% on average compared to the state-of-the-art approach",
    "checked": true,
    "id": "e03f41877da45c04a38aa37af99cfdec9a0379dd",
    "semantic_title": "dataflow-guided retrieval augmentation for repository-level code completion",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=OSvk-fMhqf": {
    "title": "Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences",
    "volume": "review",
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated proficiency in handling a variety of visual-language tasks. However, current MLLM benchmarks are predominantly designed to evaluate reasoning based on static information about a single image, and the ability of modern MLLMs to extrapolate from image sequences, which is essential for understanding our ever-changing world, has been less investigated. To address this challenge, this paper introduces Mementos, a new benchmark designed to assess MLLMs' sequential image reasoning abilities. Mementos features 4,761 diverse image sequences with varying lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning performance. Through a careful evaluation of nine recent MLLMs on Mementos, including GPT-4V and Gemini, we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors. Our quantitative analysis and case studies identify three key factors impacting MLLMs' sequential image reasoning: the correlation between object and behavioral hallucinations, the influence of co-occurring behaviors, and the compounding impact of behavioral hallucinations",
    "checked": true,
    "id": "e0702a22e0841c54ab865b4996d7b07af192a3e1",
    "semantic_title": "mementos: a comprehensive benchmark for multimodal large language model reasoning over image sequences",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=IS4CPvDgTn": {
    "title": "Unsupervised Domain Adaptation for Keyphrase Generation using Citation Contexts",
    "volume": "review",
    "abstract": "Adapting keyphrase generation models to new domains typically involves few-shot fine-tuning with in-domain labeled data. However, annotating documents with keyphrases is often prohibitively expensive and impractical, requiring expert annotators. This paper presents silk, an unsupervised method designed to address this issue by extracting silver-standard keyphrases from citation contexts to create synthetic labeled data for domain adaptation. Extensive experiments across three distinct domains demonstrate that our method yields high-quality synthetic samples, resulting in significant and consistent improvements in in-domain performance over strong baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kf1yTkWnTLH": {
    "title": "Deep Learning-Based Knowledge Injection for Metaphor Detection: A Comprehensive Review",
    "volume": "review",
    "abstract": "Metaphor as an advanced cognitive modality works by extracting familiar concepts in the target domain in order to understand vague and abstract concepts in the source domain. This helps humans to quickly understand and master new domains and thus adapt to changing environments. With the continuous development of metaphor research in the natural language community, many studies using knowledge-assisted models to detect textual metaphors have emerged in recent years. Compared to not using knowledge, systems that introduce various kinds of knowledge achieve greater performance gains and reach SOTA in a recent study. Based on this, the goal of this paper is to provide a comprehensive review of research advances in the application of deep learning for knowledge injection in metaphor detection tasks. We will first systematically summarize and generalize the mainstream knowledge and knowledge injection principles. Then, the datasets, evaluation metrics, and benchmark models used in metaphor detection tasks are examined. Finally, we explore the current issues facing knowledge injection methods and provide an outlook on future research directions",
    "checked": true,
    "id": "3f6acc800217389a3270cb200b3485ea0bf0132a",
    "semantic_title": "deep learning-based knowledge injection for metaphor detection: a comprehensive review",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gI6HS2G1u9Y": {
    "title": "MolTC: Towards Molecular Relational Modeling In Language Models",
    "volume": "review",
    "abstract": "Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research. Recently, the adoption of large language models (LLMs), known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL. Despite their potential, these methods predominantly rely on textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs.Moreover, the absence of a unified framework exacerbates the issue of insufficient data exploitation, as it hinders the sharing of interaction mechanism learned across various datasets. To address these challenges, this work proposes a novel LLM-based multi-modal framework for molecular interaction modeling following Chain-of-Thought (CoT) theory, termed MolTC, which effectively integrate graphical information of two molecules in pair. For achieving a unified training paradigm, MolTC innovatively develops a dynamic parameter-sharing strategy for cross-dataset information exchange. Moreover, to train this integrated framework efficiently, we introduce a multi-hierarchical CoT theory to refine its training paradigm, and conduct a comprehensive molecular Interactive Instructions dataset for the development of biochemical LLMs involving MRL. Ourexperiments,conducted across various datasets involving over 4,000,000 molecular pairs, exhibit the superiority of our method over current GNN and LLM-based baselines. Code is available at https://anonymous.4open.science/r/MolTC-F",
    "checked": true,
    "id": "81448c69a0b900f3721596c635c849987eec1a4b",
    "semantic_title": "moltc: towards molecular relational modeling in language models",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=T2bxJzLpJI": {
    "title": "Dual Grained Quantization: efficient fine-grained quantization for LLM",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) demonstrate considerable potential across a range of tasks; however, they pose significant challenges due to their extensive memory requirements and computational demands. Fine-grained quantization effectively preserves model performance during aggressive weight compression, yet its inefficiency on hardware platforms hinders its applicability in real-world production environments. To enhance hardware efficiency while preserving the performance of fine-grained quantization, we propose a novel quantization framework, Dual Grained Quantization (DGQ), employing a W4A8 configuration specifically tailored for LLMs. By employing a dual-phase search strategy, DGQ minimizes quantization error without significantly extending quantization time. To improve the accuracy of W4A8-configured LLMs, we introduce aggressive selective equalization. This approach is grounded in the observation that key weights and outliers frequently coexist within the same channels. Comprehensive experiments with our W4A8 CUDA kernel highlight DGQ's exceptional performance, delivering speedups of 1.37$\\times$ and 2.5$\\times$ over standard INT8 and FP16 kernels, respectively, while preserving the superior performance of fine-grained quantization",
    "checked": true,
    "id": "74f614d1130f0bc50db3a54abe0d85eefaad894b",
    "semantic_title": "dual grained quantization: efficient fine-grained quantization for llm",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=T-NQFdcpuIx": {
    "title": "Neighbors Always Help: A Relation-aware Knowledge Reasoning Model for Inductive Relation Prediction",
    "volume": "review",
    "abstract": "Inductive relation prediction, an important task for knowledge graph completion, is to predict the relations between entities that are unseen at the training stage. The latest methods use pre-trained language models (PLMs) to encode the paths between the head entity and tail entity and achieve state-of-the-art prediction performance. However, these methods cannot well handle no-path situations and are also unable to learn comprehensive representations for different relations to overcome the difficulty of inductive relation prediction. To tackle this issue, we propose a novel \\textbf{R}elation-\\textbf{a}ware \\textbf{k}nowledg\\textbf{e} \\textbf{r}easoning model entitled Raker which develops an adaptive reasoning information extraction method to identify relation-aware reasoning neighbors of entities in the target triple to handle no-path situations, and enables PLMs to be aware of the predicted relation by the relation-specific soft prompting. Raker is evaluated on three public datasets and achieves SOTA performance in inductive relation prediction when compared with the baseline methods. Notably, the absolute improvement of Raker is even more than 10\\% on the FB15k-237 inductive setting. Moreover, Raker also demonstrates its superiority in both transductive and few-shot settings. The code of Raker will be publicly available after the double-blind review process",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DVb2O5oe71z": {
    "title": "PlatoLM: Teaching LLMs in Multi-Round Dialogue via a User Simulator",
    "volume": "review",
    "abstract": "The unparalleled performance of closed-sourced ChatGPT has sparked efforts towards its democratization, with notable strides made by leveraging real user and ChatGPT conversations, as evidenced by Vicuna. However, due to challenges in gathering conversations involving human participation, current endeavors like Baize and UltraChat rely on ChatGPT conducting roleplay to simulate humans based on instructions, resulting in overdependence on seeds, diminished human-likeness, limited topic diversity, and an absence of genuine multi-round conversational dynamics. To address the above issues, we propose a paradigm to simulate human behavior better and explore the benefits of incorporating more human-like questions in multi-turn conversations. Specifically, we directly target human questions extracted from genuine human-machine conversations as a learning goal and provide a novel user simulator called `Socratic`. The experimental results show our response model, `PlatoLM`, achieves SOTA performance among LLaMA-based 7B models in MT-Bench. Our findings further demonstrate that our method introduces highly human-like questioning patterns, which can teach the response model better than previous works in multi-round conversations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sC8rrMGUUhD": {
    "title": "Assessing the Reasoning Abilities of ChatGPT in the Context of Claim Verification",
    "volume": "review",
    "abstract": "The reasoning capabilities of LLMs are currently hotly debated. We examine the issue from the perspective of claim/rumour verification. We propose the first logical reasoning framework designed to break down any claim or rumor paired with evidence into the atomic reasoning steps necessary for verification. Based on our framework, we curate two annotated collections of such claim/evidence pairs: a synthetic dataset from Wikipedia and a real-world set stemming from rumours circulating on Twitter. We use them to evaluate the reasoning capabilities of GPT-3.5-Turbo and GPT-4 (hereinafter referred to as ChatGPT) within the context of our framework, providing a thorough analysis. Our results show that ChatGPT struggles in abductive reasoning, although this can be somewhat mitigated by using manual Chain of Thought (CoT) as opposed to Zero Shot (ZS) and ZS CoT approaches. Our study contributes to the growing body of research suggesting that ChatGPT's reasoning processes are unlikely to mirror human-like reasoning, and that LLMs need to be more rigorously evaluated in order to distinguish between hype and actual capabilities, especially in high stake real-world tasks such as claim verification",
    "checked": true,
    "id": "4243f5e3e2fb405aba2648512eb950c06a4fd900",
    "semantic_title": "assessing the reasoning abilities of chatgpt in the context of claim verification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WFiJJM61vM": {
    "title": "ProtLLM: An Interleaved Protein-Language LLM with Protein-as-Word Pre-Training",
    "volume": "review",
    "abstract": "We propose ProtLLM, a versatile cross-modal large language model (LLM) for both protein-centric and protein-language tasks. ProtLLM features a unique dynamic protein mounting mechanism, enabling it to handle complex inputs where the natural language text is interspersed with an arbitrary number of proteins. Besides, we propose the protein-as-word language modeling approach to train ProtLLM. By developing a specialized protein vocabulary, we equip the model with the capability to predict not just natural language but also proteins from a vast pool of candidates. Additionally, we construct a large-scale interleaved protein-text dataset, named InterPT, for pre-training. This dataset comprehensively encompasses both (1) structured data sources like protein annotations and (2) unstructured data sources like biological research papers, thereby endowing ProtLLM with crucial knowledge for understanding proteins. We evaluate ProtLLM on classic supervised protein-centric tasks and explore its novel protein-language applications. Experimental results demonstrate that ProtLLM not only achieves superior performance against protein-specialized baselines on protein-centric tasks but also induces zero-shot and in-context learning capabilities on protein-language tasks. Our data and models will be publicly available",
    "checked": true,
    "id": "631066c55a852fac7b9c9129e166550a6310fa3c",
    "semantic_title": "protllm: an interleaved protein-language llm with protein-as-word pre-training",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Sk7NWnNeeEx": {
    "title": "Gender Bias in News Summarization: Measures, Pitfalls and Corpora",
    "volume": "review",
    "abstract": "Summarization is an important application of large language models (LLMs). Most previous evaluation of summarization models has focused on their performance in content selection, faithfulness, grammaticality and coherence. However, it is well known that LLMs reproduce and reinforce harmful social biases. This raises the question: Do these biases affect model outputs in a relatively constrained setting like summarization?To help answer this question, we first motivate and introduce a number of definitions for biased behaviours in summarization models, along with practical operationalizations. Since we find that biases inherent to input documents can confound bias analysis in summaries, we propose a method to generate input documents with carefully controlled demographic attributes. This allows us to study summarizer behavior in a controlled setting, while still working with realistic input documents.Finally, we measure gender bias in English summaries generated by both purpose-built summarization models and general purpose chat models as a case study. We find content selection in single document summarization to be largely unaffected by gender bias, while hallucinations exhibit evidence of downstream biases in summarization",
    "checked": false,
    "id": "635534fb5a0a9bbe286f175b02213d71dd575d32",
    "semantic_title": "bias in news summarization: measures, pitfalls and corpora",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_kbBD6A6n_": {
    "title": "Do We Need Language-Specific Fact-Checking Models? The Case of Chinese",
    "volume": "review",
    "abstract": "This paper investigates the potential benefits of language-specific fact-checking models, focusing on the case of Chinese. We first demonstrate the limitations of translation-based methods and multilingual large language models (e.g., GPT-4), highlighting the need for language-specific systems. We further propose a Chinese fact-checking system that can better retrieve evidence from a document by incorporating context information. To better analyze token-level biases in different systems, we construct an adversarial dataset based on the CHEF dataset, where each instance has large word overlap with the original one but holds the opposite veracity label. Experimental results on the CHEF dataset and our adversarial dataset show that our proposed method outperforms translation-based methods and multilingual LLMs and is more robust toward biases, while there is still large room for improvement, emphasizing the importance of language-specific fact-checking systems",
    "checked": true,
    "id": "99f53c35aaeec3c7c4a02eb3ca30a303a3d8291b",
    "semantic_title": "do we need language-specific fact-checking models? the case of chinese",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=iezS9wMqoV": {
    "title": "TELLER: A Trustworthy Framework for Explainable, Generalizable and Controllable Fake News Detection",
    "volume": "review",
    "abstract": "The proliferation of fake news has emerged as a severe societal problem, raising significant interest from industry and academia. While existing deep-learning based methods have made progress in detecting fake news accurately, their reliability may be compromised caused by the non-transparent reasoning processes, poor generalization abilities and inherent risks of integration with large language models (LLMs). To address this challenge, we propose {\\methodname}, a novel framework for trustworthy fake news detection that prioritizes explainability, generalizability and controllability of models. This is achieved via a dual-system framework that integrates cognition and decision systems, adhering to the principles above. The cognition system harnesses human expertise to generate logical predicates, which guide LLMs in generating human-readable logic atoms. Meanwhile, the decision system deduces generalizable logic rules to aggregate these atoms, enabling the identification of the truthfulness of the input news across diverse domains and enhancing transparency in the decision-making process. Finally, we present comprehensive evaluation results on four datasets, demonstrating the feasibility and trustworthiness of our proposed framework",
    "checked": true,
    "id": "025d4cff7a4fd528fe7f40abf3edefa81d0b969b",
    "semantic_title": "teller: a trustworthy framework for explainable, generalizable and controllable fake news detection",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ansqtJyk9QN": {
    "title": "Handling Ambiguity in Emotion: From Out-of-Domain Detection to Distribution Estimation",
    "volume": "review",
    "abstract": "The subjective perception of emotion leads to inconsistent labels from human annotators. Typically, utterances lacking majority-agreed labels are excluded when training an emotion classifier, which cause problems when encountering ambiguous emotional expressions during testing. This paper investigates three methods to handle ambiguous emotion. First, we show that incorporating utterances without majority-agreed labels as an additional class in the classifier reduces the classification performance of the other emotion classes. Then, we propose detecting utterances with ambiguous emotions as out-of-domain samples by quantifying the uncertainty in emotion classification using evidential deep learning. This approach retains the classification accuracy while effectively detects ambiguous emotion expressions. Furthermore, to obtain fine-grained distinctions among ambiguous emotions, we propose representing emotion as a distribution instead of a single class label. The task is thus re-framed from classification to distribution estimation where every individual annotation is taken into account, not just the majority opinion. The evidential uncertainty measure is extended to quantify the uncertainty in emotion distribution estimation. Experimental results on the IEMOCAP and CREMA-D datasets demonstrate the superior capability of the proposed method in terms of majority class prediction, emotion distribution estimation, and uncertainty estimation",
    "checked": true,
    "id": "20784462127e6ae91a8e5e6b77a0d87f0e723394",
    "semantic_title": "handling ambiguity in emotion: from out-of-domain detection to distribution estimation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BpANDmBIeyQ": {
    "title": "MoE-CT: A Novel Approach For Large Language Models Training With Resistance To Catastrophic Forgetting",
    "volume": "review",
    "abstract": "The advent of large language models (LLMs) has predominantly catered to high-resource languages, leaving a disparity in performance for low-resource languages. Conventional Continual Training (CT) approaches to bridge this gap often undermine a model's original linguistic proficiency when expanding to multilingual contexts. Addressing this issue, we introduce a novel MoE-CT architecture, a paradigm that innovatively separates the base model's learning from the multilingual expansion process. Our design freezes the original LLM parameters, thus safeguarding its performance in high-resource languages, while an appended MoE module, trained on diverse language datasets, augments low-resource language proficiency. Our approach significantly outperforms conventional CT methods, as evidenced by our experiments, which show marked improvements in multilingual benchmarks without sacrificing the model's original language performance. Moreover, our MoE-CT framework demonstrates enhanced resistance to forgetting and superior transfer learning capabilities. By preserving the base model's integrity and focusing on strategic parameter expansion, our methodology advances multilingual language modeling and represents a significant step forward for low-resource language inclusion in LLMs, indicating a fruitful direction for future research in language technologies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bMii2SyenH": {
    "title": "Realistic Evaluation of Toxicity in Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) have become integral to our professional workflows and daily lives. Nevertheless, these machine companions of ours have a critical flaw: the huge amount of data which endows them with vast and diverse knowledge, also exposes them to the inevitable toxicity and bias. While most LLMs incorporate defense mechanisms to prevent the generation of harmful content, these safeguards can be easily bypassed with minimal prompt engineering. In this paper, we introduce the new Thoroughly Engineered Toxicity (TET) dataset, comprising manually crafted prompts designed to nullify the protective layers of such models. Through extensive evaluations, we demonstrate the pivotal role of TET in providing a rigorous benchmark for evaluation of toxicity awareness in several popular LLMs: it highlights the toxicity in the LLMs that might remain hidden when using normal prompts, thus revealing subtler issues in their behavior",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BL2zt5HDQ4": {
    "title": "Women Are Beautiful, Men Are Leaders: Gender Stereotypes in Machine Translation and Language Modeling",
    "volume": "review",
    "abstract": "We present GEST -- a new dataset for measuring gender-stereotypical reasoning in language models and machine translation systems. GEST contains samples for 16 gender stereotypes about men and women (e.g., Women are beautiful, Men are leaders) that are compatible with the English language and 9 Slavic languages. The definition of said stereotypes was informed by gender experts. We used GEST to evaluate English and Slavic masked LMs, English generative LMs, and machine translation systems. We discovered significant and consistent amounts of gender-stereotypical reasoning in almost all the evaluated models and languages. Our experiments confirm the previously postulated hypothesis that the larger the model, the more biased it usually is",
    "checked": true,
    "id": "27bb304d93aab9dd4d12f0d15a204df5674545ac",
    "semantic_title": "women are beautiful, men are leaders: gender stereotypes in machine translation and language modeling",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=UVSHvW6w2zl": {
    "title": "Towards Real-World Writing Assistance: A Chinese Character Checking Benchmark with Faked and Misspelled Characters",
    "volume": "review",
    "abstract": "Writing assistance aims to improve the correctness and quality of input texts, with character checking being crucial in detecting and correcting wrong characters. In the real world where handwriting occupies the vast majority, characters that humans get wrong include faked characters (i.e., untrue characters created due to writing errors) and misspelled characters (i.e., true characters used incorrectly due to spelling errors). However, existing datasets and related studies only focus on misspelled characters that can be represented by computer text encoding systems, thereby ignoring faked characters which are more common and difficult. To break through this dilemma, we present $\\textbf{Visual-C}$$^3$, a human-annotated $\\textbf{Visual}$ $\\textbf{C}$hinese $\\textbf{C}$haracter $\\textbf{C}$hecking dataset with faked and misspelled Chinese characters. To the best of our knowledge, Visual-C$^3$ is the first real-world visual and the largest human-crafted dataset for the Chinese character checking scenario. Additionally, we also propose and evaluate novel baseline methods on Visual-C$^3$. Extensive empirical results and analyses show that Visual-C$^3$ is high-quality yet challenging. As the first study focusing on Chinese faked characters, the Visual-C$^3$ dataset and the baseline methods will be publicly available to facilitate further research in the community",
    "checked": true,
    "id": "78520dac908718d7ad7b2a8e90e2a55bd3149c1b",
    "semantic_title": "towards real-world writing assistance: a chinese character checking benchmark with faked and misspelled characters",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=irzGzWwQ9C": {
    "title": "Evaluating Hallucinations in Chinese Large Language Models",
    "volume": "review",
    "abstract": "In this paper, we establish a benchmark named HalluQA (Chinese Hallucination Question-Answering) to measure the hallucination phenomenon in Chinese large language models. HalluQA contains 450 meticulously designed adversarial questions, spanning multiple domains, and takes into account Chinese historical culture, customs, and social phenomena. During the construction of HalluQA, we consider two types of hallucinations: imitative falsehoods and factual errors, and we construct adversarial samples based on GLM-130B and ChatGPT.For evaluation, we design an automated evaluation method using GPT-4 to judge whether a model output is hallucinated.We conduct extensive experiments on 24 large language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk and etc. Out of the 24 models, 18 achieved non-hallucination rates lower than 50\\%. This indicates that HalluQA is highly challenging.We analyze the primary types of hallucinations in different types of models and their causes. Additionally, we discuss which types of hallucinations should be prioritized for different types of models",
    "checked": true,
    "id": "29652bb2dc0396ab27c0be0c5f24c114c757df0f",
    "semantic_title": "evaluating hallucinations in chinese large language models",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=44OBESTLNcO": {
    "title": "RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language Models",
    "volume": "review",
    "abstract": "The development of large language models(LLMs) has initiated a new chapter in complex tasks such as role-playing, enhancing user interaction experiences by enabling models to imitate various characters.However, LLMs are somewhat lacking in their ability to portray lesser-known characters, especially in aspects of dialogue delivery and scriptwriting skills. To this end, we aim to swiftly acquire essential language skills for character development, greatly enhancing role-playing comfort. In this work, we present RoleCraft, an innovative framework designed to enrich personalized role-playing experiences. Central to this framework is RoleInstruct, a distinctive dataset featuring emotional annotations, transitioning from traditional celebrity-focused roles to more authentic, daily non-celebrity roles,each accompanied by carefully crafted character descriptions. We combined RoleInstruct with open-source instructions from the general domain, employing a hybrid instruction tuning strategy to create RoleCraft-GLM. Experiments in role-playing demonstrate that our model excels in generating dialogue that accurately reflects character traits and emotions, outperforming most mainstream LLMs, including GPT-4",
    "checked": true,
    "id": "4fbf46125b56ff96f255927206ec565f0962159e",
    "semantic_title": "rolecraft-glm: advancing personalized role-playing in large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=8yWcmuqbV2": {
    "title": "PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA",
    "volume": "review",
    "abstract": "With the rapid scaling of large language models (LLMs), serving numerous LoRAs concurrently has become increasingly impractical, leading to unaffordable costs and necessitating more parameter-efficient finetuning methods. In this work, we introduce \\textbf{P}artially \\textbf{Ro}tation-enhanced \\textbf{Lo}w-\\textbf{R}ank \\textbf{A}daptation (PRoLoRA), an intra-layer sharing mechanism comprising four essential components: broadcast reduction, rotation enhancement, partially-sharing refinement, and rectified initialization strategy. As a superset of LoRA, PRoLoRA pertains its advantages, and effectively circumvent the drawbacks of peer parameter-sharing methods with superior model capacity, practical feasibility, and broad applicability. Empirical experiments demonstrate the remarkably higher parameter efficiency of PRoLoRA in both specific parameter budget and performance target scenarios, and its scalability to larger LLMs. Notably, with one time less trainable parameters, PRoLoRA still outperforms LoRA on multiple instruction tuning datasets. Subsequently, an ablation study is conducted to validate the necessity of individual components and highlight the superiority of PRoLoRA over three potential variants. Hopefully, the conspicuously higher parameter efficiency can establish PRoLoRA as a resource-friendly alternative to LoRA",
    "checked": true,
    "id": "3b13389584c554fb5d7eb2f2ffc9b11b50385664",
    "semantic_title": "prolora: partial rotation empowers more parameter-efficient lora",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=DPEKEVi18gg": {
    "title": "Learning from Implicit User Feedback, Demographic Information and User Emotions in Task-Oriented Document-Grounded Dialogues",
    "volume": "review",
    "abstract": "Trustworthiness, interaction quality and empathy have a great influence on whether users accept a dialogue system. To address this, recent works on open-domain dialogues suggest to learn from implicit user feedback or to consider demographic information and user emotions in response generation to improve generation accuracy and user engagement. However, for task-oriented and document-grounded dialogue systems, task completion and factual consistency of the generated responses are almost more important. The impact of such data on these quality criteria is not yet known. To address this gap, we (1) introduce \\ourdata, the first English task-oriented document-grounded dialogue dataset annotated with implicit user feedback, demographic information and user emotions, and (2) investigate the impact of including such data on task completion, and the factual consistency of responses generated by Flan-T5, GPT-2, and Llama 2. Our results show a particularly positive impact on task completion and factual consistency, and that responses generated by models trained with implicit user feedback are preferred by human users",
    "checked": false,
    "id": "0bc0168f2c92861f6711cd5a2e54c027c9c024d5",
    "semantic_title": "learning from emotions, demographic information and implicit user feedback in task-oriented document-grounded dialogues",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kf5zaujg-K": {
    "title": "CriticBench: Evaluating Large Language Models as Critic",
    "volume": "review",
    "abstract": "Critique ability are crucial in the scalable oversight and self-improvement of Large Language Models (LLMs). While many recent studies explore the critique ability of LLMs to judge and refine flaws in generations, how to comprehensively and reliably measure the critique abilities of LLMs is under-explored. This paper introduces CriticBench, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. CriticBench encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity. Our extensive evaluations of open-source and closed-source LLMs reveal intriguing relationships between the critique ability and tasks, response qualities, and model scales. Datasets, resources and evaluation toolkit for CriticBench will be released",
    "checked": true,
    "id": "da01b426baea356687c7ee1d006c9cf986f498b5",
    "semantic_title": "criticbench: evaluating large language models as critic",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nrDIt9tDKDx": {
    "title": "Decomposed Prompting for Vision and Language Arithmetic Reasoning",
    "volume": "review",
    "abstract": "Math problems that involve both vision and language pose a challenging multi-modal task that requires the integration of visual information, textual information, and strong numerical reasoning for adequately solving it. While large language models (LLMs) have achieved impressive performance on arithmetic word problems based solely on text, we found that introducing visual data significantly increases the difficulty. Specifically, the compositional task of counting objects following recognition becomes a formidable hurdle for large vision-language models (LVLMs). The dual demands of recognizing objects and performing arithmetic reasoning pose a significant challenge, hindering LVLMs from excelling in both tasks simultaneously. The commonly employed chain-of-thought (CoT) approach, designed for LLMs, proves ineffective when applied to this multimodal task. As an alternative to the demonstration-based CoT method, we propose a novel decomposition prompting approach, explicitly breaking down the task breakdown into two stages as follows. The first stage performs object detection and enumeration referenced within the mathematical problem. The second stage then leverages the output from stage one to directly address the posed question. Our results demonstrate that this approach leads to substantial performance improvements on established benchmarks for visual and language arithmetic problems. This breaks the chains of CoT, paving the way towards a multimodal breakdown approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wrJhH1fsF5": {
    "title": "Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment Analysis",
    "volume": "review",
    "abstract": "Aspect-based Sentiment Analysis (ABSA) evaluates sentiment expressions within a text to comprehend sentiment information. Previous studies integrated external knowledge, such as knowledge graphs, to enhance the semantic features in ABSA models. Recent research has examined the use of Graph Neural Networks (GNNs) on dependency and constituent trees for syntactic analysis. With the ongoing development of ABSA, more innovative linguistic and structural features are being incorporated (e.g. latent graph), but this also introduces complexity and confusion. As of now, a scalable framework for integrating diverse linguistic and structural features into ABSA does not exist. This paper presents the Extensible Multi-Granularity Fusion (EMGF) network, which integrates information from dependency and constituent syntactic, attention semantic , and external knowledge graphs. EMGF, equipped with multi-anchor triplet learning and orthogonal projection, efficiently harnesses the combined potential of each granularity feature and their synergistic interactions, resulting in a cumulative effect without additional computational expenses. Experimental findings on SemEval 2014 and Twitter datasets confirm EMGF's superiority over existing ABSA methods",
    "checked": true,
    "id": "f99a4aa79a9e81ed79ea3b48b41e10c5d2d6e5c3",
    "semantic_title": "extensible multi-granularity fusion network for aspect-based sentiment analysis",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=voKs9qbXN1L": {
    "title": "M2BIT: A Multi-Modal Bilingual Instruction Tuning Dataset for Visual Language Models",
    "volume": "review",
    "abstract": "In this paper, we introduce the Multi-Modal Bilingual Instruction Tuning dataset (M2BIT), specifically designed to enhance the performance of vision language models (VLMs). Our M2BIT dataset is one of the largest multi-modal instruction tuning datasets available, covering 40 diverse vision-language tasks in both English and Chinese. It comprises 2 million instances, each accompanied by 400 manually written task instructions.With a carefully curated annotation process, we strive to elevate the quality of response, thereby enriching the user experience while minimizing the generation of potential hallucinations. To validate the efficacy of M2BIT, we train a VLM known as Ying-VLM using this dataset, delving into the impact of instruction tuning across diverse languages and modalities. Upon comparing it with strong VLM baselines, Ying-VLM demonstrates superior performance on complex knowledge vision question answering tasks. Moreover, it exhibits a lower propensity for hallucination, displays greater generalization capabilities to previously unseen video tasks, and better comprehends novel instructions in Chinese.We will open-source the M2BIT dataset and trained models to facilitate future research",
    "checked": false,
    "id": "e4d41f2b0c5dfbf0c7078da5c984cea0b46411fd",
    "semantic_title": "ziya-visual: bilingual large vision-language model via multi-task instruction tuning",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=o8BclN0AJQ": {
    "title": "Do Large Language Models Understand Logic or Just Mimick Context?",
    "volume": "review",
    "abstract": "Over the past few years, the abilities of large language models (LLMs) have received extensive attention, which have performed exceptionally well in complicated scenarios such as logical reasoning and symbolic inference. A significant factor contributing to this progress is the benefit of in-context learning and few-shot prompting. However, the reasons behind the success of such models using contextual reasoning have not been fully explored. Do LLMs have understand logical rules to draw inferences, or do they ``guess'' the answers by learning a type of probabilistic mapping through context? This paper investigates the reasoning capabilities of LLMs on two logical reasoning datasets by using counterfactual methods to replace context text and modify logical concepts. Based on our analysis, it is found that LLMs do not truly understand logical rules; rather, in-context learning has simply enhanced the likelihood of these models arriving at the correct answers. If one alters certain words in the context text or changes the concepts of logical terms, the outputs of LLMs can be significantly disrupted, leading to counter-intuitive responses.This work provides critical insights into the limitations of LLMs, underscoring the need for more robust mechanisms to ensure reliable logical reasoning in LLMs",
    "checked": true,
    "id": "bd06f9d931d980117edebf8163298b88ac36e8ea",
    "semantic_title": "do large language models understand logic or just mimick context?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oUbYQA1mV_h": {
    "title": "Explore Spurious Correlations at the Concept Level in Language Models for Text Classification",
    "volume": "review",
    "abstract": "Language models (LMs) have achieved notable success in numerous NLP tasks, employing both fine-tuning and in-context learning (ICL) methods. While language models demonstrate exceptional performance, they face robustness challenges due to spurious correlations arising from imbalanced label distributions in training data or ICL exemplars. Previous research has primarily concentrated on word, phrase, and syntax features, neglecting the concept level, often due to the absence of concept labels and difficulty in identifying conceptual content in input texts. This paper introduces two main contributions. First, we employ ChatGPT to assign concept labels to texts, assessing concept bias in models during fine-tuning or ICL on test data. We find that LMs, when encountering spurious correlations between a concept and a label in training or prompts, resort to shortcuts for predictions. Second, we introduce a data rebalancing technique that incorporates ChatGPT-generated counterfactual data, thereby balancing label distribution and mitigating spurious correlations. Our method's efficacy, surpassing traditional token removal approaches, is validated through extensive testing",
    "checked": true,
    "id": "01efb3fd2d3ae4b5f4389c916c94f2c6d9c11b81",
    "semantic_title": "explore spurious correlations at the concept level in language models for text classification",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=zcjCSEXMGOo": {
    "title": "Consistency Training by Synthetic Question Generation for Conversational Question Answering",
    "volume": "review",
    "abstract": "Efficiently modeling historical information is a critical component in addressing user queries within a conversational question-answering (QA) context, as historical context plays a vital role in clarifying the user's questions. However, irrelevant history induces noise in the reasoning process, especially for those questions with a considerable historical context. In our novel approach, referred to as CoTaH (Consistency-Trained augmented History), we augment the historical information with synthetic questions and subsequently employ consistency training to train a model that utilizes both real and augmented historical data to implicitly make the reasoning robust to irrelevant history. To the best of our knowledge, this is the first instance of research using data augmentation to model conversational QA settings. By citing a common modeling error prevalent in previous research, we introduce a new baseline model and compare our model's performance against it, demonstrating an improvement in results, particularly when dealing with questions that include a substantial amount of historical context",
    "checked": true,
    "id": "0f79c9b95665be5274b11bb09a13cfa1f0900e4f",
    "semantic_title": "consistency training by synthetic question generation for conversational question answering",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AiMeO1Fj_X": {
    "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition",
    "volume": "review",
    "abstract": "Self-attention is an essential component of large language models(LLMs) but a significant source of inference latency for long sequences. In multi-tenant LLMs serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the presence of shared system prompts. Experiments show that ChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$ compared to the start-of-the-art implementation, with the length of the system prompt ranging from 1024 to 4096",
    "checked": true,
    "id": "c8a744a1f47ba30db89e2b7102971fafbd6118c1",
    "semantic_title": "chunkattention: efficient self-attention with prefix-aware kv cache and two-phase partition",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=2sDI54AX-v": {
    "title": "Penetrative AI: Making LLMs Comprehend the Physical World",
    "volume": "review",
    "abstract": "Recent developments in Large Language Models (LLMs) have demonstrated their remarkable capabilities across a range of tasks. Questions, however, persist about the nature of LLMs and their potential to integrate common-sense human knowledge when performing tasks involving information about the real physical world. This paper delves into these questions by exploring how LLMs can be extended to interact with and reason about the physical world through IoT sensors and actuators, a concept that we term \"Penetrative AI\". The paper explores such an extension at two levels of LLMs' ability to penetrate into the physical world via the processing of sensory signals. Our preliminary findings indicate that LLMs, with ChatGPT being the representative example in our exploration, have considerable and unique proficiency in employing the embedded world knowledge for interpreting IoT sensor data and reasoning over them about tasks in the physical realm. Not only this opens up new applications for LLMs beyond traditional text-based tasks, but also enables new ways of incorporating human knowledge in cyber-physical systems",
    "checked": true,
    "id": "a3c1f6809dd1455da73eec407bcb3be92e680112",
    "semantic_title": "penetrative ai: making llms comprehend the physical world",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=e-xZEFZ66J": {
    "title": "Predicting Text Preference Via Structured Comparative Reasoning",
    "volume": "review",
    "abstract": "Comparative reasoning plays a crucial role in predicting text preferences; however, large language models (LLMs) often demonstrate inconsistencies in their reasoning, leading to incorrect preference predictions. While approaches like Chain-of-Thought improve accuracy in many settings, they struggle to consistently distinguish the similarities and differences of complex texts. We introduce SC$^2$, a model that prompts LLMs to predict text preferences by generating structured intermediate comparisons. SC$^2$ begins by proposing aspects for comparison, followed by generating textual comparisons under each aspect. We select consistent comparisons with a pairwise comparator that ensures each comparison of a given aspect clearly distinguishes differences between texts, significantly reducing hallucination and improving consistency. Our empirical studies across various NLP tasks, including summarization, retrieval, and automatic rating, demonstrate that SC$^2$'s enhanced performance in text preference prediction is significant",
    "checked": true,
    "id": "7a147a745f69329afb1c86becdba7b3029a169ca",
    "semantic_title": "predicting text preference via structured comparative reasoning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=ENwJ-eXG17O": {
    "title": "Is it Possible to Edit Large Language Models Robustly?",
    "volume": "review",
    "abstract": "Large language models (LLMs) have played a pivotal role in building communicative AI to imitate human behaviors but face the challenge of efficient customization. To tackle this challenge, recent studies have delved into the realm of model editing, which manipulates specific memories of language models and changes the related language generation. However, the robustness of model editing remains an open question. This work seeks to understand the strengths and limitations of editing methods, thus facilitating robust, realistic applications of communicative AI. Concretely, we conduct extensive analysis to address the three key research questions. Q1: Can edited LLMs behave consistently resembling communicative AI in realistic situations? Q2: To what extent does the rephrasing of prompts lead LLMs to deviate from the edited knowledge memory? Q3: Which knowledge features are correlated with the performance and robustness of editing? Our experimental results uncover a substantial disparity between existing editing methods and the practical application of LLMs. On rephrased prompts that are complex and flexible but common in realistic applications, the performance of editing experiences a significant decline. Further analysis shows that more popular knowledge is memorized better, easier to recall, and more challenging to edit effectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7rrAA6Te9b": {
    "title": "Is Bigger and Deeper Always Better? Probing LLaMA Across Scales and Layers",
    "volume": "review",
    "abstract": "This paper presents an in-depth analysis of Large Language Models (LLMs), focusing on LLaMA, a prominent open-source foundational model in natural language processing. Instead of assessing LLaMA through its generative output, we design multiple-choice tasks to probe its intrinsic understanding in high-order tasks such as reasoning and calculation. We examine the model horizontally, comparing different sizes, and vertically, assessing different layers.We unveil several key and uncommon findings based on the designed probing tasks: (1) Horizontally, enlarging model sizes almost could not automatically impart additional knowledge or computational prowess. Instead, it can enhance reasoning abilities, especially in math problem solving, and helps reduce hallucinations, but only beyond certain size thresholds; (2) In vertical analysis, the lower layers of LLaMA lack substantial arithmetic and factual knowledge, showcasing logical thinking, multilingual and recognitive abilities, with top layers housing most computational power and real-world knowledge. These findings provide new observations into LLaMA's capabilities, offering insights into the current state of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DzzJQe_reJC": {
    "title": "Improving Event Definition Following For Zero-Shot Event Detection",
    "volume": "review",
    "abstract": "Existing approaches on zero-shot event detection usually train models on datasets annotated with known event types, and prompt them with unseen event definitions. These approaches yield sporadic successes, yet generally fall short of expectations.In this work, we aim to improve zero-shot event detection by training models to better follow event definitions. We hypothesize that a diverse set of event types and definitions are the key for models to learn to follow event definitions while existing event extraction datasets focus on annotating many high-quality examples for a few event types. To verify our hypothesis, we construct an automatically generated Diverse Event Definition (DivED) dataset and conduct comparative studies. Our experiments reveal that a large number of event types (200) and diverse event definitions can significantly boost event extraction performance; on the other hand, the performance does not scale with over ten examples per event type.Beyond scaling, we incorporate event ontology information and hard-negative samples during training, further boosting the performance. Based on these findings, we fine-tuned a LLaMA-2-7B model on our DivED dataset, yielding performance that surpasses SOTA large language models like GPT-3.5 across three open benchmarks on zero-shot event detection",
    "checked": true,
    "id": "9957879871784d9d2b352a31bda2abbecfd65476",
    "semantic_title": "improving event definition following for zero-shot event detection",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=XseKK6xzr8": {
    "title": "DiffuSent: Towards a Unified Diffusion Framework for Aspect-Based Sentiment Analysis",
    "volume": "review",
    "abstract": "Aspect-Based Sentiment Analysis (ABSA) encompasses seven distinct subtasks, each focusing on different extracted elements. Despite the proven success of generative models in unified aspect sentiment analysis, existing approaches often rely on autoregressive token-by-token generation without grasping the whole information of the aspect and opinion terms, resulting in boundary insensitivity, particularly in context of multi-word aspect and opinion terms. To address these issues, we present DiffuSent, a non-autoregressive diffusion framework that systematically formulates all ABSA subtasks as boundary denoising diffusion processes, progressively refining boundaries over noisy states. Furthermore, we introduce a contrastive denoising training strategy which effectively address duplicate predictions with subtle variations introduced by diffusion process. Extensive experiments on four datasets for seven subtasks demonstrate that DiffuSent achieves state-of-the-art performances",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Lm3gSa81M": {
    "title": "Advancing Test-Time Adaptation for Acoustic Foundation Models in Open-World Shifts",
    "volume": "review",
    "abstract": "Test-Time Adaptation (TTA) is a critical paradigm for tackling distribution shifts during inference, especially in visual recognition tasks. However, while acoustic models face similar challenges due to distribution shifts in test-time speech, TTA techniques specifically designed for acoustic modeling in the context of open-world data shifts remain scarce. This gap is further exacerbated when considering the unique characteristics of acoustic foundation models: 1) they are primarily built on transformer architectures with layer normalization and 2) they deal with test-time speech data of varying lengths in a non-stationary manner. These aspects make the direct application of vision-focused TTA methods, which are mostly reliant on batch normalization and assume independent samples, infeasible. In this paper, we delve into TTA for pre-trained acoustic models facing open-world data shifts. We find that noisy, high-entropy speech frames, often non-silent, carry key semantic content. Traditional TTA methods might inadvertently filter out this information using potentially flawed heuristics. In response, we introduce a learning-based adaptation enriched by confidence enhancement. Noting that speech signals' short-term consistency, we also apply consistency regularization during test-time optimization. Our experiments on synthetic and real-world datasets affirm our method's superiority over existing baselines",
    "checked": true,
    "id": "18cefedca4ac2cb9ee8274642bcc3c02459b1025",
    "semantic_title": "advancing test-time adaptation for acoustic foundation models in open-world shifts",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=0VBkEVLwbz1": {
    "title": "LoRAPrune: Structured Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning",
    "volume": "review",
    "abstract": "Large Language Models (LLMs), such as LLaMA and T5, have shown exceptional performance across various tasks through fine-tuning. Although low-rank adaption (LoRA) has emerged to cheaply fine-tune these LLMs on downstream tasks, their deployment is still hindered by the vast model scale and computational costs. Post-training model pruning offers a way to compress LLMs. However, the current pruning methods designed for LLMs are not compatible with LoRA. This is due to their utilization of unstructured pruning on LLMs, impeding the merging of LoRA weights, or their dependence on the gradients of pre-trained weights to guide pruning, which can impose significant memory overhead. To this end, we propose LoRAPrune, a new framework that delivers an accurate structured pruned model in a highly memory-efficient manner. Specifically, we first design a LoRA-guided pruning criterion, which uses the weights and gradients of LoRA, rather than the gradients of pre-trained weights for importance estimation. We subsequently integrate this criterion into an iterative pruning process, effectively removing redundant channels and heads. Extensive experimental results demonstrate the superior performance of our LoRAPrune over existing approaches on the LLaMA series models. At a 50% compression rate, LoRAPrune demonstrates superior performance over LLM-Pruner, achieving a reduction in perplexity by 4.81 on WikiText2 and 3.46 on PTB, while also decreasing memory usage by 52.6%. Besides, LoRAPrune also matches semi-structural pruning across multiple LLMs, proving its wide applicability",
    "checked": true,
    "id": "9237363485443e297b64b0e5c64afbb2dd603696",
    "semantic_title": "loraprune: structured pruning meets low-rank parameter-efficient fine-tuning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C7sY_3Jwkb": {
    "title": "From Adoption to Adaption: Tracing the Diffusion of New Emojis on Twitter",
    "volume": "review",
    "abstract": "In the rapidly evolving landscape of social media, the introduction of new emojis in Unicode release versions presents a structured opportunity to explore digital language evolution. Analyzing a large dataset of sampled English tweets, we examine how newly released emojis gain traction and evolve in meaning. We find that community size of early adopters and emoji semantics are crucial in determining their popularity. Certain emojis experienced notable shifts in the meanings and sentiment associations during the diffusion process. Additionally, we propose a novel framework utilizing language models to extract words and pre-existing emojis with semantically similar contexts, which enhances interpretation of new emojis. The framework demonstrates its effectiveness in improving sentiment classification performance by substituting unknown new emojis with familiar ones. This study offeres a new perspective in understanding how new language units are adopted, adapted, and integrated into the fabric of online communication",
    "checked": true,
    "id": "c315de920f4538b5a27219fd6bbb3cd4b50659f5",
    "semantic_title": "from adoption to adaption: tracing the diffusion of new emojis on twitter",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Xxt-udzr6P5": {
    "title": "An Embarrassingly Simple Approach for LLM with Strong ASR Capacity",
    "volume": "review",
    "abstract": "In this paper, we focus on solving one of the most important tasks in the field of speech processing, i.e., automatic speech recognition (ASR), with speech foundation encoders and large language models (LLM). Recent works have complex designs such as compressing the output temporally for the speech encoder, tackling modal alignment for the projector, and utilizing parameter-efficient fine-tuning for the LLM. We found that delicate designs are not necessary, while an embarrassingly simple composition of off-the-shelf speech encoder, LLM, and the only trainable linear projector is competent for the ASR task. To be more specific, we benchmark and explore various combinations of LLMs and speech encoders, leading to the optimal LLM-based ASR system, which we call SLAM-ASR. The proposed SLAM-ASR provides a clean setup and little task-specific design, where only the linear projector is trained. To the best of our knowledge, SLAM-ASR achieves the best performance on the Librispeech benchmark among LLM-based ASR models and even outperforms the latest LLM-based audio-universal model trained on massive pair data. Finally, we explore the capability emergence of LLM-based ASR in the process of modal alignment. We hope that our study can facilitate the research on extending LLM with cross-modality capacity and shed light on the LLM-based ASR community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LO-NO1-PwJR": {
    "title": "Boosting LLM Agents with Recursive Contemplation for Effective Deception Handling",
    "volume": "review",
    "abstract": "Recent advances in large language models (LLMs) have led to significant success in using LLMs as agents. Nevertheless, a common assumption that LLMs always process honest information neglects the widespread deceptive or misleading content in human and AI-generated material. This oversight might expose LLMs to malicious manipulations. To enhance LLMs' ability to identify and counteract deceptive information, in this paper, inspired by humans' recursive thinking and perspective-taking, we introduce a novel cognitive framework, Recursive Contemplation (ReCon). ReCon combines formulation and refinement contemplation processes; formulation contemplation produces initial thoughts and speech, while refinement contemplation further polishes them. Additionally, we incorporate first-order and second-order perspective transitions into these processes respectively. Specifically, the first-order allows an LLM agent to infer others' mental states, and the second-order involves understanding how others perceive the agent's mental state. After integrating ReCon with various LLMs, extensive experiment results from the Avalon game and BigTom benchmark indicate ReCon's efficacy in aiding LLMs to discern and maneuver around deceptive information without extra fine-tuning and data. Finally, we demonstrate ReCon's scaling trend with model parameters, and explore the current limitations of LLMs in terms of safety and reasoning, potentially furnishing insights for subsequent research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d8-_oR8DU_": {
    "title": "LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation",
    "volume": "review",
    "abstract": "Adapting English-based large language models (LLMs) to other languages has become increasingly popular due to the efficiency and potential of cross-lingual transfer. However, existing language adaptation methods often overlook the benefits of cross-lingual supervision. In this study, we introduce LEIA, a language adaptation tuning method that utilizes Wikipedia entity names aligned across languages. This method involves augmenting the target language corpus with English entity names and training the model using left-to-right language modeling. We assess LEIA on diverse question answering datasets using 7B-parameter LLMs, demonstrating significant performance gains across various non-English languages",
    "checked": true,
    "id": "7bfc782d127aa989ab8424f6f2f9cb4cad5d6f87",
    "semantic_title": "leia: facilitating cross-lingual knowledge transfer in language models with entity-based data augmentation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Du05AJvD4B": {
    "title": "Semantic Steganography: A Framework for Robust and High-Capacity Information Hiding using Large Language Models",
    "volume": "review",
    "abstract": "In the era of Large Language Models (LLMs), generative linguistic steganography has become a prevalent technique for hiding information within model-generated texts. However, traditional steganography methods struggle to effectively align steganographic texts with original model-generated texts due to the lower entropy of the predicted probability distribution of LLMs. This results in a decrease in embedding capacity and poses challenges for decoding stegos in real-world communication channels.To address these challenges, we propose a semantic steganography framework based on LLMs, which constructs a semantic space and maps secret messages onto this space using ontology-entity trees. This framework offers robustness and reliability for transmission in complex channels, as well as resistance to text rendering and word blocking. Additionally, the stegos generated by our framework are indistinguishable from the covers and achieve a higher embedding capacity compared to state-of-the-art steganography methods, while producing higher quality stegos",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sTtF2GJK7Fb": {
    "title": "PHAnToM: Personality Has An Effect on Theory-of-Mind Reasoning in Large Language Models",
    "volume": "review",
    "abstract": "Recent advances in large language models (LLMs) demonstrate that their capabilities are comparable, or even superior, to humans in many tasks in natural language processing. Despite this progress, LLMs are still inadequate at social-cognitive reasoning, which humans are naturally good at. Drawing inspiration from psychological research on the links between certain personality traits and Theory-of-Mind (ToM) reasoning, and from prompt engineering research on the hyper-sensitivity of prompts in affecting LLMs capabilities, this study investigates how inducing personalities in LLMs using prompts affects their ToM reasoning capabilities. Our findings show that certain induced personalities can significantly affect the LLMs' reasoning capabilities in three different ToM tasks. In particular, traits from the Dark Triad have a larger variable effect on LLMs like GPT-3.5, Llama 2, and Mistral across the different ToM tasks. We find that LLMs that exhibit a higher variance across personality prompts in ToM also tends to be more controllable in personality tests: personality traits in LLMs like GPT-3.5, Llama 2 and Mistral can be controllably adjusted through our personality prompts. In today's landscape where role-play is a common strategy when using LLMs, our research highlights the need for caution, as models that adopt specific personas with personalities potentially also alter their reasoning abilities in an unexpected manner",
    "checked": true,
    "id": "65e9acc0b4253b3e5540487be03861b5aeb9e9c5",
    "semantic_title": "phantom: personality has an effect on theory-of-mind reasoning in large language models",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=KKQZ89438ui": {
    "title": "Whose Emotions and Moral Sentiments do Language Models Reflect?",
    "volume": "review",
    "abstract": "Language models (LMs) are known to represent the perspectives of some social groups better than others, which may impact their performance, especially on subjective tasks such as content moderation and hate speech detection. To explore how LMs represent different perspectives, existing research focused on positional alignment, i.e., how closely the models mimic the opinions and stances of different groups, e.g., liberals or conservatives. However, human communication also encompasses emotional and moral dimensions. We define the problem of affective alignment, which measures how LMs' emotional and moral tone represents those of different groups. By comparing the affect of responses generated by 36 LMs to the affect of Twitter messages, we observe significant misalignment of LMs with both ideological groups. This misalignment is larger than the partisan divide in the U.S. Even after steering the LMs towards specific ideological perspectives, the misalignment and liberal tendencies of the model persist, suggesting a systemic bias within LMs",
    "checked": true,
    "id": "4d3274a59b4aa0e9d4b57dc3b58b11d35530511c",
    "semantic_title": "whose emotions and moral sentiments do language models reflect?",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=M3jFpmPrLn": {
    "title": "NeuralSpeak: Non-invasive Brain-to-Speech Synthesis with Language Models",
    "volume": "review",
    "abstract": "Speech Synthesis from non-invasive brain activities offers a promising avenue for restoring communication abilities in patients with neurological disorders. Significant progress has been made in reconstructing natural speech from invasive brain recordings; however, these methods face practical challenges such as the high risk associated with brain surgery and the difficulties encountered in maintaining such devices over time. In this work, we formulate the task of non-invasive brain-to-speech synthesis and propose \\textit{NeuralSpeak} tailored for this task, Specifically, we 1) leverage a multi-scale transformer model to address the challenges of handling excessively long sequences caused by the residual vector quantization-based neural codec in tokenization; 2) introduce a multi-window fMRI encoder, trained with contrastive learning to produce brain-derived embeddings that align closely with semantically rich text representations. \\textit{NeuralSpeak} achieves state-of-the-art results in both objective and subjective benchmark evaluation. Furthermore, we provide evidence that our model is biologically plausible and interpretable, mirroring established physiological processes.\\footnote{Audio samples are available at \\url{https://NeuralSpeak.github.io}}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CyfZxLbs4b": {
    "title": "Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate",
    "volume": "review",
    "abstract": "Fact-checking research has extensively explored verification but less so the generation of natural-language explanations, crucial for user trust. While Large Language Models (LLMs) excel in text generation, their capability for producing faithful explanations in fact-checking remains underexamined. Our study investigates LLMs' ability to generate such explanations, finding that zero-shot prompts often result in unfaithfulness. To address these challenges, we propose the Multi-Agent Debate Refinement (MADR) framework, leveraging multiple LLMs as agents with diverse roles in an iterative refining process aimed at enhancing faithfulness in generated explanations. MADR ensures that the final explanation undergoes rigorous validation, significantly reducing the likelihood of unfaithful elements and aligning closely with the provided evidence. Experimental results demonstrate that MADR significantly improves the faithfulness of LLM-generated explanations to the evidence, advancing the credibility and trustworthiness of these explanations",
    "checked": true,
    "id": "19499cc52d7945d35811e9959fcec59f3b07f6f7",
    "semantic_title": "can llms produce faithful explanations for fact-checking? towards faithful explainable fact-checking via multi-agent debate",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=tSqMro2ZUP": {
    "title": "Faithful Persona-based Conversational Dataset Generation with Large Language Models",
    "volume": "review",
    "abstract": "High-quality conversational datasets are essential for developing AI models that can communicate with users. One way to foster deeper interactions between a chatbot and its user is through personas, aspects of the user's character that provide insights into their personality, motivations, and behaviors. Training Natural Language Processing (NLP) models on a diverse and comprehensive persona-based dataset can lead to conversational models that create a deeper connection with the user, and maintain their engagement. In this paper, we leverage the power of Large Language Models (LLMs) to create a large, high-quality conversational dataset from a seed dataset. We propose a Generator-Critic architecture framework to expand the initial dataset, while improving the quality of its conversations. The Generator is an LLM prompted to output conversations. The Critic consists of a mixture of expert LLMs that control the quality of the generated conversations. These experts select the best generated conversations, which we then use to improve the Generator. We release Synthetic- Persona-Chat, consisting of 20k conversations seeded from Persona-Chat. We evaluate the quality of Synthetic-Persona-Chat and our generation framework on different dimensions through extensive experiments, and observe that the losing rate of Synthetic-Persona-Chat against Persona-Chat during Turing test decreases from 17.2% to 8.8% over three iterations",
    "checked": true,
    "id": "a86d3692af34ce70fd8caaa319009f85a88fdb4d",
    "semantic_title": "faithful persona-based conversational dataset generation with large language models",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=VwJLMt5Jd_": {
    "title": "Temporal Validity Change Prediction",
    "volume": "review",
    "abstract": "Temporal validity is an important property of text that has many downstream applications, such as recommender systems, conversational AI, and user status tracking. Existing benchmarking tasks often require models to identify the temporal validity duration of a single statement. However, many data sources contain additional context, such as successive sentences in a story or posts on a social media profile. This context may alter the duration for which the originally collected statement is expected to be valid. We propose Temporal Validity Change Prediction, a natural language processing task benchmarking the capability of machine learning models to detect context statements that induce such change. We create a dataset consisting of temporal target statements sourced from Twitter and crowdsource corresponding context statements. We then benchmark a set of transformer-based language models on our dataset. Finally, we experiment with a multitasking approach to improve the state-of-the-art performance",
    "checked": true,
    "id": "8d3d792c28ad8b72899193449a2603efec24275d",
    "semantic_title": "temporal validity change prediction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UZgU3wPI69": {
    "title": "SyntaxShap: Syntax-aware Explainability Method for Text Generation",
    "volume": "review",
    "abstract": "To harness the power of large language models in safety-critical domains we need to ensure the explainability of their predictions. However, despite the significant attention to model interpretability, there remains an unexplored domain in explaining sequence-to-sequence tasks using methods tailored for textual data. This paper introduces SyntaxShap, a local, model-agnostic explainability method for text generation that takes into consideration the syntax in the text data. The presented work extends Shapley values to account for parsing-based syntactic dependencies. Taking a game theoric approach, SyntaxShap only considers coalitions constraint by the dependency tree. We adopt a model-based evaluation to compare SyntaxShap and its weighted form to state-of-the-art explainability methods adapted to text generation tasks, using diverse metrics including faithfulness, complexity, coherency, and semantic alignment of the explanations to the model. We show that our syntax-aware method produces explanations that help build more faithful, coherent, and interpretable explanations for predictions by autoregressive models",
    "checked": true,
    "id": "1cbea673c605ef27d620e12d6a12f641220e0371",
    "semantic_title": "syntaxshap: syntax-aware explainability method for text generation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=QJTnBv22_K": {
    "title": "TransferSinger: Zero-Shot Singing Voice Synthesis with Style Transfer",
    "volume": "review",
    "abstract": "Zero-shot Singing Voice Synthesis (SVS) with style transfer aims to generate high-quality singing voices of unseen timbres and styles (including singing methods, rhythm, techniques, and pronunciation) from the prompt audio. However, the multifaceted nature of singing voice styles poses a significant challenge for comprehensive modeling and effective transfer. Furthermore, existing SVS models often fail to generate singing voices with a wealth of stylistic nuances for unseen singers. In this paper, we introduce TransferSinger, a novel zero-shot SVS model that primarily employs three modules to address these challenges: 1) the style encoder that employs a Vector Quantization (VQ) model to condense style information into a compact latent space, thus facilitating subsequent predictions; 2) the Style and Duration Language Model (S\\&D-LM), which concurrently predicts style information and phoneme duration, thereby enhancing both; and 3) the style adaptive decoder that uses a novel style adaptive normalization method to generate singing voices with enhanced details. Experimental results show that TransferSinger outperforms baseline models in terms of both synthesis quality and singer similarity across various tasks, including zero-shot SVS, controllable style synthesis, cross-lingual style transfer, and speech-to-singing style transfer. Singing voice samples can be accessed at \\url{https://transfersinger.github.io/}",
    "checked": false,
    "id": "6e3de0676874c1391f6ec01cefa79a3bab4b393a",
    "semantic_title": "midi-voice: expressive zero-shot singing voice synthesis via midi-driven priors",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=spDIf9k1A9": {
    "title": "RulE: Knowledge Graph Reasoning with Rule Embedding",
    "volume": "review",
    "abstract": "Knowledge graph reasoning is an important problem for knowledge graphs. In this paper, we propose a novel and principled framework called \\textbf{RulE} (stands for {Rul}e {E}mbedding) to effectively leverage logical rules to enhance KG reasoning. Unlike knowledge graph embedding methods, RulE learns rule embeddings from existing triplets and first-order {rules} by jointly representing \\textbf{entities}, \\textbf{relations} and \\textbf{logical rules} in a unified embedding space. Based on the learned rule embeddings, a confidence score can be calculated for each rule, reflecting its consistency with the observed triplets. This allows us to perform logical rule inference in a soft way, thus alleviating the brittleness of logic. On the other hand, RulE injects prior logical rule information into the embedding space, enriching and regularizing the entity/relation embeddings. This makes KGE alone perform better too. RulE is conceptually simple and empirically effective. We conduct extensive experiments to verify each component of RulE.Results on multiple benchmarks reveal that our model outperforms the majority of existing embedding-based and rule-based approaches",
    "checked": true,
    "id": "191815e4109ee392b9120b61642c0e859fb662a1",
    "semantic_title": "rule: knowledge graph reasoning with rule embedding",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=FlEEr26-J2": {
    "title": "A Framework for PromptOps in GenAI Application Development Lifecycle",
    "volume": "review",
    "abstract": "The use of \"prompts\" in the creation process of Generative Artificial Intelligence (GenAI) systems is receiving increasing interest. The significance of these prompts throughout the development cycle, however, is not properly used by current software development lifecycle approaches. This study proposes a unique methodology for integrating timely engineering and management into the creation of GenAI applications. Organizations may benefit from using \"PromptOps\" to create GenAI applications more quickly, effectively, and securely. It offers a technique to lower the danger of bias, increase the accuracy and dependability of GenAI systems, and decrease the cost of development and implementation. Our platform facilitates the seamless integration of several automated technologies in software development by performing prompt operations (PromptOps). These include Continuous Integration/Continuous Deployment (CI/CD) pipelines, workflows, APIs, and more. Our approach enables developers to easily include automated technologies, leading to a more simplified and efficient process. Furthermore, this study indicates that the framework may enable all stakeholders, including non-engineering units, to convert prompts into services, expanding their use in the building of applications. This study emphasizes the critical significance of prompts in GenAI and shows how their incorporation may improve AI application development, eventually stimulating creativity and driving the adoption of Generative AI technology",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gRO3FB1xWi": {
    "title": "Exploring Rollback Inference for Aspect-based Sentiment Analysis",
    "volume": "review",
    "abstract": "With the giant help from pre-trained large language models, templated sequence of how to organize the aspect-level elements become the hottest research target while only a few of them move their steps to inference, not to mention utilizing the semantic connection between aspect-level elements during it. We argue that, compared with the high computational cost methods of training language models, considering the inference process can also bring us potential benefits. Motivated by this, we propose Rollback Inference strategies for aspect-based sentiment analysis, which can boost the performance of fine-tuned large language models with a tiny cost, and adapt to various language models. Extensive experiments in three datasets and multiple language models underscore the effectiveness of our proposed rollback inference strategies and the value of the semantic connections in inference",
    "checked": false,
    "id": "7af66a73925da9cbd4447067eef6589d5acf08e7",
    "semantic_title": "exploring bert for aspect-based sentiment analysis in portuguese language",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=aWH7yvi7Xx": {
    "title": "Pushing The Limit of LLM Capacity for Text Classification",
    "volume": "review",
    "abstract": "The value of text classification's future research has encountered challenges and uncertainties, due to the extraordinary efficacy demonstrated by large language models (LLMs) across numerous downstream NLP tasks. In this era of open-ended language modeling, where task boundaries are gradually fading, an urgent question emerges: have we made significant progress in text classification with the full benefit of LLMs? To answer this question, we propose RGPT, an adaptive boosting framework tailored to produce a specialized text classification LLM by recurrently ensembling a pool of strong base learners. The base learners are constructed by adaptively adjusting the distribution of training samples and iteratively fine-tuning LLMs with them. Such base learners are then ensembled to be a specialized text classification LLM, by recurrently incorporating the historical predictions from the previous learners. Through a comprehensive empirical comparison, we show that RGPT significantly outperforms 8 SOTA PLMs and 7 SOTA LLMs on four benchmarks by 1.36% on average. Further evaluation experiments reveal a clear superiority of RGPT over average human classification performance",
    "checked": true,
    "id": "e6e584fc07b6cc6a78cb465019871d751856ba08",
    "semantic_title": "pushing the limit of llm capacity for text classification",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=kVdTw8uLjA": {
    "title": "Question Translation Training for Better Multilingual Reasoning",
    "volume": "review",
    "abstract": "Large language models show compelling performance on reasoning tasks but they tend to perform much worse in languages other than English. This is unsurprising given that their training data largely consists of English text and instructions. A typical solution is to translate instruction data into all languages of interest, and then train on the resulting multilingual data, which is called translate-training. This approach not only incurs high cost, but also results in poorly translated data due to the non-standard formatting of mathematical chain-of-thought. In this paper, we explore the benefits of question alignment, where we train the model to translate reasoning questions into English by finetuning on X-English parallel question data. In this way we perform targeted, in-domain language alignment which makes best use of English instruction data to unlock the LLMs' multilingual reasoning abilities. Experimental results on LLaMA2-13B show that question alignment leads to consistent improvements over the translate-training approach: an average improvement of 11.3\\% and 16.1\\% accuracy across ten languages on the MGSM and MSVAMP reasoning benchmarks",
    "checked": true,
    "id": "e7bbfb2eb08cce711fd39f2081d116d7e760651e",
    "semantic_title": "question translation training for better multilingual reasoning",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=DaHqBdLoA5": {
    "title": "HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) can play a vital role in psychotherapy by adeptly handling the crucial task of cognitive reframing and overcoming challenges such as shame, distrust, therapist skill variability, and resource scarcity. Previous LLMs in cognitive reframing mainly converted negative emotions to positive ones, but these approaches have limited efficacy, often not promoting clients' self-discovery of alternative perspectives. In this paper, we unveil the Helping and Empowering through Adaptive Language in Mental Enhancement (HealMe) model. This novel cognitive reframing therapy method effectively addresses deep-rooted negative thoughts and fosters rational, balanced perspectives. Diverging from traditional LLM methods, HealMe employs empathetic dialogue based on psychotherapeutic frameworks. It systematically guides clients through distinguishing circumstances from feelings, brainstorming alternative viewpoints, and developing empathetic, actionable suggestions. Moreover, we adopt the first comprehensive and expertly crafted psychological evaluation metrics, specifically designed to rigorously assess the performance of cognitive reframing, in both AI-simulated dialogues and real-world therapeutic conversations. Experimental results show that our model outperforms others in terms of empathy, guidance, and logical coherence, demonstrating its effectiveness and potential positive impact on psychotherapy",
    "checked": true,
    "id": "31a32cbc7b9ed59666cbeb783505897885916669",
    "semantic_title": "healme: harnessing cognitive reframing in large language models for psychotherapy",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=67qntOrWVt": {
    "title": "ICON: Improving Inter-Report Consistency of Radiology Report Generation via Lesion-aware Mix-up Augmentation",
    "volume": "review",
    "abstract": "Previous research on radiology report generation has made significant progress in terms of increasing the clinical accuracy of generated reports. In this paper, we emphasize another crucial quality that it should possess, i.e., inter-report consistency, which refers to the capability of generating consistent reports for semantically equivalent radiographs. This quality is even of greater significance than the overall report accuracy in terms of ensuring the system's credibility, as a system prone to providing conflicting results would severely erode users' trust. Regrettably, existing approaches struggle to maintain inter-report consistency, exhibiting biases towards common patterns and susceptibility to lesion variants. To address this issue, we propose ICON, which improves the inter-report consistency of radiology report generation. Aiming at enhancing the system's ability to capture the similarities in semantically equivalent lesions, our approach involves first extracting lesions from input images and examining their characteristics. Then, we introduce a lesion-aware mix-up augmentation technique to ensure that the representations of the semantically equivalent lesions align with the same attributes, by linearly interpolating them during the training phase. Extensive experiments on three publicly available chest X-ray datasets verify the effectiveness of our approach, both in terms of improving the consistency and accuracy of the generated reports",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ObLJxsFQHAL": {
    "title": "Assessment and manipulation of latent constructs in pre-trained language models using psychometric scales",
    "volume": "review",
    "abstract": "Recent discoveries suggest that large language models demonstrate personality-like traits. This evidence suggests that known and yet undiscovered biases of language models conform to standard human-like latent psychological constructs. While large conversational models may be tricked into genuinely answering questionnaires, psychometric assessment methods are lacking for thousands of simpler transformers trained for other tasks. This article teaches how to reformulate psychological questionnaires into natural language inference prompts and provides a code library to support the psychometric assessment of arbitrary models. Experiments performed with a sample of 88 publicly available models demonstrate the existence of mental health-related constructs, such as anxiety, depression, and the sense of coherence. Extensive validation of the constructs reveals that they conform with standard theories in human psychology, including known correlations, and mitigation strategies. The ability to interpret and rectify the performance of language models using psychological tools will help to develop more explainable, controllable, and trustworthy models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PQQz-5tNko": {
    "title": "LIRE: listwise reward enhancement for preference alignment",
    "volume": "review",
    "abstract": "Recently, tremendous strides have been made to align the generation of Large Language Models (LLMs) with human values to mitigate toxic or unhelpful content. Leveraging Reinforcement Learning from Human Feedback (RLHF) proves effective and is widely adopted by researchers. However, implementing RLHF is complex, and its sensitivity to hyperparameters renders achieving stable performance and scalability challenging. Furthermore, prevailing approaches to preference alignment primarily concentrate on pairwise comparisons, with limited exploration into multi-response scenarios, thereby overlooking the potential richness within the candidate pool. For the above reasons, we propose a new approach: Listwise Reward Enhancement for Preference Alignment (LIRE), a gradient-based reward optimization approach that incorporates the offline rewards of multiple responses into a streamlined listwise framework, thus eliminating the need for online sampling during training. LIRE is straightforward to implement, requiring minimal parameter tuning, and seamlessly aligns with the pairwise paradigm while naturally extending to multi-response scenarios. Moreover, we introduce a self-enhancement algorithm aimed at iteratively refining the reward during training. Our experiments demonstrate that LIRE consistently outperforms existing methods across several benchmarks on dialogue and summarization tasks, with good transferability to out-of-distribution data, assessed using proxy reward models and human annotators",
    "checked": true,
    "id": "58bf4853effe89282eb8d8cd1e0dd1b782eee62f",
    "semantic_title": "lire: listwise reward enhancement for preference alignment",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t1GVRmk7an": {
    "title": "Stance Detection for Macro Topics based on Multi-factor Aggregation Analysis",
    "volume": "review",
    "abstract": "This paper presents an innovative exploration into stance detection, with a specific focus on subjects characterized by their inherently abstract and macroscopic nature, termed as ``macro topics.'' Due to the intricate complexity associated with these subjects, individuals often refrain from explicitly stating their opinions, thereby introducing challenges to stance detection when the target is implicit or unmentioned in the text. To address this complexity, we propose a tailored representation model designed to effectively encapsulate the nuanced aspects of macro topics. Our model relies on a comprehensive multidimensional analysis of sub-topics within a given macro topic, employing a specially designed discourse-based Latent Dirichlet Allocation (LDA) model. Utilizing this representation, an aggregation analysis is implemented to deduce stances on the macro topic by examining the array of sub-topic stances. The analysis of stances associated with sub-topics expressed in text is achieved by leveraging the semantic analysis capability of large language models (LLMs). Our approach attains superior stance detection accuracy, as validated through extensive experiments conducted on large-scale social media and finance text datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8cf97Yick1X": {
    "title": "ASGEA: Exploiting Logic Rules from Align-Subgraphs for Entity Alignment",
    "volume": "review",
    "abstract": "Entity alignment (EA) aims to identify entities across different knowledge graphs that represent the same real-world objects. Recent embedding-based EA methods have achieved state-of-the-art performance in EA yet faced interpretability challenges as they purely rely on the embedding distance and neglect the logic rules behind a pair of aligned entities. In this paper, we propose the Align-Subgraph Entity Alignment (ASGEA) framework to exploit logic rules from Align-Subgraphs. ASGEA uses anchor links as bridges to construct Align-Subgraphs and spreads along the paths across KGs, which distinguishes it from the embedding-based methods. Furthermore, we design an interpretable Path-based Graph Neural Network, ASGNN, to effectively identify and integrate the logic rules across KGs. We also introduce a node-level multi-modal attention mechanism coupled with multi-modal enriched anchors to augment the Align-Subgraph. Our experimental results demonstrate the superior performance of ASGEA over the existing embedding-based methods in both EA and Multi-Modal EA (MMEA) tasks. Our code is available at https://anonymous.4open.science/r/ASGEA-B2FE",
    "checked": true,
    "id": "1669f8e4b31b64e3d48c395fb4683de32045f828",
    "semantic_title": "asgea: exploiting logic rules from align-subgraphs for entity alignment",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=F-Xmgwiv2rF": {
    "title": "CodeBC: A More Secure Large Language Model for Smart Contract Code Generation in Blockchain",
    "volume": "review",
    "abstract": "Blockchain, a decentralized distributed ledger database, records transactions across multiple computers in a secure, transparent and tamper-resistant manner. To ensure this, smart contract code is introduced to predefine transaction rules, and stipulate that the code should automatically execute without intermediaries when someone calls it. That is, if malicious actors call the code with vulnerabilities, these automatic execution codes may cause significant economic losses to users. Therefore, the security of smart contract code is crucial in Blockchain domain. Currently, smart contracts are primarily manually written by developers, facing challenges such as experienced developer shortage, low development efficiency, and substantial security risks. There is an urgent need for code generation technology to assist both developers and non-professional programmers in creating secure and efficient smart contract codes.In this paper, we propose CodeBC, a more secure smart contract Code generation model for Blockchain, which employs a two-stage fine-tuning approach based on CodeLlama: the first stage uses a multi-task learning framework for code infilling and vulnerability detection, enhancing the model's understanding of smart contract code and its ability to identify security vulnerabilities; in the second stage, tags-guided instruction fine-tuning is employed to improve the model's comprehension of human instructions, thereby generating higher-security code.We construct an Blockchain-HumanEval dataset to assess whether the generated code meets human requirements. Experimental results demonstrate that CodeBC achieves higher BLEU, CodeBLEU, compilation pass rates and lower vulnerability rates compared to baselines, validating the effectiveness of our two-stage fine-tuning strategy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PquHFU8m_Ed": {
    "title": "K-QA: A Real-World Medical Q&A Benchmark",
    "volume": "review",
    "abstract": "Ensuring the accuracy of responses provided by large language models (LLMs) is crucial, particularly in clinical settings where incorrect information may directly impact patient health. To address this challenge, we construct K-QA, a dataset containing 1,212 patient questions originating from real-world conversations held on a popular clinical online platform. We employ a panel of in-house physicians to answer and manually decompose a subset of K-QA into self-contained statements. Additionally, we formulate two NLI-based evaluation metrics approximating recall and precision: (1) comprehensiveness, measuring the percentage of essential clinical information in the generated answer and (2) hallucination rate, measuring the number of statements from the physician-curated response contradicted by the LLM answer. Finally, we use K-QA along with these metrics to evaluate several state-of-the-art models, as well as the effect of in-context learning and medically-oriented augmented retrieval schemes developed by the authors. Our findings indicate that in-context learning improves the comprehensiveness of the models, and augmented retrieval is effective in reducing hallucinations. We will make K-QA available to to the community to spur research into medically accurate NLP applications",
    "checked": true,
    "id": "00d4e5bccaba9248538295f4e117c9620f676ec1",
    "semantic_title": "k-qa: a real-world medical q&a benchmark",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=sWWNiiDOBix": {
    "title": "Reasons to Reject? Aligning Language Models with Judgments",
    "volume": "review",
    "abstract": "As humans, we consistently interact with our peers and receive feedback in the form of natural language. This language feedback allows us to maintain appropriate behavior, and rectify potential errors. The question arises naturally: can we use language feedback to align large language models (LLMs)? In contrast to previous research that aligns LLMs with scalar rewards, we present the first systematic exploration of alignment through the lens of language feedback (i.e., judgment).We start with an in-depth investigation of potential methods that can be adapted for aligning LLMs with judgments, revealing that these methods cannot fully capitalize on judgments. To facilitate more effective utilization of judgments, we propose a novel framework, Contrastive Unlikelihood Training (CUT), that allows for fine-grained inappropriate content detection and correction based on judgments.Our results show that, with merely 1317 off-the-shelf judgment data, CUT (LLaMA2-13b) can beat the 175B DaVinci003 and surpass the best baseline by 52.34 points on AlpacaEval. CUT (LLaMA2-chat-13b) can also align LLMs in an iterative fashion using up-to-date model-specific judgments, improving performance from 81.09 to 91.36 points on AlpacaEval. Further analysis suggests that judgments hold greater potential than rewards in LLM alignment",
    "checked": true,
    "id": "485f8a429cf5f70c558181187f2d62e31784deaa",
    "semantic_title": "reasons to reject? aligning language models with judgments",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=5u1mRMPojDy": {
    "title": "Chinese Spoken Named Entity Recognition in Real-world Scenarios: Dataset and Approaches",
    "volume": "review",
    "abstract": "Spoken Named Entity Recognition (NER) aims to extract entities from speech. The extracted entities can help voice assistants better understand user's questions and instructions. However, current Chinese Spoken NER datasets are laboratory-controlled data that collected by reading existing texts in quiet environments, rather than natural spoken data, and the texts used for reading are also limited in topics. These limitations obstruct the development of Spoken NER in more natural and common real-world scenarios. To address this gap, we introduce a real-world Chinese Spoken NER dataset (RWCS-NER), encompassing open-domain daily conversations and task-oriented intelligent cockpit instructions. We compare several mainstream pipeline approaches on RWCS-NER. The results indicate that the current methods, affected by Automatic Speech Recognition (ASR) errors, do not perform satisfactorily in real settings. Aiming to enhance Spoken NER in real-world scenarios, we propose two approaches: self-training-asr and mapping then distilling (MDistilling). Experiments show that both approaches can achieve significant improvements, particularly MDistilling. Even compared with GPT4.0, MDistilling still reaches better results. We believe that our work will advance the field of Spoken NER in real-world settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E3tydLtucE-": {
    "title": "Advancing Event Causality Identification via Heuristic Semantic Consistency Inquiry Network",
    "volume": "review",
    "abstract": "Event Causality Identification (ECI) focuses on extracting causal relations between events in texts. Existing methods primarily utilize causal features and external knowledge to identify causality. However, such approaches fall short in two dimensions: (1) the causal features between events in a text often lack explicit clues, and (2) external knowledge may introduce bias, while specific problems require specific analyses. In light of these issues, we introduce a novel Semantic Consistency Inquiry (SemCI) to the ECI task and propose the Heuristic Semantic Consistency Discriminator (HSemCD), a model that is both straightforward and effective. HSemCD utilizes a Cloze Analyzer to facilitate a gap-filling game, aiming to help uncover the semantic dependency in the context. Subsequently, it assesses the semantic consistency between the fill-in token and the given sentence to detect the existence of causality. Through this assessment, HSemCD reveals the causal relations between events indirectly. Comprehensive experiments validate the effectiveness of HSemCD, which surpasses previous state-of-the-art methods on three widely used benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NkmSNrwOd92": {
    "title": "Chain-of-Specificity: An Iteratively Refining Method for Eliciting Knowledge from Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) exhibit remarkable generative capabilities, enabling the generation of valuable information. Despite these advancements, previous research found that LLMs sometimes struggle with adhering to specific constraints (e.g., in specific place or at specific time), at times even overlooking them, which leads to responses that are either too generic or not fully satisfactory. Existing approaches attempted to address this issue by decomposing or rewriting input instructions, yet they fall short in adequately emphasizing specific constraints and in unlocking the underlying knowledge (e.g., programming within the context of software development). In response, this paper proposes a simple yet effective method named Chain-of-Specificity (CoS). Specifically, CoS iteratively emphasizes the specific constraints in the input instructions, unlocks knowledge within LLMs, and refines responses. Experiments conducted on publicly available and self-build complex datasets demonstrate that CoS outperforms existing methods in enhancing generated content especially for the specificity. Besides, as the number of specific constraints increase, other baselines falter, while CoS still performs well. Moreover, we show that distilling responses generated by CoS effectively enhances the ability of smaller models to follow the constrained instructions. Resources of this paper will be released for further research",
    "checked": true,
    "id": "fa971b9918725a47acfddaa67831565129be81b0",
    "semantic_title": "chain-of-specificity: an iteratively refining method for eliciting knowledge from large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dl2jqGSn0y2": {
    "title": "Can ChatGPT's Performance be Improved on Verb Metaphor Detection Tasks? Bootstrapping and Combining Tacit Knowledge",
    "volume": "review",
    "abstract": "Metaphors Detection, as an important task in the field of Natural Language Processing, has been receiving sustained academic attention in recent years. Current research focuses on the development of supervised Metaphors Detection systems, which usually require large-scale, high-quality labeled data support. With the rapid development of large-scale generative language models (e.g., ChatGPT), they have been widely used in a number of domains, including automatic summarization, sentiment analysis, and question and answer systems. However, it is worth noting that the use of ChatGPT for unsupervised Metaphors Detection task is often challenged with less-than-expected performance. Therefore, the aim of this paper is to explore how to bootstrap and combine ChatGPT by detecting the most prevalent verb metaphors among metaphors. Our approach first utilizes ChatGPT to obtain literal collocations of target verbs and subject-object pairs of verbs in the text to be detected. Subsequently, these literal collocations and subject-object pairs are mapped to the same set of topics, and finally the verb metaphors are detected through the analysis of entailment relations. The experimental results show that our method achieves the best performance on the unsupervised verb Metaphors Detection task compared to past unsupervised methods or direct prediction using ChatGPT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dxMBwg8Bmjc": {
    "title": "Derivative-Free Optimization for Low-Rank Adaptation in Large Language Models",
    "volume": "review",
    "abstract": "Parameter-efficient tuning methods such as LoRA could achieve comparable performance to model tuning by tuning a small portion of the parameters. However, substantial computational resources are still required, as this process involves calculating gradients and performing back-propagation throughout the model. Much effort has recently been devoted to utilizing the derivative-free optimization method to eschew the computation of gradients and showcase an augmented level of robustness in few-shot settings. In this paper, we prepend the low-rank modules into each self-attention layer of the model and employ two derivative-free optimization methods to optimize these low-rank modules at each layer alternately. Extensive results on various tasks and language models demonstrate that our proposed method achieves substantial improvement and exhibits clear advantages in memory usage and convergence speed compared to existing gradient-based parameter-efficient tuning and derivative-free optimization methods in few-shot settings",
    "checked": true,
    "id": "59a48bf399484a2e5b28c1f1168639acc8cf3412",
    "semantic_title": "derivative-free optimization for low-rank adaptation in large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=p2EnwBLkfhG": {
    "title": "Direct Metric Optimization for Image Captioning through Reward-Weighted Augmented Data Utilization",
    "volume": "review",
    "abstract": "While image captioning is an essential field of vision language models (VLM), a lack of continuity between the learning objective and final performance metrics of VLMs complicates their training and optimization. Reinforcement learning (RL) can directly optimize such metrics, but it is accompanied by a significant computational cost, making it difficult to apply to recent large-scale VLMs. In this paper, we propose Direct Metric Optimization (DMO), which is a lightweight final-metric-optimizing training method. We replace the computationally expensive exploration process in RL with an offline, diverse text data augmentation and show that self-supervised training on reward-weighted augmented data leads to direct and stable metric optimization. Our experiments demonstrate that DMO achieves performance comparable to those of the state-of-the-art RL method while saving hundreds of times more model forwarding iterations and greater amounts of computation time. This suggests that DMO constitutes a promising alternative for metric optimization in the era of large-scale VLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wmL0DKQkpQJ": {
    "title": "Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information",
    "volume": "review",
    "abstract": "In recent years, Large Language Models (LLM) have emerged as pivotal tools in various applications. However, these models are susceptible to adversarial prompt attacks, where attackers can carefully curate input strings that mislead LLMs into generating incorrect or undesired outputs. Previous work has revealed that with relatively simple yet effective attacks based on discrete optimization, it is possible to generate adversarial prompts that bypass moderation and alignment of the models. This vulnerability to adversarial prompts underscores a significant concern regarding the robustness and reliability of LLMs. Our work aims to address this concern by introducing a novel approach to detecting adversarial prompts at a token level, leveraging the LLM's capability to predict the next token's probability. We measure the degree of the model's perplexity, where tokens predicted with high probability are considered normal, and those exhibiting high perplexity are flagged as adversarial. Additionaly, our method also integrates context understanding by incorporating neighboring token information to encourage the detection of contiguous adversarial prompt sequences. To this end, we design two algorithms for adversarial prompt detection: one based on optimization techniques and another on Probabilistic Graphical Models (PGM). Both methods are equipped with efficient solving methods, ensuring efficient adversarial prompt detection. Our token-level detection result can be visualized as heatmap overlays on the text sequence, allowing for a clearer and more intuitive representation of which part of the text may contain adversarial prompts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_NY1TABWEV1": {
    "title": "Reason out Your Layout: Evoking the Layout Master from Large Language Models for Text-to-Image Synthesis",
    "volume": "review",
    "abstract": "Recent advancements in text-to-image (T2I) generative models have shown remarkable capabilities in producing diverse and imaginative visuals based on text prompts. Despite the advancement, these diffusion models sometimes struggle to translate the semantic content from the text into images entirely. While conditioning on the layout has shown to be effective in improving the compositional ability of T2I diffusion models, they typically require manual layout input. In this work, we introduce a novel approach to improving T2I diffusion models using Large Language Models (LLMs) as layout generators. Our method leverages the Chain-of-Thought prompting of LLMs to interpret text and generate spatially reasonable object layouts. The generated layout is then used to enhance the generated images' composition and spatial accuracy. Moreover, we propose an efficient adapter based on a cross-attention mechanism, which explicitly integrates the layout information into the stable diffusion models. Our experiments demonstrate significant improvements in image quality and layout accuracy, showcasing the potential of LLMs in augmenting generative image models",
    "checked": true,
    "id": "a6bcc9f118dd3812d05d85d38a074cb5cf54ca56",
    "semantic_title": "reason out your layout: evoking the layout master from large language models for text-to-image synthesis",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=yuG7kmOmP2e": {
    "title": "TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation",
    "volume": "review",
    "abstract": "The recent advances in natural language processing have predominantly favored well-resourced English-centric models, resulting in a significant gap with low-resource languages.In this work, we introduce TURNA, a language model developed for the low-resource language Turkish and is capable of both natural language understanding and generation tasks.TURNA is pretrained with an encoder-decoder architecture based on the unified framework UL2 with a diverse corpus that we specifically curated for this purpose. We evaluated TURNA with three generation tasks and five understanding tasks for Turkish.The results show that TURNA outperforms several multilingual models in both understanding and generation tasks and competes with monolingual Turkish models in understanding tasks",
    "checked": true,
    "id": "a2ca24ae72fbbc54d41083307fc2a24d12f4f23c",
    "semantic_title": "turna: a turkish encoder-decoder language model for enhanced understanding and generation",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=AiIVrF8hwDq": {
    "title": "Fine-grained and Explanable Factuality Evaluation for Multimodal Summarization",
    "volume": "review",
    "abstract": "Multimodal summarization aims to generate a concise summary based on the input text and image. However, the existing methods always suffer from unfaithful output. To evaluate the factuality of multimodal summarization models, we propose two fine-grained and explainable evaluation frameworks for different application scenarios, i.e. reference-based factuality evaluation framework and reference-free factuality evaluation framework. Notably, the reference-free factuality evaluation framework doesn't need ground truth and hence it has a wider application scenario. To evaluate the effectiveness of the proposed frameworks, we compute the correlation between our frameworks and the other metrics. The experimental results show the effectiveness of our proposed method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dd3anRoaW1h": {
    "title": "ProxyQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have succeeded remarkably in understanding long-form contents. However, exploring their capability for generating long-form contents, such as reports and articles, has been relatively unexplored and inadequately assessed by existing benchmarks. The prevalent evaluation methods, which predominantly rely on crowdsourcing, are recognized for their labor-intensive nature and lack of efficiency, whereas automated metrics, such as the ROUGE score, demonstrate discordance with human judgment criteria. In this paper, we propose ProxyQA, an innovative framework dedicated to assessing long-text generation. ProxyQA comprises in-depth human-curated meta-questions spanning various domains, each accompanied by specific proxy-questions with pre-annotated answers. LLMs are tasked to generate extensive content in response to these meta-questions, by engaging an evaluator and incorporating the generated texts as contextual background, ProxyQA assesses the generated content's quality through the evaluator's accuracy in addressing the proxy-questions. We examine multiple LLMs, emphasizing ProxyQA's demanding nature as a high-quality assessment tool. Human evaluation demonstrates that the proxy-question method is notably self-consistent and aligns closely with human evaluative standards. The dataset and leaderboard will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nUlfYks9AqS": {
    "title": "AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension",
    "volume": "review",
    "abstract": "Recently, instruction-following audio-language models have received broad attention for human-audio interaction. However, the absence of benchmarks capable of evaluating audio-centric interaction capabilities has impeded advancements in this field. Previous models primarily focus on assessing different fundamental tasks, such as Automatic Speech Recognition (ASR), and lack an assessment of the open-ended generative capabilities centered around audio. Thus, it is challenging to track the progression in the Large Audio-Language Models (LALMs) domain and to provide guidance for future improvement. In this paper, we introduce AIR-Bench (Audio Instruction Benchmark), the first benchmark designed to evaluate the ability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format. AIR-Bench encompasses two dimensions: foundation and chat benchmarks. The former consists of 19 tasks with approximately 19k single-choice questions, intending to inspect the basic single-task ability of LALMs. The latter one contains 2k instances of open-ended question-and-answer data, directly assessing the comprehension of the model on complex audio and its capacity to follow instructions. Both benchmarks require the model to generate hypotheses directly. We design a unified framework that leverages advanced language models, such as GPT-4, to evaluate the scores of generated hypotheses given the meta-information of the audio. Experimental results demonstrate a high level of consistency between GPT-4-based evaluation and human evaluation. By revealing the limitations of existing LALMs through evaluation results, AIR-Bench can provide insights into the direction of future research",
    "checked": true,
    "id": "57a16e741016638dfd5b5f4c8a839c65ef83b817",
    "semantic_title": "air-bench: benchmarking large audio-language models via generative comprehension",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=6S95QDk4fs7": {
    "title": "Improving Domain Adaptation through Extended-Text Reading Comprehension",
    "volume": "review",
    "abstract": "To enhance the domain-specific capabilities of large language models, continued pre-training on a domain-specific corpus is a prevalent method. Recent work demonstrates that adapting models using reading comprehension data formatted by regex-based patterns can significantly improve performance on domain-specific tasks. However, regex-based patterns are incapable of parsing raw corpora using domain-specific knowledge. Furthermore, the question and answer pairs are extracted directly from the corpus in predefined formats offers limited context. To address this limitation, we improve reading comprehension via LLM and clustering. LLM focuses on leveraging domain knowledge within the corpus to refine comprehension stage, while clustering supplies relevant knowledge by extending the context to enrich reading stage. Additionally, our method incorporates parameter-efficient fine-tuning to improve the efficiency of domain adaptation. In comparison to AdaptLLM, our method achieves an improvement exceeding 5% in domain-specific tasks",
    "checked": true,
    "id": "c1474dc03848cb26d118bc37c26636d7082c3854",
    "semantic_title": "improving domain adaptation through extended-text reading comprehension",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=V7IDyuC9oh_": {
    "title": "EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction",
    "volume": "review",
    "abstract": "There has been a rising interest in utilizing tools in applications of autonomous agents based on large language models (LLMs) to address intricate real-world tasks. To develop LLM-based agents, it usually requires LLMs to understand many tool functions from different tool documentations. However, these documentations could be diverse, redundant, or incomplete, which immensely affects the capability of LLMs in using tools. To solve this, we introduce EASYTOOL, a framework transforming diverse and lengthy tool documentation into a unified and concise tool instruction for easier tool usage. EASYTOOL purifies essential information from extensive tool documentation of different sources, and elaborates a unified interface (i.e., tool instruction) to offer standardized tool descriptions and functionalities for LLM-based agents. Extensive experiments on multiple different tasks demonstrate that EASYTOOL can significantly reduce token consumption and improve the performance of LLM-based agents on tool utilization in real-world scenarios",
    "checked": true,
    "id": "bf21281f3faba32b275012bc90b10e7e988b8867",
    "semantic_title": "easytool: enhancing llm-based agents with concise tool instruction",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=wlxDJ-rMsiI": {
    "title": "SPOR: A Comprehensive and Practical Evaluation Method for Compositional Generalization in Data-to-Text Generation",
    "volume": "review",
    "abstract": "Compositional generalization is an important ability of language models and has many different manifestations. For data-to-text generation, previous research on this ability is limited to a single manifestation called Systematicity and lacks consideration of large language models (LLMs), which cannot fully cover practical application scenarios. In this work, we propose SPOR, a comprehensive and practical evaluation method for compositional generalization in data-to-text generation. SPOR includes four aspects of manifestations (Systematicity, Productivity, Order invariance, and Rule learnability) and allows high-quality evaluation without additional manual annotations based on existing datasets. We demonstrate SPOR on two different datasets and evaluate some existing language models including LLMs. We find that the models are deficient in various aspects of the evaluation and need further improvement. Our work shows the necessity for comprehensive research on different manifestations of compositional generalization in data-to-text generation and provides a framework for evaluation",
    "checked": true,
    "id": "b0621ed0dce4dcf210eb4a75b9d8175a404b56ee",
    "semantic_title": "spor: a comprehensive and practical evaluation method for compositional generalization in data-to-text generation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=5i4S-FxC-i-": {
    "title": "CareAgent: In Our Next Session, Let Me Actively Care for You",
    "volume": "review",
    "abstract": "Despite the fact that the large language models (LLMs) facilitate conversations with rationalization and knowledge, they are still restricted to the passive response mechanism that relies on user instructions, which are hard to actively care for the user. To realize the actively caring ability, we propose an active conversational agent (ACA) named CareAgent which creates new session to approach user potential interest. Specifically, inspired by the Jung's theory of psychological types and the active exploration mechanism of agent in environment, we designed three components to support the goal of caring for user. The Character Extractor (CE) obtains the personality through Myers-Briggs Type Indicator (MBTI) and attributes for character descriptions; the Memory Reconstructor (MR) achieves multi-topic summaries based on multi-clue branching for complete memories; the Decision Adapter (DA) selects the best topic summary as the background memory and adapts the agent intention to control the scenario of new session. The results of experiments demonstrated that CareAgent was able to maintain reliability in character understanding and extract complete multi-topic summaries from conversational history. Evaluators also believed that this agent enhanced the actively caring and personification level in new session",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HacUOhH8nFq": {
    "title": "\\textbf{TEAL}: \\textbf{T}okenize and \\textbf{E}mbed \\textbf{ALL} for Multi-modal Large Language Models",
    "volume": "review",
    "abstract": "Despite Multi-modal Large Language Models (MM-LLMs) have made exciting strides recently, they are still struggling to efficiently model the interactions among multi-modal inputs and the generation in non-textual modalities. In this work, we propose \\textit{TEAL (Tokenize and Embed ALL)}, an approach to treat the input from any modality as a token sequence and learn a joint embedding space for all modalities. Specifically, for the input from any modality, \\textit{TEAL} firstly discretizes it into a token sequence with the off-the-shelf tokenizer and embeds the token sequence into a joint embedding space with a learnable embedding matrix. MM-LLMs just need to predict the multi-modal tokens autoregressively as conventional textual LLMs do. Finally, the corresponding de-tokenizer is applied to generate the output in each modality based on the predicted token sequence. With the joint embedding space, \\textit{TEAL} enables the frozen LLMs to perform both understanding and generation tasks involving non-textual modalities, such as image and audio. Thus, the textual LLM can just work as an interface and maintain its high performance in textual understanding and generation. Experiments show that \\textit{TEAL} achieves substantial improvements in multi-modal understanding, and implements a simple scheme for multi-modal generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nWReNjlqtxN": {
    "title": "LV-CHAT: Facilitating Long Video Comprehension",
    "volume": "review",
    "abstract": "Enabling large language models (LLMs) to read videos is vital for multimodal LLMs. Existing works show promise on short videos whereas long video (longer than e.g. 1 minute) comprehension remains challenging. The major problem lies in the over-compression of videos, i.e., the encoded video representations are not enough to represent the whole video. To address this issue, we propose Long Video Chat (LV-CHAT), where Frame-Scalable Encoding (FSE) is introduced to dynamically adjust the number of embeddings in alignment with the duration of the video to ensure long videos are not overly compressed into a few embeddings. To deal with long videos whose length is beyond videos seen during training, we propose Interleaved Frame Encoding (IFE), repeating positional embedding and interleaving multiple groups of videos to enable long video input, avoiding performance degradation due to overly long videos. Experimental results show that LV-CHAT significantly outperforms existing methods by up to 27% in accuracy on long-video QA datasets and long-video captioning benchmarks. Codes and data will be released upon publishing",
    "checked": false,
    "id": "8f6061730f9964ebda3bac4f99e5a170df2c38c9",
    "semantic_title": "lvchat: facilitating long video comprehension",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=AQYowVRKUgP": {
    "title": "Retrieval over Large Language Model's Latent Causal Knowledge Graph for Deductive Reasoning",
    "volume": "review",
    "abstract": "Deductive reasoning refers to the task of drawing conclusions based on a premise. While some deductive reasoning benchmarks exist, none focus on causal deductive reasoning and are from real-world applications. Therefore, this paper explores the causal deductive reasoning task conducted by Accident Investigators, who analyze accidents to determine probable causes. Recently, large language models (LLMs) used with prompt engineering techniques like retrieval-augmented generation (RAG) have demonstrated remarkable performance across various natural language processing benchmarks. However, adapting these techniques to handle scenarios with no knowledge bases and to different data structures, such as graphs, remains an ongoing challenge. In our study, we introduce a novel framework leveraging LLMs' decent ability to detect and infer causal relations to construct a causal Knowledge Graph (KG) which represents knowledge that the LLM recognizes. Additionally, we propose a RoBERTa-based Transformer Graph Neural Network (RoTG) specifically designed to select relevant nodes within this KG. Integrating RoTG-retrieved causal chains into prompts effectively enhances LLM performance, demonstrating usefulness of our approach in advancing LLMs' causal deductive reasoning capabilities",
    "checked": false,
    "id": "26702869774950e03b10a3657bca0621aa0d8d07",
    "semantic_title": "kg-rag: bridging the gap between knowledge and creativity",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=T5v_BTLVCHj": {
    "title": "Extreme Miscalibration and the Illusion of Adversarial Robustness",
    "volume": "review",
    "abstract": "Deep learning-based Natural Language Processing (NLP) models are vulnerable to adversarial attacks, where small perturbations can cause a model to misclassify. Adversarial Training (AT) is often used to increase model robustness. However, we have discovered an intriguing phenomenon: deliberately or accidentally miscalibrating models masks gradients in a way that interferes with adversarial attack search methods, giving rise to an apparent increase in robustness. We show that this observed gain in robustness is an illusion of robustness (IOR), and demonstrate how an adversary can perform various forms of test-time temperature calibration to nullify the aforementioned interference and allow the adversarial attack to find adversarial examples. Hence, we urge the NLP community to incorporate test-time temperature scaling into their robustness evaluations to ensure that any observed gains are genuine. Finally, we show how the temperature can be scaled during training to improve genuine robustness",
    "checked": true,
    "id": "4efc5543d34de96e0f6eb97cc9ccecbaf94ceae3",
    "semantic_title": "extreme miscalibration and the illusion of adversarial robustness",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1fCBgKYFsfn": {
    "title": "SToRI: Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in Vision-Language Models",
    "volume": "review",
    "abstract": "A text encoder within Vision-Language Models (VLMs) plays a crucial role in translating textual input into an embedding space shared with images, thereby facilitating the interpretative analysis of vision tasks through natural language. Despite varying significance of different textual elements within a sentence, depending on the context or intended purpose, efforts to control the prominence of diverse textual information when constructing text embeddings have been lacking. This paper proposes a framework called Semantic Token Reweighting, aiming to incorporate Controllability while ensuring Interpretability of text embeddings (SToRI). SToRI refines the text encoding process in VLMs by differentially weighting semantic elements based on contextual importance, enabling finer control over emphasis responsive to user preferences and data-driven insights. The efficacy of SToRI is demonstrated through comprehensive experiments, showcasing its strength in image retrieval tailored to user preferences and its capability in few-shot image classification tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9mYEb-l30Qu": {
    "title": "Palo: A Polyglot Large Multimodal Model for 5B People",
    "volume": "review",
    "abstract": "In pursuit of more inclusive Vision-Language Models (VLMs), this study introduces a Large Multilingual Multimodal Model called \\textsc{Palo}. \\textsc{Palo} offers visual reasoning capabilities in 10 major languages, including English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian, Urdu, and Japanese, that span a total of $\\sim$5B people (65\\% of the world population). Our approach involves a semi-automated translation approach to adapt the multimodal instruction dataset from English to the target languages using a fine-tuned Large Language Model, thereby ensuring high linguistic fidelity while allowing scalability due to minimal manual effort. The incorporation of diverse instruction sets helps us boost overall performance across multiple languages especially those that are underrepresented like Hindi, Arabic, Bengali, and Urdu. The resulting models are trained across three scales (1.7B, 7B and 13B parameters) to show the generalization and scalability where we observe substantial improvements compared to strong baselines. We also propose the first multilingual multimodal benchmark for the forthcoming approaches to evaluate their vision-language reasoning capabilities across languages. Our codes, models and datasets will be publicly released",
    "checked": true,
    "id": "990062c5ce04f6539d3ad4f3f24d12cd63b21706",
    "semantic_title": "palo: a polyglot large multimodal model for 5b people",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=u656zXmyEC7": {
    "title": "ChartLlama: A Multimodal LLM for Chart Understanding and Generation",
    "volume": "review",
    "abstract": "Multi-modal large language models have demonstrated impressive performances on most vision-language tasks. However, the model generally lacks the understanding capabilities for specific domain data, particularly when it comes to interpreting chart figures. This is mainly due to the lack of relevant multi-modal instruction tuning datasets. In this article, we create a high-quality instruction-tuning dataset leveraging GPT-4. We develop a multi-step data generation process in which different steps are responsible for generating tabular data, creating chart figures, and designing instruction tuning data separately. Our method's flexibility enables us to generate diverse, high-quality instruction-tuning data consistently and efficiently while maintaining a low resource expenditure. Additionally, it allows us to incorporate a wider variety of chart and task types not yet featured in existing datasets. Next, we introduce ChartLlama, a multi-modal large language model that we've trained using our created dataset. ChartLlama outperforms all prior methods in ChartQA, Chart-to-text, and Chart-extraction evaluation benchmarks. Additionally, ChartLlama significantly improves upon the baseline in our specially compiled chart dataset, which includes new chart and task types. The results of ChartLlama confirm the value and huge potential of our proposed data generation method in enhancing chart comprehension",
    "checked": true,
    "id": "40cd34f260d5596263654caf9d911d4355bf4f4e",
    "semantic_title": "chartllama: a multimodal llm for chart understanding and generation",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=8aLVG7K7GAZ": {
    "title": "Towards Red Teaming in Multimodal and Multilingual Translation",
    "volume": "review",
    "abstract": "This paper presents the first study on human-based red teaming for Machine Translation (MT), marking a significant step towards understanding and improving the performance of translation models. We delve into both human-based red teaming and a study on automation, reporting lessons learned and providing recommendations for both translation models and red teaming drills. This pioneering work opens up new avenues for research and development in the field of MT",
    "checked": true,
    "id": "c717b7367a54e6f91ef098d485782bde668b2049",
    "semantic_title": "towards red teaming in multimodal and multilingual translation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=d54KdZ__fTb": {
    "title": "Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation",
    "volume": "review",
    "abstract": "Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging. It not only requires checking whether the VLM follows the given instruction but also verifying whether the text output is properly grounded on the given image. Inspired by the recent approach of evaluating LMs with LMs, in this work, we propose to evaluate VLMs with VLMs. For this purpose, we present a new feedback dataset called the Perception Collection, encompassing 15K customized score rubrics that users might care about during assessment. Using the Perception Collection, we train Prometheus-Vision, the first open-source VLM evaluator model that can understand the user-defined score criteria during evaluation. Prometheus-Vision shows the highest Pearson correlation with human evaluators and GPT-4V among open-source models, showing its effectiveness for transparent and accessible evaluation of VLMs. We open-source our code, dataset, and model",
    "checked": true,
    "id": "556bfb22059245c3dd388e97867cd88aa9dd078d",
    "semantic_title": "prometheus-vision: vision-language model as a judge for fine-grained evaluation",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=emWxlvV732": {
    "title": "Muffin: Mitigating Unhelpfulness in Emotional Support Conversations with Multifaceted AI Feedback",
    "volume": "review",
    "abstract": "Emotional support conversation systems are designed to alleviate users' emotional distress and assist them in overcoming their challenges. While previous studies have made progress, their models occasionally generate unhelpful responses, which are intended to be supportive but instead have counterproductive effects. Since unhelpful responses can hinder the effectiveness of emotional support, it is crucial to mitigate them within conversations. Our solution is motivated by two principal considerations: (1) multiple facets of emotional support are expected to be considered when developing emotional support conversation models, and (2) directly reducing the probability of generating unhelpful responses can effectively mitigate their occurrence. Accordingly, we introduce a novel $\\textbf{model-agnostic}$ framework named $\\underline{M}$itigating $\\underline{u}$nhelpfulness with multi\\underline{f}aceted AI $\\underline{f}$eedback for emot$\\underline{i}$o$\\underline{n}$al support ($\\textit{Muffin}$). It first employs a multifaceted AI feedback module designed to assess the helpfulness model responses across various facets of emotional support. Leveraging contrastive learning, Muffin then reduces the unhelpful responses' likelihoods. To validate the effectiveness of our proposed framework, we apply Muffin to various previous emotional support generation models, including the state-of-the-art. Experimental results demonstrate that Muffin can significantly mitigate unhelpful response generation while enhancing response fluency and relevance",
    "checked": false,
    "id": "0fe0c070df0bfbbc30ce444a5251ece063091271",
    "semantic_title": "mitigating unhelpfulness in emotional support conversations with multifaceted ai feedback",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tOLCqreMCw": {
    "title": "Pattern-Aware Chain-of-Thought Prompting in Large Language Models",
    "volume": "review",
    "abstract": "Chain-of-thought (CoT) prompting can guide language models to engage in complex multi-step reasoning. The quality of provided demonstrations significantly impacts the success of downstream inference tasks. While existing automated methods prioritize accuracy and semantics in these demonstrations, we show that the underlying reasoning patterns play a more crucial role in such tasks. In this paper, we propose PA-CoT, a prompting method that considers the diversity of demonstration patterns. By incorporating patterns such as step length and mathematical symbols within intermediate steps, PA-CoT effectively mitigates the issue of bias induced by demonstrations and enables better generalization to diverse scenarios. We conduct experiments on six reasoning benchmark tasks using two open-source LLMs. The results show that our method substantially enhances reasoning performance and exhibits robustness to errors. The code will be made available upon acceptance",
    "checked": true,
    "id": "449257147fe40e0016f3ef89a62f20db8ff29039",
    "semantic_title": "pattern-aware chain-of-thought prompting in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Icn2e33up5f": {
    "title": "Tuning LLMs with Contrastive Alignment Instructions for Machine Translation in Unseen, Low-resource Languages",
    "volume": "review",
    "abstract": "This article introduces contrastive alignment instructions (AlignInstruct) to address two challenges in machine translation (MT) on large language models (LLMs). One is the expansion of supported languages to previously unseen ones. The second relates to the lack of data in low-resource languages. Model fine-tuning through MT instructions (MTInstruct) is a straightforward approach to the first challenge. However, MTInstruct is limited by weak cross-lingual signals inherent in the second challenge. AlignInstruct emphasizes cross-lingual supervision via a cross-lingual discriminator built using statistical word alignments. Our results based on fine-tuning the BLOOMZ models (1b1, 3b, and 7b1) in up to 24 unseen languages showed that: (1) LLMs can effectively translate unseen languages using MTInstruct; (2) AlignInstruct led to consistent improvements in translation quality across 48 translation directions involving English; (3) Discriminator-based instructions outperformed their generative counterparts as cross-lingual instructions; (4) AlignInstruct improved performance in 30 zero-shot directions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YMK3qJtFI6": {
    "title": "Planning First, Question Second: An LLM-Guided Method for Controllable Question Generation",
    "volume": "review",
    "abstract": "In the field of education, for better assessment of students' abilities, generated questions often need to meet experts' requirements, indicating the need for controllable question generation (CQG). However, current CQG methods mainly focus on difficulty control, neglecting the control of question content and assessed abilities, which are also crucial in educational QG. In this paper, we introduce an LLM-guided method PFQS (for Planning First, Question Second), which utilizes Llama 2 to generate an answer plan and then generates questions based on it. The plan not only includes candidate answers but also integrates LLM's understanding and multiple requirements, which make question generation simple and controllable. We evaluate our approach on the FairytaleQA dataset, a well-structured QA dataset derived from child-friendly storybooks. In the dataset, the attribute label represents content control, while the local_or_sum and ex_or_im labels denote difficulty control. Experimental results demonstrate that our approach outperforms previous state-of-the-art results and achieves better consistency with requirements compared to prompt-based method. Further application of our method to Llama 2 and Mistral also leads to improved requirement consistency in a zero-shot setting",
    "checked": false,
    "id": "1280f0dfaded71fb083c8d196ae012798ff66d4d",
    "semantic_title": "developing supervision capacity for training rural generalist doctors in small towns in victoria",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=t8gdho7U4UQ": {
    "title": "Unsupervised Distractor Generation via Large Language Model Distilling and Counterfactual Contrastive Decoding",
    "volume": "review",
    "abstract": "Within the context of reading comprehension, the task of Distractor Generation (DG) aims to generate several incorrect options to confuse readers. In recent years, the emergence of Large Language Models (LLMs) provides a potential for unsupervised DG without expensive human-annotated distractor labels. In this paper, we leverage LLMs as a cost-effective annotator to enhance the DG capability of smaller student models. To perform knowledge distilling, we propose a dual task training framework that integrates pseudo distractors from LLMs and answer information as the objective target with a two-stage training process. Moreover, we devise a counterfactual contrastive decoding mechanism for increasing the distracting capability of the DG model. Experiments show that our unsupervised generation method with Bart-base greatly surpasses GPT-3.5-turbo zero-shot performance with only 200$\\times$ fewer model parameters. Our proposed unsupervised DG method offers a cost-effective framework for practical reading comprehension applications, without the need of laborious distractor annotation and costly large-size models",
    "checked": true,
    "id": "d9f9d619c6dd1cee9c748cd4e1a947cadf1fe255",
    "semantic_title": "unsupervised distractor generation via large language model distilling and counterfactual contrastive decoding",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Riupf7d6Jb": {
    "title": "WizardEvent: Empowering Event Reasoning by Hybrid Event-Aware Instruction Tuning",
    "volume": "review",
    "abstract": "Events refer to specific occurrences, incidents, or happenings that take place under a particular background. Event reasoning aims to reason according to certain relations. The cutting-edge techniques for event reasoning play crucial and fundamental abilities underlying various natural language processing applications. Large language models (LLMs) have made significant advancements in event reasoning owing to their wealth of knowledge and reasoning capabilities. However, open-source LLMs currently in use do not consistently demonstrate exceptional proficiency in managing event reasoning. This discrepancy arises from insufficient learning of knowledge of event relations and incomplete reasoning paradigms. In this paper, we propose WizardEvent, the hybrid event-aware instruction tuning leading to better event reasoning abilities. Specifically, we first represent the events and relation of the event relational knowledge in a novel structure. We then mine the knowledge from raw text. Second, we introduce the prototypical event reasoning paradigms which include four reasoning formats. Lastly, we wrap our constructed \\eqs with our reasoning paradigms to create the instruction tuning dataset. We fine-tune to obtain WizardEvent using this enriched dataset, significantly improving their event reasoning. The performance of WizardEvent is rigorously evaluated through a series of extensive experiments across 10 event reasoning tasks. We also annotate a new dataset for event relational knowledge evaluation. The results from these evaluations demonstrate that WizardEvent substantially outperforms other instruction-tuned models, indicating the success of our approach in enhancing LLMs' proficiency in event reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yWCQU54VQ45": {
    "title": "A Comprehensive Evaluation on Event Reasoning of Large Language Models",
    "volume": "review",
    "abstract": "Event reasoning is a fundamental ability that underlies many applications. It requires event schema knowledge to perform global reasoning and needs to deal with the diversity of the inter-event relations and the reasoning paradigms. How well LLMs accomplish event reasoning in terms of competence and knowledge remains unknown. To mitigate this disparity, we comprehensively evaluate the abilities of event reasoning of LLMs. We introduce a novel benchmark EV2 for EValuation of EVent reasoning. EV2 consists of two levels of evaluation of schema and instance and is comprehensive in relations and reasoning paradigms. We conduct extensive experiments on EV2. We find that LLMs have abilities to accomplish event reasoning but their performances are far from satisfactory. We also notice the imbalance of event reasoning abilities in LLMs. Besides, LLMs have event schema knowledge, however, they're not aligned with humans on how to utilize the knowledge.Based on these findings, we introduce two methods to guide the LLMs to utilize the event schema knowledge. Both methods achieve improvements",
    "checked": true,
    "id": "1885369d494c0d51aa785665ae266d9380804bc4",
    "semantic_title": "a comprehensive evaluation on event reasoning of large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rPmSO8YR9K": {
    "title": "Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations",
    "volume": "review",
    "abstract": "Long-form generations from large language models (LLMs) contain a mix of factual and non-factual claims, making evaluating factuality difficult.To evaluate *factual precision* of long-form generations in a more fine-grained way, prior works propose to decompose long-form generations into multiple verifiable facts and verify those facts independently. The factuality of the generation is the proportion of verifiable facts among all the facts.Such methods assume that combining factual claims forms a factual paragraph.This paper shows that the assumption can be violated.We show that LLMs can generate paragraphs that contain verifiable facts, but the facts are combined to form a non-factual paragraph due to entity ambiguity.We further reveal that existing factual precision metrics, including FActScore and citation recall, cannot properly evaluate the factuality of these non-factual paragraphs.To address this, we introduce an enhanced metric, **D-FActScore**, specifically designed for content with ambiguous entities.We evaluate the D-FActScores of people biographies generated by retrieval-augmented LLMs.We show that D-FActScore can better assess the factuality of paragraphs with entity ambiguity than FActScore.We also find that four widely used open-source LLMs tend to mix information of distinct entities to form non-factual paragraphs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LujaF5Shyo": {
    "title": "MR-GSM8K: A Meta-Reasoning Revolution in Large Language Model Evaluation",
    "volume": "review",
    "abstract": "In this work, we introduce a novel evaluation paradigm for Large Language Models, one that challenges them to engage in meta-reasoning. This approach addresses critical shortcomings in existing math problem-solving benchmarks, traditionally used to evaluate the cognitive capabilities of agents. Our paradigm shifts the focus from result-oriented assessments, which often overlook the reasoning process, to a more holistic evaluation that effectively differentiates the cognitive capabilities among models. For example, in our benchmark, GPT-4 demonstrates a performance five times better than GPT3.5. The significance of this new paradigm lies in its ability to reveal potential cognitive deficiencies in LLMs that current benchmarks, such as GSM8K, fail to uncover due to their saturation and lack of effective differentiation among varying reasoning abilities. Our comprehensive analysis includes several state-of-the-art math models from both open-source and closed-source communities, uncovering fundamental deficiencies in their training and evaluation approaches",
    "checked": false,
    "id": "490e465ef0909f01a82916f4adab0c410576c866",
    "semantic_title": "mr-gsm8k: a meta-reasoning benchmark for large language model evaluation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=T6mlK54D_S": {
    "title": "Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations",
    "volume": "review",
    "abstract": "Existing research predominantly focuses on developing powerful language learning models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs.Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, \\texttt{MGSM8KInstruct}, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named \\ModelName, notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, \\ModelName-13B reaches 47.6\\% accuracy which exceeds ChatGPT 46.3\\% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights from extensive experiments: (1) When extending the rejection sampling strategy to the multilingual context, it proves effective for model performances, albeit limited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT) across multiple languages not only significantly enhances model performance multilingually but also elevates their monolingual performance. This indicates that crafting multilingual corpora can be regarded as a vital strategy for enhancing model performance in a specific language, especially in mathematical reasoning tasks. For instance, \\ModelName-7B improves its counterparts that trained on English from 42.2\\% to 50.8\\% on GSM8K testset",
    "checked": true,
    "id": "07cdf957a11506f87fbc030dcfaaa6399847648c",
    "semantic_title": "breaking language barriers in multilingual mathematical reasoning: insights and observations",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=579gET1Q6u": {
    "title": "Unified Hallucination Detection for Multimodal Large Language Models",
    "volume": "review",
    "abstract": "Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucina- tion. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguard- ing of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD through meticulous evaluation and comprehensive analysis. We also provide strategic insights on the application of specific tools for addressing various categories of hallucinations",
    "checked": true,
    "id": "19e909f88b8b9b0635bd6e441094e1738c3bba9a",
    "semantic_title": "unified hallucination detection for multimodal large language models",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=kdzX2spL5H": {
    "title": "While Large Language Models May Be Good Mathematical Problem Solvers, Are They Good Examiners?",
    "volume": "review",
    "abstract": "Recently, large language models (LLMs) have demonstrated breakthrough mathematical problem-solving capabilities in grade school math word problems (MWP). For example, on the MWP benchmark GSM8K, the accuracy of GPT-3.5-Turbo and MetaMath-70B reaches 80.80% and 82.30%, respectively. One question arises, does it mean that LLMs have truly mastered related mathematical problem-solving abilities, such as the ability to evaluate the mathematical reasoning process of MWP? In this paper, by presenting two types of benchmarks, where MCGSM8K aims at selecting one correct solution from four solutions, while GSM8K-Judgement judges whether a solution to a given question is true or false, we show that the ability of most LLMs to evaluate the mathematical reasoning process of MWP is far from sufficient. To compensate for this issue, we propose hybrid supervised fine-tuning data from the training data of GSM8K, MCGSM8K, and GSM8K-Judegment, which significantly improves performance on the proposed reasoning process evaluation benchmarks. For example, fine-tuning improves the performance of LLaMA-2-13B from 33.51% to 70.89% on MCGSM8K. In conclusion, we experimentally demonstrate that most LLMs have limited ability to evaluate the mathematical reasoning process of MWP, which can be enhanced through fine-tuning",
    "checked": false,
    "id": "b6ceb6ee67bbf9e96b2dd5f3383cbd943d8c2d8c",
    "semantic_title": "and simulation of power electronics system and simulation of electrical machines and electromagnetic",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SsZtaCvI2E": {
    "title": "Modelling Variability in Human Annotator Simulation",
    "volume": "review",
    "abstract": "Human annotator simulation (HAS) serves as a cost-effective substitute for human evaluation tasks such as data annotation and system assessment. It is important to incorporate the variability present in human evaluation into HAS, since it helps capture diverse subjective interpretations and mitigate potential biases and over-representation. This work introduces a novel framework for modelling variability in HAS. Conditional softmax flow (S-CNF) is proposed to model the distribution of subjective human annotations, which leverages diverse human annotations via meta-learning. This enables the efficient generation of annotations that exhibit human variability for unlabelled input. In addition, a wide range of evaluation metrics are adopted to assess the capability and efficiency of HAS systems in predicting the aggregated behaviours of human annotators, matching the distribution of human annotations, and simulating the inter-annotator disagreements. Results demonstrate that the proposed method achieves state-of-the-art performance on two real-world human evaluation tasks: emotion recognition and toxic speech detection",
    "checked": false,
    "id": "20f81c6c71d381123590aa85ef3f9a0d30420249",
    "semantic_title": "methods for including human variability in system performance models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5EWZN866GW": {
    "title": "Inductive or Deductive? Rethinking the Fundamental Reasoning Abilities of LLMs",
    "volume": "review",
    "abstract": "Reasoning encompasses two typical types: deductive reasoning and inductive reasoning. Despite extensive research into the reasoning capabilities of Large Language Models (LLMs), most studies have failed to rigorously differentiate between inductive and deductive reasoning, leading to a blending of the two. This raises an essential question: In LLM reasoning, which poses a greater challenge - deductive or inductive reasoning? While the deductive reasoning capabilities of LLMs, (i.e. their capacity to follow instructions in reasoning tasks), have received considerable attention, their abilities in true inductive reasoning remain largely unexplored. To delve into the true inductive reasoning capabilities of LLMs, we propose a novel framework, SolverLearner. This framework enables LLMs to learn the underlying function (i.e., $y = f_w(x)$), that maps input data points $(x)$ to their corresponding output values $(y)$, using only in-context examples. By focusing on inductive reasoning and separating it from LLM-based deductive reasoning, we can isolate and investigate inductive reasoning of LLMs in its pure form via SolverLearner. Our observations reveal that LLMs demonstrate remarkable inductive reasoning capabilities through SolverLearner, achieving near-perfect performance with ACC of 1 in most cases. Surprisingly, despite their strong inductive reasoning abilities, LLMs tend to relatively lack deductive capabilities, particularly in tasks involving ``counterfactual'' reasoning",
    "checked": true,
    "id": "f42b97cdfbf1a78c02e78cfce6f8b0e277766ae2",
    "semantic_title": "inductive or deductive? rethinking the fundamental reasoning abilities of llms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nt1PDJbQDFm": {
    "title": "A Thorough Examination of Decoding Methods in the Era of LLMs",
    "volume": "review",
    "abstract": "Decoding methods play an indispensable role in converting language models from next-token predictors into practical task solvers. Prior research on decoding methods, primarily focusing on task-specific models, may not extend to the current era of general-purpose large language models (LLMs). Moreover, the recent influx of decoding strategies has further complicated this landscape. This paper provides a comprehensive and multifaceted analysis of various decoding methods within the context of LLMs, evaluating their performance, robustness to hyperparameter changes, and decoding speeds across a wide range of tasks, models, and deployment environments. Our findings reveal that decoding method performance is notably task-dependent and influenced by factors such as alignment, model size, and quantization. Intriguingly, sensitivity analysis exposes that certain methods achieve superior performance at the cost of extensive hyperparameter tuning, highlighting the trade-off between attaining optimal results and the practicality of implementation in varying contexts",
    "checked": true,
    "id": "7428003193db96ef573155c3e3d3daf3e361a048",
    "semantic_title": "a thorough examination of decoding methods in the era of llms",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=Ooqr2Qs3F7J": {
    "title": "Is ChatGPT More Empathetic than Humans?",
    "volume": "review",
    "abstract": "This paper investigates the empathetic responding capabilities of ChatGPT, particularly its latest iteration, GPT-4, in comparison to human-generated responses to a wide range of emotional scenarios, both positive and negative. We employ a rigorous evaluation methodology, involving a between-groups study with 600 participants, to evaluate the level of empathy in responses generated by humans and ChatGPT. ChatGPT is prompted in two distinct ways: a standard approach and one explicitly detailing empathy's cognitive, affective, and compassionate counterparts. Our findings indicate that the average empathy rating of responses generated by ChatGPT exceeds those crafted by humans by approximately 10%. Additionally, instructing ChatGPT to incorporate a clear understanding of empathy in its responses makes the responses align â‰ˆ5 times more closely with the expectations of individuals possessing a high degree of empathy, compared to human responses. The proposed evaluation framework serves as a scalable and adaptable framework to assess the empathetic capabilities of newer and updated versions of large language models, eliminating the need to replicate the current study's results in future research",
    "checked": true,
    "id": "eb7a04c2f4ca8236cd67c26c8181c65c1f65e48c",
    "semantic_title": "is chatgpt more empathetic than humans?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wGqUfpbANfZ": {
    "title": "Speech-to-Speech Translation with Discrete-Unit-Based Style Transfer",
    "volume": "review",
    "abstract": "Direct speech-to-speech translation (S2ST) with discrete self-supervised representations has achieved remarkable accuracy, but is unable to preserve the speaker timbre of the source speech. Meanwhile, the scarcity of high-quality speaker-parallel data poses a challenge for learning style transfer during translation. We propose an S2ST framework with style-transfer capability on the basis of discrete self-supervised speech representations and codec units. The acoustic language model we introduce for style transfer leverages self-supervised in-context learning, acquiring style transfer ability without relying on any speaker-parallel data, thereby overcoming the issue of data scarcity. By using extensive training data, our model achieves zero-shot cross-lingual style transfer on previously unseen source languages. Experiments show that our model generates translated speeches with high fidelity and style similarity. Audio samples are available at http://stylelm.github.io/",
    "checked": true,
    "id": "191b685607b1801a0bc536ae4bc56def49e210a0",
    "semantic_title": "speech-to-speech translation with discrete-unit-based style transfer",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=JdrduGd80O4": {
    "title": "DictLLM: Harnessing Key-Value Data Structures with Large Language Models for Enhanced Medical Diagnostics",
    "volume": "review",
    "abstract": "Structured data offers an efficient means of organizing information. Exsisting text-serialization based methods for processing structured data using large language models (LLMs) are not designed to explicitly capture the heterogeneity of structured data. Such methods are suboptimal for LLMs to process structured data, and may lead to large input token size and poor robustness to input perturbation. In this paper, we propose a novel framework called DictLLM, which is an efficient and effective framework for the modeling of medical lab report to deal with the report-assisted diagnosis generation task. DictLLM introduce 1) group positional encoding to maintain the permutation invariance, 2) hierarchical attention bias to capture the inductive bias of structured data, and 3) a optimal transport alignment layer to align the embeddings generated by the dict encoder with the LLM, producing a list of fixed-length virtual tokens. We conduct experiments with multiple LLM models on a large-scale real-world medical lab report dataset for automatic diagnosis generation. The results show that our proposed framework outperforms the baseline methods and few-shot GPT-4 in terms of both Rouge-L and Knowledge F1 score. We also conduct multiple experiments and analyze the scalability and robustness of our proposed framework, demonstrating the superiority of our method in modeling the heterogeneous structure of medical dictionaries data",
    "checked": true,
    "id": "a9a0d63182bea28924156295a3e91174358d5026",
    "semantic_title": "dictllm: harnessing key-value data structures with large language models for enhanced medical diagnostics",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EBFRB_LbK_O": {
    "title": "Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication",
    "volume": "review",
    "abstract": "Natural language (NL) has long been the predominant format for human cognition and communication, and by extension, has been similarly pivotal in the development and application of Large Language Models (LLMs). Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression. NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined. In this work, we challenge the default use of NL by exploring the utility of non-NL formats in these contexts. We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7% improvement in reasoning efficiency for different LLMs, and up to a 72.7% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness. Our comprehensive analysis further reveals that LLMs can devise a format from limited task instructions and that the devised format is effectively transferable across different LLMs. Intriguingly, the structured communication format decided by LLMs exhibits notable parallels with established agent communication languages, suggesting a natural evolution towards efficient, structured communication in agent communication",
    "checked": true,
    "id": "bef263083bf5ea965c37b152bc5f0b43aaf74824",
    "semantic_title": "beyond natural language: llms leveraging alternative formats for enhanced reasoning and communication",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=E5ZHBvWrXIU": {
    "title": "AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight Relabeling on Multi-source Data",
    "volume": "review",
    "abstract": "Open-source Large Language Models (LLMs) and their specialized variants, particularly Code LLMs, have recently delivered impressive performance. However, previous Code LLMs are typically fine-tuned on a single dataset, which may insufficiently elicit the potential of pre-trained Code LLMs. This paper presents AlchemistCoder, a series of Code LLMs with better code generation and generalization abilities fine-tuned on multi-source data. To harmonize the inherent conflicts among the various styles and qualities in multi-source data, we introduce data-specific prompts, termed AlchemistPrompts, inspired by hindsight relabeling, to improve the consistency between instructions and responses. We further propose to incorporate the data evolution process itself into the fine-tuning data to enhance the code comprehension capabilities of LLMs, including instruction evolution, data filtering, and code review. Extensive experiments demonstrate that AlchemistCoder holds a clear lead among all models of the same size (6.7B/7B) and rivals or even surpasses larger models (15B/33B/70B), showcasing the efficacy of our method in refining instruction-following capabilities and advancing the boundaries of code intelligence",
    "checked": false,
    "id": "1371a9170f802d56c60940693ddeab60b71d08bb",
    "semantic_title": "alchemistcoder: harmonizing and eliciting code capability by hindsight tuning on multi-source data",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pQgYBC7sg9O": {
    "title": "Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models",
    "volume": "review",
    "abstract": "A pivotal advancement in the evolution of large language models (LLMs) is the recent emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks. Extensive experiments show that our proposed methods can simultaneously reduce model sizes and increase the inference speed, while maintaining satisfactory performance. Code will be made available",
    "checked": true,
    "id": "b42e5a92890053ef48f794311c28c45e9fe55ddd",
    "semantic_title": "not all experts are equal: efficient expert pruning and skipping for mixture-of-experts large language models",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=OUgRDiRaRRN": {
    "title": "Enhancing Multi-Domain Fake News Detection: Causal Intervention & Cross-Attention Mechanism",
    "volume": "review",
    "abstract": "With the accelerating growth of the amountand variety of information on social platforms.there is also an increasing threat of fake newsto the real world. Most traditional fake newsdetection methods rely on single-domain training, and their effectiveness is often subopti-mal when applied to multi-domain Fake NewsDetection (MFND). Therefore, the necessityarises to design a specialized multi-domaindetection model. However, based on our research, we identify two primary challenges inthe current MEND problem:(1)Current research predominantly focuses on the extrac.tion of information from a single event itself.lacking a method to extract potentially useful information from other events in the feldfor detection.(2)We have discovered noncausal confounding effects that may exist inthe process of domain fusion feature extrac-tion. To address the former challenge, we in-troduce a multi-head cross-attention modulewith multi-domain storage. To address the lat-ter challenge, we propose a causal interventionmodule for deconfounding. Finally, our Causaland Cross-attention Multi-domain Fake NewsDetection model (C2MFND) is formulated. Ex-periments demonstrate that our method can significantly improve the performance of MFND. Our code is available at https://anonymous4open.science/r/C2MFND-151D",
    "checked": false,
    "id": "a0fc7788c631a9b076790fdb651eda1b7f9aaec3",
    "semantic_title": "multi-modal chinese fake news detection",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9v52WCBNeMd": {
    "title": "Balancing Speciality and Versatility: a Coarse to Fine Framework for Supervised Fine-tuning Large Language Model",
    "volume": "review",
    "abstract": "However, fine-tuning with extra data, a common practice to gain speciality, often leads to catastrophic forgetting (CF) of previously acquired versatility, hindering the model's performance across diverse tasks. In response to this challenge, we propose CoFiTune, a coarse to fine framework in an attempt to strike the balance between speciality and versatility. At the coarse-grained level, an empirical tree-search algorithm is utilized to pinpoint and update specific modules that are crucial for speciality, while keeping other parameters frozen; at the fine-grained level, a soft-masking mechanism regulates the update to the LLMs, mitigating the CF issue without harming speciality. In an overall evaluation of both speciality and versatility, CoFiTune consistently outperforms baseline methods across diverse tasks and model scales. Compared to the full-parameter SFT, CoFiTune leads to about 14% versatility improvement and marginal speciality loss on a 13B model. Lastly, based on further analysis, we provide a speculative insight into the information forwarding process in LLMs, which helps explain the effectiveness of the proposed method. The code is available at https://anonymous.4open.science/r/CoFiTune-542C",
    "checked": true,
    "id": "72662bc43367548fc6094594dfed21fecf6361ba",
    "semantic_title": "balancing speciality and versatility: a coarse to fine framework for supervised fine-tuning large language model",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QatG27TWcax": {
    "title": "Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals",
    "volume": "review",
    "abstract": "Interpretability research aims to bridge the gap between the empirical success and our scientific understanding of the inner workings of large language models (LLMs). However, most existing research in this area focused on analyzing a single mechanism, such as how models copy or recall factual knowledge. In this work, we propose the formulation of competition of mechanisms, which instead of individual mechanisms focuses on the interplay of multiple mechanisms, and traces how one of them becomes dominant in the final prediction. We uncover how and where the competition of mechanisms happens within LLMs using two interpretability methods, logit inspection and attention modification. Our findings show traces of the mechanisms and their competition across various model components, and reveal attention positions that effectively control the strength of certain mechanisms",
    "checked": true,
    "id": "585ab11510d0e8504289991919976afba64d4c96",
    "semantic_title": "competition of mechanisms: tracing how language models handle facts and counterfactuals",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=W60YCfI6EyM": {
    "title": "MS2SL: Multimodal Spoken Data-Driven Continuous Sign Language Production",
    "volume": "review",
    "abstract": "Sign language translation has made significant strides; however, there is still no viable solution for directly generating sign sequences from spoken content, e.g., text or speech. This paper proposes a unified framework for continuous sign language production, easing communication between sign and non-sign language users. In particular, a sequence diffusion model, utilizing embeddings extracted from text or speech, is crafted to generate sign predictions step by step. Moreover, by formulating a joint embedding space for text, audio, and sign, we bind data from the three modalities and leverage the semantic consistency across modalities to provide informative feedback signals for the model training. This embedding-consistency learning strategy minimizes the reliance on triplet sign language data and ensures continuous model refinement, even with a missing audio modality. Experiments on How2Sign and PHOENIX14T datasets demonstrate that our model achieves competitive performance in producing signs from both speech and text. We will release our implementation code and demos",
    "checked": true,
    "id": "1950ef81f3f01f1bc043615232e4688b6cc08cf8",
    "semantic_title": "ms2sl: multimodal spoken data-driven continuous sign language production",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=A02_JmAcdBD": {
    "title": "ChatCRS: Incorporating External Knowledge and Goal Guidance for LLM-based Conversational Recommender Systems",
    "volume": "review",
    "abstract": "This paper aims to efficiently enable Large Language Models (LLMs) to use external knowledge and goal guidance in conversational recommendation system (CRS) tasks. Advanced LLMs (e.g., ChatGPT) are limited in CRS tasks for 1) generating grounded responses with recommendation-oriented knowledge, or 2) proactively guiding users through different dialogue goals. In this work, we first analyze those limitations through a comprehensive evaluation to assess LLMs' intrinsic capabilities, showing the necessity of incorporating external knowledge and goal guidance which contribute significantly to the recommendation accuracy and language quality. In light of this finding, we propose a novel ChatCRS framework to decompose the complex CRS task into several sub-tasksthrough the implementation of 1) a knowledge retrieval agent using a tool-augmented approach to reason over external Knowledge Bases and 2) a goal-planning agent for dialogue goal prediction. Experimental results on two CRS datasets reveal that ChatCRS sets new state-of-the-art benchmarks, improving language quality of informativeness by 17% and proactivity by 27%, and achieving a tenfold enhancement in recommendation accuracy over LLM-based CRS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AsMIQdpMM5Y": {
    "title": "AutoAct: Automatic Agent Learning from Scratch via Self-Planning",
    "volume": "review",
    "abstract": "Language agents have achieved considerable performance on various complex question-answering tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to various strong baselines. Further analysis demonstrates the effectiveness of the division-of-labor strategy, with the trajectory quality generated by AutoAct significantly outperforming that of others",
    "checked": false,
    "id": "f5b077e01f6e3d91f58cb4ed7158fa61eec5a1f8",
    "semantic_title": "autoact: automatic agent learning from scratch for qa via self-planning",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=8vhLrFQRbUX": {
    "title": "Never Lost in the Middle: Mastering Long-Context Question Answering with Position-Agnostic Decompositional Training",
    "volume": "review",
    "abstract": "While large language models (LLMs) are equipped with longer text input capabilities than before, they are struggling to seek correct information in long contexts. The \"lost in the middle\" problem challenges most LLMs, referring to the dramatic decline in accuracy when correct information is located in the middle. To overcome this crucial issue, this paper proposes to enhance the information searching and reflection ability of LLMs in long contexts via specially designed tasks called Position-Agnostic Multi-step QA (PAM QA). Trained in this task, our model excels in focusing more precisely on the desired information. Experimental results show substantial improvement in Multi-doc QA and other benchmarks, superior to state-of-the-art models by 13.7\\% absolute gain in shuffled settings, by 21.5\\% in passage retrieval task. We release our model and code to promote related research in the community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vhxPFnmeQAz": {
    "title": "Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts for Open-Domain QA?",
    "volume": "review",
    "abstract": "While auxiliary information has become a key to enhancing Large Language Models (LLMs), relatively little is known about how LLMs merge these contexts, specifically contexts generated by LLMs and those retrieved from external sources. To investigate this, we formulate a systematic framework to identify whether LLMs' responses, derived from the integration of generated and retrieved contexts, are attributed to either generated or retrieved contexts. To easily trace the origin of the response, we construct datasets with conflicting contexts, i.e., each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer. Our experiments reveal a significant bias in several LLMs (GPT-4/3.5 and Llama2) to favor generated contexts, even when they provide incorrect information. We further identify two key factors contributing to this bias: i) contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of being selected; ii) the segmentation process used in retrieved contexts disrupts their completeness, thereby hindering their full utilization in LLMs. Our analysis enhances the understanding of how LLMs merge diverse contexts, offering valuable insights for advancing current augmentation methods for LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YlVsU8jdqPg": {
    "title": "ðŸ¦‹ðŸŒªï¸ The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse",
    "volume": "review",
    "abstract": "Although model editing has shown promise in revising knowledge in Large Language Models (LLMs), its impact on the inherent capabilities of LLMs is often overlooked. In this work, we reveal a critical phenomenon: even a single edit can trigger model collapse, manifesting as significant performance degradation in various benchmark tasks. However, benchmarking LLMs after each edit, while necessary to prevent such collapses, is impractically time-consuming and resource-intensive. To mitigate this, we propose using perplexity as a surrogate metric, validated by extensive experiments demonstrating its strong correlation with downstream tasks performance. We further conduct an in-depth study on sequential editing, a practical setting for real-world scenarios, across various editing methods and LLMs, focusing on hard cases from our previous single edit studies. The results indicate that nearly all examined editing methods result in model collapse after only few edits. To facilitate further research, we have utilized ChatGPT to develop a new dataset, HardEdit, based on those hard cases. This dataset aims to establish the foundation for pioneering research in reliable model editing and the mechanisms underlying editing-induced model collapse. We hope this work can draw the community's attention to the potential risks inherent in model editing practices",
    "checked": false,
    "id": "b8ffaec3e8e64a246ac4b9e4016e4efea0e156ac",
    "semantic_title": "the butterfly effect of model editing: few edits can trigger large language models collapse",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=Pw_8QERDw6l": {
    "title": "DeltaDQ: Distribution-Driven Delta Compression for Fine-tuned LLMs",
    "volume": "review",
    "abstract": "Large language models have demonstrated remarkable success across a wide range of domains, with supervised fine-tuning being widely adapted to make them more suitable for real-world scenarios. Given the diversity of downstream tasks and varying demands, efficiently deploying multiple full-parameter fine-tuned models presents a significant challenge. To address this, we analyze $\\textit{Balanced Intermediate Dropout}$, a distribution-related phenomenon, whereby the matrix-computed intermediate results for the delta weight of each fine-tuned model have extremely small variance and min-max range. Leveraging this phenomenon, we propose a novel distribution-driven delta compression framework DeltaDQ, which employs $\\textit{Group-wise Balanced Dropout}$ and $\\textit{Delta Quantization}$ to efficiently compress the delta weight. $\\textit{Group-wise Balanced Dropout}$ achieves a favorable trade-off with accuracy and performance, ensuring an N:M sparsity pattern. $\\textit{Delta Quantization}$ further compresses the delta weight based on distribution characteristics. Experimental results show that the accuracy of our framework on WizardMath-7B,13B at 96.875% compress rate is improved by 4.47 and 4.70 compared with baseline, and we even improve the accuracy by 1.83 and 0.61 compared with the original model on WizardCoder-13B,34B",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2MHUPGRLap7": {
    "title": "StreamVoice: Streamable Context-Aware Language Modeling for Real-time Zero-Shot Voice Conversion",
    "volume": "review",
    "abstract": "Recent language model (LM) advancements have showcased impressive zero-shot voice conversion (VC) performance. However, existing LM-based VC models usually apply offline conversion from source semantics to acoustic features, demanding the complete source speech, and limiting their deployment to real-time applications. In this paper, we introduce StreamVoice, a novel streaming LM-based model for zero-shot VC, facilitating real-time conversion given arbitrary speaker prompts and source speech. Specifically, to enable streaming capability, StreamVoice employs a fully causal context-aware LM with a temporal-independent acoustic predictor, while alternately processing semantic and acoustic features at each time step of autoregression which eliminates the dependence on complete source speech. To address the potential performance degradation from the incomplete context in streaming processing, we enhance the context-awareness of the LM through two strategies: 1) teacher-guided context foresight, using a teacher model to summarize the present and future semantic context during training to guide the model's forecasting for missing context; 2) semantic masking strategy, promoting acoustic prediction from preceding corrupted semantic and acoustic input, enhancing context-learning ability. Notably, StreamVoice is the first LM-based streaming zero-shot VC model without any future look-ahead. Experiments demonstrate StreamVoice's streaming conversion capability while achieving zero-shot performance comparable to non-streaming VC systems",
    "checked": true,
    "id": "f7fcb8dd8ccfd3d4e718aee5d9aa938f2edc9005",
    "semantic_title": "streamvoice: streamable context-aware language modeling for real-time zero-shot voice conversion",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Mff9Xkz6Fqw": {
    "title": "Personalization Enhancement for Large Language Models through Guided Profile Generation",
    "volume": "review",
    "abstract": "In modern commercial systems, including Recommendation, Ranking, and E-Commerce platforms, there is a trend towards improving customer experiences by incorporating Personalization context as input into Large Language Models (LLM). Retrieval Augmented Generation (RAG) has emerged as an adopted approach to assist in furnishing LLM with supplementary personal context derived from a personal database. However, LLMs often struggle to effectively parse and utilize sparse and complex personal context without additional processing or contextual enrichment, underscoring the need for more sophisticated context understanding mechanisms. In this work, we propose Guided Profile Generation (GPG), a general method designed to generate personal profiles in natural language. As is observed, intermediate guided profile generation enables LLMs to summarize, and extract the important, distinctive features from the personal context into concise, descriptive sentences, precisely tailoring their generation more closely to an individual's unique habits and preferences. Our experimental results show that GPG improves LLM's personalization ability across different tasks, for example, it increases 37% accuracy in predicting personal preference of shopping compared to directly feeding the LLMs with raw personal context",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8nN0FNSDhl7": {
    "title": "Generative Pre-Trained Speech Language Model with Efficient Hierarchical Transformer",
    "volume": "review",
    "abstract": "While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequence of neural audio codecs. In this paper, we introduce \\textbf{G}enerative \\textbf{P}re-Trained \\textbf{S}peech \\textbf{T}ransformer (GPST)\\footnote{We provide an anonymous demo page \\url{https://GPSTACL.github.io/GPST/demo}.}, a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. When provided a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VonVCtRipL8": {
    "title": "Exploring Chain-of-Thought for Multi-modal Metaphor Identification",
    "volume": "review",
    "abstract": "Metaphors are commonly found in advertising and internet memes. However, the free form of internet memes often leads to a lack of high-quality textual data. Metaphor identification demands a deep interpretation of both textual and visual elements, requiring extensive common-sense knowledge, which poses a challenge to language models. To address these challenges, we propose a compact framework that enhances the small model by distilling knowledge from Multi-modal Large Language Models(MLLMS). Specifically, our approach designs a three-step process inspired by Chain-of-Thought (CoT) that extracts and integrates knowledge from larger models into smaller ones. We also developed a modality fusion architecture to transform knowledge from large models into metaphor features, supplemented by auxiliary tasks to improve model performance. Experimental results on the MET-MEME dataset demonstrate that our method not only effectively enhances the metaphor identification capabilities of small models but also outperforms existing models. To our knowledge, this is the first systematic study leveraging MLLMs in metaphor identification tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hjQraXaZX0E": {
    "title": "WRP: Weight Recover Prune for Structured Sparsity",
    "volume": "review",
    "abstract": "As the scale of Large Language Models (LLMs) increases, it is necessary to compress the models to reduce the substantial demand on computational resources. Network pruning significantly reduces the model size by converting the weight matrix from dense to sparse data format. Current methodologies advocate for one-shot pruning to avoid the expense of retraining, ensuring the maintenance of model performance under conditions of 50%-60% unstructured pruning. Nevertheless, matrices characterized by this level of sparsity could not be treated as sparse matrices, because the indices would incur significant costs. To mitigate this problem, NVIDIA introduced the 2:4 structured sparsity. However, we observe a notable decline in model performance when adopting 2:4 structured sparsity due to group constraints. In this paper, we introduce the Weight Recover Prune (WRP) approach. By recovering a minimal set of critical weights, WRP aims to enhance model performance while maintaining the efficiency of the compression. Our evaluation of the WRP method on the LLAMA2 and OPT models shows that it outperforms other 2:4 pattern one-shot pruning methods. Meanwhile, WRP can guarantee a compression rate of about 60\\% compared to the dense model. Our code is available at: https://anonymous.4open.science/r/WRP-0A5F",
    "checked": false,
    "id": "1900de2b966ca55ee5ca24ec94d5debe66e80c5b",
    "semantic_title": "dynamic n:m fine-grained structured sparse attention mechanism",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=wY8FZuWYMyf": {
    "title": "LLaMA Pro: Progressive LLaMA with Block Expansion",
    "volume": "review",
    "abstract": "Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with an expansion of Transformer blocks. We tune the expanded blocks using only new corpus, efficiently and effectively improving the model's knowledge while mitigating forgetting. In this paper, we experiment on the corpus of code and math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro and its instruction-following counterpart (LLaMA Pro - Instruct) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent. Our findings provide valuable insights into integrating natural and programming languages, laying a solid foundation for developing advanced language agents that operate effectively in various environments",
    "checked": true,
    "id": "c3d1832ed0444f75d44116fabbdda891aebc4b01",
    "semantic_title": "llama pro: progressive llama with block expansion",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=_l6GYAi8fwl": {
    "title": "EditBias: Debiasing Stereotyped Language Models via Model Editing",
    "volume": "review",
    "abstract": "Previous studies have established that pre-trained language models inherently manifest various biases. Although several debiasing strategies, such as fine-tuning a model with counterfactual data, prompt tuning, and representation projection, have been introduced, they often fall short of efficiently unlearning bias or directly altering the models' biased essence. To address these issues, we propose EditBias, an efficient model editing method to remove stereotyped bias from language models with small editor networks. We design a debiasing loss to guide editor networks to conduct local edits on partial parameters for debiasing, and a remaining loss to preserve the original language modeling abilities of models during editing. Experiments demonstrate the high effectiveness and robustness of EditBias on eliminating bias compared to classical debiasing baselines. Additionally, we explore the effects of bias and debiasing on language models, finding that it is challenging to debias larger and causal language models, and necessary to balance the trade-off between debiasing efforts and language modeling abilities when designing debiasing strategies",
    "checked": false,
    "id": "60999ddb99aea5a93bd2ba16fb7671dc76bf3ba5",
    "semantic_title": "causal-debias: unifying debiasing in pretrained language models and fine-tuning via causal invariant learning",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=vVi0Vz0V-sV": {
    "title": "AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback",
    "volume": "review",
    "abstract": "The notable success of large language models (LLMs) has sparked an upsurge in building language agents to complete various complex tasks. We present A MOR, an agent framework based on open-source LLMs, which reasons with external knowledge bases and adapts to specific domains through human supervision to the reasoning process. A MOR builds reasoning logic over a finite state machine (FSM) that solves problems through autonomous executions and transitions over disentangled modules. This allows humans to provide direct feedback to the individual modules, and thus naturally forms process supervision. Based on this reasoning and feedback framework, we develop A MOR through two-stage fine-tuning: warm-up and adaptation. The former fine-tunes the LLM with examples automatically constructed from various public datasets and enables A MOR to generalize across different knowledge environments, while the latter tailors A MOR to specific domains using process feedback. Extensive experiments across multiple domains demonstrate the advantage of A MOR to strong baselines, thanks to its FSM-based reasoning and process feedback mechanism",
    "checked": true,
    "id": "61b3260f774bdaf2f7c1894286a49f2eb4dfbb10",
    "semantic_title": "amor: a recipe for building adaptable modular knowledge agents through process feedback",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=Y2KufczxQ-8": {
    "title": "A Simple Reward Composition Method for Effectively Finetuning the Large Language Model with Diverse Feedbacks",
    "volume": "review",
    "abstract": "Reinforcement learning from human feedback has emerged as a promising paradigm that significantly enhances the performance of large language models. Typically, reward models are trained to align with human preferences and are then utilized to optimize the pretrained language models. However, given the multifaceted nature of human preferences, it is challenging to appropriately combine rewards from different aspects. Recent studies have developed algorithms to address this issue by employing techniques such as weighting, and ranking. Nonetheless, these methods can perform poorly in certain scenarios despite their elegant design. In this paper, we explore the reward composition problem from a novel perspective. We posit that different reward models focus on distinct optimization directions, which the language model cannot discern, perceiving only the reward value. To formulate an appropriate reward signal, we introduce a global reward model that composes rewards from various aspects in a self-supervised manner, a simple yet effective approach. This global reward model can be trained without the need for additional supervised data and is compatible with any type of reward model. Experimental results demonstrate the superiority of our method across a range of scenarios with different types of rewards",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Km1xK9MQGzQ": {
    "title": "Zero-Shot Position Debiasing for Large Language Models",
    "volume": "review",
    "abstract": "Fine-tuning has been demonstrated to be an effective method to improve the domain performance of large language models (LLMs). However, LLMs might fit the dataset bias and shortcuts for prediction, leading to poor generation performance. Previous works have proven that LLMs are prone to exhibit position bias, i.e., leveraging information positioned at the beginning or end, or specific positional cues within the input. Existing debiasing methods for LLMs require external bias knowledge or annotated non-biased samples, which is lacking for position debiasing and impractical in reality. In this work, we propose a zero-shot position debiasing (ZOE) framework to mitigate position bias for LLMs. ZOE leverages unsupervised responses from pre-trained LLMs for debiasing without relying on any external knowledge. To improve the quality of unsupervised responses, we propose a MSA module to prune these responses. Experiments on eight datasets and five tasks show that ZOE consistently outperforms existing methods in mitigating three types of position biases. Besides, ZOE achieves this by sacrificing only a small performance on biased samples, which is general and effective",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7ybtxRdIePz": {
    "title": "PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition",
    "volume": "review",
    "abstract": "In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs). The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length. To this end, we introduce Parallel Decoding in LLM for NER (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLM-NER accelerates decoding by simultaneous generating all mentions at once, i.e., a label-mention pair per sequence. This results in shorter sequences and faster inference. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Concurrently, it maintains the prediction quality as evidenced by the micro F-score that is on par with the state-of-the-art across various datasets",
    "checked": true,
    "id": "a3a273bbd169ed6e67c2735449bf609662a42cb0",
    "semantic_title": "padellm-ner: parallel decoding in large language models for named entity recognition",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=oUb6VAfNBjO": {
    "title": "LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT",
    "volume": "review",
    "abstract": "Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks, and have shown great potential as backbones for audio-and-text large language models (LLMs). Previous mainstream audio-and-text LLMs use discrete audio tokens to represent both input and output audio; however, they suffer from performance degradation on tasks such as automatic speech recognition, speech-to-text translation, and speech enhancement over models using continuous speech features. In this paper, we propose LauraGPT, a novel unified audio-and-text GPT-based LLM for audio recognition, understanding, and generation. LauraGPT is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities. We propose a novel data representation that combines continuous and discrete features for audio: LauraGPT encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes. We propose a one-step codec vocoder to overcome the prediction challenge caused by the multimodal distribution of codec tokens. We fine-tune LauraGPT using supervised multi-task learning. Extensive experiments show that LauraGPT consistently achieves comparable to superior performance compared to strong baselines on a wide range of audio tasks related to content, semantics, paralinguistics, and audio-signal analysis, such as automatic speech recognition, speech-to-text translation, text-to-speech synthesis, speech enhancement, automated audio captioning, speech emotion recognition, and spoken language understanding",
    "checked": true,
    "id": "ffa05cb5504ba08254f498223f613b3ebcf87692",
    "semantic_title": "lauragpt: listen, attend, understand, and regenerate audio with gpt",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=_stkNZ5od2Q": {
    "title": "Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models?",
    "volume": "review",
    "abstract": "Building machines with commonsense has been a longstanding challenge in NLP due to the reporting bias of commonsense rules and the exposure bias of rule-based commonsense reasoning. In contrast, humans convey and pass down commonsense implicitly through stories. This paper investigates the inherent commonsense ability of large language models (LLMs) expressed through storytelling. We systematically investigate and compare stories and rules for retrieving and leveraging commonsense in LLMs. Experimental results on 28 commonsense QA datasets show that stories outperform rules as the expression for retrieving commonsense from LLMs, exhibiting higher generation confidence and commonsense accuracy. Moreover, stories are the more effective commonsense expression for answering questions regarding daily events, while rules are more effective for scientific questions. This aligns with the reporting bias of commonsense in text corpora. We further show that the correctness and relevance of commonsense stories can be further improved via iterative self-supervised fine-tuning. These findings emphasize the importance of using appropriate language to express, retrieve, and leverage commonsense for LLMs, highlighting a promising direction for better exploiting their commonsense abilities",
    "checked": true,
    "id": "de6f93849a84c128d72d14649ec4a6115be3c68d",
    "semantic_title": "rule or story, which is a better commonsense expression for talking with large language models?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0v1Q8UkqqXC": {
    "title": "What Evidence Do Language Models Find Convincing?",
    "volume": "review",
    "abstract": "Large language models (LLMs) are being tasked with increasingly open-ended, delicate, and subjective tasks. In particular, retrieval-augmented models can now answer contentious or subjective questions (e.g., \"is aspartame linked to cancer\") and in doing so, conditioning on arbitrary websites that vary wildly in style, format, and veracity. Importantly, information from these websites will often conflict with one another. Humans are faced with similar conflicts, and in order to come to an answer they critically evaluate the arguments, trustworthiness, and credibility of a source. In this work, we study what types of evidence current LLMs find convincing, and if they make judgements that align with human preferences. Specifically, we construct ConflictingQA, a benchmark that pairs controversial questions with a series of evidence documents that contain different facts (e.g., quantitative results), argument styles (e.g., appeals to authority), and answers (Yes or No). Using this benchmark, we perform sensitivity analyses and counterfactual experiments to explore how in-the-wild differences in text affect model judgements. We find that models overkey off the relevance of a website to the user's search query. On the other hand, the stylistic features tested tended to have little influence on model predictions",
    "checked": true,
    "id": "9988a412f879731cb67e536663b8ceeb273f5c34",
    "semantic_title": "what evidence do language models find convincing?",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=vfzRYRLG3bn": {
    "title": "UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language Models",
    "volume": "review",
    "abstract": "Large language models (LLMs) may generate text that lacks consistency with human knowledge, leading to factual inaccuracies or hallucination. Existing research for evaluating the factuality of LLMs involves extracting fact claims using an LLM and verifying them against a predefined fact source. However, these evaluation metrics are task-specific, and not scalable, and the substitutability of fact sources in different tasks is under-explored. To address these challenges, we categorize four available fact sources: human-written evidence, reference documents, search engine results, and LLM knowledge, along with five text generation tasks containing six representative datasets. Then, we propose UFO, an LLM-based unified and flexible evaluation framework to verify facts against plug-and-play fact sources. We implement five evaluation scenarios based on this framework. Experimental results show that for most QA tasks, human-written evidence and reference documents are crucial, and they can substitute for each other in retrieval-augmented QA tasks. In news fact generation tasks, search engine results and LLM knowledge are essential. Our dataset and code are available at https://anonymous.4open.science/r/UFO-813F",
    "checked": true,
    "id": "d38af39275524068e7aab12fa8d54d342eff7dfe",
    "semantic_title": "ufo: a unified and flexible framework for evaluating factuality of large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=G1hjFDre0NF": {
    "title": "RelayAttention for Efficient Large Language Model Serving with Long System Prompts",
    "volume": "review",
    "abstract": "Practical large language model (LLM) services may involve a long system prompt, which specifies the instructions, examples, and knowledge documents of the task and is reused across numerous requests. However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows w.r.t. the sequence length. This paper aims to improve the efficiency of LLM services that involve long system prompts. Our key observation is that handling these system prompts requires heavily redundant memory accesses in existing causal attention computation algorithms. Specifically, for batched requests, the cached hidden states (\\ie, key-value pairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM multiple times, each corresponding to an individual request. To eliminate such a redundancy, we propose RelayAttention, an attention algorithm that allows reading these hidden states from DRAM exactly once for a batch of input tokens. RelayAttention is a free lunch: it maintains the generation quality while requiring no model retraining, as it is based on a mathematical reformulation of causal attention",
    "checked": true,
    "id": "de9b37396a6fafc7b11223ef6682e9f13b2edaf1",
    "semantic_title": "relayattention for efficient large language model serving with long system prompts",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=SogS73z84Ne": {
    "title": "Systematic Biases in LLM Simulations of Debates",
    "volume": "review",
    "abstract": "The emergence of Large Language Models (LLMs), has opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately. However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates. Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the altered biases. These results underscore the need for further research to develop methods that help agents overcome these biases, a critical step toward creating more realistic simulations",
    "checked": true,
    "id": "f503b95c0a64f6a84eb1d90e5ea1e094b1e1892b",
    "semantic_title": "systematic biases in llm simulations of debates",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=b5VEUDu4E_D": {
    "title": "CycleOIE: An Unsupervised Low-Resource Training Framework For Open Information Extraction",
    "volume": "review",
    "abstract": "In Open Information Extraction (OpenIE), the acquisition of manually annotated sentence-extraction pairs is expensive, while automatically labeled datasets may struggle to accurately reflect real-world requirements for OpenIE systems. Existing neural models often demonstrate impressive performance on large-scale training sets but falter when tested on smaller-scale datasets due to the discrepancy in attributes between the training and test sets. In real-world scenarios, it is crucial for OpenIE systems to align closely with test sets, even when faced with limited annotated data for training.This paper introduces CycleOIE, a novel training framework applied to a pair of inverse text-to-text models. Through CycleOIE, we train a pair of T5 models on our curated dataset, LSOIE-g, achieving performance levels that surpass baselines trained on significantly larger fully supervised training sets. Ablation studies offer a detailed comparison between fully supervised training and CycleOIE, highlighting the effectiveness of CycleOIE on LSOIE-g as the primary factor in enhancing T5's OpenIE performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h4wTxMqYEyb": {
    "title": "Continual Contrastive Spoken Language Understanding",
    "volume": "review",
    "abstract": "Recently, neural networks have shown impressive progress across diverse fields, with speech processing being no exception. However, recent breakthroughs in this area require extensive offline training using large datasets and tremendous computing resources. Unfortunately, these models struggle to retain their previously acquired knowledge when learning new tasks continually. In this paper, we investigate the problem of learning sequence-to-sequence models for spoken language understanding in a class-incremental learning (CIL) setting and we propose COCONUT \\emojismiley, a CIL method that relies on the combination of experience replay and contrastive learning. Through a modified version of the standard supervised contrastive loss, COCONUT preserves the learned representations by pulling closer samples from the same class and pushing away the others. Moreover, we leverage a multimodal contrastive loss that helps the model learn more discriminative representations of the new data by aligning audio and text features. We also investigate different contrastive designs to combine the strengths of the contrastive loss with teacher-student architectures used for distillation. Experiments on two established SLU datasets reveal the effectiveness of our proposed approach and significant improvements over the baselines. We also show that COCONUT can be combined with methods that operate on the decoder side of the model, resulting in further metrics improvements",
    "checked": true,
    "id": "f5a7a4fda49c657742072a2758f43b1cbcde3886",
    "semantic_title": "continual contrastive spoken language understanding",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=-b4vXOI-abr": {
    "title": "ChatGPT as a Solver and Grader of Programming Exams written in Spanish",
    "volume": "review",
    "abstract": "Evaluating the capabilities of Large Language Models (LLMs) to assist teachers and students in educational tasks is receiving increasing attention. In this paper, we assess ChatGPT's capacities to solve and grade real programming exams, from an accredited BSc degree in Computer Science, written in Spanish. Our findings suggest that this AI model is only effective for solving simple coding tasks. Its proficiency in tackling complex problems or evaluating solutions authored by others are far from effective. As part of this research, we also release a new corpus of programming tasks and the corresponding prompts for solving the problems or grading the solutions. This resource can be further exploited by other research teams",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kDVw5B_em3O": {
    "title": "Samoyed: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories",
    "volume": "review",
    "abstract": "Fine-tuning on agent-environment interaction trajectory data is of high potential for surfacing generalized agent capabilities in open-source large language models (LLMs). In this work, we curate SuperAgent, by far the largest trajectory tuning data collection featuring more than 50k diverse high-quality interaction trajectories with GPT-annotated rationale. SuperAgent comprises 16 tasks covering five distinct agent skill dimensions. Furthermore, we present Samoyed, a series of open-source LLMs fine-tuned on SuperAgent. Our comparative experiments show that Samoyed outperforms strong baseline LLMs on both held-in and held-out tasks, demonstrating the effectiveness of scaling the interaction trajectory data to acquire generalized agent capabilities. Additional studies also reveal some key observations regarding trajectory tuning and agent skill generalization. To facilitate future research on developing open-source LLM agents, we will release SuperAgent dataset, checkpoints of Samoyed, and the unified evaluation framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0VEKzyzElep": {
    "title": "Humanity in AI: Detecting the Personality of Large Language Models",
    "volume": "review",
    "abstract": "Exploring the personality of large language models (LLMs) is an important way to gain an in-depth understanding of LLMs. It is well known that ChatGPT has reached a level of linguistic proficiency comparable to that of a 9-year-old child, prompting a closer examination of its personality. In this paper, we propose to detect the personality of LLMs by questionnaires and text mining methods, with the guide of BigFive psychological model. To explore the origins of the LLMs personality, we conduct experiments on pre-trained language models (PLMs, such as BERT and GPT) and Chat models (ChatLLMs, such as ChatGPT). The results show that LLMs do contain certain personalities, for example, we think ChatGPT tends to exhibit openness, conscientiousness and neuroticism, while ChatGLM only exhibited conscientiousness and neuroticism. More importantly, we find that the personality of LLMs comes from their pre-training data, and the instruction data can facilitate the generation of data containing personality. We also compare the results of LLMs with the human average personality score, and find that the humanity of FLAN-T5 in PLMs and ChatGPT in ChatLLMs is more similar to that of a human, with score differences of 0.34 and 0.22, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5W4Ce2SQHeT": {
    "title": "Early Exit: A Nature Capability in Transformer-based Model Unveiling By joint Optimization",
    "volume": "review",
    "abstract": "Large language models demonstrate remarkable performance across various downstream tasks but face challenges with slow inference speeds due to their extensive parameters. The early exit approach, involving obtaining results from different intermediate layers for each token before reaching the final layer, offers a promising solution for accelerating auto-regressive decoding. However, additional output layers and the complexities of joint optimization challenges hinder the implementation of early exit in large language models (LLMs).In this paper, we explore the possibility of early exit within LLMs using the original final output layer and without relying on joint optimization. Our findings indicate that early exit is a natural capability within Transformer-based models. While joint optimization is not strictly necessary for early exit capability, it must be employed to address challenges by 1) improving the accuracy of locating the optimal early exit layer through gating functions and 2) mitigating key-value copy issues. Additionally, we uncover tendencies in early exit behavior that make the model struggle to predict the first sub-word in a word and identify the potential for early exit strategies based on sub-layers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QZB1rTrrkp": {
    "title": "More Semantically Focused Modeling for Semantic Text Matching",
    "volume": "review",
    "abstract": "As is widely acknowledged, Pre-trained Language Models (PLMs) acquire the capability to encode deep sentence semantics through pre-training. Semantic Text Matching (STM) task has greatly benefited from this capacity. However, the extent to which PLMs can fully exploit semantic encoding, rather than merely relying on some superficial pattern recognition in this task, remains a matter for investigation. We argue that a model's ability to provide consistent judgments despite variations in phrasing indicates its reliance on semantic interpretation. Based on this perspective, we investigate the extent to which the model captures semantics and introduce a novel training architecture aimed at enhancing the semantic modeling capacity of PLMs in STM tasks. Our approach is validated through rigorous experimentation on four benchmark datasets: LCQMC, BQ, QQP, and MRPC, where we achieve state-of-the-art performance on three of them",
    "checked": false,
    "id": "7a14b7856ca3170a1ee87db9fe20eb499c4bb01c",
    "semantic_title": "cross-modal semantically augmented network for image-text matching",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=_XYDM7bQYg": {
    "title": "FragRel: Exploiting Fragment-level Relations in LLM External Memory",
    "volume": "review",
    "abstract": "To process contexts with unlimited length using Large Language Models (LLMs), recent studies explore hierarchically managing the long text. Only several text fragments are taken from the external memory and passed into the temporary working memory, i.e., LLM's context window. However, existing approaches isolatedly handle the text fragments without considering their structural connections, thereby suffering limited capability on texts with intensive inter-relations, e.g., coherent stories and code repositories. This work attempts to resolve this by exploiting the fragment-level relations in external memory. First, we formulate the fragment-level relations and present several instantiations for different text types. Next, we introduce a relation-aware fragment assessment criteria upon previous independent fragment assessment. Finally, we present the fragment-connected Hierarchical Memory based LLM. We validate the benefits of involving these relations on long story understanding, repository-level code generation, and long-term chatting",
    "checked": false,
    "id": "ca0df686daf086421363760eacd94d3bc79eb27b",
    "semantic_title": "fragrel: exploiting fragment-level relations in the external memory of large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5q2k8j2nbdA": {
    "title": "Towards Verifiable Text Generation with Evolving Memory and Self-Reflection",
    "volume": "review",
    "abstract": "Despite the remarkable ability of large language models (LLMs) in language comprehension and generation, they often suffer from producing factually incorrect information, also known as hallucination. A promising solution to this issue is verifiable text generation, which prompts LLMs to generate content with citations for accuracy verification. However, verifiable text generation is non-trivial due to the focus-shifting phenomenon, the intricate reasoning needed to align the claim with correct citations, and the dilemma between the precision and breadth of retrieved documents. In this paper, we present VTG, an innovative framework for Verifiable Text Generation with evolving memory and self-reflection. VTG introduces evolving long short-term memory to retain both valuable documents and recent documents. A two-tier verifier equipped with an evidence finder is proposed to rethink and reflect on the relationship between the claim and citations. Furthermore, active retrieval and diverse query generation are utilized to enhance both the precision and breadth of the retrieved documents. We conduct extensive experiments on five datasets across three knowledge-intensive tasks and the results reveal that VTG significantly outperforms baselines",
    "checked": true,
    "id": "69e2aa1723b46e4df0445e8ecb838636b0911cd7",
    "semantic_title": "towards verifiable text generation with evolving memory and self-reflection",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=UqilA7ijsVG": {
    "title": "Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs",
    "volume": "review",
    "abstract": "The integration of large language models (LLMs) and search engines represents a significant evolution in knowledge acquisition methodologies. However, determining the knowledge that an LLM already possesses and the knowledge that requires the help of a search engine remains an unresolved issue. Most existing methods solve this problem through the results of preliminary answers or reasoning done by the LLM itself, but this incurs excessively high computational costs. This paper introduces a novel collaborative approach, namely SlimPLM, that detects missing knowledge in LLMs with a slim proxy model, to enhance the LLM's knowledge acquisition process. We employ a proxy model which has far fewer parameters, and take its answers as heuristic answers. Heuristic answers are then utilized to predict the knowledge required to answer the user question, as well as the known and unknown knowledge within the LLM. We only conduct retrieval for the missing knowledge in questions that the LLM does not know. Extensive experimental results on five datasets with two LLMs demonstrate a notable improvement in the end-to-end performance of LLMs in question-answering tasks, achieving or surpassing current state-of-the-art models with lower LLM inference costs",
    "checked": true,
    "id": "3df6c6cd13e49e637ced0ade0daa87738907a377",
    "semantic_title": "small models, big insights: leveraging slim proxy models to decide when and what to retrieve for llms",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=c2oOG4Z_iZT": {
    "title": "Learning to Maximize Mutual Information for Chain-of-Thought Distillation",
    "volume": "review",
    "abstract": "Knowledge distillation, the technique of transferring knowledge from large, complex models to smaller ones, marks a pivotal step towards efficient AI deployment. Distilling Step-by-Step (DSS), a novel method utilizing chain-of-thought (CoT) distillation, has demonstrated promise by imbuing smaller models with the superior reasoning capabilities of their larger counterparts. In DSS, the distilled model acquires the ability to generate rationales and predict labels concurrently through a multi-task learning framework. However, DSS overlooks the intrinsic relationship between the two training tasks, leading to ineffective integration of CoT knowledge with the task of label prediction. To this end, we investigate the mutual relationship of the two tasks from Information Bottleneck perspective and formulate it as maximizing the mutual information of the representation features of the two tasks. We propose a variational approach to solve this optimization problem using a learning-based method. Our experimental results across four datasets demonstrate that our method outperforms the state-of-the-art DSS. Our findings offer insightful guidance for future research on language model distillation as well as applications involving CoT. Code and models will be released soon",
    "checked": true,
    "id": "d8576994500d031d8611c64de3a96f12418f8de7",
    "semantic_title": "learning to maximize mutual information for chain-of-thought distillation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=iq5KVo3avCZ": {
    "title": "Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have shown impressive capabilities, yet they still struggle with math reasoning. In this work, we propose CoT-Influx, a novel approach that pushes the boundary of few-shot Chain-of-Thoughts (CoT) learning to improve LLM mathematical reasoning. Motivated by the observation that adding more concise CoT examples in the prompt can improve LLM reasoning performance, CoT-Influx employs a coarse-to-fine pruner to maximize the input of effective and concise CoT examples. The pruner first selects as many crucial CoT examples as possible and then prunes unimportant tokens to fit the context window. A math reasoning dataset with diverse difficulty levels and reasoning steps is used to train the pruner, along with a math-specialized reinforcement learning approach. As a result, by enabling more CoT examples with double the context window size in tokens, CoT-Influx significantly outperforms various prompting baselines across various LLMs (LLaMA2-7B, 13B, 70B) and 5 math datasets, achieving up to 4.55% absolute improvements. Remarkably, without any fine-tuning, LLaMA2-70B with CoT-Influx surpasses GPT-3.5 and a wide range of larger LLMs (PaLM, Minerva 540B, etc.) on the GSM8K. CoT-Influx serves as a plug-and-play module for LLMs and is compatible with most existing reasoning prompting techniques, such as self-consistency and self-verification",
    "checked": true,
    "id": "1f5b4e393a1e02ab49e5ca2e6819cc28841918a2",
    "semantic_title": "fewer is more: boosting llm reasoning with reinforced context pruning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=AwnPEEzzOJH": {
    "title": "Zero-Shot Chain-of-Thought Reasoning Guided by Evolutionary Algorithms in Large Language Models",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across diverse tasks and exhibited impressive reasoning abilities by applying zero-shot Chain-of-Thought (CoT) prompting. However, due to the evolving nature of sentence prefixes during the pre-training phase, existing zero-shot CoT prompting methods that employ identical CoT prompting across all task instances may not be optimal. In this paper, we introduce a novel zero-shot prompting method that leverages evolutionary algorithms to generate diverse promptings for LLMs dynamically. Our approach involves initializing two CoT promptings, performing evolutionary operations based on LLMs to create a varied set, and utilizing the LLMs to select a suitable CoT prompting for a given problem. Additionally, a rewriting operation, guided by the selected CoT prompting, enhances the understanding of the LLMs about the problem. Extensive experiments conducted across ten reasoning datasets demonstrate the superior performance of our proposed method compared to current zero-shot CoT prompting methods on GPT-3.5-turbo and GPT-4. Moreover, in-depth analytical experiments underscore the adaptability and effectiveness of our method in various reasoning tasks",
    "checked": true,
    "id": "f859b66e55aebdd4d460e2bc1e74640eafead5ad",
    "semantic_title": "zero-shot chain-of-thought reasoning guided by evolutionary algorithms in large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=vjuuo2rz5hZ": {
    "title": "An Element is Worth a Thousand Words: Enhancing Legal Case Retrieval by Incorporating Legal Elements",
    "volume": "review",
    "abstract": "Legal case retrieval plays an important role in promoting judicial justice and fairness. One of its greatest challenges is that the definition of relevance goes far beyond the common semantic relevance as in ad-hoc retrieval.In this paper, we reveal that the legal elements, which typically comprise key facts in a specialized legal context, can largely improve the relevance matching of legal case retrieval. To facilitate the use of legal elements, we construct a Chinese legal element dataset called LeCaRD-Elem based on the widely-used LeCaRD dataset, through a two-stage semi-automatic method with a minimized reliance on human labor. Meanwhile, we introduce two new models to enhance legal search using legal elements. The first, Elem4LCR-E, is a two-stage model that explicitly predicts legal elements from texts and then leverages them for improved ranking. Recognizing the potential benefits of more seamless integration, we further propose an end-to-end model called Elem4LCR-I, which internalizes the legal element knowledge into its model parameters using a tailored teacher-student training framework. Extensive experiments underscore the significant value of legal elements and demonstrate the superiority of our two proposed models in enhancing legal search over existing methods",
    "checked": true,
    "id": "55636d8295ada7c39ff4e100e738394ebfb2ad47",
    "semantic_title": "an element is worth a thousand words: enhancing legal case retrieval by incorporating legal elements",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xhUwJeU4mYr": {
    "title": "Structure-Aware Adapter for Large Language Model",
    "volume": "review",
    "abstract": "Pre-trained Large Language Models (LLMs) have been shown effective in various natural language processing tasks, especially when fine-tuned on specific downstream scenarios. However, the full fine-tuning of LLMs is usually computationally expensive and time-consuming due to the ever-increasing parameter size. In addition, while the LLMs are pre-trained to memorize the facts and knowledge from unstructured textual corpora, they cannot be well generalized to some domain-specific scenarios where additional structured knowledge is required, such as enterprise databases or social graphs. In this paper, we design a novel structure-aware adapter for LLMs to utilize structured relational information from knowledge graphs with a structure-aware relational attention mechanism. The proposed adapter framework only introduces a small scale of new parameters and therefore significantly reduces the cost of fine-tuning, without perturbing the initial pre-trained parameters of LLMs. We also propose a knowledge-graph-induced path-of-thought prompt to enhance the utilization of the LLM adapter to retrieve information from the knowledge graph. We evaluate the proposed model on two question-answering benchmarks. The evaluation results show that the proposed method outperforms the state-of-the-art LLM adapters by 4.1%-15.9% and 1.4%-17.6% in question-answering accuracy of CSQA and OBQA datasets. Ablation studies are also discussed to prove the effectiveness of the proposed modules",
    "checked": false,
    "id": "9cfb4fd70cee64cd0d300472147c4fda7962c93b",
    "semantic_title": "towards concept-aware large language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=psM5vRCHzaC": {
    "title": "KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases",
    "volume": "review",
    "abstract": "Program induction (PI) has become a promising paradigm for using knowledge bases (KBs) to help large language models (LLMs) answer complex knowledge-intensive questions. Nonetheless, PI typically relies on a large number of parallel question-program pairs to make the LLM aware of the schema of the given KB, and is thus challenging for many low-resourced KBs that lack annotated data. To this end, we propose KB-Plugin, a plug-and-play framework that enables LLMs to induce programs over any low-resourced KB. Firstly, KB-Plugin adopts self-supervised learning to encode the detailed schema information of a given KB into a pluggable module, namely schema plugin. Secondly, KB-Plugin utilizes abundant annotated data from a rich-resourced KB to train another pluggable module, namely PI plugin, which can help the LLM extract question-relevant schema information from the schema plugin of any KB and utilize this information to induce programs over this KB. Experiments on five heterogeneous KBQA datasets show that KB-Plugin achieves better or comparable performance with 25x smaller backbone LLM compared to SoTA PI methods for low-resourced KBs, and even approaches the performance of supervised methods",
    "checked": true,
    "id": "db62b76906ab2663c79a149aab58b2b08473ab16",
    "semantic_title": "kb-plugin: a plug-and-play framework for large language models to induce programs over low-resourced knowledge bases",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AqfbZdmCSJm": {
    "title": "Do LVLMs Understand Charts? Analyzing and Correcting Factual Errors in Chart Captioning",
    "volume": "review",
    "abstract": "Advances in large vision-language models (LVLMs) have led to significant progress in generating natural language descriptions for visual contents. These powerful models are known for producing texts that are factually inconsistent with the visual input. While some efforts mitigate such inconsistencies in natural image captioning, the factuality of generated captions for structured visuals, such as charts, has not received as much scrutiny. This work introduces a comprehensive typology of factual errors in generated chart captions. A large-scale human annotation effort provides insight into the error patterns in captions generated by various models, ultimately forming the foundation of a dataset, CHOCOLATE. Our analysis reveals that even advanced models like GPT-4V frequently produce captions laced with factual inaccuracies. To combat this, we establish the task of Chart Caption Factual Error Correction and introduce CHARTVE, a visual entailment model that outperforms current LVLMs in evaluating caption factuality. Furthermore, we propose C2TFEC, an interpretable two-stage framework that excels at correcting factual errors. This work inaugurates a new domain in factual error correction for chart captions, presenting a novel evaluation metric, and demonstrating an effective approach to ensuring the factuality of generated chart captions",
    "checked": true,
    "id": "3528ddcd62575aa4fbfc06141b97b80ad34e7c66",
    "semantic_title": "do lvlms understand charts? analyzing and correcting factual errors in chart captioning",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=jWpog5yDQRo": {
    "title": "QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction",
    "volume": "review",
    "abstract": "Employing Large Language Models (LLMs) for semantic parsing has achieved remarkable success. However, we find existing methods fall short in terms of reliability and efficiency when hallucinations are encountered. In this paper, we address these challenges with a framework called QueryAgent, which solves a question step-by-step and performs stepwise self-correction. We introduce an environmental feedback based self-correction method called ERASER. Unlike traditional approaches, ERASER leverages rich environmental feedback in the intermediate steps to perform selective and differentiated self-correction only when necessary. Experimental results demonstrate that QueryAgent notably outperforms all previous few-shot methods using only one example on GrailQA and GraphQ by 7.0 and 15.0 points. Furthermore, our approach exhibits superiority in terms of efficiency, including run-time, query overhead, and API invocation costs. By leveraging ERASER, we further improve another baseline (i.e., AgentBench) by approximately 10 points, validating the strong transferability of our approach",
    "checked": true,
    "id": "1a7eb221f88c2956cb5dbe77c474bad55a019738",
    "semantic_title": "queryagent: a reliable and efficient reasoning framework with environmental feedback based self-correction",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=g4o4izxdFn8": {
    "title": "Interpreting Conversational Dense Retrieval by Rewriting-Enhanced Inversion of Session Embedding",
    "volume": "review",
    "abstract": "Conversational dense retrieval has shown to be effective in conversational search. However, a major limitation of conversational dense retrieval is their lack of interpretability, hindering intuitive understanding of model behaviors for targeted improvements. This paper presents CONVINV, a simple yet effective approach to shed light on interpretable conversational dense retrieval models. CONVINV transforms opaque conversational session embeddings into explicitly interpretable text while faithfully maintaining their original retrieval performance as much as possible. Such transformation is achieved by training a recently proposed Vec2Text model based on the ad-hoc query encoder, leveraging the fact that the session and query embeddings share the same space in existing conversational dense retrieval.To further enhance interpretability, we propose to incorporate external interpretable query rewrites into the transformation process. Extensive evaluations on three conversational search benchmarks demonstrate that CONVINV can yield more interpretable text and faithfully preserve original retrieval performance than baselines. Our work connects opaque session embeddings with transparent query rewriting, paving the way toward trustworthy conversational search",
    "checked": true,
    "id": "9b10439739e68f1ad2468c1ab193c16c45304b35",
    "semantic_title": "interpreting conversational dense retrieval by rewriting-enhanced inversion of session embedding",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=99zgQW-dKPa": {
    "title": "RevOrder: A Novel Method for Enhanced Arithmetic in Language Models",
    "volume": "review",
    "abstract": "This paper presents RevOrder, a novel technique aimed at improving arithmetic operations in large language models (LLMs) by reversing the output digits in addition, subtraction, and n-digit by 1-digit (nD by 1D) multiplication tasks. Our method significantly reduces the Count of Sequential Intermediate Digits (CSID) to $\\mathcal{O}(1)$, a new metric we introduce to assess equation complexity. Through comprehensive testing, RevOrder not only achieves perfect accuracy in basic arithmetic operations but also substantially boosts LLM performance in division tasks, particularly with large numbers where traditional models struggle. Implementation of RevOrder is cost-effective for both training and inference phases. Moreover, applying RevOrder to fine-tune the LLaMA2-7B model on the GSM8K math task results in a considerable improvement, reducing equation calculation errors by 46% and increasing overall scores from 41.6 to 44.4",
    "checked": true,
    "id": "dc9ffc09f51f8c4c828219fec7d5967847f09684",
    "semantic_title": "revorder: a novel method for enhanced arithmetic in language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qk0Bsskwqos": {
    "title": "Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation",
    "volume": "review",
    "abstract": "Conversational search utilizes muli-turn natural language contexts to retrieve relevant passages. Existing conversational dense retrieval models mostly view a conversation as a fixed sequence of questions and responses, overlooking the severe data sparsity problem -- that is, users can perform a conversation in various ways, and these alternate conversations are unrecorded. Consequently, they often struggle to generalize to diverse conversations in real-world scenarios. In this work, we propose a framework for generalizing Conversational dense retrieval via LLM-cognition data Augmentation (ConvAug). We first generate multi-level augmented conversations to capture the diverse nature of conversational contexts. Inspired by human cognition, we devise a cognition-aware prompting process to mitigate the generation of false positives, false negatives, and hallucinations. Moreover, we develop a difficulty-adaptive sample filter that selects challenging samples for complex conversations, thereby giving the model a larger learning space. A contrastive learning objective is then employed to train a better conversational context encoder. Extensive experiments conducted on four public datasets, under both normal and zero-shot settings, demonstrate the effectiveness, generalizability, and applicability of ConvAug",
    "checked": true,
    "id": "0acb1b219c6c25a6d23e4267a061789a43ddd16b",
    "semantic_title": "generalizing conversational dense retrieval via llm-cognition data augmentation",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=4Av6E6dYKtn": {
    "title": "Learning to Rewrite Prompt for Boostrapping LLMs on Downstream Tasks",
    "volume": "review",
    "abstract": "As versatile agents, large-scale language models (LLMs) have demonstrated impressive performance across various domains. Their capabilities in language-based pattern recognition and machine learning have garnered significant attention and have been applied to numerous tasks with remarkable success. However, different LLMs still rely on specific instruction prompts, and the design of prompt tokens is still heavily dependent on manual design, which hinders the widespread application of LLMs. In response to this challenge, we propose a concise and effective input optimization method, which consists of two modules: original input rewriting and filtering. Inspired by the concept of collaboration between large and small models, we introduce a rewriting module between input prompts and LLMs inference. This module rewrites the input component based on the preferences of the LLMs for the data. The filtering module performs the quality inspection on the rewritten data and filters out invalid and hallucinatory data. Experimental results on language pattern recognition tasks verify that our rewriting and filtering method effectively transforms ambiguous data into more precise input prompts. In comparison to the original inputs, performance improvement is consistently observed across various tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hSZUj8AELJy": {
    "title": "An Empirical Analysis on Large Language Models in Debate Evaluation",
    "volume": "review",
    "abstract": "In this study, we investigate the capabilities and inherent biases of advanced large language models (LLMs) such as GPT-3.5 and GPT-4 in the context of debate evaluation. We discover that LLM's performance exceeds humans and surpasses the performance of state-of-the-art methods fine-tuned on extensive datasets. We additionally explore and analyze biases present in LLMs, including positional bias, lexical bias, order bias, which may affect their evaluative judgments. Our findings reveal a consistent bias in both GPT-3.5 and GPT-4 towards the second candidate response presented, attributed to prompt design. We also uncover a lexical bias in both GPT-3.5 and GPT-4, especially when label sets carry connotations such as numerical or sequential, highlighting the critical need for careful label verbalizer selection in prompt design. Additionally, our analysis indicates a tendency of both models to favor the debate's concluding side as the winner, suggesting an end-of-discussion bias",
    "checked": true,
    "id": "6527d22c3ab21206e4bfe88ce5533e2b2be329bd",
    "semantic_title": "an empirical analysis on large language models in debate evaluation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=wGXat_Ix5bC": {
    "title": "A Data-Efficient Nearest-Neighbor Language Model via Lightweight Nets",
    "volume": "review",
    "abstract": "The $k$NN-MT method incorporates non-parametric techniques into Neural Machine Translation (NMT) to improve translation performance without fine-tuning the model. However, this approach utilizes the $k$-nearest neighbors algorithm at the token level, which requires significant storage space and slows down the inference speed. In this paper, we propose training a lightweight neural network as a substitute for $k$NN search, resulting in a substantial reduction in storage overhead while sacrificing a marginal amount of translation performance. Moreover, the lightweight network also ensures fast inference speed. According to our calculations, the storage overhead of our method is negligible compared to $k$NN-MT, and we can achieve its 92\\% to 99\\% translation performance on different datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PH2RuB_pOIR": {
    "title": "MemAPM: Memory-augmented Large Language Model Agent for Asset Pricing",
    "volume": "review",
    "abstract": "In this study, we propose a hybrid asset pricing model, MemAPM, which utilizes a Large Language Model (LLM) agent to refine information from news and augment it with a memory of past refined news. We perform experiments on a two-year span of news and around 70 years of market data, our method outperforms the state-of-the-art machine learning-based asset pricing baselines in multiple portfolio optimization and asset pricing error evaluations. We also performed an ablation study and evaluated the predictive power of the augmented news features for the price movement of individual stocks and economic indicators. The results show the effectiveness of our proposed memory augmentation technique and hybrid asset pricing network architecture",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KiE5MNxMXiK": {
    "title": "I Think, Therefore I am: Benchmarking Awareness of Large Language Models Using AwareBench",
    "volume": "review",
    "abstract": "Do large language models (LLMs) exhibit any forms of awareness similar to humans? In this paper, we introduce AwareBench, a benchmark designed to evaluate awareness in LLMs. Drawing from theories in psychology and philosophy, we define awareness in LLMs as the ability to understand themselves as AI models and to exhibit social intelligence. Subsequently, we categorize awareness in LLMs into five dimensions, including capability, mission, emotion, culture, and perspective. Based on this taxonomy, we create a dataset called AwareEval, which contains binary, multiple-choice, and open-ended questions to assess LLMs' understandings of specific awareness dimensions. Our experiments, conducted on 13 LLMs, reveal that the majority of them struggle to fully recognize their capabilities and missions while demonstrating decent social intelligence. We conclude by connecting awareness of LLMs with AI alignment and safety, emphasizing its significance to the trustworthy and ethical development of LLMs",
    "checked": true,
    "id": "a3c05430f5af431d21c3707905ae4deec84c0041",
    "semantic_title": "i think, therefore i am: benchmarking awareness of large language models using awarebench",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=aDmhtLHiduX": {
    "title": "Beyond Text: Unveiling Multimodal Proficiency of Large Language Models with MultiAPI Benchmark",
    "volume": "review",
    "abstract": "The proliferation of Large Language Models like ChatGPT has significantly advanced language understanding and generation, impacting a broad spectrum of applications. However, these models predominantly excel in text-based tasks, overlooking the complexity of real-world multimodal information. This study introduces \\textbf{MultiAPI}, a pioneering comprehensive large-scale API benchmark dataset aimed at expanding LLMs' proficiency in multimodal contexts. Developed collaboratively through ChatGPT, \\textbf{MultiAPI} consists of 187 diverse API calls and 1,799 contextual prompts, offering a unique platform evaluation of tool-augmented LLMs handling multimodal tasks. Through comprehensive experiments, our findings reveal that while LLMs demonstrate proficiency in API call decision-making, they face challenges in domain identification, function selection, and argument generation. What's more, we surprisingly notice that auxiliary context can actually impair the performance. An in-depth error analysis paves the way for a new paradigm to address these challenges, suggesting a potential direction for future LLM research",
    "checked": true,
    "id": "36e85116f25bd02dc9445a4be80118e7cab59316",
    "semantic_title": "beyond text: unveiling multimodal proficiency of large language models with multiapi benchmark",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=j4UcrN4-vyU": {
    "title": "ControlLM: Crafting Diverse Personalities for Language Models",
    "volume": "review",
    "abstract": "As language models continue to scale in size and capability, they display an array of emerging behaviors, both beneficial and concerning. This heightens the need to control model behaviors. We hope to be able to control the personality traits of language models at the inference-time so as to have various character features, on top of which the requirements of different types of tasks can be met. Personality is a higher-level and more abstract behavioral representation for language models. We introduce ControlLM, which leverages differential activation patterns, derived from contrasting behavioral prompts in the model's latent space, to influence the model's personality traits at inference. This approach allows for the precise, real-time adjustment of model behavior. First, we demonstrate ControlLM's capacity to elicit diverse persona behaviors without any training, while precision control allows personality traits to closely match average human values. Subsequently, we showcase improved reasoning and question answering through selective amplification of beneficial attributes like conscientiousness and friendliness. We hope that this work will inspire research on controlling human-like behaviors of language models and provide insights for future research",
    "checked": true,
    "id": "972e63aab853267c42b18a125edac531ace3a35f",
    "semantic_title": "controllm: crafting diverse personalities for language models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=xuPyWt0gEEe": {
    "title": "HAM-TTS: Hierarchical Acoustic Modeling for Token-Based Zero-Shot TTS with Model and Data Scaling",
    "volume": "review",
    "abstract": "Token-based text-to-speech (TTS) models have emerged as a promising avenue for generating natural and realistic speech, yet they grapple with low pronunciation accuracy, speaking style and timbre inconsistency, and a substantial need for diverse training data. In response, we introduce a novel hierarchical acoustic modeling approach complemented by a tailored data augmentation strategy and train it on the combination of real and synthetic data, scaling the data size up to 650k hours, leading to the zero-shot TTS model with 0.8B parameters. Specifically, our method incorporates a latent variable sequence containing supplementary acoustic information based on refined self-supervised learning (SSL) discrete units into the TTS model by a predictor. This significantly mitigates pronunciation errors and style mutations in synthesized speech. During training, we strategically replace and duplicate segments of the data to enhance timbre uniformity. Moreover, a pretrained few-shot voice conversion model is utilized to generate a plethora of voices with identical content yet varied timbres. This facilitates the explicit learning of utterance-level one-to-many mappings, enriching speech diversity and also ensuring consistency in timbre. Comparative experiments demonstrate our model's superiority over VALL-E in pronunciation precision and maintaining speaking style, as well as timbre continuity",
    "checked": false,
    "id": "4a0692c5e5c33588cb33362284a978ca7a9b8663",
    "semantic_title": "ham-tts: hierarchical acoustic modeling for token-based zero-shot text-to-speech with model and data scaling",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M7Hcxm9khdV": {
    "title": "Towards Boosting Many-to-Many Multilingual Machine Translation with Large Language Models",
    "volume": "review",
    "abstract": "The training paradigm for machine translation has gradually shifted, from learning neural machine translation (NMT) models with extensive parallel corpora to instruction finetuning on multilingual large language models (LLMs) with high-quality translation pairs. In this paper, we focus on boosting many-to-many multilingual translation of LLMs with an emphasis on zero-shot translation directions. We demonstrate that prompt strategies adopted during finetuning are crucial to zero-shot translation and introduce a cross-lingual consistency regularization, XConST, to bridge the representation gap among different languages and improve zero-shot translation performance. XConST is not a new method, but a version of CrossConST (Gao et al., 2023a) adapted for translation instruction finetuning with LLMs. Experimental results on ALMA (Xu et al., 2023), Tower (Team, 2024), and LLaMA-2 (Touvron et al., 2023) show that our approach consistently improves translation performance",
    "checked": true,
    "id": "d5a50700c8850559ed27adb9f59cbf5b3a210529",
    "semantic_title": "towards boosting many-to-many multilingual machine translation with large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=J_NsjPEli_m": {
    "title": "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully",
    "volume": "review",
    "abstract": "Large language models (LLMs) demonstrate great performance in text generation. However, LLMs are still suffering from hallucinations. In this work, we propose an inference-time method, Self-Highlighted Hesitation (SH2), to help LLMs decode more truthfully. SH2 is based on a simple fact rooted in information theory that for an LLM, the tokens predicted with lower probabilities are prone to be more informative than others. Our analysis shows that the tokens assigned with lower probabilities by an LLM are more likely to be closely related to factual information, such as nouns, proper nouns, and adjectives. Therefore, we propose to \"highlight'' the factual information by selecting the tokens with the lowest probabilities and concatenating them to the original context, thus forcing the model to repeatedly read and hesitate on these tokens before generation. During decoding, we also adopt contrastive decoding to emphasize the difference in the output probabilities brought by the hesitation. Experimental results demonstrate that our SH2, requiring no additional data or models, can effectively help LLMs elicit factual knowledge and distinguish hallucinated contexts. Significant and consistent improvements are achieved by SH2 for LLaMA-7b, LLaMA2-7b and Mistral-7b on multiple hallucination tasks",
    "checked": true,
    "id": "a27529194bd61c6013fb0f48797f08e4998a1be8",
    "semantic_title": "sh2: self-highlighted hesitation helps you decode more truthfully",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=MH6MpQLEMXA": {
    "title": "ANAH: Analytical Annotation of Hallucinations in Large Language Models",
    "volume": "review",
    "abstract": "Reducing the `hallucination' problem of Large Language Models (LLMs) is crucial for their wide applications. A comprehensive and fine-grained measurement of the hallucination is the first key step for the governance of this issue but is under-explored in the community. Thus, we present ANAH, a bilingual dataset that offers ANalytical Annotation of Hallucinations in LLMs within Generative Question Answering. Each answer sentence in our dataset undergoes rigorous annotation, involving the retrieval of a reference fragment, the judgment of the hallucination type, and the correction of hallucinated content. ANAH consists of ~12k sentence-level annotations for ~4.3k LLM responses covering over 700 topics, constructed by a human-in-the-loop pipeline. Thanks to the fine granularity of the hallucination annotations, we can quantitatively confirm that the hallucinations of LLMs progressively accumulate in the answer and use ANAH to train and evaluate hallucination annotators. We conduct extensive experiments on studying generative and discriminative annotators and show that, although current open-source LLMs have difficulties in fine-grained hallucination annotation, the generative annotator trained with ANAH can surpass all open-source LLMs and GPT-3.5, obtain performance competitive with GPT-4, and exhibits better generalization ability on unseen questions",
    "checked": true,
    "id": "3c3f5af1aee19bf0093c40f35a120744d099723e",
    "semantic_title": "anah: analytical annotation of hallucinations in large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=fuBbr3tb6Yo": {
    "title": "PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety",
    "volume": "review",
    "abstract": "Multi-agent systems, when enhanced with Large Language Models (LLMs), exhibit profound capabilities in collective intelligence. However, the potential misuse of this intelligence for malicious purposes presents significant risks. To date, comprehensive research on the safety issues associated with multi-agent systems remains limited. In this paper, we explore these concerns through the innovative lens of agent psychology, revealing that the dark psychological states of agents constitute a significant threat to safety. To tackle these concerns, we propose a comprehensive framework~(\\textit{PsySafe}) grounded in agent psychology, focusing on three key areas: firstly, identifying how dark personality traits in agents can lead to risky behaviors; secondly, evaluating the safety of multi-agent systems from the psychological and behavioral perspectives, and thirdly, devising effective strategies to mitigate these risks. Our experiments reveal several intriguing phenomena, such as the collective dangerous behaviors among agents, agents' self-reflection when engaging in dangerous behavior, and the correlation between agents' psychological assessments and dangerous behaviors. We anticipate that our framework and observations will provide valuable insights for further research into the safety of multi-agent systems. Our data and code will be released after the manuscript is accepted",
    "checked": true,
    "id": "684ede7013b3d844dae13090350a5c27b196727f",
    "semantic_title": "psysafe: a comprehensive framework for psychological-based attack, defense, and evaluation of multi-agent system safety",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=tdjCkiUu6q_": {
    "title": "TransliCo: A Contrastive Learning Framework to Address the Script Barrier in Multilingual Pretrained Language Models",
    "volume": "review",
    "abstract": "The world's more than 7000 languages are written in at least 293 scripts. Due to various reasons, many closely related languages use different scripts, which poses a difficulty for multilingual pretrained language models (mPLMs) in learning crosslingual knowledge through lexical overlap. As a consequence, mPLMs are faced with a script barrier: representations from different scripts are located in different subspaces, which can result in crosslingual transfer involving languages of different scripts performing suboptimally. To address this problem, we propose Furina, a framework that optimizes the Transliteration Contrastive Modeling (TCM) objective to fine-tune an mPLM by contrasting sentences in its training data and their transliterations in a unified script (in our case Latin), which enhances uniformity in the representation space for different scripts. Using Glot500-m, an mPLM pretrained on over 500 languages, as our source model, we fine-tune it on a small portion (5%) of its training data, and refer to the resulting model as Furina. We show that Furina not only better aligns representations from distinct scripts but also outperforms the original Glot500-m on various crosslingual transfer tasks. Additionally, we achieve consistent improvement in a case study on the Indic group where the languages are highly related but use different scripts. We make our code and models publicly available",
    "checked": true,
    "id": "0bb0b4507683a0ad4271e1245234b80f03683f50",
    "semantic_title": "translico: a contrastive learning framework to address the script barrier in multilingual pretrained language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cnJQJYs3SLI": {
    "title": "Data Management For Large Language Models: A Survey",
    "volume": "review",
    "abstract": "Data plays a fundamental role in training Large Language Models (LLMs). Efficient data management, particularly in formulating a well-suited training dataset, is significant for enhancing model performance and improving training efficiency during pretraining and supervised fine-tuning stages. Despite the considerable importance of data management, the current research community still falls short in providing a systematic analysis of the effects of data management strategy selection, methodologies for evaluating curated datasets, and the ongoing pursuit of improved strategies. Consequently, the exploration of data management has attracted more and more attention among the research community. This survey provides a comprehensive overview of current research in data management within both the pretraining and supervised fine-tuning stages of LLMs, covering various noteworthy aspects of data management strategy design: data quantity, data quality, domain/task composition, etc. Looking toward the future, we extrapolate existing challenges and outline promising directions for development in this field. Therefore, this survey serves as a guiding resource for practitioners aspiring to construct powerful LLMs through efficient data management practices",
    "checked": false,
    "id": "2f79b5eebe8f04566938d4d1eafbc885346f4f80",
    "semantic_title": "data management for training large language models: a survey",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=7-49YX6LL0B": {
    "title": "T-Eval: Evaluating the Tool Utilization Capability of Large Language Models Step by Step",
    "volume": "review",
    "abstract": "Large language models (LLMs) have achieved remarkable performance on various NLP tasks and are augmented by tools for broader applications. Yet, how to evaluate and analyze the tool utilization capability of LLMs is still under-explored. In contrast to previous works that evaluate models holistically, we comprehensively decompose the tool utilization into multiple sub-processes, including instruction following, planning, reasoning, retrieval, understanding, and review. Based on that, we further introduce T-Eval to evaluate the tool-utilization capability step by step. T-Eval disentangles the tool utilization evaluation into several sub-domains along model capabilities, facilitating the inner understanding of both holistic and isolated competency of LLMs. We conduct extensive experiments on T-Eval and in-depth analysis of various LLMs. T-Eval not only exhibits consistency with the outcome-oriented evaluation but also provides a more fine-grained analysis of the capabilities of LLMs, providing a new perspective in LLM evaluation on tool-utilization ability. The benchmark will be available",
    "checked": true,
    "id": "caf60d1120c2d5a894098f01b51d2e2ad32301d7",
    "semantic_title": "t-eval: evaluating the tool utilization capability of large language models step by step",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=X2NECFTFh6i": {
    "title": "NAT3DSound: 3D Spatial Sound Field Synthesis with Multi-Modal Non-Autoregressive Transformer",
    "volume": "review",
    "abstract": "3D spatial sound field synthesis takes the head-mounted audio signals and body poses as input and renders a 3D sound field around the center body, in which spatial audio can be inferred at any arbitrary position. To achieve this, a multi-modal system is required to spatialize input audio signals with the guidance of pose streams. Then, a numerical method is used to render the sound field with hundreds of spatial audio. However, there exist several challenges including hybrid audio input signals, cross-modal matching, and few data resources. To address them, we propose NAT3DSound, a unit-based framework for high-quality 3D spatial sound synthesis, which consists of 1) a general tokenization scheme for both dense and impulsive input audio signals, 2) a non-autoregressive transformer with multi-scale modality fusion for efficient cross-modal alignment and 3) a parallel sampling strategy for fast prediction. Furthermore, we investigate the feasibility of acoustic pre-training for low-resource learning in data-scarce scenarios. Extensive experiments and ablation studies demonstrate the effectiveness of NAT3DSound in terms of spatialization quality and generalization ability. Audio samples are available at \\url{http://NAT3DSound.github.io}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VrkAbKWnSuq": {
    "title": "Beyond Literal Descriptions: Understanding and Locating Open-World Objects Aligned with Human Intentions",
    "volume": "review",
    "abstract": "Visual grounding (VG) aims at locating the foreground entities that match the given natural language expression. Previous datasets and methods for classic VG task mainly rely on the prior assumption that the given expression must literally refer to the target object, which greatly impedes the practical deployment of agents in real-world scenarios. Since users usually prefer to provide the intention-based expressions for the desired object instead of covering all the details, it is necessary for the agents to interpret the intention-driven instructions. Thus, in this work, we take a step further to the intention-driven visual-language (V-L) understanding. To promote classic VG towards human intention interpretation, we propose a new intention-driven visual grounding (IVG) task and build a largest-scale IVG dataset named IntentionVG with free-form intention expressions. Considering that practical agents need to move and find specific targets among various scenarios to realize the grounding task, our IVG task and IntentionVG dataset have taken the crucial properties of both multi-scenario perception and egocentric view into consideration. Besides, various types of models are set up as the baselines to realize our IVG task. Extensive experiments on our IntentionVG dataset and baselines demonstrate the necessity and efficacy of our method for the V-L field. To foster future research in this direction, our newly built dataset and baselines will be publicly available",
    "checked": true,
    "id": "57247a1d8575795b4713284318f960cd3f4d6bd7",
    "semantic_title": "beyond literal descriptions: understanding and locating open-world objects aligned with human intentions",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=cRUJaGKZ5qT": {
    "title": "Evaluating Large Language Models on Wikipedia-Style Survey Generation",
    "volume": "review",
    "abstract": "Educational materials such as survey articles in specialized fields like computer science traditionally require tremendous expert inputs and are therefore expensive to create and update. Recently, Large Language Models (LLMs) have achieved significant success across various general tasks. However, their effectiveness and limitations in the education domain are yet to be fully explored. In this work, we examine the proficiency of LLMs in crafting succinct survey articles specific to the niche field of NLP in computer science, focusing on a curated list of 99 topics. Automated benchmarks reveal that GPT-4 surpasses its predecessors like GPT-3.5, PaLM2, and LLaMa2 in comparison to the established ground truth. We compare both human and GPT-based evaluation scores and provide in-depth analysis. While our findings suggest that GPT-created surveys are more contemporary and accessible than human-authored ones, certain limitations were observed. Notably, GPT-4, despite often delivering outstanding content, occasionally exhibited lapses like missing details or factual errors. At last, we compared the rating behavior between humans and GPT-4 and found systematic bias in using GPT evaluation",
    "checked": false,
    "id": "d8cb4c151aabdd04ded60c23dd2182060d6d522f",
    "semantic_title": "large language models on wikipedia-style survey generation: an evaluation in nlp concepts",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=JZ_1NxzKYvY": {
    "title": "Evaluation Ethics of LLMs in Legal Domain",
    "volume": "review",
    "abstract": "In recent years, the utilization of large language models for natural language dialogue has gained momentum, leading to their widespread adoption across various domains. However, their universal competence in addressing challenges specific to specialized fields such as law remains a subject of scrutiny. The incorporation of legal ethics into the model has been overlooked by researchers. We asserts that rigorous ethic evaluation is essential to ensure the effective integration of large language models in legal domains, emphasizing the need to assess domain-specific proficiency and domain-specific ethic. To address this, we propose a novelty evaluation methodology, utilizing authentic legal cases to evaluate the fundamental language abilities, specialized legal knowledge and legal robustness of large language models (LLMs). The findings from our comprehensive evaluation contribute significantly to the academic discourse surrounding the suitability and performance of large language models in legal domains",
    "checked": true,
    "id": "9433efc73dc22a786eebce80502a8ef8b5fb265b",
    "semantic_title": "evaluation ethics of llms in legal domain",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=jaYchsQpqm4": {
    "title": "Advancing Abductive Reasoning in Knowledge Graphs through Complex Logical Hypothesis Generation",
    "volume": "review",
    "abstract": "Abductive reasoning is the process of making educated guesses to provide explanations for observations. Although many applications require the use of knowledge for explanations, the utilization of abductive reasoning in conjunction with structured knowledge, such as a knowledge graph, remains largely unexplored.To fill this gap, this paper introduces the task of complex logical hypothesis generation, as an initial step towards abductive logical reasoning with KG.In this task, we aim to generate a complex logical hypothesis so that it can explain a set of observations.We find that the supervised trained generative model can generate logical hypotheses that are structurally closer to the reference hypothesis. However, when generalized to unseen observations, this training objective does not guarantee better hypothesis generation. To address this, we introduce the Reinforcement Learning from Knowledge Graph (RLF-KG) method, which minimizes differences between observations and conclusions drawn from generated hypotheses according to the KG.Experiments show that, with RLF-KG's assistance, the generated hypotheses provide better explanations, and achieve state-of-the-art results on three widely used KGs",
    "checked": true,
    "id": "cbc573028f5eef2fffe37b69fb9b510b7c373184",
    "semantic_title": "advancing abductive reasoning in knowledge graphs through complex logical hypothesis generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6-1cnVIMruY": {
    "title": "VideoQA in the Era of LLMs: Insights on Model Behavior",
    "volume": "review",
    "abstract": "We analyze the behavior of emerging multimodal large language models (MLLMs) in video question answering (VideoQA), aiming to elucidate their strengths, weaknesses, and open up new frontiers for robust and trustworthy VideoQA. Our results expose that despite the higher QA accuracy, MLLMs remain similar to previous non-LLM techniques in their uninterpretability and vulnerability to a series of adversarial probes. For example, they are extremely weak in temporal understanding (\\eg, \\emph{insensitive to question variations of before/after/when}), visual grounding (\\eg, \\emph{jump to the answer without taking into account video or with integration of coarse-grained visual signal from irrelevant video context.}), and display limited capability of multimodel reasoning from video-question to answer (\\eg, \\emph{prioritizing answers that share key words with questions or containing detected visual concepts.}). Moreover, despite featuring LLMs, the models are not robust to language variation (\\eg, question rephrase). Finally, MLLMs still tend to make predictions skewed towards answers of high frequency and struggle to generalize as well",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yPrfueg1TSp": {
    "title": "Inconsistencies in Masked Language Models",
    "volume": "review",
    "abstract": "Learning to predict masked tokens in a sequence has been shown to be a helpful pretraining objective for powerful language models such as PaLM2. After training, such masked language models (MLMs) can provide distributions of tokens in the masked positions in a sequence. However, this paper shows that distributions corresponding to different masking patterns can demonstrate considerable inconsistencies, i.e., they cannot be derived from a coherent joint distribution when considered together. This fundamental flaw in MLMs can lead to self-contradictory behaviors during inference. On various benchmark datasets including MMLU, MLMs can give different predictions to the same input question. From BERT-base to UL2-20B, we show that such inconsistencies exist ubiquitously in MLMs of diverse sizes and configurations. In light of our observations, we further propose an inference-time strategy for MLMs called Ensemble of Conditionals. It jointly considers a selected range of inconsistent conditionals directly produced by the MLM for the final prediction, which often leads to considerable accuracy improvement",
    "checked": true,
    "id": "9961b7c0d904ab5997763f61193c17f3220082f7",
    "semantic_title": "inconsistencies in masked language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=pDqKM4EKspN": {
    "title": "Isotropy, Clusters, and Classifiers",
    "volume": "review",
    "abstract": "Whether embedding spaces use all their dimensions equally, i.e., whether they are isotropic, has been a recent subject of discussion.Evidence has been accrued both for and against enforcing isotropy in embedding spaces. In the present paper, we stress that isotropy imposes requirements on the embedding space that are not compatible with the presence of clusters---which also negatively impacts linear classification objectives. We demonstrate this fact empirically and use it to shed light on previous results from the literature",
    "checked": true,
    "id": "e2c9be05cf6f0d5c2febf4d129040bc7a2e3fa0a",
    "semantic_title": "isotropy, clusters, and classifiers",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H9do60Pr62G": {
    "title": "MULTI: Multimodal Understanding Leaderboard with Text and Images",
    "volume": "review",
    "abstract": "Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community, while existing benchmarks primarily focus on understanding simple natural images and short context. In this paper, we present MULTI, as a cutting-edge benchmark for evaluating MLLMs on understanding complex tables and images, and reasoning with long context. MULTI provides multimodal inputs and requires responses that are either precise or open-ended, reflecting real-life examination styles. MULTI includes over 18,000 questions and challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis and cross-modality reasoning. We also introduce MULTI-Elite, a 500-question selected hard subset, and MULTI-Extend, with more than 4,500 external knowledge context pieces. Our evaluation indicates significant potential for MLLM advancement, with GPT-4V achieving a 63.7% accuracy rate on MULTI, in contrast to other MLLMs scoring between 28.5% and 55.3%. MULTI serves not only as a robust evaluation platform but also paves the way for the development of expert-level AI",
    "checked": true,
    "id": "9b86d92b01d923dbf386aeeb80ad4d79e530379e",
    "semantic_title": "multi: multimodal understanding leaderboard with text and images",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CsR2BSlK-2U": {
    "title": "Generating Demonstrations for In-Context Compositional Generalization in Grounded Language Learning",
    "volume": "review",
    "abstract": "In-Context-learning and few-shot prompting are viable methods compositional output generation. However, these methods can be very sensitive to the choice of support examples used. Retrieving good supports from the training data for a given test query is already a difficult problem, but in some cases solving this may not even be enough. We consider the setting of grounded language learning problems where finding relevant supports in the same or similar states as the query may be difficult. We design an agent which instead generates possible supports inputs and targets current state of the world, then uses them in-context-learning to solve the test query. We show substantially improved performance on a previously unsolved compositional generalization test without a loss of performance in other areas. The approach is general and can even scale to instructions expressed in natural language",
    "checked": false,
    "id": "1d41a0ddda57caa6c8d268dd1703e4c9b35db18b",
    "semantic_title": "one-shot learning from a demonstration with hierarchical latent language",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=2svzns2fFir": {
    "title": "Are Multilingual Autoregressive Language Models Good Universal Embedders?",
    "volume": "review",
    "abstract": "In the large language model (LLM) revolution, embedding is a key component of various systems, such as retrieving knowledge or memories for LLMs or building content moderation filters. As such cases span from English to other natural or programming languages, from retrieval to classification and beyond, it is advantageous to build a unified embedding model rather than dedicated ones for each scenario. In this context, the pre-trained multilingual decoder-only large language models, \\eg BLOOM, emerge as a viable backbone option. To assess their potential, we propose straightforward strategies for constructing embedders and introduce a universal evaluation benchmark. Experimental results show that our trained model is proficient at generating good embeddings across languages and tasks, even extending to languages and tasks for which no finetuning/pretraining data is available. We also present detailed analyses and additional evaluations. We hope that this work could encourage the development of more robust open-source universal embedders",
    "checked": false,
    "id": "4438aee4bc82fd733f6d212d4d21c95712f4be30",
    "semantic_title": "towards language-universal mandarin-english speech recognition with unsupervised label synchronous adaptation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yz_hubUAm0": {
    "title": "How does a text preprocessing pipeline affect ontology matching?",
    "volume": "review",
    "abstract": "The generic text preprocessing pipeline, comprising Tokenisation, Normalisation, Stop Words Removal, and Stemming/Lemmatisation, has been implemented in many ontology matching (OM) systems. However, the lack of standardisation in text preprocessing creates diversity in mapping results. In this paper, we investigate the effect of the text preprocessing pipeline on OM tasks at syntactic levels. Our experiments on 8 Ontology Alignment Evaluation Initiative (OAEI) track repositories with 44 distinct alignments indicate: (1) Tokenisation and Normalisation are currently more effective than Stop Words Removal and Stemming/Lemmatisation; and (2) The selection of Lemmatisation and Stemming is task-specific. We recommend standalone Lemmatisation or Stemming with post-hoc corrections. We find that (3) Porter Stemmer and Snowball Stemmer perform better than Lancaster Stemmer; and that (4) Part-of-Speech (POS) Tagging does not help Lemmatisation. To repair less effective Stop Words Removal and Stemming/Lemmatisation used in OM tasks, we propose a novel context-based pipeline repair approach that significantly improves matching correctness and overall matching performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CdlbfwyYa-J": {
    "title": "Knowledgeable Preference Alignment for LLMs in Domain-specific Question Answering",
    "volume": "review",
    "abstract": "Deploying large language models (LLMs) to real scenarios for domain-specific question answering (QA) is a key thrust for LLM applications, which poses numerous challenges, especially in ensuring that responses are both accommodating to user requirements and appropriately leveraging domain-specific knowledge. They are the two major difficulties for LLM application as vanilla fine-tuning falls short of addressing. Combining these requirements, we conceive of them as the requirement for the model's preference to be harmoniously aligned with humans'. Thus, we introduce Knowledgeable Preference AlignmenT (KnowPAT), which constructs two kinds of preference sets to tackle the two issues. Besides, we design a new alignment objective to align the LLM preference with different human preferences uniformly, aiming to optimize LLM performance in real-world, domain-specific QA settings. Adequate experiments and comprehensive comparisons with 15 baseline methods illustrate that our KnowPAT is a superior pipeline for real-scenario domain-specific QA with LLMs. Our code is available at https://anonymous.4open.science/r/KnowPAT-0135",
    "checked": true,
    "id": "a8568bac56c24b5d25e373a117f947171d5f97be",
    "semantic_title": "knowledgeable preference alignment for llms in domain-specific question answering",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=_NHO1p00_yA": {
    "title": "Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning",
    "volume": "review",
    "abstract": "Large language models exhibit high-level commonsense reasoning abilities, especially with enhancement methods like Chain-of-Thought (CoT). However, we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem. To interpret and mitigate this problem, we first utilize attribution tracing and causal tracing methods to probe the internal working mechanism of the LLM during CoT reasoning. Through comparisons, we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers. Based on the probing findings, we design a novel method called RIDERS (Residual decodIng and sERial-position Swap), which compensates for the information deficit in the model from both decoding and serial-position perspectives. Through extensive experiments on multiple commonsense reasoning benchmarks, we validate that this method not only significantly eliminates Toxic CoT problems (decreased by $\\textbf{23.6\\%}$), but also effectively improves the model's overall commonsense reasoning performance (increased by $\\textbf{5.5\\%}$)",
    "checked": true,
    "id": "879b54d587afd74a18378ef44bef0170d07ab526",
    "semantic_title": "focus on your question! interpreting and mitigating toxic cot problems in commonsense reasoning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KaXYHDJYJx": {
    "title": "STEP-RLHF: Step-wise Reinforcement Learning from Human Feedback",
    "volume": "review",
    "abstract": "Recently, advancements in large language models have enhanced the ability to perform intricate multi-step reasoning. Reinforcement learning from human feedback poses a significant challenge, particularly in tasks requiring intricate reasoning over multiple steps. In this paper, we introduces the Step-wise Reinforcement Learning from Human Feedback (Step-RLHF) algorithm, designed to address this challenge. Step-RLHF incorporates a step-wise reward model, providing feedback at each intermediate reasoning step. Additionally, during Proximal Policy Optimization (PPO) training, the algorithm applies Generalized Advantage Estimation (GAE) and policy optimization at each step. In our investigation, we showcase the applicability of our approach in mathematical tasks, illustrating that learning from step-wise reward functions and updating the policy step by step significantly improves model performance. This work represents a crucial step towards enhancing the adaptability and precision of language models in multi-step reasoning tasks through the integration of step-wise human feedback within the RLHF framework",
    "checked": false,
    "id": "38c8111873cb40d28c8bbc8aa6836a172234b5fa",
    "semantic_title": "nash learning from human feedback",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=z3pIsErqWy": {
    "title": "Crayon: Customized On-Device LLM via Instant Adapter Blending and Edge-Server Hybrid Inference",
    "volume": "review",
    "abstract": "The customization of large language models (LLMs) for user-specified tasks gets important. However, maintaining all the customized LLMs on cloud servers incurs substantial memory and computational overheads, and uploading user data can also lead to privacy concerns. On-device LLMs can offer a promising solution by mitigating these issues. Yet, the performance of on-device LLMs is inherently constrained by the limitations of small-scaled models that edge devices can feasibly support. To overcome these restrictions, we first propose Crayon, a novel approach for on-device LLM customization. Crayon begins by constructing a pool of diverse base adapters, and then we instantly blend them into a customized adapter without extra training. In addition, we develop a device-server hybrid inference strategy, which deftly allocates more demanding queries or non-customized tasks to a larger, more capable LLM housed on a server. This ensures optimal performance without sacrificing the benefits of on-device customization. We carefully craft a novel benchmark from multiple question-answer datasets, and show the efficacy of our method in the LLM customization",
    "checked": true,
    "id": "4d8bfd68c9d1eb514cb55583a1ad1f6dce969083",
    "semantic_title": "crayon: customized on-device llm via instant adapter blending and edge-server hybrid inference",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=oeQDE3Q4bTY": {
    "title": "Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search",
    "volume": "review",
    "abstract": "In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally Rewrites the Code (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy across sparse (up to 35.7\\%), zero-shot dense (up to 27.6\\%), and fine-tuned dense (up to 23.6\\%) retrieval settings in diverse search scenarios. To further elucidate the advantages of ReCo and stimulate research in code style normalization, we introduce Code Style Similarity, the first metric tailored to quantify stylistic similarities in code. Notably, our empirical findings reveal the inadequacy of existing metrics in capturing stylistic nuances",
    "checked": true,
    "id": "3ddcd4d50468c9ede8887b8bf27898247128b3f9",
    "semantic_title": "rewriting the code: a simple method for large language model augmented code search",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gXhxZegD_a": {
    "title": "Making Large Language Models Perform Better in Knowledge Graph Completion",
    "volume": "review",
    "abstract": "Large language model (LLM) based knowledge graph completion (KGC) aims to predict the missing triples in the KGs with LLMs. However, research about LLM-based KGC fails to sufficiently harness LLMs' inference proficiencies, overlooking critical structural information integral to KGs. In this paper, we explore methods to incorporate structural information into the LLMs, with the overarching goal of facilitating structurally-aware reasoning. We propose a Knowledge Prefix Adapter (KoPA) to fulfill this stated goal. The KoPA uses a structural pre-training phase to comprehend the intricate relations and entities within KGs. Then KoPA communicates such structural understanding to the LLMs through a knowledge prefix adapter which projects the structural embeddings into the textual space and obtains virtual knowledge tokens positioned as a prefix of the input prompt. We conduct comprehensive experiments and provide incisive analysis concerning how the introduction of structural information would be better for LLM's knowledge reasoning ability. Our code and data are available at https://anonymous.4open.science/r/KoPA-0122",
    "checked": true,
    "id": "582c2f270a6c0ce89679eebaa78797711fa20293",
    "semantic_title": "making large language models perform better in knowledge graph completion",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=o3ZRLXPMAdM": {
    "title": "Cognitive Bias in High-Stakes Decision-Making with LLMs",
    "volume": "review",
    "abstract": "Large language models (LLMs) offer significant potential as tools to support an expanding range of decision-making tasks. However, given their training on human (created) data, LLMs can inherit both societal biases against protected groups, as well as be subject to cognitive bias. Such human-like bias can impede fair and explainable decisions made with LLM assistance. Our work introduces \\ours, a framework designed to uncover, evaluate, and mitigate cognitive bias in LLMs, particularly in high-stakes decision-making tasks. Inspired by prior research in psychology and cognitive sciences, we develop a dataset containing 16,800 prompts to evaluate different cognitive biases (e.g., prompt-induced, sequential, inherent). We test various bias mitigation strategies, amidst proposing a novel method utilising LLMs to debias their own prompts. Our analysis provides a comprehensive picture on the presence and effects of cognitive bias across different commercial and open-source models. We demonstrate that our self-help debiasing effectively mitigate cognitive bias without having to manually craft examples for each bias type",
    "checked": true,
    "id": "d0758435b0ee56e28eccb04fa2d429d37bea68b0",
    "semantic_title": "cognitive bias in high-stakes decision-making with llms",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=1m2Iz2klBnf": {
    "title": "CPUQ: Categorical Perplexity based Uncertainty Quantification with Language Models",
    "volume": "review",
    "abstract": "Agent-Based Modelling (ABM) for Economic Allocation (EA) analyzes interactions between economic agents and indicators over time, aiding policymakers and decision analysts in scenario analysis for complex systems like government spending or financial market contagion. These EA-ABMs, when graphed, often have limited datasets (timesteps) but a large number of nodes (agents or indicators) and edges (relationships), which hinders statistical network estimation methods. Additionally, statistical relationship estimation methods lack interpretability for non-technical users. To address these issues, we introduce the CPUQ framework, compatible with any Language Model (LM) and utilizing a LM's reasoning to generate predictive hurdle distributions that quantify relationship strength between agents/indicators, coupled with textual explanations for each prediction to enhance interpretability for non-technical audiences. CPUQ also includes a novel post-hoc calibration approach for network estimation. Evaluation on a real EA dataset demonstrates CPUQ's alignment with expert opinions and its superior forecasting capability over existing statistical and LM methods in assessing relationships in EA-ABMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tqD5zU6EeSB": {
    "title": "Grounding Language Model with Chunking-Free In-Context Retrieval",
    "volume": "review",
    "abstract": "This paper presents a novel Chunking-Free In-Context (CFIC) retrieval approach, specifically tailored for Retrieval-Augmented Generation (RAG) systems. Traditional RAG systems often struggle with grounding responses using precise evidence text due to the challenges of processing lengthy documents and filtering out irrelevant content. Commonly employed solutions, such as document chunking and adapting language models to handle longer contexts, have their limitations. These methods either disrupt the semantic coherence of the text or fail to effectively address the issues of noise and inaccuracy in evidence retrieval.The CFIC approach addresses these challenges by circumventing the conventional chunking process. It utilizes the encoded hidden states of documents for in-context retrieval, employing auto-aggressive decoding to accurately identify the specific evidence text required for user queries, eliminating the need for chunking. CFIC is further enhanced by incorporating two innovative decoding strategies, namely Constrained Sentence Prefix Decoding and Skip Decoding. These strategies not only improve the efficiency of the retrieval process but also ensure that the fidelity of the generated grounding text evidence is maintained.Our evaluations of CFIC on a range of open question answering datasets demonstrate its superiority in retrieving relevant and accurate information, offering a significant improvement over traditional methods. By doing away with the need for document chunking, CFIC presents a more streamlined, effective, and efficient retrieval solution, making it a valuable advancement in the field of RAG systems",
    "checked": true,
    "id": "84e6665a27552a46a18ae18833ebf7e6871d5cc3",
    "semantic_title": "grounding language model with chunking-free in-context retrieval",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=x6pTrW3y3pg": {
    "title": "RAP: Efficient Text-Video Retrieval with Sparse-and-Correlated Adapter",
    "volume": "review",
    "abstract": "Text-Video Retrieval (TVR) aims to align relevant video content with natural language queries. To date, most of the state-of-the-art TVR methods learn image-to-video transfer learning based on the large-scale pre-trained vision-language models (e.g., CLIP). However, fully fine-tuning these pre-trained models for TVR incurs prohibitively expensive computation cost. To this end, we propose to conduct efficient text-video Retrieval with a salient-and-correlated AdaPter (RAP), i.e., fine-tuning the pre-trained model with a few parameterized layers. To accommodate the text-video scenario, we equip our RAP with two indispensable characteristics including temporal sparsity and correlation. Specifically, we propose a low-rank modulation module to refine the per-image features from frozen CLIP backbone, which accentuates silent frames within the video features while alleviating temporal redundancy. Besides, we introduce an asynchronous self-attention mechanism which firstly selects top responsive visual patch and augments the correlation modeling between them with learnable temporal and patch offsets. Extensive experiments on four TVR datasets demonstrate that our RAP achieves superior or comparable performance compared to the fully fine-tuned counterpart and other parameter-efficient finetuning methods",
    "checked": true,
    "id": "8a5587454a7624e60021c3bfbbb95c138946eff7",
    "semantic_title": "rap: efficient text-video retrieval with sparse-and-correlated adapter",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=PB3sxZBivMN": {
    "title": "How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning",
    "volume": "review",
    "abstract": "We explore the mechanism of in-context learning and propose a hypothesis using locate-and-project method. In shallow layers, the features of demonstrations are merged into their corresponding labels, and the features of the input text are aggregated into the last token. In deep layers, in-context heads make great contributions. In each in-context head, the value-output matrix extracts the labels' features. Query and key matrices compute the attention weights between the input text and each demonstration. The larger the attention weight is, the more label information is transferred into the last token for predicting the next word. Query and key matrices can be regarded as two towers for learning the similarity metric between the input text and each demonstration. Based on this hypothesis, we explain why imbalanced labels and demonstration order affect predictions. We conduct experiments on GPT2 large, Llama 7B, 13B and 30B. The results can support our analysis. Overall, our study provides a new method and a reasonable hypothesis for understanding the mechanism of in-context learning. Our code will be released on github",
    "checked": true,
    "id": "46955794ab197d56b40595fcb8e74b6948097075",
    "semantic_title": "how do large language models learn in-context? query and key matrices of in-context heads are two towers for metric learning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=qFGdavU8c43": {
    "title": "Integrating Physician Diagnostic Logic into Large Language Models: Preference Learning from Process Feedback",
    "volume": "review",
    "abstract": "The utilization of large language models for medical dialogue generation has attracted considerable attention due to its potential to enhance response richness and coherence. While previous studies have made strides in optimizing model performance, there is a pressing need to bolster the model's capacity for diagnostic logic to ensure patient safety. In response to this need, we propose an approach termed preference learning from process feedback (PLPF), which involves integrating the doctor's diagnostic logic into LLMs. PLPF encompasses three key components: rule modeling, preference data generation, and preference alignment. These components collectively serve to train the model to adhere to the diagnostic process. Our experimental results, utilizing Standardized Patient Testing, demonstrate that PLPF enhances the diagnostic accuracy of the baseline model in medical conversations by 17.6%, surpassing the performance of traditional approaches. Moreover, PLPF exhibits effectiveness in both multi-round and single-round dialogue tasks, thereby highlighting its potential in improving medical dialogue generation",
    "checked": true,
    "id": "dabf7edde0efb9b1e092aa27847a547cf2961192",
    "semantic_title": "integrating physician diagnostic logic into large language models: preference learning from process feedback",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=HkZPu82lM5e": {
    "title": "Compress to Impress: Unleashing the Potential of Compressive Memory in Real-World Long-Term Conversations",
    "volume": "review",
    "abstract": "Existing retrieval-based methods have made significant strides in maintaining long-term conversations. However, these approaches face challenges in memory database management and accurate memory retrieval, hindering their efficacy in dynamic, real-world interactions. This study introduces a novel framework, COmpressive Memory-Enhanced Dialogue sYstems (COMEDY), which eschews traditional retrieval modules and memory databases. Instead, COMEDYadopts a \"One-for-All\" approach, utilizing a single language model to manage memory generation, compression, and response generation. Central to this framework is the concept of \"compressive memory\", which integrates session-specific summaries, user-bot dynamics, and past events into a concise memory format. To support COMEDY, we curated a large-scale Chinese instruction-tuning dataset, Dolphin, derived from real user-chatbot interactions. Comparative evaluations demonstrate COMEDY's superiority over traditional retrieval-based methods in producing more nuanced and human-like conversational experiences",
    "checked": true,
    "id": "d4ae96230a443974ed2bfebbfe5ac22be336dbe8",
    "semantic_title": "compress to impress: unleashing the potential of compressive memory in real-world long-term conversations",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=p4n-YV8d3w": {
    "title": "CausalCite: A Causal Formulation of Paper Citations",
    "volume": "review",
    "abstract": "Evaluating the significance of a paper is pivotal yet challenging for the scientific community. While the citation count is the most commonly used proxy for this purpose, they are widely criticized for failing to accurately reflect a paper's true impact. In this work, we propose a causal inference method, TextMatch, which adapts the traditional matching framework to high-dimensional text embeddings. Specifically, we encode each paper using the text embeddings by large language models, extract similar samples by cosine similarity, and synthesize a counterfactual sample by the weighted average of similar papers according to their similarity values. We apply the resulting metric, called CausalCite, as a causal formulation of paper citations. We show its effectiveness on various criteria, such as high correlation with paper impact as reported by scientific experts on a previous dataset of 1K papers, and test-of-time awards for past papers, and sub-field stable behavior. We also provide a set of findings that can serve as suggested ways for future researchers to use our metric for a better understanding of a paper's quality",
    "checked": true,
    "id": "bae9c39adf7070ef37ae989bc8716a88691c33f2",
    "semantic_title": "causalcite: a causal formulation of paper citations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lCbLur9H87n": {
    "title": "Generating Contrastive Narratives Using the Brownian Bridge Process for Narrative Coherence Learning",
    "volume": "review",
    "abstract": "A major challenge for narrative reasoning is to learn narrative coherence. Existing works mainly follow the contrastive learning paradigm. However, the negative samples in their methods can be easily distinguished, which makes their methods unsatisfactory. In this work, we devise two strategies for mining hard negatives, including (1) crisscrossing a narrative and its contrastive variants; and (2) event-level replacement. To obtain contrastive variants, we utilize the Brownian Bridge process to guarantee the quality of generated contrastive narratives. We evaluate our model on several tasks. The result proves the effectiveness of our method, and shows that our method is applicable to many applications",
    "checked": true,
    "id": "bec3ec74bd0ebb068364a796bee2df0014f0cb02",
    "semantic_title": "generating contrastive narratives using the brownian bridge process for narrative coherence learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QWruHR5hCN": {
    "title": "Empowering Character-level Text Infilling by Eliminating Sub-Tokens",
    "volume": "review",
    "abstract": "In infilling tasks, sub-tokens, representing instances where a complete token is segmented into two parts, often emerge at the boundaries of prefixes, middles, and suffixes. Traditional methods focused on training models at the token level, leading to sub-optimal performance in character-level infilling tasks during the inference stage. Alternately, some approaches considered character-level infilling but they relied on predicting sub-tokens in inference, yet this strategy diminished ability in character-level infilling tasks due to the large perplexity of the model on sub-tokens. In this paper, we introduce FIM-SE, which stands for Fill-In-the-Middle with both Starting and Ending character constraints. The proposed method addresses character-level infilling tasks by utilizing a line-level format to avoid predicting any sub-token in inference. In addition, we incorporate two special tokens to signify the rest incomplete lines, thereby enhancing generation guidance. Extensive experiments demonstrate that our proposed approach surpasses previous methods, offering a significant advantage",
    "checked": true,
    "id": "1f814aba998e701a6a515dc2bbe9e1f3e3949481",
    "semantic_title": "empowering character-level text infilling by eliminating sub-tokens",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=agLKmCO0EFs": {
    "title": "LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs",
    "volume": "review",
    "abstract": "Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts from new KGs that are not seen during training, has been widely adopted in various applications. One critical challenge of KG inductive reasoning is handling low-resource scenarios with scarcity in both textual and structural aspects. In this paper, we attempt to address this challenge with Large Language Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to generate a graph-structural prompt to enhance the pre-trained Graph Neural Networks (GNNs), which brings us new methodological insights into the KG inductive reasoning methods, as well as high generalizability in practice. On the methodological side, we introduce a novel pretraining and prompting framework ProLINK, designed for low-resource inductive reasoning across arbitrary KGs without requiring additional training. On the practical side, we experimentally evaluate our approach on 36 low-resource KG datasets and find that ProLINK outperforms previous methods in three-shot, one-shot, and zero-shot reasoning tasks, exhibiting {average} performance improvements by 20%, 45%, and 147%, respectively. Furthermore, ProLINK demonstrates strong robustness for various LLM promptings as well as full-shot scenarios",
    "checked": true,
    "id": "914d7562bcfe3f3546d94687839de8c0edc5866e",
    "semantic_title": "llm as prompter: low-resource inductive reasoning on arbitrary knowledge graphs",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=GhzNDVXgGX4": {
    "title": "Language Models Understand Numbers, at Least Partially",
    "volume": "review",
    "abstract": "Large language models (LLMs) have exhibited impressive competence in various tasks, but their opaque internal mechanisms hinder their use in mathematical problems.In this paper, we study a fundamental question: whether language models understand numbers, a basic element in math.Based on an assumption that LLMs should be capable of compressing numbers in their hidden states to solve mathematical problems, we construct a synthetic dataset comprising addition problems and utilize linear probes to read out input numbers from the hidden states.Experimental results support the existence of compressed numbers in LLMs.However, it is difficult to precisely reconstruct the original numbers, indicating that the compression process may not be lossless.Further experiments show that LLMs can utilize encoded numbers to perform arithmetic computations, and the computational ability scales up with the model size.Our preliminary research suggests that LLMs exhibit a partial understanding of numbers, offering insights for future investigations about the models' mathematical capability",
    "checked": false,
    "id": "90b5791db80f04342f935f20e7a9dc16b082a704",
    "semantic_title": "the study of models and the understanding of reality",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rhSOxKYq89O": {
    "title": "CoCA: Fusing Position Embedding with Collinear Constrained Attention in Transformers for Long Context Window Extending",
    "volume": "review",
    "abstract": "Self-attention and position embedding are two key modules in transformer-based Large Language Models (LLMs). However, the potential relationship between them is far from well studied, especially for long context window extending. In fact, anomalous behaviors harming long context extrapolation exist between Rotary Position Embedding (RoPE) and vanilla self-attention unveiled by our work. To address this issue, we propose a novel attention mechanism, CoCA (\\textbf{Co}llinear \\textbf{C}onstrained \\textbf{A}ttention). Specifically, we enforce a collinear constraint between $Q$ and $K$ to seamlessly integrate RoPE and self-attention. While only adding minimal computational and spatial complexity, this integration significantly enhances long context window extrapolation ability. We provide an optimized implementation, making it a drop-in replacement for any existing transformer-based models. Extensive experiments show that CoCA performs extraordinarily well in extending context windows. A CoCA-based GPT model, trained with a context length of 512, can seamlessly extend the context window up to 32K (60$\\times$), without any fine-tuning. Additionally, by dropping CoCA in LLaMA-7B, we achieve extrapolation up to 32K within only 2K training length. Our code is publicly available at: https://github.com/****/****",
    "checked": true,
    "id": "d599905b8966e5d735ae024ffe2768e92cd2822e",
    "semantic_title": "coca: fusing position embedding with collinear constrained attention in transformers for long context window extending",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=lKbx4trnc9F": {
    "title": "CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges",
    "volume": "review",
    "abstract": "Large Language Models (LLMs) have shown promise in automated code generation but typically excel only in simpler tasks such as generating standalone code units. However, real-world software development often involves complex code repositories with complex dependencies and extensive documentation. To enable LLMs to handle these realworld repo-level code generation, we present CodeAgent, a novel LLM-based agent framework that employs external tools for effective repo-level code generation. CodeAgent integrates five programming tools, enabling interaction with software artifacts for information retrieval, code implementation, and code testing. We implement four agent strategies to optimize these tools' usage. To the best of our knowledge, CodeAgent is the first agent tool framework specifically for repo-level code generation. In order to measure the effectiveness of our method at the repository level, we have introduced a benchmark dataset CodAgentBench. The performance on this dataset shows a significant improvement brought by our method, with improvements of pass rate ranging from 2.0 to 15.8. Further tests on the HumanEval benchmark confirm CodeAgent's adaptability and efficacy across various code generation tasks. Notably, CodeAgent outperforms commercial products like Github Copilot, showcasing superior accuracy and efficiency. These results demonstrate CodeAgent's robust capabilities in code generation, highlighting its potential for real-world repo-level coding challenges",
    "checked": true,
    "id": "3793a5f435fef59a901f5ba0d8ef43df88d97161",
    "semantic_title": "codeagent: enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=VqzuhebU34": {
    "title": "Evaluating and Enhancing Large Language Models for Conversational Reasoning on Knowledge Graphs",
    "volume": "review",
    "abstract": "The development of large language models (LLMs) has been catalyzed by advancements in pre-training techniques. These models have demonstrated robust reasoning capabilities through manually designed prompts. In this work, we evaluate the conversational reasoning capabilities of the current state-of-the-art LLM (GPT-4) on knowledge graphs (KGs). However, the performance of LLMs is constrained due to a lack of KG environment awareness and the difficulties in developing effective optimization mechanisms for intermediary reasoning stages. We further introduce LLM-ARK, a LLM grounded KG reasoning agent designed to deliver precise and adaptable predictions on KG paths. LLM-ARK leverages Full Textual Environment (FTE) prompt to assimilate state information within each reasoning step. We reframe the challenge of multi-hop reasoning on the KG as a sequential decision-making task. Utilizing the Proximal Policy Optimization (PPO) online policy gradient reinforcement learning algorithm, our model is optimized to learn from rich reward signals. Additionally, we conduct an evaluation of our model and GPT-4 on the OpenDialKG dataset. The experimental results reveal that LLaMA7B-ARK outperforms the current state-of-the-art model by 5.28 percentage points, with a performance rate of 36.39\\% on the target@1 evaluation metric. Meanwhile, GPT-4 scored 14.91\\%, further demonstrating the effectiveness of our methodology",
    "checked": true,
    "id": "a5817e982024a360922f0919e8deee66635ac3d3",
    "semantic_title": "evaluating and enhancing large language models for conversational reasoning on knowledge graphs",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jL_SeS_4sv": {
    "title": "Ploutos: Towards interpretable stock movement prediction with financial large language model",
    "volume": "review",
    "abstract": "Recent advancements in large language models (LLMs) have opened new pathways for many domains. However, the full potential of LLMs in financial investments remains largely untapped. There are two main challenges for typical deep learning-based methods for quantitative finance. First, they struggle to fuse textual and numerical information flexibly for stock movement prediction. Second, traditional methods lack clarity and interpretability, which impedes their application in scenarios where the justification for predictions is essential. To solve the above challenges, we propose Ploutos, a novel financial LLM framework that consists of PloutosGen and PloutosGPT. The PloutosGen contains multiple primary experts that can analyze different modal data, such as text and numbers, and provide quantitative strategies from different perspectives. Then PloutosGPT combines their insights and predictions and generates interpretable rationales. To generate accurate and faithful rationales, the training strategy of PloutosGPT leverage rearview-mirror prompting mechanism to guide GPT-4 to generate rationales, and a dynamic token weighting mechanism to finetune LLM by increasing key tokens weight. Extensive experiments show our framework outperforms the state-of-the-art methods on both prediction accuracy and interpretability",
    "checked": true,
    "id": "fc4968617eae1d875a77ed0372be8f2e6118440a",
    "semantic_title": "ploutos: towards interpretable stock movement prediction with financial large language model",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=GiAjIXnayrN": {
    "title": "Large Language Models are Null-Shot Learners",
    "volume": "review",
    "abstract": "This paper presents null-shot prompting. Null-shot prompting exploits hallucination in large language models (LLMs) by instructing LLMs to utilize information from the \"Examples\" section that never exists within the provided context to perform a task. While reducing hallucination is crucial and non-negligible for daily and critical uses of LLMs, we propose that in the current landscape in which these LLMs still hallucinate, it is possible, in fact, to exploit hallucination to increase performance in performing tasks compared to standard zero-shot prompting. Experiments with eight LLMs show improvements in performance across the majority of eight datasets, including reading comprehension, arithmetic reasoning, and closed-book question answering. The observed inconsistency in increased relative performance across the LLMs also potentially indicates a different degree of inherent hallucination in each model. These differences show that it is possible to utilize null-shot prompting as a way to detect degrees of hallucination in LLMs using existing benchmarking datasets. We also perform ablation studies, including experimenting with a modified version of null-shot prompting that incorporates ideas from zero-shot chain-of-thought prompting, which shows different trends of results",
    "checked": true,
    "id": "d16f8d624ab16c8bb35dde676f522b66a771d271",
    "semantic_title": "large language models are null-shot learners",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=H2pNHtB-59": {
    "title": "Are LLMs Effective Backbones for Fine-tuning? An Experimental Investigation of Supervised LLMs on Chinese Short Text Matching",
    "volume": "review",
    "abstract": "The recent success of Large Language Models (LLMs) has garnered significant attention in both academia and industry. Prior research on LLMs has primarily focused on enhancing or leveraging their generalization capabilities in zero- and few-shot settings. However, there has been limited investigation into effectively fine-tuning LLMs for a specific natural language understanding task in supervised settings. In this study, we conduct an experimental analysis by fine-tuning LLMs for the task of Chinese short text matching. We explore various factors that influence performance when fine-tuning LLMs, including task modeling methods, prompt formats, and output formats",
    "checked": true,
    "id": "e4d40e18e4ee908dab0f574064885b70b07fcb23",
    "semantic_title": "are llms effective backbones for fine-tuning? an experimental investigation of supervised llms on chinese short text matching",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJcUGHVpNYL": {
    "title": "Calibrated Offline Knowledge Distillation for Large Language Model Training",
    "volume": "review",
    "abstract": "Knowledge Distillation (KD) in large language models (LLMs) which involves training a small model to mimic the behaviour of a large model by matching their output distribution, has shown remarkable improvement in performance and efficiency over standard fine-tuning.Despite the great success of these methods, distilled student models are still suffering from catastrophic mis-calibration due to the over-confident nature of the teacher model.In this paper, we present a comprehensive study on the importance and necessity of re-calibration during soft-label-based distillation. We further propose a soft-label-based Calibrated Offline Knowledge Distillation (COD) pipeline that can effectively determine to what extent different token probability should be reduced or raised, resulting in a consistent distillation of a reliable model. Specifically, we start by re-calibrating the token probability distribution generated by the teacher model, by reducing the probability of over-confident tokens and raising the under-confident ones. Then we train a student model to fit the calibrated distribution. We conduct extensive experiments on both in-domain and out-of-domain settings by comparing re-calibrated distillation with non-calibrated distillation and standard fin-tuning over three popular open-sourced language model family (Llama-1, Llama-2, and OpenLlama). Experimental results demonstrate that re-calibration before distillation can greatly improve the reliability of the model (by 4.3\\% expected calibration error on average) and generally further boost the downstream performance (by 2.5\\% accuracy on average)",
    "checked": false,
    "id": "30ad1ae844ad40b3412e3de361a57bf7ddad100e",
    "semantic_title": "o3d: offline data-driven discovery and distillation for sequential decision-making with large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AlGBigCU9j": {
    "title": "CodeM: Less Data Yields More Versatility via Ability Matrix",
    "volume": "review",
    "abstract": "In the era of code large language models (code LLMs), data engineering plays a pivotal role during the instruction fine-tuning phase. To train a versatile model, previous efforts devote tremendous efforts into crafting instruction data covering all the downstream scenarios. Nonetheless, this will incur significant expenses in constructing data and training model. Therefore, this paper introduces CodeM, a novel data construction strategy, which can efficiently train a versatile model using less data via our newly proposed ability matrix. CodeM uses ability matrix to decouple code LLMs' abilities into two dimensions, constructing a lightweight training corpus that only covers a subset of target scenarios. Extensive experiments on HumanEvalPack and MultiPL-E imply that code LLMs can combine the single-dimensional abilities to master composed abilities, validating the effectiveness of CodeM",
    "checked": false,
    "id": "58cc9ccffaee81913562f4e0aa4b2ccc139fdde3",
    "semantic_title": "48 th colloquium & annual general meeting 2022 geoscience",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GAdeWD0igdx": {
    "title": "See the Unseen: Better Context-Consistent Knowledge-Editing via Noises",
    "volume": "review",
    "abstract": "Knowledge-editing updates knowledge of large language models (LLMs) and contributes to the interpretability and application of LLMs. However, knowledge applying is context-consistent: LLMs can recall the same knowledge in different contexts. Existing works ignore this property and the editing lacks generalization. Based on empirical evidence, we have observed that the effect of different contexts in recalling the same knowledge follows a Gaussian-like distribution. Hence, when editing LLMs, we sample Gaussian noises to simulate the effect of different contexts rather than requiring real contexts. We make LLMs see the unseen contexts where edited knowledge will be applied, thereby improving editing generalization. Experimental results on three LLMs demonstrate the effectiveness of our method and distinguish ours from the others of fine-tuning LLMs via noises",
    "checked": false,
    "id": "098da337aa9521aae80cbaab3572b75857cd911c",
    "semantic_title": "see the unseen: better context-consistent knowledge-editing by noises",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=L3F6aTb5hr": {
    "title": "Speak Like a Native: Prompting Large Language Models in a Native Style",
    "volume": "review",
    "abstract": "In-context learning (ICL) with large language models (LLMs) has become the modern tool of choice for many natural language processing tasks. However, how the text style of in-context examples influences the performance of LLMs still remains under-explored. This paper presents a novel and effective approach, named \\textbf{AlignedCoT}, to improve the reasoning capability of LLMs by aligning the in-context examples with the native style of LLMs. ``Native'' refers to the inherent characteristic of LLMs which can be probed by zero-shot scenarios. We conduct extensive and comprehensive experiments on several benchmarks on mathematical question-answering and common-sense reasoning. The empirical results demonstrate that our AlignedCoT significantly improves performance over the carefully handcrafted demonstrations. Specifically, with AlignedCoT, we observe an average +3.2\\% improvement for \\texttt{gpt-3.5-turbo} compared to the carefully handcrafted CoT on multi-step reasoning benchmarks. Furthermore, we use AlignedCoT to rewrite the CoT text style in the training set, which improves the performance of Retrieval Augmented Generation by 3.6\\%",
    "checked": true,
    "id": "9690eda895336c02db338ffacb6a90c5bdeb5263",
    "semantic_title": "speak like a native: prompting large language models in a native style",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=2-EUjMtmhqX": {
    "title": "Don't Be Overconfident When You're Wrong; Don't Be Underconfident When You're Right",
    "volume": "review",
    "abstract": "When responding to any question from the user or an API,a conversational search or question answering systemshould ideally be able to attach an appropriate confidence score to its output.While such systems are often overconfident,there are also situations where the system responds correctly yet lacks enough confidence.Underconfident responses cannot be relied upon, and therefore may not be utilised by the user or downstream tasks.Ideally, we want to know when systems are underconfident as well as when they are overconfident,and want to suppress both phenomena in a balanced manner.Furthermore, in this scenario,we want an evaluation measure that is guaranteed to(a) penalise a lowered confidence for a correct response; and also(b) penalise a raised confidence for an incorrect response.In light of this,we propose HMR (Harmonic Mean of Rewards)and demonstrate its advantages over existing calibration measures for our purposeby means of examples, axioms, and theorems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6n7143SBD7": {
    "title": "Holistic Analysis of Hallucination in Large Vision-Language Models: Bias and Interference Challenges",
    "volume": "review",
    "abstract": "While GPT-4V(ision) impressively models both visual and textual information simultaneously, it's hallucination behavior has not been systematically assessed. To bridge this gap, we introduce a new benchmark, namely, the Bias and Interference Challenges in Visual Language Models (Bingo). This benchmark is designed to evaluate and shed light on the two common types of hallucinations in visual language models: bias and interference. Here, bias refers to the model's tendency to hallucinate certain types of responses, possibly due to imbalance in its training data. Interference pertains to scenarios where the judgment of GPT-4V(ision) can be disrupted due to how the text prompt is phrased or how the input image is presented. We identify a notable regional bias, whereby GPT-4V(ision) is better at interpreting Western images or images with English writing compared to images from other countries or containing text in other languages. Moreover, GPT-4V(ision) is vulnerable to leading questions and is often confused when interpreting multiple images together. Popular mitigation approaches, such as self-correction and chain-of-thought reasoning, are not effective in resolving these challenges. We also identified similar biases and interference vulnerabilities with LLaVA and Bard. Our results characterize the hallucination challenges in GPT-4V(ision) and state-of-the-art visual-language models, and highlight the need for new solutions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rvEFynYX3E": {
    "title": "Investigating Human Values in Online Communities",
    "volume": "review",
    "abstract": "Social media platforms offer an invaluable data source for studying diverse aspects of human society, providing direct and expansive access to human expressions. This paper addresses the limitations of traditional survey-based studies of human values by proposing a computational application of Schwartz's values framework to Reddit, a platform organized into distinct online communities. After ensuring the reliability of automated value extraction tools for Reddit content, we automatically annotate 6 million posts across 10,000 subreddits with Schwartz values. Our analysis unveils both previously recorded and novel insights into the values prevalent within various online communities. For instance, when examining subreddits with differing opinions on controversial topics, we discover higher universalism values in the Vegan subreddit compared to Carnivores. Additionally, our study of geographically specific subreddits highlights the connection between traditional values and conservative U.S. states",
    "checked": true,
    "id": "054de13a92ff1d696710177554aaddcc2d346d93",
    "semantic_title": "investigating human values in online communities",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ehHgeyZ2q-": {
    "title": "Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale",
    "volume": "review",
    "abstract": "A syntactic language model (SLM) incrementally generates a sentence with its syntactic tree in a left-to-right manner.We present Generative Pretrained Structured Transformers (GPST), an unsupervised SLM at scale capable of being pre-trained from scratch on raw texts with high parallelism. GPST circumvents the limitations of previous SLMs such as relying on gold trees and sequential training. It consists of two components, a usual SLM supervised by a uni-directional language modeling loss, and an additional composition model, which induces syntactic parse trees and computes constituent representations, supervised by a bi-directional language modeling loss. We propose a representation surrogate to enable joint parallel training of the two models in a hard-EM fashion.We pre-train GPST on OpenWebText, a corpus with $9$ billion tokens, and demonstrate the superiority of GPST over GPT-2 with a comparable size in numerous tasks covering both language understanding and language generation. Meanwhile, GPST also significantly outperforms existing unsupervised SLMs on left-to-right grammar induction, while holding a substantial acceleration on training",
    "checked": true,
    "id": "b2d838a70b44b7098e2f755fdfe6e9e5f3c1ed86",
    "semantic_title": "generative pretrained structured transformers: unsupervised syntactic language models at scale",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=eStYegKOXFz": {
    "title": "Chinese Sentence Paraphrasing",
    "volume": "review",
    "abstract": "Sentence paraphrasing involves understanding the semantics and generating alternative expressions that are equivalent to the original sentence but not identical. However, there lack of an evaluation metric for paraphrasing that aligns well with human annotation and a lack of high-quality Chinese paraphrase datasets which makes it difficult to train a Chinese paraphrase model. To address these challenges, we present the first large-scale automatically constructed Chinese sentence paraphrase corpus, consisting of 9.45 million annotated sentence pairs for paraphrasing. We also introduce a core dataset with 2.5 thousand Chinese sentence pairs that are completely paraphrased by the crowd and annotated by experts. With this high-quality data, we establish an automatic evaluation metric for Chinese paraphrasing evaluation, achieving a Spearman coefficient of 0.726 in human-annotated data and significantly outperforming existing metrics. Additionally, we build a strong baseline for Chinese paraphrasing generation with few entity and logical errors while preserving the meaning of the sentence and generating diverse and innovative sentences",
    "checked": false,
    "id": "a0ed3adda058ea3f29dc4224c44c1e19f07c2e57",
    "semantic_title": "chinese idiom paraphrasing",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=MuCTKtIj8O": {
    "title": "Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models",
    "volume": "review",
    "abstract": "Recently, retrieval augmentation and tool augmentation have demonstrated a remarkable capability to expand the internal memory boundaries of language models (LMs) by providing external context. However, internal memory and external context inevitably clash, leading to knowledge conflicts within LMs. In this paper, we aim to interpret the mechanism of knowledge conflicts through the lens of information flow, and then mitigate conflicts by precise interventions at the pivotal point. We find there are some attention heads with opposite effects in the later layers, where memory heads can recall knowledge from internal memory, and context heads can retrieve knowledge from external context. Therefore, we reveal that the pivotal point at which knowledge conflicts emerge in LMs is the integration of inconsistent information flows by memory heads and context heads. Inspired by our insights, we propose a novel method called Pruning Head via PatH PatcHing (PH3), which can efficiently mitigate knowledge conflicts by pruning conflicting attention heads without updating model parameters. PH3 can flexibly control eight LMs to use internal memory ($\\uparrow$ 44.0%) or external context ($\\uparrow$ 38.5%). Moreover, PH3 can also improve the performance of LMs on open-domain QA tasks. We also conduct extensive experiments to demonstrate the cross-model, cross-relation, and cross-format generalization of our method",
    "checked": true,
    "id": "4c0a545b1eb73084e3d395435af9cf6649c2b1da",
    "semantic_title": "cutting off the head ends the conflict: a mechanism for interpreting and mitigating knowledge conflicts in language models",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=xYTmtVK83Xq": {
    "title": "LoRA Meets Dropout under a Unified Framework",
    "volume": "review",
    "abstract": "With the remarkable capabilities, large language models (LLMs) have emerged as essential elements in numerous NLP applications, while parameter-efficient finetuning, especially LoRA, has gained popularity as a lightweight approach for model customization. Meanwhile, various dropout methods, initially designed for full finetuning with all the parameters updated, alleviates overfitting associated with excessive parameter redundancy. Hence, a possible contradiction arises from negligible trainable parameters of LoRA and the effectiveness of previous dropout methods, which has been largely overlooked. To fill this gap, we first confirm that parameter-efficient LoRA is also overfitting-prone. We then revisit transformer-specific dropout methods, and establish their equivalence and distinctions mathematically and empirically. Building upon this comparative analysis, we introduce a unified framework for a comprehensive investigation, which instantiates these methods based on dropping position, structural pattern and compensation measure. Through this framework, we reveal the new preferences and performance comparisons of them when involved with limited trainable parameters. This framework also allows us to amalgamate the most favorable aspects into a novel dropout method named HiddenKey. Extensive experiments verify the remarkable superiority and sufficiency of HiddenKey across multiple models and tasks, which highlights it as the preferred approach for high-performance and parameter-efficient finetuning of LLMs",
    "checked": true,
    "id": "034b4d52040db023e912768b7ef7eafb88e72a3e",
    "semantic_title": "lora meets dropout under a unified framework",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=zLnt4945ftX": {
    "title": "A Causal Approach for Counterfactual Reasoning in Narratives",
    "volume": "review",
    "abstract": "Counterfactual reasoning in narratives requires predicting how alternative conditions, contrary to what actually happened, might have resulted in different outcomes.One major challenge is to maintain the causality between the counterfactual condition and the generated counterfactual outcome. In this paper, we propose a basic VAE module for counterfactual reasoning in narratives. We further introduce a pre-trained classifier and external event commonsense to mitigate the model collapse problem in the VAE approach, and improve the causality between the counterfactual condition and the generated counterfactual outcome. We evaluate our method on two public benchmarks. Experiments show that our method is effective",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aUhtqE1CNx": {
    "title": "Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector",
    "volume": "review",
    "abstract": "Chinese grammatical error correction (CGEC) faces serious overcorrection challenges when employing autoregressive generative models such as sequence-to-sequence (Seq2Seq) models and decoder-only large language models (LLMs). While previous methods aim to address overcorrection in Seq2Seq models, they are difficult to adapt to decoder-only LLMs. In this paper, we propose an alignment-enhanced corrector for the overcorrection problem that applies to both Seq2Seq models and decoder-only LLMs. Our method first trains a correction model to generate an initial correction of the source sentence. Then, we combine the source sentence with the initial correction and feed it through an alignment model for another round of correction, aiming to enforce the alignment model to focus on potential overcorrection. Moreover, to enhance the model's ability to identify nuances, we further explore the reverse alignment of the source sentence and the initial correction. Finally, we transfer the alignment knowledge from two alignment models to the correction model, instructing it on how to avoid overcorrection. Experimental results on three CGEC datasets demonstrate the effectiveness of our approach in alleviating overcorrection and improving overall performance",
    "checked": true,
    "id": "025c52b538696ab13523a2bbb2b74d185a1dcf5d",
    "semantic_title": "alirector: alignment-enhanced chinese grammatical error corrector",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bixS_Tm2RSk": {
    "title": "Rethinking the Instruction Quality: LIFT is What You Need",
    "volume": "review",
    "abstract": "Instruction tuning, a specialized technique to enhance large language model (LLM) performance via instruction datasets, relies heavily on the quality of the employed data. Existing quality improvement methods alter instruction data through dataset expansion or curation. However, the expansion method introduces the risk of data deficiency and redundancy, potentially compromising the correctness and accuracy of the LLM's knowledge, while the curation approach confines the LLM's potential to the original dataset. Our aim is to surpass the original data quality without confronting these shortcomings. To achieve this, we propose LIFT (LLM Instruction Fusion Transfer), a novel and versatile paradigm designed to elevate the instruction quality to new heights. LIFT strategically broadens data distribution to encompass more high-quality subspaces and eliminates redundancy, concentrating on high-quality segments across overall data subspaces. Experimental results demonstrate that, even with a limited quantity of high-quality instruction data selected by our paradigm, LLMs not only consistently uphold robust performance across natural language understanding and code generation tasks but also surpass many state-of-the-art results, highlighting the significant improvement in instruction quality achieved by our paradigm",
    "checked": true,
    "id": "83d12fa58743015f8abe4097fb58088f2d13c7f0",
    "semantic_title": "rethinking the instruction quality: lift is what you need",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=yYDf8nUmtid": {
    "title": "SemAug: Shaping the Future of Semantically-Enriched, Format-Specific Data Augmentation",
    "volume": "review",
    "abstract": "In the realm of artificial intelligence, the significance of high-quality data cannot be overstated, especially data that adheres to stringent formatting rules and structures. Addressing this need, our study introduces an advanced data augmentation method specifically designed for format-specific datasets. This method utilizes the capabilities of Large Language Models (LLMs) to generate data that not only meets the rigid formatting criteria but also maintains the integrity of the information. Central to our approach is the integration of specific format requirements into natural language prompts, which guides the LLMs to produce precisely formatted outputs. A salient feature of our approach is its self-evaluative mechanism, which autonomously assesses the semantic quality of the augmented data, distinguishing it from prior methodologies that require manual validation, thereby streamlining the augmentation process. Our research represents a pioneering step forward, enabling more efficient enhancement of datasets that demand exacting format adherence without the extensive resource investment typically associated with such tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7SKY9HEJS9": {
    "title": "FSMR: A Feature Swapping Multi-modal Reasoning Approach with Joint Textual and Visual Clues",
    "volume": "review",
    "abstract": "Multi-modal reasoning plays a vital role in bridging the gap between textual and visual information, enabling a deeper understanding of the context. This paper presents the Feature Swapping Multi-modal Reasoning (FSMR) model, designed to enhance multi-modal reasoning through feature swapping. FSMR leverages a pre-trained visual-language model as an encoder, accommodating both text and image inputs for effective feature representation from both modalities. It introduces a unique feature swapping module, enabling the exchange of features between identified objects in images and corresponding vocabulary words in text, thereby enhancing the model's comprehension of the interplay between images and text. To further bolster its multi-modal alignment capabilities, FSMR incorporates a multi-modal cross-attention mechanism, facilitating the joint modeling of textual and visual information. During training, we employ image-text matching and cross-entropy losses to ensure semantic consistency between visual and language elements. Extensive experiments on the PMR dataset demonstrate FSMR's superiority over state-of-the-art baseline models across various performance metrics",
    "checked": true,
    "id": "85a25999a1b28aaaaac4cf9eb35e41ebdd73cfbf",
    "semantic_title": "fsmr: a feature swapping multi-modal reasoning approach with joint textual and visual clues",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ceGBmj1dtaL": {
    "title": "Language-based Valence and Arousal Expressions between the United States and China: a Cross-Cultural Examination",
    "volume": "review",
    "abstract": "Although affective expressions of individuals have been extensively studied using social media, research has primarily focused on the Western context. There are substantial differences among cultures that contribute to their affective expressions. This paper examines the differences between Twitter (X) in the United States and Sina Weibo posts in China on two primary dimensions of affect - valence and arousal. We study the difference in the functional relationship between arousal and valence (so-called V-shaped) among individuals in the US and China and explore the associated content differences. Furthermore, we correlate word usage and topics in both platforms to interpret their differences. We observe that for Twitter users, the variation in emotional intensity is less distinct between negative and positive emotions compared to Weibo users, and there is a sharper escalation in arousal corresponding with heightened emotions. From language features, we discover that affective expressions are associated with personal life and feelings on Twitter, while on Weibo such discussions are about socio-political topics in the society. These results suggest a West-East difference in the V-shaped relationship between valence and arousal of affective expressions on social media influenced by content differences. Our findings have implications for applications and theories related to cultural differences in affective expressions",
    "checked": true,
    "id": "a24c3f7406cb045bcbb5c926b68194d8dcc601d4",
    "semantic_title": "language-based valence and arousal expressions between the united states and china: a cross-cultural examination",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bt6efZOZwxm": {
    "title": "Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models",
    "volume": "review",
    "abstract": "Ensuring the trustworthiness of large language models (LLMs) is crucial. Most studies concentrate on fully pre-trained LLMs to better understand and improve LLMs' trustworthiness. In this paper, to reveal the untapped potential of pre-training, we pioneer the exploration of LLMs' trustworthiness during this period, focusing on five key dimensions: reliability, privacy, toxicity, fairness, and robustness. To begin with, we apply linear probing to LLMs. The high probing accuracy suggests that \\textit{LLMs in early pre-training can already distinguish concepts in each trustworthiness dimension}. Therefore, to further uncover the hidden possibilities of pre-training, we extract steering vectors from a LLM's pre-training checkpoints to enhance the LLM's trustworthiness. Finally, inspired by the theoretical result that mutual information estimation is bounded by linear probing accuracy, we also probe LLMs with mutual information to investigate the dynamics of trustworthiness during pre-training. We are the first to observe a similar two-phase phenomenon: fitting and compression. This research provides an initial exploration of trustworthiness modeling during LLM pre-training, seeking to unveil new insights and spur further developments in the field",
    "checked": true,
    "id": "c7137aa84b5a48bda9a96432524f9948e8c823e0",
    "semantic_title": "towards tracing trustworthiness dynamics: revisiting pre-training period of large language models",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=biDrS_uKcSa": {
    "title": "Grimoire is All You Need for Enhancing Large Language Models",
    "volume": "review",
    "abstract": "In-context learning (ICL) is one of the key methods for enhancing the performance of large language models on specific tasks by providing a set of few-shot examples. However, the ICL capability of different types of models shows significant variation due to factors such as model architecture, volume of learning data, and the size of parameters. Generally, the larger the model's parameter size and the more extensive the learning data, the stronger its ICL capability. In this paper, we propose a method SLEICL that involves learning from examples using strong language models and then summarizing and transferring these learned skills to weak language models for inference and application. This ensures the stability and effectiveness of ICL. Compared to directly enabling weak language models to learn from prompt examples, SLEICL reduces the difficulty of ICL for these models. Our experiments, conducted on up to eight datasets with five language models, demonstrate that weak language models achieve consistent improvement over their own zero-shot or few-shot capabilities using the SLEICL method. Some weak language models even surpass the performance of GPT4-1106-preview (zero-shot) with the aid of SLEICL",
    "checked": true,
    "id": "3a6e67c28c9563c187d93b57cff1ca41ec4c245b",
    "semantic_title": "grimoire is all you need for enhancing large language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=_5ObZ_eV1OX": {
    "title": "SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models",
    "volume": "review",
    "abstract": "In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \\emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our extensive experiments shed light on the resilience of LLMs against emerging threats and the efficacy of contemporary defense tactics. The data and evaluator of SALAD-Bench will be publicly available. Warning: this paper includes examples that may be offensive or harmful",
    "checked": true,
    "id": "868fcbc6127e9cf79990e92116ec482051d470f3",
    "semantic_title": "salad-bench: a hierarchical and comprehensive safety benchmark for large language models",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=gDqdC8dGfjO": {
    "title": "PPTSER: A Plug-and-Play Tag-guided Method for Few-shot Semantic Entity Recognition on Visually-rich Documents",
    "volume": "review",
    "abstract": "Visually-rich document information extraction (VIE) is a vital aspect of document understanding, wherein Semantic Entity Recognition (SER) plays a significant role. However, few-shot SER on visually-rich documents remains relatively unexplored despite its considerable potential for practical applications. To address this issue, we propose a simple yet effective Plug-and-Play Tag-guided method for few-shot Semantic Entity Recognition (PPTSER) on visually-rich documents. PPTSER is built upon off-the-shelf multi-modal pre-trained models. It leverages the semantics of the tags to guide the SER task, reformulating SER into entity typing and span detection, handling both tasks simultaneously via cross-attention. Experimental results illustrate that PPTSER outperforms existing fine-tuning and few-shot methods, especially in low-data regimes. With full training data, PPTSER achieves comparable or superior performance to fine-tuning baseline. For instance, on the FUNSD benchmark, our method improves the performance of LayoutLMv3-base in 1-shot, 3-shot and 5-shot scenarios by 15.61%, 2.13%, and 2.01%, respectively. Overall, PPTSER demonstrates promising generalizability, effectiveness, and plug-and-play nature for few-shot SER on visually-rich documents. The codes will be available at https://anonymous.for.review",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kaO5dmvZSwm": {
    "title": "CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning",
    "volume": "review",
    "abstract": "The sequential process of conceptualization and instantiation is essential to generalizable commonsense reasoning as it allows the application of existing knowledge to unfamiliar scenarios. However, existing works tend to undervalue the step of instantiation and heavily rely on pre-built concept taxonomies and human annotations to collect both types of knowledge, resulting in a lack of instantiated knowledge to complete reasoning, high cost, and limited scalability. To tackle these challenges, we introduce CANDLE (ConceptuAlization and INstantiation Distillation from Large Language ModEls), a distillation framework that iteratively performs contextualized conceptualization and instantiation over commonsense knowledge bases by instructing large language models to generate both types of knowledge with critic filtering. By applying CANDLE to ATOMIC (Sap et al., 2019a), we construct a comprehensive knowledge base comprising six million conceptualizations and instantiated commonsense knowledge triples. Both types of knowledge are firmly rooted in the original ATOMIC dataset, and intrinsic evaluations demonstrate their exceptional quality and diversity. Empirical results indicate that distilling CANDLE on student models provides benefits across three downstream tasks",
    "checked": true,
    "id": "5aec6865043cb7c7f281699ae95652e0ff680f09",
    "semantic_title": "candle: iterative conceptualization and instantiation distillation from large language models for commonsense reasoning",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=CV4J_ZQ2tJ": {
    "title": "Efficient Active Learning with Adapters",
    "volume": "review",
    "abstract": "One of the main obstacles for deploying Active Learning (AL) in practical NLP tasks is high computational cost of modern deep learning models. This issue can be partially mitigated by applying lightweight models as an acquisition model, but it can lead to the acquisition-successor mismatch (ASM) problem. Previous works show that the ASM problem can be partially alleviated by using distilled versions of a successor models as acquisition ones. However, distilled versions of pretrained models are not always available. Also, the exact pipeline of model distillation that does not lead to the ASM problem is not clear. To address these issues, we propose to use adapters as an alternative to full fine-tuning for acquisition model training. Since adapters are lightweight, this approach reduces the training cost of the model. We provide empirical evidence that it does not cause the ASM problem and can help to deploy active learning in practical NLP tasks",
    "checked": false,
    "id": "7b50b4e98b7f4a5662dd87e4de929b1037bc37c4",
    "semantic_title": "parameter-efficient language model tuning with active learning in low-resource settings",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=GTrOPsuT9_": {
    "title": "VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation",
    "volume": "review",
    "abstract": "In the rapidly advancing field of conditional image generation research, challenges such as limited explainability lie in effectively evaluating the performance and capabilities of various models. This paper introduces VIEScore, a Visual Instruction-guided Explainable metric for evaluating any conditional image generation tasks. VIEScore leverages general knowledge from Multimodal Large Language Models (MLLMs) as the backbone and does not require training or fine-tuning. We evaluate VIEScore on seven prominent tasks in conditional image tasks and found: (1) VIEScore (GPT4-v) achieves a high Spearman correlation of 0.3 with human evaluations, while the human-to-human correlation is 0.45. (2) VIEScore (with open-source MLLM) is significantly weaker than GPT-4v in evaluating synthetic images. (3) VIEScore achieves a correlation on par with human ratings in the generation tasks but struggles in editing tasks. With these results, we believe VIEScore shows its great potential to replace human judges in evaluating image synthesis tasks",
    "checked": true,
    "id": "55a305a44829a9dbdcca9f4e06a68ca5c319e12e",
    "semantic_title": "viescore: towards explainable metrics for conditional image synthesis evaluation",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=hj0aiaJB84": {
    "title": "LAI: Label Annotation Interaction based Representation Ehancement for End to End Relation Extraction",
    "volume": "review",
    "abstract": "While numerous studies on end-to-end relation extraction (E2ERE) have centered on enhancing span representations to improve model performance, challenges remain due to the gaps between subtasks (named entity recognition and relation extraction) and the modeling discrepancies between entities and relations. In this paper, we propose a novel Label Annotation Interaction based representation enhancement method for E2ERE, which institutes a two-phase semantic interaction to augment representations. Specifically, we firstly feed label annotations that are easy to manually annotate into a language model, and conduct the first round interaction between three types of tokens with a partial attention mechanism; Then construct a latent multi-view graph to capture various possible links between label and entity (pair) nodes, facilitating the second round interaction between entities and labels. A regimen of rigorous experimentation demonstrates that LAI-Net achieving performance parity with the current SOTA models on ADE/SciERC dataset in terms of NER task (a SOTA performance has been achieved on the ACE05 dataset pecifically), and establishing a new SOTA result (with nearly a 10% advance on the SciERC dataset for RE specifically) in terms of RE task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nmcEkKauCk": {
    "title": "SlideAVSR: A Dataset of Paper Explanation Videos for Audio-Visual Speech Recognition",
    "volume": "review",
    "abstract": "Audio-visual speech recognition (AVSR) is a multimodal extension of automatic speech recognition (ASR), using video as a complement to audio. In AVSR, considerable efforts have been directed at datasets for facial features such as lip-readings, while they often fall short in evaluating the image comprehension capabilities in broader contexts. In this paper, we construct SlideAVSR, an AVSR dataset using scientific paper explanation videos. SlideAVSR provides a new benchmark where models transcribe speech utterances with texts on the slides on the presentation recordings. As technical terminologies that are frequent in paper explanations are notoriously challenging to transcribe without reference texts, our SlideAVSR dataset spotlights a new aspect of AVSR problems. As a simple yet effective baseline, we propose DocWhisper, an AVSR model that can refer to textual information from slides, and confirm its effectiveness on SlideAVSR",
    "checked": true,
    "id": "10f94715a561813d2a666663d7e501c2732be4c9",
    "semantic_title": "slideavsr: a dataset of paper explanation videos for audio-visual speech recognition",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=8968jQhQdS": {
    "title": "UP4LS: User Profile Constructed by Multiple Attributes for Enhancing Linguistic Steganalysis",
    "volume": "review",
    "abstract": "Linguistic steganalysis (LS) tasks aim to effectively detect stegos generated by linguistic steganography. Existing LS methods overlook the distinctive user characteristics, leading to weak performance in social networks. The limited occurrence of stegos further complicates detection. In this paper, we propose the UP4LS, a novel framework with the User Profile for enhancing LS performance. Specifically, by delving into post content, we explore user attributes like writing habits, psychological states, and focal areas, thereby building the user profile for LS. For each attribute, we design the identified feature extraction module. The extracted features are mapped to high-dimensional user features via deep-learning networks from existing methods. Then the language model is employed to extract content features. The user and content features are integrated to optimize feature representation. During the training phase, we prioritize the distribution of stegos. Experiments demonstrate that UP4LS can significantly enhance the performance of existing methods, and an overall accuracy improvement of nearly 25%. In particular, the improvement is especially pronounced with fewer stego samples. Additionally, UP4LS also sets the stage for studies on related tasks, encouraging extensive applications on LS tasks",
    "checked": true,
    "id": "1840b6363cc9507c4417db262fcc07c2ef3bf619",
    "semantic_title": "up4ls: user profile constructed by multiple attributes for enhancing linguistic steganalysis",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=fuKElBt1wAV": {
    "title": "CEsArg: A Chinese High School Argumentative Essays Corpus for Comprehensive Argument Analysis",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cfa8TYfK-6": {
    "title": "CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "72f51c3ef967f7905e3194296cf6fd8337b1a437",
    "semantic_title": "codechameleon: personalized encryption framework for jailbreaking large language models",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=nMO6RdQl3H1": {
    "title": "FSL: Finding the Skilled Layers for Finetuning LLMs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=18ErSFOICd7": {
    "title": "DeathNote: Facilitating Fine-grained Suicidal Ideation Detection -- Hierarchical Taxonomy, Dataset, and Benchmarks",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pmSiOVYP4q": {
    "title": "Structural Entropy Guided Graph Prompt Learning for Relation Extraction",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v_4jfPR3zNx": {
    "title": "Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "1cb37e155e3414ff7774c6c91e72699c35d971c7",
    "semantic_title": "analyzing and reducing catastrophic forgetting in parameter efficient tuning",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=rBGUcde36N": {
    "title": "What is the Best Way for ChatGPT to Translate Poetry?",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "a09fd9590b32f62589a5fb3d71ec015442fb9d87",
    "semantic_title": "what is the best way for chatgpt to translate poetry?",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Zq4NH3yzHj": {
    "title": "Can You be 30% Extroverted: A Method to Quantitatively Control Generation Attributes in LLMs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dBF1_JpIG": {
    "title": "MetaRM: Shifted Distributions Alignment via Meta-Learning",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "3cdb8432d11eed394906637f4eedf38d3f971015",
    "semantic_title": "metarm: shifted distributions alignment via meta-learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=yHdX1wTXdfh": {
    "title": "Representativeness Heuristics on GPT-3.5",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "2fd85798a742460c468777e150056cf0e43f9010",
    "semantic_title": "do large language models show decision heuristics similar to humans? a case study using gpt-3.5",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=Iuaq2rBN9qL": {
    "title": "Probing the Boundaries: Specificity and Uncertainty in LLM-Based Medical Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cvRzULx0kBA": {
    "title": "CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "4fc8f86074772c3a2ff80cd1b99627b6c8bf6838",
    "semantic_title": "cmmmu: a chinese massive multi-discipline multimodal understanding benchmark",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=dHXBYP-mSl": {
    "title": "Unraveling Babel: Exploring Multilingual Activation Patterns within Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "fe5119a306010cfd05bd367b5e6223c258cd1c45",
    "semantic_title": "unraveling babel: exploring multilingual activation patterns within large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YywWmtDl5_s": {
    "title": "A Unimodal Valence-Arousal Driven Contrastive Learning Framework for Multimodal Multi-Label Emotion Recognition",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jCDkwfpMzO": {
    "title": "Modality-Emotion-Aware Contrastive Learning for Multimodal Emotion Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "95a4be1dd9573bb47848599438c035db7d56d50a",
    "semantic_title": "joyful: joint modality fusion and graph contrastive learning for multimodal emotion recognition",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=tG80hAj1C1": {
    "title": "Joint Automatic Speech Recognition And Structure Learning For Better Spoken Language Understanding",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "4acda28797d273d72d0f8ed343e84aadf34801ff",
    "semantic_title": "joint unsupervised and supervised learning for context-aware language identification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=65MwWztxFX": {
    "title": "Are Large Language Models Rational Investors?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "ac771182d1780c863954243809d1e144433919f9",
    "semantic_title": "aligning large language models with human: a survey",
    "citation_count": 189,
    "authors": []
  },
  "https://openreview.net/forum?id=pnrAZsVNGH": {
    "title": "TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "a64067c6c4286fc60f4430829ae6b18519c088e3",
    "semantic_title": "trace: a comprehensive benchmark for continual learning in large language models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=yWYJCA3WHj": {
    "title": "Cause and Effect: Can Large Language Models Truly Understand Causality?",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "df3409be8eb74814614c6a6dc8970865dcf70dcc",
    "semantic_title": "cause and effect: can large language models truly understand causality?",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=bX_F_ufLYX": {
    "title": "Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data?",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "5d0b03d545c6468c943858ce46a865380811d05b",
    "semantic_title": "struc-bench: are large language models really good at generating complex structured data?",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=KELee6ZZXt": {
    "title": "Towards Generalist Prompting for Large Language Models by Mental Models",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "2c9e99d5d7ba2ae411329e59c3523df6413d771e",
    "semantic_title": "towards generalist prompting for large language models by mental models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kMQaYM-Xqq": {
    "title": "Simple Transformers: Open-source for All",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "6db5f84eb63a6e201c4fd1a8c0ad9b0a7067573e",
    "semantic_title": "simple open-source uhf rfid tag platform",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=6J38bthSrr": {
    "title": "Beyond Literal Understanding: Generative Language Models and Sentimental Analysis in Sarcasm Detection",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qKQEfBqE5nY": {
    "title": "From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "aa85dc60e88ca135c272f1ccb2b471f7b60a7e78",
    "semantic_title": "from prejudice to parity: a new approach to debiasing large language model word embeddings",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gjb5ZThyRg": {
    "title": "All Knowledge You Need about DPO and its Variants",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "662fe1cb7ea8dec962839c137927b17bdcf96a7a",
    "semantic_title": "models of socio-cyber-physical systems security",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=IIb8pi8LeZ": {
    "title": "Find and Replace: A Two Step Method for ASR Low Frequency Entity Error Correction",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BbwDvK6ikjV": {
    "title": "DEEP-ICL: Definition-Enriched Experts for Language Model In-Context Learning",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "111de5ec26af38a283d3e632194dec231c681eb8",
    "semantic_title": "deep-icl: definition-enriched experts for language model in-context learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=nEHOp_ebt5r": {
    "title": "Brain-Inspired Two-Stage Approach: Enhancing Mathematical Reasoning by Imitating Human Thought Processes",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "09a516b19897e20860dcde8fea20da3bd867d356",
    "semantic_title": "brain-inspired two-stage approach: enhancing mathematical reasoning by imitating human thought processes",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jXobZrl2zBW": {
    "title": "HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "155b54d11285a29c4086891da7c1c1975d6bfe39",
    "semantic_title": "hyperllava: dynamic visual and language expert tuning for multimodal large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=0HXzfQgzhZz": {
    "title": "Enhancing Code Retrieval with High-Quality Dataset Generation through Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "cb17ecd99f4c588c9e0b627d534d219b7541cfda",
    "semantic_title": "enhancing code tracing question generation with refined prompts in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-34XVIKnxK5": {
    "title": "Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "312b3593304722609b834724c1cb40cd787b5412",
    "semantic_title": "born with a silver spoon? investigating socioeconomic bias in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z4V3qlOrWln": {
    "title": "VO-V2S:Visual-OnlyVideo-to-SpeechSynthesis",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D1iAiSaLOy": {
    "title": "Specuna: A Speculative Vicuna with Shallow Layer Reuse",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KNRVEgpfZm": {
    "title": "MDT: Multi-view sampling and dynamic contrastive training for long-form document matching",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8mpjKoWaIir": {
    "title": "NoisyHate: Towards Robust Content Moderation Models using Human-Written Perturbations",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-XCotGaOyRG": {
    "title": "ImgTrojan: Jailbreaking Vision-Language Models with ONE Image",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "37149662dd8eb59fdc0fae23bce8edc16a51cd79",
    "semantic_title": "imgtrojan: jailbreaking vision-language models with one image",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=GuldbHECGCY": {
    "title": "Extrapolating to Unknown Opinions using LLMs",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "4cd9389c227b771ec40cf09d9e7657391098f285",
    "semantic_title": "an opinion on chatgpt in health careâ€”written by humans only",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=No1aYanRsX3": {
    "title": "Multi-modal Auto-regressive Modeling via Visual Words",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "3d33d2d3c51d7a2b9dfb52d13c10cb8dfd09b1c6",
    "semantic_title": "multi-modal auto-regressive modeling via visual words",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=K2Y71h0FGXy": {
    "title": "Text-Vision Interacted and Auxiliary Contrastive Learning for Document Layout Analysis",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2TrDYAqnw6m": {
    "title": "Light Up the Shadows: Enhance Long-Tail Entity Grounding with Concept-Guided Vision-Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "fdc7ea136d7b3711aaa9ce30e950acc0215a2fb1",
    "semantic_title": "light up the shadows: enhance long-tailed entity grounding with concept-guided vision-language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zuWjPdpGJj": {
    "title": "Zero shot VLMs for hate meme detection: Are we there yet?",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "52d57b4f0a6e140c625e4f9064231ac811c53309",
    "semantic_title": "zero shot vlms for hate meme detection: are we there yet?",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Uf98WfXCiur": {
    "title": "Laying the Foundation First? Investigating the Generalization from Atomic Skills to Complex Reasoning Tasks",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "7de1c68121c19529ef5610bb4041f215db464f6b",
    "semantic_title": "laying the foundation first? investigating the generalization from atomic skills to complex reasoning tasks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W_5-0lCTDX": {
    "title": "R^3-NL2GQL: A Hybrid Models Approach for Accuracy Enhancing and Hallucinations Mitigation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "11a50efec882fe23445b9717bae51bf48de48065",
    "semantic_title": "r3-nl2gql: a hybrid models approach for for accuracy enhancing and hallucinations mitigation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qaytEiMl129": {
    "title": "SELF-[IN]CORRECT: Large Language Models Struggle with Correcting Self-Generated Responses",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "44d74b0d77b4056ddd4c6611a76711c8bab2e0a7",
    "semantic_title": "dehallucinating large language models using formal methods guided iterative prompting",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=bMp8AfE0hR4": {
    "title": "Reference Trustable Decoding: A New Training-free Paradigm for Improving Large Language Model Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lz4jBWzfRv": {
    "title": "Entropy-Constrained Non-Negative Kernel Regression for Sentence-Level Anomaly Detection",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=caGBfk7cWz": {
    "title": "Relevant or Random: Can LLMs Truly Perform Analogical Reasoning?",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "b2ee9afe5142bd47163c4261a544e77a622023e0",
    "semantic_title": "relevant or random: can llms truly perform analogical reasoning?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2kF9CJ4mcNR": {
    "title": "Neuro-symbolic Contrastive Learning for Cross-domain Inference",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NoWKyWOMUC": {
    "title": "Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "dcfe736fbe1e72b46ee28a744d9525d152912c4c",
    "semantic_title": "learning to check: unleashing potentials for self-correction in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-Aqn5HcMOYv": {
    "title": "Mat-Rules: A Continuous Rule-based Model for Extrapolation Reasoning on Temporal Knowledge Graph",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RA2O2cAkfUa": {
    "title": "Prompt-based Pseudo-labeling Strategy for Sample-Efficient Semi-Supervised Extractive Summarization",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "7d2a9814f7fb5d6b806fdd0e451981986c2b23f3",
    "semantic_title": "prompt-based pseudo-labeling strategy for sample-efficient semi-supervised extractive summarization",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=ye1rfFHYYle": {
    "title": "Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing Conversational LLMs with Direct RLHF",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "9ddfb1583ce7f5370ace2751bb5f260fa4af1961",
    "semantic_title": "balancing enhancement, harmlessness, and general capabilities: enhancing conversational llms with direct rlhf",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=0mmyzdV9yqp": {
    "title": "Enhancing Multimodal Object-Entity Relation Extraction via Multi-Aspect Contrastive Learning in Large Multimodal Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BTG-UaFsjF": {
    "title": "Can Language Models Act as Knowledge Bases at Scale?",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "e98392f4906cf8fc912697b8fdb808e5cef0aa71",
    "semantic_title": "can language models act as knowledge bases at scale?",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=CMXhkufll2": {
    "title": "Distributional Differences Estimation of Large Language Models for Few-shot Open Information Extraction",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M_vr_yg9qpR": {
    "title": "Towards the Law of Capacity Gap in Distilling Language Models",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "a6348981246adcd42e8ba39acf139da745696eff",
    "semantic_title": "towards the law of capacity gap in distilling language models",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=frBWtCm285-": {
    "title": "Fine-grained Hallucination Detection and Editing for Language Models",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "028d75496e51943f52c7b2177344a3c089c18058",
    "semantic_title": "fine-grained hallucination detection and editing for language models",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=acVs9wjydkH": {
    "title": "Fine-Grained Detoxification via Instance-Level Prefixes for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "4a3caeadc881e3ed20fb8585749cf7acfc3aa833",
    "semantic_title": "fine-grained detoxification via instance-level prefixes for large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JAVaC--XWh": {
    "title": "UNO Arena for Evaluating Sequential Reasoning of LLMs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5VJblWT1lit": {
    "title": "A Hierarchical Reasoning Framework for Complex Question Answering over Knowledge Graph with Reinforcement Learning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=czAUjE6Tf6e": {
    "title": "On Diversified Preferences of Large Language Model Alignment",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "f218583fdd398a0841eb7767e7bf21e90fc60f81",
    "semantic_title": "on diversified preferences of large language model alignment",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=zTRH70G0Ux": {
    "title": "Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "c25f3d54f63a1725d31ecae60a24ddbaea7aa683",
    "semantic_title": "finer: investigating and enhancing fine-grained visual concept recognition in large vision language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nUxaBQuwfw": {
    "title": "MisToL: Mistake-Tolerant Learning in Distantly Supervised Named Entity Recognition",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8cF-rrBRjA4": {
    "title": "Causal ATE Mitigates Unintended Bias in Controlled Text Generation",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "6d5152ad8297da881c369401a96da51531089ac0",
    "semantic_title": "causal ate mitigates unintended bias in controlled text generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=knd45VbzNNS": {
    "title": "Heterogeneous Graphormer for Extractive Multimodal Summarization",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "998b39d3266987524dc9cb04a8f2a8738de0d3a5",
    "semantic_title": "single-cell molecular barcoding to decode multimodal information defining cell states",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=IRjR3EUxXy": {
    "title": "Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "e98037a0363e168d35ea236a9502618e7d1817e7",
    "semantic_title": "re-examine distantly supervised ner: a new benchmark and a simple approach",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qtdj6PbpD8": {
    "title": "TPD: Enhancing Student Language Model Reasoning via Principle Discovery and Guidance",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "2c6896c025b29a2cc3d90bbf9b77646b255b2090",
    "semantic_title": "tpd: enhancing student language model reasoning via principle discovery and guidance",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Q6w78cqB7i": {
    "title": "HRSpeech: Enhancing Zero-Shot Speech Generation through Speaker Hash Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-83oqTxarH0": {
    "title": "Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "05839a68bd05880beef2f171cee7aab960bb6d2f",
    "semantic_title": "hal-eval: a universal and fine-grained hallucination evaluation framework for large vision language models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=YFllsHC9Vug": {
    "title": "CharacterChat: Learning towards Conversational AI with Personalized Social Support",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "beaf718e3fdc127e2323a6071866720dc6a3e87e",
    "semantic_title": "characterchat: learning towards conversational ai with personalized social support",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=ZzXaxJ8XAxm": {
    "title": "CharacterGPT: God's Blessing on This Assistants API!",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u3Lh0L-Ct5": {
    "title": "Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics for Domain Specialized Text Analysis",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "d7f89348f673f9915bc2752d3172d37402acbe28",
    "semantic_title": "reasoning before comparison: llm-enhanced semantic similarity metrics for domain specialized text analysis",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=ivxn2AHDbF": {
    "title": "Mixed Distillation Helps Smaller Language Model Better Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "1bd9466f0bb10d29a16f614943ec7823e13cb210",
    "semantic_title": "mixed distillation helps smaller language model better reasoning",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=I3Ark7oMJM": {
    "title": "Don't Just Pay Attention, PLANT It Transfer L2R Models to Fine-tune Attention in Extreme Multi-Label Text Classification For ICD Coding",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ojFK8Yb2HE": {
    "title": "Decoupled Vocabulary Learning Enables Zero-Shot Translation from Unseen Languages",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "d28ae8d330f47b222f0a2cc3937d2a8dc617e56c",
    "semantic_title": "decoupled vocabulary learning enables zero-shot translation from unseen languages",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fDJLgz2Dbpc": {
    "title": "Self-Improvement of Image Synthesis via Natural Language Feedback",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7QG2fWrPQU": {
    "title": "ActiveRAG: Revealing the Treasures of Knowledge via Active Learning",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "7ae8f3fea5c8db1283bae48b5537a8ed7e8bec7b",
    "semantic_title": "activerag: revealing the treasures of knowledge via active learning",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=EV46z1RKhz3": {
    "title": "LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "0e92de315ecc3cd3d86e0e84047ed8368ece690f",
    "semantic_title": "lora-as-an-attack! piercing llm safety under the share-and-play scenario",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=X_eFhT-vZrZ": {
    "title": "Multimodal Question Answering for Unified Information Extraction",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "7b8a85e0da9ac4701640f9cd77b744a4dda80794",
    "semantic_title": "multimodal question answering for unified information extraction",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=K7YiOnULV9": {
    "title": "Prompting Is Not Enough, but LLaMA-2-Chat Can Identify Subtle Social Biases",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WTdN1XQMRM": {
    "title": "Towards Real-World Rumor Detection: Anomaly Detection Framework with Graph Supervised Contrastive Learning",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "2c597bd6e68d8451f5fa98b4c3ef65470e2c67d9",
    "semantic_title": "federated graph anomaly detection via contrastive self-supervised learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=2IClwskmrAE": {
    "title": "Segment-Based Interactive Machine Translation for Pre-trained Models",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "3189132a5f17310c5de78302dcd26707ba5ca881",
    "semantic_title": "segment-based interactive machine translation for pre-trained models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qb5fQRaj7p": {
    "title": "PreAct: Predicting Future in ReAct Enhances Agent's Planning Ability",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "2c6b2f3e6f31e2e3f4b2875a2841681fa7fe4acb",
    "semantic_title": "preact: predicting future in react enhances agent's planning ability",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=x5EMc4dhV1": {
    "title": "Multi-Perspective Consistency Enhances Confidence Estimation in Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "5fb4264c69842aab6c33225faa52a7114c28cf7e",
    "semantic_title": "multi-perspective consistency enhances confidence estimation in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zcgukb4AEZz": {
    "title": "Identifying Multiple Personalities in Large Language Models with External Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "f2f2339f320df0021825a3160cfbc027cb442d13",
    "semantic_title": "identifying multiple personalities in large language models with external evaluation",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=8MIidL-LZ4": {
    "title": "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "0ad16e2c1c30d8ed5b63970e5fb3459a08218ea3",
    "semantic_title": "nlebench+norglm: a comprehensive empirical analysis and benchmark dataset for generative language models in norwegian",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2CAMSjrWN2D": {
    "title": "Reducing Hallucinations in Vision-Language Models through Linguistic Uncertainty Augmentation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7PK6hXnLT8_": {
    "title": "Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Generation",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "e118080d723a62560b56c91c78986b5da6aaf21f",
    "semantic_title": "language and task arithmetic with parameter-efficient layers for zero-shot summarization",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=AFRPuVas3K": {
    "title": "Treetod: A Bilingual Tree-Structure Human-Labeled Dataset for Models in Multi-Domain Task-Oriented Dialogues with Clarification Questions",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xh04mYfdRQ": {
    "title": "Using Natural Language Explanations to Rescale Human Judgments",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "4bf3fd4859cb0d37e333ee9ed4024387e265c99e",
    "semantic_title": "using natural language explanations to rescale human judgments",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=xxc2D7FwGY": {
    "title": "Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "0ea8ceb4f6a759b07a5b871fd25cb5b7bf33acd8",
    "semantic_title": "why lift so heavy? slimming large language models by cutting off the layers",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s_cDQa1cWn": {
    "title": "Graph Agent: Explicit Reasoning Agent for Graphs",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "e55f2b27f70b0e223e8e5af67b56f6b6ffc8316e",
    "semantic_title": "graph agent: explicit reasoning agent for graphs",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Uk9kB1qFqb": {
    "title": "Fusion-Eval: Integrating Assistant Evaluators with LLMs",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "00c9c7d85b485e46203994df13f0dd545fe3d471",
    "semantic_title": "fusion-eval: integrating assistant evaluators with llms",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=5WXPgDMW7sX": {
    "title": "U-Eval: An Advanced LLM-based Evaluation Framework for Call Center Dialogue Summarization",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WYszbPubCAB": {
    "title": "MST5 â€” Transformers-based Approach to SPARQL Query Generation from Multilingual Natural Language Questions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "0b871a9f12e5c2da1b291a8b166c671256ebe1cd",
    "semantic_title": "a copy mechanism for handling knowledge base elements in sparql neural machine translation",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=9t6GUwQpYgD": {
    "title": "Explaining Hate: Optimal Detection and Characterization of Hate-Speech using Dynamically Weighted QLoRA LLMs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kyocy8GKY8N": {
    "title": "High-Confident Data Extraction from LLM with LoRA Calibration",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5unqXlOI-L8": {
    "title": "Controllable Data Augmentation for Few-Shot Text Mining with Chain-of-Thought Attribute Manipulation",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "5f08a63a0bb6eb1e0f5f8c9489c33cd63702796a",
    "semantic_title": "controllable data augmentation for few-shot text mining with chain-of-thought attribute manipulation",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=rZWPCgWoGn": {
    "title": "Personalized LLM Response Generation with Parameterized Memory Injection",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "c1f3c757da46a029ea7fad35c1b183ca460f4100",
    "semantic_title": "personalized llm response generation with parameterized memory injection",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=0CjGRHGDu3_": {
    "title": "Tears of Joy or Sorrow? Deciphering Multimodal Situation with Context",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8XUKUtEooTD": {
    "title": "DANER: Dataset-agnostic Instruction Tuning for Open Named Entity Recognition",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JBmbs3WeNlv": {
    "title": "Cross-domain Chinese Sentence Pattern Structure Parsing",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "5eeb634b1bee29c484f69f24d2362b0fb0dd9a0e",
    "semantic_title": "cross-domain chinese sentence pattern parsing",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tvPd4mIbjF": {
    "title": "VisFine-LLM: A Multimodal Large Language Model Empowered with Fine-grained Visual Interactions",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "a0994c48c5015a5c17bc61febce38fe4cc07d846",
    "semantic_title": "multi-modal instruction tuned llms with fine-grained visual perception",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=ZeNEH1uJHYG": {
    "title": "Is Embedding-as-a-Service Safe? Meta-Prompt-based Backdoor Attacks for User-Specific Trigger Migration",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2WxeIjVGf9p": {
    "title": "Self-Assessment Tests are Unreliable Measures of LLM Personality",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "fb5a8983e1a7573de1386f4fea5398874e2c96c7",
    "semantic_title": "self-assessment tests are unreliable measures of llm personality",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=SpQWZ1F2kzH": {
    "title": "To Prompt or not to Prompt: Generating Affective Conversational Datasets with a Reproducible Affordable Computation-Efficient Approach",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WPX4yOXKRD": {
    "title": "Grasping the Why: Explainable Graph Learning for Rumor Detection with Lucid Insights",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZpDBNwwYSt": {
    "title": "PiT: Prompt Injection Tuning for Pre-trained Lanaguage Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uHydEfsDL7P": {
    "title": "Rethinking the Evaluation of Pre-trained Text-and-Layout Models from an Entity-Centric Perspective",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "0d945a12a321e086f13b7d46e6d42eec94d464cf",
    "semantic_title": "rethinking the evaluation of pre-trained text-and-layout models from an entity-centric perspective",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WNnnj9MsUva": {
    "title": "Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "5e761e9f5cd9672a181b256299cd2916a8079461",
    "semantic_title": "augmenting black-box llms with medical textbooks for clinical question answering",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=SJO2fRC1zy": {
    "title": "Mercury: An Efficiency Benchmark for LLM Code Synthesis",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "15dc4534ae1e5aefdc196cf1da983b2ee56c1dda",
    "semantic_title": "mercury: an efficiency benchmark for llm code synthesis",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=evG4-X3c04": {
    "title": "ArtGPT-4: Towards Artistic-understanding Large Vision-Language Models with Enhanced Adapter",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "9d7cbf4c81da9058f6349b5170f67dcc798597c9",
    "semantic_title": "artgpt-4: towards artistic-understanding large vision-language models with enhanced adapter",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Z4jsbQZpmx": {
    "title": "Adaptive Activation Steering: A Tuning-Free LLM Reliability Improvement Method for Diverse Hallucinations Categories",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "db37dd57fbf61ea0377011acbc2bf53f7134d330",
    "semantic_title": "adaptive activation steering: a tuning-free llm truthfulness improvement method for diverse hallucinations categories",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SgYUxdTv__": {
    "title": "Interaction Matters: A Three-Level Multi-Class English Second Language Conversation Dialogue through Interactive Based Evaluation Metrics",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aosAnHaplv": {
    "title": "TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and Agent Generation",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "a7875733fe7638b513ad6f0e42f75fd10f287d9c",
    "semantic_title": "tdag: a multi-agent framework based on dynamic task decomposition and agent generation",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=YuvuKCh4O_z": {
    "title": "Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "1991666b99d1f9781aab32af504168748c117557",
    "semantic_title": "towards cross-tokenizer distillation: the universal logit distillation loss for llms",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=PAqn7VGQEa": {
    "title": "URAN: Universal and Practical Vision-and-Language Navigation through Zero-shot Learning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9KGEY-mOs1": {
    "title": "Inferring the Phylogeny of Large Language Models and Predicting their Performances in Benchmarks",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "1e631d2570228d5a0b5d5929e15fc588e1db09cd",
    "semantic_title": "inferring the phylogeny of large language models and predicting their performances in benchmarks",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Zz2BK2_4f1E": {
    "title": "HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "61bb3de454e60d054850277dd8c1b539c66c807e",
    "semantic_title": "hift: a hierarchical full parameter fine-tuning strategy",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=jsWrx7ho3H": {
    "title": "SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "305856f630fb6172ba3ebfb350927c525d363867",
    "semantic_title": "swea: updating factual knowledge in large language models via subject word embedding altering",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=OutFtRIrZiu": {
    "title": "Can native language samples improve code-mixed hate detection?: A case study for Hindi-English code-mixed hate detection",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "75f47c922025977303185f56241b53ee4fedc2b1",
    "semantic_title": "improving code-mixed hate detection by native sample mixing: a case study for hindi-english code-mixed scenario",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ce1Fr5iKw3K": {
    "title": "LLMs with Chain-of-Thought Are Non-Causal Reasoners",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "d2001a89304cfee821b6b7c1ba92199b68c1fa00",
    "semantic_title": "llms with chain-of-thought are non-causal reasoners",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=dxSYGumpadz": {
    "title": "Leveraging Large Language Models for Concept Graph Recovery and Question Answering in NLP Education",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "2ee64b780d371c9b5f7ed51d1f7a9f9fca01042c",
    "semantic_title": "leveraging large language models for concept graph recovery and question answering in nlp education",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=VHC1Oa6AvBy": {
    "title": "Radiology-Aware Model-Based Evaluation Metric for Report Generation",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "e811df37cf44de1772761a6bf64fb2971362f39c",
    "semantic_title": "radiology-aware model-based evaluation metric for report generation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=6qIosChDVO": {
    "title": "Hypergraph based Understanding for Document Semantic Entity Recognition",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "5ba7a5db13f0d1c77feb978c87d8fc9697568cc7",
    "semantic_title": "hypergraph based understanding for document semantic entity recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OMP6H7KWuu": {
    "title": "CoT-Planner: Chain-of-Thought as the Content Planner for Few-shot Table-to-Text Generation Reduces the Hallucinations from LLMs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qnJ0RfTBsGz": {
    "title": "Beyond Turing: A Comparative Analysis of Approaches for Detecting Machine-Generated Text",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "3bd072b495c54dde9848f420b6071b0bedc05427",
    "semantic_title": "beyond turing: a comparative analysis of approaches for detecting machine-generated text",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rjuILZbiBg": {
    "title": "Dynamic Moment Query Refocusing for Moment Retrieval and Highlight Detection",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CVCswRwESk": {
    "title": "Enhancing Mathematical Reasoning in Math Word Problems: A Numerical Masking Approach for Encoder-Decoder Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xQLTPREDCr": {
    "title": "Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "ebba25e925c4ff6ab12a60431835e7387e9edf4f",
    "semantic_title": "babysit a language model from scratch: interactive language learning by trials and demonstrations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ubjjZXlx3Z": {
    "title": "Efficient Multilingual Visual Speech Recognition by Modeling Discretized Visual Speech Units",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "955b2633b4279a55dd8c066ed65b1028c48a5bb2",
    "semantic_title": "efficient training for multilingual visual speech recognition: pre-training with discretized visual speech representation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=dGrgz4vzjX": {
    "title": "Unifying Vision-language Models and Knowledge Graphs for Zero-shot Counter-intuitive Reasoning in Images",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w3XQieszHOD": {
    "title": "A Translation model that uses a Factored Phrase-Based SMT Approach to translate Amharic text into EthSL",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x9W57mq7kAa": {
    "title": "MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "59b4b5b1e2198f264536d83e33d96b0a45ed3bac",
    "semantic_title": "moral: moe augmented lora for llms' lifelong learning",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=l2vVZ20lmbO": {
    "title": "Synergizing Large Language Models with Contrastive Learning for Aspect-based Sentiment Analysis",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "7a8a753359a42985690e01b1f061b5048e396474",
    "semantic_title": "boosting large language models with continual learning for aspect-based sentiment analysis",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pDQ1bNyMkov": {
    "title": "Enhancing LLM Pretraining by Checkpoint Merging: An Almost Free Lunch Approach",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jrR2xsJi8MS": {
    "title": "Double Mixture for Better Experts: \\\\Continual Speech Event Detection with Compositional Generalization",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4wLYEhUaVT": {
    "title": "M2D-DKTL: Multi-modal Multi-agent Debate-based Dialectical Knowledge Transfer Learning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ODc6t9X9r3": {
    "title": "C-ICL: Contrastive In-context Learning for Information Extraction",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "28e9fb24bfb25f6e40d3897a5cf9c39f6f6b8d1f",
    "semantic_title": "c-icl: contrastive in-context learning for information extraction",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=KNfGa9WK9n": {
    "title": "Tailoring Personality Traits in Large Language Models via Unsupervisedly-Built Personalized Lexicons",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "1fc9b369f0690af9fb9b911bfff4dae0d4999083",
    "semantic_title": "tailoring personality traits in large language models via unsupervisedly-built personalized lexicons",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=qcVhpTijjw6": {
    "title": "TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "904b37ae6466cc90826adce53ed23b1eee3dbde0",
    "semantic_title": "tmt: tri-modal translation between speech, image, and text by processing different modalities as different languages",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=sPkqCJXtBuk": {
    "title": "How Do In-Context Documents Affect Long-form Answer Generation?",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "469fb5af039e9ebff2e109773b6bf485e1fc3ae5",
    "semantic_title": "parallel session 1: constructing community",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=McZGkxWz2j": {
    "title": "MPPQA: Structure-Aware Multi-Span Style Extraction for Prerequisite Associated Procedural Reading Comprehension",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b6JeWmLobGj": {
    "title": "Style Over Substance: Evaluation Biases for Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "7ace46ab8e71c4304682ab126b1212deb54b9b03",
    "semantic_title": "style over substance: evaluation biases for large language models",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=dN5ddmNrab": {
    "title": "Label Alignment based Instruction Fine-Tuning for Cross-Domain Named Entity Recognition",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "9141480721653789597b6e537ee0eeab401f3e60",
    "semantic_title": "promptner: prompting for named entity recognition",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=IhgTa0GEd9": {
    "title": "WordPlay: An Agent Framework for Language Learning Games",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "adc069674d7738cbc67047c9316cd52ec4b94fd1",
    "semantic_title": "wordplay: an agent framework for language learning games",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=h4vDjKpl5Y": {
    "title": "Turn the News Inside Out: Deconstructing News Intent for Better News Understanding",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YL5OTwnG_c": {
    "title": "Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "0639e2e209213ecb54eb4d6555e271d070344842",
    "semantic_title": "everything of thoughts: defying the law of penrose triangle for thought generation",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=TnC9M6x3uC": {
    "title": "Emulating the Human Mind: A Neural-symbolic Link Prediction Model with Fast and Slow Reasoning and Filtered Rules",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "88fa3c87856564d2dee8a628f828bad018552311",
    "semantic_title": "emulating the human mind: a neural-symbolic link prediction model with fast and slow reasoning and filtered rules",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1GP0xwDvcw": {
    "title": "Analysis of Key Relevance in Multi-Document Summarization Using Dynamic Topic Modeling In Korean Reviews",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HTFWYwVIYh": {
    "title": "Human-AI Interactions in the Communication Era: Autophagy Makes Large Models Achieving Local Optima",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pFRguGNbGW": {
    "title": "AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "bd15acc371153925bf86924cb2d13114ca6c95f8",
    "semantic_title": "ai hospital: interactive evaluation and collaboration of llms as intern doctors for clinical diagnosis",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=5Gbq1Dlybld": {
    "title": "Self-RLMF: Interest Modeling via Self-Representation and Reinforcement Learning from Mixed Feedback",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=15DbNSgN89X": {
    "title": "GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "094883e42bb9a41f602c0715c1059bc431e33fb2",
    "semantic_title": "gpt4roi: instruction tuning large language model on region-of-interest",
    "citation_count": 138,
    "authors": []
  },
  "https://openreview.net/forum?id=XjripHacKD8": {
    "title": "BAD: BiAs Detection for Large Language Models in the context of candidate screening",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "04f24f929ebe31c9c6f6e5fc1cb6fc6c070bd05b",
    "semantic_title": "bad: bias detection for large language models in the context of candidate screening",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=8Yfrc2Doo1T": {
    "title": "An Improved Method for Evaluating Natural Language Generation by Measuring Spectral Distance of Surprisals",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CsM2_kpWlI": {
    "title": "esCorpiusDialog: A Dialogue Dataset in Spanish, Catalan, Basque, and Galician to Build Conversational Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cJvvq_SxwjM": {
    "title": "Checkpoint Merging via Bayesian Optimization in LLM Pretraining",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "a49f5b8d9c2731697163fe45a52f3ec9ba0e18eb",
    "semantic_title": "checkpoint merging via bayesian optimization in llm pretraining",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=W1vj74rJOh-": {
    "title": "TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "d238a9770d24d0725656ef6cf4789afebf2126e7",
    "semantic_title": "tigerscore: towards building explainable metric for all text generation tasks",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=6UQn12Sjc_": {
    "title": "$\\rm SP^3$: Enhancing Structured Pruning via PCA Projection",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "abea8484e09c75cef09445c97213db47711d9fef",
    "semantic_title": "$\\rm sp^3$: enhancing structured pruning via pca projection",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zgp3SLU8v2n": {
    "title": "Danish Foundation Models",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "c298d7e1f4d3bc7cfda943b020bca01de6b2272a",
    "semantic_title": "danish foundation models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=givWP-ku1UY": {
    "title": "Knowledge Editing on Black-box Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "6acd1576ed0abfb034035cb52bd34bf43abcfea3",
    "semantic_title": "knowledge editing on black-box large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kXHvGb0sWxx": {
    "title": "Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "b93ac10de176c4a7aaa2cc652b90bb25636532cd",
    "semantic_title": "benchmark self-evolving: a multi-agent framework for dynamic llm evaluation",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=HGQp5Y84E88": {
    "title": "Dual Prompt-Guided Entity Inferring and Distilling for Scene-Text Aware Cross-modal Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cJSypROtX7Y": {
    "title": "Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "90538881baa3ff6461ab6edc756832337219f9b4",
    "semantic_title": "rethinking jailbreaking through the lens of representation engineering",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=Qdq1OvO2qKV": {
    "title": "Match, Compare, or Select? An Investigation of Entity Matching with Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "3042f56baf64b06ba5f236f8237ec10174ba6ae2",
    "semantic_title": "match, compare, or select? an investigation of large language models for entity matching",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oHWC3BFnThN": {
    "title": "LLM-Augmented Retrieval: Enhancing Retrieval Models Through Language Models and Doc-Level Embedding",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "0b1928437d9cba344d47b7d75c5ec44228f973da",
    "semantic_title": "llm-augmented retrieval: enhancing retrieval models through language models and doc-level embedding",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1pmBRIWK7x": {
    "title": "Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "bcdecfa83a1715c03e5c3cddeb27260e9707632b",
    "semantic_title": "towards trustworthy reranking: a simple yet effective abstention mechanism",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=26j58s6zfe": {
    "title": "PromptSync: Bridging Domain Gaps in Vision-Language Models through Class-Aware Prototype Alignment and Discrimination",
    "volume": "review",
    "abstract": "",
    "checked": true,
    "id": "6e398ad2b0158089a435f2e1ad1bcb911ad62ad9",
    "semantic_title": "promptsync: bridging domain gaps in vision-language models through class-aware prototype alignment and discrimination",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CsCPzPnHKE": {
    "title": "SBERT-InterprEVAL: SBERT for Interpretability Evaluation of transformer-derived architectures on mental-health topic classification",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9OyYTWB_kT": {
    "title": "Natural Language Reasoning for Aleatoric Uncertainty in Rationales",
    "volume": "review",
    "abstract": "",
    "checked": false,
    "id": "fed3a8a542a9067c68d5807a0809dc9ab9a815a1",
    "semantic_title": "aura: natural language reasoning for aleatoric uncertainty in rationales",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EDqCH_sC11": {
    "title": "Good Idea or Not, Representation of LLM Could Tell",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f7cSyYOxQd": {
    "title": "GCOF: Self-iterative Control of Text Generation for Copywriting Using Large Language Model Prompts",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=64F8FCfzHpx": {
    "title": "ALMs: Authorial Language Models for Authorship Attribution",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dFOHP-FSzxI": {
    "title": "Query of CC: Unearthing Large Scale Domain-Specific Knowledge from Public Corpora",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZSOd0TYRy8": {
    "title": "HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ceqd8vnaR0": {
    "title": "Examining Alignment of Large Language Models with Human Values through Representative Heuristics, tested for the Case of Political Stereotypes",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MSf0eaU72On": {
    "title": "OceanGPT: A Large Language Model for Ocean Science Tasks",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WZhgN1Dw4li": {
    "title": "Progressive Prefix-Memory Tuning for Complex Logical Query Answering on Knowledge Graphs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jmuKjbjN1YR": {
    "title": "LAiW: A Chinese Legal Large Language Models Benchmark",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aX7BfkPmfR3": {
    "title": "PE: A Poincare Explanation Method for Text Hierarchy Generation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YemBiYiLq4": {
    "title": "FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uf6Z_98PbF": {
    "title": "Improving Self-training with Prototypical Learning for Source-Free Domain Adaptation on Clinical Text",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hkUa3TOlWW": {
    "title": "The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x5KVZd6Wj_": {
    "title": "Towards Efficient Story Plot Generation via End-to-end Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pA871-thPSi": {
    "title": "Keqing: Knowledge-based Question Answering is a Natural Chain-of-Thought Mentor of LLMs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NWUXjFER7Pb": {
    "title": "DRAK: Unlocking Molecular Insights with Domain-Specific Retrieval-Augmented Knowledge in LLMs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kh3CXl281Fl": {
    "title": "PPN: Parallel Pointer-based Network for Key Information Extraction with Complex Layouts",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q7ud6SuNFZV": {
    "title": "Event-Keyed Summarization",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8AOZGg0lMI7": {
    "title": "A Data-Centric Hypothesis-Testing Approach for Distinguishing Between Thematic and Stylometric/Generic Differences in Texts: Application to English Prose",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0h8446y_2RF": {
    "title": "Parameter-Efficient Weakly Supervised Referring Video Object Segmentation via Chain-of-Thought Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bl0Tu69YSu8": {
    "title": "MemoChat: Tuning LLMs to Use On-the-fly Text Memos for Consistent Long-Range Conversations",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UpdOiC4w1fY": {
    "title": "From Unimodal to Multimodal: A Framework for Generating High-Quality Data for Multimodal Emotional Chit-chat Dialogue",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VkagP-bKwv": {
    "title": "Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VCslSx6zMa": {
    "title": "Paraphrasing The Original Text\" Makes High Accuracy Long-Context QA",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kD05Awkeqz": {
    "title": "Personality Profiling: How informative are social media profiles in predicting personal information?",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kCieofV9p-k": {
    "title": "Reading, seeing, watching Fakes? An Empirical Study on the Media Types of Misinformation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qn6V-zJsFas": {
    "title": "Salience-aware Dialogue Summarization via Parallel Original-Extracted Streams",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oHbjrAdP4HV": {
    "title": "Consistency Matters: Explore LLMs Consistency From a Black-Box Prespective",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R9P97AodDV3": {
    "title": "Improving Natural Language Capability of Code Large Language Model",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=29CVVJeqFA6": {
    "title": "MUSA: Multi-lingual Speaker Anonymization via Serial Disentanglement",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=-tjzF-23A5g": {
    "title": "Going Beyond Word Matching: Syntax Improves In-context Example Selection for Machine Translation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6UiBSqxQzpc": {
    "title": "Description and Revision With an Explanation: A New Perspective to Review Multi-modal Translation Based on Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zZmAlRXtYub": {
    "title": "AutoSPE: Auto Stretch Positional Encodings for Finetuning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0V6S5oJTVQs": {
    "title": "Enhancing Recommendation with LLMs through User Preference Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2tnLG6iGHY7": {
    "title": "DQ-Former: Querying Transformer with Dynamic Modality Priority for Cognitive-aligned Multimodal Emotion Recognition",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MTH6T0Y_oyF": {
    "title": "Variational Multi-Modal Hypergraph Attention Network for Multi-Modal Relation Extraction",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pl5XgA8W6j-": {
    "title": "HoLLMwood: Unleashing Creativity of Large Language Models through Role-Playing",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xDl78-CQODZ": {
    "title": "KERMIT: Knowledge Graph Completion of Enhanced Relation Modeling with Inverse Transformation",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=39ap_urW7Ei": {
    "title": "CDEval: A Benchmark for Measuring the Cultural Dimensions of Large Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=niUwUuXbKVr": {
    "title": "AppAgent: Multimodal Agents as Smartphone Users",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fDDlsksNdaU": {
    "title": "GraphTeacher: Transductive Fine-Tuning of Encoders through Graph Neural Networks",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YtBzt7ZWaAw": {
    "title": "TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LdBVcU4jdqb": {
    "title": "De-fine: Decomposing and Refining Visual Programs with Auto-Feedback",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d7cjdWbHQZ5": {
    "title": "Project-specific Retrieval Augmentation for Better Code Summarization",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8p8Usm9mQu5": {
    "title": "FakeNewsGPT4: Advancing Multimodal Fake News Detection through Knowledge-Augmented LVLMs",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TMjnJLLVzR0": {
    "title": "Think from Words(TFW): Initiating Human-Like Cognition in Large Language Models Through Think from Words for Japanese Text-level Classification",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G4NfZFyj0H": {
    "title": "Evolver: Chain-of-Evolution Prompting to Boost Large Multimodal Models for Hateful Meme Detection",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gnIfhunrlLX": {
    "title": "eXplainable Bayesian Multi-Perspective Generative Retrieval",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=_bCmplIJg0f": {
    "title": "On the importance of Data Scale in Pretraining Arabic Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cJ4mvzhIhQL": {
    "title": "Enhancing Visual and Acoustic Modality Representations in Multimodal Sentiment Analysis with Body Gesture and Automated Speech Recognition",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ConXX-oKVu": {
    "title": "MMTTS: Visual-Guided Text-to-Speech with Hierarchical Multi-Modal Learning",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VVUQGQKzI7": {
    "title": "Learning Common and Private Representations for Text Readability Assessment from Deep and Linguistic Features with Self-Supervision",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Owf5IG9GEm": {
    "title": "Distilling Mathematical Reasoning Capabilities into Small Language Models",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hbtyP4BfFnM": {
    "title": "Towards Better Parameter-Efficient Fine-Tuning for Large Language Models: A Position Paper",
    "volume": "review",
    "abstract": "",
    "checked": null,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  }
}