{
  "http://arxiv.org/abs/2312.00784": {
    "title": "Making Large Multimodal Models Understand Arbitrary Visual Prompts",
    "volume": "Dec",
    "abstract": "While existing large vision-language multimodal models focus on whole image understanding, there is a prominent gap in achieving region-specific comprehension. Current approaches that use textual coordinates or spatial encodings often fail to provide a user-friendly interface for visual prompting. To address this challenge, we introduce a novel multimodal model capable of decoding arbitrary visual prompts. This allows users to intuitively mark images and interact with the model using natural cues like a \"red bounding box\" or \"pointed arrow\". Our simple design directly overlays visual markers onto the RGB image, eliminating the need for complex region encodings, yet achieves state-of-the-art performance on region-understanding tasks like Visual7W, PointQA, and Visual Commonsense Reasoning benchmark. Furthermore, we present ViP-Bench, a comprehensive benchmark to assess the capability of models in understanding visual prompts across multiple dimensions, enabling future research in this domain. Code, data, and model are publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mu Cai",
      "Haotian Liu",
      "Siva Karthik Mustikovela",
      "Gregory P. Meyer",
      "Yuning Chai",
      "Dennis Park",
      "Yong Jae Lee"
    ]
  },
  "http://arxiv.org/abs/2312.00774": {
    "title": "Context Retrieval via Normalized Contextual Latent Interaction for Conversational Agent",
    "volume": "Dec",
    "abstract": "Conversational agents leveraging AI, particularly deep learning, are emerging in both academic research and real-world applications. However, these applications still face challenges, including disrespecting knowledge and facts, not personalizing to user preferences, and enormous demand for computational resources during training and inference. Recent research efforts have been focused on addressing these challenges from various aspects, including supplementing various types of auxiliary information to the conversational agents. However, existing methods are still not able to effectively and efficiently exploit relevant information from these auxiliary supplements to further unleash the power of the conversational agents and the language models they use. In this paper, we present a novel method, PK-NCLI, that is able to accurately and efficiently identify relevant auxiliary information to improve the quality of conversational responses by learning the relevance among persona, chat history, and knowledge background through low-level normalized contextual latent interaction. Our experimental results indicate that PK-NCLI outperforms the state-of-the-art method, PK-FoCus, by 47.80%/30.61%/24.14% in terms of perplexity, knowledge grounding, and training efficiency, respectively, and maintained the same level of persona grounding performance. We also provide a detailed analysis of how different factors, including language model choices and trade-offs on training weights, would affect the performance of PK-NCLI",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junfeng Liu",
      "Zhuocheng Mei",
      "Kewen Peng",
      "Ranga Raju Vatsavai"
    ]
  },
  "http://arxiv.org/abs/2312.00763": {
    "title": "Beyond ChatBots: ExploreLLM for Structured Thoughts and Personalized Model Responses",
    "volume": "Dec",
    "abstract": "Large language model (LLM) powered chatbots are primarily text-based today, and impose a large interactional cognitive load, especially for exploratory or sensemaking tasks such as planning a trip or learning about a new city. Because the interaction is textual, users have little scaffolding in the way of structure, informational \"scent\", or ability to specify high-level preferences or goals. We introduce ExploreLLM that allows users to structure thoughts, help explore different options, navigate through the choices and recommendations, and to more easily steer models to generate more personalized responses. We conduct a user study and show that users find it helpful to use ExploreLLM for exploratory or planning tasks, because it provides a useful schema-like structure to the task, and guides users in planning. The study also suggests that users can more easily personalize responses with high-level preferences with ExploreLLM. Together, ExploreLLM points to a future where users interact with LLMs beyond the form of chatbots, and instead designed to support complex user tasks with a tighter integration between natural language and graphical user interfaces",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Ma",
      "Swaroop Mishra",
      "Ariel Liu",
      "Sophie Su",
      "Jilin Chen",
      "Chinmay Kulkarni",
      "Heng-Tze Cheng",
      "Quoc Le",
      "Ed Chi"
    ]
  },
  "http://arxiv.org/abs/2312.00751": {
    "title": "Mitigating Over-smoothing in Transformers via Regularized Nonlocal Functionals",
    "volume": "Dec",
    "abstract": "Transformers have achieved remarkable success in a wide range of natural language processing and computer vision applications. However, the representation capacity of a deep transformer model is degraded due to the over-smoothing issue in which the token representations become identical when the model's depth grows. In this work, we show that self-attention layers in transformers minimize a functional which promotes smoothness, thereby causing token uniformity. We then propose a novel regularizer that penalizes the norm of the difference between the smooth output tokens from self-attention and the input tokens to preserve the fidelity of the tokens. Minimizing the resulting regularized energy functional, we derive the Neural Transformer with a Regularized Nonlocal Functional (NeuTRENO), a novel class of transformer models that can mitigate the over-smoothing issue. We empirically demonstrate the advantages of NeuTRENO over the baseline transformers and state-of-the-art methods in reducing the over-smoothing of token representations on various practical tasks, including object classification, image segmentation, and language modeling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tam Nguyen",
      "Tan M. Nguyen",
      "Richard G. Baraniuk"
    ]
  },
  "http://arxiv.org/abs/2312.00738": {
    "title": "SeaLLMs -- Large Language Models for Southeast Asia",
    "volume": "Dec",
    "abstract": "Despite the remarkable achievements of large language models (LLMs) in various tasks, there remains a linguistic bias that favors high-resource languages, such as English, often at the expense of low-resource and regional languages. To address this imbalance, we introduce SeaLLMs, an innovative series of language models that specifically focuses on Southeast Asian (SEA) languages. SeaLLMs are built upon the Llama-2 model and further advanced through continued pre-training with an extended vocabulary, specialized instruction and alignment tuning to better capture the intricacies of regional languages. This allows them to respect and reflect local cultural norms, customs, stylistic preferences, and legal considerations. Our comprehensive evaluation demonstrates that SeaLLM-13b models exhibit superior performance across a wide spectrum of linguistic tasks and assistant-style instruction-following capabilities relative to comparable open-source models. Moreover, they outperform ChatGPT-3.5 in non-Latin languages, such as Thai, Khmer, Lao, and Burmese, by large margins while remaining lightweight and cost-effective to operate",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan-Phi Nguyen",
      "Wenxuan Zhang",
      "Xin Li",
      "Mahani Aljunied",
      "Qingyu Tan",
      "Liying Cheng",
      "Guanzheng Chen",
      "Yue Deng",
      "Sen Yang",
      "Chaoqun Liu",
      "Hang Zhang",
      "Lidong Bing"
    ]
  },
  "http://arxiv.org/abs/2312.00688": {
    "title": "Towards Transparency in Coreference Resolution: A Quantum-Inspired Approach",
    "volume": "Dec",
    "abstract": "Guided by grammatical structure, words compose to form sentences, and guided by discourse structure, sentences compose to form dialogues and documents. The compositional aspect of sentence and discourse units is often overlooked by machine learning algorithms. A recent initiative called Quantum Natural Language Processing (QNLP) learns word meanings as points in a Hilbert space and acts on them via a translation of grammatical structure into Parametrised Quantum Circuits (PQCs). Previous work extended the QNLP translation to discourse structure using points in a closure of Hilbert spaces. In this paper, we evaluate this translation on a Winograd-style pronoun resolution task. We train a Variational Quantum Classifier (VQC) for binary classification and implement an end-to-end pronoun resolution system. The simulations executed on IBMQ software converged with an F1 score of 87.20%. The model outperformed two out of three classical coreference resolution systems and neared state-of-the-art SpanBERT. A mixed quantum-classical model yet improved these results with an F1 score increase of around 6%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hadi Wazni",
      "Mehrnoosh Sadrzadeh"
    ]
  },
  "http://arxiv.org/abs/2312.00680": {
    "title": "Contextualized word senses: from attention to compositionality",
    "volume": "Dec",
    "abstract": "The neural architectures of language models are becoming increasingly complex, especially that of Transformers, based on the attention mechanism. Although their application to numerous natural language processing tasks has proven to be very fruitful, they continue to be models with little or no interpretability and explainability. One of the tasks for which they are best suited is the encoding of the contextual sense of words using contextualized embeddings. In this paper we propose a transparent, interpretable, and linguistically motivated strategy for encoding the contextual sense of words by modeling semantic compositionality. Particular attention is given to dependency relations and semantic notions such as selection preferences and paradigmatic classes. A partial implementation of the proposed model is carried out and compared with Transformer-based architectures for a given semantic task, namely the similarity calculation of word senses in context. The results obtained show that it is possible to be competitive with linguistically motivated models instead of using the black boxes underlying complex neural architectures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pablo Gamallo"
    ]
  },
  "http://arxiv.org/abs/2312.00678": {
    "title": "The Efficiency Spectrum of Large Language Models: An Algorithmic Survey",
    "volume": "Dec",
    "abstract": "The rapid growth of Large Language Models (LLMs) has been a driving force in transforming various domains, reshaping the artificial general intelligence landscape. However, the increasing computational and memory demands of these models present substantial challenges, hindering both academic research and practical applications. To address these issues, a wide array of methods, including both algorithmic and hardware solutions, have been developed to enhance the efficiency of LLMs. This survey delivers a comprehensive review of algorithmic advancements aimed at improving LLM efficiency. Unlike other surveys that typically focus on specific areas such as training or model compression, this paper examines the multi-faceted dimensions of efficiency essential for the end-to-end algorithmic development of LLMs. Specifically, it covers various topics related to efficiency, including scaling laws, data utilization, architectural innovations, training and tuning strategies, and inference techniques. This paper aims to serve as a valuable resource for researchers and practitioners, laying the groundwork for future innovations in this critical research area. Our repository of relevant references is maintained at url{https://github.com/tding1/Efficient-LLM-Survey}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Ding",
      "Tianyi Chen",
      "Haidong Zhu",
      "Jiachen Jiang",
      "Yiqi Zhong",
      "Jinxin Zhou",
      "Guangzhi Wang",
      "Zhihui Zhu",
      "Ilya Zharkov",
      "Luming Liang"
    ]
  },
  "http://arxiv.org/abs/2312.00662": {
    "title": "Nonparametric Variational Regularisation of Pretrained Transformers",
    "volume": "Dec",
    "abstract": "The current paradigm of large-scale pre-training and fine-tuning Transformer large language models has lead to significant improvements across the board in natural language processing. However, such large models are susceptible to overfitting to their training data, and as a result the models perform poorly when the domain changes. Also, due to the model's scale, the cost of fine-tuning the model to the new domain is large. Nonparametric Variational Information Bottleneck (NVIB) has been proposed as a regulariser for training cross-attention in Transformers, potentially addressing the overfitting problem. We extend the NVIB framework to replace all types of attention functions in Transformers, and show that existing pretrained Transformers can be reinterpreted as Nonparametric Variational (NV) models using a proposed identity initialisation. We then show that changing the initialisation introduces a novel, information-theoretic post-training regularisation in the attention mechanism, which improves out-of-domain generalisation without any training. This success supports the hypothesis that pretrained Transformers are implicitly NV Bayesian models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabio Fehr",
      "James Henderson"
    ]
  },
  "http://arxiv.org/abs/2312.00584": {
    "title": "The Ethics of Automating Legal Actors",
    "volume": "Dec",
    "abstract": "The introduction of large public legal datasets has brought about a renaissance in legal NLP. Many of these datasets are comprised of legal judgements - the product of judges deciding cases. This fact, together with the way machine learning works, means that several legal NLP models are models of judges. While some have argued for the automation of judges, in this position piece, we argue that automating the role of the judge raises difficult ethical challenges, in particular for common law legal systems. Our argument follows from the social role of the judge in actively shaping the law, rather than merely applying it. Since current NLP models come nowhere close to having the facilities necessary for this task, they should not be used to automate judges. Furthermore, even in the case the models could achieve human-level capabilities, there would still be remaining ethical concerns inherent in the automation of the legal process",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Josef Valvoda",
      "Alec Thompson",
      "Ryan Cotterell",
      "Simone Teufel"
    ]
  },
  "http://arxiv.org/abs/2312.00575": {
    "title": "Instruction-tuning Aligns LLMs to the Human Brain",
    "volume": "Dec",
    "abstract": "Instruction-tuning is a widely adopted method of finetuning that enables large language models (LLMs) to generate output that more closely resembles human responses to natural language queries, in many cases leading to human-level performance on diverse testbeds. However, it remains unclear whether instruction-tuning truly makes LLMs more similar to how humans process language. We investigate the effect of instruction-tuning on LLM-human similarity in two ways: (1) brain alignment, the similarity of LLM internal representations to neural activity in the human language system, and (2) behavioral alignment, the similarity of LLM and human behavior on a reading task. We assess 25 vanilla and instruction-tuned LLMs across three datasets involving humans reading naturalistic stories and sentences. We discover that instruction-tuning generally enhances brain alignment by an average of 6%, but does not have a similar effect on behavioral alignment. To identify the factors underlying LLM-brain alignment, we compute correlations between the brain alignment of LLMs and various model properties, such as model size, various problem-solving abilities, and performance on tasks requiring world knowledge spanning various domains. Notably, we find a strong positive correlation between brain alignment and model size (r = 0.95), as well as performance on tasks requiring world knowledge (r = 0.81). Our results demonstrate that instruction-tuning LLMs improves both world knowledge representations and brain alignment, suggesting that mechanisms that encode world knowledge in LLMs also improve representational alignment to the human brain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khai Loong Aw",
      "Syrielle Montariol",
      "Badr AlKhamissi",
      "Martin Schrimpf",
      "Antoine Bosselut"
    ]
  },
  "http://arxiv.org/abs/2312.00567": {
    "title": "Explanatory Argument Extraction of Correct Answers in Resident Medical Exams",
    "volume": "Dec",
    "abstract": "Developing the required technology to assist medical experts in their everyday activities is currently a hot topic in the Artificial Intelligence research field. Thus, a number of large language models (LLMs) and automated benchmarks have recently been proposed with the aim of facilitating information extraction in Evidence-Based Medicine (EBM) using natural language as a tool for mediating in human-AI interaction. The most representative benchmarks are limited to either multiple-choice or long-form answers and are available only in English. In order to address these shortcomings, in this paper we present a new dataset which, unlike previous work: (i) includes not only explanatory arguments for the correct answer, but also arguments to reason why the incorrect answers are not correct; (ii) the explanations are written originally by medical doctors to answer questions from the Spanish Residency Medical Exams. Furthermore, this new benchmark allows us to setup a novel extractive task which consists of identifying the explanation of the correct answer written by medical doctors. An additional benefit of our setting is that we can leverage the extractive QA paradigm to automatically evaluate performance of LLMs without resorting to costly manual evaluation by medical experts. Comprehensive experimentation with language models for Spanish shows that sometimes multilingual models fare better than monolingual ones, even outperforming models which have been adapted to the medical domain. Furthermore, results across the monolingual models are mixed, with supposedly smaller and inferior models performing competitively. In any case, the obtained results show that our novel dataset and approach can be an effective technique to help medical practitioners in identifying relevant evidence-based explanations for medical questions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iakes Goenaga",
      "Aitziber Atutxa",
      "Koldo Gojenola",
      "Maite Oronoz",
      "Rodrigo Agerri"
    ]
  },
  "http://arxiv.org/abs/2312.00554": {
    "title": "Questioning Biases in Case Judgment Summaries: Legal Datasets or Large Language Models?",
    "volume": "Dec",
    "abstract": "The evolution of legal datasets and the advent of large language models (LLMs) have significantly transformed the legal field, particularly in the generation of case judgment summaries. However, a critical concern arises regarding the potential biases embedded within these summaries. This study scrutinizes the biases present in case judgment summaries produced by legal datasets and large language models. The research aims to analyze the impact of biases on legal decision making. By interrogating the accuracy, fairness, and implications of biases in these summaries, this study contributes to a better understanding of the role of technology in legal contexts and the implications for justice systems worldwide. In this study, we investigate biases wrt Gender-related keywords, Race-related keywords, Keywords related to crime against women, Country names and religious keywords. The study shows interesting evidences of biases in the outputs generated by the large language models and pre-trained abstractive summarization models. The reasoning behind these biases needs further studies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aniket Deroy",
      "Subhankar Maity"
    ]
  },
  "http://arxiv.org/abs/2312.00552": {
    "title": "Improving Unsupervised Relation Extraction by Augmenting Diverse Sentence Pairs",
    "volume": "Dec",
    "abstract": "Unsupervised relation extraction (URE) aims to extract relations between named entities from raw text without requiring manual annotations or pre-existing knowledge bases. In recent studies of URE, researchers put a notable emphasis on contrastive learning strategies for acquiring relation representations. However, these studies often overlook two important aspects: the inclusion of diverse positive pairs for contrastive learning and the exploration of appropriate loss functions. In this paper, we propose AugURE with both within-sentence pairs augmentation and augmentation through cross-sentence pairs extraction to increase the diversity of positive pairs and strengthen the discriminative power of contrastive learning. We also identify the limitation of noise-contrastive estimation (NCE) loss for relation representation learning and propose to apply margin loss for sentence pairs. Experiments on NYT-FB and TACRED datasets demonstrate that the proposed relation representation learning and a simple K-Means clustering achieves state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qing Wang",
      "Kang Zhou",
      "Qiao Qiao",
      "Yuepei Li",
      "Qi Li"
    ]
  },
  "http://arxiv.org/abs/2312.00536": {
    "title": "Trained MT Metrics Learn to Cope with Machine-translated References",
    "volume": "Dec",
    "abstract": "Neural metrics trained on human evaluations of MT tend to correlate well with human judgments, but their behavior is not fully understood. In this paper, we perform a controlled experiment and compare a baseline metric that has not been trained on human evaluations (Prism) to a trained version of the same metric (Prism+FT). Surprisingly, we find that Prism+FT becomes more robust to machine-translated references, which are a notorious problem in MT evaluation. This suggests that the effects of metric training go beyond the intended effect of improving overall correlation with human judgments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jannis Vamvas",
      "Tobias Domhan",
      "Sony Trenous",
      "Rico Sennrich",
      "Eva Hasler"
    ]
  },
  "http://arxiv.org/abs/2312.00525": {
    "title": "SurreyAI 2023 Submission for the Quality Estimation Shared Task",
    "volume": "Dec",
    "abstract": "Quality Estimation (QE) systems are important in situations where it is necessary to assess the quality of translations, but there is no reference available. This paper describes the approach adopted by the SurreyAI team for addressing the Sentence-Level Direct Assessment shared task in WMT23. The proposed approach builds upon the TransQuest framework, exploring various autoencoder pre-trained language models within the MonoTransQuest architecture using single and ensemble settings. The autoencoder pre-trained language models employed in the proposed systems are XLMV, InfoXLM-large, and XLMR-large. The evaluation utilizes Spearman and Pearson correlation coefficients, assessing the relationship between machine-predicted quality scores and human judgments for 5 language pairs (English-Gujarati, English-Hindi, English-Marathi, English-Tamil and English-Telugu). The MonoTQ-InfoXLM-large approach emerges as a robust strategy, surpassing all other individual models proposed in this study by significantly improving over the baseline for the majority of the language pairs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Archchana Sindhujan",
      "Diptesh Kanojia",
      "Constantin Orasan",
      "Tharindu Ranasinghe"
    ]
  },
  "http://arxiv.org/abs/2312.00513": {
    "title": "Summarization-based Data Augmentation for Document Classification",
    "volume": "Dec",
    "abstract": "Despite the prevalence of pretrained language models in natural language understanding tasks, understanding lengthy text such as document is still challenging due to the data sparseness problem. Inspired by that humans develop their ability of understanding lengthy text from reading shorter text, we propose a simple yet effective summarization-based data augmentation, SUMMaug, for document classification. We first obtain easy-to-learn examples for the target document classification task by summarizing the input of the original training examples, while optionally merging the original labels to conform to the summarized input. We then use the generated pseudo examples to perform curriculum learning. Experimental results on two datasets confirmed the advantage of our method compared to existing baseline methods in terms of robustness and accuracy. We release our code and data at https://github.com/etsurin/summaug",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yueguan Wang",
      "Naoki Yoshinaga"
    ]
  },
  "http://arxiv.org/abs/2312.00480": {
    "title": "Japanese Tort-case Dataset for Rationale-supported Legal Judgment Prediction",
    "volume": "Dec",
    "abstract": "This paper presents the first dataset for Japanese Legal Judgment Prediction (LJP), the Japanese Tort-case Dataset (JTD), which features two tasks: tort prediction and its rationale extraction. The rationale extraction task identifies the court's accepting arguments from alleged arguments by plaintiffs and defendants, which is a novel task in the field. JTD is constructed based on annotated 3,477 Japanese Civil Code judgments by 41 legal experts, resulting in 7,978 instances with 59,697 of their alleged arguments from the involved parties. Our baseline experiments show the feasibility of the proposed two tasks, and our error analysis by legal experts identifies sources of errors and suggests future directions of the LJP research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroaki Yamada",
      "Takenobu Tokunaga",
      "Ryutaro Ohara",
      "Akira Tokutsu",
      "Keisuke Takeshita",
      "Mihoko Sumida"
    ]
  },
  "http://arxiv.org/abs/2312.00413": {
    "title": "Abstract Syntax Tree for Programming Language Understanding and Representation: How Far Are We?",
    "volume": "Dec",
    "abstract": "Programming language understanding and representation (a.k.a code representation learning) has always been a hot and challenging task in software engineering. It aims to apply deep learning techniques to produce numerical representations of the source code features while preserving its semantics. These representations can be used for facilitating subsequent code-related tasks. The abstract syntax tree (AST), a fundamental code feature, illustrates the syntactic information of the source code and has been widely used in code representation learning. However, there is still a lack of systematic and quantitative evaluation of how well AST-based code representation facilitates subsequent code-related tasks. In this paper, we first conduct a comprehensive empirical study to explore the effectiveness of the AST-based code representation in facilitating follow-up code-related tasks. To do so, we compare the performance of models trained with code token sequence (Token for short) based code representation and AST-based code representation on three popular types of code-related tasks. Surprisingly, the overall quantitative statistical results demonstrate that models trained with AST-based code representation consistently perform worse across all three tasks compared to models trained with Token-based code representation. Our further quantitative analysis reveals that models trained with AST-based code representation outperform models trained with Token-based code representation in certain subsets of samples across all three tasks. We also conduct comprehensive experiments to evaluate and reveal the impact of the choice of AST parsing/preprocessing/encoding methods on AST-based code representation and subsequent code-related tasks. Our study provides future researchers with detailed guidance on how to select solutions at each stage to fully exploit AST",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weisong Sun",
      "Chunrong Fang",
      "Yun Miao",
      "Yudu You",
      "Mengzhe Yuan",
      "Yuchen Chen",
      "Quanjun Zhang",
      "An Guo",
      "Xiang Chen",
      "Yang Liu",
      "Zhenyu Chen"
    ]
  },
  "http://arxiv.org/abs/2312.00407": {
    "title": "CoLLiE: Collaborative Training of Large Language Models in an Efficient Way",
    "volume": "Dec",
    "abstract": "Large language models (LLMs) are increasingly pivotal in a wide range of natural language processing tasks. Access to pre-trained models, courtesy of the open-source community, has made it possible to adapt these models to specific applications for enhanced performance. However, the substantial resources required for training these models necessitate efficient solutions. This paper introduces CoLLiE, an efficient library that facilitates collaborative training of large language models using 3D parallelism, parameter-efficient fine-tuning (PEFT) methods, and optimizers such as Lion, Adan, Sophia, LOMO and AdaLomo. With its modular design and comprehensive functionality, CoLLiE offers a balanced blend of efficiency, ease of use, and customization. CoLLiE has proven superior training efficiency in comparison with prevalent solutions in pre-training and fine-tuning scenarios. Furthermore, we provide an empirical evaluation of the correlation between model size and GPU memory consumption under different optimization methods, as well as an analysis of the throughput. Lastly, we carry out a comprehensive comparison of various optimizers and PEFT methods within the instruction-tuning context. CoLLiE is available at https://github.com/OpenLMLab/collie",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Lv",
      "Shuo Zhang",
      "Tianle Gu",
      "Shuhao Xing",
      "Jiawei Hong",
      "Keyu Chen",
      "Xiaoran Liu",
      "Yuqing Yang",
      "Honglin Guo",
      "Tengxiao Liu",
      "Yu Sun",
      "Qipeng Guo",
      "Hang Yan",
      "Xipeng Qiu"
    ]
  },
  "http://arxiv.org/abs/2312.00372": {
    "title": "Event-driven Real-time Retrieval in Web Search",
    "volume": "Dec",
    "abstract": "Information retrieval in real-time search presents unique challenges distinct from those encountered in classical web search. These challenges are particularly pronounced due to the rapid change of user search intent, which is influenced by the occurrence and evolution of breaking news events, such as earthquakes, elections, and wars. Previous dense retrieval methods, which primarily focused on static semantic representation, lack the capacity to capture immediate search intent, leading to inferior performance in retrieving the most recent event-related documents in time-sensitive scenarios. To address this issue, this paper expands the query with event information that represents real-time search intent. The Event information is then integrated with the query through a cross-attention mechanism, resulting in a time-context query representation. We further enhance the model's capacity for event representation through multi-task training. Since publicly available datasets such as MS-MARCO do not contain any event information on the query side and have few time-sensitive queries, we design an automatic data collection and annotation pipeline to address this issue, which includes ModelZoo-based Coarse Annotation and LLM-driven Fine Annotation processes. In addition, we share the training tricks such as two-stage training and hard negative sampling. Finally, we conduct a set of offline experiments on a million-scale production dataset to evaluate our approach and deploy an A/B testing in a real online system to verify the performance. Extensive experimental results demonstrate that our proposed approach significantly outperforms existing state-of-the-art baseline methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nan Yang",
      "Shusen Zhang",
      "Yannan Zhang",
      "Xiaoling Bai",
      "Hualong Deng",
      "Tianhua Zhou",
      "Jin Ma"
    ]
  },
  "http://arxiv.org/abs/2312.00353": {
    "title": "On Exploring the Reasoning Capability of Large Language Models with Knowledge Graphs",
    "volume": "Dec",
    "abstract": "This paper examines the capacity of LLMs to reason with knowledge graphs using their internal knowledge graph, i.e., the knowledge graph they learned during pre-training. Two research questions are formulated to investigate the accuracy of LLMs in recalling information from pre-training knowledge graphs and their ability to infer knowledge graph relations from context. To address these questions, we employ LLMs to perform four distinct knowledge graph reasoning tasks. Furthermore, we identify two types of hallucinations that may occur during knowledge reasoning with LLMs: content and ontology hallucination. Our experimental results demonstrate that LLMs can successfully tackle both simple and complex knowledge graph reasoning tasks from their own memory, as well as infer from input context",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pei-Chi Lo",
      "Yi-Hang Tsai",
      "Ee-Peng Lim",
      "San-Yih Hwang"
    ]
  },
  "http://arxiv.org/abs/2312.00349": {
    "title": "The Case for Scalable, Data-Driven Theory: A Paradigm for Scientific Progress in NLP",
    "volume": "Dec",
    "abstract": "I propose a paradigm for scientific progress in NLP centered around developing scalable, data-driven theories of linguistic structure. The idea is to collect data in tightly scoped, carefully defined ways which allow for exhaustive annotation of behavioral phenomena of interest, and then use machine learning to construct explanatory theories of these phenomena which can form building blocks for intelligible AI systems. After laying some conceptual groundwork, I describe several investigations into data-driven theories of shallow semantic structure using Question-Answer driven Semantic Role Labeling (QA-SRL), a schema for annotating verbal predicate-argument relations using highly constrained question-answer pairs. While this only scratches the surface of the complex language behaviors of interest in AI, I outline principles for data collection and theoretical modeling which can inform future scientific progress. This note summarizes and draws heavily on my PhD thesis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julian Michael"
    ]
  },
  "http://arxiv.org/abs/2312.00347": {
    "title": "RTQ: Rethinking Video-language Understanding Based on Image-text Model",
    "volume": "Dec",
    "abstract": "Recent advancements in video-language understanding have been established on the foundation of image-text models, resulting in promising outcomes due to the shared knowledge between images and videos. However, video-language understanding presents unique challenges due to the inclusion of highly complex semantic details, which result in information redundancy, temporal dependency, and scene complexity. Current techniques have only partially tackled these issues, and our quantitative analysis indicates that some of these methods are complementary. In light of this, we propose a novel framework called RTQ (Refine, Temporal model, and Query), which addresses these challenges simultaneously. The approach involves refining redundant information within frames, modeling temporal relations among frames, and querying task-specific information from the videos. Remarkably, our model demonstrates outstanding performance even in the absence of video-language pre-training, and the results are comparable with or superior to those achieved by state-of-the-art pre-training methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Wang",
      "Yaoyu Li",
      "Tian Gan",
      "Zheng Zhang",
      "Jingjing Lv",
      "Liqiang Nie"
    ]
  },
  "http://arxiv.org/abs/2312.00326": {
    "title": "Agent-OM: Leveraging Large Language Models for Ontology Matching",
    "volume": "Dec",
    "abstract": "Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM-based agents have become revolutionary in data engineering and have been applied creatively in various domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With thoughtful consideration of several specific challenges to leverage LLMs for OM, we propose a generic framework, namely Agent-OM, consisting of two Siamese agents for retrieval and matching, with a set of simple prompt-based OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve very close results to the best long-standing performance on simple OM tasks and significantly improve the performance on complex and few-shot OM tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangcheng Qiang",
      "Weiqing Wang",
      "Kerry Taylor"
    ]
  },
  "http://arxiv.org/abs/2312.00293": {
    "title": "PsyAttention: Psychological Attention Model for Personality Detection",
    "volume": "Dec",
    "abstract": "Work on personality detection has tended to incorporate psychological features from different personality models, such as BigFive and MBTI. There are more than 900 psychological features, each of which is helpful for personality detection. However, when used in combination, the application of different calculation standards among these features may result in interference between features calculated using distinct systems, thereby introducing noise and reducing performance. This paper adapts different psychological models in the proposed PsyAttention for personality detection, which can effectively encode psychological features, reducing their number by 85%. In experiments on the BigFive and MBTI models, PysAttention achieved average accuracy of 65.66% and 86.30%, respectively, outperforming state-of-the-art methods, indicating that it is effective at encoding psychological features",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baohua Zhang",
      "Yongyi Huang",
      "Wenyao Cui",
      "Huaping Zhang",
      "Jianyun Shang"
    ]
  },
  "http://arxiv.org/abs/2312.00292": {
    "title": "SEPSIS: I Can Catch Your Lies -- A New Paradigm for Deception Detection",
    "volume": "Dec",
    "abstract": "Deception is the intentional practice of twisting information. It is a nuanced societal practice deeply intertwined with human societal evolution, characterized by a multitude of facets. This research explores the problem of deception through the lens of psychology, employing a framework that categorizes deception into three forms: lies of omission, lies of commission, and lies of influence. The primary focus of this study is specifically on investigating only lies of omission. We propose a novel framework for deception detection leveraging NLP techniques. We curated an annotated dataset of 876,784 samples by amalgamating a popular large-scale fake news dataset and scraped news headlines from the Twitter handle of Times of India, a well-known Indian news media house. Each sample has been labeled with four layers, namely: (i) the type of omission (speculation, bias, distortion, sounds factual, and opinion), (ii) colors of lies(black, white, etc), and (iii) the intention of such lies (to influence, etc) (iv) topic of lies (political, educational, religious, etc). We present a novel multi-task learning pipeline that leverages the dataless merging of fine-tuned language models to address the deception detection task mentioned earlier. Our proposed model achieved an F1 score of 0.87, demonstrating strong performance across all layers including the type, color, intent, and topic aspects of deceptive content. Finally, our research explores the relationship between lies of omission and propaganda techniques. To accomplish this, we conducted an in-depth analysis, uncovering compelling findings. For instance, our analysis revealed a significant correlation between loaded language and opinion, shedding light on their interconnectedness. To encourage further research in this field, we will be making the models and dataset available with the MIT License, making it favorable for open-source research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anku Rani",
      "Dwip Dalal",
      "Shreya Gautam",
      "Pankaj Gupta",
      "Vinija Jain",
      "Aman Chadha",
      "Amit Sheth",
      "Amitava Das"
    ]
  },
  "http://arxiv.org/abs/2312.00277": {
    "title": "Text Attribute Control via Closed-Loop Disentanglement",
    "volume": "Dec",
    "abstract": "Changing an attribute of a text without changing the content usually requires to first disentangle the text into irrelevant attributes and content representations. After that, in the inference phase, the representation of one attribute is tuned to a different value, expecting that the corresponding attribute of the text can also be changed accordingly. The usual way of disentanglement is to add some constraints on the latent space of an encoder-decoder architecture, including adversarial-based constraints and mutual-information-based constraints. However, the previous semi-supervised processes of attribute change are usually not enough to guarantee the success of attribute change and content preservation. In this paper, we propose a novel approach to achieve a robust control of attributes while enhancing content preservation. In this approach, we use a semi-supervised contrastive learning method to encourage the disentanglement of attributes in latent spaces. Differently from previous works, we re-disentangle the reconstructed sentence and compare the re-disentangled latent space with the original latent space, which makes a closed-loop disentanglement process. This also helps content preservation. In addition, the contrastive learning method is also able to replace the role of minimizing mutual information and adversarial training in the disentanglement process, which alleviates the computation cost. We conducted experiments on three text datasets, including the Yelp Service review dataset, the Amazon Product review dataset, and the GoEmotions dataset. The experimental results show the effectiveness of our model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Sha",
      "Thomas Lukasiewicz"
    ]
  },
  "http://arxiv.org/abs/2312.00273": {
    "title": "Mark My Words: Analyzing and Evaluating Language Model Watermarks",
    "volume": "Dec",
    "abstract": "The capabilities of large language models have grown significantly in recent years and so too have concerns about their misuse. In this context, the ability to distinguish machine-generated text from human-authored content becomes important. Prior works have proposed numerous schemes to watermark text, which would benefit from a systematic evaluation framework. This work focuses on text watermarking techniques - as opposed to image watermarks - and proposes a comprehensive benchmark for them under different tasks as well as practical attacks. We focus on three main metrics: quality, size (e.g. the number of tokens needed to detect a watermark), and tamper-resistance. Current watermarking techniques are good enough to be deployed: Kirchenbauer et al. can watermark Llama2-7B-chat with no perceivable loss in quality in under 100 tokens, and with good tamper-resistance to simple attacks, regardless of temperature. We argue that watermark indistinguishability is too strong a requirement: schemes that slightly modify logit distributions outperform their indistinguishable counterparts with no noticeable loss in generation quality. We publicly release our benchmark",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julien Piet",
      "Chawin Sitawarin",
      "Vivian Fang",
      "Norman Mu",
      "David Wagner"
    ]
  },
  "http://arxiv.org/abs/2312.00220": {
    "title": "Multi-Modal Video Topic Segmentation with Dual-Contrastive Domain Adaptation",
    "volume": "Nov",
    "abstract": "Video topic segmentation unveils the coarse-grained semantic structure underlying videos and is essential for other video understanding tasks. Given the recent surge in multi-modal, relying solely on a single modality is arguably insufficient. On the other hand, prior solutions for similar tasks like video scene/shot segmentation cater to short videos with clear visual shifts but falter for long videos with subtle changes, such as livestreams. In this paper, we introduce a multi-modal video topic segmenter that utilizes both video transcripts and frames, bolstered by a cross-modal attention mechanism. Furthermore, we propose a dual-contrastive learning framework adhering to the unsupervised domain adaptation paradigm, enhancing our model's adaptability to longer, more semantically complex videos. Experiments on short and long video corpora demonstrate that our proposed solution, significantly surpasses baseline methods in terms of both accuracy and transferability, in both intra- and cross-domain settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linzi Xing",
      "Quan Tran",
      "Fabian Caba",
      "Franck Dernoncourt",
      "Seunghyun Yoon",
      "Zhaowen Wang",
      "Trung Bui",
      "Giuseppe Carenini"
    ]
  },
  "http://arxiv.org/abs/2312.00214": {
    "title": "Relevance-guided Neural Machine Translation",
    "volume": "Nov",
    "abstract": "With the advent of the Transformer architecture, Neural Machine Translation (NMT) results have shown great improvement lately. However, results in low-resource conditions still lag behind in both bilingual and multilingual setups, due to the limited amount of available monolingual and/or parallel data; hence, the need for methods addressing data scarcity in an efficient, and explainable way, is eminent. We propose an explainability-based training approach for NMT, applied in Unsupervised and Supervised model training, for translation of three languages of varying resources, French, Gujarati, Kazakh, to and from English. Our results show our method can be promising, particularly when training in low-resource conditions, outperforming simple training baselines; though the improvement is marginal, it sets the ground for further exploration of the approach and the parameters, and its extension to other languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isidora Chara Tourni",
      "Derry Wijaya"
    ]
  },
  "http://arxiv.org/abs/2312.00194": {
    "title": "Robust Concept Erasure via Kernelized Rate-Distortion Maximization",
    "volume": "Nov",
    "abstract": "Distributed representations provide a vector space that captures meaningful relationships between data instances. The distributed nature of these representations, however, entangles together multiple attributes or concepts of data instances (e.g., the topic or sentiment of a text, characteristics of the author (age, gender, etc), etc). Recent work has proposed the task of concept erasure, in which rather than making a concept predictable, the goal is to remove an attribute from distributed representations while retaining other information from the original representation space as much as possible. In this paper, we propose a new distance metric learning-based objective, the Kernelized Rate-Distortion Maximizer (KRaM), for performing concept erasure. KRaM fits a transformation of representations to match a specified distance measure (defined by a labeled concept to erase) using a modified rate-distortion function. Specifically, KRaM's objective function aims to make instances with similar concept labels dissimilar in the learned representation space while retaining other information. We find that optimizing KRaM effectively erases various types of concepts: categorical, continuous, and vector-valued variables from data representations across diverse domains. We also provide a theoretical analysis of several properties of KRaM's objective. To assess the quality of the learned representations, we propose an alignment score to evaluate their similarity with the original representation space. Additionally, we conduct experiments to showcase KRaM's efficacy in various settings, from erasing binary gender variables in word embeddings to vector-valued variables in GPT-3 representations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Somnath Basu Roy Chowdhury",
      "Nicholas Monath",
      "Avinava Dubey",
      "Amr Ahmed",
      "Snigdha Chaturvedi"
    ]
  },
  "http://arxiv.org/abs/2312.00174": {
    "title": "Compression of end-to-end non-autoregressive image-to-speech system for low-resourced devices",
    "volume": "Nov",
    "abstract": "People with visual impairments have difficulty accessing touchscreen-enabled personal computing devices like mobile phones and laptops. The image-to-speech (ITS) systems can assist them in mitigating this problem, but their huge model size makes it extremely hard to be deployed on low-resourced embedded devices. In this paper, we aim to overcome this challenge by developing an efficient endto-end neural architecture for generating audio from tiny segments of display content on low-resource devices. We introduced a vision transformers-based image encoder and utilized knowledge distillation to compress the model from 6.1 million to 2.46 million parameters. Human and automatic evaluation results show that our approach leads to a very minimal drop in performance and can speed up the inference time by 22%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gokul Srinivasagan",
      "Michael Deisher",
      "Munir Georges"
    ]
  },
  "http://arxiv.org/abs/2312.00168": {
    "title": "Navigating News Narratives: A Media Bias Analysis Dataset",
    "volume": "Nov",
    "abstract": "The proliferation of biased news narratives across various media platforms has become a prominent challenge, influencing public opinion on critical topics like politics, health, and climate change. This paper introduces the \"Navigating News Narratives: A Media Bias Analysis Dataset\", a comprehensive dataset to address the urgent need for tools to detect and analyze media bias. This dataset encompasses a broad spectrum of biases, making it a unique and valuable asset in the field of media studies and artificial intelligence. The dataset is available at https://figshare.com/articles/dataset/news-media-bias_data_json/24422122",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaina Raza"
    ]
  },
  "http://arxiv.org/abs/2312.00115": {
    "title": "A Video is Worth 10,000 Words: Training and Benchmarking with Diverse Captions for Better Long Video Retrieval",
    "volume": "Nov",
    "abstract": "Existing long video retrieval systems are trained and tested in the paragraph-to-video retrieval regime, where every long video is described by a single long paragraph. This neglects the richness and variety of possible valid descriptions of a video, which could be described in moment-by-moment detail, or in a single phrase summary, or anything in between. To provide a more thorough evaluation of the capabilities of long video retrieval systems, we propose a pipeline that leverages state-of-the-art large language models to carefully generate a diverse set of synthetic captions for long videos. We validate this pipeline's fidelity via rigorous human inspection. We then benchmark a representative set of video language models on these synthetic captions using a few long video datasets, showing that they struggle with the transformed data, especially the shortest captions. We also propose a lightweight fine-tuning method, where we use a contrastive loss to learn a hierarchical embedding loss based on the differing levels of information among the various captions. Our method improves performance both on the downstream paragraph-to-video retrieval task (+1.1% R@1 on ActivityNet), as well as for the various long video retrieval metrics we compute using our synthetic data (+3.6% R@1 for short descriptions on ActivityNet). For data access and other details, please refer to our project website at https://mgwillia.github.io/10k-words",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Gwilliam",
      "Michael Cogswell",
      "Meng Ye",
      "Karan Sikka",
      "Abhinav Shrivastava",
      "Ajay Divakaran"
    ]
  },
  "http://arxiv.org/abs/2311.18812": {
    "title": "What Do Llamas Really Think? Revealing Preference Biases in Language Model Representations",
    "volume": "Nov",
    "abstract": "Do large language models (LLMs) exhibit sociodemographic biases, even when they decline to respond? To bypass their refusal to \"speak,\" we study this research question by probing contextualized embeddings and exploring whether this bias is encoded in its latent representations. We propose a logistic Bradley-Terry probe which predicts word pair preferences of LLMs from the words' hidden vectors. We first validate our probe on three pair preference tasks and thirteen LLMs, where we outperform the word embedding association test (WEAT), a standard approach in testing for implicit association, by a relative 27% in error rate. We also find that word pair preferences are best represented in the middle layers. Next, we transfer probes trained on harmless tasks (e.g., pick the larger number) to controversial ones (compare ethnicities) to examine biases in nationality, politics, religion, and gender. We observe substantial bias for all target classes: for instance, the Mistral model implicitly prefers Europe to Africa, Christianity to Judaism, and left-wing to right-wing politics, despite declining to answer. This suggests that instruction fine-tuning does not necessarily debias contextualized embeddings. Our codebase is at https://github.com/castorini/biasprobe",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raphael Tang",
      "Xinyu Zhang",
      "Jimmy Lin",
      "Ferhan Ture"
    ]
  },
  "http://arxiv.org/abs/2311.18805": {
    "title": "Unnatural Error Correction: GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text",
    "volume": "Nov",
    "abstract": "While Large Language Models (LLMs) have achieved remarkable performance in many tasks, much about their inner workings remains unclear. In this study, we present novel experimental insights into the resilience of LLMs, particularly GPT-4, when subjected to extensive character-level permutations. To investigate this, we first propose the Scrambled Bench, a suite designed to measure the capacity of LLMs to handle scrambled input, in terms of both recovering scrambled sentences and answering questions given scrambled context. The experimental results indicate that most powerful LLMs demonstrate the capability akin to typoglycemia, a phenomenon where humans can understand the meaning of words even when the letters within those words are scrambled, as long as the first and last letters remain in place. More surprisingly, we found that only GPT-4 nearly flawlessly processes inputs with unnatural errors, even under the extreme condition, a task that poses significant challenges for other LLMs and often even for humans. Specifically, GPT-4 can almost perfectly reconstruct the original sentences from scrambled ones, decreasing the edit distance by 95%, even when all letters within each word are entirely scrambled. It is counter-intuitive that LLMs can exhibit such resilience despite severe disruption to input tokenization caused by scrambled text",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Cao",
      "Takeshi Kojima",
      "Yutaka Matsuo",
      "Yusuke Iwasawa"
    ]
  },
  "http://arxiv.org/abs/2311.18803": {
    "title": "BioCLIP: A Vision Foundation Model for the Tree of Life",
    "volume": "Dec",
    "abstract": "Images of the natural world, collected by a variety of cameras, from drones to individual phones, are increasingly abundant sources of biological information. There is an explosion of computational methods and tools, particularly computer vision, for extracting biologically relevant information from images for science and conservation. Yet most of these are bespoke approaches designed for a specific task and are not easily adaptable or extendable to new questions, contexts, and datasets. A vision model for general organismal biology questions on images is of timely need. To approach this, we curate and release TreeOfLife-10M, the largest and most diverse ML-ready dataset of biology images. We then develop BioCLIP, a foundation model for the tree of life, leveraging the unique properties of biology captured by TreeOfLife-10M, namely the abundance and variety of images of plants, animals, and fungi, together with the availability of rich structured biological knowledge. We rigorously benchmark our approach on diverse fine-grained biology classification tasks, and find that BioCLIP consistently and substantially outperforms existing baselines (by 17% to 20% absolute). Intrinsic evaluation reveals that BioCLIP has learned a hierarchical representation conforming to the tree of life, shedding light on its strong generalizability. Our code, models and data will be made available at https://github.com/Imageomics/bioclip",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Stevens",
      "Jiaman Wu",
      "Matthew J Thompson",
      "Elizabeth G Campolongo",
      "Chan Hee Song",
      "David Edward Carlyn",
      "Li Dong",
      "Wasila M Dahdul",
      "Charles Stewart",
      "Tanya Berger-Wolf",
      "Wei-Lun Chao",
      "Yu Su"
    ]
  },
  "http://arxiv.org/abs/2311.18799": {
    "title": "X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning",
    "volume": "Nov",
    "abstract": "Vision-language pre-training and instruction tuning have demonstrated general-purpose capabilities in 2D visual reasoning tasks by aligning visual encoders with state-of-the-art large language models (LLMs). In this paper, we introduce a simple, yet effective, cross-modality framework built atop frozen LLMs that allows the integration of various modalities without extensive modality-specific customization. To facilitate instruction-modality fine-tuning, we collect high-quality instruction tuning data in an automatic and scalable manner, composed of 24K QA samples for audio and 250K QA samples for 3D. Leveraging instruction-aware representations, our model performs comparably with leading-edge counterparts without the need of extensive modality-specific pre-training or customization. Furthermore, our approach demonstrates cross-modal reasoning abilities across two or more input modalities, despite each modality projection being trained individually. To study the model's cross-modal abilities, we contribute a novel Discriminative Cross-modal Reasoning (DisCRn) evaluation task, comprising 9K audio-video QA samples and 28K image-3D QA samples that require the model to reason discriminatively across disparate input modalities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Artemis Panagopoulou",
      "Le Xue",
      "Ning Yu",
      "Junnan Li",
      "Dongxu Li",
      "Shafiq Joty",
      "Ran Xu",
      "Silvio Savarese",
      "Caiming Xiong",
      "Juan Carlos Niebles"
    ]
  },
  "http://arxiv.org/abs/2311.18778": {
    "title": "Mavericks at BLP-2023 Task 1: Ensemble-based Approach Using Language Models for Violence Inciting Text Detection",
    "volume": "Nov",
    "abstract": "This paper presents our work for the Violence Inciting Text Detection shared task in the First Workshop on Bangla Language Processing. Social media has accelerated the propagation of hate and violence-inciting speech in society. It is essential to develop efficient mechanisms to detect and curb the propagation of such texts. The problem of detecting violence-inciting texts is further exacerbated in low-resource settings due to sparse research and less data. The data provided in the shared task consists of texts in the Bangla language, where each example is classified into one of the three categories defined based on the types of violence-inciting texts. We try and evaluate several BERT-based models, and then use an ensemble of the models as our final submission. Our submission is ranked 10th in the final leaderboard of the shared task with a macro F1 score of 0.737",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saurabh Page",
      "Sudeep Mangalvedhekar",
      "Kshitij Deshpande",
      "Tanmay Chavan",
      "Sheetal Sonawane"
    ]
  },
  "http://arxiv.org/abs/2311.18775": {
    "title": "CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation",
    "volume": "Nov",
    "abstract": "We present CoDi-2, a versatile and interactive Multimodal Large Language Model (MLLM) that can follow complex multimodal interleaved instructions, conduct in-context learning (ICL), reason, chat, edit, etc., in an any-to-any input-output modality paradigm. By aligning modalities with language for both encoding and generation, CoDi-2 empowers Large Language Models (LLMs) to not only understand complex modality-interleaved instructions and in-context examples, but also autoregressively generate grounded and coherent multimodal outputs in the continuous feature space. To train CoDi-2, we build a large-scale generation dataset encompassing in-context multimodal instructions across text, vision, and audio. CoDi-2 demonstrates a wide range of zero-shot capabilities for multimodal generation, such as in-context learning, reasoning, and compositionality of any-to-any modality generation through multi-round interactive conversation. CoDi-2 surpasses previous domain-specific models on tasks such as subject-driven image generation, vision transformation, and audio editing. CoDi-2 signifies a substantial breakthrough in developing a comprehensive multimodal foundation model adept at interpreting in-context language-vision-audio interleaved instructions and producing multimodal outputs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zineng Tang",
      "Ziyi Yang",
      "Mahmoud Khademi",
      "Yang Liu",
      "Chenguang Zhu",
      "Mohit Bansal"
    ]
  },
  "http://arxiv.org/abs/2311.18765": {
    "title": "MLLMs-Augmented Visual-Language Representation Learning",
    "volume": "Dec",
    "abstract": "Visual-language pre-training (VLP) have achieved remarkable success in multi-modal tasks, largely attributed to the availability of large-scale image-text datasets. In this work, we demonstrate that multi-modal large language models (MLLMs) can enhance visual-language representation learning by improving data quality. Our approach is simple, utilizing MLLMs to extend multiple captions for each image. To prevent the bias that introduced by MLLMs' hallucinations and intrinsic caption styles, we propose a \"text shearing\" to keep the lengths of extended captions identical to the originals. In image-text retrieval, our method consistently obtains 5.6 ~ 35.0% and 16.8 ~ 46.1% improvement on R@1 under the fine-tuning and zero-shot settings, respectively. Notably, our zero-shot results are comparable to fine-tuning on target datasets, which encourages more exploration on the versatile use of MLLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanqing Liu",
      "Kai Wang",
      "Wenqi Shao",
      "Ping Luo",
      "Yu Qiao",
      "Mike Zheng Shou",
      "Kaipeng Zhang",
      "Yang You"
    ]
  },
  "http://arxiv.org/abs/2311.18761": {
    "title": "Can training neural language models on a curriculum with developmentally plausible data improve alignment with human reading behavior?",
    "volume": "Nov",
    "abstract": "The use of neural language models to model human behavior has met with mixed success. While some work has found that the surprisal estimates from these models can be used to predict a wide range of human neural and behavioral responses, other work studying more complex syntactic phenomena has found that these surprisal estimates generate incorrect behavioral predictions. This paper explores the extent to which the misalignment between empirical and model-predicted behavior can be minimized by training models on more developmentally plausible data, such as in the BabyLM Challenge. We trained teacher language models on the BabyLM \"strict-small\" dataset and used sentence level surprisal estimates from these teacher models to create a curriculum. We found tentative evidence that our curriculum made it easier for models to acquire linguistic knowledge from the training data: on the subset of tasks in the BabyLM challenge suite evaluating models' grammatical knowledge of English, models first trained on the BabyLM data curriculum and then on a few randomly ordered training epochs performed slightly better than models trained on randomly ordered epochs alone. This improved linguistic knowledge acquisition did not result in better alignment with human reading behavior, however: models trained on the BabyLM dataset (with or without a curriculum) generated predictions that were as misaligned with human behavior as models trained on larger less curated datasets. This suggests that training on developmentally plausible datasets alone is likely insufficient to generate language models capable of accurately predicting human language processing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aryaman Chobey",
      "Oliver Smith",
      "Anzi Wang",
      "Grusha Prasad"
    ]
  },
  "http://arxiv.org/abs/2311.18760": {
    "title": "TaskBench: Benchmarking Large Language Models for Task Automation",
    "volume": "Nov",
    "abstract": "Recently, the incredible progress of large language models (LLMs) has ignited the spark of task automation, which decomposes the complex tasks described by user instructions into sub-tasks, and invokes external tools to execute them, and plays a central role in autonomous agents. However, there lacks a systematic and standardized benchmark to foster the development of LLMs in task automation. To this end, we introduce TaskBench to evaluate the capability of LLMs in task automation. Specifically, task automation can be formulated into three critical stages: task decomposition, tool invocation, and parameter prediction to fulfill user intent. This complexity makes data collection and evaluation more challenging compared to common NLP tasks. To generate high-quality evaluation datasets, we introduce the concept of Tool Graph to represent the decomposed tasks in user intent, and adopt a back-instruct method to simulate user instruction and annotations. Furthermore, we propose TaskEval to evaluate the capability of LLMs from different aspects, including task decomposition, tool invocation, and parameter prediction. Experimental results demonstrate that TaskBench can effectively reflects the capability of LLMs in task automation. Benefiting from the mixture of automated data construction and human verification, TaskBench achieves a high consistency compared to the human evaluation, which can be utilized as a comprehensive and faithful benchmark for LLM-based autonomous agents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongliang Shen",
      "Kaitao Song",
      "Xu Tan",
      "Wenqi Zhang",
      "Kan Ren",
      "Siyu Yuan",
      "Weiming Lu",
      "Dongsheng Li",
      "Yueting Zhuang"
    ]
  },
  "http://arxiv.org/abs/2311.18751": {
    "title": "Language Model Agents Suffer from Compositional Generalization in Web Automation",
    "volume": "Nov",
    "abstract": "Language model agents (LMA) recently emerged as a promising paradigm on muti-step decision making tasks, often outperforming humans and other reinforcement learning agents. Despite the promise, their performance on real-world applications that often involve combinations of tasks is still underexplored. In this work, we introduce a new benchmark, called CompWoB -- 50 new compositional web automation tasks reflecting more realistic assumptions. We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks. On the other hand, transferred LMAs (finetuned only on base tasks) show less generalization gap, dropping from 85.4% to 54.8%. By balancing data distribution across tasks, we train a new model, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.5%). While these highlight the promise of small-scale finetuned and transferred models for compositional generalization, their performance further degrades under different instruction compositions changing combinational order. In contrast to the recent remarkable success of LMA, our benchmark and detailed analysis emphasize the necessity of building LMAs that are robust and generalizable to task compositionality for real-world deployment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroki Furuta",
      "Yutaka Matsuo",
      "Aleksandra Faust",
      "Izzeddin Gur"
    ]
  },
  "http://arxiv.org/abs/2311.18743": {
    "title": "AlignBench: Benchmarking Chinese Alignment of Large Language Models",
    "volume": "Dec",
    "abstract": "Alignment has become a critical step for instruction-tuned Large Language Models (LLMs) to become helpful assistants. However, effective evaluation of alignment for emerging Chinese LLMs is still significantly lacking, calling for real-scenario grounded, open-ended, challenging and automatic evaluations tailored for alignment. To fill in this gap, we introduce AlignBench, a comprehensive multi-dimensional benchmark for evaluating LLMs' alignment in Chinese. Equipped with a human-in-the-loop data curation pipeline, our benchmark employs a rule-calibrated multi-dimensional LLM-as-Judge with Chain-of-Thought to generate explanations and final ratings as evaluations, ensuring high reliability and interpretability. Furthermore, we developed a dedicated companion evaluator LLM -- CritiqueLLM, which recovers 95\\% of GPT-4's evaluation ability and will be provided via public APIs to researchers for evaluation of alignment in Chinese LLMs. All evaluation codes, data, and LLM generations are available at \\url{https://github.com/THUDM/AlignBench}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Liu",
      "Xuanyu Lei",
      "Shengyuan Wang",
      "Yue Huang",
      "Zhuoer Feng",
      "Bosi Wen",
      "Jiale Cheng",
      "Pei Ke",
      "Yifan Xu",
      "Weng Lam Tam",
      "Xiaohan Zhang",
      "Lichao Sun",
      "Hongning Wang",
      "Jing Zhang",
      "Minlie Huang",
      "Yuxiao Dong",
      "Jie Tang"
    ]
  },
  "http://arxiv.org/abs/2311.18739": {
    "title": "Mavericks at NADI 2023 Shared Task: Unravelling Regional Nuances through Dialect Identification using Transformer-based Approach",
    "volume": "Nov",
    "abstract": "In this paper, we present our approach for the \"Nuanced Arabic Dialect Identification (NADI) Shared Task 2023\". We highlight our methodology for subtask 1 which deals with country-level dialect identification. Recognizing dialects plays an instrumental role in enhancing the performance of various downstream NLP tasks such as speech recognition and translation. The task uses the Twitter dataset (TWT-2023) that encompasses 18 dialects for the multi-class classification problem. Numerous transformer-based models, pre-trained on Arabic language, are employed for identifying country-level dialects. We fine-tune these state-of-the-art models on the provided dataset. The ensembling method is leveraged to yield improved performance of the system. We achieved an F1-score of 76.65 (11th rank on the leaderboard) on the test dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vedant Deshpande",
      "Yash Patwardhan",
      "Kshitij Deshpande",
      "Sudeep Mangalvedhekar",
      "Ravindra Murumkar"
    ]
  },
  "http://arxiv.org/abs/2311.18730": {
    "title": "Mavericks at ArAIEval Shared Task: Towards a Safer Digital Space -- Transformer Ensemble Models Tackling Deception and Persuasion",
    "volume": "Nov",
    "abstract": "In this paper, we highlight our approach for the \"Arabic AI Tasks Evaluation (ArAiEval) Shared Task 2023\". We present our approaches for task 1-A and task 2-A of the shared task which focus on persuasion technique detection and disinformation detection respectively. Detection of persuasion techniques and disinformation has become imperative to avoid distortion of authentic information. The tasks use multigenre snippets of tweets and news articles for the given binary classification problem. We experiment with several transformer-based models that were pre-trained on the Arabic language. We fine-tune these state-of-the-art models on the provided dataset. Ensembling is employed to enhance the performance of the systems. We achieved a micro F1-score of 0.742 on task 1-A (8th rank on the leaderboard) and 0.901 on task 2-A (7th rank on the leaderboard) respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sudeep Mangalvedhekar",
      "Kshitij Deshpande",
      "Yash Patwardhan",
      "Vedant Deshpande",
      "Ravindra Murumkar"
    ]
  },
  "http://arxiv.org/abs/2311.18727": {
    "title": "Automatic Functional Differentiation in JAX",
    "volume": "Nov",
    "abstract": "We extend JAX with the capability to automatically differentiate higher-order functions (functionals and operators). By representing functions as a generalization of arrays, we seamlessly use JAX's existing primitive system to implement higher-order functions. We present a set of primitive operators that serve as foundational building blocks for constructing several key types of functionals. For every introduced primitive operator, we derive and implement both linearization and transposition rules, aligning with JAX's internal protocols for forward and reverse mode automatic differentiation. This enhancement allows for functional differentiation in the same syntax traditionally use for functions. The resulting functional gradients are themselves functions ready to be invoked in python. We showcase this tool's efficacy and simplicity through applications where functional derivatives are indispensable. The source code of this work is released at https://github.com/sail-sg/autofd",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Min Lin"
    ]
  },
  "http://arxiv.org/abs/2311.18712": {
    "title": "CoRec: An Easy Approach for Coordination Recognition",
    "volume": "Nov",
    "abstract": "In this paper, we observe and address the challenges of the coordination recognition task. Most existing methods rely on syntactic parsers to identify the coordinators in a sentence and detect the coordination boundaries. However, state-of-the-art syntactic parsers are slow and suffer from errors, especially for long and complicated sentences. To better solve the problems, we propose a pipeline model COordination RECognizer (CoRec). It consists of two components: coordinator identifier and conjunct boundary detector. The experimental results on datasets from various domains demonstrate the effectiveness and efficiency of the proposed method. Further experiments show that CoRec positively impacts downstream tasks, improving the yield of state-of-the-art Open IE models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qing Wang",
      "Haojie Jia",
      "Wenfei Song",
      "Qi Li"
    ]
  },
  "http://arxiv.org/abs/2311.18711": {
    "title": "Women Are Beautiful, Men Are Leaders: Gender Stereotypes in Machine Translation and Language Modeling",
    "volume": "Nov",
    "abstract": "We present GEST -- a new dataset for measuring gender-stereotypical reasoning in masked LMs and English-to-X machine translation systems. GEST contains samples that are compatible with 9 Slavic languages and English for 16 gender stereotypes about men and women (e.g., Women are beautiful, Men are leaders). The definition of said stereotypes was informed by gender experts. We used GEST to evaluate 11 masked LMs and 4 machine translation systems. We discovered significant and consistent amounts of stereotypical reasoning in almost all the evaluated models and languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mat Pikuliak",
      "Andrea Hrckova",
      "Stefan Oresko",
      "Marin imko"
    ]
  },
  "http://arxiv.org/abs/2311.18702": {
    "title": "CritiqueLLM: Scaling LLM-as-Critic for Effective and Explainable Evaluation of Large Language Model Generation",
    "volume": "Nov",
    "abstract": "Since the natural language processing (NLP) community started to make large language models (LLMs), such as GPT-4, act as a critic to evaluate the quality of generated texts, most of them only train a critique generation model of a specific scale on specific datasets. We argue that a comprehensive investigation on the key factor of LLM-based evaluation models, such as scaling properties, is lacking, so that it is still inconclusive whether these models have potential to replace GPT-4's evaluation in practical scenarios. In this paper, we propose a new critique generation model called CritiqueLLM, which includes a dialogue-based prompting method for high-quality referenced / reference-free evaluation data. Experimental results show that our model can achieve comparable evaluation performance to GPT-4 especially in system-level correlations, and even outperform GPT-4 in 3 out of 8 tasks in a challenging reference-free setting. We conduct detailed analysis to show promising scaling properties of our model in the quality of generated critiques. We also demonstrate that our generated critiques can act as scalable feedback to directly improve the generation quality of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pei Ke",
      "Bosi Wen",
      "Zhuoer Feng",
      "Xiao Liu",
      "Xuanyu Lei",
      "Jiale Cheng",
      "Shengyuan Wang",
      "Aohan Zeng",
      "Yuxiao Dong",
      "Hongning Wang",
      "Jie Tang",
      "Minlie Huang"
    ]
  },
  "http://arxiv.org/abs/2311.18681": {
    "title": "RaDialog: A Large Vision-Language Model for Radiology Report Generation and Conversational Assistance",
    "volume": "Nov",
    "abstract": "Conversational AI tools that can generate and discuss clinically correct radiology reports for a given medical image have the potential to transform radiology. Such a human-in-the-loop radiology assistant could facilitate a collaborative diagnostic process, thus saving time and improving the quality of reports. Towards this goal, we introduce RaDialog, the first thoroughly evaluated and publicly available large vision-language model for radiology report generation and interactive dialog. RaDialog effectively integrates visual image features and structured pathology findings with a large language model (LLM) while simultaneously adapting it to a specialized domain using parameter-efficient fine-tuning. To keep the conversational abilities of the underlying LLM, we propose a comprehensive, semi-automatically labeled, image-grounded instruct dataset for chest X-ray radiology tasks. By training with this dataset, our method achieves state-of-the-art clinical correctness in report generation and shows impressive abilities in interactive tasks such as correcting reports and answering questions, serving as a foundational step toward clinical dialog systems. Our code is available on github: https://github.com/ChantalMP/RaDialog",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chantal Pellegrini",
      "Ege zsoy",
      "Benjamin Busam",
      "Nassir Navab",
      "Matthias Keicher"
    ]
  },
  "http://arxiv.org/abs/2311.18658": {
    "title": "ArcMMLU: A Library and Information Science Benchmark for Large Language Models",
    "volume": "Nov",
    "abstract": "In light of the rapidly evolving capabilities of large language models (LLMs), it becomes imperative to develop rigorous domain-specific evaluation benchmarks to accurately assess their capabilities. In response to this need, this paper introduces ArcMMLU, a specialized benchmark tailored for the Library & Information Science (LIS) domain in Chinese. This benchmark aims to measure the knowledge and reasoning capability of LLMs within four key sub-domains: Archival Science, Data Science, Library Science, and Information Science. Following the format of MMLU/CMMLU, we collected over 6,000 high-quality questions for the compilation of ArcMMLU. This extensive compilation can reflect the diverse nature of the LIS domain and offer a robust foundation for LLM evaluation. Our comprehensive evaluation reveals that while most mainstream LLMs achieve an average accuracy rate above 50% on ArcMMLU, there remains a notable performance gap, suggesting substantial headroom for refinement in LLM capabilities within the LIS domain. Further analysis explores the effectiveness of few-shot examples on model performance and highlights challenging questions where models consistently underperform, providing valuable insights for targeted improvements. ArcMMLU fills a critical gap in LLM evaluations within the Chinese LIS domain and paves the way for future development of LLMs tailored to this specialized area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shitou Zhang",
      "Zuchao Li",
      "Xingshen Liu",
      "Liming Yang",
      "Ping Wang"
    ]
  },
  "http://arxiv.org/abs/2312.00100": {
    "title": "Introducing Rhetorical Parallelism Detection: A New Task with Datasets, Metrics, and Baselines",
    "volume": "Nov",
    "abstract": "Rhetoric, both spoken and written, involves not only content but also style. One common stylistic tool is $\\textit{parallelism}$: the juxtaposition of phrases which have the same sequence of linguistic ($\\textit{e.g.}$, phonological, syntactic, semantic) features. Despite the ubiquity of parallelism, the field of natural language processing has seldom investigated it, missing a chance to better understand the nature of the structure, meaning, and intent that humans convey. To address this, we introduce the task of $\\textit{rhetorical parallelism detection}$. We construct a formal definition of it; we provide one new Latin dataset and one adapted Chinese dataset for it; we establish a family of metrics to evaluate performance on it; and, lastly, we create baseline systems and novel sequence labeling schemes to capture it. On our strictest metric, we attain $F_{1}$ scores of $0.40$ and $0.43$ on our Latin and Chinese datasets, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephen Bothwell",
      "Justin DeBenedetto",
      "Theresa Crnkovich",
      "Hildegund Mller",
      "David Chiang"
    ]
  },
  "http://arxiv.org/abs/2311.18609": {
    "title": "ArthModel: Enhance Arithmetic Skills to Large Language Model",
    "volume": "Nov",
    "abstract": "With the great success of ChatGPT, the research of large language models has become increasingly popular. However, the models have several limitations, such as toxicity and pool performance of arithmetic solving. Meanwhile, LLM may have some potential abilities that have yet to be exploited. In this paper, we choose a different way to enhance the arithmetic ability of LLM. We propose to train LLM to generate a postfix expression related to the arithmetic problem and incorporate it with small pretrained models. Moreover, this small model transfers the token embeddings into real dense numbers and invokes native functions of a deep learning platform to get the correct answer. To generate the final result, we propose prompt injection for adding the result outputs by the small model to LLM. This work provides different ways of thinking, training and using a language model. The codes and models will be released at \\url{https://github.com/eteced/arithmetic_finetuning_v1}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingdi Guo"
    ]
  },
  "http://arxiv.org/abs/2311.18580": {
    "title": "FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality, Fairness, Toxicity",
    "volume": "Nov",
    "abstract": "The widespread of generative artificial intelligence has heightened concerns about the potential harms posed by AI-generated texts, primarily stemming from factoid, unfair, and toxic content. Previous researchers have invested much effort in assessing the harmlessness of generative language models. However, existing benchmarks are struggling in the era of large language models (LLMs), due to the stronger language generation and instruction following capabilities, as well as wider applications. In this paper, we propose FFT, a new benchmark with 2116 elaborated-designed instances, for LLM harmlessness evaluation with factuality, fairness, and toxicity. To investigate the potential harms of LLMs, we evaluate 9 representative LLMs covering various parameter scales, training stages, and creators. Experiments show that the harmlessness of LLMs is still under-satisfactory, and extensive analysis derives some insightful findings that could inspire future research for harmless LLM research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyao Cui",
      "Zhenyu Zhang",
      "Yilong Chen",
      "Wenyuan Zhang",
      "Tianyun Liu",
      "Siqi Wang",
      "Tingwen Liu"
    ]
  },
  "http://arxiv.org/abs/2311.18567": {
    "title": "Grammatical Gender's Influence on Distributional Semantics: A Causal Perspective",
    "volume": "Nov",
    "abstract": "How much meaning influences gender assignment across languages is an active area of research in modern linguistics and cognitive science. We can view current approaches as aiming to determine where gender assignment falls on a spectrum, from being fully arbitrarily determined to being largely semantically determined. For the latter case, there is a formulation of the neo-Whorfian hypothesis, which claims that even inanimate noun gender influences how people conceive of and talk about objects (using the choice of adjective used to modify inanimate nouns as a proxy for meaning). We offer a novel, causal graphical model that jointly represents the interactions between a noun's grammatical gender, its meaning, and adjective choice. In accordance with past results, we find a relationship between the gender of nouns and the adjectives which modify them. However, when we control for the meaning of the noun, we find that grammatical gender has a near-zero effect on adjective choice, thereby calling the neo-Whorfian hypothesis into question",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karolina Staczak",
      "Kevin Du",
      "Adina Williams",
      "Isabelle Augenstein",
      "Ryan Cotterell"
    ]
  },
  "http://arxiv.org/abs/2311.18481": {
    "title": "ESG Accountability Made Easy: DocQA at Your Service",
    "volume": "Nov",
    "abstract": "We present Deep Search DocQA. This application enables information extraction from documents via a question-answering conversational assistant. The system integrates several technologies from different AI disciplines consisting of document conversion to machine-readable format (via computer vision), finding relevant data (via natural language processing), and formulating an eloquent response (via large language models). Users can explore over 10,000 Environmental, Social, and Governance (ESG) disclosure reports from over 2000 corporations. The Deep Search platform can be accessed at: https://ds4sd.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lokesh Mishra",
      "Cesar Berrospi",
      "Kasper Dinkla",
      "Diego Antognini",
      "Francesco Fusco",
      "Benedikt Bothur",
      "Maksym Lysak",
      "Nikolaos Livathinos",
      "Ahmed Nassar",
      "Panagiotis Vagenas",
      "Lucas Morin",
      "Christoph Auer",
      "Michele Dolfi",
      "Peter Staar"
    ]
  },
  "http://arxiv.org/abs/2311.18466": {
    "title": "Use of explicit replies as coordination mechanisms in online student debate",
    "volume": "Nov",
    "abstract": "People in conversation entrain their linguistic behaviours through spontaneous alignment mechanisms [7] - both in face-to-face and computer-mediated communication (CMC) [8]. In CMC, one of the mechanisms through which linguistic entrainment happens is through explicit replies. Indeed, the use of explicit replies influences the structure of conversations, favouring the formation of reply-trees typically delineated by topic shifts [5]. The interpersonal coordination mechanisms realized by how actors address each other have been studied using a probabilistic framework proposed by David Gibson [2,3]. Other recent approaches use computational methods and information theory to quantify changes in text. We explore coordination mechanisms concerned with some of the roles utterances play in dialogues - specifically in explicit replies. We identify these roles by finding community structure in the conversation's vocabulary using a non-parametric, hierarchical topic model. Some conversations may always stay on the ground, remaining at the level of general introductory chatter. Some others may develop a specific sub-topic in significant depth and detail. Even others may jump between general chatter, out-of-topic remarks and people agreeing or disagreeing without further elaboration",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bruno D. Ferreira-Saraiva",
      "Joao P. Matos-Carvalho",
      "Manuel Pita"
    ]
  },
  "http://arxiv.org/abs/2311.18397": {
    "title": "IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions",
    "volume": "Nov",
    "abstract": "Retrieval-Augmented Generation (RAG), by incorporating external knowledge with parametric memory of language models, has become the state-of-the-art architecture for open-domain QA tasks. However, common knowledge bases are inherently constrained by limited coverage and noisy information, making retrieval-based approaches inadequate to answer implicit reasoning questions. In this paper, we propose an Induction-Augmented Generation (IAG) framework that utilizes inductive knowledge along with the retrieved documents for implicit reasoning. We leverage large language models (LLMs) for deriving such knowledge via a novel prompting method based on inductive reasoning patterns. On top of this, we implement two versions of IAG named IAG-GPT and IAG-Student, respectively. IAG-GPT directly utilizes the knowledge generated by GPT-3 for answer prediction, while IAG-Student gets rid of dependencies on GPT service at inference time by incorporating a student inductor model. The inductor is firstly trained via knowledge distillation and further optimized by back-propagating the generator feedback via differentiable beam scores. Experimental results show that IAG outperforms RAG baselines as well as ChatGPT on two Open-Domain QA tasks. Notably, our best models have won the first place in the official leaderboards of CSQA2.0 (since Nov 1, 2022) and StrategyQA (since Jan 8, 2023)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhebin Zhang",
      "Xinyu Zhang",
      "Yuanhang Ren",
      "Saijiang Shi",
      "Meng Han",
      "Yongkang Wu",
      "Ruofei Lai",
      "Zhao Cao"
    ]
  },
  "http://arxiv.org/abs/2311.18364": {
    "title": "Hubness Reduction Improves Sentence-BERT Semantic Spaces",
    "volume": "Nov",
    "abstract": "Semantic representations of text, i.e. representations of natural language which capture meaning by geometry, are essential for areas such as information retrieval and document grouping. High-dimensional trained dense vectors have received much attention in recent years as such representations. We investigate the structure of semantic spaces that arise from embeddings made with Sentence-BERT and find that the representations suffer from a well-known problem in high dimensions called hubness. Hubness results in asymmetric neighborhood relations, such that some texts (the hubs) are neighbours of many other texts while most texts (so-called anti-hubs), are neighbours of few or no other texts. We quantify the semantic quality of the embeddings using hubness scores and error rate of a neighbourhood based classifier. We find that when hubness is high, we can reduce error rate and hubness using hubness reduction methods. We identify a combination of two methods as resulting in the best reduction. For example, on one of the tested pretrained models, this combined method can reduce hubness by about 75% and error rate by about 9%. Thus, we argue that mitigating hubness in the embedding space provides better semantic representations of text",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beatrix M. G. Nielsen",
      "Lars Kai Hansen"
    ]
  },
  "http://arxiv.org/abs/2311.18353": {
    "title": "Evaluating the Rationale Understanding of Critical Reasoning in Logical Reading Comprehension",
    "volume": "Nov",
    "abstract": "To precisely evaluate a language model's capability for logical reading comprehension, we present a dataset for testing the understanding of the rationale behind critical reasoning. For questions taken from an existing multiplechoice logical reading comprehension dataset, we crowdsource rationale texts that explain why we should select or eliminate answer options, resulting in 3,003 multiple-choice subquestions that are associated with 943 main questions. Experiments on our dataset show that recent large language models (e.g., InstructGPT) struggle to answer the subquestions even if they are able to answer the main questions correctly. We find that the models perform particularly poorly in answering subquestions written for the incorrect options of the main questions, implying that the models have a limited capability for explaining why incorrect alternatives should be eliminated. These results suggest that our dataset encourages further investigation into the critical reasoning ability of language models while focusing on the elimination process of relevant alternatives",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akira Kawabata",
      "Saku Sugawara"
    ]
  },
  "http://arxiv.org/abs/2311.18260": {
    "title": "Consensus, dissensus and synergy between clinicians and specialist foundation models in radiology report generation",
    "volume": "Nov",
    "abstract": "Radiology reports are an instrumental part of modern medicine, informing key clinical decisions such as diagnosis and treatment. The worldwide shortage of radiologists, however, restricts access to expert care and imposes heavy workloads, contributing to avoidable errors and delays in report delivery. While recent progress in automated report generation with vision-language models offer clear potential in ameliorating the situation, the path to real-world adoption has been stymied by the challenge of evaluating the clinical quality of AI-generated reports. In this study, we build a state-of-the-art report generation system for chest radiographs, Flamingo-CXR, by fine-tuning a well-known vision-language foundation model on radiology data. To evaluate the quality of the AI-generated reports, a group of 16 certified radiologists provide detailed evaluations of AI-generated and human written reports for chest X-rays from an intensive care setting in the United States and an inpatient setting in India. At least one radiologist (out of two per case) preferred the AI report to the ground truth report in over 60$\\%$ of cases for both datasets. Amongst the subset of AI-generated reports that contain errors, the most frequently cited reasons were related to the location and finding, whereas for human written reports, most mistakes were related to severity and finding. This disparity suggested potential complementarity between our AI system and human experts, prompting us to develop an assistive scenario in which Flamingo-CXR generates a first-draft report, which is subsequently revised by a clinician. This is the first demonstration of clinician-AI collaboration for report writing, and the resultant reports are assessed to be equivalent or preferred by at least one radiologist to reports written by experts alone in 80$\\%$ of in-patient cases and 66$\\%$ of intensive care cases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryutaro Tanno",
      "David G. T. Barrett",
      "Andrew Sellergren",
      "Sumedh Ghaisas",
      "Sumanth Dathathri",
      "Abigail See",
      "Johannes Welbl",
      "Karan Singhal",
      "Shekoofeh Azizi",
      "Tao Tu",
      "Mike Schaekermann",
      "Rhys May",
      "Roy Lee",
      "SiWai Man",
      "Zahra Ahmed",
      "Sara Mahdavi",
      "Danielle Belgrave",
      "Vivek Natarajan",
      "Shravya Shetty",
      "Pushmeet Kohli",
      "Po-Sen Huang",
      "Alan Karthikesalingam",
      "Ira Ktena"
    ]
  },
  "http://arxiv.org/abs/2311.18248": {
    "title": "mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model",
    "volume": "Nov",
    "abstract": "Recently, the strong text creation ability of Large Language Models(LLMs) has given rise to many tools for assisting paper reading or even writing. However, the weak diagram analysis abilities of LLMs or Multimodal LLMs greatly limit their application scenarios, especially for scientific academic paper writing. In this work, towards a more versatile copilot for academic paper writing, we mainly focus on strengthening the multi-modal diagram analysis ability of Multimodal LLMs. By parsing Latex source files of high-quality papers, we carefully build a multi-modal diagram understanding dataset M-Paper. By aligning diagrams in the paper with related paragraphs, we construct professional diagram analysis samples for training and evaluation. M-Paper is the first dataset to support joint comprehension of multiple scientific diagrams, including figures and tables in the format of images or Latex codes. Besides, to better align the copilot with the user's intention, we introduce the `outline' as the control signal, which could be directly given by the user or revised based on auto-generated ones. Comprehensive experiments with a state-of-the-art Mumtimodal LLM demonstrate that training on our dataset shows stronger scientific diagram understanding performance, including diagram captioning, diagram analysis, and outline recommendation. The dataset, code, and model are available at https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/PaperOwl",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anwen Hu",
      "Yaya Shi",
      "Haiyang Xu",
      "Jiabo Ye",
      "Qinghao Ye",
      "Ming Yan",
      "Chenliang Li",
      "Qi Qian",
      "Ji Zhang",
      "Fei Huang"
    ]
  },
  "http://arxiv.org/abs/2311.18232": {
    "title": "LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models",
    "volume": "Nov",
    "abstract": "Large language models (LLMs) provide excellent text-generation capabilities, but standard prompting and generation methods generally do not lead to intentional or goal-directed agents and might necessitate considerable prompt tuning. This becomes particularly apparent in multi-turn conversations: even the best current LLMs rarely ask clarifying questions, engage in explicit information gathering, or take actions now that lead to better decisions after multiple turns. Reinforcement learning has the potential to leverage the powerful modeling capabilities of LLMs, as well as their internal representation of textual interactions, to create capable goal-directed language agents. This can enable intentional and temporally extended interactions, such as with humans, through coordinated persuasion and carefully crafted questions, or in goal-directed play through text games to bring about desired final outcomes. However, enabling this requires the community to develop stable and reliable reinforcement learning algorithms that can effectively train LLMs. Developing such algorithms requires tasks that can gauge progress on algorithm design, provide accessible and reproducible evaluations for multi-turn interactions, and cover a range of task properties and challenges in improving reinforcement learning algorithms. Our paper introduces the LMRL-Gym benchmark for evaluating multi-turn RL for LLMs, together with an open-source research framework containing a basic toolkit for getting started on multi-turn RL with offline value-based and policy-based RL methods. Our benchmark consists of 8 different language tasks, which require multiple rounds of language interaction and cover a range of tasks in open-ended dialogue and text games",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marwa Abdulhai",
      "Isadora White",
      "Charlie Snell",
      "Charles Sun",
      "Joey Hong",
      "Yuexiang Zhai",
      "Kelvin Xu",
      "Sergey Levine"
    ]
  },
  "http://arxiv.org/abs/2311.18215": {
    "title": "Automatic Construction of a Korean Toxic Instruction Dataset for Ethical Tuning of Large Language Models",
    "volume": "Nov",
    "abstract": "Caution: this paper may include material that could be offensive or distressing. The advent of Large Language Models (LLMs) necessitates the development of training approaches that mitigate the generation of unethical language and aptly manage toxic user queries. Given the challenges related to human labor and the scarcity of data, we present KoTox, comprising 39K unethical instruction-output pairs. This collection of automatically generated toxic instructions refines the training of LLMs and establishes a foundational framework for improving LLMs' ethical awareness and response to various toxic inputs, promoting more secure and responsible interactions in Natural Language Processing (NLP) applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungjoo Byun",
      "Dongjun Jang",
      "Hyemi Jo",
      "Hyopil Shin"
    ]
  },
  "http://arxiv.org/abs/2311.18200": {
    "title": "INarIG: Iterative Non-autoregressive Instruct Generation Model For Word-Level Auto Completion",
    "volume": "Nov",
    "abstract": "Computer-aided translation (CAT) aims to enhance human translation efficiency and is still important in scenarios where machine translation cannot meet quality requirements. One fundamental task within this field is Word-Level Auto Completion (WLAC). WLAC predicts a target word given a source sentence, translation context, and a human typed character sequence. Previous works either employ word classification models to exploit contextual information from both sides of the target word or directly disregarded the dependencies from the right-side context. Furthermore, the key information, i.e. human typed sequences, is only used as prefix constraints in the decoding module. In this paper, we propose the INarIG (Iterative Non-autoregressive Instruct Generation) model, which constructs the human typed sequence into Instruction Unit and employs iterative decoding with subwords to fully utilize input information given in the task. Our model is more competent in dealing with low-frequency words (core scenario of this task), and achieves state-of-the-art results on the WMT22 and benchmark datasets, with a maximum increase of over 10% prediction accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengchao Shang",
      "Zongyao Li",
      "Daimeng Wei",
      "Jiaxin Guo",
      "Minghan Wang",
      "Xiaoyu Chen",
      "Lizhi Lei",
      "Hao Yang"
    ]
  },
  "http://arxiv.org/abs/2312.00079": {
    "title": "HiFi Tuner: High-Fidelity Subject-Driven Fine-Tuning for Diffusion Models",
    "volume": "Nov",
    "abstract": "This paper explores advancements in high-fidelity personalized image generation through the utilization of pre-trained text-to-image diffusion models. While previous approaches have made significant strides in generating versatile scenes based on text descriptions and a few input images, challenges persist in maintaining the subject fidelity within the generated images. In this work, we introduce an innovative algorithm named HiFi Tuner to enhance the appearance preservation of objects during personalized image generation. Our proposed method employs a parameter-efficient fine-tuning framework, comprising a denoising process and a pivotal inversion process. Key enhancements include the utilization of mask guidance, a novel parameter regularization technique, and the incorporation of step-wise subject representations to elevate the sample fidelity. Additionally, we propose a reference-guided generation approach that leverages the pivotal inversion of a reference image to mitigate unwanted subject variations and artifacts. We further extend our method to a novel image editing task: substituting the subject in an image through textual manipulations. Experimental evaluations conducted on the DreamBooth dataset using the Stable Diffusion model showcase promising results. Fine-tuning solely on textual embeddings improves CLIP-T score by 3.6 points and improves DINO score by 9.6 points over Textual Inversion. When fine-tuning all parameters, HiFi Tuner improves CLIP-T score by 1.2 points and improves DINO score by 1.2 points over DreamBooth, establishing a new state of the art",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhonghao Wang",
      "Wei Wei",
      "Yang Zhao",
      "Zhisheng Xiao",
      "Mark Hasegawa-Johnson",
      "Humphrey Shi",
      "Tingbo Hou"
    ]
  },
  "http://arxiv.org/abs/2311.18195": {
    "title": "COVID-19 Vaccine Misinformation in Middle Income Countries",
    "volume": "Nov",
    "abstract": "This paper introduces a multilingual dataset of COVID-19 vaccine misinformation, consisting of annotated tweets from three middle-income countries: Brazil, Indonesia, and Nigeria. The expertly curated dataset includes annotations for 5,952 tweets, assessing their relevance to COVID-19 vaccines, presence of misinformation, and the themes of the misinformation. To address challenges posed by domain specificity, the low-resource setting, and data imbalance, we adopt two approaches for developing COVID-19 vaccine misinformation detection models: domain-specific pre-training and text augmentation using a large language model. Our best misinformation detection models demonstrate improvements ranging from 2.7 to 15.9 percentage points in macro F1-score compared to the baseline models. Additionally, we apply our misinformation detection models in a large-scale study of 19 million unlabeled tweets from the three countries between 2020 and 2022, showcasing the practical application of our dataset and models for detecting and analyzing vaccine misinformation in multiple countries and languages. Our analysis indicates that percentage changes in the number of new COVID-19 cases are positively associated with COVID-19 vaccine misinformation rates in a staggered manner for Brazil and Indonesia, and there are significant positive associations between the misinformation rates across the three countries",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongin Kim",
      "Byeo Rhee Back",
      "Aditya Agrawal",
      "Jiaxi Wu",
      "Veronika J. Wirtz",
      "Traci Hong",
      "Derry Wijaya"
    ]
  },
  "http://arxiv.org/abs/2311.18194": {
    "title": "Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes",
    "volume": "Nov",
    "abstract": "In-context learning (ICL) refers to the ability of a model to condition on a few in-context demonstrations (input-output examples of the underlying task) to generate the answer for a new query input, without updating parameters. Despite the impressive ICL ability of LLMs, it has also been found that ICL in LLMs is sensitive to input demonstrations and limited to short context lengths. To understand the limitations and principles for successful ICL, we conduct an investigation with ICL linear regression of transformers. We characterize several Out-of-Distribution (OOD) cases for ICL inspired by realistic LLM ICL failures and compare transformers with DeepSet, a simple yet powerful architecture for ICL. Surprisingly, DeepSet outperforms transformers across a variety of distribution shifts, implying that preserving permutation invariance symmetry to input demonstrations is crucial for OOD ICL. The phenomenon specifies a fundamental requirement by ICL, which we termed as ICL invariance. Nevertheless, the positional encodings in LLMs will break ICL invariance. To this end, we further evaluate transformers with identical positional encodings and find preserving ICL invariance in transformers achieves state-of-the-art performance across various ICL distribution shifts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongqiang Chen",
      "Binghui Xie",
      "Kaiwen Zhou",
      "Bo Han",
      "Yatao Bian",
      "James Cheng"
    ]
  },
  "http://arxiv.org/abs/2311.18151": {
    "title": "Uncertainty Guided Global Memory Improves Multi-Hop Question Answering",
    "volume": "Nov",
    "abstract": "Transformers have become the gold standard for many natural language processing tasks and, in particular, for multi-hop question answering (MHQA). This task includes processing a long document and reasoning over the multiple parts of it. The landscape of MHQA approaches can be classified into two primary categories. The first group focuses on extracting supporting evidence, thereby constraining the QA model's context to predicted facts. Conversely, the second group relies on the attention mechanism of the long input encoding model to facilitate multi-hop reasoning. However, attention-based token representations lack explicit global contextual information to connect reasoning steps. To address these issues, we propose GEMFormer, a two-stage method that first collects relevant information over the entire document to the memory and then combines it with local context to solve the task. Our experimental results show that fine-tuning a pre-trained model with memory-augmented input, including the most certain global elements, improves the model's performance on three MHQA datasets compared to the baseline. We also found that the global explicit memory contains information from supporting facts required for the correct answer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alsu Sagirova",
      "Mikhail Burtsev"
    ]
  },
  "http://arxiv.org/abs/2311.18147": {
    "title": "DisCGen: A Framework for Discourse-Informed Counterspeech Generation",
    "volume": "Nov",
    "abstract": "Counterspeech can be an effective method for battling hateful content on social media. Automated counterspeech generation can aid in this process. Generated counterspeech, however, can be viable only when grounded in the context of topic, audience and sensitivity as these factors influence both the efficacy and appropriateness. In this work, we propose a novel framework based on theories of discourse to study the inferential links that connect counter speeches to the hateful comment. Within this framework, we propose: i) a taxonomy of counterspeech derived from discourse frameworks, and ii) discourse-informed prompting strategies for generating contextually-grounded counterspeech. To construct and validate this framework, we present a process for collecting an in-the-wild dataset of counterspeech from Reddit. Using this process, we manually annotate a dataset of 3.9k Reddit comment pairs for the presence of hatespeech and counterspeech. The positive pairs are annotated for 10 classes in our proposed taxonomy. We annotate these pairs with paraphrased counterparts to remove offensiveness and first-person references. We show that by using our dataset and framework, large language models can generate contextually-grounded counterspeech informed by theories of discourse. According to our human evaluation, our approaches can act as a safeguard against critical failures of discourse-agnostic models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sabit Hassan",
      "Malihe Alikhani"
    ]
  },
  "http://arxiv.org/abs/2311.18140": {
    "title": "ROBBIE: Robust Bias Evaluation of Large Generative Language Models",
    "volume": "Nov",
    "abstract": "As generative large language models (LLMs) grow more performant and prevalent, we must develop comprehensive enough tools to measure and improve their fairness. Different prompt-based datasets can be used to measure social bias across multiple text domains and demographic axes, meaning that testing LLMs on more datasets can potentially help us characterize their biases more fully, and better ensure equal and equitable treatment of marginalized demographic groups. In this work, our focus is two-fold: (1) Benchmarking: a comparison of 6 different prompt-based bias and toxicity metrics across 12 demographic axes and 5 families of generative LLMs. Out of those 6 metrics, AdvPromptSet and HolisticBiasR are novel datasets proposed in the paper. The comparison of those benchmarks gives us insights about the bias and toxicity of the compared models. Therefore, we explore the frequency of demographic terms in common LLM pre-training corpora and how this may relate to model biases. (2) Mitigation: we conduct a comprehensive study of how well 3 bias/toxicity mitigation techniques perform across our suite of measurements. ROBBIE aims to provide insights for practitioners while deploying a model, emphasizing the need to not only measure potential harms, but also understand how they arise by characterizing the data, mitigate harms once found, and balance any trade-offs. We open-source our analysis code in hopes of encouraging broader measurements of bias in future LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Esiobu",
      "Xiaoqing Tan",
      "Saghar Hosseini",
      "Megan Ung",
      "Yuchen Zhang",
      "Jude Fernandes",
      "Jane Dwivedi-Yu",
      "Eleonora Presani",
      "Adina Williams",
      "Eric Michael Smith"
    ]
  },
  "http://arxiv.org/abs/2311.18063": {
    "title": "TurkishBERTweet: Fast and Reliable Large Language Model for Social Media Analysis",
    "volume": "Nov",
    "abstract": "Turkish is one of the most popular languages in the world. Wide us of this language on social media platforms such as Twitter, Instagram, or Tiktok and strategic position of the country in the world politics makes it appealing for the social network researchers and industry. To address this need, we introduce TurkishBERTweet, the first large scale pre-trained language model for Turkish social media built using almost 900 million tweets. The model shares the same architecture as base BERT model with smaller input length, making TurkishBERTweet lighter than BERTurk and can have significantly lower inference time. We trained our model using the same approach for RoBERTa model and evaluated on two text classification tasks: Sentiment Classification and Hate Speech Detection. We demonstrate that TurkishBERTweet outperforms the other available alternatives on generalizability and its lower inference time gives significant advantage to process large-scale datasets. We also compared our models with the commercial OpenAI solutions in terms of cost and performance to demonstrate TurkishBERTweet is scalable and cost-effective solution. As part of our research, we released TurkishBERTweet and fine-tuned LoRA adapters for the mentioned tasks under the MIT License to facilitate future research and applications on Turkish social media. Our TurkishBERTweet model is available at: https://github.com/ViralLab/TurkishBERTweet",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Najafi",
      "Onur Varol"
    ]
  },
  "http://arxiv.org/abs/2311.18054": {
    "title": "I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text",
    "volume": "Nov",
    "abstract": "Potential harms of Large Language Models such as mass misinformation and plagiarism can be partially mitigated if there exists a reliable way to detect machine generated text. In this paper, we propose a new watermarking method to detect machine-generated texts. Our method embeds a unique pattern within the generated text, ensuring that while the content remains coherent and natural to human readers, it carries distinct markers that can be identified algorithmically. Specifically, we intervene with the token sampling process in a way which enables us to trace back our token choices during the detection phase. We show how watermarking affects textual quality and compare our proposed method with a state-of-the-art watermarking method in terms of robustness and detectability. Through extensive experiments, we demonstrate the effectiveness of our watermarking scheme in distinguishing between watermarked and non-watermarked text, achieving high detection rates while maintaining textual quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaan Efe Kele",
      "mer Kaan Grbz",
      "Mucahid Kutlu"
    ]
  },
  "http://arxiv.org/abs/2311.18041": {
    "title": "Zero-shot Conversational Summarization Evaluations with small Large Language Models",
    "volume": "Nov",
    "abstract": "Large Language Models (LLMs) exhibit powerful summarization abilities. However, their capabilities on conversational summarization remains under explored. In this work we evaluate LLMs (approx. 10 billion parameters) on conversational summarization and showcase their performance on various prompts. We show that the summaries generated by models depend on the instructions and the performance of LLMs vary with different instructions sometimes resulting steep drop in ROUGE scores if prompts are not selected carefully. We also evaluate the models with human evaluations and discuss the limitations of the models on conversational summarization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ramesh Manuvinakurike",
      "Saurav Sahay",
      "Sangeeta Manepalli",
      "Lama Nachman"
    ]
  },
  "http://arxiv.org/abs/2311.18034": {
    "title": "Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings",
    "volume": "Nov",
    "abstract": "Cross-lingual transfer learning is an important property of multilingual large language models (LLMs). But how do LLMs represent relationships between languages? Every language model has an input layer that maps tokens to vectors. This ubiquitous layer of language models is often overlooked. We find that similarities between these input embeddings are highly interpretable and that the geometry of these embeddings differs between model families. In one case (XLM-RoBERTa), embeddings encode language: tokens in different writing systems can be linearly separated with an average of 99.2% accuracy. Another family (mT5) represents cross-lingual semantic similarity: the 50 nearest neighbors for any token represent an average of 7.61 writing systems, and are frequently translations. This result is surprising given that there is no explicit parallel cross-lingual training corpora and no explicit incentive for translations in pre-training objectives. Our research opens the door for investigations in 1) The effect of pre-training and model architectures on representations of languages and 2) The applications of cross-lingual representations embedded in language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea W Wen-Yi",
      "David Mimno"
    ]
  },
  "http://arxiv.org/abs/2311.18028": {
    "title": "Filtered Semi-Markov CRF",
    "volume": "Nov",
    "abstract": "Semi-Markov CRF has been proposed as an alternative to the traditional Linear Chain CRF for text segmentation tasks such as Named Entity Recognition (NER). Unlike CRF, which treats text segmentation as token-level prediction, Semi-CRF considers segments as the basic unit, making it more expressive. However, Semi-CRF suffers from two major drawbacks: (1) quadratic complexity over sequence length, as it operates on every span of the input sequence, and (2) inferior performance compared to CRF for sequence labeling tasks like NER. In this paper, we introduce Filtered Semi-Markov CRF, a variant of Semi-CRF that addresses these issues by incorporating a filtering step to eliminate irrelevant segments, reducing complexity and search space. Our approach is evaluated on several NER benchmarks, where it outperforms both CRF and Semi-CRF while being significantly faster. The implementation of our method is available on \\href{https://github.com/urchade/Filtered-Semi-Markov-CRF}{Github}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Urchade Zaratiana",
      "Nadi Tomeh",
      "Niama El Khbir",
      "Pierre Holat",
      "Thierry Charnois"
    ]
  },
  "http://arxiv.org/abs/2311.17898": {
    "title": "Knowledge Pursuit Prompting for Zero-Shot Multimodal Synthesis",
    "volume": "Nov",
    "abstract": "Hallucinations and unfaithful synthesis due to inaccurate prompts with insufficient semantic details are widely observed in multimodal generative models. A prevalent strategy to align multiple modalities is to fine-tune the generator with a large number of annotated text-image pairs. However, such a procedure is labor-consuming and resource-draining. The key question we ask is: can we enhance the quality and faithfulness of text-driven generative models beyond extensive text-image pair annotations? To address this question, we propose Knowledge Pursuit Prompting (KPP), a zero-shot framework that iteratively incorporates external knowledge to help generators produce reliable visual content. Instead of training generators to handle generic prompts, KPP employs a recursive knowledge query process to gather informative external facts from the knowledge base, instructs a language model to compress the acquired knowledge for prompt refinement, and utilizes text-driven generators for visual synthesis. The entire process is zero-shot, without accessing the architectures and parameters of generative models. We evaluate the framework across multiple text-driven generative tasks (image, 3D rendering, and video) on datasets of different domains. We further demonstrate the extensibility and adaptability of KPP through varying foundation model bases and instructions. Our results show that KPP is capable of generating faithful and semantically rich content across diverse visual domains, offering a promising solution to improve multimodal generative models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinqi Luo",
      "Kwan Ho Ryan Chan",
      "Dimitris Dimos",
      "Ren Vidal"
    ]
  },
  "http://arxiv.org/abs/2311.17892": {
    "title": "A Pipeline For Discourse Circuits From CCG",
    "volume": "Nov",
    "abstract": "There is a significant disconnect between linguistic theory and modern NLP practice, which relies heavily on inscrutable black-box architectures. DisCoCirc is a newly proposed model for meaning that aims to bridge this divide, by providing neuro-symbolic models that incorporate linguistic structure. DisCoCirc represents natural language text as a `circuit' that captures the core semantic information of the text. These circuits can then be interpreted as modular machine learning models. Additionally, DisCoCirc fulfils another major aim of providing an NLP model that can be implemented on near-term quantum computers. In this paper we describe a software pipeline that converts English text to its DisCoCirc representation. The pipeline achieves coverage over a large fragment of the English language. It relies on Combinatory Categorial Grammar (CCG) parses of the input text as well as coreference resolution information. This semantic and syntactic information is used in several steps to convert the text into a simply-typed $\\lambda$-calculus term, and then into a circuit diagram. This pipeline will enable the application of the DisCoCirc framework to NLP tasks, using both classical and quantum approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathon Liu",
      "Razin A. Shaikh",
      "Benjamin Rodatz",
      "Richie Yeung",
      "Bob Coecke"
    ]
  },
  "http://arxiv.org/abs/2311.17842": {
    "title": "Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning",
    "volume": "Nov",
    "abstract": "In this study, we are interested in imbuing robots with the capability of physically-grounded task planning. Recent advancements have shown that large language models (LLMs) possess extensive knowledge useful in robotic tasks, especially in reasoning and planning. However, LLMs are constrained by their lack of world grounding and dependence on external affordance models to perceive environmental information, which cannot jointly reason with LLMs. We argue that a task planner should be an inherently grounded, unified multimodal system. To this end, we introduce Robotic Vision-Language Planning (ViLa), a novel approach for long-horizon robotic planning that leverages vision-language models (VLMs) to generate a sequence of actionable steps. ViLa directly integrates perceptual data into its reasoning and planning process, enabling a profound understanding of commonsense knowledge in the visual world, including spatial layouts and object attributes. It also supports flexible multimodal goal specification and naturally incorporates visual feedback. Our extensive evaluation, conducted in both real-robot and simulated environments, demonstrates ViLa's superiority over existing LLM-based planners, highlighting its effectiveness in a wide array of open-world manipulation tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingdong Hu",
      "Fanqi Lin",
      "Tong Zhang",
      "Li Yi",
      "Yang Gao"
    ]
  },
  "http://arxiv.org/abs/2311.17813": {
    "title": "Higher-Order DisCoCat (Peirce-Lambek-Montague semantics)",
    "volume": "Nov",
    "abstract": "We propose a new definition of higher-order DisCoCat (categorical compositional distributional) models where the meaning of a word is not a diagram, but a diagram-valued higher-order function. Our models can be seen as a variant of Montague semantics based on a lambda calculus where the primitives act on string diagrams rather than logical formulae. As a special case, we show how to translate from the Lambek calculus into Peirce's system beta for first-order logic. This allows us to give a purely diagrammatic treatment of higher-order and non-linear processes in natural language semantics: adverbs, prepositions, negation and quantifiers. The theoretical definition presented in this article comes with a proof-of-concept implementation in DisCoPy, the Python library for string diagrams",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexis Toumi",
      "Giovanni de Felice"
    ]
  },
  "http://arxiv.org/abs/2311.17786": {
    "title": "DSS: Synthesizing long Digital Ink using Data augmentation, Style encoding and Split generation",
    "volume": "Nov",
    "abstract": "As text generative models can give increasingly long answers, we tackle the problem of synthesizing long text in digital ink. We show that the commonly used models for this task fail to generalize to long-form data and how this problem can be solved by augmenting the training data, changing the model architecture and the inference procedure. These methods use contrastive learning technique and are tailored specifically for the handwriting domain. They can be applied to any encoder-decoder model that works with digital ink. We demonstrate that our method reduces the character error rate on long-form English data by half compared to baseline RNN and by 16% compared to the previous approach that aims at addressing the same problem. We show that all three parts of the method improve recognizability of generated inks. In addition, we evaluate synthesized data in a human study and find that people perceive most of generated data as real",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksandr Timofeev",
      "Anastasiia Fadeeva",
      "Andrei Afonin",
      "Claudiu Musat",
      "Andrii Maksai"
    ]
  },
  "http://arxiv.org/abs/2311.17771": {
    "title": "Supervising the Centroid Baseline for Extractive Multi-Document Summarization",
    "volume": "Nov",
    "abstract": "The centroid method is a simple approach for extractive multi-document summarization and many improvements to its pipeline have been proposed. We further refine it by adding a beam search process to the sentence selection and also a centroid estimation attention model that leads to improved results. We demonstrate this in several multi-document summarization datasets, including in a multilingual scenario",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simo Gonalves",
      "Gonalo Correia",
      "Diogo Pernes",
      "Afonso Mendes"
    ]
  },
  "http://arxiv.org/abs/2311.17972": {
    "title": "Self-Infilling Code Generation",
    "volume": "Nov",
    "abstract": "This work introduces a general code generation framework that incorporates infilling operations into auto-regressive decoding. Our approach capitalizes on the observation that recent code language models with infilling capabilities can perform \\emph{self-infilling}: whereas infilling operations aim to fill in the middle based on a predefined prefix and suffix, self-infilling sequentially generates both such surrounding context and the infilled content. We utilize this feature to develop an infilling-augmented decoding process that facilitates non-monotonic generation. This approach allows for postponing the generation of uncertain code snippets until a definitive suffix is established, leading to improved control over the generation sequence. In addition, it facilitates a looping mechanism, which can iteratively update and synchronize each piece of generation in a cyclic manner. Extensive experiments are conducted to demonstrate that our proposed decoding process is effective in enhancing regularity and quality across several code generation benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Zheng",
      "Jianbo Yuan",
      "Zhi Zhang",
      "Hongxia Yang",
      "Lingpeng Kong"
    ]
  },
  "http://arxiv.org/abs/2311.17743": {
    "title": "Mukhyansh: A Headline Generation Dataset for Indic Languages",
    "volume": "Nov",
    "abstract": "The task of headline generation within the realm of Natural Language Processing (NLP) holds immense significance, as it strives to distill the true essence of textual content into concise and attention-grabbing summaries. While noteworthy progress has been made in headline generation for widely spoken languages like English, there persist numerous challenges when it comes to generating headlines in low-resource languages, such as the rich and diverse Indian languages. A prominent obstacle that specifically hinders headline generation in Indian languages is the scarcity of high-quality annotated data. To address this crucial gap, we proudly present Mukhyansh, an extensive multilingual dataset, tailored for Indian language headline generation. Comprising an impressive collection of over 3.39 million article-headline pairs, Mukhyansh spans across eight prominent Indian languages, namely Telugu, Tamil, Kannada, Malayalam, Hindi, Bengali, Marathi, and Gujarati. We present a comprehensive evaluation of several state-of-the-art baseline models. Additionally, through an empirical analysis of existing works, we demonstrate that Mukhyansh outperforms all other models, achieving an impressive average ROUGE-L score of 31.43 across all 8 languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lokesh Madasu",
      "Gopichand Kanumolu",
      "Nirmal Surange",
      "Manish Shrivastava"
    ]
  },
  "http://arxiv.org/abs/2311.17741": {
    "title": "End-to-end Joint Rich and Normalized ASR with a limited amount of rich training data",
    "volume": "Nov",
    "abstract": "Joint rich and normalized automatic speech recognition (ASR), that produces transcriptions both with and without punctuation and capitalization, remains a challenge. End-to-end (E2E) ASR models offer both convenience and the ability to perform such joint transcription of speech. Training such models requires paired speech and rich text data, which is not widely available. In this paper, we compare two different approaches to train a stateless Transducer-based E2E joint rich and normalized ASR system, ready for streaming applications, with a limited amount of rich labeled data. The first approach uses a language model to generate pseudo-rich transcriptions of normalized training data. The second approach uses a single decoder conditioned on the type of the output. The first approach leads to E2E rich ASR which perform better on out-of-domain data, with up to 9% relative reduction in errors. The second approach demonstrates the feasibility of an E2E joint rich and normalized ASR system using as low as 5% rich training data with moderate (2.42% absolute) increase in errors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Can Cui",
      "Imran Ahamad Sheikh",
      "Mostafa Sadeghi",
      "Emmanuel Vincent"
    ]
  },
  "http://arxiv.org/abs/2311.17722": {
    "title": "SenTest: Evaluating Robustness of Sentence Encoders",
    "volume": "Nov",
    "abstract": "Contrastive learning has proven to be an effective method for pre-training models using weakly labeled data in the vision domain. Sentence transformers are the NLP counterparts to this architecture, and have been growing in popularity due to their rich and effective sentence representations. Having effective sentence representations is paramount in multiple tasks, such as information retrieval, retrieval augmented generation (RAG), and sentence comparison. Keeping in mind the deployability factor of transformers, evaluating the robustness of sentence transformers is of utmost importance. This work focuses on evaluating the robustness of the sentence encoders. We employ several adversarial attacks to evaluate its robustness. This system uses character-level attacks in the form of random character substitution, word-level attacks in the form of synonym replacement, and sentence-level attacks in the form of intra-sentence word order shuffling. The results of the experiments strongly undermine the robustness of sentence encoders. The models produce significantly different predictions as well as embeddings on perturbed datasets. The accuracy of the models can fall up to 15 percent on perturbed datasets as compared to unperturbed datasets. Furthermore, the experiments demonstrate that these embeddings does capture the semantic and syntactic structure (sentence order) of sentences. However, existing supervised classification strategies fail to leverage this information, and merely function as n-gram detectors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanmay Chavan",
      "Shantanu Patankar",
      "Aditya Kane",
      "Omkar Gokhale",
      "Geetanjali Kale",
      "Raviraj Joshi"
    ]
  },
  "http://arxiv.org/abs/2311.17696": {
    "title": "How to Build an AI Tutor that Can Adapt to Any Course and Provide Accurate Answers Using Large Language Model and Retrieval-Augmented Generation",
    "volume": "Nov",
    "abstract": "Artificial intelligence is transforming education through data-driven, personalized learning solutions. This paper introduces AI Tutor, an innovative web application that provides personalized tutoring in any subject using state-of-the-art Large Language Model (LLM). AI Tutor ingests course materials to construct an adaptive knowledge base tailored to the course. When students pose questions, it retrieves the most relevant information and generates detailed, conversational responses citing supporting evidence. The system is powered by advanced large language models and Retrieval-Augmented Generation (RAG) techniques for accurate, natural question answering. We present a fully-functional web interface and video demonstration that showcase AI Tutor's versatility across diverse subjects and its ability to produce pedagogically cogent responses. While an initial prototype, this work represents a pioneering step toward AI-enabled tutoring systems that can democratize access to high-quality, customized educational support",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxi Dong"
    ]
  },
  "http://arxiv.org/abs/2311.17686": {
    "title": "AviationGPT: A Large Language Model for the Aviation Domain",
    "volume": "Nov",
    "abstract": "The advent of ChatGPT and GPT-4 has captivated the world with large language models (LLMs), demonstrating exceptional performance in question-answering, summarization, and content generation. The aviation industry is characterized by an abundance of complex, unstructured text data, replete with technical jargon and specialized terminology. Moreover, labeled data for model building are scarce in this domain, resulting in low usage of aviation text data. The emergence of LLMs presents an opportunity to transform this situation, but there is a lack of LLMs specifically designed for the aviation domain. To address this gap, we propose AviationGPT, which is built on open-source LLaMA-2 and Mistral architectures and continuously trained on a wealth of carefully curated aviation datasets. Experimental results reveal that AviationGPT offers users multiple advantages, including the versatility to tackle diverse natural language processing (NLP) problems (e.g., question-answering, summarization, document writing, information extraction, report querying, data cleaning, and interactive data exploration). It also provides accurate and contextually relevant responses within the aviation domain and significantly improves performance (e.g., over a 40% performance gain in tested cases). With AviationGPT, the aviation industry is better equipped to address more complex research problems and enhance the efficiency and safety of National Airspace System (NAS) operations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liya Wang",
      "Jason Chou",
      "Xin Zhou",
      "Alex Tien",
      "Diane M Baumgartner"
    ]
  },
  "http://arxiv.org/abs/2311.17676": {
    "title": "Improving Minority Stress Detection with Emotions",
    "volume": "Nov",
    "abstract": "Psychological stress detection is an important task for mental healthcare research, but there has been little prior work investigating the effectiveness of psychological stress models on minority individuals, who are especially vulnerable to poor mental health outcomes. In this work, we use the related task of minority stress detection to evaluate the ability of psychological stress models to understand the language of sexual and gender minorities. We find that traditional psychological stress models underperform on minority stress detection, and we propose using emotion-infused models to reduce that performance disparity. We further demonstrate that multi-task psychological stress models outperform the current state-of-the-art for minority stress detection without directly training on minority stress data. We provide explanatory analysis showing that minority communities have different distributions of emotions than the general population and that emotion-infused models improve the performance of stress models on underrepresented groups because of their effectiveness in low-data environments, and we propose that integrating emotions may benefit underrepresented groups in other mental health detection tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Ivey",
      "Susan Gauch"
    ]
  },
  "http://arxiv.org/abs/2311.17667": {
    "title": "TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models",
    "volume": "Nov",
    "abstract": "Understanding time is a pivotal aspect of human cognition, crucial in the broader framework of grasping the intricacies of the world. Previous studies typically focus on specific aspects of time, lacking a comprehensive temporal reasoning benchmark. To address this issue, we propose TimeBench, a comprehensive hierarchical temporal reasoning benchmark that covers a broad spectrum of temporal reasoning phenomena, which provides a thorough evaluation for investigating the temporal reasoning capabilities of large language models. We conduct extensive experiments on popular LLMs, such as GPT-4, LLaMA2, and Mistral, incorporating chain-of-thought prompting. Our experimental results indicate a significant performance gap between the state-of-the-art LLMs and humans, highlighting that there is still a considerable distance to cover in temporal reasoning. We aspire for TimeBench to serve as a comprehensive benchmark, fostering research in temporal reasoning for LLMs. Our resource is available at https://github.com/zchuz/TimeBench",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Chu",
      "Jingchang Chen",
      "Qianglong Chen",
      "Weijiang Yu",
      "Haotian Wang",
      "Ming Liu",
      "Bing Qin"
    ]
  },
  "http://arxiv.org/abs/2311.17647": {
    "title": "VIM: Probing Multimodal Large Language Models for Visual Embedded Instruction Following",
    "volume": "Nov",
    "abstract": "We introduce VISUAL EMBEDDED INSTRUCTION (VIM), a new framework designed to evaluate the visual instruction following capability of Multimodal Large Language Models (MLLMs). As illustrated in Figure 2, VIM challenges the MLLMs by embedding the instructions into the visual scenes, demanding strong visual interpretative skills for instruction following. We adapt VIM to various benchmarks, including VQAv2, MME, MM-Vet, and RefCOCO series, compose a VIM bench, and probe diverse MLLMs across three distinct in-context learning settings: Zero Shot, One Shot, and Pair Shot. We observe that there is a significant performance disparity between the open-source MLLMs and GPT-4V, implying that their proficiency in visual instruction comprehension is not up to par. Our results highlight a promising direction for the enhancement of MLLMs capabilities on instruction following. We aim VIM to serve as a useful norm for advancing the state of the art and driving further progress in the field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujie Lu",
      "Xiujun Li",
      "William Yang Wang",
      "Yejin Choi"
    ]
  },
  "http://arxiv.org/abs/2311.17633": {
    "title": "Introduction to Transformers: an NLP Perspective",
    "volume": "Nov",
    "abstract": "Transformers have dominated empirical machine learning models of natural language processing. In this paper, we introduce basic concepts of Transformers and present key techniques that form the recent advances of these models. This includes a description of the standard Transformer architecture, a series of model refinements, and common applications. Given that Transformers and related deep learning techniques might be evolving in ways we have never seen, we cannot dive into all the model details or cover all the technical areas. Instead, we focus on just those concepts that are helpful for gaining a good understanding of Transformers and their variants. We also summarize the key ideas that impact this field, thereby yielding some insights into the strengths and limitations of these models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Xiao",
      "Jingbo Zhu"
    ]
  },
  "http://arxiv.org/abs/2311.17593": {
    "title": "LanGWM: Language Grounded World Model",
    "volume": "Nov",
    "abstract": "Recent advances in deep reinforcement learning have showcased its potential in tackling complex tasks. However, experiments on visual control tasks have revealed that state-of-the-art reinforcement learning models struggle with out-of-distribution generalization. Conversely, expressing higher-level concepts and global contexts is relatively easy using language. Building upon recent success of the large language models, our main objective is to improve the state abstraction technique in reinforcement learning by leveraging language for robust action selection. Specifically, we focus on learning language-grounded visual features to enhance the world model learning, a model-based reinforcement learning technique. To enforce our hypothesis explicitly, we mask out the bounding boxes of a few objects in the image observation and provide the text prompt as descriptions for these masked objects. Subsequently, we predict the masked objects along with the surrounding regions as pixel reconstruction, similar to the transformer-based masked autoencoder approach. Our proposed LanGWM: Language Grounded World Model achieves state-of-the-art performance in out-of-distribution test at the 100K interaction steps benchmarks of iGibson point navigation tasks. Furthermore, our proposed technique of explicit language-grounded visual representation learning has the potential to improve models for human-robot interaction because our extracted visual features are language grounded",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rudra P. K. Poudel",
      "Harit Pandya",
      "Chao Zhang",
      "Roberto Cipolla"
    ]
  },
  "http://arxiv.org/abs/2311.17514": {
    "title": "Reinforcement Replaces Supervision: Query focused Summarization using Deep Reinforcement Learning",
    "volume": "Nov",
    "abstract": "Query-focused Summarization (QfS) deals with systems that generate summaries from document(s) based on a query. Motivated by the insight that Reinforcement Learning (RL) provides a generalization to Supervised Learning (SL) for Natural Language Generation, and thereby performs better (empirically) than SL, we use an RL-based approach for this task of QfS. Additionally, we also resolve the conflict of employing RL in Transformers with Teacher Forcing. We develop multiple Policy Gradient networks, trained on various reward signals: ROUGE, BLEU, and Semantic Similarity, which lead to a 10-point improvement over the State-of-the-Art approach on the ROUGE-L metric for a benchmark dataset (ELI5). We also show performance of our approach in zero-shot setting for another benchmark dataset (DebatePedia) -- our approach leads to results comparable to baselines, which were specifically trained on DebatePedia. To aid the RL training, we propose a better semantic similarity reward, enabled by a novel Passage Embedding scheme developed using Cluster Hypothesis. Lastly, we contribute a gold-standard test dataset to further research in QfS and Long-form Question Answering (LfQA)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Swaroop Nath",
      "Harshad Khadilkar",
      "Pushpak Bhattacharyya"
    ]
  },
  "http://arxiv.org/abs/2311.17502": {
    "title": "Enhancing Answer Selection in Community Question Answering with Pre-trained and Large Language Models",
    "volume": "Nov",
    "abstract": "Community Question Answering (CQA) becomes increasingly prevalent in recent years. However, there are a large number of answers, which is difficult for users to select the relevant answers. Therefore, answer selection is a very significant subtask of CQA. In this paper, we first propose the Question-Answer cross attention networks (QAN) with pre-trained models for answer selection and utilize large language model (LLM) to perform answer selection with knowledge augmentation. Specifically, we apply the BERT model as the encoder layer to do pre-training for question subjects, question bodies and answers, respectively, then the cross attention mechanism selects the most relevant answer for different questions. Experiments show that the QAN model achieves state-of-the-art performance on two datasets, SemEval2015 and SemEval2017. Moreover, we use the LLM to generate external knowledge from questions and correct answers to achieve knowledge augmentation for the answer selection task by LLM, while optimizing the prompt of LLM in different aspects. The results show that the introduction of external knowledge can improve the correct answer selection rate of LLM on datasets SemEval2015 and SemEval2017. Meanwhile, LLM can also select the correct answer on more questions by optimized prompt",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinghang Hu"
    ]
  },
  "http://arxiv.org/abs/2311.17492": {
    "title": "Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data",
    "volume": "Nov",
    "abstract": "The Manchu language, with its roots in the historical Manchurian region of Northeast China, is now facing a critical threat of extinction, as there are very few speakers left. In our efforts to safeguard the Manchu language, we introduce Mergen, the first-ever attempt at a Manchu-Korean Machine Translation (MT) model. To develop this model, we utilize valuable resources such as the Manwen Laodang(a historical book) and a Manchu-Korean dictionary. Due to the scarcity of a Manchu-Korean parallel dataset, we expand our data by employing word replacement guided by GloVe embeddings, trained on both monolingual and parallel texts. Our approach is built around an encoder-decoder neural machine translation model, incorporating a bi-directional Gated Recurrent Unit (GRU) layer. The experiments have yielded promising results, showcasing a significant enhancement in Manchu-Korean translation, with a remarkable 20-30 point increase in the BLEU score",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jean Seo",
      "Sungjoo Byun",
      "Minha Kang",
      "Sangah Lee"
    ]
  },
  "http://arxiv.org/abs/2311.17487": {
    "title": "Taiwan LLM: Bridging the Linguistic Divide with a Culturally Aligned Language Model",
    "volume": "Nov",
    "abstract": "In the realm of language models, the nuanced linguistic and cultural intricacies of Traditional Chinese, as spoken in Taiwan, have been largely overlooked. This paper introduces Taiwan LLM, a pioneering Large Language Model that specifically caters to the Traditional Chinese language, with a focus on the variant used in Taiwan. Leveraging a comprehensive pretraining corpus and instruction-finetuning datasets, we have developed a model that not only understands the complexities of Traditional Chinese but also embodies the cultural context of Taiwan. Taiwan LLM represents the first of its kind, a model that is not only linguistically accurate but also culturally resonant with its user base. Our evaluations demonstrate that Taiwan LLM achieves superior performance in understanding and generating Traditional Chinese text, outperforming existing models that are predominantly trained on Simplified Chinese or English. The open-source release of Taiwan LLM invites collaboration and further innovation, ensuring that the linguistic diversity of Chinese speakers is embraced and well-served. The model, datasets, and further resources are made publicly available to foster ongoing research and development in this field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yen-Ting Lin",
      "Yun-Nung Chen"
    ]
  },
  "http://arxiv.org/abs/2311.17438": {
    "title": "CLOMO: Counterfactual Logical Modification with Large Language Models",
    "volume": "Nov",
    "abstract": "In this study, we delve into the realm of counterfactual reasoning capabilities of large language models (LLMs). Our primary objective is to cultivate the counterfactual thought processes within LLMs and rigorously assess these processes for their validity. Specifically, we introduce a novel task, Counterfactual Logical Modification (CLOMO), and a high-quality human-annotated benchmark. In this task, LLMs must adeptly alter a given argumentative text to uphold a predetermined logical relationship. To effectively evaluate a generation model's counterfactual capabilities, we propose an innovative evaluation metric, the LogicAware Counterfactual Score to directly evaluate the natural language output of LLMs instead of modeling the task as a multiple-choice problem. Analysis shows that the proposed automatic metric aligns well with human preference. Our experimental results show that while LLMs demonstrate a notable capacity for logical counterfactual thinking, there remains a discernible gap between their current abilities and human performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinya Huang",
      "Ruixin Hong",
      "Hongming Zhang",
      "Wei Shao",
      "Zhicheng Yang",
      "Dong Yu",
      "Changshui Zhang",
      "Xiaodan Liang",
      "Linqi Song"
    ]
  },
  "http://arxiv.org/abs/2311.17429": {
    "title": "TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4",
    "volume": "Nov",
    "abstract": "Prompt-based learning has been widely applied in many low-resource NLP tasks such as few-shot scenarios. However, this paradigm has been shown to be vulnerable to backdoor attacks. Most of the existing attack methods focus on inserting manually predefined templates as triggers in the pre-training phase to train the victim model and utilize the same triggers in the downstream task to perform inference, which tends to ignore the transferability and stealthiness of the templates. In this work, we propose a novel approach of TARGET (Template-trAnsfeRable backdoor attack aGainst prompt-basEd NLP models via GPT4), which is a data-independent attack method. Specifically, we first utilize GPT4 to reformulate manual templates to generate tone-strong and normal templates, and the former are injected into the model as a backdoor trigger in the pre-training phase. Then, we not only directly employ the above templates in the downstream task, but also use GPT4 to generate templates with similar tone to the above templates to carry out transferable attacks. Finally we have conducted extensive experiments on five NLP datasets and three BERT series models, with experimental results justifying that our TARGET method has better attack performance and stealthiness compared to the two-external baseline methods on direct attacks, and in addition achieves satisfactory attack capability in the unseen tone-similar templates",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Tan",
      "Qingliang Chen",
      "Yongjian Huang",
      "Chen Liang"
    ]
  },
  "http://arxiv.org/abs/2311.17404": {
    "title": "VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models",
    "volume": "Nov",
    "abstract": "The ability to perceive how objects change over time is a crucial ingredient in human intelligence. However, current benchmarks cannot faithfully reflect the temporal understanding abilities of video-language models (VidLMs) due to the existence of static visual shortcuts. To remedy this issue, we present VITATECS, a diagnostic VIdeo-Text dAtaset for the evaluation of TEmporal Concept underStanding. Specifically, we first introduce a fine-grained taxonomy of temporal concepts in natural language in order to diagnose the capability of VidLMs to comprehend different temporal aspects. Furthermore, to disentangle the correlation between static and temporal information, we generate counterfactual video descriptions that differ from the original one only in the specified temporal aspect. We employ a semi-automatic data collection framework using large language models and human-in-the-loop annotation to obtain high-quality counterfactual descriptions efficiently. Evaluation of representative video-language understanding models confirms their deficiency in temporal understanding, revealing the need for greater emphasis on the temporal elements in video-language research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shicheng Li",
      "Lei Li",
      "Shuhuai Ren",
      "Yuanxin Liu",
      "Yi Liu",
      "Rundong Gao",
      "Xu Sun",
      "Lu Hou"
    ]
  },
  "http://arxiv.org/abs/2311.17400": {
    "title": "Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention",
    "volume": "Nov",
    "abstract": "Transformer-based models, such as BERT and GPT, have been widely adopted in natural language processing (NLP) due to their exceptional performance. However, recent studies show their vulnerability to textual adversarial attacks where the model's output can be misled by intentionally manipulating the text inputs. Despite various methods that have been proposed to enhance the model's robustness and mitigate this vulnerability, many require heavy consumption resources (e.g., adversarial training) or only provide limited protection (e.g., defensive dropout). In this paper, we propose a novel method called dynamic attention, tailored for the transformer architecture, to enhance the inherent robustness of the model itself against various adversarial attacks. Our method requires no downstream task knowledge and does not incur additional costs. The proposed dynamic attention consists of two modules: (I) attention rectification, which masks or weakens the attention value of the chosen tokens, and (ii) dynamic modeling, which dynamically builds the set of candidate tokens. Extensive experiments demonstrate that dynamic attention significantly mitigates the impact of adversarial attacks, improving up to 33\\% better performance than previous methods against widely-used adversarial attacks. The model-level design of dynamic attention enables it to be easily combined with other defense methods (e.g., adversarial training) to further enhance the model's robustness. Furthermore, we demonstrate that dynamic attention preserves the state-of-the-art robustness space of the original model compared to other dynamic modeling methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lujia Shen",
      "Yuwen Pu",
      "Shouling Ji",
      "Changjiang Li",
      "Xuhong Zhang",
      "Chunpeng Ge",
      "Ting Wang"
    ]
  },
  "http://arxiv.org/abs/2311.17391": {
    "title": "Unveiling the Implicit Toxicity in Large Language Models",
    "volume": "Nov",
    "abstract": "The open-endedness of large language models (LLMs) combined with their impressive capabilities may lead to new safety issues when being exploited for malicious use. While recent studies primarily focus on probing toxic outputs that can be easily detected with existing toxicity classifiers, we show that LLMs can generate diverse implicit toxic outputs that are exceptionally difficult to detect via simply zero-shot prompting. Moreover, we propose a reinforcement learning (RL) based attacking method to further induce the implicit toxicity in LLMs. Specifically, we optimize the language model with a reward that prefers implicit toxic outputs to explicit toxic and non-toxic ones. Experiments on five widely-adopted toxicity classifiers demonstrate that the attack success rate can be significantly improved through RL fine-tuning. For instance, the RL-finetuned LLaMA-13B model achieves an attack success rate of 90.04% on BAD and 62.85% on Davinci003. Our findings suggest that LLMs pose a significant threat in generating undetectable implicit toxic outputs. We further show that fine-tuning toxicity classifiers on the annotated examples from our attacking method can effectively enhance their ability to detect LLM-generated implicit toxic language. The code is publicly available at https://github.com/thu-coai/Implicit-Toxicity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Wen",
      "Pei Ke",
      "Hao Sun",
      "Zhexin Zhang",
      "Chengfei Li",
      "Jinfeng Bai",
      "Minlie Huang"
    ]
  },
  "http://arxiv.org/abs/2311.17376": {
    "title": "CESAR: Automatic Induction of Compositional Instructions for Multi-turn Dialogs",
    "volume": "Nov",
    "abstract": "Instruction-based multitasking has played a critical role in the success of large language models (LLMs) in multi-turn dialog applications. While publicly available LLMs have shown promising performance, when exposed to complex instructions with multiple constraints, they lag against state-of-the-art models like ChatGPT. In this work, we hypothesize that the availability of large-scale complex demonstrations is crucial in bridging this gap. Focusing on dialog applications, we propose a novel framework, CESAR, that unifies a large number of dialog tasks in the same format and allows programmatic induction of complex instructions without any manual effort. We apply CESAR on InstructDial, a benchmark for instruction-based dialog tasks. We further enhance InstructDial with new datasets and tasks and utilize CESAR to induce complex tasks with compositional instructions. This results in a new benchmark called InstructDial++, which includes 63 datasets with 86 basic tasks and 68 composite tasks. Through rigorous experiments, we demonstrate the scalability of CESAR in providing rich instructions. Models trained on InstructDial++ can follow compositional prompts, such as prompts that ask for multiple stylistic constraints",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taha Aksu",
      "Devamanyu Hazarika",
      "Shikib Mehri",
      "Seokhwan Kim",
      "Dilek Hakkani-Tr",
      "Yang Liu",
      "Mahdi Namazifar"
    ]
  },
  "http://arxiv.org/abs/2311.17371": {
    "title": "Are we going MAD? Benchmarking Multi-Agent Debate between Language Models for Medical Q&A",
    "volume": "Nov",
    "abstract": "Recent advancements in large language models (LLMs) underscore their potential for responding to medical inquiries. However, ensuring that generative agents provide accurate and reliable answers remains an ongoing challenge. In this context, multi-agent debate (MAD) has emerged as a prominent strategy for enhancing the truthfulness of LLMs. In this work, we provide a comprehensive benchmark of MAD strategies for medical Q&A, along with open-source implementations. This explores the effective utilization of various strategies including the trade-offs between cost, time, and accuracy. We build upon these insights to provide a novel debate-prompting strategy based on agent agreement that outperforms previously published strategies on medical Q&A tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andries Smit",
      "Paul Duckworth",
      "Nathan Grinsztajn",
      "Kale-ab Tessera",
      "Thomas D. Barrett",
      "Arnu Pretorius"
    ]
  },
  "http://arxiv.org/abs/2311.17355": {
    "title": "Are Large Language Models Good Fact Checkers: A Preliminary Study",
    "volume": "Nov",
    "abstract": "Recently, Large Language Models (LLMs) have drawn significant attention due to their outstanding reasoning capabilities and extensive knowledge repository, positioning them as superior in handling various natural language processing tasks compared to other language models. In this paper, we present a preliminary investigation into the potential of LLMs in fact-checking. This study aims to comprehensively evaluate various LLMs in tackling specific fact-checking subtasks, systematically evaluating their capabilities, and conducting a comparative analysis of their performance against pre-trained and state-of-the-art low-parameter models. Experiments demonstrate that LLMs achieve competitive performance compared to other small models in most scenarios. However, they encounter challenges in effectively handling Chinese fact verification and the entirety of the fact-checking pipeline due to language inconsistencies and hallucinations. These findings underscore the need for further exploration and research to enhance the proficiency of LLMs as reliable fact-checkers, unveiling the potential capability of LLMs and the possible challenges in fact-checking tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Cao",
      "Lingwei Wei",
      "Mengyang Chen",
      "Wei Zhou",
      "Songlin Hu"
    ]
  },
  "http://arxiv.org/abs/2311.17352": {
    "title": "Efficient Stitchable Task Adaptation",
    "volume": "Nov",
    "abstract": "The paradigm of pre-training and fine-tuning has laid the foundation for deploying deep learning models. However, most fine-tuning methods are designed to meet a specific resource budget. Recently, considering diverse deployment scenarios with various resource budgets, stitchable neural network (SN-Net) is introduced to quickly obtain numerous new networks (stitches) from the pre-trained models (anchors) in a model family via model stitching. Although promising, SN-Net confronts new challenges when adapting it to new target domains, including huge memory and storage requirements and a long and sub-optimal multistage adaptation process. In this work, we present a novel framework, Efficient Stitchable Task Adaptation (ESTA), to efficiently produce a palette of fine-tuned models that adhere to diverse resource constraints. Specifically, we first tailor parameter-efficient fine-tuning to share low-rank updates among the stitches while maintaining independent bias terms. In this way, we largely reduce fine-tuning memory burdens and mitigate the interference among stitches that arises in task adaptation. Furthermore, we streamline a simple yet effective one-stage deployment pipeline, which estimates the important stitches to deploy with training-time gradient statistics. By assigning higher sampling probabilities to important stitches, we also get a boosted Pareto frontier. Extensive experiments on 25 downstream visual recognition tasks demonstrate that our ESTA is capable of generating stitches with smooth accuracy-efficiency trade-offs and surpasses the direct SN-Net adaptation by remarkable margins with significantly lower training time and fewer trainable parameters. Furthermore, we demonstrate the flexibility and scalability of our ESTA framework by stitching LLMs from LLaMA family, obtaining chatbot stitches of assorted sizes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu He",
      "Zizheng Pan",
      "Jing Liu",
      "Jianfei Cai",
      "Bohan Zhuang"
    ]
  },
  "http://arxiv.org/abs/2311.17351": {
    "title": "Exploring Large Language Models for Human Mobility Prediction under Public Events",
    "volume": "Nov",
    "abstract": "Public events, such as concerts and sports games, can be major attractors for large crowds, leading to irregular surges in travel demand. Accurate human mobility prediction for public events is thus crucial for event planning as well as traffic or crowd management. While rich textual descriptions about public events are commonly available from online sources, it is challenging to encode such information in statistical or machine learning models. Existing methods are generally limited in incorporating textual information, handling data sparsity, or providing rationales for their predictions. To address these challenges, we introduce a framework for human mobility prediction under public events (LLM-MPE) based on Large Language Models (LLMs), leveraging their unprecedented ability to process textual data, learn from minimal examples, and generate human-readable explanations. Specifically, LLM-MPE first transforms raw, unstructured event descriptions from online sources into a standardized format, and then segments historical mobility data into regular and event-related components. A prompting strategy is designed to direct LLMs in making and rationalizing demand predictions considering historical mobility and event features. A case study is conducted for Barclays Center in New York City, based on publicly available event information and taxi trip data. Results show that LLM-MPE surpasses traditional models, particularly on event days, with textual data significantly enhancing its accuracy. Furthermore, LLM-MPE offers interpretable insights into its predictions. Despite the great potential of LLMs, we also identify key challenges including misinformation and high costs that remain barriers to their broader adoption in large-scale human mobility analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuebing Liang",
      "Yichao Liu",
      "Xiaohan Wang",
      "Zhan Zhao"
    ]
  },
  "http://arxiv.org/abs/2311.17946": {
    "title": "DreamSync: Aligning Text-to-Image Generation with Image Understanding Feedback",
    "volume": "Nov",
    "abstract": "Despite their wide-spread success, Text-to-Image models (T2I) still struggle to produce images that are both aesthetically pleasing and faithful to the user's input text. We introduce DreamSync, a model-agnostic training algorithm by design that improves T2I models to be faithful to the text input. DreamSync builds off a recent insight from TIFA's evaluation framework -- that large vision-language models (VLMs) can effectively identify the fine-grained discrepancies between generated images and the text inputs. DreamSync uses this insight to train T2I models without any labeled data; it improves T2I models using its own generations. First, it prompts the model to generate several candidate images for a given input text. Then, it uses two VLMs to select the best generation: a Visual Question Answering model that measures the alignment of generated images to the text, and another that measures the generation's aesthetic quality. After selection, we use LoRA to iteratively finetune the T2I model to guide its generation towards the selected best generations. DreamSync does not need any additional human annotation. model architecture changes, or reinforcement learning. Despite its simplicity, DreamSync improves both the semantic alignment and aesthetic appeal of two diffusion-based T2I models, evidenced by multiple benchmarks (+1.7% on TIFA, +2.9% on DSG1K, +3.4% on VILA aesthetic) and human evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiao Sun",
      "Deqing Fu",
      "Yushi Hu",
      "Su Wang",
      "Royi Rassin",
      "Da-Cheng Juan",
      "Dana Alon",
      "Charles Herrmann",
      "Sjoerd van Steenkiste",
      "Ranjay Krishna",
      "Cyrus Rashtchian"
    ]
  },
  "http://arxiv.org/abs/2311.17330": {
    "title": "Biomedical knowledge graph-enhanced prompt generation for large language models",
    "volume": "Nov",
    "abstract": "Large Language Models (LLMs) have been driving progress in AI at an unprecedented rate, yet still face challenges in knowledge-intensive domains like biomedicine. Solutions such as pre-training and domain-specific fine-tuning add substantial computational overhead, and the latter require domain-expertise. External knowledge infusion is task-specific and requires model training. Here, we introduce a task-agnostic Knowledge Graph-based Retrieval Augmented Generation (KG-RAG) framework by leveraging the massive biomedical KG SPOKE with LLMs such as Llama-2-13b, GPT-3.5-Turbo and GPT-4, to generate meaningful biomedical text rooted in established knowledge. KG-RAG consistently enhanced the performance of LLMs across various prompt types, including one-hop and two-hop prompts, drug repurposing queries, biomedical true/false questions, and multiple-choice questions (MCQ). Notably, KG-RAG provides a remarkable 71% boost in the performance of the Llama-2 model on the challenging MCQ dataset, demonstrating the framework's capacity to empower open-source models with fewer parameters for domain-specific questions. Furthermore, KG-RAG enhanced the performance of proprietary GPT models, such as GPT-3.5 which exhibited improvement over GPT-4 in context utilization on MCQ data. Our approach was also able to address drug repurposing questions, returning meaningful repurposing suggestions. In summary, the proposed framework combines explicit and implicit knowledge of KG and LLM, respectively, in an optimized fashion, thus enhancing the adaptability of general-purpose LLMs to tackle domain-specific questions in a unified framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karthik Soman",
      "Peter W Rose",
      "John H Morris",
      "Rabia E Akbas",
      "Brett Smith",
      "Braian Peetoom",
      "Catalina Villouta-Reyes",
      "Gabriel Cerono",
      "Yongmei Shi",
      "Angela Rizk-Jackson",
      "Sharat Israni",
      "Charlotte A Nelson",
      "Sui Huang",
      "Sergio E Baranzini"
    ]
  },
  "http://arxiv.org/abs/2311.17311": {
    "title": "Universal Self-Consistency for Large Language Model Generation",
    "volume": "Nov",
    "abstract": "Self-consistency with chain-of-thought prompting (CoT) has demonstrated remarkable performance gains on various challenging tasks, by utilizing multiple reasoning paths sampled from large language models (LLMs). However, self-consistency relies on the answer extraction process to aggregate multiple solutions, which is not applicable to free-form answers. In this work, we propose Universal Self-Consistency (USC), which leverages LLMs themselves to select the most consistent answer among multiple candidates. We evaluate USC on a variety of benchmarks, including mathematical reasoning, code generation, long-context summarization, and open-ended question answering. On open-ended generation tasks where the original self-consistency method is not applicable, USC effectively utilizes multiple samples and improves the performance. For mathematical reasoning, USC matches the standard self-consistency performance without requiring the answer formats to be similar. Finally, without access to execution results, USC also matches the execution-based voting performance on code generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyun Chen",
      "Renat Aksitov",
      "Uri Alon",
      "Jie Ren",
      "Kefan Xiao",
      "Pengcheng Yin",
      "Sushant Prakash",
      "Charles Sutton",
      "Xuezhi Wang",
      "Denny Zhou"
    ]
  },
  "http://arxiv.org/abs/2311.17307": {
    "title": "RoKEPG: RoBERTa and Knowledge Enhancement for Prescription Generation of Traditional Chinese Medicine",
    "volume": "Nov",
    "abstract": "Traditional Chinese medicine (TCM) prescription is the most critical form of TCM treatment, and uncovering the complex nonlinear relationship between symptoms and TCM is of great significance for clinical practice and assisting physicians in diagnosis and treatment. Although there have been some studies on TCM prescription generation, these studies consider a single factor and directly model the symptom-prescription generation problem mainly based on symptom descriptions, lacking guidance from TCM knowledge. To this end, we propose a RoBERTa and Knowledge Enhancement model for Prescription Generation of Traditional Chinese Medicine (RoKEPG). RoKEPG is firstly pre-trained by our constructed TCM corpus, followed by fine-tuning the pre-trained model, and the model is guided to generate TCM prescriptions by introducing four classes of knowledge of TCM through the attention mask matrix. Experimental results on the publicly available TCM prescription dataset show that RoKEPG improves the F1 metric by about 2% over the baseline model with the best results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hua Pu",
      "Jiacong Mi",
      "Shan Lu",
      "Jieyue He"
    ]
  },
  "http://arxiv.org/abs/2311.17301": {
    "title": "Language Models: A Guide for the Perplexed",
    "volume": "Nov",
    "abstract": "Given the growing importance of AI literacy, we decided to write this tutorial to help narrow the gap between the discourse among those who study language models -- the core technology underlying ChatGPT and similar products -- and those who are intrigued and want to learn more about them. In short, we believe the perspective of researchers and educators can add some clarity to the public's understanding of the technologies beyond what's currently available, which tends to be either extremely technical or promotional material generated about products by their purveyors. Our approach teases apart the concept of a language model from products built on them, from the behaviors attributed to or desired from those products, and from claims about similarity to human cognition. As a starting point, we (1) offer a scientific viewpoint that focuses on questions amenable to study through experimentation; (2) situate language models as they are today in the context of the research that led to their development; and (3) describe the boundaries of what is known about the models at this writing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sofia Serrano",
      "Zander Brumbaugh",
      "Noah A. Smith"
    ]
  },
  "http://arxiv.org/abs/2311.17295": {
    "title": "Elo Uncovered: Robustness and Best Practices in Language Model Evaluation",
    "volume": "Nov",
    "abstract": "In Natural Language Processing (NLP), the Elo rating system, originally designed for ranking players in dynamic games such as chess, is increasingly being used to evaluate Large Language Models (LLMs) through \"A vs B\" paired comparisons. However, while popular, the system's suitability for assessing entities with constant skill levels, such as LLMs, remains relatively unexplored. We study two fundamental axioms that evaluation methods should adhere to: reliability and transitivity. We conduct extensive evaluation of Elo behaviour, illustrating that individual Elo computations exhibit volatility and delving into the impact of varying the Elo rating system's hyperparameters. We show that these axioms are not always satisfied raising questions about the reliability of current comparative evaluations of LLMs. If the current use of Elo scores is intended to substitute the costly head-to-head comparison of LLMs, it is crucial to ensure the ranking is as robust as possible. Guided by the axioms, our findings offer concrete guidelines for enhancing the reliability of LLM evaluation methods, suggesting a need for reassessment of existing comparative approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meriem Boubdir",
      "Edward Kim",
      "Beyza Ermis",
      "Sara Hooker",
      "Marzieh Fadaee"
    ]
  },
  "http://arxiv.org/abs/2311.17280": {
    "title": "Does VLN Pretraining Work with Nonsensical or Irrelevant Instructions?",
    "volume": "Dec",
    "abstract": "Data augmentation via back-translation is common when pretraining Vision-and-Language Navigation (VLN) models, even though the generated instructions are noisy. But: does that noise matter? We find that nonsensical or irrelevant language instructions during pretraining can have little effect on downstream performance for both HAMT and VLN-BERT on R2R, and is still better than only using clean, human data. To underscore these results, we concoct an efficient augmentation method, Unigram + Object, which generates nonsensical instructions that nonetheless improve downstream performance. Our findings suggest that what matters for VLN R2R pretraining is the quantity of visual trajectories, not the quality of instructions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wang Zhu",
      "Ishika Singh",
      "Yuan Huang",
      "Robin Jia",
      "Jesse Thomason"
    ]
  },
  "http://arxiv.org/abs/2311.17264": {
    "title": "RETSim: Resilient and Efficient Text Similarity",
    "volume": "Nov",
    "abstract": "This paper introduces RETSim (Resilient and Efficient Text Similarity), a lightweight, multilingual deep learning model trained to produce robust metric embeddings for near-duplicate text retrieval, clustering, and dataset deduplication tasks. We demonstrate that RETSim is significantly more robust and accurate than MinHash and neural text embeddings, achieving new state-of-the-art performance on dataset deduplication, adversarial text retrieval benchmarks, and spam clustering tasks. We also introduce the W4NT3D benchmark (Wiki-40B 4dversarial Near-T3xt Dataset) for evaluating multilingual, near-duplicate text retrieval capabilities under adversarial settings. RETSim and the W4NT3D benchmark are open-sourced under the MIT License at https://github.com/google/unisim",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marina Zhang",
      "Owen Vallis",
      "Aysegul Bumin",
      "Tanay Vakharia",
      "Elie Bursztein"
    ]
  },
  "http://arxiv.org/abs/2311.17233": {
    "title": "Quantifying the redundancy between prosody and text",
    "volume": "Nov",
    "abstract": "Prosody -- the suprasegmental component of speech, including pitch, loudness, and tempo -- carries critical aspects of meaning. However, the relationship between the information conveyed by prosody vs. by the words themselves remains poorly understood. We use large language models (LLMs) to estimate how much information is redundant between prosody and the words themselves. Using a large spoken corpus of English audiobooks, we extract prosodic features aligned to individual words and test how well they can be predicted from LLM embeddings, compared to non-contextual word embeddings. We find a high degree of redundancy between the information carried by the words and prosodic information across several prosodic features, including intensity, duration, pauses, and pitch contours. Furthermore, a word's prosodic information is redundant with both the word itself and the context preceding as well as following it. Still, we observe that prosodic features can not be fully predicted from text, suggesting that prosody carries information above and beyond the words. Along with this paper, we release a general-purpose data processing pipeline for quantifying the relationship between linguistic information and extra-linguistic features",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lukas Wolf",
      "Tiago Pimentel",
      "Evelina Fedorenko",
      "Ryan Cotterell",
      "Alex Warstadt",
      "Ethan Wilcox",
      "Tamar Regev"
    ]
  },
  "http://arxiv.org/abs/2311.17227": {
    "title": "War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars",
    "volume": "Nov",
    "abstract": "Can we avoid wars at the crossroads of history? This question has been pursued by individuals, scholars, policymakers, and organizations throughout human history. In this research, we attempt to answer the question based on the recent advances of Artificial Intelligence (AI) and Large Language Models (LLMs). We propose \\textbf{WarAgent}, an LLM-powered multi-agent AI system, to simulate the participating countries, their decisions, and the consequences, in historical international conflicts, including the World War I (WWI), the World War II (WWII), and the Warring States Period (WSP) in Ancient China. By evaluating the simulation effectiveness, we examine the advancements and limitations of cutting-edge AI systems' abilities in studying complex collective human behaviors such as international conflicts under diverse settings. In these simulations, the emergent interactions among agents also offer a novel perspective for examining the triggers and conditions that lead to war. Our findings offer data-driven and AI-augmented insights that can redefine how we approach conflict resolution and peacekeeping strategies. The implications stretch beyond historical analysis, offering a blueprint for using AI to understand human history and possibly prevent future international conflicts. Code and data are available at \\url{https://github.com/agiresearch/WarAgent}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyue Hua",
      "Lizhou Fan",
      "Lingyao Li",
      "Kai Mei",
      "Jianchao Ji",
      "Yingqiang Ge",
      "Libby Hemphill",
      "Yongfeng Zhang"
    ]
  },
  "http://arxiv.org/abs/2311.17213": {
    "title": "General-Purpose vs. Domain-Adapted Large Language Models for Extraction of Data from Thoracic Radiology Reports",
    "volume": "Dec",
    "abstract": "Radiologists produce unstructured data that could be valuable for clinical care when consumed by information systems. However, variability in style limits usage. Study compares performance of system using domain-adapted language model (RadLing) and general-purpose large language model (GPT-4) in extracting common data elements (CDE) from thoracic radiology reports. Three radiologists annotated a retrospective dataset of 1300 thoracic reports (900 training, 400 test) and mapped to 21 pre-selected relevant CDEs. RadLing was used to generate embeddings for sentences and identify CDEs using cosine-similarity, which were mapped to values using light-weight mapper. GPT-4 system used OpenAI's general-purpose embeddings to identify relevant CDEs and used GPT-4 to map to values. The output CDE:value pairs were compared to the reference standard; an identical match was considered true positive. Precision (positive predictive value) was 96% (2700/2824) for RadLing and 99% (2034/2047) for GPT-4. Recall (sensitivity) was 94% (2700/2876) for RadLing and 70% (2034/2887) for GPT-4; the difference was statistically significant (P<.001). RadLing's domain-adapted embeddings were more sensitive in CDE identification (95% vs 71%) and its light-weight mapper had comparable precision in value assignment (95.4% vs 95.0%). RadLing system exhibited higher performance than GPT-4 system in extracting CDEs from radiology reports. RadLing system's domain-adapted embeddings outperform general-purpose embeddings from OpenAI in CDE identification and its light-weight value mapper achieves comparable precision to large GPT-4. RadLing system offers operational advantages including local deployment and reduced runtime costs. Domain-adapted RadLing system surpasses GPT-4 system in extracting common data elements from radiology reports, while providing benefits of local deployment and lower costs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali H. Dhanaliwala",
      "Rikhiya Ghosh",
      "Sanjeev Kumar Karn",
      "Poikavila Ullaskrishnan",
      "Oladimeji Farri",
      "Dorin Comaniciu",
      "Charles E. Kahn"
    ]
  },
  "http://arxiv.org/abs/2311.17154": {
    "title": "Pragmatic Radiology Report Generation",
    "volume": "Nov",
    "abstract": "When pneumonia is not found on a chest X-ray, should the report describe this negative observation or omit it? We argue that this question cannot be answered from the X-ray alone and requires a pragmatic perspective, which captures the communicative goal that radiology reports serve between radiologists and patients. However, the standard image-to-text formulation for radiology report generation fails to incorporate such pragmatic intents. Following this pragmatic perspective, we demonstrate that the indication, which describes why a patient comes for an X-ray, drives the mentions of negative observations and introduce indications as additional input to report generation. With respect to the output, we develop a framework to identify uninferable information from the image as a source of model hallucinations, and limit them by cleaning groundtruth reports. Finally, we use indications and cleaned groundtruth reports to develop pragmatic models, and show that they outperform existing methods not only in new pragmatics-inspired metrics (+4.3 Negative F1) but also in standard metrics (+6.3 Positive F1 and +11.0 BLEU-2)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dang Nguyen",
      "Chacha Chen",
      "He He",
      "Chenhao Tan"
    ]
  },
  "http://arxiv.org/abs/2311.17136": {
    "title": "UniIR: Training and Benchmarking Universal Multimodal Information Retrievers",
    "volume": "Nov",
    "abstract": "Existing information retrieval (IR) models often assume a homogeneous format, limiting their applicability to diverse user needs, such as searching for images with text descriptions, searching for a news article with a headline image, or finding a similar photo with a query image. To approach such different information-seeking demands, we introduce UniIR, a unified instruction-guided multimodal retriever capable of handling eight distinct retrieval tasks across modalities. UniIR, a single retrieval system jointly trained on ten diverse multimodal-IR datasets, interprets user instructions to execute various retrieval tasks, demonstrating robust performance across existing datasets and zero-shot generalization to new tasks. Our experiments highlight that multi-task training and instruction tuning are keys to UniIR's generalization ability. Additionally, we construct the M-BEIR, a multimodal retrieval benchmark with comprehensive results, to standardize the evaluation of universal multimodal information retrieval",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Wei",
      "Yang Chen",
      "Haonan Chen",
      "Hexiang Hu",
      "Ge Zhang",
      "Jie Fu",
      "Alan Ritter",
      "Wenhu Chen"
    ]
  },
  "http://arxiv.org/abs/2311.17049": {
    "title": "MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training",
    "volume": "Nov",
    "abstract": "Contrastive pretraining of image-text foundation models, such as CLIP, demonstrated excellent zero-shot performance and improved robustness on a wide range of downstream tasks. However, these models utilize large transformer-based encoders with significant memory and latency overhead which pose challenges for deployment on mobile devices. In this work, we introduce MobileCLIP -- a new family of efficient image-text models optimized for runtime performance along with a novel and efficient training approach, namely multi-modal reinforced training. The proposed training approach leverages knowledge transfer from an image captioning model and an ensemble of strong CLIP encoders to improve the accuracy of efficient models. Our approach avoids train-time compute overhead by storing the additional knowledge in a reinforced dataset. MobileCLIP sets a new state-of-the-art latency-accuracy tradeoff for zero-shot classification and retrieval tasks on several datasets. Our MobileCLIP-S2 variant is 2.3$\\times$ faster while more accurate compared to previous best CLIP model based on ViT-B/16. We further demonstrate the effectiveness of our multi-modal reinforced training by training a CLIP model based on ViT-B/16 image backbone and achieving +2.9% average performance improvement on 38 evaluation benchmarks compared to the previous best. Moreover, we show that the proposed approach achieves 10$\\times$-1000$\\times$ improved learning efficiency when compared with non-reinforced CLIP training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pavan Kumar Anasosalu Vasu",
      "Hadi Pouransari",
      "Fartash Faghri",
      "Raviteja Vemulapalli",
      "Oncel Tuzel"
    ]
  },
  "http://arxiv.org/abs/2311.17043": {
    "title": "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models",
    "volume": "Nov",
    "abstract": "In this work, we present a novel method to tackle the token generation challenge in Vision Language Models (VLMs) for video and image understanding, called LLaMA-VID. Current VLMs, while proficient in tasks like image captioning and visual question answering, face computational burdens when processing long videos due to the excessive visual tokens. LLaMA-VID addresses this issue by representing each frame with two distinct tokens, namely context token and content token. The context token encodes the overall image context based on user input, whereas the content token encapsulates visual cues in each frame. This dual-token strategy significantly reduces the overload of long videos while preserving critical information. Generally, LLaMA-VID empowers existing frameworks to support hour-long videos and pushes their upper limit with an extra context token. It is proved to surpass previous methods on most of video- or image-based benchmarks. Code is available https://github.com/dvlab-research/LLaMA-VID}{https://github.com/dvlab-research/LLaMA-VID",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanwei Li",
      "Chengyao Wang",
      "Jiaya Jia"
    ]
  },
  "http://arxiv.org/abs/2311.17041": {
    "title": "Efficient In-Context Learning in Vision-Language Models for Egocentric Videos",
    "volume": "Nov",
    "abstract": "Recent advancements in text-only large language models (LLMs) have highlighted the benefit of in-context learning for adapting to new tasks with a few demonstrations. However, extending in-context learning to large vision-language models (VLMs) using a huge amount of naturalistic vision-language data has shown limited success, particularly for egocentric videos, due to high data collection costs. We propose a novel training method $\\mathbb{E}$fficient $\\mathbb{I}$n-context $\\mathbb{L}$earning on $\\mathbb{E}$gocentric $\\mathbb{V}$ideos ($\\mathbb{EILEV}$), which elicits in-context learning in VLMs for egocentric videos without requiring massive, naturalistic egocentric video datasets. $\\mathbb{EILEV}$ involves architectural and training data adaptations to allow the model to process contexts interleaved with video clips and narrations, sampling of in-context examples with clusters of similar verbs and nouns, use of data with skewed marginal distributions with a long tail of infrequent verbs and nouns, as well as homonyms and synonyms. Our evaluations show that $\\mathbb{EILEV}$-trained models outperform larger VLMs trained on a huge amount of naturalistic data in in-context learning. Furthermore, they can generalize to not only out-of-distribution, but also novel, rare egocentric videos and texts via in-context learning, demonstrating potential for applications requiring cost-effective training, and rapid post-deployment adaptability. Our code and demo are available at \\url{https://github.com/yukw777/EILEV}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keunwoo Peter Yu",
      "Zheyuan Zhang",
      "Fengyuan Hu",
      "Joyce Chai"
    ]
  },
  "http://arxiv.org/abs/2311.17035": {
    "title": "Scalable Extraction of Training Data from (Production) Language Models",
    "volume": "Nov",
    "abstract": "This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Milad Nasr",
      "Nicholas Carlini",
      "Jonathan Hayase",
      "Matthew Jagielski",
      "A. Feder Cooper",
      "Daphne Ippolito",
      "Christopher A. Choquette-Choo",
      "Eric Wallace",
      "Florian Tramr",
      "Katherine Lee"
    ]
  },
  "http://arxiv.org/abs/2311.17030": {
    "title": "Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching",
    "volume": "Nov",
    "abstract": "Mechanistic interpretability aims to understand model behaviors in terms of specific, interpretable features, often hypothesized to manifest as low-dimensional subspaces of activations. Specifically, recent studies have explored subspace interventions (such as activation patching) as a way to simultaneously manipulate model behavior and attribute the features behind it to given subspaces. In this work, we demonstrate that these two aims diverge, potentially leading to an illusory sense of interpretability. Counterintuitively, even if a subspace intervention makes the model's output behave as if the value of a feature was changed, this effect may be achieved by activating a dormant parallel pathway leveraging another subspace that is causally disconnected from model outputs. We demonstrate this phenomenon in a distilled mathematical example, in two real-world domains (the indirect object identification task and factual recall), and present evidence for its prevalence in practice. In the context of factual recall, we further show a link to rank-1 fact editing, providing a mechanistic explanation for previous work observing an inconsistency between fact editing performance and fact localization. However, this does not imply that activation patching of subspaces is intrinsically unfit for interpretability. To contextualize our findings, we also show what a success case looks like in a task (indirect object identification) where prior manual circuit analysis informs an understanding of the location of a feature. We explore the additional evidence needed to argue that a patched subspace is faithful",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksandar Makelov",
      "Georg Lange",
      "Neel Nanda"
    ]
  },
  "http://arxiv.org/abs/2311.16989": {
    "title": "ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?",
    "volume": "Nov",
    "abstract": "Upon its release in late 2022, ChatGPT has brought a seismic shift in the entire landscape of AI, both in research and commerce. Through instruction-tuning a large language model (LLM) with supervised fine-tuning and reinforcement learning from human feedback, it showed that a model could answer human questions and follow instructions on a broad panel of tasks. Following this success, interests in LLMs have intensified, with new LLMs flourishing at frequent interval across academia and industry, including many start-ups focused on LLMs. While closed-source LLMs (e.g., OpenAI's GPT, Anthropic's Claude) generally outperform their open-source counterparts, the progress on the latter has been rapid with claims of achieving parity or even better on certain tasks. This has crucial implications not only on research but also on business. In this work, on the first anniversary of ChatGPT, we provide an exhaustive overview of this success, surveying all tasks where an open-source LLM has claimed to be on par or better than ChatGPT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hailin Chen",
      "Fangkai Jiao",
      "Xingxuan Li",
      "Chengwei Qin",
      "Mathieu Ravaut",
      "Ruochen Zhao",
      "Caiming Xiong",
      "Shafiq Joty"
    ]
  },
  "http://arxiv.org/abs/2311.16978": {
    "title": "Assessing the influence of attractor-verb distance on grammatical agreement in humans and language models",
    "volume": "Nov",
    "abstract": "Subject-verb agreement in the presence of an attractor noun located between the main noun and the verb elicits complex behavior: judgments of grammaticality are modulated by the grammatical features of the attractor. For example, in the sentence \"The girl near the boys likes climbing\", the attractor (boys) disagrees in grammatical number with the verb (likes), creating a locally implausible transition probability. Here, we parametrically modulate the distance between the attractor and the verb while keeping the length of the sentence equal. We evaluate the performance of both humans and two artificial neural network models: both make more mistakes when the attractor is closer to the verb, but neural networks get close to the chance level while humans are mostly able to overcome the attractor interference. Additionally, we report a linear effect of attractor distance on reaction times. We hypothesize that a possible reason for the proximity effect is the calculation of transition probabilities between adjacent words. Nevertheless, classical models of attraction such as the cue-based model might suffice to explain this phenomenon, thus paving the way for new research. Data and analyses available at https://osf.io/d4g6k",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christos-Nikolaos Zacharopoulos",
      "Tho Desbordes",
      "Mathias Sabl-Meyer"
    ]
  },
  "http://arxiv.org/abs/2311.16965": {
    "title": "Natural Language Processing Through Transfer Learning: A Case Study on Sentiment Analysis",
    "volume": "Nov",
    "abstract": "Artificial intelligence and machine learning have significantly bolstered the technological world. This paper explores the potential of transfer learning in natural language processing focusing mainly on sentiment analysis. The models trained on the big data can also be used where data are scarce. The claim is that, compared to training models from scratch, transfer learning, using pre-trained BERT models, can increase sentiment classification accuracy. The study adopts a sophisticated experimental design that uses the IMDb dataset of sentimentally labelled movie reviews. Pre-processing includes tokenization and encoding of text data, making it suitable for NLP models. The dataset is used on a BERT based model, measuring its performance using accuracy. The result comes out to be 100 per cent accurate. Although the complete accuracy could appear impressive, it might be the result of overfitting or a lack of generalization. Further analysis is required to ensure the model's ability to handle diverse and unseen data. The findings underscore the effectiveness of transfer learning in NLP, showcasing its potential to excel in sentiment analysis tasks. However, the research calls for a cautious interpretation of perfect accuracy and emphasizes the need for additional measures to validate the model's generalization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aman Yadav",
      "Abhishek Vichare"
    ]
  },
  "http://arxiv.org/abs/2311.16941": {
    "title": "Debiasing Multimodal Models via Causal Information Minimization",
    "volume": "Nov",
    "abstract": "Most existing debiasing methods for multimodal models, including causal intervention and inference methods, utilize approximate heuristics to represent the biases, such as shallow features from early stages of training or unimodal features for multimodal tasks like VQA, etc., which may not be accurate. In this paper, we study bias arising from confounders in a causal graph for multimodal data and examine a novel approach that leverages causally-motivated information minimization to learn the confounder representations. Robust predictive features contain diverse information that helps a model generalize to out-of-distribution data. Hence, minimizing the information content of features obtained from a pretrained biased model helps learn the simplest predictive features that capture the underlying data distribution. We treat these features as confounder representations and use them via methods motivated by causal theory to remove bias from models. We find that the learned confounder representations indeed capture dataset biases, and the proposed debiasing methods improve out-of-distribution (OOD) performance on multiple multimodal datasets without sacrificing in-distribution performance. Additionally, we introduce a novel metric to quantify the sufficiency of spurious features in models' predictions that further demonstrates the effectiveness of our proposed methods. Our code is available at: https://github.com/Vaidehi99/CausalInfoMin",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vaidehi Patil",
      "Adyasha Maharana",
      "Mohit Bansal"
    ]
  },
  "http://arxiv.org/abs/2311.16922": {
    "title": "Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding",
    "volume": "Nov",
    "abstract": "Large Vision-Language Models (LVLMs) have advanced considerably, intertwining visual recognition and language understanding to generate content that is not only coherent but also contextually attuned. Despite their success, LVLMs still suffer from the issue of object hallucinations, where models generate plausible yet incorrect outputs that include objects that do not exist in the images. To mitigate this issue, we introduce Visual Contrastive Decoding (VCD), a simple and training-free method that contrasts output distributions derived from original and distorted visual inputs. The proposed VCD effectively reduces the over-reliance on statistical bias and unimodal priors, two essential causes of object hallucinations. This adjustment ensures the generated content is closely grounded to visual inputs, resulting in contextually accurate outputs. Our experiments show that VCD, without either additional training or the usage of external tools, significantly mitigates the object hallucination issue across different LVLM families. Beyond mitigating object hallucinations, VCD also excels in general LVLM benchmarks, highlighting its wide-ranging applicability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sicong Leng",
      "Hang Zhang",
      "Guanzheng Chen",
      "Xin Li",
      "Shijian Lu",
      "Chunyan Miao",
      "Lidong Bing"
    ]
  },
  "http://arxiv.org/abs/2311.16882": {
    "title": "Optimisation-Based Multi-Modal Semantic Image Editing",
    "volume": "Nov",
    "abstract": "Image editing affords increased control over the aesthetics and content of generated images. Pre-existing works focus predominantly on text-based instructions to achieve desired image modifications, which limit edit precision and accuracy. In this work, we propose an inference-time editing optimisation, designed to extend beyond textual edits to accommodate multiple editing instruction types (e.g. spatial layout-based; pose, scribbles, edge maps). We propose to disentangle the editing task into two competing subtasks: successful local image modifications and global content consistency preservation, where subtasks are guided through two dedicated loss functions. By allowing to adjust the influence of each loss function, we build a flexible editing solution that can be adjusted to user preferences. We evaluate our method using text, pose and scribble edit conditions, and highlight our ability to achieve complex edits, through both qualitative and quantitative experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Li",
      "Yongxin Yang",
      "Steven McDonagh",
      "Shifeng Zhang",
      "Petru-Daniel Tudosiu",
      "Sarah Parisot"
    ]
  },
  "http://arxiv.org/abs/2311.16867": {
    "title": "The Falcon Series of Open Language Models",
    "volume": "Nov",
    "abstract": "We introduce the Falcon series: 7B, 40B, and 180B parameters causal decoder-only models trained on a diverse high-quality corpora predominantly assembled from web data. The largest model, Falcon-180B, has been trained on over 3.5 trillion tokens of text--the largest openly documented pretraining run. Falcon-180B significantly outperforms models such as PaLM or Chinchilla, and improves upon concurrently developed models such as LLaMA 2 or Inflection-1. It nears the performance of PaLM-2-Large at a reduced pretraining and inference cost, making it, to our knowledge, one of the three best language models in the world along with GPT-4 and PaLM-2-Large. We report detailed evaluations, as well as a deep dive into the methods and custom tooling employed to pretrain Falcon. Notably, we report on our custom distributed training codebase, allowing us to efficiently pretrain these models on up to 4,096 A100s on cloud AWS infrastructure with limited interconnect. We release a 600B tokens extract of our web dataset, as well as the Falcon-7/40/180B models under a permissive license to foster open-science and accelerate the development of an open ecosystem of large language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ebtesam Almazrouei",
      "Hamza Alobeidli",
      "Abdulaziz Alshamsi",
      "Alessandro Cappelli",
      "Ruxandra Cojocaru",
      "Mrouane Debbah",
      "tienne Goffinet",
      "Daniel Hesslow",
      "Julien Launay",
      "Quentin Malartic",
      "Daniele Mazzotta",
      "Badreddine Noune",
      "Baptiste Pannier",
      "Guilherme Penedo"
    ]
  },
  "http://arxiv.org/abs/2311.16865": {
    "title": "A Benchmark for Evaluating Machine Translation Metrics on Dialects Without Standard Orthography",
    "volume": "Nov",
    "abstract": "For sensible progress in natural language processing, it is important that we are aware of the limitations of the evaluation metrics we use. In this work, we evaluate how robust metrics are to non-standardized dialects, i.e. spelling differences in language varieties that do not have a standard orthography. To investigate this, we collect a dataset of human translations and human judgments for automatic machine translations from English to two Swiss German dialects. We further create a challenge set for dialect variation and benchmark existing metrics' performances. Our results show that existing metrics cannot reliably evaluate Swiss German text generation outputs, especially on segment level. We propose initial design adaptations that increase robustness in the face of non-standardized dialects, although there remains much room for further improvement. The dataset, code, and models are available here: https://github.com/textshuttle/dialect_eval",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nomi Aepli",
      "Chantal Amrhein",
      "Florian Schottmann",
      "Rico Sennrich"
    ]
  },
  "http://arxiv.org/abs/2311.16842": {
    "title": "RELIC: Investigating Large Language Model Responses using Self-Consistency",
    "volume": "Nov",
    "abstract": "Large Language Models (LLMs) are notorious for blending fact with fiction and generating non-factual content, known as hallucinations. To tackle this challenge, we propose an interactive system that helps users obtain insights into the reliability of the generated text. Our approach is based on the idea that the self-consistency of multiple samples generated by the same LLM relates to its confidence in individual claims in the generated texts. Using this idea, we design RELIC, an interactive system that enables users to investigate and verify semantic-level variations in multiple long-form responses. This allows users to recognize potentially inaccurate information in the generated text and make necessary corrections. From a user study with ten participants, we demonstrate that our approach helps users better verify the reliability of the generated text. We further summarize the design implications and lessons learned from this research for inspiring future studies on reliable human-LLM interactions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Furui Cheng",
      "Vilm Zouhar",
      "Simran Arora",
      "Mrinmaya Sachan",
      "Hendrik Strobelt",
      "Mennatallah El-Assady"
    ]
  },
  "http://arxiv.org/abs/2311.16840": {
    "title": "The Claire French Dialogue Dataset",
    "volume": "Nov",
    "abstract": "We present the Claire French Dialogue Dataset (CFDD), a resource created by members of LINAGORA Labs in the context of the OpenLLM France initiative. CFDD is a corpus containing roughly 160 million words from transcripts and stage plays in French that we have assembled and publicly released in an effort to further the development of multilingual, open source language models. This paper describes the 24 individual corpora of which CFDD is composed and provides links and citations to their original sources. It also provides our proposed breakdown of the full CFDD dataset into eight categories of subcorpora and describes the process we followed to standardize the format of the final dataset. We conclude with a discussion of similar work and future directions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julie Hunter",
      "Jrme Louradour",
      "Virgile Rennard",
      "Ismal Harrando",
      "Guokan Shang",
      "Jean-Pierre Lorr"
    ]
  },
  "http://arxiv.org/abs/2311.16839": {
    "title": "Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization",
    "volume": "Nov",
    "abstract": "Multimodal large language models have made significant advancements in recent years, yet they still suffer from a common issue known as the \"hallucination problem\" where the models generate textual descriptions that contain inaccurate or non-existent content from the image. To address this issue, this paper introduces a novel strategy: Hallucination-Aware Direct Preference Optimization (HA-DPO). Our approach treats the hallucination problem as a unique preference selection issue, where the model is trained to favor the non-hallucinating response when presented with two responses of the same image (one accurate and one hallucinating). This paper also presents an efficient process for constructing hallucination sample pairs to ensure high-quality, style-consistent pairs for stable HA-DPO training. We applied this strategy to two mainstream multimodal models, and the results showed a significant reduction in the hallucination problem and an enhancement in the models' generalization capabilities. With HA-DPO, the MiniGPT-4 model demonstrates significant advancements: POPE accuracy increases from 51.13% to 85.66% (34.5% absolute improvement), and the MME score escalates from 968.58 to 1365.76 (41% relative improvement). The code, models, and datasets will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Zhao",
      "Bin Wang",
      "Linke Ouyang",
      "Xiaoyi Dong",
      "Jiaqi Wang",
      "Conghui He"
    ]
  },
  "http://arxiv.org/abs/2311.17126": {
    "title": "Reason out Your Layout: Evoking the Layout Master from Large Language Models for Text-to-Image Synthesis",
    "volume": "Nov",
    "abstract": "Recent advancements in text-to-image (T2I) generative models have shown remarkable capabilities in producing diverse and imaginative visuals based on text prompts. Despite the advancement, these diffusion models sometimes struggle to translate the semantic content from the text into images entirely. While conditioning on the layout has shown to be effective in improving the compositional ability of T2I diffusion models, they typically require manual layout input. In this work, we introduce a novel approach to improving T2I diffusion models using Large Language Models (LLMs) as layout generators. Our method leverages the Chain-of-Thought prompting of LLMs to interpret text and generate spatially reasonable object layouts. The generated layout is then used to enhance the generated images' composition and spatial accuracy. Moreover, we propose an efficient adapter based on a cross-attention mechanism, which explicitly integrates the layout information into the stable diffusion models. Our experiments demonstrate significant improvements in image quality and layout accuracy, showcasing the potential of LLMs in augmenting generative image models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohui Chen",
      "Yongfei Liu",
      "Yingxiang Yang",
      "Jianbo Yuan",
      "Quanzeng You",
      "Li-Ping Liu",
      "Hongxia Yang"
    ]
  },
  "http://arxiv.org/abs/2311.16832": {
    "title": "CharacterGLM: Customizing Chinese Conversational AI Characters with Large Language Models",
    "volume": "Nov",
    "abstract": "In this paper, we present CharacterGLM, a series of models built upon ChatGLM, with model sizes ranging from 6B to 66B parameters. Our CharacterGLM is designed for generating Character-based Dialogues (CharacterDial), which aims to equip a conversational AI system with character customization for satisfying people's inherent social desires and emotional needs. On top of CharacterGLM, we can customize various AI characters or social agents by configuring their attributes (identities, interests, viewpoints, experiences, achievements, social relationships, etc.) and behaviors (linguistic features, emotional expressions, interaction patterns, etc.). Our model outperforms most mainstream close-source large langauge models, including the GPT series, especially in terms of consistency, human-likeness, and engagement according to manual evaluations. We will release our 6B version of CharacterGLM and a subset of training data to facilitate further research development in the direction of character-based dialogue generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinfeng Zhou",
      "Zhuang Chen",
      "Dazhen Wan",
      "Bosi Wen",
      "Yi Song",
      "Jifan Yu",
      "Yongkang Huang",
      "Libiao Peng",
      "Jiaming Yang",
      "Xiyao Xiao",
      "Sahand Sabour",
      "Xiaohan Zhang",
      "Wenjing Hou",
      "Yijia Zhang",
      "Yuxiao Dong",
      "Jie Tang",
      "Minlie Huang"
    ]
  },
  "http://arxiv.org/abs/2311.16822": {
    "title": "Large Language Models Suffer From Their Own Output: An Analysis of the Self-Consuming Training Loop",
    "volume": "Nov",
    "abstract": "Large language models (LLM) have become state of the art in many benchmarks and conversational LLM applications like ChatGPT are now widely used by the public. Those LLMs can be used to generate large amounts of content which is posted on the internet to various platforms. As LLMs are trained on datasets usually collected from the internet, this LLM-generated content might be used to train the next generation of LLMs. Therefore, a self-consuming training loop emerges in which new LLM generations are trained on the output from the previous generations. We empirically study this self-consuming training loop using a novel dataset to analytically and accurately measure quality and diversity of generated outputs. We find that this self-consuming training loop initially improves both quality and diversity. However, after a few generations the output inevitably degenerates in diversity. We find that the rate of degeneration depends on the proportion of real and generated data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martin Briesch",
      "Dominik Sobania",
      "Franz Rothlauf"
    ]
  },
  "http://arxiv.org/abs/2311.16789": {
    "title": "A Survey of the Evolution of Language Model-Based Dialogue Systems",
    "volume": "Nov",
    "abstract": "Dialogue systems, including task-oriented_dialogue_system (TOD) and open-domain_dialogue_system (ODD), have undergone significant transformations, with language_models (LM) playing a central role. This survey delves into the historical trajectory of dialogue systems, elucidating their intricate relationship with advancements in language models by categorizing this evolution into four distinct stages, each marked by pivotal LM breakthroughs: 1) Early_Stage: characterized by statistical LMs, resulting in rule-based or machine-learning-driven dialogue_systems; 2) Independent development of TOD and ODD based on neural_language_models (NLM; e.g., LSTM and GRU), since NLMs lack intrinsic knowledge in their parameters; 3) fusion between different types of dialogue systems with the advert of pre-trained_language_models (PLMs), starting from the fusion between four_sub-tasks_within_TOD, and then TOD_with_ODD; and 4) current LLM-based_dialogue_system, wherein LLMs can be used to conduct TOD and ODD seamlessly. Thus, our survey provides a chronological perspective aligned with LM breakthroughs, offering a comprehensive review of state-of-the-art research outcomes. What's more, we focus on emerging topics and discuss open challenges, providing valuable insights into future directions for LLM-based_dialogue_systems. Through this exploration, we pave the way for a deeper_comprehension of the evolution, guiding future developments in LM-based dialogue_systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongru Wang",
      "Lingzhi Wang",
      "Yiming Du",
      "Liang Chen",
      "Jingyan Zhou",
      "Yufei Wang",
      "Kam-Fai Wong"
    ]
  },
  "http://arxiv.org/abs/2311.16787": {
    "title": "Evaluating Optimal Reference Translations",
    "volume": "Nov",
    "abstract": "The overall translation quality reached by current machine translation (MT) systems for high-resourced language pairs is remarkably good. Standard methods of evaluation are not suitable nor intended to uncover the many translation errors and quality deficiencies that still persist. Furthermore, the quality of standard reference translations is commonly questioned and comparable quality levels have been reached by MT alone in several language pairs. Navigating further research in these high-resource settings is thus difficult. In this article, we propose a methodology for creating more reliable document-level human reference translations, called \"optimal reference translations,\" with the simple aim to raise the bar of what should be deemed \"human translation quality.\" We evaluate the obtained document-level optimal reference translations in comparison with \"standard\" ones, confirming a significant quality increase and also documenting the relationship between evaluation and translation editing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vilm Zouhar",
      "Vra Kloudov",
      "Martin Popel",
      "Ondej Bojar"
    ]
  },
  "http://arxiv.org/abs/2311.16764": {
    "title": "Radiology-Aware Model-Based Evaluation Metric for Report Generation",
    "volume": "Nov",
    "abstract": "We propose a new automated evaluation metric for machine-generated radiology reports using the successful COMET architecture adapted for the radiology domain. We train and publish four medically-oriented model checkpoints, including one trained on RadGraph, a radiology knowledge graph. Our results show that our metric correlates moderately to high with established metrics such as BERTscore, BLEU, and CheXbert scores. Furthermore, we demonstrate that one of our checkpoints exhibits a high correlation with human judgment, as assessed using the publicly available annotations of six board-certified radiologists, using a set of 200 reports. We also performed our own analysis gathering annotations with two radiologists on a collection of 100 reports. The results indicate the potential effectiveness of our method as a radiology-specific evaluation metric. The code, data, and model checkpoints to reproduce our findings will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amos Calamida",
      "Farhad Nooralahzadeh",
      "Morteza Rohanian",
      "Koji Fujimoto",
      "Mizuho Nishio",
      "Michael Krauthammer"
    ]
  },
  "http://arxiv.org/abs/2311.16733": {
    "title": "LLMs for Science: Usage for Code Generation and Data Analysis",
    "volume": "Nov",
    "abstract": "Large language models (LLMs) have been touted to enable increased productivity in many areas of today's work life. Scientific research as an area of work is no exception: the potential of LLM-based tools to assist in the daily work of scientists has become a highly discussed topic across disciplines. However, we are only at the very onset of this subject of study. It is still unclear how the potential of LLMs will materialise in research practice. With this study, we give first empirical evidence on the use of LLMs in the research process. We have investigated a set of use cases for LLM-based tools in scientific research, and conducted a first study to assess to which degree current tools are helpful. In this paper we report specifically on use cases related to software engineering, such as generating application code and developing scripts for data analytics. While we studied seemingly simple use cases, results across tools differ significantly. Our results highlight the promise of LLM-based tools in general, yet we also observe various issues, particularly regarding the integrity of the output these tools provide",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Nejjar",
      "Luca Zacharias",
      "Fabian Stiehle",
      "Ingo Weber"
    ]
  },
  "http://arxiv.org/abs/2311.16678": {
    "title": "Entity-Aspect-Opinion-Sentiment Quadruple Extraction for Fine-grained Sentiment Analysis",
    "volume": "Nov",
    "abstract": "Product reviews often contain a large number of implicit aspects and object-attribute co-existence cases. Unfortunately, many existing studies in Aspect-Based Sentiment Analysis (ABSA) have overlooked this issue, which can make it difficult to extract opinions comprehensively and fairly. In this paper, we propose a new task called Entity-Aspect-Opinion-Sentiment Quadruple Extraction (EASQE), which aims to hierarchically decompose aspect terms into entities and aspects to avoid information loss, non-exclusive annotations, and opinion misunderstandings in ABSA tasks. To facilitate research in this new task, we have constructed four datasets (Res14-EASQE, Res15-EASQE, Res16-EASQE, and Lap14-EASQE) based on the SemEval Restaurant and Laptop datasets. We have also proposed a novel two-stage sequence-tagging based Trigger-Opinion framework as the baseline for the EASQE task. Empirical evaluations show that our Trigger-Opinion framework can generate satisfactory EASQE results and can also be applied to other ABSA tasks, significantly outperforming state-of-the-art methods. We have made the four datasets and source code of Trigger-Opinion publicly available to facilitate further research in this area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Ma",
      "Jun Xu",
      "Zongyu Wang",
      "Xuezhi Cao",
      "Yunsen Xian"
    ]
  },
  "http://arxiv.org/abs/2311.16675": {
    "title": "A Distribution-Based Threshold for Determining Sentence Similarity",
    "volume": "Nov",
    "abstract": "We hereby present a solution to a semantic textual similarity (STS) problem in which it is necessary to match two sentences containing, as the only distinguishing factor, highly specific information (such as names, addresses, identification codes), and from which we need to derive a definition for when they are similar and when they are not. The solution revolves around the use of a neural network, based on the siamese architecture, to create the distributions of the distances between similar and dissimilar pairs of sentences. The goal of these distributions is to find a discriminating factor, that we call \"threshold\", which represents a well-defined quantity that can be used to distinguish vector distances of similar pairs from vector distances of dissimilar pairs in new predictions and later analyses. In addition, we developed a way to score the predictions by combining attributes from both the distributions' features and the way the distance function works. Finally, we generalize the results showing that they can be transferred to a wider range of domains by applying the system discussed to a well-known and widely used benchmark dataset for STS problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gioele Cadamuro",
      "Marco Gruppo"
    ]
  },
  "http://arxiv.org/abs/2311.17107": {
    "title": "ClimateX: Do LLMs Accurately Assess Human Expert Confidence in Climate Statements?",
    "volume": "Nov",
    "abstract": "Evaluating the accuracy of outputs generated by Large Language Models (LLMs) is especially important in the climate science and policy domain. We introduce the Expert Confidence in Climate Statements (ClimateX) dataset, a novel, curated, expert-labeled dataset consisting of 8094 climate statements collected from the latest Intergovernmental Panel on Climate Change (IPCC) reports, labeled with their associated confidence levels. Using this dataset, we show that recent LLMs can classify human expert confidence in climate-related statements, especially in a few-shot learning setting, but with limited (up to 47%) accuracy. Overall, models exhibit consistent and significant over-confidence on low and medium confidence statements. We highlight implications of our results for climate communication, LLMs evaluation strategies, and the use of LLMs in information retrieval systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Romain Lacombe",
      "Kerrie Wu",
      "Eddie Dilworth"
    ]
  },
  "http://arxiv.org/abs/2311.16650": {
    "title": "Text2Tree: Aligning Text Representation to the Label Tree Hierarchy for Imbalanced Medical Classification",
    "volume": "Nov",
    "abstract": "Deep learning approaches exhibit promising performances on various text tasks. However, they are still struggling on medical text classification since samples are often extremely imbalanced and scarce. Different from existing mainstream approaches that focus on supplementary semantics with external medical information, this paper aims to rethink the data challenges in medical texts and present a novel framework-agnostic algorithm called Text2Tree that only utilizes internal label hierarchy in training deep learning models. We embed the ICD code tree structure of labels into cascade attention modules for learning hierarchy-aware label representations. Two new learning schemes, Similarity Surrogate Learning (SSL) and Dissimilarity Mixup Learning (DML), are devised to boost text classification by reusing and distinguishing samples of other labels following the label representation hierarchy, respectively. Experiments on authoritative public datasets and real-world medical records show that our approach stably achieves superior performances over classical and advanced imbalanced classification methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahuan Yan",
      "Haojun Gao",
      "Zhang Kai",
      "Weize Liu",
      "Danny Chen",
      "Jian Wu",
      "Jintai Chen"
    ]
  },
  "http://arxiv.org/abs/2311.16639": {
    "title": "Scaling Political Texts with ChatGPT",
    "volume": "Nov",
    "abstract": "We use GPT-4 to obtain position estimates of political texts in continuous spaces. We develop and validate a new approach by positioning British party manifestos on the economic, social, and immigration policy dimensions and tweets by members of the US Congress on the left-right ideological spectrum. For the party manifestos, the correlation between the positions produced by GPT-4 and experts is 93% or higher, a performance similar to or better than that obtained with crowdsourced position estimates. For individual tweets, the positions obtained with GPT-4 achieve a correlation of 91% with crowdsourced position estimates. For senators of the 117th US Congress, the positions obtained with GPT-4 achieve a correlation of 97% with estimates based on roll call votes and of 96% with those based on campaign funding. Correlations are also substantial within party, indicating that position estimates produced with GPT-4 capture within-party differences between senators. Overall, using GPT-4 for ideological scaling is fast, cost-efficient, and reliable. This approach provides a viable alternative to scaling by both expert raters and crowdsourcing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gal Le Mens",
      "Aina Gallego"
    ]
  },
  "http://arxiv.org/abs/2311.16620": {
    "title": "On the Long Range Abilities of Transformers",
    "volume": "Nov",
    "abstract": "Despite their dominance in modern DL and, especially, NLP domains, transformer architectures exhibit sub-optimal performance on long-range tasks compared to recent layers that are specifically designed for this purpose. In this work, drawing inspiration from key attributes of long-range layers, such as state-space layers, linear RNN layers, and global convolution layers, we demonstrate that minimal modifications to the transformer architecture can significantly enhance performance on the Long Range Arena (LRA) benchmark, thus narrowing the gap with these specialized layers. We identify that two key principles for long-range tasks are (i) incorporating an inductive bias towards smoothness, and (ii) locality. As we show, integrating these ideas into the attention mechanism improves results with a negligible amount of additional computation and without any additional trainable parameters. Our theory and experiments also shed light on the reasons for the inferior performance of transformers on long-range tasks and identify critical properties that are essential for successfully capturing long-range dependencies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Itamar Zimerman",
      "Lior Wolf"
    ]
  },
  "http://arxiv.org/abs/2311.16588": {
    "title": "MedGen: A Python Natural Language Processing Toolkit for Medical Text Processing",
    "volume": "Nov",
    "abstract": "This study introduces MedGen, a comprehensive natural language processing (NLP) toolkit designed for medical text processing. MedGen is tailored for biomedical researchers and healthcare professionals with an easy-to-use, all-in-one solution that requires minimal programming expertise. It includes (1) Generative Functions: For the first time, MedGen includes four advanced generative functions: question answering, text summarization, text simplification, and machine translation; (2) Basic NLP Functions: MedGen integrates 12 essential NLP functions such as word tokenization and sentence segmentation; and (3) Query and Search Capabilities: MedGen provides user-friendly query and search functions on text corpora. We fine-tuned 32 domain-specific language models, evaluated them thoroughly on 24 established benchmarks and conducted manual reviews with clinicians. Additionally, we expanded our toolkit by introducing query and search functions, while also standardizing and integrating functions from third-party libraries. The toolkit, its models, and associated data are publicly available via https://github.com/Yale-LILY/MedGen",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Yang",
      "Qingcheng Zeng",
      "Keen You",
      "Yujie Qiao",
      "Lucas Huang",
      "Chia-Chun Hsieh",
      "Benjamin Rosand",
      "Jeremy Goldwasser",
      "Amisha D Dave",
      "Tiarnan D. L. Keenan",
      "Emily Y Chew",
      "Dragomir Radev",
      "Zhiyong Lu",
      "Hua Xu",
      "Qingyu Chen",
      "Irene Li"
    ]
  },
  "http://arxiv.org/abs/2311.16579": {
    "title": "Recognizing Conditional Causal Relationships about Emotions and Their Corresponding Conditions",
    "volume": "Nov",
    "abstract": "The study of causal relationships between emotions and causes in texts has recently received much attention. Most works focus on extracting causally related clauses from documents. However, none of these works has considered that the causal relationships among the extracted emotion and cause clauses can only be valid under some specific context clauses. To highlight the context in such special causal relationships, we propose a new task to determine whether or not an input pair of emotion and cause has a valid causal relationship under different contexts and extract the specific context clauses that participate in the causal relationship. Since the task is new for which no existing dataset is available, we conduct manual annotation on a benchmark dataset to obtain the labels for our tasks and the annotations of each context clause's type that can also be used in some other applications. We adopt negative sampling to construct the final dataset to balance the number of documents with and without causal relationships. Based on the constructed dataset, we propose an end-to-end multi-task framework, where we design two novel and general modules to handle the two goals of our task. Specifically, we propose a context masking module to extract the context clauses participating in the causal relationships. We propose a prediction aggregation module to fine-tune the prediction results according to whether the input emotion and causes depend on specific context clauses. Results of extensive comparative experiments and ablation studies demonstrate the effectiveness and generality of our proposed framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinhong Chen",
      "Zongxi Li",
      "Yaowei Wang",
      "Haoran Xie",
      "Jianping Wang",
      "Qing Li"
    ]
  },
  "http://arxiv.org/abs/2311.16522": {
    "title": "Evaluation of dynamic characteristics of power grid based on GNN and application on knowledge graph",
    "volume": "Nov",
    "abstract": "A novel method for detecting faults in power grids using a graph neural network (GNN) has been developed, aimed at enhancing intelligent fault diagnosis in network operation and maintenance. This GNN-based approach identifies faulty nodes within the power grid through a specialized electrical feature extraction model coupled with a knowledge graph. Incorporating temporal data, the method leverages the status of nodes from preceding and subsequent time periods to aid in current fault detection. To validate the effectiveness of this GNN in extracting node features, a correlation analysis of the output features from each node within the neural network layer was conducted. The results from experiments show that this method can accurately locate fault nodes in simulated scenarios with a remarkable 99.53% accuracy. Additionally, the graph neural network's feature modeling allows for a qualitative examination of how faults spread across nodes, providing valuable insights for analyzing fault nodes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Pei",
      "Si Lin",
      "Chuanfu Li",
      "Che Wang",
      "Haoming Chen",
      "Sizhe Li"
    ]
  },
  "http://arxiv.org/abs/2311.16509": {
    "title": "StyleCap: Automatic Speaking-Style Captioning from Speech Based on Speech and Language Self-supervised Learning Models",
    "volume": "Nov",
    "abstract": "We propose StyleCap, a method to generate natural language descriptions of speaking styles appearing in speech. Although most of conventional techniques for para-/non-linguistic information recognition focus on the category classification or the intensity estimation of pre-defined labels, they cannot provide the reasoning of the recognition result in an interpretable manner. As a first step towards an end-to-end method for generating speaking-style prompts from speech, i.e., automatic speaking-style captioning, StyleCap uses paired data of speech and natural language descriptions to train neural networks that predict prefix vectors fed into a large language model (LLM)-based text decoder from a speech representation vector. We explore an appropriate text decoder and speech feature representation suitable for this new task. The experimental results demonstrate that our StyleCap leveraging richer LLMs for the text decoder, speech self-supervised learning (SSL) features, and sentence rephrasing augmentation improves the accuracy and diversity of generated speaking-style captions. Samples of speaking-style captions generated by our StyleCap are publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kazuki Yamauchi",
      "Yusuke Ijima",
      "Yuki Saito"
    ]
  },
  "http://arxiv.org/abs/2311.16466": {
    "title": "Enhancing Human Persuasion With Large Language Models",
    "volume": "Nov",
    "abstract": "Although large language models (LLMs) are reshaping various aspects of human life, our current understanding of their impacts remains somewhat constrained. Here we investigate the impact of LLMs on human communication, in the context of consumer complaints in the financial industry. Employing an AI detection tool on more than 780K complaints gathered by the Consumer Financial Protection Bureau (CFPB), we find evidence of LLM usage in the writing of complaints - shortly after the release of ChatGPT. Our analyses reveal that LLM usage is positively correlated with the likelihood of obtaining desirable outcomes (i.e., offer of relief from financial firms) and suggest that this positive correlation may be partly due to the linguistic features improved by LLMs. We test this conjecture with a preregistered experiment, which reveals results consistent with those from observational studies: Consumer complaints written with ChatGPT for improved linguistic qualities were more likely to receive hypothetical relief offers than the original consumer complaints, demonstrating the LLM's ability to enhance message persuasiveness in human communication. Being some of the earliest empirical evidence on LLM usage for enhancing persuasion, our results highlight the transformative potential of LLMs in human communication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minkyu Shin",
      "Jin Kim"
    ]
  },
  "http://arxiv.org/abs/2311.16452": {
    "title": "Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine",
    "volume": "Nov",
    "abstract": "Generalist foundation models such as GPT-4 have displayed surprising capabilities in a wide variety of domains and tasks. Yet, there is a prevalent assumption that they cannot match specialist capabilities of fine-tuned models. For example, most explorations to date on medical competency benchmarks have leveraged domain-specific training, as exemplified by efforts on BioGPT and Med-PaLM. We build on a prior study of GPT-4's capabilities on medical challenge benchmarks in the absence of special training. Rather than using simple prompting to highlight the model's out-of-the-box capabilities, we perform a systematic exploration of prompt engineering. We find that prompting innovation can unlock deeper specialist capabilities and show that GPT-4 easily tops prior leading results for medical benchmarks. The prompting methods we explore are general purpose, and make no specific use of domain expertise, removing the need for expert-curated content. Our experimental design carefully controls for overfitting during the prompt engineering process. We introduce Medprompt, based on a composition of several prompting strategies. With Medprompt, GPT-4 achieves state-of-the-art results on all nine of the benchmark datasets in the MultiMedQA suite. The method outperforms leading specialist models such as Med-PaLM 2 by a significant margin with an order of magnitude fewer calls to the model. Steering GPT-4 with Medprompt achieves a 27% reduction in error rate on the MedQA dataset over the best methods to date achieved with specialist models and surpasses a score of 90% for the first time. Beyond medical problems, we show the power of Medprompt to generalize to other domains and provide evidence for the broad applicability of the approach via studies of the strategy on exams in electrical engineering, machine learning, philosophy, accounting, law, nursing, and clinical psychology",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harsha Nori",
      "Yin Tat Lee",
      "Sheng Zhang",
      "Dean Carignan",
      "Richard Edgar",
      "Nicolo Fusi",
      "Nicholas King",
      "Jonathan Larson",
      "Yuanzhi Li",
      "Weishung Liu",
      "Renqian Luo",
      "Scott Mayer McKinney",
      "Robert Osazuwa Ness",
      "Hoifung Poon",
      "Tao Qin",
      "Naoto Usuyama",
      "Chris White",
      "Eric Horvitz"
    ]
  },
  "http://arxiv.org/abs/2311.16444": {
    "title": "Exo2EgoDVC: Dense Video Captioning of Egocentric Procedural Activities Using Web Instructional Videos",
    "volume": "Nov",
    "abstract": "We propose a novel benchmark for cross-view knowledge transfer of dense video captioning, adapting models from web instructional videos with exocentric views to an egocentric view. While dense video captioning (predicting time segments and their captions) is primarily studied with exocentric videos (e.g., YouCook2), benchmarks with egocentric videos are restricted due to data scarcity. To overcome the limited video availability, transferring knowledge from abundant exocentric web videos is demanded as a practical approach. However, learning the correspondence between exocentric and egocentric views is difficult due to their dynamic view changes. The web videos contain mixed views focusing on either human body actions or close-up hand-object interactions, while the egocentric view is constantly shifting as the camera wearer moves. This necessitates the in-depth study of cross-view transfer under complex view changes. In this work, we first create a real-life egocentric dataset (EgoYC2) whose captions are shared with YouCook2, enabling transfer learning between these datasets assuming their ground-truth is accessible. To bridge the view gaps, we propose a view-invariant learning method using adversarial training in both the pre-training and fine-tuning stages. While the pre-training is designed to learn invariant features against the mixed views in the web videos, the view-invariant fine-tuning further mitigates the view gaps between both datasets. We validate our proposed method by studying how effectively it overcomes the view change problem and efficiently transfers the knowledge to the egocentric domain. Our benchmark pushes the study of the cross-view transfer into a new task domain of dense video captioning and will envision methodologies to describe egocentric videos in natural language",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takehiko Ohkawa",
      "Takuma Yagi",
      "Taichi Nishimura",
      "Ryosuke Furuta",
      "Atsushi Hashimoto",
      "Yoshitaka Ushiku",
      "Yoichi Sato"
    ]
  },
  "http://arxiv.org/abs/2311.17086": {
    "title": "PEA-Diffusion: Parameter-Efficient Adapter with Knowledge Distillation in non-English Text-to-Image Generation",
    "volume": "Nov",
    "abstract": "Text-to-image diffusion models are well-known for their ability to generate realistic images based on textual prompts. However, the existing works have predominantly focused on English, lacking support for non-English text-to-image models. The most commonly used translation methods cannot solve the generation problem related to language culture, while training from scratch on a specific language dataset is prohibitively expensive. In this paper, we are inspired to propose a simple plug-and-play language transfer method based on knowledge distillation. All we need to do is train a lightweight MLP-like parameter-efficient adapter (PEA) with only 6M parameters under teacher knowledge distillation along with a small parallel data corpus. We are surprised to find that freezing the parameters of UNet can still achieve remarkable performance on the language-specific prompt evaluation set, demonstrating that PEA can stimulate the potential generation ability of the original UNet. Additionally, it closely approaches the performance of the English text-to-image model on a general prompt evaluation set. Furthermore, our adapter can be used as a plugin to achieve significant results in downstream tasks in cross-lingual text-to-image generation. Code will be available at: https://github.com/OPPO-Mente-Lab/PEA-Diffusion",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Ma",
      "Chen Chen",
      "Qingsong Xie",
      "Haonan Lu"
    ]
  },
  "http://arxiv.org/abs/2311.16421": {
    "title": "CDEval: A Benchmark for Measuring the Cultural Dimensions of Large Language Models",
    "volume": "Nov",
    "abstract": "As the scaling of Large Language Models (LLMs) has dramatically enhanced their capabilities, there has been a growing focus on the alignment problem to ensure their responsible and ethical use. While existing alignment efforts predominantly concentrate on universal values such as the HHH principle, the aspect of culture, which is inherently pluralistic and diverse, has not received adequate attention. This work introduces a new benchmark, CDEval, aimed at evaluating the cultural dimensions of LLMs. CDEval is constructed by incorporating both GPT-4's automated generation and human verification, covering six cultural dimensions across seven domains. Our comprehensive experiments provide intriguing insights into the culture of mainstream LLMs, highlighting both consistencies and variations across different dimensions and domains. The findings underscore the importance of integrating cultural considerations in LLM development, particularly for applications in diverse cultural settings. Through CDEval, we aim to broaden the horizon of LLM alignment research by including cultural dimensions, thus providing a more holistic framework for the future development and evaluation of LLMs. This benchmark serves as a valuable resource for cultural studies in LLMs, paving the way for more culturally aware and sensitive models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Wang",
      "Yanxu Zhu",
      "Chao Kong",
      "Shuyu Wei",
      "Xiaoyuan Yi",
      "Xing Xie",
      "Jitao Sang"
    ]
  },
  "http://arxiv.org/abs/2311.16362": {
    "title": "Reducing Gender Bias in Machine Translation through Counterfactual Data Generation",
    "volume": "Nov",
    "abstract": "Recent advances in neural methods have led to substantial improvement in the quality of Neural Machine Translation (NMT) systems. However, these systems frequently produce translations with inaccurate gender (Stanovsky et al., 2019), which can be traced to bias in training data. Saunders and Byrne (2020) tackle this problem with a handcrafted dataset containing balanced gendered profession words. By using this data to fine-tune an existing NMT model, they show that gender bias can be significantly mitigated, albeit at the expense of translation quality due to catastrophic forgetting. They recover some of the lost quality with modified training objectives or additional models at inference. We find, however, that simply supplementing the handcrafted dataset with a random sample from the base model training corpus is enough to significantly reduce the catastrophic forgetting. We also propose a novel domain-adaptation technique that leverages in-domain data created with the counterfactual data generation techniques proposed by Zmigrod et al. (2019) to further improve accuracy on the WinoMT challenge test set without significant loss in translation quality. We show its effectiveness in NMT systems from English into three morphologically rich languages French, Spanish, and Italian. The relevant dataset and code will be available at Github",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ranjita Naik",
      "Spencer Rarrick",
      "Vishal Chowdhary"
    ]
  },
  "http://arxiv.org/abs/2311.17076": {
    "title": "Compositional Chain-of-Thought Prompting for Large Multimodal Models",
    "volume": "Nov",
    "abstract": "The combination of strong visual backbones and Large Language Model (LLM) reasoning has led to Large Multimodal Models (LMMs) becoming the current standard for a wide range of vision and language (VL) tasks. However, recent research has shown that even the most advanced LMMs still struggle to capture aspects of compositional visual reasoning, such as attributes and relationships between objects. One solution is to utilize scene graphs (SGs)--a formalization of objects and their relations and attributes that has been extensively used as a bridge between the visual and textual domains. Yet, scene graph data requires scene graph annotations, which are expensive to collect and thus not easily scalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic forgetting of the pretraining objective. To overcome this, inspired by chain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a novel zero-shot Chain-of-Thought prompting method that utilizes SG representations in order to extract compositional knowledge from an LMM. Specifically, we first generate an SG using the LMM, and then use that SG in the prompt to produce a response. Through extensive experiments, we find that the proposed CCoT approach not only improves LMM performance on several vision and language VL compositional benchmarks but also improves the performance of several popular LMMs on general multimodal benchmarks, without the need for fine-tuning or annotated ground-truth SGs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chancharik Mitra",
      "Brandon Huang",
      "Trevor Darrell",
      "Roei Herzig"
    ]
  },
  "http://arxiv.org/abs/2311.16338": {
    "title": "Releasing the CRaQAn (Coreference Resolution in Question-Answering): An open-source dataset and dataset creation methodology using instruction-following models",
    "volume": "Nov",
    "abstract": "Instruction-following language models demand robust methodologies for information retrieval to augment instructions for question-answering applications. A primary challenge is the resolution of coreferences in the context of chunking strategies for long documents. The critical barrier to experimentation of handling coreferences is a lack of open source datasets, specifically in question-answering tasks that require coreference resolution. In this work we present our Coreference Resolution in Question-Answering (CRaQAn) dataset, an open-source dataset that caters to the nuanced information retrieval requirements of coreference resolution in question-answering tasks by providing over 250 question-answer pairs containing coreferences. To develop this dataset, we developed a novel approach for creating high-quality datasets using an instruction-following model (GPT-4) and a Recursive Criticism and Improvement Loop",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rob Grzywinski",
      "Joshua D'Arcy",
      "Rob Naidoff",
      "Ashish Shukla",
      "Alex Browne",
      "Ren Gibbons",
      "Brinnae Bent"
    ]
  },
  "http://arxiv.org/abs/2311.16302": {
    "title": "Comprehensive Benchmarking of Entropy and Margin Based Scoring Metrics for Data Selection",
    "volume": "Nov",
    "abstract": "While data selection methods have been studied extensively in active learning, data pruning, and data augmentation settings, there is little evidence for the efficacy of these methods in industry scale settings, particularly in low-resource languages. Our work presents ways of assessing prospective training examples in those settings for their \"usefulness\" or \"difficulty\". We also demonstrate how these measures can be used in selecting important examples for training supervised machine learning models. We primarily experiment with entropy and Error L2-Norm (EL2N) scores. We use these metrics to curate high quality datasets from a large pool of \\textit{Weak Signal Labeled} data, which assigns no-defect high confidence hypotheses during inference as ground truth labels. We then conduct training data augmentation experiments using these de-identified datasets and demonstrate that score-based selection can result in a 2% decrease in semantic error rate and 4%-7% decrease in domain classification error rate when compared to the baseline technique of random selection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anusha Sabbineni",
      "Nikhil Anand",
      "Maria Minakova"
    ]
  },
  "http://arxiv.org/abs/2311.16298": {
    "title": "Influence Scores at Scale for Efficient Language Data Sampling",
    "volume": "Nov",
    "abstract": "Modern ML systems ingest data aggregated from diverse sources, such as synthetic, human-annotated, and live customer traffic. Understanding \\textit{which} examples are important to the performance of a learning algorithm is crucial for efficient model training. Recently, a growing body of literature has given rise to various \"influence scores,\" which use training artifacts such as model confidence or checkpointed gradients to identify important subsets of data. However, these methods have primarily been developed in computer vision settings, and it remains unclear how well they generalize to language-based tasks using pretrained models. In this paper, we explore the applicability of influence scores in language classification tasks. We evaluate a diverse subset of these scores on the SNLI dataset by quantifying accuracy changes in response to pruning training data through random and influence-score-based sampling. We then stress-test one of the scores -- \"variance of gradients\" (VoG) from Agarwal et al. (2022) -- in an NLU model stack that was exposed to dynamic user speech patterns in a voice assistant type of setting. Our experiments demonstrate that in many cases, encoder-based language models can be finetuned on roughly 50% of the original data without degradation in performance metrics. Along the way, we summarize lessons learned from applying out-of-the-box implementations of influence scores, quantify the effects of noisy and class-imbalanced data, and offer recommendations on score-based sampling for better accuracy and training efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikhil Anand",
      "Joshua Tan",
      "Maria Minakova"
    ]
  },
  "http://arxiv.org/abs/2311.16292": {
    "title": "Student Mastery or AI Deception? Analyzing ChatGPT's Assessment Proficiency and Evaluating Detection Strategies",
    "volume": "Nov",
    "abstract": "Generative AI systems such as ChatGPT have a disruptive effect on learning and assessment. Computer science requires practice to develop skills in problem solving and programming that are traditionally developed using assignments. Generative AI has the capability of completing these assignments for students with high accuracy, which dramatically increases the potential for academic integrity issues and students not achieving desired learning outcomes. This work investigates the performance of ChatGPT by evaluating it across three courses (CS1,CS2,databases). ChatGPT completes almost all introductory assessments perfectly. Existing detection methods, such as MOSS and JPlag (based on similarity metrics) and GPTzero (AI detection), have mixed success in identifying AI solutions. Evaluating instructors and teaching assistants using heuristics to distinguish between student and AI code shows that their detection is not sufficiently accurate. These observations emphasize the need for adapting assessments and improved detection methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Wang",
      "Seth Akins",
      "Abdallah Mohammed",
      "Ramon Lawrence"
    ]
  },
  "http://arxiv.org/abs/2311.16267": {
    "title": "Applications of Large Language Models in Data Processing: Innovative Approaches to Segmenting and Renewing Information",
    "volume": "Nov",
    "abstract": "Our paper investigates effective methods for code generation in \"specific-domain\" applications, including the use of Large Language Models (LLMs) for data segmentation and renewal, as well as stimulating deeper thinking in LLMs through prompt adjustments. Using a real company product as an example, we provide user manuals, API documentation, and other data. The ideas discussed in this paper help segment and then convert this data into semantic vectors to better reflect their true positioning. Subsequently, user requirements are transformed into vectors to retrieve the most relevant content, achieving about 70% accuracy in simple to medium-complexity tasks through various prompt techniques. This paper is the first to enhance specific-domain code generation effectiveness from this perspective. Additionally, we experiment with generating more scripts from a limited number using llama2-based fine-tuning to test its effectiveness in professional domain code generation. This is a challenging and promising field, and once achieved, it will not only lead to breakthroughs in LLM development across multiple industries but also enable LLMs to understand and learn any new knowledge effectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Chen Lin",
      "Akhilesh Kumar",
      "Wen-Liang Zhang",
      "Norman Chang",
      "Muhammad Zakir",
      "Rucha Apte",
      "Chao Wang",
      "Jyh-Shing Roger Jang"
    ]
  },
  "http://arxiv.org/abs/2311.16258": {
    "title": "An Exploration of Left-Corner Transformations",
    "volume": "Nov",
    "abstract": "The left-corner transformation (Rosenkrantz and Lewis, 1970) is used to remove left recursion from context-free grammars, which is an important step towards making the grammar parsable top-down with simple techniques. This paper generalizes prior left-corner transformations to support semiring-weighted production rules and to provide finer-grained control over which left corners may be moved. Our generalized left-corner transformation (GLCT) arose from unifying the left-corner transformation and speculation transformation (Eisner and Blatz, 2007), originally for logic programming. Our new transformation and speculation define equivalent weighted languages. Yet, their derivation trees are structurally different in an important way: GLCT replaces left recursion with right recursion, and speculation does not. We also provide several technical results regarding the formal relationships between the outputs of GLCT, speculation, and the original grammar. Lastly, we empirically investigate the efficiency of GLCT for left-recursion elimination from grammars of nine languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Opedal",
      "Eleftheria Tsipidi",
      "Tiago Pimentel",
      "Ryan Cotterell",
      "Tim Vieira"
    ]
  },
  "http://arxiv.org/abs/2311.16254": {
    "title": "Removing NSFW Concepts from Vision-and-Language Models for Text-to-Image Retrieval and Generation",
    "volume": "Nov",
    "abstract": "Vision-and-Language models such as CLIP have demonstrated remarkable effectiveness across a wide range of tasks. However, these models are typically trained on web-scale data, which can introduce inappropriate content and lead to the development of unsafe and biased behavior. This, in turn, hampers their applicability in sensitive and trustworthy contexts and could raise significant concern in their adoption. To overcome these limitations, we introduce a methodology to make Vision-and-Language models safer by removing their sensitivity to not-safe-for-work concepts. We show how this can be done by distilling from a large language model which converts between safe and unsafe sentences and which is fine-tuned starting from just 100 manually-curated pairs. We conduct extensive experiments on the resulting embedding space for both retrieval and text-to-image generation, where we show that our model can also be properly employed with pre-trained image generators. Our source code and trained models are available at: https://github.com/aimagelab/safe-clip",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuele Poppi",
      "Tobia Poppi",
      "Federico Cocchi",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ]
  },
  "http://arxiv.org/abs/2311.16101": {
    "title": "How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs",
    "volume": "Nov",
    "abstract": "This work focuses on the potential of Vision LLMs (VLLMs) in visual reasoning. Different from prior studies, we shift our focus from evaluating standard performance to introducing a comprehensive safety evaluation suite, covering both out-of-distribution (OOD) generalization and adversarial robustness. For the OOD evaluation, we present two novel VQA datasets, each with one variant, designed to test model performance under challenging conditions. In exploring adversarial robustness, we propose a straightforward attack strategy for misleading VLLMs to produce visual-unrelated responses. Moreover, we assess the efficacy of two jailbreaking strategies, targeting either the vision or language component of VLLMs. Our evaluation of 21 diverse models, ranging from open-source VLLMs to GPT-4V, yields interesting observations: 1) Current VLLMs struggle with OOD texts but not images, unless the visual information is limited; and 2) These VLLMs can be easily misled by deceiving vision encoders only, and their vision-language training often compromise safety protocols. We release this safety evaluation suite at https://github.com/UCSC-VLAA/vllm-safety-benchmark",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoqin Tu",
      "Chenhang Cui",
      "Zijun Wang",
      "Yiyang Zhou",
      "Bingchen Zhao",
      "Junlin Han",
      "Wangchunshu Zhou",
      "Huaxiu Yao",
      "Cihang Xie"
    ]
  },
  "http://arxiv.org/abs/2311.16087": {
    "title": "DUnE: Dataset for Unified Editing",
    "volume": "Nov",
    "abstract": "Even the most advanced language models remain susceptible to errors necessitating to modify these models without initiating a comprehensive retraining process. Model editing refers to the modification of a model's knowledge or representations in a manner that produces the desired outcomes. Prior research primarily centered around editing factual data e.g. \"Messi plays for Inter Miami\" confining the definition of an edit to a knowledge triplet i.e. (subject, object, relation). However, as the applications of language models expand, so do the diverse ways in which we wish to edit and refine their outputs. In this study, we broaden the scope of the editing problem to include an array of editing cases such as debiasing and rectifying reasoning errors and define an edit as any natural language expression that solicits a change in the model's outputs. We are introducing DUnE-an editing benchmark where edits are natural language sentences and propose that DUnE presents a challenging yet relevant task. To substantiate this claim, we conduct an extensive series of experiments testing various editing approaches to address DUnE, demonstrating their respective strengths and weaknesses. We show that retrieval-augmented language modeling can outperform specialized editing techniques and neither set of approaches has fully solved the generalized editing problem covered by our benchmark",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Afra Feyza Akyrek",
      "Eric Pan",
      "Garry Kuwanto",
      "Derry Wijaya"
    ]
  },
  "http://arxiv.org/abs/2311.16083": {
    "title": "BERT Goes Off-Topic: Investigating the Domain Transfer Challenge using Genre Classification",
    "volume": "Nov",
    "abstract": "While performance of many text classification tasks has been recently improved due to Pre-trained Language Models (PLMs), in this paper we show that they still suffer from a performance gap when the underlying distribution of topics changes. For example, a genre classifier trained on \\textit{political} topics often fails when tested on documents about \\textit{sport} or \\textit{medicine}. In this work, we quantify this phenomenon empirically with a large corpus and a large set of topics. Consequently, we verify that domain transfer remains challenging both for classic PLMs, such as BERT, and for modern large models, such as GPT-3. We also suggest and successfully test a possible remedy: after augmenting the training dataset with topically-controlled synthetic texts, the F1 score improves by up to 50\\% for some topics, nearing on-topic training results, while others show little to no improvement. While our empirical results focus on genre classification, our methodology is applicable to other classification tasks such as gender, authorship, or sentiment classification. The code and data to replicate the experiments are available at https://github.com/dminus1/genre",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dmitri Roussinov",
      "Serge Sharoff"
    ]
  },
  "http://arxiv.org/abs/2311.16079": {
    "title": "MEDITRON-70B: Scaling Medical Pretraining for Large Language Models",
    "volume": "Nov",
    "abstract": "Large language models (LLMs) can potentially democratize access to medical knowledge. While many efforts have been made to harness and improve LLMs' medical knowledge and reasoning capacities, the resulting models are either closed-source (e.g., PaLM, GPT-4) or limited in scale (<= 13B parameters), which restricts their abilities. In this work, we improve access to large-scale medical LLMs by releasing MEDITRON: a suite of open-source LLMs with 7B and 70B parameters adapted to the medical domain. MEDITRON builds on Llama-2 (through our adaptation of Nvidia's Megatron-LM distributed trainer), and extends pretraining on a comprehensively curated medical corpus, including selected PubMed articles, abstracts, and internationally-recognized medical guidelines. Evaluations using four major medical benchmarks show significant performance gains over several state-of-the-art baselines before and after task-specific finetuning. Overall, MEDITRON achieves a 6% absolute performance gain over the best public baseline in its parameter class and 3% over the strongest baseline we finetuned from Llama-2. Compared to closed-source LLMs, MEDITRON-70B outperforms GPT-3.5 and Med-PaLM and is within 5% of GPT-4 and 10% of Med-PaLM-2. We release our code for curating the medical pretraining corpus and the MEDITRON model weights to drive open-source development of more capable medical LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeming Chen",
      "Alejandro Hernndez Cano",
      "Angelika Romanou",
      "Antoine Bonnet",
      "Kyle Matoba",
      "Francesco Salvi",
      "Matteo Pagliardini",
      "Simin Fan",
      "Andreas Kpf",
      "Amirkeivan Mohtashami",
      "Alexandre Sallinen",
      "Alireza Sakhaeirad",
      "Vinitra Swamy",
      "Igor Krawczuk",
      "Deniz Bayazit",
      "Axel Marmet",
      "Syrielle Montariol",
      "Mary-Anne Hartley",
      "Martin Jaggi",
      "Antoine Bosselut"
    ]
  },
  "http://arxiv.org/abs/2311.16075": {
    "title": "BioLORD-2023: Semantic Textual Representations Fusing LLM and Clinical Knowledge Graph Insights",
    "volume": "Nov",
    "abstract": "In this study, we investigate the potential of Large Language Models to complement biomedical knowledge graphs in the training of semantic models for the biomedical and clinical domains. Drawing on the wealth of the UMLS knowledge graph and harnessing cutting-edge Large Language Models, we propose a new state-of-the-art approach for obtaining high-fidelity representations of biomedical concepts and sentences, consisting of three steps: an improved contrastive learning phase, a novel self-distillation phase, and a weight averaging phase. Through rigorous evaluations via the extensive BioLORD testing suite and diverse downstream tasks, we demonstrate consistent and substantial performance improvements over the previous state of the art (e.g. +2pts on MedSTS, +2.5pts on MedNLI-S, +6.1pts on EHR-Rel-B). Besides our new state-of-the-art biomedical model for English, we also distill and release a multilingual model compatible with 50+ languages and finetuned on 7 European languages. Many clinical pipelines can benefit from our latest models. Our new multilingual model enables a range of languages to benefit from our advancements in biomedical semantic representation learning, opening a new avenue for bioinformatics researchers around the world. As a result, we hope to see BioLORD-2023 becoming a precious tool for future biomedical applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Franois Remy",
      "Kris Demuynck",
      "Thomas Demeester"
    ]
  },
  "http://arxiv.org/abs/2311.16502": {
    "title": "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI",
    "volume": "Nov",
    "abstract": "We introduce MMMU: a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering. These questions span 30 subjects and 183 subfields, comprising 30 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures. Unlike existing benchmarks, MMMU focuses on advanced perception and reasoning with domain-specific knowledge, challenging models to perform tasks akin to those faced by experts. Our evaluation of 14 open-source LMMs and the proprietary GPT-4V(ision) highlights the substantial challenges posed by MMMU. Even the advanced GPT-4V only achieves a 56% accuracy, indicating significant room for improvement. We believe MMMU will stimulate the community to build next-generation multimodal foundation models towards expert artificial general intelligence",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Yue",
      "Yuansheng Ni",
      "Kai Zhang",
      "Tianyu Zheng",
      "Ruoqi Liu",
      "Ge Zhang",
      "Samuel Stevens",
      "Dongfu Jiang",
      "Weiming Ren",
      "Yuxuan Sun",
      "Cong Wei",
      "Botao Yu",
      "Ruibin Yuan",
      "Renliang Sun",
      "Ming Yin",
      "Boyuan Zheng",
      "Zhenzhu Yang",
      "Yibo Liu",
      "Wenhao Huang",
      "Huan Sun",
      "Yu Su",
      "Wenhu Chen"
    ]
  },
  "http://arxiv.org/abs/2311.15983": {
    "title": "Sparsify-then-Classify: From Internal Neurons of Large Language Models To Efficient Text Classifiers",
    "volume": "Nov",
    "abstract": "Among the many tasks that Large Language Models (LLMs) have revolutionized is text classification. However, existing approaches for applying pretrained LLMs to text classification predominantly rely on using single token outputs from only the last layer of hidden states. As a result, they suffer from limitations in efficiency, task-specificity, and interpretability. In our work, we contribute an approach that uses all internal representations by employing multiple pooling strategies on all activation and hidden states. Our novel lightweight strategy, Sparsify-then-Classify (STC) first sparsifies task-specific features layer-by-layer, then aggregates across layers for text classification. STC can be applied as a seamless plug-and-play module on top of existing LLMs. Our experiments on a comprehensive set of models and datasets demonstrate that STC not only consistently improves the classification performance of pretrained and fine-tuned models, but is also more efficient for both training and inference, and is more intrinsically interpretable",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilun Liu",
      "Difan Jiao",
      "Ashton Anderson"
    ]
  },
  "http://arxiv.org/abs/2311.15964": {
    "title": "Efficient Pre-training for Localized Instruction Generation of Videos",
    "volume": "Nov",
    "abstract": "Procedural videos show step-by-step demonstrations of tasks like recipe preparation. Understanding such videos is challenging, involving the precise localization of steps and the generation of textual instructions. Manually annotating steps and writing instructions is costly, which limits the size of current datasets and hinders effective learning. Leveraging large but noisy video-transcript datasets for pre-training can boost performance, but demands significant computational resources. Furthermore, transcripts contain irrelevant content and exhibit style variation compared to instructions written by human annotators. To mitigate both issues, we propose a technique, Sieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters irrelevant transcripts and (ii) Swap enhances the quality of the text instruction by automatically replacing the transcripts with human-written instructions from a text-only recipe dataset. The curated dataset, three orders of magnitude smaller than current web-scale datasets, enables efficient training of large-scale models with competitive performance. We complement our Sieve-\\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step localization and instruction generation for procedural videos. When this model is pre-trained on our curated dataset, it achieves state-of-the-art performance in zero-shot and finetuning settings on YouCook2 and Tasty, while using a fraction of the computational resources",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anil Batra",
      "Davide Moltisanti",
      "Laura Sevilla-Lara",
      "Marcus Rohrbach",
      "Frank Keller"
    ]
  },
  "http://arxiv.org/abs/2311.15954": {
    "title": "A Quantitative Approach to Understand Self-Supervised Models as Cross-lingual Feature Extractors",
    "volume": "Nov",
    "abstract": "In this work, we study the features extracted by English self-supervised learning (SSL) models in cross-lingual contexts and propose a new metric to predict the quality of feature representations. Using automatic speech recognition (ASR) as a downstream task, we analyze the effect of model size, training objectives, and model architecture on the models' performance as a feature extractor for a set of topologically diverse corpora. We develop a novel metric, the Phonetic-Syntax Ratio (PSR), to measure the phonetic and synthetic information in the extracted representations using deep generalized canonical correlation analysis. Results show the contrastive loss in the wav2vec2.0 objective facilitates more effective cross-lingual feature extraction. There is a positive correlation between PSR scores and ASR performance, suggesting that phonetic information extracted by monolingual SSL models can be used for downstream tasks in cross-lingual settings. The proposed metric is an effective indicator of the quality of the representations and can be useful for model selection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyue Stella Li",
      "Beining Xu",
      "Xiangyu Zhang",
      "Hexin Liu",
      "Wenhan Chao",
      "Leibny Paola Garcia"
    ]
  },
  "http://arxiv.org/abs/2311.15946": {
    "title": "Leveraging deep active learning to identify low-resource mobility functioning information in public clinical notes",
    "volume": "Nov",
    "abstract": "Function is increasingly recognized as an important indicator of whole-person health, although it receives little attention in clinical natural language processing research. We introduce the first public annotated dataset specifically on the Mobility domain of the International Classification of Functioning, Disability and Health (ICF), aiming to facilitate automatic extraction and analysis of functioning information from free-text clinical notes. We utilize the National NLP Clinical Challenges (n2c2) research dataset to construct a pool of candidate sentences using keyword expansion. Our active learning approach, using query-by-committee sampling weighted by density representativeness, selects informative sentences for human annotation. We train BERT and CRF models, and use predictions from these models to guide the selection of new sentences for subsequent annotation iterations. Our final dataset consists of 4,265 sentences with a total of 11,784 entities, including 5,511 Action entities, 5,328 Mobility entities, 306 Assistance entities, and 639 Quantification entities. The inter-annotator agreement (IAA), averaged over all entity types, is 0.72 for exact matching and 0.91 for partial matching. We also train and evaluate common BERT models and state-of-the-art Nested NER models. The best F1 scores are 0.84 for Action, 0.7 for Mobility, 0.62 for Assistance, and 0.71 for Quantification. Empirical results demonstrate promising potential of NER models to accurately extract mobility functioning information from clinical text. The public availability of our annotated dataset will facilitate further research to comprehensively capture functioning information in electronic health records (EHRs)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuan-Dung Le",
      "Zhuqi Miao",
      "Samuel Alvarado",
      "Brittany Smith",
      "William Paiva",
      "Thanh Thieu"
    ]
  },
  "http://arxiv.org/abs/2311.15941": {
    "title": "Tell2Design: A Dataset for Language-Guided Floor Plan Generation",
    "volume": "Nov",
    "abstract": "We consider the task of generating designs directly from natural language descriptions, and consider floor plan generation as the initial research area. Language conditional generative models have recently been very successful in generating high-quality artistic images. However, designs must satisfy different constraints that are not present in generating artistic images, particularly spatial and relational constraints. We make multiple contributions to initiate research on this task. First, we introduce a novel dataset, \\textit{Tell2Design} (T2D), which contains more than $80k$ floor plan designs associated with natural language instructions. Second, we propose a Sequence-to-Sequence model that can serve as a strong baseline for future research. Third, we benchmark this task with several text-conditional image generation models. We conclude by conducting human evaluations on the generated samples and providing an analysis of human performance. We hope our contributions will propel the research on language-guided design generation forward",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sicong Leng",
      "Yang Zhou",
      "Mohammed Haroon Dupty",
      "Wee Sun Lee",
      "Sam Conrad Joyce",
      "Wei Lu"
    ]
  },
  "http://arxiv.org/abs/2311.15930": {
    "title": "WorldSense: A Synthetic Benchmark for Grounded Reasoning in Large Language Models",
    "volume": "Nov",
    "abstract": "We propose WorldSense, a benchmark designed to assess the extent to which LLMs are consistently able to sustain tacit world models, by testing how they draw simple inferences from descriptions of simple arrangements of entities. Worldsense is a synthetic benchmark with three problem types, each with their own trivial control, which explicitly avoids bias by decorrelating the abstract structure of problems from the vocabulary and expressions, and by decorrelating all problem subparts with the correct response. We run our benchmark on three state-of-the-art chat-LLMs (GPT3.5, GPT4 and Llama2-chat) and show that these models make errors even with as few as three objects. Furthermore, they have quite heavy response biases, preferring certain responses irrespective of the question. Errors persist even with chain-of-thought prompting and in-context learning. Lastly, we show that while finetuning on similar problems does result in substantial improvements -- within- and out-of-distribution -- the finetuned models do not generalise beyond a constraint problem space",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youssef Benchekroun",
      "Megi Dervishi",
      "Mark Ibrahim",
      "Jean-Baptiste Gaya",
      "Xavier Martinet",
      "Grgoire Mialon",
      "Thomas Scialom",
      "Emmanuel Dupoux",
      "Dieuwke Hupkes",
      "Pascal Vincent"
    ]
  },
  "http://arxiv.org/abs/2311.16483": {
    "title": "ChartLlama: A Multimodal LLM for Chart Understanding and Generation",
    "volume": "Nov",
    "abstract": "Multi-modal large language models have demonstrated impressive performances on most vision-language tasks. However, the model generally lacks the understanding capabilities for specific domain data, particularly when it comes to interpreting chart figures. This is mainly due to the lack of relevant multi-modal instruction tuning datasets. In this article, we create a high-quality instruction-tuning dataset leveraging GPT-4. We develop a multi-step data generation process in which different steps are responsible for generating tabular data, creating chart figures, and designing instruction tuning data separately. Our method's flexibility enables us to generate diverse, high-quality instruction-tuning data consistently and efficiently while maintaining a low resource expenditure. Additionally, it allows us to incorporate a wider variety of chart and task types not yet featured in existing datasets. Next, we introduce ChartLlama, a multi-modal large language model that we've trained using our created dataset. ChartLlama outperforms all prior methods in ChartQA, Chart-to-text, and Chart-extraction evaluation benchmarks. Additionally, ChartLlama significantly improves upon the baseline in our specially compiled chart dataset, which includes new chart and task types. The results of ChartLlama confirm the value and huge potential of our proposed data generation method in enhancing chart comprehension",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yucheng Han",
      "Chi Zhang",
      "Xin Chen",
      "Xu Yang",
      "Zhibin Wang",
      "Gang Yu",
      "Bin Fu",
      "Hanwang Zhang"
    ]
  },
  "http://arxiv.org/abs/2311.15896": {
    "title": "Data Generation for Post-OCR correction of Cyrillic handwriting",
    "volume": "Nov",
    "abstract": "This paper introduces a novel approach to post-Optical Character Recognition Correction (POC) for handwritten Cyrillic text, addressing a significant gap in current research methodologies. This gap is due to the lack of large text corporas that provide OCR errors for further training of language-based POC models, which are demanding in terms of corpora size. Our study primarily focuses on the development and application of a synthetic handwriting generation engine based on B\\'ezier curves. Such an engine generates highly realistic handwritten text in any amounts, which we utilize to create a substantial dataset by transforming Russian text corpora sourced from the internet. We apply a Handwritten Text Recognition (HTR) model to this dataset to identify OCR errors, forming the basis for our POC model training. The correction model is trained on a 90-symbol input context, utilizing a pre-trained T5 architecture with a seq2seq correction task. We evaluate our approach on HWR200 and School_notebooks_RU datasets as they provide significant challenges in the HTR domain. Furthermore, POC can be used to highlight errors for teachers, evaluating student performance. This can be done simply by comparing sentences before and after correction, displaying differences in text. Our primary contribution lies in the innovative use of B\\'ezier curves for Cyrillic text generation and subsequent error correction using a specialized POC model. We validate our approach by presenting Word Accuracy Rate (WAR) and Character Accuracy Rate (CAR) results, both with and without post-OCR correction, using real open corporas of handwritten Cyrillic text. These results, coupled with our methodology, are designed to be reproducible, paving the way for further advancements in the field of OCR and handwritten text analysis. Paper contributions can be found in https://github.com/dbrainio/CyrillicHandwritingPOC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evgenii Davydkin",
      "Aleksandr Markelov",
      "Egor Iuldashev",
      "Anton Dudkin",
      "Ivan Krivorotov"
    ]
  },
  "http://arxiv.org/abs/2311.15786": {
    "title": "YUAN 2.0: A Large Language Model with Localized Filtering-based Attention",
    "volume": "Nov",
    "abstract": "In this work, the Localized Filtering-based Attention (LFA) is introduced to incorporate prior knowledge of local dependencies of natural language into Attention. Based on LFA, we develop and release Yuan 2.0, a large language model with parameters ranging from 2.1 billion to 102.6 billion. A data filtering and generation method is presented to build pretraining and fine-tuning dataset in high quality. A distributed training method with non-uniform pipeline parallel, data parallel, and optimizer parallel is proposed, which greatly reduces the bandwidth requirements of intra-node communication, and achieves good performance in large-scale distributed training. Yuan 2.0 models display impressive ability in code generation, math problem-solving, and chat compared with existing models. The latest version of YUAN 2.0, including model weights and source code, is accessible at Github",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaohua Wu",
      "Xudong Zhao",
      "Shenling Wang",
      "Jiangang Luo",
      "Lingjun Li",
      "Xi Chen",
      "Bing Zhao",
      "Wei Wang",
      "Tong Yu",
      "Rongguo Zhang",
      "Jiahua Zhang",
      "Chao Wang"
    ]
  },
  "http://arxiv.org/abs/2311.15781": {
    "title": "Increasing Coverage and Precision of Textual Information in Multilingual Knowledge Graphs",
    "volume": "Nov",
    "abstract": "Recent work in Natural Language Processing and Computer Vision has been using textual information -- e.g., entity names and descriptions -- available in knowledge graphs to ground neural models to high-quality structured data. However, when it comes to non-English languages, the quantity and quality of textual information are comparatively scarce. To address this issue, we introduce the novel task of automatic Knowledge Graph Enhancement (KGE) and perform a thorough investigation on bridging the gap in both the quantity and quality of textual information between English and non-English languages. More specifically, we: i) bring to light the problem of increasing multilingual coverage and precision of entity names and descriptions in Wikidata; ii) demonstrate that state-of-the-art methods, namely, Machine Translation (MT), Web Search (WS), and Large Language Models (LLMs), struggle with this task; iii) present M-NTA, a novel unsupervised approach that combines MT, WS, and LLMs to generate high-quality textual information; and, iv) study the impact of increasing multilingual coverage and precision of non-English textual information in Entity Linking, Knowledge Graph Completion, and Question Answering. As part of our effort towards better multilingual knowledge graphs, we also introduce WikiKGE-10, the first human-curated benchmark to evaluate KGE approaches in 10 languages across 7 language families",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simone Conia",
      "Min Li",
      "Daniel Lee",
      "Umar Farooq Minhas",
      "Ihab Ilyas",
      "Yunyao Li"
    ]
  },
  "http://arxiv.org/abs/2311.15766": {
    "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
    "volume": "Nov",
    "abstract": "In recent years, large language models (LLMs) have spurred a new research paradigm in natural language processing. Despite their excellent capability in knowledge-based question answering and reasoning, their potential to retain faulty or even harmful knowledge poses risks of malicious application. The challenge of mitigating this issue and transforming these models into purer assistants is crucial for their widespread applicability. Unfortunately, Retraining LLMs repeatedly to eliminate undesirable knowledge is impractical due to their immense parameters. Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge in an efficient manner, without affecting unrelated knowledge in the model. To this end, we provide a survey of knowledge unlearning in the era of LLMs. Firstly, we formally define the knowledge unlearning problem and distinguish it from related works. Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and in-context learning, and introduce details of these unlearning methods. We further present evaluation datasets used in existing methods, and finally conclude this survey by presenting the ongoing challenges and future directions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nianwen Si",
      "Hao Zhang",
      "Heyu Chang",
      "Wenlin Zhang",
      "Dan Qu",
      "Weiqiang Zhang"
    ]
  },
  "http://arxiv.org/abs/2311.15759": {
    "title": "Towards Vision Enhancing LLMs: Empowering Multimodal Knowledge Storage and Sharing in LLMs",
    "volume": "Nov",
    "abstract": "Recent advancements in multimodal large language models (MLLMs) have achieved significant multimodal generation capabilities, akin to GPT-4. These models predominantly map visual information into language representation space, leveraging the vast knowledge and powerful text generation abilities of LLMs to produce multimodal instruction-following responses. We could term this method as LLMs for Vision because of its employing LLMs for visual-language understanding, yet observe that these MLLMs neglect the potential of harnessing visual knowledge to enhance overall capabilities of LLMs, which could be regraded as Vision Enhancing LLMs. In this paper, we propose an approach called MKS2, aimed at enhancing LLMs through empowering Multimodal Knowledge Storage and Sharing in LLMs. Specifically, we introduce the Modular Visual Memory, a component integrated into the internal blocks of LLMs, designed to store open-world visual information efficiently. Additionally, we present a soft Mixtures-of-Multimodal Experts architecture in LLMs to invoke multimodal knowledge collaboration during generation. Our comprehensive experiments demonstrate that MKS2 substantially augments the reasoning capabilities of LLMs in contexts necessitating physical or commonsense knowledge. It also delivers competitive results on multimodal benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunxin Li",
      "Baotian Hu",
      "Wei Wang",
      "Xiaochun Cao",
      "Min Zhang"
    ]
  },
  "http://arxiv.org/abs/2311.15723": {
    "title": "Italian Crossword Generator: Enhancing Education through Interactive Word Puzzles",
    "volume": "Nov",
    "abstract": "Educational crosswords offer numerous benefits for students, including increased engagement, improved understanding, critical thinking, and memory retention. Creating high-quality educational crosswords can be challenging, but recent advances in natural language processing and machine learning have made it possible to use language models to generate nice wordplays. The exploitation of cutting-edge language models like GPT3-DaVinci, GPT3-Curie, GPT3-Babbage, GPT3-Ada, and BERT-uncased has led to the development of a comprehensive system for generating and verifying crossword clues. A large dataset of clue-answer pairs was compiled to fine-tune the models in a supervised manner to generate original and challenging clues from a given keyword. On the other hand, for generating crossword clues from a given text, Zero/Few-shot learning techniques were used to extract clues from the input text, adding variety and creativity to the puzzles. We employed the fine-tuned model to generate data and labeled the acceptability of clue-answer parts with human supervision. To ensure quality, we developed a classifier by fine-tuning existing language models on the labeled dataset. Conversely, to assess the quality of clues generated from the given text using zero/few-shot learning, we employed a zero-shot learning approach to check the quality of generated clues. The results of the evaluation have been very promising, demonstrating the effectiveness of the approach in creating high-standard educational crosswords that offer students engaging and rewarding learning experiences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kamyar Zeinalipour",
      "Tommaso laquinta",
      "Asya Zanollo",
      "Giovanni Angelini",
      "Leonardo Rigutini",
      "Marco Maggini",
      "Marco Gori"
    ]
  },
  "http://arxiv.org/abs/2311.15716": {
    "title": "Justifiable Artificial Intelligence: Engineering Large Language Models for Legal Applications",
    "volume": "Nov",
    "abstract": "In this work, I discuss how Large Language Models can be applied in the legal domain, circumventing their current drawbacks. Despite their large success and acceptance, their lack of explainability hinders legal experts to trust in their output, and this happens rightfully so. However, in this paper, I argue in favor of a new view, Justifiable Artificial Intelligence, instead of focusing on Explainable Artificial Intelligence. I discuss in this paper how gaining evidence for and against a Large Language Model's output may make their generated texts more trustworthy - or hold them accountable for misinformation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sabine Wehnert"
    ]
  },
  "http://arxiv.org/abs/2311.15698": {
    "title": "Cerbero-7B: A Leap Forward in Language-Specific LLMs Through Enhanced Chat Corpus Generation and Evaluation",
    "volume": "Nov",
    "abstract": "This study introduces a novel approach for generating high-quality, language-specific chat corpora using a self-chat mechanism. We combine a generator LLM for creating new samples and an embedder LLM to ensure diversity. A new Masked Language Modelling (MLM) model-based quality assessment metric is proposed for evaluating and filtering the corpora. Utilizing the llama2-70b as the generator and a multilingual sentence transformer as embedder, we generate an Italian chat corpus and refine the Fauno corpus, which is based on translated English ChatGPT self-chat data. The refinement uses structural assertions and Natural Language Processing techniques. Both corpora undergo a comprehensive quality evaluation using the proposed MLM model-based quality metric. The Italian LLM fine-tuned with these corpora demonstrates significantly enhanced language comprehension and question-answering skills. The resultant model, cerbero-7b, establishes a new state-of-the-art for Italian LLMs. This approach marks a substantial advancement in the development of language-specific LLMs, with a special emphasis on augmenting corpora for underrepresented languages like Italian",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Federico A. Galatolo",
      "Mario G. C. A. Cimino"
    ]
  },
  "http://arxiv.org/abs/2311.15653": {
    "title": "MoDS: Model-oriented Data Selection for Instruction Tuning",
    "volume": "Nov",
    "abstract": "Instruction tuning has become the de facto method to equip large language models (LLMs) with the ability of following user instructions. Usually, hundreds of thousands or millions of instruction-following pairs are employed to fine-tune the foundation LLMs. Recently, some studies show that a small number of high-quality instruction data is enough. However, how to select appropriate instruction data for a given LLM is still an open problem. To address this problem, in this paper we present a model-oriented data selection (MoDS) approach, which selects instruction data based on a new criteria considering three aspects: quality, coverage and necessity. First, our approach utilizes a quality evaluation model to filter out the high-quality subset from the original instruction dataset, and then designs an algorithm to further select from the high-quality subset a seed instruction dataset with good coverage. The seed dataset is applied to fine-tune the foundation LLM to obtain an initial instruction-following LLM. Finally, we develop a necessity evaluation model to find out the instruction data which are performed badly in the initial instruction-following LLM and consider them necessary instructions to further improve the LLMs. In this way, we can get a small high-quality, broad-coverage and high-necessity subset from the original instruction datasets. Experimental results show that, the model fine-tuned with 4,000 instruction pairs selected by our approach could perform better than the model fine-tuned with the full original dataset which includes 214k instruction data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianlong Du",
      "Chengqing Zong",
      "Jiajun Zhang"
    ]
  },
  "http://arxiv.org/abs/2311.15648": {
    "title": "Reinforcement Learning from Diffusion Feedback: Q* for Image Search",
    "volume": "Nov",
    "abstract": "Large vision-language models are steadily gaining personalization capabilities at the cost of fine-tuning or data augmentation. We present two models for image generation using model-agnostic learning that align semantic priors with generative capabilities. RLDF, or Reinforcement Learning from Diffusion Feedback, is a singular approach for visual imitation through prior-preserving reward function guidance. This employs Q-learning (with standard Q*) for generation and follows a semantic-rewarded trajectory for image search through finite encoding-tailored actions. The second proposed method, noisy diffusion gradient, is optimization driven. At the root of both methods is a special CFG encoding that we propose for continual semantic guidance. Using only a single input image and no text input, RLDF generates high-quality images over varied domains including retail, sports and agriculture showcasing class-consistency and strong visual diversity. Project website is available at https://infernolia.github.io/RLDF",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aboli Marathe"
    ]
  },
  "http://arxiv.org/abs/2311.15642": {
    "title": "InfoPattern: Unveiling Information Propagation Patterns in Social Media",
    "volume": "Nov",
    "abstract": "Social media play a significant role in shaping public opinion and influencing ideological communities through information propagation. Our demo InfoPattern centers on the interplay between language and human ideology. The demo (Code: https://github.com/blender-nlp/InfoPattern ) is capable of: (1) red teaming to simulate adversary responses from opposite ideology communities; (2) stance detection to identify the underlying political sentiments in each message; (3) information propagation graph discovery to reveal the evolution of claims across various communities over time. (Live Demo: https://incas.csl.illinois.edu/blender/About )",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi Han",
      "Jialiang Xu",
      "Manling Li",
      "Hanning Zhang",
      "Tarek Abdelzaher",
      "Heng Ji"
    ]
  },
  "http://arxiv.org/abs/2311.16203": {
    "title": "ChatTraffic: Text-to-Traffic Generation via Diffusion Model",
    "volume": "Nov",
    "abstract": "Traffic prediction is one of the most significant foundations in Intelligent Transportation Systems (ITS). Traditional traffic prediction methods rely only on historical traffic data to predict traffic trends and face two main challenges. 1) insensitivity to unusual events. 2) poor performance in long-term prediction. In this work, we explore how generative models combined with text describing the traffic system can be applied for traffic generation and name the task Text-to-Traffic Generation (TTG). The key challenge of the TTG task is how to associate text with the spatial structure of the road network and traffic data for generating traffic situations. To this end, we propose ChatTraffic, the first diffusion model for text-to-traffic generation. To guarantee the consistency between synthetic and real data, we augment a diffusion model with the Graph Convolutional Network (GCN) to extract spatial correlations of traffic data. In addition, we construct a large dataset containing text-traffic pairs for the TTG task. We benchmarked our model qualitatively and quantitatively on the released dataset. The experimental results indicate that ChatTraffic can generate realistic traffic situations from the text. Our code and dataset are available at https://github.com/ChyaZhang/ChatTraffic",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengyang Zhang",
      "Yong Zhang",
      "Qitan Shao",
      "Bo Li",
      "Yisheng Lv",
      "Xinglin Piao",
      "Baocai Yin"
    ]
  },
  "http://arxiv.org/abs/2311.15626": {
    "title": "The WebCrow French Crossword Solver",
    "volume": "Nov",
    "abstract": "Crossword puzzles are one of the most popular word games, played in different languages all across the world, where riddle style can vary significantly from one country to another. Automated crossword resolution is challenging, and typical solvers rely on large databases of previously solved crosswords. In this work, we extend WebCrow 2.0, an automatic crossword solver, to French, making it the first program for crossword solving in the French language. To cope with the lack of a large repository of clue-answer crossword data, WebCrow 2.0 exploits multiple modules, called experts, that retrieve candidate answers from heterogeneous resources, such as the web, knowledge graphs, and linguistic rules. We compared WebCrow's performance against humans in two different challenges. Despite the limited amount of past crosswords, French WebCrow was competitive, actually outperforming humans in terms of speed and accuracy, thus proving its capabilities to generalize to new languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giovanni Angelini",
      "Marco Ernandes",
      "Tommaso laquinta",
      "Caroline Stehl",
      "Fanny Simes",
      "Kamyar Zeinalipour",
      "Andrea Zugarini",
      "Marco Gori"
    ]
  },
  "http://arxiv.org/abs/2311.15623": {
    "title": "Injecting linguistic knowledge into BERT for Dialogue State Tracking",
    "volume": "Nov",
    "abstract": "Dialogue State Tracking (DST) models often employ intricate neural network architectures, necessitating substantial training data, and their inference processes lack transparency. This paper proposes a method that extracts linguistic knowledge via an unsupervised framework and subsequently utilizes this knowledge to augment BERT's performance and interpretability in DST tasks. The knowledge extraction procedure is computationally economical and does not necessitate annotations or additional training data. The injection of the extracted knowledge necessitates the addition of only simple neural modules. We employ the Convex Polytopic Model (CPM) as a feature extraction tool for DST tasks and illustrate that the acquired features correlate with the syntactic and semantic patterns in the dialogues. This correlation facilitates a comprehensive understanding of the linguistic features influencing the DST model's decision-making process. We benchmark this framework on various DST tasks and observe a notable improvement in accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohan Feng",
      "Xixin Wu",
      "Helen Meng"
    ]
  },
  "http://arxiv.org/abs/2311.15614": {
    "title": "FreeAL: Towards Human-Free Active Learning in the Era of Large Language Models",
    "volume": "Nov",
    "abstract": "Collecting high-quality labeled data for model training is notoriously time-consuming and labor-intensive for various NLP tasks. While copious solutions, such as active learning for small language models (SLMs) and prevalent in-context learning in the era of large language models (LLMs), have been proposed and alleviate the labeling burden to some extent, their performances are still subject to human intervention. It is still underexplored how to reduce the annotation cost in the LLMs era. To bridge this, we revolutionize traditional active learning and propose an innovative collaborative learning framework FreeAL to interactively distill and filter the task-specific knowledge from LLMs. During collaborative training, an LLM serves as an active annotator inculcating its coarse-grained knowledge, while a downstream SLM is incurred as a student to filter out high-quality in-context samples to feedback LLM for the subsequent label refinery. Extensive experiments on eight benchmark datasets demonstrate that FreeAL largely enhances the zero-shot performances for both SLM and LLM without any human supervision. The code is available at https://github.com/Justherozen/FreeAL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruixuan Xiao",
      "Yiwen Dong",
      "Junbo Zhao",
      "Runze Wu",
      "Minmin Lin",
      "Gang Chen",
      "Haobo Wang"
    ]
  },
  "http://arxiv.org/abs/2311.15596": {
    "title": "Can Vision-Language Models Think from a First-Person Perspective?",
    "volume": "Nov",
    "abstract": "Vision-language models (VLMs) have recently shown promising results in traditional downstream tasks. Evaluation studies have emerged to assess their abilities, with the majority focusing on the third-person perspective, and only a few addressing specific tasks from the first-person perspective. However, the capability of VLMs to \"think\" from a first-person perspective, a crucial attribute for advancing autonomous agents and robotics, remains largely unexplored. To bridge this research gap, we introduce EgoThink, a novel visual question-answering benchmark that encompasses six core capabilities with twelve detailed dimensions. The benchmark is constructed using selected clips from egocentric videos, with manually annotated question-answer pairs containing first-person information. To comprehensively assess VLMs, we evaluate eighteen popular VLMs on EgoThink. Moreover, given the open-ended format of the answers, we use GPT-4 as the automatic judge to compute single-answer grading. Experimental results indicate that although GPT-4V leads in numerous dimensions, all evaluated VLMs still possess considerable potential for improvement in first-person perspective tasks. Meanwhile, enlarging the number of trainable parameters has the most significant impact on model performance on EgoThink. In conclusion, EgoThink serves as a valuable addition to existing evaluation benchmarks for VLMs, providing an indispensable resource for future research in the realm of embodied artificial intelligence and robotics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sijie Cheng",
      "Zhicheng Guo",
      "Jingwen Wu",
      "Kechen Fang",
      "Peng Li",
      "Huaping Liu",
      "Yang Liu"
    ]
  },
  "http://arxiv.org/abs/2311.16201": {
    "title": "Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation",
    "volume": "Nov",
    "abstract": "Recent advances in image tokenizers, such as VQ-VAE, have enabled text-to-image generation using auto-regressive methods, similar to language modeling. However, these methods have yet to leverage pre-trained language models, despite their adaptability to various downstream tasks. In this work, we explore this gap by adapting a pre-trained language model for auto-regressive text-to-image generation, and find that pre-trained language models offer limited help. We provide a two-fold explanation by analyzing tokens from each modality. First, we demonstrate that image tokens possess significantly different semantics compared to text tokens, rendering pre-trained language models no more effective in modeling them than randomly initialized ones. Second, the text tokens in the image-text datasets are too simple compared to normal language model pre-training data, which causes the catastrophic degradation of language models' capability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhui Zhang",
      "Brandon McKinzie",
      "Zhe Gan",
      "Vaishaal Shankar",
      "Alexander Toshev"
    ]
  },
  "http://arxiv.org/abs/2311.15566": {
    "title": "SpotServe: Serving Generative Large Language Models on Preemptible Instances",
    "volume": "Nov",
    "abstract": "The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them cheaply. This paper aims to reduce the monetary cost for serving LLMs by leveraging preemptible GPU instances on modern clouds, which offer accesses to spare GPUs at a much cheaper price than regular instances but may be preempted by the cloud at any time. Serving LLMs on preemptible instances requires addressing challenges induced by frequent instance preemptions and the necessity of migrating instances to handle these preemptions. This paper presents SpotServe, the first distributed LLM serving system on preemptible instances. Several key techniques in SpotServe realize fast and reliable serving of generative LLMs on cheap preemptible instances. First, SpotServe dynamically adapts the LLM parallelization configuration for dynamic instance availability and fluctuating workload, while balancing the trade-off among the overall throughput, inference latency and monetary costs. Second, to minimize the cost of migrating instances for dynamic reparallelization, the task of migrating instances is formulated as a bipartite graph matching problem, which uses the Kuhn-Munkres algorithm to identify an optimal migration plan that minimizes communications. Finally, to take advantage of the grace period offered by modern clouds, we introduce stateful inference recovery, a new inference mechanism that commits inference progress at a much finer granularity and allows SpotServe to cheaply resume inference upon preemption. We evaluate on real spot instance preemption traces and various popular LLMs and show that SpotServe can reduce the P99 tail latency by 2.4 - 9.1x compared with the best existing LLM serving systems. We also show that SpotServe can leverage the price advantage of preemptive instances, saving 54% monetary cost compared with only using on-demand instances",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xupeng Miao",
      "Chunan Shi",
      "Jiangfei Duan",
      "Xiaoli Xi",
      "Dahua Lin",
      "Bin Cui",
      "Zhihao Jia"
    ]
  },
  "http://arxiv.org/abs/2311.15565": {
    "title": "Evaluating the Efficacy of Hybrid Deep Learning Models in Distinguishing AI-Generated Text",
    "volume": "Nov",
    "abstract": "My research investigates the use of cutting-edge hybrid deep learning models to accurately differentiate between AI-generated text and human writing. I applied a robust methodology, utilising a carefully selected dataset comprising AI and human texts from various sources, each tagged with instructions. Advanced natural language processing techniques facilitated the analysis of textual features. Combining sophisticated neural networks, the custom model enabled it to detect nuanced differences between AI and human content",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Finbarrs Oketunji"
    ]
  },
  "http://arxiv.org/abs/2311.15564": {
    "title": "Boot and Switch: Alternating Distillation for Zero-Shot Dense Retrieval",
    "volume": "Nov",
    "abstract": "Neural 'dense' retrieval models are state of the art for many datasets, however these models often exhibit limited domain transfer ability. Existing approaches to adaptation are unwieldy, such as requiring explicit supervision, complex model architectures, or massive external models. We present $\\texttt{ABEL}$, a simple but effective unsupervised method to enhance passage retrieval in zero-shot settings. Our technique follows a straightforward loop: a dense retriever learns from supervision signals provided by a reranker, and subsequently, the reranker is updated based on feedback from the improved retriever. By iterating this loop, the two components mutually enhance one another's performance. Experimental results demonstrate that our unsupervised $\\texttt{ABEL}$ model outperforms both leading supervised and unsupervised retrievers on the BEIR benchmark. Meanwhile, it exhibits strong adaptation abilities to tasks and domains that were unseen during training. By either fine-tuning $\\texttt{ABEL}$ on labelled data or integrating it with existing supervised dense retrievers, we achieve state-of-the-art results.\\footnote{Source code is available at \\url{https://github.com/Fantabulous-J/BootSwitch}.}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Jiang",
      "Qiongkai Xu",
      "Tom Drummond",
      "Trevor Cohn"
    ]
  },
  "http://arxiv.org/abs/2311.15563": {
    "title": "Noisy Self-Training with Synthetic Queries for Dense Retrieval",
    "volume": "Nov",
    "abstract": "Although existing neural retrieval models reveal promising results when training data is abundant and the performance keeps improving as training data increases, collecting high-quality annotated data is prohibitively costly. To this end, we introduce a novel noisy self-training framework combined with synthetic queries, showing that neural retrievers can be improved in a self-evolution manner with no reliance on any external models. Experimental results show that our method improves consistently over existing methods on both general-domain (e.g., MS-MARCO) and out-of-domain (i.e., BEIR) retrieval benchmarks. Extra analysis on low-resource settings reveals that our method is data efficient and outperforms competitive baselines, with as little as 30% of labelled training data. Further extending the framework for reranker training demonstrates that the proposed method is general and yields additional gains on tasks of diverse domains.\\footnote{Source code is available at \\url{https://github.com/Fantabulous-J/Self-Training-DPR}}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Jiang",
      "Tom Drummond",
      "Trevor Cohn"
    ]
  },
  "http://arxiv.org/abs/2311.15548": {
    "title": "Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination",
    "volume": "Nov",
    "abstract": "The hallucination issue is recognized as a fundamental deficiency of large language models (LLMs), especially when applied to fields such as finance, education, and law. Despite the growing concerns, there has been a lack of empirical investigation. In this paper, we provide an empirical examination of LLMs' hallucination behaviors in financial tasks. First, we empirically investigate LLM model's ability of explaining financial concepts and terminologies. Second, we assess LLM models' capacity of querying historical stock prices. Third, to alleviate the hallucination issue, we evaluate the efficacy of four practical methods, including few-shot learning, Decoding by Contrasting Layers (DoLa), the Retrieval Augmentation Generation (RAG) method and the prompt-based tool learning method for a function to generate a query command. Finally, our major finding is that off-the-shelf LLMs experience serious hallucination behaviors in financial tasks. Therefore, there is an urgent need to call for research efforts in mitigating LLMs' hallucination",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoqiang Kang",
      "Xiao-Yang Liu"
    ]
  },
  "http://arxiv.org/abs/2311.15544": {
    "title": "The effect of source disclosure on evaluation of AI-generated messages: A two-part study",
    "volume": "Nov",
    "abstract": "Advancements in artificial intelligence (AI) over the last decade demonstrate that machines can exhibit communicative behavior and influence how humans think, feel, and behave. In fact, the recent development of ChatGPT has shown that large language models (LLMs) can be leveraged to generate high-quality communication content at scale and across domains, suggesting that they will be increasingly used in practice. However, many questions remain about how knowing the source of the messages influences recipients' evaluation of and preference for AI-generated messages compared to human-generated messages. This paper investigated this topic in the context of vaping prevention messaging. In Study 1, which was pre-registered, we examined the influence of source disclosure on people's evaluation of AI-generated health prevention messages compared to human-generated messages. We found that source disclosure (i.e., labeling the source of a message as AI vs. human) significantly impacted the evaluation of the messages but did not significantly alter message rankings. In a follow-up study (Study 2), we examined how the influence of source disclosure may vary by the participants' negative attitudes towards AI. We found a significant moderating effect of negative attitudes towards AI on message evaluation, but not for message selection. However, for those with moderate levels of negative attitudes towards AI, source disclosure decreased the preference for AI-generated messages. Overall, the results of this series of studies showed a slight bias against AI-generated messages once the source was disclosed, adding to the emerging area of study that lies at the intersection of AI and communication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sue Lim",
      "Ralf Schmlzle"
    ]
  },
  "http://arxiv.org/abs/2311.16480": {
    "title": "MI-Gen: Multiple Instance Generation of Pathology Reports for Gigapixel Whole-Slide Images",
    "volume": "Nov",
    "abstract": "Whole slide images are the foundation of digital pathology for the diagnosis and treatment of carcinomas. Writing pathology reports is laborious and error-prone for inexperienced pathologists. To reduce the workload and improve clinical automation, we investigate how to generate pathology reports given whole slide images. On the data end, we curated the largest WSI-text dataset (TCGA-PathoText). In specific, we collected nearly 10000 high-quality WSI-text pairs for visual-language models by recognizing and cleaning pathology reports which narrate diagnostic slides in TCGA. On the model end, we propose the multiple instance generative model (MI-Gen) which can produce pathology reports for gigapixel WSIs. We benchmark our model on the largest subset of TCGA-PathoText. Experimental results show our model can generate pathology reports which contain multiple clinical clues. Furthermore, WSI-text prediction can be seen as an approach of visual-language pre-training, which enables our model to be transferred to downstream diagnostic tasks like carcinoma grading and phenotyping. We observe that simple semantic extraction from the pathology reports can achieve the best performance (0.838 of F1 score) on BRCA subtyping without adding extra parameters or tricky fine-tuning. Our collected dataset and related code will all be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pingyi Chen",
      "Honglin Li",
      "Chenglu Zhu",
      "Sunyi Zheng",
      "Lin Yang"
    ]
  },
  "http://arxiv.org/abs/2311.15525": {
    "title": "Overview of the VLSP 2022 -- Abmusu Shared Task: A Data Challenge for Vietnamese Abstractive Multi-document Summarization",
    "volume": "Nov",
    "abstract": "This paper reports the overview of the VLSP 2022 - Vietnamese abstractive multi-document summarization (Abmusu) shared task for Vietnamese News. This task is hosted at the 9$^{th}$ annual workshop on Vietnamese Language and Speech Processing (VLSP 2022). The goal of Abmusu shared task is to develop summarization systems that could create abstractive summaries automatically for a set of documents on a topic. The model input is multiple news documents on the same topic, and the corresponding output is a related abstractive summary. In the scope of Abmusu shared task, we only focus on Vietnamese news summarization and build a human-annotated dataset of 1,839 documents in 600 clusters, collected from Vietnamese news in 8 categories. Participated models are evaluated and ranked in terms of \\texttt{ROUGE2-F1} score, the most typical evaluation metric for document summarization problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mai-Vu Tran",
      "Hoang-Quynh Le",
      "Duy-Cat Can",
      "Quoc-An Nguyen"
    ]
  },
  "http://arxiv.org/abs/2311.15513": {
    "title": "A Comparative and Experimental Study on Automatic Question Answering Systems and its Robustness against Word Jumbling",
    "volume": "Nov",
    "abstract": "Question answer generation using Natural Language Processing models is ubiquitous in the world around us. It is used in many use cases such as the building of chat bots, suggestive prompts in google search and also as a way of navigating information in banking mobile applications etc. It is highly relevant because a frequently asked questions (FAQ) list can only have a finite amount of questions but a model which can perform question answer generation could be able to answer completely new questions that are within the scope of the data. This helps us to be able to answer new questions accurately as long as it is a relevant question. In commercial applications, it can be used to increase customer satisfaction and ease of usage. However a lot of data is generated by humans so it is susceptible to human error and this can adversely affect the model's performance and we are investigating this through our work",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shashidhar Reddy Javaji",
      "Haoran Hu",
      "Sai Sameer Vennam",
      "Vijaya Gajanan Buddhavarapu"
    ]
  },
  "http://arxiv.org/abs/2311.15509": {
    "title": "A Corpus for Named Entity Recognition in Chinese Novels with Multi-genres",
    "volume": "Nov",
    "abstract": "Entities like person, location, organization are important for literary text analysis. The lack of annotated data hinders the progress of named entity recognition (NER) in literary domain. To promote the research of literary NER, we build the largest multi-genre literary NER corpus containing 263,135 entities in 105,851 sentences from 260 online Chinese novels spanning 13 different genres. Based on the corpus, we investigate characteristics of entities from different genres. We propose several baseline NER models and conduct cross-genre and cross-domain experiments. Experimental results show that genre difference significantly impact NER performance though not as much as domain difference like literary domain and news domain. Compared with NER in news domain, literary NER still needs much improvement and the Out-of-Vocabulary (OOV) problem is more challenging due to the high variety of entities in literary works",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanjie Zhao",
      "Jinge Xie",
      "Yuchen Yan",
      "Yuxiang Jia",
      "Yawen Ye",
      "Hongying Zan"
    ]
  },
  "http://arxiv.org/abs/2311.15507": {
    "title": "Improving Word Sense Disambiguation in Neural Machine Translation with Salient Document Context",
    "volume": "Nov",
    "abstract": "Lexical ambiguity is a challenging and pervasive problem in machine translation (\\mt). We introduce a simple and scalable approach to resolve translation ambiguity by incorporating a small amount of extra-sentential context in neural \\mt. Our approach requires no sense annotation and no change to standard model architectures. Since actual document context is not available for the vast majority of \\mt training data, we collect related sentences for each input to construct pseudo-documents. Salient words from pseudo-documents are then encoded as a prefix to each source sentence to condition the generation of the translation. To evaluate, we release \\docmucow, a challenge set for translation disambiguation based on the English-German \\mucow \\cite{raganato-etal-2020-evaluation} augmented with document IDs. Extensive experiments show that our method translates ambiguous source words better than strong sentence-level baselines and comparable document-level baselines while reducing training costs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elijah Rippeth",
      "Marine Carpuat",
      "Kevin Duh",
      "Matt Post"
    ]
  },
  "http://arxiv.org/abs/2311.15500": {
    "title": "Function-constrained Program Synthesis",
    "volume": "Nov",
    "abstract": "This work introduces (1) a technique that allows large language models (LLMs) to leverage user-provided code when solving programming tasks and (2) a method to iteratively generate modular sub-functions that can aid future code generation attempts when the initial code generated by the LLM is inadequate. Generating computer programs in general-purpose programming languages like Python poses a challenge for LLMs when instructed to use code provided in the prompt. Code-specific LLMs (e.g., GitHub Copilot, CodeLlama2) can generate code completions in real-time by drawing on all code available in a development environment. However, restricting code-specific LLMs to use only in-context code is not straightforward, as the model is not explicitly instructed to use the user-provided code and users cannot highlight precisely which snippets of code the model should incorporate into its context. Moreover, current systems lack effective recovery methods, forcing users to iteratively re-prompt the model with modified prompts until a sufficient solution is reached. Our method differs from traditional LLM-powered code-generation by constraining code-generation to an explicit function set and enabling recovery from failed attempts through automatically generated sub-functions. When the LLM cannot produce working code, we generate modular sub-functions to aid subsequent attempts at generating functional code. A by-product of our method is a library of reusable sub-functions that can solve related tasks, imitating a software team where efficiency scales with experience. We also introduce a new \"half-shot\" evaluation paradigm that provides tighter estimates of LLMs' coding abilities compared to traditional zero-shot evaluation. Our proposed evaluation method encourages models to output solutions in a structured format, decreasing syntax errors that can be mistaken for poor coding ability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Hajali",
      "Ignas Budvytis"
    ]
  },
  "http://arxiv.org/abs/2311.15490": {
    "title": "Optimizing and Fine-tuning Large Language Model for Urban Renewal",
    "volume": "Nov",
    "abstract": "This study aims to innovatively explore adaptive applications of large language models (LLM) in urban renewal. It also aims to improve its performance and text generation quality for knowledge question-answering (QA) tasks. Based on the ChatGLM, we automatically generate QA datasets using urban renewal scientific literature corpora in a self-instruct manner and then conduct joint fine-tuning training on the model using the Prefix and LoRA fine-tuning methods to create an LLM for urban renewal. By guiding the LLM to automatically generate QA data based on prompt words and given text, it is possible to quickly obtain datasets in the urban renewal field and provide data support for the fine-tuning training of LLMs. The experimental results show that the joint fine-tuning training method proposed in this study can significantly improve the performance of LLM on the QA tasks. Compared with LoRA fine-tuning, the method improves the Bleu and Rouge metrics on the test by about 5%; compared with the model before fine-tuning, the method improves the Bleu and Rouge metrics by about 15%-20%. This study demonstrates the effectiveness and superiority of the joint fine-tuning method using Prefix and LoRA for ChatGLM in the urban renewal knowledge QA tasks. It provides a new approach for fine-tuning LLMs on urban renewal-related tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Wang",
      "Xianyao Ling",
      "Tom Zhang",
      "Xuecao Li",
      "Shaolan Wang",
      "Zhixing Li",
      "Liang Zhang",
      "Peng Gong"
    ]
  },
  "http://arxiv.org/abs/2311.15480": {
    "title": "Automatic Time Signature Determination for New Scores Using Lyrics for Latent Rhythmic Structure",
    "volume": "Nov",
    "abstract": "There has recently been a sharp increase in interest in Artificial Intelligence-Generated Content (AIGC). Despite this, musical components such as time signatures have not been studied sufficiently to form an algorithmic determination approach for new compositions, especially lyrical songs. This is likely because of the neglect of musical details, which is critical for constructing a robust framework. Specifically, time signatures establish the fundamental rhythmic structure for almost all aspects of a song, including the phrases and notes. In this paper, we propose a novel approach that only uses lyrics as input to automatically generate a fitting time signature for lyrical songs and uncover the latent rhythmic structure utilizing explainable machine learning models. In particular, we devise multiple methods that are associated with discovering lyrical patterns and creating new features that simultaneously contain lyrical, rhythmic, and statistical information. In this approach, the best of our experimental results reveal a 97.6% F1 score and a 0.996 Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) score. In conclusion, our research directly generates time signatures from lyrics automatically for new scores utilizing machine learning, which is an innovative idea that approaches an understudied component of musicology and therefore contributes significantly to the future of Artificial Intelligence (AI) music generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Callie C. Liao",
      "Duoduo Liao",
      "Jesse Guessford"
    ]
  },
  "http://arxiv.org/abs/2311.15451": {
    "title": "Uncertainty-aware Language Modeling for Selective Question Answering",
    "volume": "Nov",
    "abstract": "We present an automatic large language model (LLM) conversion approach that produces uncertainty-aware LLMs capable of estimating uncertainty with every prediction. Our approach is model- and data-agnostic, is computationally-efficient, and does not rely on external models or systems. We evaluate converted models on the selective question answering setting -- to answer as many questions as possible while maintaining a given accuracy, forgoing providing predictions when necessary. As part of our results, we test BERT and Llama 2 model variants on the SQuAD extractive QA task and the TruthfulQA generative QA task. We show that using the uncertainty estimates provided by our approach to selectively answer questions leads to significantly higher accuracy over directly using model probabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Yang",
      "Shreya Ravikumar",
      "Fynn Schmitt-Ulms",
      "Satvik Lolla",
      "Ege Demir",
      "Iaroslav Elistratov",
      "Alex Lavaee",
      "Sadhana Lolla",
      "Elaheh Ahmadi",
      "Daniela Rus",
      "Alexander Amini",
      "Alejandro Perez"
    ]
  },
  "http://arxiv.org/abs/2311.15436": {
    "title": "Learning to Skip for Language Modeling",
    "volume": "Nov",
    "abstract": "Overparameterized large-scale language models have impressive generalization performance of in-context few-shot learning. However, most language models allocate the same amount of parameters or computation to each token, disregarding the complexity or importance of the input data. We argue that in language model pretraining, a variable amount of computation should be assigned to different tokens, and this can be efficiently achieved via a simple routing mechanism. Different from conventional early stopping techniques where tokens can early exit at only early layers, we propose a more general method that dynamically skips the execution of a layer (or module) for any input token with a binary router. In our extensive evaluation across 24 NLP tasks, we demonstrate that the proposed method can significantly improve the 1-shot performance compared to other competitive baselines only at mild extra cost for inference",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dewen Zeng",
      "Nan Du",
      "Tao Wang",
      "Yuanzhong Xu",
      "Tao Lei",
      "Zhifeng Chen",
      "Claire Cui"
    ]
  },
  "http://arxiv.org/abs/2311.15425": {
    "title": "Machine-Generated Text Detection using Deep Learning",
    "volume": "Nov",
    "abstract": "Our research focuses on the crucial challenge of discerning text produced by Large Language Models (LLMs) from human-generated text, which holds significance for various applications. With ongoing discussions about attaining a model with such functionality, we present supporting evidence regarding the feasibility of such models. We evaluated our models on multiple datasets, including Twitter Sentiment, Football Commentary, Project Gutenberg, PubMedQA, and SQuAD, confirming the efficacy of the enhanced detection approaches. These datasets were sampled with intricate constraints encompassing every possibility, laying the foundation for future research. We evaluate GPT-3.5-Turbo against various detectors such as SVM, RoBERTa-base, and RoBERTa-large. Based on the research findings, the results predominantly relied on the sequence length of the sentence",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raghav Gaggar",
      "Ashish Bhagchandani",
      "Harsh Oza"
    ]
  },
  "http://arxiv.org/abs/2311.15402": {
    "title": "Learning Section Weights for Multi-Label Document Classification",
    "volume": "Nov",
    "abstract": "Multi-label document classification is a traditional task in NLP. Compared to single-label classification, each document can be assigned multiple classes. This problem is crucially important in various domains, such as tagging scientific articles. Documents are often structured into several sections such as abstract and title. Current approaches treat different sections equally for multi-label classification. We argue that this is not a realistic assumption, leading to sub-optimal results. Instead, we propose a new method called Learning Section Weights (LSW), leveraging the contribution of each distinct section for multi-label classification. Via multiple feed-forward layers, LSW learns to assign weights to each section of, and incorporate the weights in the prediction. We demonstrate our approach on scientific articles. Experimental results on public (arXiv) and private (Elsevier) datasets confirm the superiority of LSW, compared to state-of-the-art multi-label document classification methods. In particular, LSW achieves a 1.3% improvement in terms of macro averaged F1-score while it achieves 1.3% in terms of macro averaged recall on the publicly available arXiv dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maziar Moradi Fard",
      "Paula Sorrolla Bayod",
      "Kiomars Motarjem",
      "Mohammad Alian Nejadi",
      "Saber Akhondi",
      "Camilo Thorne"
    ]
  },
  "http://arxiv.org/abs/2311.15316": {
    "title": "Enhancing Empathetic and Emotion Support Dialogue Generation with Prophetic Commonsense Inference",
    "volume": "Nov",
    "abstract": "The interest in Empathetic and Emotional Support conversations among the public has significantly increased. To offer more sensitive and understanding responses, leveraging commonsense knowledge has become a common strategy to better understand psychological aspects and causality. However, such commonsense inferences can be out of context and unable to predict upcoming dialogue themes, resulting in responses that lack coherence and empathy. To remedy this issue, we present Prophetic Commonsense Inference, an innovative paradigm for inferring commonsense knowledge. By harnessing the capabilities of Large Language Models in understanding dialogue and making commonsense deductions, we train tunable models to bridge the gap between past and potential future dialogues. Extensive experiments conducted on EmpatheticDialogues and Emotion Support Conversation show that equipping dialogue agents with our proposed prophetic commonsense inference significantly enhances the quality of their responses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lanrui Wang",
      "Jiangnan Li",
      "Chenxu Yang",
      "Zheng Lin",
      "Weiping Wang"
    ]
  },
  "http://arxiv.org/abs/2311.15296": {
    "title": "UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation",
    "volume": "Nov",
    "abstract": "Large language models (LLMs) have emerged as pivotal contributors in contemporary natural language processing and are increasingly being applied across a diverse range of industries. However, these large-scale probabilistic statistical models cannot currently ensure the requisite quality in professional content generation. These models often produce hallucinated text, compromising their practical utility in professional contexts. To assess the authentic reliability of LLMs in text generation, numerous initiatives have developed benchmark evaluations for hallucination phenomena. Nevertheless, these benchmarks frequently utilize constrained generation techniques due to cost and temporal constraints. These techniques encompass the use of directed hallucination induction and strategies that deliberately alter authentic text to produce hallucinations. These approaches are not congruent with the unrestricted text generation demanded by real-world applications. Furthermore, a well-established Chinese-language dataset dedicated to the evaluation of hallucinations in text generation is presently lacking. Consequently, we have developed an Unconstrained Hallucination Generation Evaluation (UHGEval) benchmark, designed to compile outputs produced with minimal restrictions by LLMs. Concurrently, we have established a comprehensive benchmark evaluation framework to aid subsequent researchers in undertaking scalable and reproducible experiments. We have also executed extensive experiments, evaluating prominent Chinese language models and the GPT series models to derive professional performance insights regarding hallucination challenges",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xun Liang",
      "Shichao Song",
      "Simin Niu",
      "Zhiyu Li",
      "Feiyu Xiong",
      "Bo Tang",
      "Zhaohui Wy",
      "Dawei He",
      "Peng Cheng",
      "Zhonghao Wang",
      "Haiying Deng"
    ]
  },
  "http://arxiv.org/abs/2311.15218": {
    "title": "Dataset for Stock Market Forecasting Based on Quantitative Analysis and Qualitative Data",
    "volume": "Nov",
    "abstract": "The application of Machine learning to finance has become a familiar approach, even more so in stock market forecasting. The stock market is highly volatile and huge amounts of data are generated every minute globally. The extraction of effective intelligence from this data is of critical importance. However, a collaboration of numerical stock data with qualitative text data can be a challenging task. In this work, we accomplish this and provide an unprecedented, publicly available dataset with technical and fundamental data, sentiment that we gathered from News Archives, TV news captions, Radio Transcripts, Tweets, Daily financial newspapers, etc. The text data entries used for sentiment extraction total more than 1.4 Million. The dataset comprises of daily entries from January 2018 to December 2022 for 8 different companies and Dow Jones Index as a whole. Holistic Fundamental and Technical data is provided training ready for Model learning and deployment. The predictive power of deep learning models is highly determined by the training data provided. This dataset would be of benefit for research globally incorporating qualitative intelligence for stock market forecasting. The dataset is made available at https://github.com/batking24/Huge-Stock-Dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sai Akash Bathini",
      "Dagli Cihan"
    ]
  },
  "http://arxiv.org/abs/2311.15211": {
    "title": "Probabilistic Transformer: A Probabilistic Dependency Model for Contextual Word Representation",
    "volume": "Nov",
    "abstract": "Syntactic structures used to play a vital role in natural language processing (NLP), but since the deep learning revolution, NLP has been gradually dominated by neural models that do not consider syntactic structures in their design. One vastly successful class of neural models is transformers. When used as an encoder, a transformer produces contextual representation of words in the input sentence. In this work, we propose a new model of contextual word representation, not from a neural perspective, but from a purely syntactic and probabilistic perspective. Specifically, we design a conditional random field that models discrete latent representations of all words in a sentence as well as dependency arcs between them; and we use mean field variational inference for approximate inference. Strikingly, we find that the computation graph of our model resembles transformers, with correspondences between dependencies and self-attention and between distributions over latent representations and contextual embeddings of words. Experiments show that our model performs competitively to transformers on small to medium sized datasets. We hope that our work could help bridge the gap between traditional syntactic and probabilistic approaches and cutting-edge neural approaches to NLP, and inspire more linguistically-principled neural approaches in the future",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyi Wu",
      "Kewei Tu"
    ]
  },
  "http://arxiv.org/abs/2311.15208": {
    "title": "LongStory: Coherent, Complete and Length Controlled Long story Generation",
    "volume": "Nov",
    "abstract": "A human author can write any length of story without losing coherence. Also, they always bring the story to a proper ending, an ability that current language models lack. In this work, we present the LongStory for coherent, complete, and length-controlled long story generation. LongStory introduces two novel methodologies: (1) the long and short-term contexts weight calibrator (CWC) and (2) long story structural positions (LSP). The CWC adjusts weights for long-term context Memory and short-term context Cheating, acknowledging their distinct roles. The LSP employs discourse tokens to convey the structural positions of a long story. Trained on three datasets with varied average story lengths, LongStory outperforms other baselines, including the strong story generator Plotmachine, in coherence, completeness, relevance, and repetitiveness. We also perform zero-shot tests on each dataset to assess the model's ability to predict outcomes beyond its training data and validate our methodology by comparing its performance with variants of our model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyeongman Park",
      "Nakyeong Yang",
      "Kyomin Jung"
    ]
  },
  "http://arxiv.org/abs/2311.15198": {
    "title": "ChatGPT and Beyond: The Generative AI Revolution in Education",
    "volume": "Nov",
    "abstract": "The wide adoption and usage of generative artificial intelligence (AI) models, particularly ChatGPT, has sparked a surge in research exploring their potential applications in the educational landscape. This survey examines academic literature published between November, 2022, and July, 2023, specifically targeting high-impact research from Scopus-indexed Q1 and Q2 journals. This survey delves into the practical applications and implications of generative AI models across a diverse range of educational contexts. Through a comprehensive and rigorous evaluation of recent academic literature, this survey seeks to illuminate the evolving role of generative AI models, particularly ChatGPT, in education. By shedding light on the potential benefits, challenges, and emerging trends in this dynamic field, the survey endeavors to contribute to the understanding of the nexus between artificial intelligence and education. The findings of this review will empower educators, researchers, and policymakers to make informed decisions about the integration of AI technologies into learning environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad AL-Smadi"
    ]
  },
  "http://arxiv.org/abs/2311.15180": {
    "title": "Benchmarking Large Language Model Volatility",
    "volume": "Nov",
    "abstract": "The impact of non-deterministic outputs from Large Language Models (LLMs) is not well examined for financial text understanding tasks. Through a compelling case study on investing in the US equity market via news sentiment analysis, we uncover substantial variability in sentence-level sentiment classification results, underscoring the innate volatility of LLM outputs. These uncertainties cascade downstream, leading to more significant variations in portfolio construction and return. While tweaking the temperature parameter in the language model decoder presents a potential remedy, it comes at the expense of stifled creativity. Similarly, while ensembling multiple outputs mitigates the effect of volatile outputs, it demands a notable computational investment. This work furnishes practitioners with invaluable insights for adeptly navigating uncertainty in the integration of LLMs into financial decision-making, particularly in scenarios dictated by non-deterministic information",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyang Yu"
    ]
  },
  "http://arxiv.org/abs/2311.15131": {
    "title": "Localizing Lying in Llama: Understanding Instructed Dishonesty on True-False Questions Through Prompting, Probing, and Patching",
    "volume": "Nov",
    "abstract": "Large language models (LLMs) demonstrate significant knowledge through their outputs, though it is often unclear whether false outputs are due to a lack of knowledge or dishonesty. In this paper, we investigate instructed dishonesty, wherein we explicitly prompt LLaMA-2-70b-chat to lie. We perform prompt engineering to find which prompts best induce lying behavior, and then use mechanistic interpretability approaches to localize where in the network this behavior occurs. Using linear probing and activation patching, we localize five layers that appear especially important for lying. We then find just 46 attention heads within these layers that enable us to causally intervene such that the lying model instead answers honestly. We show that these interventions work robustly across many prompts and dataset splits. Overall, our work contributes a greater understanding of dishonesty in LLMs so that we may hope to prevent it",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Campbell",
      "Richard Ren",
      "Phillip Guo"
    ]
  },
  "http://arxiv.org/abs/2311.15110": {
    "title": "Relevance feedback strategies for recall-oriented neural information retrieval",
    "volume": "Nov",
    "abstract": "In a number of information retrieval applications (e.g., patent search, literature review, due diligence, etc.), preventing false negatives is more important than preventing false positives. However, approaches designed to reduce review effort (like \"technology assisted review\") can create false negatives, since they are often based on active learning systems that exclude documents automatically based on user feedback. Therefore, this research proposes a more recall-oriented approach to reducing review effort. More specifically, through iteratively re-ranking the relevance rankings based on user feedback, which is also referred to as relevance feedback. In our proposed method, the relevance rankings are produced by a BERT-based dense-vector search and the relevance feedback is based on cumulatively summing the queried and selected embeddings. Our results show that this method can reduce review effort between 17.85% and 59.04%, compared to a baseline approach (of no feedback), given a fixed recall target",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Timo Kats",
      "Peter van der Putten",
      "Jan Scholtes"
    ]
  },
  "http://arxiv.org/abs/2311.15106": {
    "title": "Solving the Right Problem is Key for Translational NLP: A Case Study in UMLS Vocabulary Insertion",
    "volume": "Nov",
    "abstract": "As the immense opportunities enabled by large language models become more apparent, NLP systems will be increasingly expected to excel in real-world settings. However, in many instances, powerful models alone will not yield translational NLP solutions, especially if the formulated problem is not well aligned with the real-world task. In this work, we study the case of UMLS vocabulary insertion, an important real-world task in which hundreds of thousands of new terms, referred to as atoms, are added to the UMLS, one of the most comprehensive open-source biomedical knowledge bases. Previous work aimed to develop an automated NLP system to make this time-consuming, costly, and error-prone task more efficient. Nevertheless, practical progress in this direction has been difficult to achieve due to a problem formulation and evaluation gap between research output and the real-world task. In order to address this gap, we introduce a new formulation for UMLS vocabulary insertion which mirrors the real-world task, datasets which faithfully represent it and several strong baselines we developed through re-purposing existing solutions. Additionally, we propose an effective rule-enhanced biomedical language model which enables important new model behavior, outperforms all strong baselines and provides measurable qualitative improvements to editors who carry out the UVI task. We hope this case study provides insight into the considerable importance of problem formulation for the success of translational NLP solutions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bernal Jimenez Gutierrez",
      "Yuqing Mao",
      "Vinh Nguyen",
      "Kin Wah Fung",
      "Yu Su",
      "Olivier Bodenreider"
    ]
  },
  "http://arxiv.org/abs/2311.16185": {
    "title": "Enhancing Sentiment Analysis Results through Outlier Detection Optimization",
    "volume": "Nov",
    "abstract": "When dealing with text data containing subjective labels like speaker emotions, inaccuracies or discrepancies among labelers are not uncommon. Such discrepancies can significantly affect the performance of machine learning algorithms. This study investigates the potential of identifying and addressing outliers in text data with subjective labels, aiming to enhance classification outcomes. We utilized the Deep SVDD algorithm, a one-class classification method, to detect outliers in nine text-based emotion and sentiment analysis datasets. By employing both a small-sized language model (DistilBERT base model with 66 million parameters) and non-deep learning machine learning algorithms (decision tree, KNN, Logistic Regression, and LDA) as the classifier, our findings suggest that the removal of outliers can lead to enhanced results in most cases. Additionally, as outliers in such datasets are not necessarily unlearnable, we experienced utilizing a large language model -- DeBERTa v3 large with 131 million parameters, which can capture very complex patterns in data. We continued to observe performance enhancements across multiple datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuetian Chen",
      "Mei Si"
    ]
  },
  "http://arxiv.org/abs/2311.15077": {
    "title": "Multilingual self-supervised speech representations improve the speech recognition of low-resource African languages with codeswitching",
    "volume": "Nov",
    "abstract": "While many speakers of low-resource languages regularly code-switch between their languages and other regional languages or English, datasets of codeswitched speech are too small to train bespoke acoustic models from scratch or do language model rescoring. Here we propose finetuning self-supervised speech representations such as wav2vec 2.0 XLSR to recognize code-switched data. We find that finetuning self-supervised multilingual representations and augmenting them with n-gram language models trained from transcripts reduces absolute word error rates by up to 20% compared to baselines of hybrid models trained from scratch on code-switched data. Our findings suggest that in circumstances with limited training data finetuning self-supervised representations is a better performing and viable solution",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tollop gnrm",
      "Christopher D. Manning",
      "Dan Jurafsky"
    ]
  },
  "http://arxiv.org/abs/2311.15055": {
    "title": "Automatically Finding and Categorizing Replication Studies",
    "volume": "Nov",
    "abstract": "In many fields of experimental science, papers that failed to replicate continue to be cited as a result of the poor discoverability of replication studies. As a first step to creating a system that automatically finds replication studies for a given paper, 334 replication studies and 344 replicated studies were collected. Replication studies could be identified in the dataset based on text content at a higher rate than chance (AUROC = 0.886). Additionally, successful replication studies could be distinguished from failed replication studies at a higher rate than chance (AUROC = 0.664)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bob de Ruiter"
    ]
  },
  "http://arxiv.org/abs/2311.15054": {
    "title": "Detection of developmental language disorder in Cypriot Greek children using a machine learning neural network algorithm",
    "volume": "Nov",
    "abstract": "Children with developmental language disorder (DLD) encounter difficulties in acquiring various language structures. Early identification and intervention are crucial to prevent negative long-term outcomes impacting the academic, social, and emotional development of children. The study aims to develop an automated method for the identification of DLD using artificial intelligence, specifically a neural network machine learning algorithm. This protocol is applied for the first time in Cypriot Greek children, which is generally considered underresearched in the context of DLD. The neural network model was trained using perceptual and production data elicited from children with DLD and healthy controls. The k-fold technique was used to crossvalidate the algorithm. The performance of the model was evaluated using metrics such as accuracy, precision, recall, F1 score, and ROC/AUC curve to assess its ability to make accurate predictions on a set of unseen data. The results demonstrated high classification values for all metrics (between 0.92 and 0.98), indicating the high accuracy of the neural model in classifying children with DLD. Additionally, the variable importance analysis revealed that the language production skills of children had a more significant impact on the performance of the model compared to perception skills. Neural networks represent powerful tools for detecting DLD, providing early and quick assessments of the disorder, and having the potential to improve clinical outcomes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgios P. Georgiou",
      "Elena Theodorou"
    ]
  },
  "http://arxiv.org/abs/2311.15032": {
    "title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla Sentiment Analysis",
    "volume": "Nov",
    "abstract": "In this paper, we discuss the nlpBDpatriots entry to the shared task on Sentiment Analysis of Bangla Social Media Posts organized at the first workshop on Bangla Language Processing (BLP) co-located with EMNLP. The main objective of this task is to identify the polarity of social media content using a Bangla dataset annotated with positive, neutral, and negative labels provided by the shared task organizers. Our best system for this task is a transfer learning approach with data augmentation which achieved a micro F1 score of 0.71. Our best system ranked 12th among 30 teams that participated in the competition",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dhiman Goswami",
      "Md Nishat Raihan",
      "Sadiya Sayara Chowdhury Puspo",
      "Marcos Zampieri"
    ]
  },
  "http://arxiv.org/abs/2311.15029": {
    "title": "nlpBDpatriots at BLP-2023 Task 1: A Two-Step Classification for Violence Inciting Text Detection in Bangla",
    "volume": "Nov",
    "abstract": "In this paper, we discuss the nlpBDpatriots entry to the shared task on Violence Inciting Text Detection (VITD) organized as part of the first workshop on Bangla Language Processing (BLP) co-located with EMNLP. The aim of this task is to identify and classify the violent threats, that provoke further unlawful violent acts. Our best-performing approach for the task is two-step classification using back translation and multilinguality which ranked 6th out of 27 teams with a macro F1 score of 0.74",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Nishat Raihan",
      "Dhiman Goswami",
      "Sadiya Sayara Chowdhury Puspo",
      "Marcos Zampieri"
    ]
  },
  "http://arxiv.org/abs/2311.15023": {
    "title": "Offensive Language Identification in Transliterated and Code-Mixed Bangla",
    "volume": "Nov",
    "abstract": "Identifying offensive content in social media is vital for creating safe online communities. Several recent studies have addressed this problem by creating datasets for various languages. In this paper, we explore offensive language identification in texts with transliterations and code-mixing, linguistic phenomena common in multilingual societies, and a known challenge for NLP systems. We introduce TB-OLID, a transliterated Bangla offensive language dataset containing 5,000 manually annotated comments. We train and fine-tune machine learning models on TB-OLID, and we evaluate their results on this dataset. Our results show that English pre-trained transformer-based models, such as fBERT and HateBERT achieve the best performance on this dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Nishat Raihan",
      "Umma Hani Tanmoy",
      "Anika Binte Islam",
      "Kai North",
      "Tharindu Ranasinghe",
      "Antonios Anastasopoulos",
      "Marcos Zampieri"
    ]
  },
  "http://arxiv.org/abs/2311.15016": {
    "title": "E-CORE: Emotion Correlation Enhanced Empathetic Dialogue Generation",
    "volume": "Nov",
    "abstract": "Achieving empathy is a crucial step toward humanized dialogue systems. Current approaches for empathetic dialogue generation mainly perceive an emotional label to generate an empathetic response conditioned on it, which simply treat emotions independently, but ignore the intrinsic emotion correlation in dialogues, resulting in inaccurate emotion perception and unsuitable response generation. In this paper, we propose a novel emotion correlation enhanced empathetic dialogue generation framework, which comprehensively realizes emotion correlation learning, utilization, and supervising. Specifically, a multi-resolution emotion graph is devised to capture context-based emotion interactions from different resolutions, further modeling emotion correlation. Then we propose an emotion correlation enhanced decoder, with a novel correlation-aware aggregation and soft/hard strategy, respectively improving the emotion perception and response generation. Experimental results on the benchmark dataset demonstrate the superiority of our model in both empathetic perception and expression",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengyi Fu",
      "Lei Zhang",
      "Quan Wang",
      "Zhendong Mao"
    ]
  },
  "http://arxiv.org/abs/2311.14966": {
    "title": "Walking a Tightrope -- Evaluating Large Language Models in High-Risk Domains",
    "volume": "Nov",
    "abstract": "High-risk domains pose unique challenges that require language models to provide accurate and safe responses. Despite the great success of large language models (LLMs), such as ChatGPT and its variants, their performance in high-risk domains remains unclear. Our study delves into an in-depth analysis of the performance of instruction-tuned LLMs, focusing on factual accuracy and safety adherence. To comprehensively assess the capabilities of LLMs, we conduct experiments on six NLP datasets including question answering and summarization tasks within two high-risk domains: legal and medical. Further qualitative analysis highlights the existing limitations inherent in current LLMs when evaluating in high-risk domains. This underscores the essential nature of not only improving LLM capabilities but also prioritizing the refinement of domain-specific metrics, and embracing a more human-centric approach to enhance safety and factual reliability. Our findings advance the field toward the concerns of properly evaluating LLMs in high-risk domains, aiming to steer the adaptability of LLMs in fulfilling societal obligations and aligning with forthcoming regulations, such as the EU AI Act",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chia-Chien Hung",
      "Wiem Ben Rim",
      "Lindsay Frost",
      "Lars Bruckner",
      "Carolin Lawrence"
    ]
  },
  "http://arxiv.org/abs/2311.14949": {
    "title": "Vector-Quantized Prompt Learning for Paraphrase Generation",
    "volume": "Nov",
    "abstract": "Deep generative modeling of natural languages has achieved many successes, such as producing fluent sentences and translating from one language into another. However, the development of generative modeling techniques for paraphrase generation still lags behind largely due to the challenges in addressing the complex conflicts between expression diversity and semantic preservation. This paper proposes to generate diverse and high-quality paraphrases by exploiting the pre-trained models with instance-dependent prompts. To learn generalizable prompts, we assume that the number of abstract transforming patterns of paraphrase generation (governed by prompts) is finite and usually not large. Therefore, we present vector-quantized prompts as the cues to control the generation of pre-trained models. Extensive experiments demonstrate that the proposed method achieves new state-of-art results on three benchmark datasets, including Quora, Wikianswers, and MSCOCO. We will release all the code upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Luo",
      "Yixin Liu",
      "Peidong Liu",
      "Xianggen Liu"
    ]
  },
  "http://arxiv.org/abs/2311.14919": {
    "title": "Faster Minimum Bayes Risk Decoding with Confidence-based Pruning",
    "volume": "Nov",
    "abstract": "Minimum Bayes risk (MBR) decoding outputs the hypothesis with the highest expected utility over the model distribution for some utility function. It has been shown to improve accuracy over beam search in conditional language generation problems and especially neural machine translation, in both human and automatic evaluations. However, the standard sampling-based algorithm for MBR is substantially more computationally expensive than beam search, requiring a large number of samples as well as a quadratic number of calls to the utility function, limiting its applicability. We describe an algorithm for MBR which gradually grows the number of samples used to estimate the utility while pruning hypotheses that are unlikely to have the highest utility according to confidence estimates obtained with bootstrap sampling. Our method requires fewer samples and drastically reduces the number of calls to the utility function compared to standard MBR while being statistically indistinguishable in terms of accuracy. We demonstrate the effectiveness of our approach in experiments on three language pairs, using chrF++ and COMET as utility/evaluation metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julius Cheng",
      "Andreas Vlachos"
    ]
  },
  "http://arxiv.org/abs/2311.14901": {
    "title": "Code Search Debiasing:Improve Search Results beyond Overall Ranking Performance",
    "volume": "Nov",
    "abstract": "Code search engine is an essential tool in software development. Many code search methods have sprung up, focusing on the overall ranking performance of code search. In this paper, we study code search from another perspective by analyzing the bias of code search models. Biased code search engines provide poor user experience, even though they show promising overall performance. Due to different development conventions (e.g., prefer long queries or abbreviations), some programmers will find the engine useful, while others may find it hard to get desirable search results. To mitigate biases, we develop a general debiasing framework that employs reranking to calibrate search results. It can be easily plugged into existing engines and handle new code search biases discovered in the future. Experiments show that our framework can effectively reduce biases. Meanwhile, the overall ranking performance of code search gets improved after debiasing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Zhang",
      "Hui Li",
      "Yanlin Wang",
      "Zhao Wei",
      "Yong Xiu",
      "Juhong Wang",
      "Rongong Ji"
    ]
  },
  "http://arxiv.org/abs/2311.14871": {
    "title": "Tracing Influence at Scale: A Contrastive Learning Approach to Linking Public Comments and Regulator Responses",
    "volume": "Nov",
    "abstract": "U.S. Federal Regulators receive over one million comment letters each year from businesses, interest groups, and members of the public, all advocating for changes to proposed regulations. These comments are believed to have wide-ranging impacts on public policy. However, measuring the impact of specific comments is challenging because regulators are required to respond to comments but they do not have to specify which comments they are addressing. In this paper, we propose a simple yet effective solution to this problem by using an iterative contrastive method to train a neural model aiming for matching text from public comments to responses written by regulators. We demonstrate that our proposal substantially outperforms a set of selected text-matching baselines on a human-annotated test set. Furthermore, it delivers performance comparable to the most advanced gigantic language model (i.e., GPT-4), and is more cost-effective when handling comments and regulator responses matching in larger scale",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linzi Xing",
      "Brad Hackinen",
      "Giuseppe Carenini"
    ]
  },
  "http://arxiv.org/abs/2311.14865": {
    "title": "Improving Cross-Domain Hate Speech Generalizability with Emotion Knowledge",
    "volume": "Nov",
    "abstract": "Reliable automatic hate speech (HS) detection systems must adapt to the in-flow of diverse new data to curtail hate speech. However, hate speech detection systems commonly lack generalizability in identifying hate speech dissimilar to data used in training, impeding their robustness in real-world deployments. In this work, we propose a hate speech generalization framework that leverages emotion knowledge in a multitask architecture to improve the generalizability of hate speech detection in a cross-domain setting. We investigate emotion corpora with varying emotion categorical scopes to determine the best corpus scope for supplying emotion knowledge to foster generalized hate speech detection. We further assess the relationship between using pretrained Transformers models adapted for hate speech and its effect on our emotion-enriched hate speech generalization model. We perform extensive experiments on six publicly available datasets sourced from different online domains and show that our emotion-enriched HS detection generalization method demonstrates consistent generalization improvement in cross-domain evaluation, increasing generalization performance up to 18.1% and average cross-domain performance up to 8.5%, according to the F1 measure",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shi Yin Hong",
      "Susan Gauch"
    ]
  },
  "http://arxiv.org/abs/2311.14838": {
    "title": "OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models",
    "volume": "Nov",
    "abstract": "Developing high quality machine translation systems is a labour intensive, challenging and confusing process for newcomers to the field. We present a pair of tools OpusCleaner and OpusTrainer that aim to simplify the process, reduce the amount of work and lower the entry barrier for newcomers. OpusCleaner is a data downloading, cleaning, and proprocessing toolkit. It is designed to allow researchers to quickly download, visualise and preprocess bilingual (or monolingual) data that comes from many different sources, each of them with different quality, issues, and unique filtering/preprocessing requirements. OpusTrainer is a data scheduling and data augmenting tool aimed at building large scale, robust machine translation systems and large language models. It features deterministic data mixing from many different sources, on-the-fly data augmentation and more. Using these tools, we showcase how we can use it to create high quality machine translation model robust to noisy user input; multilingual models and terminology aware models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikolay Bogoychev",
      "Jelmer van der Linde",
      "Graeme Nail",
      "Barry Haddow",
      "Jaume Zaragoza-Bernabeu",
      "Gema Ramrez-Snchez",
      "Lukas Weymann",
      "Tudor Nicolae Mateiu",
      "Jindich Helcl",
      "Mikko Aulamo"
    ]
  },
  "http://arxiv.org/abs/2311.14836": {
    "title": "Custom Data Augmentation for low resource ASR using Bark and Retrieval-Based Voice Conversion",
    "volume": "Nov",
    "abstract": "This paper proposes two innovative methodologies to construct customized Common Voice datasets for low-resource languages like Hindi. The first methodology leverages Bark, a transformer-based text-to-audio model developed by Suno, and incorporates Meta's enCodec and a pre-trained HuBert model to enhance Bark's performance. The second methodology employs Retrieval-Based Voice Conversion (RVC) and uses the Ozen toolkit for data preparation. Both methodologies contribute to the advancement of ASR technology and offer valuable insights into addressing the challenges of constructing customized Common Voice datasets for under-resourced languages. Furthermore, they provide a pathway to achieving high-quality, personalized voice generation for a range of applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anand Kamble",
      "Aniket Tathe",
      "Suyash Kumbharkar",
      "Atharva Bhandare",
      "Anirban C. Mitra"
    ]
  },
  "http://arxiv.org/abs/2311.14835": {
    "title": "Weak Alignment Supervision from Hybrid Model Improves End-to-end ASR",
    "volume": "Nov",
    "abstract": "In this paper, we aim to create weak alignment supervision to aid the end-to-end modeling. Towards this end, we use the existing hybrid ASR system to produce triphone alignments of the training audios. We then create a cross-entropy loss at a certain layer of the encoder using the derived alignments. In contrast to the general one-hot cross-entropy losses with or without loss weighting, here we use a cross-entropy loss with a label smoothing parameter to regularize the supervision. As a comparison, we also conduct the experiments with one-hot cross-entropy losses and CTC losses with loss weighting. The results show that placing the weak alignment supervision with the label smoothing parameter of 0.5 at the third encoder layer outperforms the other two approaches and leads to about 5% relative WER reduction on the TED-LIUM 2 dataset over the baseline. We see similar improvements when applying the method out-of-the-box on a Tagalog end-to-end ASR system",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jintao Jiang",
      "Yingbo Gao",
      "Zoltan Tuske"
    ]
  },
  "http://arxiv.org/abs/2311.14808": {
    "title": "Data-to-Text Bilingual Generation",
    "volume": "Nov",
    "abstract": "This document illustrates the use of pyrealb for generating two parallel texts (English and French) from a single source of data. The data selection and text organisation processes are shared between the two languages. only language dependent word and phrasing choices are distinct processes. The realized texts thus convey identical information in both languages without the risk of being lost in translation. This is especially important in cases where strict and simultaneous bilingualism is required. We first present the types of applications targeted by this approach and how the pyrealb English and French realizer can be used for achieving this goal in a natural way. We describe an object-oriented organization to ensure a convenient realization in both languages. To illustrate the process, different types of applications are then briefly sketched with links to the source code. A brief comparison of the text generation is given with the output of an instance of a GPT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guy Lapalme"
    ]
  },
  "http://arxiv.org/abs/2311.14788": {
    "title": "Evaluating Large Language Models through Gender and Racial Stereotypes",
    "volume": "Nov",
    "abstract": "Language Models have ushered a new age of AI gaining traction within the NLP community as well as amongst the general population. AI's ability to make predictions, generations and its applications in sensitive decision-making scenarios, makes it even more important to study these models for possible biases that may exist and that can be exaggerated. We conduct a quality comparative study and establish a framework to evaluate language models under the premise of two kinds of biases: gender and race, in a professional setting. We find out that while gender bias has reduced immensely in newer models, as compared to older ones, racial bias still exists",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ananya Malik"
    ]
  },
  "http://arxiv.org/abs/2311.14652": {
    "title": "One Pass Streaming Algorithm for Super Long Token Attention Approximation in Sublinear Space",
    "volume": "Nov",
    "abstract": "Deploying Large Language Models (LLMs) in streaming applications that involve long contexts, particularly for extended dialogues and text analysis, is of paramount importance but presents two significant challenges. Firstly, the memory consumption is substantial during the decoding phase due to the caching of Key and Value states (KV) of previous tokens. Secondly, attention computation is time-consuming with a time complexity of $O(n^2)$ for the generation of each token. In recent OpenAI DevDay (Nov 6, 2023), OpenAI released a new model that is able to support a 128K-long document, in our paper, we focus on the memory-efficient issue when context length $n$ is much greater than 128K ($n \\gg 2^d$). Considering a single-layer self-attention with Query, Key, and Value matrices $Q, K, V \\in \\mathbb{R}^{n \\times d}$, the polynomial method approximates the attention output $T \\in \\mathbb{R}^{n \\times d}$. It accomplishes this by constructing $U_1, U_2 \\in \\mathbb{R}^{n \\times t}$ to expedite attention ${\\sf Attn}(Q, K, V)$ computation within $n^{1+o(1)}$ time executions. Despite this, storing the Key and Value matrices $K, V \\in \\mathbb{R}^{n \\times d}$ still necessitates $O( n d)$ space, leading to significant memory usage. In response to these challenges, we introduce a new algorithm that only reads one pass of the data in streaming fashion. This method employs sublinear space $o(n)$ to store three sketch matrices, alleviating the need for exact $K, V$ storage. Notably, our algorithm exhibits exceptional memory-efficient performance with super-long tokens. As the token length $n$ increases, our error guarantee diminishes while the memory usage remains nearly constant. This unique attribute underscores the potential of our technique in efficiently handling LLMs in streaming applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raghav Addanki",
      "Chenyang Li",
      "Zhao Song",
      "Chiwun Yang"
    ]
  },
  "http://arxiv.org/abs/2311.14648": {
    "title": "Calibrated Language Models Must Hallucinate",
    "volume": "Nov",
    "abstract": "Recent language models have a mysterious tendency to generate false but plausible-sounding text. Such \"hallucinations\" are an obstacle to the usability of language-based AI systems and can harm people who rely upon their outputs. This work shows shows that there is an inherent statistical reason that pretrained language models hallucinate certain types of facts, having nothing to do with the transformer LM architecture or data quality. For \"arbitrary\" facts whose veracity cannot be determined from the training data, we show that hallucination is necessary for language models that satisfy a statistical calibration condition appropriate for generative language models. Specifically, if the maximum probability of any fact is bounded, we show that the probability of generating a hallucination is close to the fraction of facts that occur exactly once in the training data (a \"Good-Turing\" estimate), even assuming ideal training data without errors. One conclusion is that models pretrained to be sufficiently good predictors (i.e., calibrated) may require post-training to mitigate hallucinations on the type of arbitrary facts that tend to appear once in the training set. However, our analysis also suggests that there is no statistical reason that pretraining will lead to hallucination on facts that tend to appear more than once in the training data (like references to publications such as articles and books, whose hallucinations have been particularly notable and problematic) or on systematic facts (like arithmetic calculations). Therefore, different architectures and learning algorithms may mitigate these latter types of hallucinations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Tauman Kalai",
      "Santosh S. Vempala"
    ]
  },
  "http://arxiv.org/abs/2311.14583": {
    "title": "GPT Struct Me: Probing GPT Models on Narrative Entity Extraction",
    "volume": "Nov",
    "abstract": "The importance of systems that can extract structured information from textual data becomes increasingly pronounced given the ever-increasing volume of text produced on a daily basis. Having a system that can effectively extract such information in an interoperable manner would be an asset for several domains, be it finance, health, or legal. Recent developments in natural language processing led to the production of powerful language models that can, to some degree, mimic human intelligence. Such effectiveness raises a pertinent question: Can these models be leveraged for the extraction of structured information? In this work, we address this question by evaluating the capabilities of two state-of-the-art language models -- GPT-3 and GPT-3.5, commonly known as ChatGPT -- in the extraction of narrative entities, namely events, participants, and temporal expressions. This study is conducted on the Text2Story Lusa dataset, a collection of 119 Portuguese news articles whose annotation framework includes a set of entity structures along with several tags and attribute values. We first select the best prompt template through an ablation study over prompt components that provide varying degrees of information on a subset of documents of the dataset. Subsequently, we use the best templates to evaluate the effectiveness of the models on the remaining documents. The results obtained indicate that GPT models are competitive with out-of-the-box baseline systems, presenting an all-in-one alternative for practitioners with limited resources. By studying the strengths and limitations of these models in the context of information extraction, we offer insights that can guide future improvements and avenues to explore in this field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hugo Sousa",
      "Nuno Guimares",
      "Alpio Jorge",
      "Ricardo Campos"
    ]
  },
  "http://arxiv.org/abs/2311.14543": {
    "title": "Data-Efficient Alignment of Large Language Models with Human Feedback Through Natural Language",
    "volume": "Nov",
    "abstract": "Learning from human feedback is a prominent technique to align the output of large language models (LLMs) with human expectations. Reinforcement learning from human feedback (RLHF) leverages human preference signals that are in the form of ranking of response pairs to perform this alignment. However, human preference on LLM outputs can come in much richer forms including natural language, which may provide detailed feedback on strengths and weaknesses of a given response. In this work we investigate data efficiency of modeling human feedback that is in natural language. Specifically, we fine-tune an open-source LLM, e.g., Falcon-40B-Instruct, on a relatively small amount (1000 records or even less) of human feedback in natural language in the form of critiques and revisions of responses. We show that this model is able to improve the quality of responses from even some of the strongest LLMs such as ChatGPT, BARD, and Vicuna, through critique and revision of those responses. For instance, through one iteration of revision of ChatGPT responses, the revised responses have 56.6% win rate over the original ones, and this win rate can be further improved to 65.9% after applying the revision for five iterations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Jin",
      "Shikib Mehri",
      "Devamanyu Hazarika",
      "Aishwarya Padmakumar",
      "Sungjin Lee",
      "Yang Liu",
      "Mahdi Namazifar"
    ]
  },
  "http://arxiv.org/abs/2311.14539": {
    "title": "CMed-GPT: Prompt Tuning for Entity-Aware Chinese Medical Dialogue Generation",
    "volume": "Nov",
    "abstract": "Medical dialogue generation relies on natural language generation techniques to enable online medical consultations. Recently, the widespread adoption of large-scale models in the field of natural language processing has facilitated rapid advancements in this technology. Existing medical dialogue models are mostly based on BERT and pre-trained on English corpora, but there is a lack of high-performing models on the task of Chinese medical dialogue generation. To solve the above problem, this paper proposes CMed-GPT, which is the GPT pre-training language model based on Chinese medical domain text. The model is available in two versions, namely, base and large, with corresponding perplexity values of 8.64 and 8.01. Additionally, we incorporate lexical and entity embeddings into the dialogue text in a uniform manner to meet the requirements of downstream dialogue generation tasks. By applying both fine-tuning and p-tuning to CMed-GPT, we lowered the PPL from 8.44 to 7.35. This study not only confirms the exceptional performance of the CMed-GPT model in generating Chinese biomedical text but also highlights the advantages of p-tuning over traditional fine-tuning with prefix prompts. Furthermore, we validate the significance of incorporating external information in medical dialogue generation, which enhances the quality of dialogue generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijie Qu",
      "Juan Li",
      "Zerui Ma",
      "Jianqiang Li"
    ]
  },
  "http://arxiv.org/abs/2311.14530": {
    "title": "Machine Translation for Ge'ez Language",
    "volume": "Nov",
    "abstract": "Machine translation (MT) for low-resource languages such as Ge'ez, an ancient language that is no longer spoken in daily life, faces challenges such as out-of-vocabulary words, domain mismatches, and lack of sufficient labeled training data. In this work, we explore various methods to improve Ge'ez MT, including transfer-learning from related languages, optimizing shared vocabulary and token segmentation approaches, finetuning large pre-trained models, and using large language models (LLMs) for few-shot translation with fuzzy matches. We develop a multilingual neural machine translation (MNMT) model based on languages relatedness, which brings an average performance improvement of about 4 BLEU compared to standard bilingual models. We also attempt to finetune the NLLB-200 model, one of the most advanced translation models available today, but find that it performs poorly with only 4k training samples for Ge'ez. Furthermore, we experiment with using GPT-3.5, a state-of-the-art LLM, for few-shot translation with fuzzy matches, which leverages embedding similarity-based retrieval to find context examples from a parallel corpus. We observe that GPT-3.5 achieves a remarkable BLEU score of 9.2 with no initial knowledge of Ge'ez, but still lower than the MNMT baseline of 15.2. Our work provides insights into the potential and limitations of different approaches for low-resource and ancient language MT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aman Kassahun Wassie"
    ]
  },
  "http://arxiv.org/abs/2311.14517": {
    "title": "tinyCLAP: Distilling Constrastive Language-Audio Pretrained Models",
    "volume": "Nov",
    "abstract": "Contrastive Language-Audio Pretraining (CLAP) became of crucial importance in the field of audio and speech processing. Its employment ranges from sound event detection to text-to-audio generation. However, one of the main limitations is the considerable amount of data required in the training process and the overall computational complexity during inference. This paper investigates how we can reduce the complexity of contrastive language-audio pre-trained models, yielding an efficient model that we call tinyCLAP. We derive an unimodal distillation loss from first principles and explore how the dimensionality of the shared, multimodal latent space can be reduced via pruning. TinyCLAP uses only 6% of the original Microsoft CLAP parameters with a minimal reduction (less than 5%) in zero-shot classification performance across the three sound event detection datasets on which it was tested",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Paissan",
      "Elisabetta Farella"
    ]
  },
  "http://arxiv.org/abs/2311.14505": {
    "title": "Analysing the Impact of Removing Infrequent Words on Topic Quality in LDA Models",
    "volume": "Nov",
    "abstract": "An initial procedure in text-as-data applications is text preprocessing. One of the typical steps, which can substantially facilitate computations, consists in removing infrequent words believed to provide limited information about the corpus. Despite popularity of vocabulary pruning, not many guidelines on how to implement it are available in the literature. The aim of the paper is to fill this gap by examining the effects of removing infrequent words for the quality of topics estimated using Latent Dirichlet Allocation. The analysis is based on Monte Carlo experiments taking into account different criteria for infrequent terms removal and various evaluation metrics. The results indicate that pruning is beneficial and that the share of vocabulary which might be eliminated can be quite considerable",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Victor Bystrov",
      "Viktoriia Naboka-Krell",
      "Anna Staszewska-Bystrova",
      "Peter Winker"
    ]
  },
  "http://arxiv.org/abs/2311.14495": {
    "title": "StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization",
    "volume": "Nov",
    "abstract": "In this paper, we investigate the long-term memory learning capabilities of state-space models (SSMs) from the perspective of parameterization. We prove that state-space models without any reparameterization exhibit a memory limitation similar to that of traditional RNNs: the target relationships that can be stably approximated by state-space models must have an exponential decaying memory. Our analysis identifies this \"curse of memory\" as a result of the recurrent weights converging to a stability boundary, suggesting that a reparameterization technique can be effective. To this end, we introduce a class of reparameterization techniques for SSMs that effectively lift its memory limitations. Besides improving approximation capabilities, we further illustrate that a principled choice of reparameterization scheme can also enhance optimization stability. We validate our findings using synthetic datasets and language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shida Wang",
      "Qianxiao Li"
    ]
  },
  "http://arxiv.org/abs/2311.14483": {
    "title": "SER_AMPEL: A multi-source dataset for SER of Italian older adults",
    "volume": "Nov",
    "abstract": "In this paper, SER_AMPEL, a multi-source dataset for speech emotion recognition (SER) is presented. The peculiarity of the dataset is that it is collected with the aim of providing a reference for speech emotion recognition in case of Italian older adults. The dataset is collected following different protocols, in particular considering acted conversations, extracted from movies and TV series, and recording natural conversations where the emotions are elicited by proper questions. The evidence of the need for such a dataset emerges from the analysis of the state of the art. Preliminary considerations on the critical issues of SER are reported analyzing the classification results on a subset of the proposed dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessandra Grossi",
      "Francesca Gasparini"
    ]
  },
  "http://arxiv.org/abs/2311.14479": {
    "title": "Controlled Text Generation via Language Model Arithmetic",
    "volume": "Nov",
    "abstract": "As Large Language Models (LLMs) are deployed more widely, customization with respect to vocabulary, style and character becomes more important. In this work we introduce model arithmetic, a novel inference framework for composing and biasing LLMs without the need for model (re)training or highly specific datasets. In addition, the framework allows for more precise control of generated text than direct prompting and prior controlled text generation (CTG) techniques. Using model arithmetic, we can express prior CTG techniques as simple formulas and naturally extend them to new and more effective formulations. Further, we show that speculative sampling, a technique for efficient LLM sampling, extends to our setting. This enables highly efficient text generation with multiple composed models with only marginal overhead over a single model. Our empirical evaluation demonstrates that model arithmetic allows fine-grained control of generated text while outperforming state-of-the-art on the task of toxicity reduction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jasper Dekoninck",
      "Marc Fischer",
      "Luca Beurer-Kellner",
      "Martin Vechev"
    ]
  },
  "http://arxiv.org/abs/2311.14465": {
    "title": "DP-NMT: Scalable Differentially-Private Machine Translation",
    "volume": "Nov",
    "abstract": "Neural machine translation (NMT) is a widely popular text generation task, yet there is a considerable research gap in the development of privacy-preserving NMT models, despite significant data privacy concerns for NMT systems. Differentially private stochastic gradient descent (DP-SGD) is a popular method for training machine learning models with concrete privacy guarantees; however, the implementation specifics of training a model with DP-SGD are not always clarified in existing models, with differing software libraries used and code bases not always being public, leading to reproducibility issues. To tackle this, we introduce DP-NMT, an open-source framework for carrying out research on privacy-preserving NMT with DP-SGD, bringing together numerous models, datasets, and evaluation metrics in one systematic software package. Our goal is to provide a platform for researchers to advance the development of privacy-preserving NMT systems, keeping the specific details of the DP-SGD algorithm transparent and intuitive to implement. We run a set of experiments on datasets from both general and privacy-related domains to demonstrate our framework in use. We make our framework publicly available and welcome feedback from the community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Timour Igamberdiev",
      "Doan Nam Long Vu",
      "Felix Knnecke",
      "Zhuo Yu",
      "Jannik Holmer",
      "Ivan Habernal"
    ]
  },
  "http://arxiv.org/abs/2311.14455": {
    "title": "Universal Jailbreak Backdoors from Poisoned Human Feedback",
    "volume": "Nov",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF training data to embed a \"jailbreak backdoor\" into the model. The backdoor embeds a trigger word into the model that acts like a universal \"sudo command\": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak backdoors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Javier Rando",
      "Florian Tramr"
    ]
  },
  "http://arxiv.org/abs/2311.14391": {
    "title": "FAL CorPipe at CRAC 2023: Larger Context Improves Multilingual Coreference Resolution",
    "volume": "Nov",
    "abstract": "We present CorPipe, the winning entry to the CRAC 2023 Shared Task on Multilingual Coreference Resolution. Our system is an improved version of our earlier multilingual coreference pipeline, and it surpasses other participants by a large margin of 4.5 percent points. CorPipe first performs mention detection, followed by coreference linking via an antecedent-maximization approach on the retrieved spans. Both tasks are trained jointly on all available corpora using a shared pretrained language model. Our main improvements comprise inputs larger than 512 subwords and changing the mention decoding to support ensembling. The source code is available at https://github.com/ufal/crac2023-corpipe",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Milan Straka"
    ]
  },
  "http://arxiv.org/abs/2311.14353": {
    "title": "Average Token Delay: A Duration-aware Latency Metric for Simultaneous Translation",
    "volume": "Nov",
    "abstract": "Simultaneous translation is a task in which the translation begins before the end of an input speech segment. Its evaluation should be conducted based on latency in addition to quality, and for users, the smallest possible amount of latency is preferable. Most existing metrics measure latency based on the start timings of partial translations and ignore their duration. This means such metrics do not penalize the latency caused by long translation output, which delays the comprehension of users and subsequent translations. In this work, we propose a novel latency evaluation metric for simultaneous translation called \\emph{Average Token Delay} (ATD) that focuses on the duration of partial translations. We demonstrate its effectiveness through analyses simulating user-side latency based on Ear-Voice Span (EVS). In our experiment, ATD had the highest correlation with EVS among baseline latency metrics under most conditions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasumasa Kano",
      "Katsuhito Sudoh",
      "Satoshi Nakamura"
    ]
  },
  "http://arxiv.org/abs/2311.14324": {
    "title": "Large Language Models as Topological Structure Enhancers for Text-Attributed Graphs",
    "volume": "Nov",
    "abstract": "The latest advancements in large language models (LLMs) have revolutionized the field of natural language processing (NLP). Inspired by the success of LLMs in NLP tasks, some recent work has begun investigating the potential of applying LLMs in graph learning tasks. However, most of the existing work focuses on utilizing LLMs as powerful node feature augmenters, leaving employing LLMs to enhance graph topological structures an understudied problem. In this work, we explore how to leverage the information retrieval and text generation capabilities of LLMs to refine/enhance the topological structure of text-attributed graphs (TAGs) under the node classification setting. First, we propose using LLMs to help remove unreliable edges and add reliable ones in the TAG. Specifically, we first let the LLM output the semantic similarity between node attributes through delicate prompt designs, and then perform edge deletion and edge addition based on the similarity. Second, we propose using pseudo-labels generated by the LLM to improve graph topology, that is, we introduce the pseudo-label propagation as a regularization to guide the graph neural network (GNN) in learning proper edge weights. Finally, we incorporate the two aforementioned LLM-based methods for graph topological refinement into the process of GNN training, and perform extensive experiments on four real-world datasets. The experimental results demonstrate the effectiveness of LLM-based graph topology refinement (achieving a 0.15%--2.47% performance gain on public benchmarks)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengyin Sun",
      "Yuxiang Ren",
      "Chen Ma",
      "Xuecang Zhang"
    ]
  },
  "http://arxiv.org/abs/2311.14212": {
    "title": "Annotation Sensitivity: Training Data Collection Methods Affect Model Performance",
    "volume": "Nov",
    "abstract": "When training data are collected from human annotators, the design of the annotation instrument, the instructions given to annotators, the characteristics of the annotators, and their interactions can impact training data. This study demonstrates that design choices made when creating an annotation instrument also impact the models trained on the resulting annotations. We introduce the term annotation sensitivity to refer to the impact of annotation data collection methods on the annotations themselves and on downstream model performance and predictions. We collect annotations of hate speech and offensive language in five experimental conditions of an annotation instrument, randomly assigning annotators to conditions. We then fine-tune BERT models on each of the five resulting datasets and evaluate model performance on a holdout portion of each condition. We find considerable differences between the conditions for 1) the share of hate speech/offensive language annotations, 2) model performance, 3) model predictions, and 4) model learning curves. Our results emphasize the crucial role played by the annotation instrument which has received little attention in the machine learning literature. We call for additional research into how and why the instrument impacts the annotations to inform the development of best practices in instrument design",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christoph Kern",
      "Stephanie Eckman",
      "Jacob Beck",
      "Rob Chew",
      "Bolei Ma",
      "Frauke Kreuter"
    ]
  },
  "http://arxiv.org/abs/2311.14199": {
    "title": "A Systematic Review of Deep Learning-based Research on Radiology Report Generation",
    "volume": "Nov",
    "abstract": "Radiology report generation (RRG) aims to automatically generate free-text descriptions from clinical radiographs, e.g., chest X-Ray images. RRG plays an essential role in promoting clinical automation and presents significant help to provide practical assistance for inexperienced doctors and alleviate radiologists' workloads. Therefore, consider these meaningful potentials, research on RRG is experiencing explosive growth in the past half-decade, especially with the rapid development of deep learning approaches. Existing studies perform RRG from the perspective of enhancing different modalities, provide insights on optimizing the report generation process with elaborated features from both visual and textual information, and further facilitate RRG with the cross-modal interactions among them. In this paper, we present a comprehensive review of deep learning-based RRG from various perspectives. Specifically, we firstly cover pivotal RRG approaches based on the task-specific features of radiographs, reports, and the cross-modal relations between them, and then illustrate the benchmark datasets conventionally used for this task with evaluation metrics, subsequently analyze the performance of different approaches and finally offer our summary on the challenges and the trends in future directions. Overall, the goal of this paper is to serve as a tool for understanding existing literature and inspiring potential valuable research in the field of RRG",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Liu",
      "Yuanhe Tian",
      "Yan Song"
    ]
  },
  "http://arxiv.org/abs/2311.14169": {
    "title": "Evaluating GPT-4's Vision Capabilities on Brazilian University Admission Exams",
    "volume": "Nov",
    "abstract": "Recent advancements in language models have showcased human-comparable performance in academic entrance exams. However, existing studies often overlook questions that require the integration of visual comprehension, thus compromising the full spectrum and complexity inherent in real-world scenarios. To address this gap, we present a comprehensive framework to evaluate language models on entrance exams, which incorporates both textual and visual elements. We evaluate the two most recent editions of Exame Nacional do Ensino M\\'edio (ENEM), the main standardized entrance examination adopted by Brazilian universities. Our study not only reaffirms the capabilities of GPT-4 as the state of the art for handling complex multidisciplinary questions, but also pioneers in offering a realistic assessment of multimodal language models on Portuguese examinations. One of the highlights is that text captions transcribing visual content outperform the direct use of images, suggesting that the vision model has room for improvement. Yet, despite improvements afforded by images or captions, mathematical questions remain a challenge for these state-of-the-art models. The code and data used on experiments are available at https://github.com/piresramon/gpt-4-enem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ramon Pires",
      "Thales Sales Almeida",
      "Hugo Abonizio",
      "Rodrigo Nogueira"
    ]
  },
  "http://arxiv.org/abs/2311.14126": {
    "title": "Towards Auditing Large Language Models: Improving Text-based Stereotype Detection",
    "volume": "Nov",
    "abstract": "Large Language Models (LLM) have made significant advances in the recent past becoming more mainstream in Artificial Intelligence (AI) enabled human-facing applications. However, LLMs often generate stereotypical output inherited from historical data, amplifying societal biases and raising ethical concerns. This work introduces i) the Multi-Grain Stereotype Dataset, which includes 52,751 instances of gender, race, profession and religion stereotypic text and ii) a novel stereotype classifier for English text. We design several experiments to rigorously test the proposed model trained on the novel dataset. Our experiments show that training the model in a multi-class setting can outperform the one-vs-all binary counterpart. Consistent feature importance signals from different eXplainable AI tools demonstrate that the new model exploits relevant text features. We utilise the newly created model to assess the stereotypic behaviour of the popular GPT family of models and observe the reduction of bias over time. In summary, our work establishes a robust and practical framework for auditing and evaluating the stereotypic bias in LLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wu Zekun",
      "Sahan Bulathwela",
      "Adriano Soares Koshiyama"
    ]
  },
  "http://arxiv.org/abs/2311.14115": {
    "title": "A density estimation perspective on learning from pairwise human preferences",
    "volume": "Nov",
    "abstract": "Learning from human feedback (LHF) -- and in particular learning from pairwise preferences -- has recently become a crucial ingredient in training large language models (LLMs), and has been the subject of much research. Most recent works frame it as a reinforcement learning problem, where a reward function is learned from pairwise preference data and the LLM is treated as a policy which is adapted to maximize the rewards, often under additional regularization constraints. We propose an alternative interpretation which centers on the generative process for pairwise preferences and treats LHF as a density estimation problem. We provide theoretical and empirical results showing that for a family of generative processes defined via preference behavior distribution equations, training a reward function on pairwise preferences effectively models an annotator's implicit preference distribution. Finally, we discuss and present findings on \"annotator misspecification\" -- failure cases where wrong modeling assumptions are made about annotator behavior, resulting in poorly-adapted models -- suggesting that approaches that learn from pairwise human preferences could have trouble learning from a population of annotators with diverse viewpoints",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Dumoulin",
      "Daniel D. Johnson",
      "Pablo Samuel Castro",
      "Hugo Larochelle",
      "Yann Dauphin"
    ]
  },
  "http://arxiv.org/abs/2311.14096": {
    "title": "Auditing and Mitigating Cultural Bias in LLMs",
    "volume": "Nov",
    "abstract": "Culture fundamentally shapes people's reasoning, behavior, and communication. Generative artificial intelligence (AI) technologies may cause a shift towards a dominant culture. As people increasingly use AI to expedite and even automate various professional and personal tasks, cultural values embedded in AI models may bias authentic expression. We audit large language models for cultural bias, comparing their responses to nationally representative survey data, and evaluate country-specific prompting as a mitigation strategy. We find that GPT-4, 3.5 and 3 exhibit cultural values resembling English-speaking and Protestant European countries. Our mitigation strategy reduces cultural bias in recent models but not for all countries/territories. To avoid cultural bias in generative AI, especially in high-stakes contexts, we suggest using culture matching and ongoing cultural audits",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Tao",
      "Olga Viberg",
      "Ryan S. Baker",
      "Rene F. Kizilcec"
    ]
  },
  "http://arxiv.org/abs/2311.14087": {
    "title": "Question Answering in Natural Language: the Special Case of Temporal Expressions",
    "volume": "Nov",
    "abstract": "Although general question answering has been well explored in recent years, temporal question answering is a task which has not received as much focus. Our work aims to leverage a popular approach used for general question answering, answer extraction, in order to find answers to temporal questions within a paragraph. To train our model, we propose a new dataset, inspired by SQuAD, specifically tailored to provide rich temporal information. We chose to adapt the corpus WikiWars, which contains several documents on history's greatest conflicts. Our evaluation shows that a deep learning model trained to perform pattern matching, often used in general question answering, can be adapted to temporal question answering, if we accept to ask questions whose answers must be directly present within a text",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Armand Stricker"
    ]
  },
  "http://arxiv.org/abs/2311.14076": {
    "title": "Searching for Snippets of Open-Domain Dialogue in Task-Oriented Dialogue Datasets",
    "volume": "Nov",
    "abstract": "Most existing dialogue corpora and models have been designed to fit into 2 predominant categories : task-oriented dialogues portray functional goals, such as making a restaurant reservation or booking a plane ticket, while chit-chat/open-domain dialogues focus on holding a socially engaging talk with a user. However, humans tend to seamlessly switch between modes and even use chitchat to enhance task-oriented conversations. To bridge this gap, new datasets have recently been created, blending both communication modes into conversation examples. The approaches used tend to rely on adding chit-chat snippets to pre-existing, human-generated task-oriented datasets. Given the tendencies observed in humans, we wonder however if the latter do not \\textit{already} hold chit-chat sequences. By using topic modeling and searching for topics which are most similar to a set of keywords related to social talk, we explore the training sets of Schema-Guided Dialogues and MultiWOZ. Our study shows that sequences related to social talk are indeed naturally present, motivating further research on ways chitchat is combined into task-oriented dialogues",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Armand Stricker",
      "Patrick Paroubek"
    ]
  },
  "http://arxiv.org/abs/2311.14067": {
    "title": "Enhancing Task-Oriented Dialogues with Chitchat: a Comparative Study Based on Lexical Diversity and Divergence",
    "volume": "Nov",
    "abstract": "As a recent development, task-oriented dialogues (TODs) have been enriched with chitchat in an effort to make dialogues more diverse and engaging. This enhancement is particularly valuable as TODs are often confined to narrow domains, making the mitigation of repetitive and predictable responses a significant challenge. This paper presents a comparative analysis of three chitchat enhancements, aiming to identify the most effective approach in terms of diversity. Additionally, we quantify the divergence between the added chitchat, the original task-oriented language, and chitchat typically found in chitchat datasets, highlighting the top 20 divergent keywords for each comparison. Our findings drive a discussion on future enhancements for augmenting TODs, emphasizing the importance of grounding dialogues beyond the task to achieve more diverse and natural exchanges",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Armand Stricker",
      "Patrick Paroubek"
    ]
  },
  "http://arxiv.org/abs/2311.14063": {
    "title": "Do VSR Models Generalize Beyond LRS3?",
    "volume": "Nov",
    "abstract": "The Lip Reading Sentences-3 (LRS3) benchmark has primarily been the focus of intense research in visual speech recognition (VSR) during the last few years. As a result, there is an increased risk of overfitting to its excessively used test set, which is only one hour duration. To alleviate this issue, we build a new VSR test set named WildVSR, by closely following the LRS3 dataset creation processes. We then evaluate and analyse the extent to which the current VSR models generalize to the new test data. We evaluate a broad range of publicly available VSR models and find significant drops in performance on our test set, compared to their corresponding LRS3 results. Our results suggest that the increase in word error rates is caused by the models inability to generalize to slightly harder and in the wild lip sequences than those found in the LRS3 test set. Our new test benchmark is made public in order to enable future research towards more robust VSR models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasser Abdelaziz Dahou Djilali",
      "Sanath Narayan",
      "Eustache Le Bihan",
      "Haithem Boussaid",
      "Ebtessam Almazrouei",
      "Merouane Debbah"
    ]
  },
  "http://arxiv.org/abs/2311.13987": {
    "title": "Jam-ALT: A Formatting-Aware Lyrics Transcription Benchmark",
    "volume": "Nov",
    "abstract": "Current automatic lyrics transcription (ALT) benchmarks focus exclusively on word content and ignore the finer nuances of written lyrics including formatting and punctuation, which leads to a potential misalignment with the creative products of musicians and songwriters as well as listeners' experiences. For example, line breaks are important in conveying information about rhythm, emotional emphasis, rhyme, and high-level structure. To address this issue, we introduce Jam-ALT, a new lyrics transcription benchmark based on the JamendoLyrics dataset. Our contribution is twofold. Firstly, a complete revision of the transcripts, geared specifically towards ALT evaluation by following a newly created annotation guide that unifies the music industry's guidelines, covering aspects such as punctuation, line breaks, spelling, background vocals, and non-word sounds. Secondly, a suite of evaluation metrics designed, unlike the traditional word error rate, to capture such phenomena. We hope that the proposed benchmark contributes to the ALT task, enabling more precise and reliable assessments of transcription systems and enhancing the user experience in lyrics applications such as subtitle renderings for live captioning or karaoke",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ondej Cfka",
      "Constantinos Dimitriou",
      "Cheng-i Wang",
      "Hendrik Schreiber",
      "Luke Miner",
      "Fabian-Robert Stter"
    ]
  },
  "http://arxiv.org/abs/2311.13982": {
    "title": "Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex Questions",
    "volume": "Nov",
    "abstract": "Large language models (LLMs) are capable of answering knowledge-intensive complex questions with chain-of-thought (CoT) reasoning. However, they tend to generate factually incorrect reasoning steps when the required knowledge is not available or up-to-date in models' parameters. Recent works turn to retrieving external knowledge to augment CoT reasoning. Despite being promising, these chain-based methods suffer from: 1) Negative retrieval. Unnecessary or incorrect retrieval may mislead the reasoning; 2) Limited sight. Lacking the ability to look backward or forward, a local error in one step will propagate along the chain. In this paper, we propose a novel approach: Probabilistic Tree-of-thought Reasoning (ProbTree). First, LLMs translate a complex question into a query tree, in which each non-root node denotes a sub-question of its parent node. Then, probabilistic reasoning is conducted over the tree, by solving questions from leaf to root considering the confidence of both question decomposing and answering. During reasoning, for leaf nodes, LLMs choose a more confident answer from Closed-book QA that employs parametric knowledge and Open-book QA that employs retrieved external knowledge, thus eliminating the negative retrieval problem. For non-leaf nodes, with the hierarchical structure, LLMs have broader sights and are able to globally reason with the information from child nodes, thus recovering from local errors. The experiments on three Complex QA datasets under the open-domain setting show that our approach outperforms SOTA methods significantly, demonstrating the effect of probabilistic tree-of-thought reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shulin Cao",
      "Jiajie Zhang",
      "Jiaxin Shi",
      "Xin Lv",
      "Zijun Yao",
      "Qi Tian",
      "Juanzi Li",
      "Lei Hou"
    ]
  },
  "http://arxiv.org/abs/2311.13957": {
    "title": "Efficient Trigger Word Insertion",
    "volume": "Nov",
    "abstract": "With the boom in the natural language processing (NLP) field these years, backdoor attacks pose immense threats against deep neural network models. However, previous works hardly consider the effect of the poisoning rate. In this paper, our main objective is to reduce the number of poisoned samples while still achieving a satisfactory Attack Success Rate (ASR) in text backdoor attacks. To accomplish this, we propose an efficient trigger word insertion strategy in terms of trigger word optimization and poisoned sample selection. Extensive experiments on different datasets and models demonstrate that our proposed method can significantly improve attack effectiveness in text classification tasks. Remarkably, our approach achieves an ASR of over 90% with only 10 poisoned samples in the dirty-label setting and requires merely 1.5% of the training data in the clean-label setting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yueqi Zeng",
      "Ziqiang Li",
      "Pengfei Xia",
      "Lei Liu",
      "Bin Li"
    ]
  },
  "http://arxiv.org/abs/2311.13951": {
    "title": "MLLM-Bench, Evaluating Multi-modal LLMs using GPT-4V",
    "volume": "Nov",
    "abstract": "In the pursuit of Artificial General Intelligence (AGI), the integration of vision in language models has marked a significant milestone. The advent of vision-language models (MLLMs) like GPT-4V have expanded AI applications, aligning with the multi-modal capabilities of the human brain. However, evaluating the efficacy of MLLMs poses a substantial challenge due to the subjective nature of tasks that lack definitive answers. Existing automatic evaluation methodologies on multi-modal large language models rely on objective queries that have standard answers, inadequately addressing the nuances of creative and associative multi-modal tasks. To address this, we introduce MLLM-Bench, an innovative benchmark inspired by Vicuna, spanning a diverse array of scenarios, including Perception, Understanding, Applying, Analyzing, Evaluating, and Creation along with the ethical consideration. MLLM-Bench is designed to reflect user experience more accurately and provide a more holistic assessment of model performance. Comparative evaluations indicate a significant performance gap between existing open-source models and GPT-4V. We posit that MLLM-Bench will catalyze progress in the open-source community towards developing user-centric vision-language models that meet a broad spectrum of real-world applications. See online leaderboard in \\url{https://mllm-bench.llmzoo.com}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wentao Ge",
      "Shunian Chen",
      "Guiming Chen",
      "Junying Chen",
      "Zhihong Chen",
      "Shuo Yan",
      "Chenghao Zhu",
      "Ziyue Lin",
      "Wenya Xie",
      "Xidong Wang",
      "Anningzhe Gao",
      "Zhiyi Zhang",
      "Jianquan Li",
      "Xiang Wan",
      "Benyou Wang"
    ]
  },
  "http://arxiv.org/abs/2311.13937": {
    "title": "Exploring Methods for Cross-lingual Text Style Transfer: The Case of Text Detoxification",
    "volume": "Nov",
    "abstract": "Text detoxification is the task of transferring the style of text from toxic to neutral. While here are approaches yielding promising results in monolingual setup, e.g., (Dale et al., 2021; Hallinan et al., 2022), cross-lingual transfer for this task remains a challenging open problem (Moskovskiy et al., 2022). In this work, we present a large-scale study of strategies for cross-lingual text detoxification -- given a parallel detoxification corpus for one language; the goal is to transfer detoxification ability to another language for which we do not have such a corpus. Moreover, we are the first to explore a new task where text translation and detoxification are performed simultaneously, providing several strong baselines for this task. Finally, we introduce new automatic detoxification evaluation metrics with higher correlations with human judgments than previous benchmarks. We assess the most promising approaches also with manual markup, determining the answer for the best strategy to transfer the knowledge of text detoxification between languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daryna Dementieva",
      "Daniil Moskovskiy",
      "David Dale",
      "Alexander Panchenko"
    ]
  },
  "http://arxiv.org/abs/2311.13921": {
    "title": "Some Like It Small: Czech Semantic Embedding Models for Industry Applications",
    "volume": "Nov",
    "abstract": "This article focuses on the development and evaluation of Small-sized Czech sentence embedding models. Small models are important components for real-time industry applications in resource-constrained environments. Given the limited availability of labeled Czech data, alternative approaches, including pre-training, knowledge distillation, and unsupervised contrastive fine-tuning, are investigated. Comprehensive intrinsic and extrinsic analyses are conducted, showcasing the competitive performance of our models compared to significantly larger counterparts, with approximately 8 times smaller size and 5 times faster speed than conventional Base-sized models. To promote cooperation and reproducibility, both the models and the evaluation pipeline are made publicly accessible. Ultimately, this article presents practical applications of the developed sentence embedding models in Seznam.cz, the Czech search engine. These models have effectively replaced previous counterparts, enhancing the overall search experience for instance, in organic search, featured snippets, and image search. This transition has yielded improved performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ji Bedn",
      "Jakub Nplava",
      "Petra Barankov",
      "Ondej Lisick"
    ]
  },
  "http://arxiv.org/abs/2311.13910": {
    "title": "Dialogue Quality and Emotion Annotations for Customer Support Conversations",
    "volume": "Nov",
    "abstract": "Task-oriented conversational datasets often lack topic variability and linguistic diversity. However, with the advent of Large Language Models (LLMs) pretrained on extensive, multilingual and diverse text data, these limitations seem overcome. Nevertheless, their generalisability to different languages and domains in dialogue applications remains uncertain without benchmarking datasets. This paper presents a holistic annotation approach for emotion and conversational quality in the context of bilingual customer support conversations. By performing annotations that take into consideration the complete instances that compose a conversation, one can form a broader perspective of the dialogue as a whole. Furthermore, it provides a unique and valuable resource for the development of text classification models. To this end, we present benchmarks for Emotion Recognition and Dialogue Quality Estimation and show that further research is needed to leverage these models in a production setting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "John Mendona",
      "Patrcia Pereira",
      "Miguel Menezes",
      "Vera Cabarro",
      "Ana C. Farinha",
      "Helena Moniz",
      "Joo Paulo Carvalho",
      "Alon Lavie",
      "Isabel Trancoso"
    ]
  },
  "http://arxiv.org/abs/2311.13892": {
    "title": "General Phrase Debiaser: Debiasing Masked Language Models at a Multi-Token Level",
    "volume": "Nov",
    "abstract": "The social biases and unwelcome stereotypes revealed by pretrained language models are becoming obstacles to their application. Compared to numerous debiasing methods targeting word level, there has been relatively less attention on biases present at phrase level, limiting the performance of debiasing in discipline domains. In this paper, we propose an automatic multi-token debiasing pipeline called \\textbf{General Phrase Debiaser}, which is capable of mitigating phrase-level biases in masked language models. Specifically, our method consists of a \\textit{phrase filter stage} that generates stereotypical phrases from Wikipedia pages as well as a \\textit{model debias stage} that can debias models at the multi-token level to tackle bias challenges on phrases. The latter searches for prompts that trigger model's bias, and then uses them for debiasing. State-of-the-art results on standard datasets and metrics show that our approach can significantly reduce gender biases on both career and multiple disciplines, across models with varying parameter sizes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingkang Shi",
      "Xiaodan Zhang",
      "Dehan Kong",
      "Yulei Wu",
      "Zongzhen Liu",
      "Honglei Lyu",
      "Longtao Huang"
    ]
  },
  "http://arxiv.org/abs/2311.13878": {
    "title": "Minimizing Factual Inconsistency and Hallucination in Large Language Models",
    "volume": "Nov",
    "abstract": "Large Language Models (LLMs) are widely used in critical fields such as healthcare, education, and finance due to their remarkable proficiency in various language-related tasks. However, LLMs are prone to generating factually incorrect responses or \"hallucinations,\" which can lead to a loss of credibility and trust among users. To address this issue, we propose a multi-stage framework that generates the rationale first, verifies and refines incorrect ones, and uses them as supporting references to generate the answer. The generated rationale enhances the transparency of the answer and our framework provides insights into how the model arrived at this answer, by using this rationale and the references to the context. In this paper, we demonstrate its effectiveness in improving the quality of responses to drug-related inquiries in the life sciences industry. Our framework improves traditional Retrieval Augmented Generation (RAG) by enabling OpenAI GPT-3.5-turbo to be 14-25% more faithful and 16-22% more accurate on two datasets. Furthermore, fine-tuning samples based on our framework improves the accuracy of smaller open-access LLMs by 33-42% and competes with RAG on commercial models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muneeswaran I",
      "Shreya Saxena",
      "Siva Prasad",
      "M V Sai Prakash",
      "Advaith Shankar",
      "Varun V",
      "Vishal Vaddina",
      "Saisubramaniam Gopalakrishnan"
    ]
  },
  "http://arxiv.org/abs/2311.13857": {
    "title": "Challenges of Large Language Models for Mental Health Counseling",
    "volume": "Nov",
    "abstract": "The global mental health crisis is looming with a rapid increase in mental disorders, limited resources, and the social stigma of seeking treatment. As the field of artificial intelligence (AI) has witnessed significant advancements in recent years, large language models (LLMs) capable of understanding and generating human-like text may be used in supporting or providing psychological counseling. However, the application of LLMs in the mental health domain raises concerns regarding the accuracy, effectiveness, and reliability of the information provided. This paper investigates the major challenges associated with the development of LLMs for psychological counseling, including model hallucination, interpretability, bias, privacy, and clinical effectiveness. We explore potential solutions to these challenges that are practical and applicable to the current paradigm of AI. From our experience in developing and deploying LLMs for mental health, AI holds a great promise for improving mental health care, if we can carefully navigate and overcome pitfalls of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neo Christopher Chung",
      "George Dyer",
      "Lennart Brocki"
    ]
  },
  "http://arxiv.org/abs/2311.13848": {
    "title": "Grammatical Error Correction via Mixed-Grained Weighted Training",
    "volume": "Nov",
    "abstract": "The task of Grammatical Error Correction (GEC) aims to automatically correct grammatical errors in natural texts. Almost all previous works treat annotated training data equally, but inherent discrepancies in data are neglected. In this paper, the inherent discrepancies are manifested in two aspects, namely, accuracy of data annotation and diversity of potential annotations. To this end, we propose MainGEC, which designs token-level and sentence-level training weights based on inherent discrepancies in accuracy and potential diversity of data annotation, respectively, and then conducts mixed-grained weighted training to improve the training effect for GEC. Empirical evaluation shows that whether in the Seq2Seq or Seq2Edit manner, MainGEC achieves consistent and significant performance improvements on two benchmark datasets, demonstrating the effectiveness and superiority of the mixed-grained weighted training. Further ablation experiments verify the effectiveness of designed weights of both granularities in MainGEC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Li",
      "Quan Wang",
      "Chiwei Zhu",
      "Zhendong Mao",
      "Yongdong Zhang"
    ]
  },
  "http://arxiv.org/abs/2311.13833": {
    "title": "Lego: Learning to Disentangle and Invert Concepts Beyond Object Appearance in Text-to-Image Diffusion Models",
    "volume": "Nov",
    "abstract": "Diffusion models have revolutionized generative content creation and text-to-image (T2I) diffusion models in particular have increased the creative freedom of users by allowing scene synthesis using natural language. T2I models excel at synthesizing concepts such as nouns, appearances, and styles. To enable customized content creation based on a few example images of a concept, methods such as Textual Inversion and DreamBooth invert the desired concept and enable synthesizing it in new scenes. However, inverting more general concepts that go beyond object appearance and style (adjectives and verbs) through natural language, remains a challenge. Two key characteristics of these concepts contribute to the limitations of current inversion methods. 1) Adjectives and verbs are entangled with nouns (subject) and can hinder appearance-based inversion methods, where the subject appearance leaks into the concept embedding and 2) describing such concepts often extends beyond single word embeddings (being frozen in ice, walking on a tightrope, etc.) that current methods do not handle. In this study, we introduce Lego, a textual inversion method designed to invert subject entangled concepts from a few example images. Lego disentangles concepts from their associated subjects using a simple yet effective Subject Separation step and employs a Context Loss that guides the inversion of single/multi-embedding concepts. In a thorough user study, Lego-generated concepts were preferred over 70% of the time when compared to the baseline. Additionally, visual question answering using a large language model suggested Lego-generated concepts are better aligned with the text description of the concept",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saman Motamed",
      "Danda Pani Paudel",
      "Luc Van Gool"
    ]
  },
  "http://arxiv.org/abs/2311.13806": {
    "title": "AdaTyper: Adaptive Semantic Column Type Detection",
    "volume": "Nov",
    "abstract": "Understanding the semantics of relational tables is instrumental for automation in data exploration and preparation systems. A key source for understanding a table is the semantics of its columns. With the rise of deep learning, learned table representations are now available, which can be applied for semantic type detection and achieve good performance on benchmarks. Nevertheless, we observe a gap between this performance and its applicability in practice. In this paper, we propose AdaTyper to address one of the most critical deployment challenges: adaptation. AdaTyper uses weak-supervision to adapt a hybrid type predictor towards new semantic types and shifted data distributions at inference time, using minimal human feedback. The hybrid type predictor of AdaTyper combines rule-based methods and a light machine learning model for semantic column type detection. We evaluate the adaptation performance of AdaTyper on real-world database tables hand-annotated with semantic column types through crowdsourcing and find that the f1-score improves for new and existing types. AdaTyper approaches an average precision of 0.6 after only seeing 5 examples, significantly outperforming existing adaptation methods based on human-provided regular expressions or dictionaries",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Madelon Hulsebos",
      "Paul Groth",
      "aatay Demiralp"
    ]
  },
  "http://arxiv.org/abs/2311.13784": {
    "title": "DaG LLM ver 1.0: Pioneering Instruction-Tuned Language Modeling for Korean NLP",
    "volume": "Nov",
    "abstract": "This paper presents the DaG LLM (David and Goliath Large Language Model), a language model specialized for Korean and fine-tuned through Instruction Tuning across 41 tasks within 13 distinct categories",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongjun Jang",
      "Sangah Lee",
      "Sungjoo Byun",
      "Jinwoong Kim",
      "Jean Seo",
      "Minseok Kim",
      "Soyeon Kim",
      "Chaeyoung Oh",
      "Jaeyoon Kim",
      "Hyemi Jo",
      "Hyopil Shin"
    ]
  },
  "http://arxiv.org/abs/2311.13755": {
    "title": "Transformer-based Named Entity Recognition in Construction Supply Chain Risk Management in Australia",
    "volume": "Nov",
    "abstract": "The construction industry in Australia is characterized by its intricate supply chains and vulnerability to myriad risks. As such, effective supply chain risk management (SCRM) becomes imperative. This paper employs different transformer models, and train for Named Entity Recognition (NER) in the context of Australian construction SCRM. Utilizing NER, transformer models identify and classify specific risk-associated entities in news articles, offering a detailed insight into supply chain vulnerabilities. By analysing news articles through different transformer models, we can extract relevant entities and insights related to specific risk taxonomies local (milieu) to the Australian construction landscape. This research emphasises the potential of NLP-driven solutions, like transformer models, in revolutionising SCRM for construction in geo-media specific contexts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Milad Baghalzadeh Shishehgarkhaneh",
      "Robert C. Moehler",
      "Yihai Fang",
      "Amer A. Hijazi",
      "Hamed Aboutorab"
    ]
  },
  "http://arxiv.org/abs/2311.13735": {
    "title": "Surpassing GPT-4 Medical Coding with a Two-Stage Approach",
    "volume": "Nov",
    "abstract": "Recent advances in large language models (LLMs) show potential for clinical applications, such as clinical decision support and trial recommendations. However, the GPT-4 LLM predicts an excessive number of ICD codes for medical coding tasks, leading to high recall but low precision. To tackle this challenge, we introduce LLM-codex, a two-stage approach to predict ICD codes that first generates evidence proposals using an LLM and then employs an LSTM-based verification stage. The LSTM learns from both the LLM's high recall and human expert's high precision, using a custom loss function. Our model is the only approach that simultaneously achieves state-of-the-art results in medical coding accuracy, accuracy on rare codes, and sentence-level evidence identification to support coding decisions without training on human-annotated evidence according to experiments on the MIMIC dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhichao Yang",
      "Sanjit Singh Batra",
      "Joel Stremmel",
      "Eran Halperin"
    ]
  },
  "http://arxiv.org/abs/2311.13729": {
    "title": "Comparison of pipeline, sequence-to-sequence, and GPT models for end-to-end relation extraction: experiments with the rare disease use-case",
    "volume": "Nov",
    "abstract": "End-to-end relation extraction (E2ERE) is an important and realistic application of natural language processing (NLP) in biomedicine. In this paper, we aim to compare three prevailing paradigms for E2ERE using a complex dataset focused on rare diseases involving discontinuous and nested entities. We use the RareDis information extraction dataset to evaluate three competing approaches (for E2ERE): NER $\\rightarrow$ RE pipelines, joint sequence to sequence models, and generative pre-trained transformer (GPT) models. We use comparable state-of-the-art models and best practices for each of these approaches and conduct error analyses to assess their failure modes. Our findings reveal that pipeline models are still the best, while sequence-to-sequence models are not far behind; GPT models with eight times as many parameters are worse than even sequence-to-sequence models and lose to pipeline models by over 10 F1 points. Partial matches and discontinuous entities caused many NER errors contributing to lower overall E2E performances. We also verify these findings on a second E2ERE dataset for chemical-protein interactions. Although generative LM-based methods are more suitable for zero-shot settings, when training data is available, our results show that it is better to work with more conventional models trained and tailored for E2ERE. More innovative methods are needed to marry the best of the both worlds from smaller encoder-decoder pipeline models and the larger GPT models to improve E2ERE. As of now, we see that well designed pipeline models offer substantial performance gains at a lower cost and carbon footprint for E2ERE. Our contribution is also the first to conduct E2ERE for the RareDis dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shashank Gupta",
      "Xuguang Ai",
      "Ramakanth Kavuluru"
    ]
  },
  "http://arxiv.org/abs/2311.13708": {
    "title": "Dynamic Analysis Method for Hidden Dangers in Substation Based on Knowledge Graph",
    "volume": "Nov",
    "abstract": "To address the challenge of identifying and understanding hidden dangers in substations from unstructured text data, a novel dynamic analysis method is proposed. This approach begins by analyzing and extracting data from the unstructured text related to hidden dangers. It then leverages a flexible, distributed data search engine built on Elastic-Search to handle this information. Following this, the hidden Markov model is employed to train the data within the engine. The Viterbi algorithm is integrated to decipher the hidden state sequences, facilitating the segmentation and labeling of entities related to hidden dangers. The final step involves using the Neo4j graph database to dynamically create a knowledge map that visualizes hidden dangers in the substation. This method's effectiveness is demonstrated through an example analysis using data from a specific substation's hidden dangers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiwei Li",
      "Xing Liu",
      "Wei Wang",
      "Lu Chen",
      "Sizhe Li",
      "Hui Fan"
    ]
  },
  "http://arxiv.org/abs/2311.13668": {
    "title": "MAIRA-1: A specialised large multimodal model for radiology report generation",
    "volume": "Nov",
    "abstract": "We present a radiology-specific multimodal model for the task for generating radiological reports from chest X-rays (CXRs). Our work builds on the idea that large language model(s) can be equipped with multimodal capabilities through alignment with pre-trained vision encoders. On natural images, this has been shown to allow multimodal models to gain image understanding and description capabilities. Our proposed model (MAIRA-1) leverages a CXR-specific image encoder in conjunction with a fine-tuned large language model based on Vicuna-7B, and text-based data augmentation, to produce reports with state-of-the-art quality. In particular, MAIRA-1 significantly improves on the radiologist-aligned RadCliQ metric and across all lexical metrics considered. Manual review of model outputs demonstrates promising fluency and accuracy of generated reports while uncovering failure modes not captured by existing evaluation practices. More information and resources can be found on the project website: https://aka.ms/maira",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephanie L. Hyland",
      "Shruthi Bannur",
      "Kenza Bouzid",
      "Daniel C. Castro",
      "Mercy Ranjit",
      "Anton Schwaighofer",
      "Fernando Prez-Garca",
      "Valentina Salvatelli",
      "Shaury Srivastav",
      "Anja Thieme",
      "Noel Codella",
      "Matthew P. Lungren",
      "Maria Teodora Wetscherek",
      "Ozan Oktay",
      "Javier Alvarez-Valle"
    ]
  },
  "http://arxiv.org/abs/2311.13657": {
    "title": "Efficient Transformer Knowledge Distillation: A Performance Review",
    "volume": "Nov",
    "abstract": "As pretrained transformer language models continue to achieve state-of-the-art performance, the Natural Language Processing community has pushed for advances in model compression and efficient attention mechanisms to address high computational requirements and limited input sequence length. Despite these separate efforts, no investigation has been done into the intersection of these two fields. In this work, we provide an evaluation of model compression via knowledge distillation on efficient attention transformers. We provide cost-performance trade-offs for the compression of state-of-the-art efficient attention architectures and the gains made in performance in comparison to their full attention counterparts. Furthermore, we introduce a new long-context Named Entity Recognition dataset, GONERD, to train and test the performance of NER models on long sequences. We find that distilled efficient attention transformers can preserve a significant amount of original model performance, preserving up to 98.6% across short-context tasks (GLUE, SQUAD, CoNLL-2003), up to 94.6% across long-context Question-and-Answering tasks (HotpotQA, TriviaQA), and up to 98.8% on long-context Named Entity Recognition (GONERD), while decreasing inference times by up to 57.8%. We find that, for most models on most tasks, performing knowledge distillation is an effective method to yield high-performing efficient attention models with low costs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathan Brown",
      "Ashton Williamson",
      "Tahj Anderson",
      "Logan Lawrence"
    ]
  },
  "http://arxiv.org/abs/2311.13647": {
    "title": "Language Model Inversion",
    "volume": "Nov",
    "abstract": "Language models produce a distribution over the next token; can we use this information to recover the prompt tokens? We consider the problem of language model inversion and show that next-token probabilities contain a surprising amount of information about the preceding text. Often we can recover the text in cases where it is hidden from the user, motivating a method for recovering unknown prompts given only the model's current distribution output. We consider a variety of model access scenarios, and show how even without predictions for every token in the vocabulary we can recover the probability vector through search. On Llama-2 7b, our inversion method reconstructs prompts with a BLEU of $59$ and token-level F1 of $78$ and recovers $27\\%$ of prompts exactly. Code for reproducing all experiments is available at http://github.com/jxmorris12/vec2text",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "John X. Morris",
      "Wenting Zhao",
      "Justin T. Chiu",
      "Vitaly Shmatikov",
      "Alexander M. Rush"
    ]
  },
  "http://arxiv.org/abs/2311.13628": {
    "title": "Prompt Risk Control: A Rigorous Framework for Responsible Deployment of Large Language Models",
    "volume": "Nov",
    "abstract": "The recent explosion in the capabilities of large language models has led to a wave of interest in how best to prompt a model to perform a given task. While it may be tempting to simply choose a prompt based on average performance on a validation set, this can lead to a deployment where unexpectedly poor responses are generated, especially for the worst-off users. To mitigate this prospect, we propose Prompt Risk Control, a lightweight framework for selecting a prompt based on rigorous upper bounds on families of informative risk measures. We offer methods for producing bounds on a diverse set of metrics, including quantities that measure worst-case responses and disparities in generation quality across the population of users. In addition, we extend the underlying statistical bounding techniques to accommodate the possibility of distribution shifts in deployment. Experiments on applications such as open-ended chat, medical question summarization, and code generation highlight how such a framework can foster responsible deployment by reducing the risk of the worst outcomes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas P. Zollo",
      "Todd Morrill",
      "Zhun Deng",
      "Jake C. Snell",
      "Toniann Pitassi",
      "Richard Zemel"
    ]
  },
  "http://arxiv.org/abs/2311.13581": {
    "title": "PaSS: Parallel Speculative Sampling",
    "volume": "Nov",
    "abstract": "Scaling the size of language models to tens of billions of parameters has led to impressive performance on a wide range of tasks. At generation, these models are used auto-regressively, requiring a forward pass for each generated token, and thus reading the full set of parameters from memory. This memory access forms the primary bottleneck for generation and it worsens as the model size increases. Moreover, executing a forward pass for multiple tokens in parallel often takes nearly the same time as it does for just one token. These two observations lead to the development of speculative sampling, where a second smaller model is used to draft a few tokens, that are then validated or rejected using a single forward pass of the large model. Unfortunately, this method requires two models that share the same tokenizer and thus limits its adoption. As an alternative, we propose to use parallel decoding as a way to draft multiple tokens from a single model with no computational cost, nor the need for a second model. Our approach only requires an additional input token that marks the words that will be generated simultaneously. We show promising performance (up to $30\\%$ speed-up) while requiring only as few as $O(d_{emb})$ additional parameters",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giovanni Monea",
      "Armand Joulin",
      "Edouard Grave"
    ]
  },
  "http://arxiv.org/abs/2311.13565": {
    "title": "Drilling Down into the Discourse Structure with LLMs for Long Document Question Answering",
    "volume": "Nov",
    "abstract": "We address the task of evidence retrieval for long document question answering, which involves locating relevant paragraphs within a document to answer a question. We aim to assess the applicability of large language models (LLMs) in the task of zero-shot long document evidence retrieval, owing to their unprecedented performance across various NLP tasks. However, currently the LLMs can consume limited context lengths as input, thus providing document chunks as inputs might overlook the global context while missing out on capturing the inter-segment dependencies. Moreover, directly feeding the large input sets can incur significant computational costs, particularly when processing the entire document (and potentially incurring monetary expenses with enterprise APIs like OpenAI's GPT variants). To address these challenges, we propose a suite of techniques that exploit the discourse structure commonly found in documents. By utilizing this structure, we create a condensed representation of the document, enabling a more comprehensive understanding and analysis of relationships between different parts. We retain $99.6\\%$ of the best zero-shot approach's performance, while processing only $26\\%$ of the total tokens used by the best approach in the information seeking evidence retrieval setup. We also show how our approach can be combined with \\textit{self-ask} reasoning agent to achieve best zero-shot performance in complex multi-hop question answering, just $\\approx 4\\%$ short of zero-shot performance using gold evidence",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inderjeet Nair",
      "Shwetha Somasundaram",
      "Apoorv Saxena",
      "Koustava Goswami"
    ]
  },
  "http://arxiv.org/abs/2311.13534": {
    "title": "LM-Cocktail: Resilient Tuning of Language Models via Model Merging",
    "volume": "Nov",
    "abstract": "The pre-trained language models are continually fine-tuned to better support downstream applications. However, this operation may result in significant performance degeneration on general tasks beyond the targeted domain. To overcome this problem, we propose a novel method which enables the fine-tuned model to stay resilient in general perspectives. Our method is conducted in the form of model merging (namely LM-Cocktail), where the fine-tuned language model is merged with the pre-trained base model or the peer models from other domains through weighted average. Despite simplicity, LM-Cocktail is surprisingly effective: the resulted model is able to achieve a strong empirical performance in the whole scope of general tasks while preserving a superior capacity in its targeted domain. We conduct comprehensive experiments with LLama and BGE model on popular benchmarks, including FLAN, MMLU, MTEB, whose results validate the efficacy of our proposed method. The code and checkpoints are available at https://github.com/FlagOpen/FlagEmbedding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shitao Xiao",
      "Zheng Liu",
      "Peitian Zhang",
      "Xingrun Xing"
    ]
  },
  "http://arxiv.org/abs/2311.13495": {
    "title": "Current Topological and Machine Learning Applications for Bias Detection in Text",
    "volume": "Nov",
    "abstract": "Institutional bias can impact patient outcomes, educational attainment, and legal system navigation. Written records often reflect bias, and once bias is identified; it is possible to refer individuals for training to reduce bias. Many machine learning tools exist to explore text data and create predictive models that can search written records to identify real-time bias. However, few previous studies investigate large language model embeddings and geometric models of biased text data to understand geometry's impact on bias modeling accuracy. To overcome this issue, this study utilizes the RedditBias database to analyze textual biases. Four transformer models, including BERT and RoBERTa variants, were explored. Post-embedding, t-SNE allowed two-dimensional visualization of data. KNN classifiers differentiated bias types, with lower k-values proving more effective. Findings suggest BERT, particularly mini BERT, excels in bias classification, while multilingual models lag. The recommendation emphasizes refining monolingual models and exploring domain-specific biases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Colleen Farrelly",
      "Yashbir Singh",
      "Quincy A. Hathaway",
      "Gunnar Carlsson",
      "Ashok Choudhary",
      "Rahul Paul",
      "Gianfranco Doretto",
      "Yassine Himeur",
      "Shadi Atalls",
      "Wathiq Mansoor"
    ]
  },
  "http://arxiv.org/abs/2311.13475": {
    "title": "Machine Translation to Control Formality Features in the Target Language",
    "volume": "Nov",
    "abstract": "Formality plays a significant role in language communication, especially in low-resource languages such as Hindi, Japanese and Korean. These languages utilise formal and informal expressions to convey messages based on social contexts and relationships. When a language translation technique is used to translate from a source language that does not pertain the formality (e.g. English) to a target language that does, there is a missing information on formality that could be a challenge in producing an accurate outcome. This research explores how this issue should be resolved when machine learning methods are used to translate from English to languages with formality, using Hindi as the example data. This was done by training a bilingual model in a formality-controlled setting and comparing its performance with a pre-trained multilingual model in a similar setting. Since there are not a lot of training data with ground truth, automated annotation techniques were employed to increase the data size. The primary modeling approach involved leveraging transformer models, which have demonstrated effectiveness in various natural language processing tasks. We evaluate the official formality accuracy(ACC) by comparing the predicted masked tokens with the ground truth. This metric provides a quantitative measure of how well the translations align with the desired outputs. Our study showcases a versatile translation strategy that considers the nuances of formality in the target language, catering to diverse language communication needs and scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harshita Tyagi",
      "Prashasta Jung",
      "Hyowon Lee"
    ]
  },
  "http://arxiv.org/abs/2311.13472": {
    "title": "Complexity-Guided Curriculum Learning for Text Graphs",
    "volume": "Nov",
    "abstract": "Curriculum learning provides a systematic approach to training. It refines training progressively, tailors training to task requirements, and improves generalization through exposure to diverse examples. We present a curriculum learning approach that builds on existing knowledge about text and graph complexity formalisms for training with text graph data. The core part of our approach is a novel data scheduler, which employs \"spaced repetition\" and complexity formalisms to guide the training process. We demonstrate the effectiveness of the proposed approach on several text graph tasks and graph neural network architectures. The proposed model gains more and uses less data; consistently prefers text over graph complexity indices throughout training, while the best curricula derived from text and graph complexity indices are equally effective; and it learns transferable curricula across GNN models and datasets. In addition, we find that both node-level (local) and graph-level (global) graph complexity indices, as well as shallow and traditional text complexity indices play a crucial role in effective curriculum learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nidhi Vakil",
      "Hadi Amiri"
    ]
  },
  "http://arxiv.org/abs/2311.13455": {
    "title": "Generation of Explanations for Logic Reasoning",
    "volume": "Nov",
    "abstract": "This thesis delves into a fortiori arguments in deductive reasoning, underscoring their relevance in various domains such as law, philosophy, and artificial intelligence. The research is centred on employing GPT-3.5-turbo to automate the analysis of these arguments, with a focus on understanding intricate reasoning processes, generating clear and coherent explanations, and creating novel arguments. The methodology encompasses a series of tasks including detailed reasoning, interpretation, and the augmentation of a fortiori arguments. It involves meticulously identifying these arguments in diverse contexts, differentiating comparative elements, and categorizing them based on their logical structure. Extensive experiments reveals the challenges encountered by GPT-3.5-turbo in accurately detecting and classifying a fortiori arguments. Nevertheless, the model demonstrates a performance that rivals specialized models, particularly in extracting key components and interpreting underlying properties. The integration of external information into the model's processing significantly elevates the quality of the generated explanations. Additionally, the model exhibits a noteworthy capability in augmenting arguments, thus contributing to the enrichment of the data set. Despite facing certain limitations, this thesis makes significant contributions to the fields of artificial intelligence and logical reasoning. It introduces novel methodologies, establishes a rigorous evaluation framework, and provides deep insights that set the stage for future advancements in automated logical reasoning. The findings and methodologies presented herein not only underscore the potential of AI in complex reasoning tasks but also highlight areas for future research and development",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanyi Pu"
    ]
  },
  "http://arxiv.org/abs/2311.13350": {
    "title": "Fact-based Court Judgment Prediction",
    "volume": "Nov",
    "abstract": "This extended abstract extends the research presented in \"ILDC for CJPE: Indian Legal Documents Corpus for Court Judgment Prediction and Explanation\" \\cite{malik-etal-2021-ildc}, focusing on fact-based judgment prediction within the context of Indian legal documents. We introduce two distinct problem variations: one based solely on facts, and another combining facts with rulings from lower courts (RLC). Our research aims to enhance early-phase case outcome prediction, offering significant benefits to legal professionals and the general public. The results, however, indicated a performance decline compared to the original ILDC for CJPE study, even after implementing various weightage schemes in our DELSumm algorithm. Additionally, using only facts for legal judgment prediction with different transformer models yielded results inferior to the state-of-the-art outcomes reported in the \"ILDC for CJPE\" study",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shubham Kumar Nigam",
      "Aniket Deroy"
    ]
  },
  "http://arxiv.org/abs/2311.13314": {
    "title": "Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based Retrofitting",
    "volume": "Nov",
    "abstract": "Incorporating factual knowledge in knowledge graph is regarded as a promising approach for mitigating the hallucination of large language models (LLMs). Existing methods usually only use the user's input to query the knowledge graph, thus failing to address the factual hallucination generated by LLMs during its reasoning process. To address this problem, this paper proposes Knowledge Graph-based Retrofitting (KGR), a new framework that incorporates LLMs with KGs to mitigate factual hallucination during the reasoning process by retrofitting the initial draft responses of LLMs based on the factual knowledge stored in KGs. Specifically, KGR leverages LLMs to extract, select, validate, and retrofit factual statements within the model-generated responses, which enables an autonomous knowledge verifying and refining procedure without any additional manual efforts. Experiments show that KGR can significantly improve the performance of LLMs on factual QA benchmarks especially when involving complex reasoning processes, which demonstrates the necessity and effectiveness of KGR in mitigating hallucination and enhancing the reliability of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyan Guan",
      "Yanjiang Liu",
      "Hongyu Lin",
      "Yaojie Lu",
      "Ben He",
      "Xianpei Han",
      "Le Sun"
    ]
  },
  "http://arxiv.org/abs/2311.13307": {
    "title": "Rethinking Radiology Report Generation via Causal Reasoning and Counterfactual Augmentation",
    "volume": "Nov",
    "abstract": "Radiology Report Generation (RRG) draws attention as an interaction between vision and language fields. Previous works inherited the ideology of vision-to-language generation tasks,aiming to generate paragraphs with high consistency as reports. However, one unique characteristic of RRG, the independence between diseases, was neglected, leading to the injection of the spurious confounder, i.e., the disease co-occurrence. Unfortunately, this confounder confuses the process of report generation worse because of the biased RRG data distribution. In this paper, to rethink this issue thoroughly, we reason about its causes and effects from a novel perspective of statistics and causality, where the Joint Vision Coupling and the Conditional Sentence Coherence Coupling are two aspects prone to implicitly decrease the accuracy of reports. Then, a counterfactual augmentation strategy that contains the Counterfactual Sample Synthesis and the Counterfactual Report Reconstruction sub-methods is proposed to break these two aspects of spurious effects. Experimental results and further analyses on two widely used datasets justify our reasoning and proposed methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Song",
      "Jiafan Liu",
      "Yun Li",
      "Wenbin Lei",
      "Ruxin Wang"
    ]
  },
  "http://arxiv.org/abs/2311.13281": {
    "title": "Intention and Context Elicitation with Large Language Models in the Legal Aid Intake Process",
    "volume": "Nov",
    "abstract": "Large Language Models (LLMs) and chatbots show significant promise in streamlining the legal intake process. This advancement can greatly reduce the workload and costs for legal aid organizations, improving availability while making legal assistance more accessible to a broader audience. However, a key challenge with current LLMs is their tendency to overconfidently deliver an immediate 'best guess' to a client's question based on the output distribution learned over the training data. This approach often overlooks the client's actual intentions or the specifics of their legal situation. As a result, clients may not realize the importance of providing essential additional context or expressing their underlying intentions, which are crucial for their legal cases. Traditionally, logic based decision trees have been used to automate intake for specific access to justice issues, such as immigration and eviction. But those solutions lack scalability. We demonstrate a proof-of-concept using LLMs to elicit and infer clients' underlying intentions and specific legal circumstances through free-form, language-based interactions. We also propose future research directions to use supervised fine-tuning or offline reinforcement learning to automatically incorporate intention and context elicitation in chatbots without explicit prompting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nick Goodson",
      "Rongfei Lu"
    ]
  },
  "http://arxiv.org/abs/2311.13274": {
    "title": "Enhancing Summarization Performance through Transformer-Based Prompt Engineering in Automated Medical Reporting",
    "volume": "Nov",
    "abstract": "Customized medical prompts enable Large Language Models (LLM) to effectively address medical dialogue summarization. The process of medical reporting is often time-consuming for healthcare professionals. Implementing medical dialogue summarization techniques presents a viable solution to alleviate this time constraint by generating automated medical reports. The effectiveness of LLMs in this process is significantly influenced by the formulation of the prompt, which plays a crucial role in determining the quality and relevance of the generated reports. In this research, we used a combination of two distinct prompting strategies, known as shot prompting and pattern prompting to enhance the performance of automated medical reporting. The evaluation of the automated medical reports is carried out using the ROUGE score and a human evaluation with the help of an expert panel. The two-shot prompting approach in combination with scope and domain context outperforms other methods and achieves the highest score when compared to the human reference set by a general practitioner. However, the automated reports are approximately twice as long as the human references, due to the addition of both redundant and relevant statements that are added to the report",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daphne van Zandvoort",
      "Laura Wiersema",
      "Tom Huibers",
      "Sandra van Dulmen",
      "Sjaak Brinkkemper"
    ]
  },
  "http://arxiv.org/abs/2311.13273": {
    "title": "Comparative Experimentation of Accuracy Metrics in Automated Medical Reporting: The Case of Otitis Consultations",
    "volume": "Nov",
    "abstract": "Generative Artificial Intelligence (AI) can be used to automatically generate medical reports based on transcripts of medical consultations. The aim is to reduce the administrative burden that healthcare professionals face. The accuracy of the generated reports needs to be established to ensure their correctness and usefulness. There are several metrics for measuring the accuracy of AI generated reports, but little work has been done towards the application of these metrics in medical reporting. A comparative experimentation of 10 accuracy metrics has been performed on AI generated medical reports against their corresponding General Practitioner's (GP) medical reports concerning Otitis consultations. The number of missing, incorrect, and additional statements of the generated reports have been correlated with the metric scores. In addition, we introduce and define a Composite Accuracy Score which produces a single score for comparing the metrics within the field of automated medical reporting. Findings show that based on the correlation study and the Composite Accuracy Score, the ROUGE-L and Word Mover's Distance metrics are the preferred metrics, which is not in line with previous work. These findings help determine the accuracy of an AI generated medical report, which aids the development of systems that generate medical reports for GPs to reduce the administrative burden",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wouter Faber",
      "Renske Eline Bootsma",
      "Tom Huibers",
      "Sandra van Dulmen",
      "Sjaak Brinkkemper"
    ]
  },
  "http://arxiv.org/abs/2311.13258": {
    "title": "ViStruct: Visual Structural Knowledge Extraction via Curriculum Guided Code-Vision Representation",
    "volume": "Nov",
    "abstract": "State-of-the-art vision-language models (VLMs) still have limited performance in structural knowledge extraction, such as relations between objects. In this work, we present ViStruct, a training framework to learn VLMs for effective visual structural knowledge extraction. Two novel designs are incorporated. First, we propose to leverage the inherent structure of programming language to depict visual structural information. This approach enables explicit and consistent representation of visual structural information of multiple granularities, such as concepts, relations, and events, in a well-organized structured format. Second, we introduce curriculum-based learning for VLMs to progressively comprehend visual structures, from fundamental visual concepts to intricate event structures. Our intuition is that lower-level knowledge may contribute to complex visual structure understanding. Furthermore, we compile and release a collection of datasets tailored for visual structural knowledge extraction. We adopt a weakly-supervised approach to directly generate visual event structures from captions for ViStruct training, capitalizing on abundant image-caption pairs from the web. In experiments, we evaluate ViStruct on visual structure prediction tasks, demonstrating its effectiveness in improving the understanding of visual structures. The code is public at \\url{https://github.com/Yangyi-Chen/vi-struct}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangyi Chen",
      "Xingyao Wang",
      "Manling Li",
      "Derek Hoiem",
      "Heng Ji"
    ]
  },
  "http://arxiv.org/abs/2311.14741": {
    "title": "@ve: A Chatbot for Latin",
    "volume": "Nov",
    "abstract": "Dead, extinct, and endangered languages have been preserved primarily through audio conservation and the collection and digitization of scripts and have been promoted through targeted language acquisition efforts. Another possibility would be to build conversational agents that can master these languages. This would provide an artificial, active conversational partner which has knowledge of the vocabulary and grammar, and one learns with it in a different way. The chatbot @ve, with which one can communicate in Latin, was developed in 2022/2023 based on GPT-3.0. It was additionally equipped with a manually created knowledge base. After conceptual groundwork, this paper presents the preparation and implementation of the project. In addition, it summarizes the test that a Latin expert conducted with the chatbot. A critical discussion elaborates advantages and disadvantages. @ve could be a new tool for teaching Latin in a memorable and entertaining way through dialogue. However, the present implementation is still too prone to glitches for stand-alone use - i.e., without the accompaniment of a teacher. The use of GPT-4 could be a solution as well as the extension of the knowledge base. In conclusion, it can be argued that conversational agents are an innovative approach to promoting and preserving languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oliver Bendel",
      "Karim N'diaye"
    ]
  },
  "http://arxiv.org/abs/2311.13246": {
    "title": "Automatic Instruction Optimization for Open-source LLM Instruction Tuning",
    "volume": "Nov",
    "abstract": "Instruction tuning is crucial for enabling Language Learning Models (LLMs) in responding to human instructions. The quality of instruction pairs used for tuning greatly affects the performance of LLMs. However, the manual creation of high-quality instruction datasets is costly, leading to the adoption of automatic generation of instruction pairs by LLMs as a popular alternative in the training of open-source LLMs. To ensure the high quality of LLM-generated instruction datasets, several approaches have been proposed. Nevertheless, existing methods either compromise dataset integrity by filtering a large proportion of samples, or are unsuitable for industrial applications. In this paper, instead of discarding low-quality samples, we propose CoachLM, a novel approach to enhance the quality of instruction datasets through automatic revisions on samples in the dataset. CoachLM is trained from the samples revised by human experts and significantly increases the proportion of high-quality samples in the dataset from 17.7% to 78.9%. The effectiveness of CoachLM is further assessed on various real-world instruction test sets. The results show that CoachLM improves the instruction-following capabilities of the instruction-tuned LLM by an average of 29.9%, which even surpasses larger LLMs with nearly twice the number of parameters. Furthermore, CoachLM is successfully deployed in a data management system for LLMs at Huawei, resulting in an efficiency improvement of up to 20% in the cleaning of 40k real-world instruction pairs. We release the training data and code of CoachLM (https://github.com/lunyiliu/CoachLM)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilun Liu",
      "Shimin Tao",
      "Xiaofeng Zhao",
      "Ming Zhu",
      "Wenbing Ma",
      "Junhao Zhu",
      "Chang Su",
      "Yutai Hou",
      "Miao Zhang",
      "Min Zhang",
      "Hongxia Ma",
      "Li Zhang",
      "Hao Yang",
      "Yanfei Jiang"
    ]
  },
  "http://arxiv.org/abs/2311.14740": {
    "title": "AutoKG: Efficient Automated Knowledge Graph Generation for Language Models",
    "volume": "Nov",
    "abstract": "Traditional methods of linking large language models (LLMs) to knowledge bases via the semantic similarity search often fall short of capturing complex relational dynamics. To address these limitations, we introduce AutoKG, a lightweight and efficient approach for automated knowledge graph (KG) construction. For a given knowledge base consisting of text blocks, AutoKG first extracts keywords using a LLM and then evaluates the relationship weight between each pair of keywords using graph Laplace learning. We employ a hybrid search scheme combining vector similarity and graph-based associations to enrich LLM responses. Preliminary experiments demonstrate that AutoKG offers a more comprehensive and interconnected knowledge retrieval mechanism compared to the semantic similarity search, thereby enhancing the capabilities of LLMs in generating more insightful and relevant outputs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bohan Chen",
      "Andrea L. Bertozzi"
    ]
  },
  "http://arxiv.org/abs/2311.13240": {
    "title": "On the Calibration of Large Language Models and Alignment",
    "volume": "Nov",
    "abstract": "As large language models attract increasing attention and find widespread application, concurrent challenges of reliability also arise at the same time. Confidence calibration, an effective analysis method for gauging the reliability of deep models, serves as a crucial tool for assessing and improving their reliability. However, such investigation has been comparatively underexplored. In this work, we conduct a systematic examination of the calibration of aligned language models throughout the entire construction process, including pretraining and alignment training. At each stage, we investigate how different training settings, such as parameter scales and training data, affect model calibration. To thoroughly assess model calibration, we evaluate models on three most concerned aspects: generation, factuality and understanding. Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chiwei Zhu",
      "Benfeng Xu",
      "Quan Wang",
      "Yongdong Zhang",
      "Zhendong Mao"
    ]
  },
  "http://arxiv.org/abs/2311.13230": {
    "title": "Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus",
    "volume": "Nov",
    "abstract": "Large Language Models (LLMs) have gained significant popularity for their impressive performance across diverse fields. However, LLMs are prone to hallucinate untruthful or nonsensical outputs that fail to meet user expectations in many real-world applications. Existing works for detecting hallucinations in LLMs either rely on external knowledge for reference retrieval or require sampling multiple responses from the LLM for consistency verification, making these methods costly and inefficient. In this paper, we propose a novel reference-free, uncertainty-based method for detecting hallucinations in LLMs. Our approach imitates human focus in factuality checking from three aspects: 1) focus on the most informative and important keywords in the given text; 2) focus on the unreliable tokens in historical context which may lead to a cascade of hallucinations; and 3) focus on the token properties such as token type and token frequency. Experimental results on relevant datasets demonstrate the effectiveness of our proposed method, which achieves state-of-the-art performance across all the evaluation metrics and eliminates the need for additional information",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhang Zhang",
      "Lin Qiu",
      "Qipeng Guo",
      "Cheng Deng",
      "Yue Zhang",
      "Zheng Zhang",
      "Chenghu Zhou",
      "Xinbing Wang",
      "Luoyi Fu"
    ]
  },
  "http://arxiv.org/abs/2311.13184": {
    "title": "AS-LLM: When Algorithm Selection Meets Large Language Model",
    "volume": "Nov",
    "abstract": "Algorithm selection aims to identify the most suitable algorithm for solving a specific problem before execution, which has become a critical process of the AutoML. Current mainstream algorithm selection techniques rely heavily on feature representations of various problems and employ the performance of each algorithm as supervised information. However, there is a significant research gap concerning the consideration of algorithm features. This gap is primarily attributed to the inherent complexity of algorithms, making it particularly challenging to find a universally effective feature extraction method that is applicable across a diverse range of algorithms. Unfortunately, neglecting this aspect undoubtedly impacts the accuracy of algorithm selection and indirectly necessitates an increased volume of problem data for training purposes. This paper takes a significant stride towards addressing this gap by proposing an approach that integrates algorithm representation into the algorithm selection process. Specifically, our proposed model employs distinct modules to extract representations of both problems and algorithms, where the algorithm representation leverages the capabilities of pre-trained LLMs in the realm of code comprehension. Following the extraction of embedding vectors for both algorithms and problems, the most suitable algorithm is determined through calculations of matching degrees. Our experiments not only validate the effectiveness of the proposed model but also showcase the performance of different embedded pre-trained LLMs, which suggests that the proposed algorithm selection framework holds the potential to serve as a baseline task for evaluating the code representation capabilities of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Wu",
      "Yan Zhong",
      "Jibin Wu",
      "Kay Chen Tan"
    ]
  },
  "http://arxiv.org/abs/2311.13171": {
    "title": "ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization",
    "volume": "Nov",
    "abstract": "Parameter-efficient fine-tuning (PEFT) techniques make it possible to efficiently adapt a language model to create \"expert\" models that specialize to new tasks or domains. Recent techniques in model merging and compositional generalization leverage these expert models by dynamically composing modules to improve zero/few-shot generalization. Despite the efficiency of PEFT methods, the size of expert models can make it onerous to retrieve expert models per query over high-latency networks like the Internet or serve multiple experts on a single GPU. To address these issues, we present ComPEFT, a novel method for compressing fine-tuning residuals (task vectors) of PEFT based models. ComPEFT employs sparsification and ternary quantization to reduce the size of the PEFT module without performing any additional retraining while preserving or enhancing model performance. In extensive evaluation across T5, T0, and LLaMA-based models with 200M - 65B parameters, ComPEFT achieves compression ratios of 8x - 50x. In particular, we show that ComPEFT improves with scale - stronger models exhibit higher compressibility and better performance. For example, we show that ComPEFT applied to LLaMA outperforms QLoRA by 4.16% on MMLU with a storage size reduction of up to 26x. In addition, we show that the compressed experts produced by ComPEFT maintain few-shot compositional generalization capabilities, facilitate efficient communication and computation, and exhibit enhanced performance when merged. Lastly, we provide an analysis of different method components, compare it with other PEFT methods, and test ComPEFT's efficacy for compressing the residual of full-finetuning. Our code is available at https://github.com/prateeky2806/compeft",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prateek Yadav",
      "Leshem Choshen",
      "Colin Raffel",
      "Mohit Bansal"
    ]
  },
  "http://arxiv.org/abs/2311.13133": {
    "title": "LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigms",
    "volume": "Nov",
    "abstract": "Large Language Models are traditionally finetuned on large instruction datasets. However recent studies suggest that small, high-quality datasets can suffice for general purpose instruction following. This lack of consensus surrounding finetuning best practices is in part due to rapidly diverging approaches to LLM evaluation. In this study, we ask whether a small amount of diverse finetuning samples can improve performance on both traditional perplexity-based NLP benchmarks, and on open-ended, model-based evaluation. We finetune open-source MPT-7B and MPT-30B models on instruction finetuning datasets of various sizes ranging from 1k to 60k samples. We find that subsets of 1k-6k instruction finetuning samples are sufficient to achieve good performance on both (1) traditional NLP benchmarks and (2) model-based evaluation. Finally, we show that mixing textbook-style and open-ended QA finetuning datasets optimizes performance on both evaluation paradigms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditi Jha",
      "Sam Havens",
      "Jeremey Dohmann",
      "Alex Trott",
      "Jacob Portes"
    ]
  },
  "http://arxiv.org/abs/2311.16173": {
    "title": "Conditions for Length Generalization in Learning Reasoning Skills",
    "volume": "Nov",
    "abstract": "Reasoning is a fundamental capability of AI agents. Recently, large language models (LLMs) have shown remarkable abilities to perform reasoning tasks. However, numerous evaluations of the reasoning capabilities of LLMs have also showed some limitations. An outstanding limitation is length generalization, meaning that when trained on reasoning problems of smaller lengths or sizes, the resulting models struggle with problems of larger sizes or lengths. This potentially indicates some theoretical limitations of generalization in learning reasoning skills. These evaluations and their observations motivated us to perform a theoretical study of the length generalization problem. This work focused on reasoning tasks that can be formulated as Markov dynamic processes (MDPs) and/or directed acyclic graphs (DAGs). It identifies and proves conditions that decide whether the length generalization problem can be solved or not for a reasoning task in a particular representation. Experiments are also conducted to verify the theoretical results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changnan Xiao",
      "Bing Liu"
    ]
  },
  "http://arxiv.org/abs/2311.13126": {
    "title": "Towards Better Parameter-Efficient Fine-Tuning for Large Language Models: A Position Paper",
    "volume": "Nov",
    "abstract": "This paper delves into the pressing need in Parameter-Efficient Fine-Tuning (PEFT) for Large Language Models (LLMs). While LLMs possess remarkable capabilities, their extensive parameter requirements and associated computational demands hinder their practicality and scalability for real-world applications. Our position paper highlights current states and the necessity of further studying into the topic, and recognizes significant challenges and open issues that must be addressed to fully harness the powerful abilities of LLMs. These challenges encompass novel efficient PEFT architectures, PEFT for different learning settings, PEFT combined with model compression techniques, and the exploration of PEFT for multi-modal LLMs. By presenting this position paper, we aim to stimulate further research and foster discussions surrounding more efficient and accessible PEFT for LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengyu Wang",
      "Junbing Yan",
      "Wei Zhang",
      "Jun Huang"
    ]
  },
  "http://arxiv.org/abs/2311.13118": {
    "title": "Combatting Human Trafficking in the Cyberspace: A Natural Language Processing-Based Methodology to Analyze the Language in Online Advertisements",
    "volume": "Nov",
    "abstract": "This project tackles the pressing issue of human trafficking in online C2C marketplaces through advanced Natural Language Processing (NLP) techniques. We introduce a novel methodology for generating pseudo-labeled datasets with minimal supervision, serving as a rich resource for training state-of-the-art NLP models. Focusing on tasks like Human Trafficking Risk Prediction (HTRP) and Organized Activity Detection (OAD), we employ cutting-edge Transformer models for analysis. A key contribution is the implementation of an interpretability framework using Integrated Gradients, providing explainable insights crucial for law enforcement. This work not only fills a critical gap in the literature but also offers a scalable, machine learning-driven approach to combat human exploitation online. It serves as a foundation for future research and practical applications, emphasizing the role of machine learning in addressing complex social issues",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alejandro Rodriguez Perez",
      "Pablo Rivas"
    ]
  },
  "http://arxiv.org/abs/2311.13110": {
    "title": "White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?",
    "volume": "Nov",
    "abstract": "In this paper, we contend that a natural objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a low-dimensional Gaussian mixture supported on incoherent subspaces. The goodness of such a representation can be evaluated by a principled measure, called sparse rate reduction, that simultaneously maximizes the intrinsic information gain and extrinsic sparsity of the learned representation. From this perspective, popular deep network architectures, including transformers, can be viewed as realizing iterative schemes to optimize this measure. Particularly, we derive a transformer block from alternating optimization on parts of this objective: the multi-head self-attention operator compresses the representation by implementing an approximate gradient descent step on the coding rate of the features, and the subsequent multi-layer perceptron sparsifies the features. This leads to a family of white-box transformer-like deep network architectures, named CRATE, which are mathematically fully interpretable. We show, by way of a novel connection between denoising and compression, that the inverse to the aforementioned compressive encoding can be realized by the same class of CRATE architectures. Thus, the so-derived white-box architectures are universal to both encoders and decoders. Experiments show that these networks, despite their simplicity, indeed learn to compress and sparsify representations of large-scale real-world image and text datasets, and achieve performance very close to highly engineered transformer-based models: ViT, MAE, DINO, BERT, and GPT2. We believe the proposed computational framework demonstrates great potential in bridging the gap between theory and practice of deep learning, from a unified perspective of data compression. Code is available at: https://ma-lab-berkeley.github.io/CRATE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaodong Yu",
      "Sam Buchanan",
      "Druv Pai",
      "Tianzhe Chu",
      "Ziyang Wu",
      "Shengbang Tong",
      "Hao Bai",
      "Yuexiang Zhai",
      "Benjamin D. Haeffele",
      "Yi Ma"
    ]
  },
  "http://arxiv.org/abs/2311.13105": {
    "title": "Perceptual Structure in the Absence of Grounding for LLMs: The Impact of Abstractedness and Subjectivity in Color Language",
    "volume": "Nov",
    "abstract": "The need for grounding in language understanding is an active research topic. Previous work has suggested that color perception and color language appear as a suitable test bed to empirically study the problem, given its cognitive significance and showing that there is considerable alignment between a defined color space and the feature space defined by a language model. To further study this issue, we collect a large scale source of colors and their descriptions, containing almost a 1 million examples , and perform an empirical analysis to compare two kinds of alignments: (i) inter-space, by learning a mapping between embedding space and color space, and (ii) intra-space, by means of prompting comparatives between color descriptions. Our results show that while color space alignment holds for monolexemic, highly pragmatic color descriptions, this alignment drops considerably in the presence of examples that exhibit elements of real linguistic usage such as subjectivity and abstractedness, suggesting that grounding may be required in such cases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pablo Loyola",
      "Edison Marrese-Taylor",
      "Andres Hoyos-Idobro"
    ]
  },
  "http://arxiv.org/abs/2311.13102": {
    "title": "Detecting out-of-distribution text using topological features of transformer-based language models",
    "volume": "Nov",
    "abstract": "We attempt to detect out-of-distribution (OOD) text samples though applying Topological Data Analysis (TDA) to attention maps in transformer-based language models. We evaluate our proposed TDA-based approach for out-of-distribution detection on BERT, a transformer-based language model, and compare the to a more traditional OOD approach based on BERT CLS embeddings. We found that our TDA approach outperforms the CLS embedding approach at distinguishing in-distribution data (politics and entertainment news articles from HuffPost) from far out-of-domain samples (IMDB reviews), but its effectiveness deteriorates with near out-of-domain (CNN/Dailymail) or same-domain (business news articles from HuffPost) datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andres Pollano",
      "Anupam Chaudhuri",
      "Anj Simmons"
    ]
  },
  "http://arxiv.org/abs/2311.13095": {
    "title": "Enhancing Logical Reasoning in Large Language Models to Facilitate Legal Applications",
    "volume": "Nov",
    "abstract": "Language serves as a vehicle for conveying thought, enabling communication among individuals. The ability to distinguish between diverse concepts, identify fairness and injustice, and comprehend a range of legal notions fundamentally relies on logical reasoning. Large Language Models (LLMs) attempt to emulate human language understanding and generation, but their competency in logical reasoning remains limited. This paper seeks to address the philosophical question: How can we effectively teach logical reasoning to LLMs while maintaining a deep understanding of the intricate relationship between language and logic? By focusing on bolstering LLMs' capabilities in logical reasoning, we aim to expand their applicability in law and other logic-intensive disciplines. To this end, we propose a Reinforcement Learning from Logical Feedback (RLLF) approach, which serves as a potential framework for refining LLMs' reasoning capacities. Through RLLF and a revised evaluation methodology, we explore new avenues for research in this domain and contribute to the development of LLMs capable of handling complex legal reasoning tasks while acknowledging the fundamental connection between language and logic",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ha-Thanh Nguyen",
      "Wachara Fungwacharakorn",
      "Ken Satoh"
    ]
  },
  "http://arxiv.org/abs/2311.14737": {
    "title": "Positional Description Matters for Transformers Arithmetic",
    "volume": "Nov",
    "abstract": "Transformers, central to the successes in modern Natural Language Processing, often falter on arithmetic tasks despite their vast capabilities --which paradoxically include remarkable coding abilities. We observe that a crucial challenge is their naive reliance on positional information to solve arithmetic problems with a small number of digits, leading to poor performance on larger numbers. Herein, we delve deeper into the role of positional encoding, and propose several ways to fix the issue, either by modifying the positional encoding directly, or by modifying the representation of the arithmetic task to leverage standard positional encoding differently. We investigate the value of these modifications for three tasks: (i) classical multiplication, (ii) length extrapolation in addition, and (iii) addition in natural language context. For (i) we train a small model on a small dataset (100M parameters and 300k samples) with remarkable aptitude in (direct, no scratchpad) 15 digits multiplication and essentially perfect up to 12 digits, while usual training in this context would give a model failing at 4 digits multiplication. In the experiments on addition, we use a mere 120k samples to demonstrate: for (ii) extrapolation from 10 digits to testing on 12 digits numbers while usual training would have no extrapolation, and for (iii) almost perfect accuracy up to 5 digits while usual training would be correct only up to 3 digits (which is essentially memorization with a training set of 120k samples)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruoqi Shen",
      "Sbastien Bubeck",
      "Ronen Eldan",
      "Yin Tat Lee",
      "Yuanzhi Li",
      "Yi Zhang"
    ]
  },
  "http://arxiv.org/abs/2311.13061": {
    "title": "Attribution and Alignment: Effects of Local Context Repetition on Utterance Production and Comprehension in Dialogue",
    "volume": "Nov",
    "abstract": "Language models are often used as the backbone of modern dialogue systems. These models are pre-trained on large amounts of written fluent language. Repetition is typically penalised when evaluating language model generations. However, it is a key component of dialogue. Humans use local and partner specific repetitions; these are preferred by human users and lead to more successful communication in dialogue. In this study, we evaluate (a) whether language models produce human-like levels of repetition in dialogue, and (b) what are the processing mechanisms related to lexical re-use they use during comprehension. We believe that such joint analysis of model production and comprehension behaviour can inform the development of cognitively inspired dialogue generation systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aron Molnar",
      "Jaap Jumelet",
      "Mario Giulianelli",
      "Arabella Sinclair"
    ]
  },
  "http://arxiv.org/abs/2311.13053": {
    "title": "Beyond Text: Unveiling Multimodal Proficiency of Large Language Models with MultiAPI Benchmark",
    "volume": "Nov",
    "abstract": "The proliferation of Large Language Models like ChatGPT has significantly advanced language understanding and generation, impacting a broad spectrum of applications. However, these models predominantly excel in text-based tasks, overlooking the complexity of real-world multimodal information. This study introduces MultiAPI, a pioneering comprehensive large-scale API benchmark dataset aimed at expanding LLMs' proficiency in multimodal contexts. Developed collaboratively through ChatGPT, MultiAPI consists of 235 diverse API calls and 2,038 contextual prompts, offering a unique platform evaluation of tool-augmented LLMs handling multimodal tasks. Through comprehensive experiments, our findings reveal that while LLMs demonstrate proficiency in API call decision-making, they face challenges in domain identification, function selection, and argument generation. What's more, we surprisingly notice that auxiliary context can actually impair the performance. An in-depth error analysis paves the way for a new paradigm to address these challenges, suggesting a potential direction for future LLM research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Liu",
      "Jianfeng Lin",
      "Jiawei Zhang"
    ]
  },
  "http://arxiv.org/abs/2311.13029": {
    "title": "Systematic word meta-sense extension",
    "volume": "Nov",
    "abstract": "The meaning of polysemous words often varies in a highly productive yet predictable way. Generalizing the regularity between conventional senses to derive novel word meaning is crucial for automated processing of non-literal language uses such as figurative expressions. We introduce a novel task called systematic word meta-sense extension (SWORME) to test and improve language models' ability to extend word meaning to denote new semantic domains (also called meta-senses) that bear regular semantic relations with existing senses. We found that language models prefer incremental lexical semantic change toward conceptually similar meta-senses such as logical metonymy, and are much worse at predicting highly non-literal meaning extensions such as metaphors. We propose a novel analogy-based method of word meaning extension, and show that it effectively improves language model systematicity in making both gradual and radical types of meta-sense extension. We further demonstrate that learning systematic meta-sense extensions benefits language models on multiple benchmarks of figurative language understanding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Yu"
    ]
  },
  "http://arxiv.org/abs/2311.12986": {
    "title": "Unsupervised Graph Attention Autoencoder for Attributed Networks using K-means Loss",
    "volume": "Nov",
    "abstract": "Multimodal Sentiment Analysis (MSA) has recently become a centric research direction for many real-world applications. This proliferation is due to the fact that opinions are central to almost all human activities and are key influencers of our behaviors. In addition, the recent deployment of Deep Learning-based (DL) models has proven their high efficiency for a wide range of Western languages. In contrast, Arabic DL-based multimodal sentiment analysis (MSA) is still in its infantile stage due, mainly, to the lack of standard datasets. % The contribution In this paper, our investigation is twofold. First, we design a pipeline that helps building our Arabic Multimodal dataset leveraging both state-of-the-art transformers and feature extraction tools within word alignment techniques. Thereafter, we validate our dataset using state-of-the-art transformer-based model dealing with multimodality. Despite the small size of the outcome dataset, experiments show that Arabic multimodality is very promising",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdelfateh Bekkaira",
      "Slimane Bellaouar",
      "Slimane Oulad-Naoui"
    ]
  },
  "http://arxiv.org/abs/2311.12983": {
    "title": "GAIA: a benchmark for General AI Assistants",
    "volume": "Nov",
    "abstract": "We introduce GAIA, a benchmark for General AI Assistants that, if solved, would represent a milestone in AI research. GAIA proposes real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency. GAIA questions are conceptually simple for humans yet challenging for most advanced AIs: we show that human respondents obtain 92\\% vs. 15\\% for GPT-4 equipped with plugins. This notable performance disparity contrasts with the recent trend of LLMs outperforming humans on tasks requiring professional skills in e.g. law or chemistry. GAIA's philosophy departs from the current trend in AI benchmarks suggesting to target tasks that are ever more difficult for humans. We posit that the advent of Artificial General Intelligence (AGI) hinges on a system's capability to exhibit similar robustness as the average human does on such questions. Using GAIA's methodology, we devise 466 questions and their answer. We release our questions while retaining answers to 300 of them to power a leader-board available at https://huggingface.co/gaia-benchmark",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Grgoire Mialon",
      "Clmentine Fourrier",
      "Craig Swift",
      "Thomas Wolf",
      "Yann LeCun",
      "Thomas Scialom"
    ]
  },
  "http://arxiv.org/abs/2311.14736": {
    "title": "Data Diversity Matters for Robust Instruction Tuning",
    "volume": "Nov",
    "abstract": "Instruction tuning has emerged as a key step in aligning large language models. One of the central challenges of instruction tuning is dataset selection, as the composition of the instruction tuning dataset can significantly impact downstream performance. In particular, researchers have hypothesized that dataset diversity and dataset quality are important indicators of downstream performance. However, it is not clear how to automatically select high quality and diverse data or how exactly quality and diversity affect instruction following ability. To resolve these issues, we propose a new algorithm, Quality-Diversity Instruction Tuning (QDIT). QDIT provides a principled algorithm to control dataset diversity and quality, allowing us to conduct an in depth study on the effect of diversity and quality on instruction tuning performance. From this study we draw two key insights (1) there is a natural tradeoff between dataset diversity and quality and (2) increasing dataset diversity significantly improves the worst case instruction following performance, therefore improving robustness. We validate the performance of QDIT on several large scale instruction tuning datasets, where we find it can improve worst case performance by 18% while maintaining or improving average performance compared to quality driven baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Bukharin",
      "Tuo Zhao"
    ]
  },
  "http://arxiv.org/abs/2311.14743": {
    "title": "A Baseline Analysis of Reward Models' Ability To Accurately Analyze Foundation Models Under Distribution Shift",
    "volume": "Nov",
    "abstract": "Foundation models, specifically Large Language Models (LLM's), have lately gained wide-spread attention and adoption. Reinforcement Learning with Human Feedback (RLHF) involves training a reward model to capture desired behaviors, which is then used to align an LLM. These reward models are additionally used at inference-time to estimate how well LLM responses adhere to those desired behaviors. However, there is little work measuring how robust these reward models are to distribution shifts. In this work, we evaluate how reward model performance - measured via accuracy and calibration (i.e. alignment between accuracy and confidence) - is affected by distribution shift. We show novel calibration patterns and accuracy drops due to OOD prompts and responses, and that the reward model is more sensitive to shifts in responses than prompts. Additionally, we adapt an OOD detection technique commonly used in classification to the reward model setting in order to detect these distribution shifts in prompts and responses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Pikus",
      "Will LeVine",
      "Tony Chen",
      "Sean Hendryx"
    ]
  },
  "http://arxiv.org/abs/2311.12735": {
    "title": "LowResource at BLP-2023 Task 2: Leveraging BanglaBert for Low Resource Sentiment Analysis of Bangla Language",
    "volume": "Nov",
    "abstract": "This paper describes the system of the LowResource Team for Task 2 of BLP-2023, which involves conducting sentiment analysis on a dataset composed of public posts and comments from diverse social media platforms. Our primary aim is to utilize BanglaBert, a BERT model pre-trained on a large Bangla corpus, using various strategies including fine-tuning, dropping random tokens, and using several external datasets. Our final model is an ensemble of the three best BanglaBert variations. Our system has achieved overall 3rd in the Test Set among 30 participating teams with a score of 0.718. Additionally, we discuss the promising systems that didn't perform well namely task-adaptive pertaining and paraphrasing using BanglaT5. Training codes and external datasets which are used for our system are publicly available at https://github.com/Aunabil4602/bnlp-workshop-task2-2023",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aunabil Chakma",
      "Masum Hasan"
    ]
  },
  "http://arxiv.org/abs/2311.12727": {
    "title": "Soft Random Sampling: A Theoretical and Empirical Analysis",
    "volume": "Nov",
    "abstract": "Soft random sampling (SRS) is a simple yet effective approach for efficient training of large-scale deep neural networks when dealing with massive data. SRS selects a subset uniformly at random with replacement from the full data set in each epoch. In this paper, we conduct a theoretical and empirical analysis of SRS. First, we analyze its sampling dynamics including data coverage and occupancy. Next, we investigate its convergence with non-convex objective functions and give the convergence rate. Finally, we provide its generalization performance. We empirically evaluate SRS for image recognition on CIFAR10 and automatic speech recognition on Librispeech and an in-house payload dataset to demonstrate its effectiveness. Compared to existing coreset-based data selection methods, SRS offers a better accuracy-efficiency trade-off. Especially on real-world industrial scale data sets, it is shown to be a powerful training strategy with significant speedup and competitive performance with almost no additional computing cost",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaodong Cui",
      "Ashish Mittal",
      "Songtao Lu",
      "Wei Zhang",
      "George Saon",
      "Brian Kingsbury"
    ]
  },
  "http://arxiv.org/abs/2311.12707": {
    "title": "Keeping Users Engaged During Repeated Administration of the Same Questionnaire: Using Large Language Models to Reliably Diversify Questions",
    "volume": "Nov",
    "abstract": "Standardized, validated questionnaires are vital tools in HCI research and healthcare, offering dependable self-report data. However, their repeated use in longitudinal or pre-post studies can induce respondent fatigue, impacting data quality via response biases and decreased response rates. We propose utilizing large language models (LLMs) to generate diverse questionnaire versions while retaining good psychometric properties. In a longitudinal study, participants engaged with our agent system and responded daily for two weeks to either a standardized depression questionnaire or one of two LLM-generated questionnaire variants, alongside a validated depression questionnaire. Psychometric testing revealed consistent covariation between the external criterion and the focal measure administered across the three conditions, demonstrating the reliability and validity of the LLM-generated variants. Participants found the repeated administration of the standardized questionnaire significantly more repetitive compared to the variants. Our findings highlight the potential of LLM-generated variants to invigorate questionnaires, fostering engagement and interest without compromising validity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hye Sun Yun",
      "Mehdi Arjmand",
      "Phillip Raymond Sherlock",
      "Michael Paasche-Orlow",
      "James W. Griffith",
      "Timothy Bickmore"
    ]
  },
  "http://arxiv.org/abs/2311.12699": {
    "title": "Can Large Language Models Understand Content and Propagation for Misinformation Detection: An Empirical Study",
    "volume": "Nov",
    "abstract": "Large Language Models (LLMs) have garnered significant attention for their powerful ability in natural language understanding and reasoning. In this paper, we present a comprehensive empirical study to explore the performance of LLMs on misinformation detection tasks. This study stands as the pioneering investigation into the understanding capabilities of multiple LLMs regarding both content and propagation across social media platforms. Our empirical studies on five misinformation detection datasets show that LLMs with diverse prompts achieve comparable performance in text-based misinformation detection but exhibit notably constrained capabilities in comprehending propagation structure compared to existing models in propagation-based misinformation detection. Besides, we further design four instruction-tuned strategies to enhance LLMs for both content and propagation-based misinformation detection. These strategies boost LLMs to actively learn effective features from multiple instances or hard instances, and eliminate irrelevant propagation structures, thereby achieving better detection performance. Extensive experiments further demonstrate LLMs would play a better capacity in content and propagation structure under these proposed strategies and achieve promising detection performance. These findings highlight the potential ability of LLMs to detect misinformation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyang Chen",
      "Lingwei Wei",
      "Han Cao",
      "Wei Zhou",
      "Songlin Hu"
    ]
  },
  "http://arxiv.org/abs/2311.12689": {
    "title": "Fair Text Classification with Wasserstein Independence",
    "volume": "Nov",
    "abstract": "Group fairness is a central research topic in text classification, where reaching fair treatment between sensitive groups (e.g. women vs. men) remains an open challenge. This paper presents a novel method for mitigating biases in neural text classification, agnostic to the model architecture. Considering the difficulty to distinguish fair from unfair information in a text encoder, we take inspiration from adversarial training to induce Wasserstein independence between representations learned to predict our target label and the ones learned to predict some sensitive attribute. Our approach provides two significant advantages. Firstly, it does not require annotations of sensitive attributes in both testing and training data. This is more suitable for real-life scenarios compared to existing methods that require annotations of sensitive attributes at train time. Second, our approach exhibits a comparable or better fairness-accuracy trade-off compared to existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thibaud Leteno",
      "Antoine Gourru",
      "Charlotte Laclau",
      "Rmi Emonet",
      "Christophe Gravier"
    ]
  },
  "http://arxiv.org/abs/2311.12664": {
    "title": "The DURel Annotation Tool: Human and Computational Measurement of Semantic Proximity, Sense Clusters and Semantic Change",
    "volume": "Nov",
    "abstract": "We present the DURel tool that implements the annotation of semantic proximity between uses of words into an online, open source interface. The tool supports standardized human annotation as well as computational annotation, building on recent advances with Word-in-Context models. Annotator judgments are clustered with automatic graph clustering techniques and visualized for analysis. This allows to measure word senses with simple and intuitive micro-task judgments between use pairs, requiring minimal preparation efforts. The tool offers additional functionalities to compare the agreement between annotators to guarantee the inter-subjectivity of the obtained judgments and to calculate summary statistics giving insights into sense frequency distributions, semantic variation or changes of senses over time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Schlechtweg",
      "Shafqat Mumtaz Virk",
      "Pauline Sander",
      "Emma Skldberg",
      "Lukas Theuer Linke",
      "Tuo Zhang",
      "Nina Tahmasebi",
      "Jonas Kuhn",
      "Sabine Schulte im Walde"
    ]
  },
  "http://arxiv.org/abs/2311.12649": {
    "title": "MathGloss: Building mathematical glossaries from text",
    "volume": "Nov",
    "abstract": "MathGloss is a project to create a knowledge graph (KG) for undergraduate mathematics from text, automatically, using modern natural language processing (NLP) tools and resources already available on the web. MathGloss is a linked database of undergraduate concepts in mathematics. So far, it combines five resources: (i) Wikidata, a collaboratively edited, multilingual knowledge graph hosted by the Wikimedia Foundation, (ii) terms covered in mathematics courses at the University of Chicago, (iii) the syllabus of the French undergraduate mathematics curriculum which includes hyperlinks to the automated theorem prover Lean 4, (iv) MuLiMa, a multilingual dictionary of mathematics curated by mathematicians, and (v) the nLab, a wiki for category theory also curated by mathematicians. MathGloss's goal is to bring together resources for learning mathematics and to allow every mathematician to tailor their learning to their own preferences. Moreover, by organizing different resources for learning undergraduate mathematics alongside those for learning formal mathematics, we hope to make it easier for mathematicians and formal tools (theorem provers, computer algebra systems, etc) experts to \"understand\" each other and break down some of the barriers to formal math",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucy Horowitz",
      "Valeria de Paiva"
    ]
  },
  "http://arxiv.org/abs/2311.12574": {
    "title": "IMGTB: A Framework for Machine-Generated Text Detection Benchmarking",
    "volume": "Nov",
    "abstract": "In the era of large language models generating high quality texts, it is a necessity to develop methods for detection of machine-generated text to avoid harmful use or simply due to annotation purposes. It is, however, also important to properly evaluate and compare such developed methods. Recently, a few benchmarks have been proposed for this purpose; however, integration of newest detection methods is rather challenging, since new methods appear each month and provide slightly different evaluation pipelines. In this paper, we present the IMGTB framework, which simplifies the benchmarking of machine-generated text detection methods by easy integration of custom (new) methods and evaluation datasets. Its configurability and flexibility makes research and development of new detection methods easier, especially their comparison to the existing state-of-the-art detectors. The default set of analyses, metrics and visualizations offered by the tool follows the established practices of machine-generated text detection benchmarking found in state-of-the-art literature",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michal Spiegel",
      "Dominik Macko"
    ]
  },
  "http://arxiv.org/abs/2311.12538": {
    "title": "In-Context Learning Functions with Varying Number of Minima",
    "volume": "Nov",
    "abstract": "Large Language Models (LLMs) have proven effective at In-Context Learning (ICL), an ability that allows them to create predictors from labeled examples. Few studies have explored the interplay between ICL and specific properties of functions it attempts to approximate. In our study, we use a formal framework to explore ICL and propose a new task of approximating functions with varying number of minima. We implement a method that allows for producing functions with given inputs as minima. We find that increasing the number of minima degrades ICL performance. At the same time, our evaluation shows that ICL outperforms 2-layer Neural Network (2NN) model. Furthermore, ICL learns faster than 2NN in all settings. We validate the findings through a set of few-shot experiments across various hyperparameter configurations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Oniani",
      "Yanshan Wang"
    ]
  },
  "http://arxiv.org/abs/2311.12537": {
    "title": "Oasis: Data Curation and Assessment System for Pretraining of Large Language Models",
    "volume": "Nov",
    "abstract": "Data is one of the most critical elements in building a large language model. However, existing systems either fail to customize a corpus curation pipeline or neglect to leverage comprehensive corpus assessment for iterative optimization of the curation. To this end, we present a pretraining corpus curation and assessment platform called Oasis -- a one-stop system for data quality improvement and quantification with user-friendly interactive interfaces. Specifically, the interactive modular rule filter module can devise customized rules according to explicit feedback. The debiased neural filter module builds the quality classification dataset in a negative-centric manner to remove the undesired bias. The adaptive document deduplication module could execute large-scale deduplication with limited memory resources. These three parts constitute the customized data curation module. And in the holistic data assessment module, a corpus can be assessed in local and global views, with three evaluation means including human, GPT-4, and heuristic metrics. We exhibit a complete process to use Oasis for the curation and assessment of pretraining data. In addition, an 800GB bilingual corpus curated by Oasis is publicly released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Zhou",
      "Yubo Chen",
      "Pengfei Cao",
      "Kang Liu",
      "Jun Zhao",
      "Shengping Liu"
    ]
  },
  "http://arxiv.org/abs/2311.12534": {
    "title": "Evaluation Metrics of Language Generation Models for Synthetic Traffic Generation Tasks",
    "volume": "Nov",
    "abstract": "Many Natural Language Generation (NLG) tasks aim to generate a single output text given an input prompt. Other settings require the generation of multiple texts, e.g., for Synthetic Traffic Generation (STG). This generation task is crucial for training and evaluating QA systems as well as conversational agents, where the goal is to generate multiple questions or utterances resembling the linguistic variability of real users. In this paper, we show that common NLG metrics, like BLEU, are not suitable for evaluating STG. We propose and evaluate several metrics designed to compare the generated traffic to the distribution of real user texts. We validate our metrics with an automatic procedure to verify whether they capture different types of quality issues of generated data; we also run human annotations to verify the correlation with human judgements. Experiments on three tasks, i.e., Shopping Utterance Generation, Product Question Generation and Query Auto Completion, demonstrate that our metrics are effective for evaluating STG tasks, and improve the agreement with human judgement up to 20% with respect to common NLG metrics. We believe these findings can pave the way towards better solutions for estimating the representativeness of synthetic text data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simone Filice",
      "Jason Ingyu Choi",
      "Giuseppe Castellucci",
      "Eugene Agichtein",
      "Oleg Rokhlenko"
    ]
  },
  "http://arxiv.org/abs/2311.12489": {
    "title": "Multilingual Word Embeddings for Low-Resource Languages using Anchors and a Chain of Related Languages",
    "volume": "Nov",
    "abstract": "Very low-resource languages, having only a few million tokens worth of data, are not well-supported by multilingual NLP approaches due to poor quality cross-lingual word representations. Recent work showed that good cross-lingual performance can be achieved if a source language is related to the low-resource target language. However, not all language pairs are related. In this paper, we propose to build multilingual word embeddings (MWEs) via a novel language chain-based approach, that incorporates intermediate related languages to bridge the gap between the distant source and target. We build MWEs one language at a time by starting from the resource rich source and sequentially adding each language in the chain till we reach the target. We extend a semi-joint bilingual approach to multiple languages in order to eliminate the main weakness of previous works, i.e., independently trained monolingual embeddings, by anchoring the target language around the multilingual space. We evaluate our method on bilingual lexicon induction for 4 language families, involving 4 very low-resource (<5M tokens) and 4 moderately low-resource (<50M) target languages, showing improved performance in both categories. Additionally, our analysis reveals the importance of good quality embeddings for intermediate languages as well as the importance of leveraging anchor points from all languages in the multilingual space",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Viktor Hangya",
      "Silvia Severini",
      "Radoslav Ralev",
      "Alexander Fraser",
      "Hinrich Schtze"
    ]
  },
  "http://arxiv.org/abs/2311.12480": {
    "title": "Speaker-Adapted End-to-End Visual Speech Recognition for Continuous Spanish",
    "volume": "Nov",
    "abstract": "Different studies have shown the importance of visual cues throughout the speech perception process. In fact, the development of audiovisual approaches has led to advances in the field of speech technologies. However, although noticeable results have recently been achieved, visual speech recognition remains an open research problem. It is a task in which, by dispensing with the auditory sense, challenges such as visual ambiguities and the complexity of modeling silence must be faced. Nonetheless, some of these challenges can be alleviated when the problem is approached from a speaker-dependent perspective. Thus, this paper studies, using the Spanish LIP-RTVE database, how the estimation of specialized end-to-end systems for a specific person could affect the quality of speech recognition. First, different adaptation strategies based on the fine-tuning technique were proposed. Then, a pre-trained CTC/Attention architecture was used as a baseline throughout our experiments. Our findings showed that a two-step fine-tuning process, where the VSR system is first adapted to the task domain, provided significant improvements when the speaker adaptation was addressed. Furthermore, results comparable to the current state of the art were reached even when only a limited amount of data was available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Gimeno-Gmez",
      "Carlos-D. Martnez-Hinarejos"
    ]
  },
  "http://arxiv.org/abs/2311.12475": {
    "title": "PhayaThaiBERT: Enhancing a Pretrained Thai Language Model with Unassimilated Loanwords",
    "volume": "Nov",
    "abstract": "While WangchanBERTa has become the de facto standard in transformer-based Thai language modeling, it still has shortcomings in regard to the understanding of foreign words, most notably English words, which are often borrowed without orthographic assimilation into Thai in many contexts. We identify the lack of foreign vocabulary in WangchanBERTa's tokenizer as the main source of these shortcomings. We then expand WangchanBERTa's vocabulary via vocabulary transfer from XLM-R's pretrained tokenizer and pretrain a new model using the expanded tokenizer, starting from WangchanBERTa's checkpoint, on a new dataset that is larger than the one used to train WangchanBERTa. Our results show that our new pretrained model, PhayaThaiBERT, outperforms WangchanBERTa in many downstream tasks and datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Panyut Sriwirote",
      "Jalinee Thapiang",
      "Vasan Timtong",
      "Attapol T. Rutherford"
    ]
  },
  "http://arxiv.org/abs/2311.12474": {
    "title": "CSMeD: Bridging the Dataset Gap in Automated Citation Screening for Systematic Literature Reviews",
    "volume": "Nov",
    "abstract": "Systematic literature reviews (SLRs) play an essential role in summarising, synthesising and validating scientific evidence. In recent years, there has been a growing interest in using machine learning techniques to automate the identification of relevant studies for SLRs. However, the lack of standardised evaluation datasets makes comparing the performance of such automated literature screening systems difficult. In this paper, we analyse the citation screening evaluation datasets, revealing that many of the available datasets are either too small, suffer from data leakage or have limited applicability to systems treating automated literature screening as a classification task, as opposed to, for example, a retrieval or question-answering task. To address these challenges, we introduce CSMeD, a meta-dataset consolidating nine publicly released collections, providing unified access to 325 SLRs from the fields of medicine and computer science. CSMeD serves as a comprehensive resource for training and evaluating the performance of automated citation screening models. Additionally, we introduce CSMeD-FT, a new dataset designed explicitly for evaluating the full text publication screening task. To demonstrate the utility of CSMeD, we conduct experiments and establish baselines on new datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wojciech Kusa",
      "Oscar E. Mendoza",
      "Matthias Samwald",
      "Petr Knoth",
      "Allan Hanbury"
    ]
  },
  "http://arxiv.org/abs/2311.12468": {
    "title": "Analysis of Visual Features for Continuous Lipreading in Spanish",
    "volume": "Nov",
    "abstract": "During a conversation, our brain is responsible for combining information obtained from multiple senses in order to improve our ability to understand the message we are perceiving. Different studies have shown the importance of presenting visual information in these situations. Nevertheless, lipreading is a complex task whose objective is to interpret speech when audio is not available. By dispensing with a sense as crucial as hearing, it will be necessary to be aware of the challenge that this lack presents. In this paper, we propose an analysis of different speech visual features with the intention of identifying which of them is the best approach to capture the nature of lip movements for natural Spanish and, in this way, dealing with the automatic visual speech recognition task. In order to estimate our system, we present an audiovisual corpus compiled from a subset of the RTVE database, which has been used in the Albayz\\'in evaluations. We employ a traditional system based on Hidden Markov Models with Gaussian Mixture Models. Results show that, although the task is difficult, in restricted conditions we obtain recognition results which determine that using eigenlips in combination with deep features is the best visual approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Gimeno-Gmez",
      "Carlos-D. Martnez-Hinarejos"
    ]
  },
  "http://arxiv.org/abs/2311.12457": {
    "title": "LIP-RTVE: An Audiovisual Database for Continuous Spanish in the Wild",
    "volume": "Nov",
    "abstract": "Speech is considered as a multi-modal process where hearing and vision are two fundamentals pillars. In fact, several studies have demonstrated that the robustness of Automatic Speech Recognition systems can be improved when audio and visual cues are combined to represent the nature of speech. In addition, Visual Speech Recognition, an open research problem whose purpose is to interpret speech by reading the lips of the speaker, has been a focus of interest in the last decades. Nevertheless, in order to estimate these systems in the currently Deep Learning era, large-scale databases are required. On the other hand, while most of these databases are dedicated to English, other languages lack sufficient resources. Thus, this paper presents a semi-automatically annotated audiovisual database to deal with unconstrained natural Spanish, providing 13 hours of data extracted from Spanish television. Furthermore, baseline results for both speaker-dependent and speaker-independent scenarios are reported using Hidden Markov Models, a traditional paradigm that has been widely used in the field of Speech Technologies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Gimeno-Gmez",
      "Carlos-D. Martnez-Hinarejos"
    ]
  },
  "http://arxiv.org/abs/2311.12420": {
    "title": "How Far Have We Gone in Vulnerability Detection Using Large Language Models",
    "volume": "Nov",
    "abstract": "As software becomes increasingly complex and prone to vulnerabilities, automated vulnerability detection is critically important, yet challenging. Given the significant successes of Large Language Models (LLMs) in various tasks, there is growing anticipation of their efficacy in vulnerability detection. However, a quantitative understanding of their potential in vulnerability detection is still missing. To bridge this gap, we introduce a comprehensive vulnerability benchmark VulBench. This benchmark aggregates high-quality data from a wide range of CTF (Capture-the-Flag) challenges and real-world applications, with annotations for each vulnerable function detailing the vulnerability type and its root cause. Through our experiments encompassing 16 LLMs and 6 state-of-the-art (SOTA) deep learning-based models and static analyzers, we find that several LLMs outperform traditional deep learning approaches in vulnerability detection, revealing an untapped potential in LLMs. This work contributes to the understanding and utilization of LLMs for enhanced software security",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Gao",
      "Hao Wang",
      "Yuchen Zhou",
      "Wenyu Zhu",
      "Chao Zhang"
    ]
  },
  "http://arxiv.org/abs/2311.12418": {
    "title": "Visual Analytics for Generative Transformer Models",
    "volume": "Nov",
    "abstract": "While transformer-based models have achieved state-of-the-art results in a variety of classification and generation tasks, their black-box nature makes them challenging for interpretability. In this work, we present a novel visual analytical framework to support the analysis of transformer-based generative networks. In contrast to previous work, which has mainly focused on encoder-based models, our framework is one of the first dedicated to supporting the analysis of transformer-based encoder-decoder models and decoder-only models for generative and classification tasks. Hence, we offer an intuitive overview that allows the user to explore different facets of the model through interactive visualization. To demonstrate the feasibility and usefulness of our framework, we present three detailed case studies based on real-world NLP research problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raymond Li",
      "Ruixin Yang",
      "Wen Xiao",
      "Ahmed AbuRaed",
      "Gabriel Murray",
      "Giuseppe Carenini"
    ]
  },
  "http://arxiv.org/abs/2311.12410": {
    "title": "nach0: Multimodal Natural and Chemical Languages Foundation Model",
    "volume": "Nov",
    "abstract": "Large Language Models (LLMs) have substantially driven scientific progress in various domains, and many papers have demonstrated their ability to tackle complex problems with creative solutions. Our paper introduces a new foundation model, nach0, capable of solving various chemical and biological tasks: biomedical question answering, named entity recognition, molecular generation, molecular synthesis, attributes prediction, and others. nach0 is a multi-domain and multi-task encoder-decoder LLM pre-trained on unlabeled text from scientific literature, patents, and molecule strings to incorporate a range of chemical and linguistic knowledge. We employed instruction tuning, where specific task-related instructions are utilized to fine-tune nach0 for the final set of tasks. To train nach0 effectively, we leverage the NeMo framework, enabling efficient parallel optimization of both base and large model versions. Extensive experiments demonstrate that our model outperforms state-of-the-art baselines on single-domain and cross-domain tasks. Furthermore, it can generate high-quality outputs in molecular and textual formats, showcasing its effectiveness in multi-domain setups",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Micha Livne",
      "Zulfat Miftahutdinov",
      "Elena Tutubalina",
      "Maksim Kuznetsov",
      "Daniil Polykovskiy",
      "Annika Brundyn",
      "Aastha Jhunjhunwala",
      "Anthony Costa",
      "Alex Aliper",
      "Alex Zhavoronkov"
    ]
  },
  "http://arxiv.org/abs/2311.12405": {
    "title": "IndoRobusta: Towards Robustness Against Diverse Code-Mixed Indonesian Local Languages",
    "volume": "Nov",
    "abstract": "Significant progress has been made on Indonesian NLP. Nevertheless, exploration of the code-mixing phenomenon in Indonesian is limited, despite many languages being frequently mixed with Indonesian in daily conversation. In this work, we explore code-mixing in Indonesian with four embedded languages, i.e., English, Sundanese, Javanese, and Malay; and introduce IndoRobusta, a framework to evaluate and improve the code-mixing robustness. Our analysis shows that the pre-training corpus bias affects the model's ability to better handle Indonesian-English code-mixing when compared to other local languages, despite having higher language diversity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Farid Adilazuarda",
      "Samuel Cahyawijaya",
      "Genta Indra Winata",
      "Pascale Fung",
      "Ayu Purwarianti"
    ]
  },
  "http://arxiv.org/abs/2311.12404": {
    "title": "InterPrompt: Interpretable Prompting for Interrelated Interpersonal Risk Factors in Reddit Posts",
    "volume": "Nov",
    "abstract": "Mental health professionals and clinicians have observed the upsurge of mental disorders due to Interpersonal Risk Factors (IRFs). To simulate the human-in-the-loop triaging scenario for early detection of mental health disorders, we recognized textual indications to ascertain these IRFs : Thwarted Belongingness (TBe) and Perceived Burdensomeness (PBu) within personal narratives. In light of this, we use N-shot learning with GPT-3 model on the IRF dataset, and underscored the importance of fine-tuning GPT-3 model to incorporate the context-specific sensitivity and the interconnectedness of textual cues that represent both IRFs. In this paper, we introduce an Interpretable Prompting (InterPrompt)} method to boost the attention mechanism by fine-tuning the GPT-3 model. This allows a more sophisticated level of language modification by adjusting the pre-trained weights. Our model learns to detect usual patterns and underlying connections across both the IRFs, which leads to better system-level explainability and trustworthiness. The results of our research demonstrate that all four variants of GPT-3 model, when fine-tuned with InterPrompt, perform considerably better as compared to the baseline methods, both in terms of classification and explanation generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MSVPJ Sathvik",
      "Surjodeep Sarkar",
      "Chandni Saxena",
      "Sunghwan Sohn",
      "Muskan Garg"
    ]
  },
  "http://arxiv.org/abs/2311.12399": {
    "title": "A Survey of Graph Meets Large Language Model: Progress and Future Directions",
    "volume": "Nov",
    "abstract": "Graph plays a significant role in representing and analyzing complex relationships in real-world applications such as citation networks, social networks, and biological data. Recently, Large Language Models (LLMs), which have achieved tremendous success in various domains, have also been leveraged in graph-related tasks to surpass traditional Graph Neural Networks (GNNs) based methods and yield state-of-the-art performance. In this survey, we first present a comprehensive review and analysis of existing methods that integrate LLMs with graphs. First of all, we propose a new taxonomy, which organizes existing methods into three categories based on the role (i.e., enhancer, predictor, and alignment component) played by LLMs in graph-related tasks. Then we systematically survey the representative methods along the three categories of the taxonomy. Finally, we discuss the remaining limitations of existing studies and highlight promising avenues for future research. The relevant papers are summarized and will be consistently updated at: https://github.com/yhLeeee/Awesome-LLMs-in-Graph-tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Li",
      "Zhixun Li",
      "Peisong Wang",
      "Jia Li",
      "Xiangguo Sun",
      "Hong Cheng",
      "Jeffrey Xu Yu"
    ]
  },
  "http://arxiv.org/abs/2311.12395": {
    "title": "Problems of Non-equivalent Words in Technical Translation",
    "volume": "Nov",
    "abstract": "Translating words which do not have equivalent in target language is not easy and finding proper equivalent of those words are very important to render correctly and understandably, the article defines some thoughts and ideas of scientists on the common problems of non-equivalent words from English to Russian language and includes English and Russian examples and ideas of certain scientist. The English language is worldwide spoken and there are 1.35 billion English speakers and over 258 million Russian speakers according to the 2021s statistics. Inevitably, these billions of speakers around the world have connection and they may have deal in different criteria. In order to understand one another they need to have a pure and fully-understood language. These pure languages understanding directly relates to translation knowledge where linguists and translators need to work and research to eradicate misunderstanding. Misunderstandings mostly appear in non-equivalent words because there are different local and internal words like food, garment, cultural and traditional words and others in every notion. Truly, most of these words do not have equivalent in the target language and these words need to be worked and find their equivalent in the target language to fully understand the both languages. However, some of these non-equivalent words are already professionally rendered to the target language but still there many other words to be rendered. Hence, this research paper includes different ways and rules of rendering non-equivalent words from source language to the target language",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Ibrahim Qani"
    ]
  },
  "http://arxiv.org/abs/2311.12375": {
    "title": "The Obscure Limitation of Modular Multilingual Language Models",
    "volume": "Nov",
    "abstract": "We expose the limitation of modular multilingual language models (MLMs) in multilingual inference scenarios with unknown languages. Existing evaluations of modular MLMs exclude the involvement of language identification (LID) modules, which obscures the performance of real-case multilingual scenarios of modular MLMs. In this work, we showcase the effect of adding LID on the multilingual evaluation of modular MLMs and provide discussions for closing the performance gap of caused by the pipelined approach of LID and modular MLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Farid Adilazuarda",
      "Samuel Cahyawijaya",
      "Ayu Purwarianti"
    ]
  },
  "http://arxiv.org/abs/2311.12373": {
    "title": "Beyond Turing: A Comparative Analysis of Approaches for Detecting Machine-Generated Text",
    "volume": "Nov",
    "abstract": "Significant progress has been made on text generation by pre-trained language models (PLMs), yet distinguishing between human and machine-generated text poses an escalating challenge. This paper offers an in-depth evaluation of three distinct methods used to address this task: traditional shallow learning, Language Model (LM) fine-tuning, and Multilingual Model fine-tuning. These approaches are rigorously tested on a wide range of machine-generated texts, providing a benchmark of their competence in distinguishing between human-authored and machine-authored linguistic constructs. The results reveal considerable differences in performance across methods, thus emphasizing the continued need for advancement in this crucial area of NLP. This study offers valuable insights and paves the way for future research aimed at creating robust and highly discriminative models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Farid Adilazuarda",
      "Nikolaos Nektarios Arkoulis",
      "Oleksii Chumakov"
    ]
  },
  "http://arxiv.org/abs/2311.12355": {
    "title": "Utilizing Language Models for Tour Itinerary Recommendation",
    "volume": "Nov",
    "abstract": "Tour itinerary recommendation involves planning a sequence of relevant Point-of-Interest (POIs), which combines challenges from the fields of both Operations Research (OR) and Recommendation Systems (RS). As an OR problem, there is the need to maximize a certain utility (e.g., popularity of POIs in the tour) while adhering to some constraints (e.g., maximum time for the tour). As a RS problem, it is heavily related to problem or filtering or ranking a subset of POIs that are relevant to a user and recommending it as part of an itinerary. In this paper, we explore the use of language models for the task of tour itinerary recommendation and planning. This task has the unique requirement of recommending personalized POIs relevant to users and planning these POIs as an itinerary that satisfies various constraints. We discuss some approaches in this area, such as using word embedding techniques like Word2Vec and GloVe for learning POI embeddings and transformer-based techniques like BERT for generating itineraries",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ngai Lam Ho",
      "Kwan Hui Lim"
    ]
  },
  "http://arxiv.org/abs/2311.12351": {
    "title": "Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey",
    "volume": "Nov",
    "abstract": "With the bomb ignited by ChatGPT, Transformer-based Large Language Models (LLMs) have paved a revolutionary path toward Artificial General Intelligence (AGI) and have been applied in diverse areas as knowledge bases, human interfaces, and dynamic agents. However, a prevailing limitation exists: many current LLMs, constrained by resources, are primarily pre-trained on shorter texts, rendering them less effective for longer-context prompts, commonly encountered in real-world settings. In this paper, we present a comprehensive survey focusing on the advancement of model architecture in Transformer-based LLMs to optimize long-context capabilities across all stages from pre-training to inference. We firstly delineate and analyze the problems of handling long-context input and output with the current Transformer-based models. Then, we mainly offer a holistic taxonomy to navigate the landscape of Transformer upgrades on architecture to solve these problems. Afterward, we provide the investigation on wildly used evaluation necessities tailored for long-context LLMs, including datasets, metrics, and baseline models, as well as some amazing optimization toolkits like libraries, systems, and compilers to augment LLMs' efficiency and efficacy across different stages. Finally, we further discuss the predominant challenges and potential avenues for future research in this domain. Additionally, we have established a repository where we curate relevant literature with real-time updates at https://github.com/Strivin0311/long-llms-learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunpeng Huang",
      "Jingwei Xu",
      "Zixu Jiang",
      "Junyu Lai",
      "Zenan Li",
      "Yuan Yao",
      "Taolue Chen",
      "Lijuan Yang",
      "Zhou Xin",
      "Xiaoxing Ma"
    ]
  },
  "http://arxiv.org/abs/2311.12337": {
    "title": "Do Smaller Language Models Answer Contextualised Questions Through Memorisation Or Generalisation?",
    "volume": "Nov",
    "abstract": "A distinction is often drawn between a model's ability to predict a label for an evaluation sample that is directly memorised from highly similar training samples versus an ability to predict the label via some method of generalisation. In the context of using Language Models for question-answering, discussion continues to occur as to the extent to which questions are answered through memorisation. We consider this issue for questions that would ideally be answered through reasoning over an associated context. We propose a method of identifying evaluation samples for which it is very unlikely our model would have memorised the answers. Our method is based on semantic similarity of input tokens and label tokens between training and evaluation samples. We show that our method offers advantages upon some prior approaches in that it is able to surface evaluation-train pairs that have overlap in either contiguous or discontiguous sequences of tokens. We use this method to identify unmemorisable subsets of our evaluation datasets. We train two Language Models in a multitask fashion whereby the second model differs from the first only in that it has two additional datasets added to the training regime that are designed to impart simple numerical reasoning strategies of a sort known to improve performance on some of our evaluation datasets but not on others. We then show that there is performance improvement between the two models on the unmemorisable subsets of the evaluation datasets that were expected to benefit from the additional training datasets. Specifically, performance on unmemorisable subsets of two of our evaluation datasets, DROP and ROPES significantly improves by 9.0%, and 25.7% respectively while other evaluation datasets have no significant change in performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Hartill",
      "Joshua Bensemann",
      "Michael Witbrock",
      "Patricia J. Riddle"
    ]
  },
  "http://arxiv.org/abs/2311.12323": {
    "title": "Modeling Political Orientation of Social Media Posts: An Extended Analysis",
    "volume": "Nov",
    "abstract": "Developing machine learning models to characterize political polarization on online social media presents significant challenges. These challenges mainly stem from various factors such as the lack of annotated data, presence of noise in social media datasets, and the sheer volume of data. The common research practice typically examines the biased structure of online user communities for a given topic or qualitatively measuring the impacts of polarized topics on social media. However, there is limited work focusing on analyzing polarization at the ground-level, specifically in the social media posts themselves. Such existing analysis heavily relies on annotated data, which often requires laborious human labeling, offers labels only to specific problems, and lacks the ability to determine the near-future bias state of a social media conversations. Understanding the degree of political orientation conveyed in social media posts is crucial for quantifying the bias of online user communities and investigating the spread of polarized content. In this work, we first introduce two heuristic methods that leverage on news media bias and post content to label social media posts. Next, we compare the efficacy and quality of heuristically labeled dataset with a randomly sampled human-annotated dataset. Additionally, we demonstrate that current machine learning models can exhibit improved performance in predicting political orientation of social media posts, employing both traditional supervised learning and few-shot learning setups. We conduct experiments using the proposed heuristic methods and machine learning approaches to predict the political orientation of posts collected from two social media forums with diverse political ideologies: Gab and Twitter",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sadia Kamal",
      "Brenner Little",
      "Jade Gullic",
      "Trevor Harms",
      "Kristin Olofsson",
      "Arunkumar Bagavathi"
    ]
  },
  "http://arxiv.org/abs/2311.12315": {
    "title": "AcademicGPT: Empowering Academic Research",
    "volume": "Nov",
    "abstract": "Large Language Models (LLMs) have demonstrated exceptional capabilities across various natural language processing tasks. Yet, many of these advanced LLMs are tailored for broad, general-purpose applications. In this technical report, we introduce AcademicGPT, designed specifically to empower academic research. AcademicGPT is a continual training model derived from LLaMA2-70B. Our training corpus mainly consists of academic papers, thesis, content from some academic domain, high-quality Chinese data and others. While it may not be extensive in data scale, AcademicGPT marks our initial venture into a domain-specific GPT tailored for research area. We evaluate AcademicGPT on several established public benchmarks such as MMLU and CEval, as well as on some specialized academic benchmarks like PubMedQA, SCIEval, and our newly-created ComputerScienceQA, to demonstrate its ability from general knowledge ability, to Chinese ability, and to academic ability. Building upon AcademicGPT's foundation model, we also developed several applications catered to the academic area, including General Academic Question Answering, AI-assisted Paper Reading, Paper Review, and AI-assisted Title and Abstract Generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shufa Wei",
      "Xiaolong Xu",
      "Xianbiao Qi",
      "Xi Yin",
      "Jun Xia",
      "Jingyi Ren",
      "Peijun Tang",
      "Yuxiang Zhong",
      "Yihao Chen",
      "Xiaoqin Ren",
      "Yuxin Liang",
      "Liankai Huang",
      "Kai Xie",
      "Weikang Gui",
      "Wei Tan",
      "Shuanglong Sun",
      "Yongquan Hu",
      "Qinxian Liu",
      "Nanjin Li",
      "Chihao Dai",
      "Lihua Wang",
      "Xiaohui Liu",
      "Lei Zhang",
      "Yutao Xie"
    ]
  },
  "http://arxiv.org/abs/2311.12298": {
    "title": "Noise in Relation Classification Dataset TACRED: Characterization and Reduction",
    "volume": "Nov",
    "abstract": "The overarching objective of this paper is two-fold. First, to explore model-based approaches to characterize the primary cause of the noise. in the RE dataset TACRED Second, to identify the potentially noisy instances. Towards the first objective, we analyze predictions and performance of state-of-the-art (SOTA) models to identify the root cause of noise in the dataset. Our analysis of TACRED shows that the majority of the noise in the dataset originates from the instances labeled as no-relation which are negative examples. For the second objective, we explore two nearest-neighbor-based strategies to automatically identify potentially noisy examples for elimination and reannotation. Our first strategy, referred to as Intrinsic Strategy (IS), is based on the assumption that positive examples are clean. Thus, we have used false-negative predictions to identify noisy negative examples. Whereas, our second approach, referred to as Extrinsic Strategy, is based on using a clean subset of the dataset to identify potentially noisy negative examples. Finally, we retrained the SOTA models on the eliminated and reannotated dataset. Our empirical results based on two SOTA models trained on TACRED-E following the IS show an average 4% F1-score improvement, whereas reannotation (TACRED-R) does not improve the original results. However, following ES, SOTA models show the average F1-score improvement of 3.8% and 4.4% when trained on respective eliminated (TACRED-EN) and reannotated (TACRED-RN) datasets respectively. We further extended the ES for cleaning positive examples as well, which resulted in an average performance improvement of 5.8% and 5.6% for the eliminated (TACRED-ENP) and reannotated (TACRED-RNP) datasets respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Parekh",
      "Ashish Anand",
      "Amit Awekar"
    ]
  },
  "http://arxiv.org/abs/2311.12289": {
    "title": "ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for Interdisciplinary Science",
    "volume": "Nov",
    "abstract": "Large language models record impressive performance on many natural language processing tasks. However, their knowledge capacity is limited to the pretraining corpus. Retrieval augmentation offers an effective solution by retrieving context from external knowledge sources to complement the language model. However, existing retrieval augmentation techniques ignore the structural relationships between these documents. Furthermore, retrieval models are not explored much in scientific tasks, especially in regard to the faithfulness of retrieved documents. In this paper, we propose a novel structure-aware retrieval augmented language model that accommodates document structure during retrieval augmentation. We create a heterogeneous document graph capturing multiple types of relationships (e.g., citation, co-authorship, etc.) that connect documents from more than 15 scientific disciplines (e.g., Physics, Medicine, Chemistry, etc.). We train a graph neural network on the curated document graph to act as a structural encoder for the corresponding passages retrieved during the model pretraining. Particularly, along with text embeddings of the retrieved passages, we obtain structural embeddings of the documents (passages) and fuse them together before feeding them to the language model. We evaluate our model extensively on various scientific benchmarks that include science question-answering and scientific document classification tasks. Experimental results demonstrate that structure-aware retrieval improves retrieving more coherent, faithful and contextually relevant passages, while showing a comparable performance in the overall accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sai Munikoti",
      "Anurag Acharya",
      "Sridevi Wagle",
      "Sameera Horawalavithana"
    ]
  },
  "http://arxiv.org/abs/2311.12275": {
    "title": "Enabling On-Device Large Language Model Personalization with Self-Supervised Data Selection and Synthesis",
    "volume": "Nov",
    "abstract": "After a large language model (LLM) is deployed on edge devices, it is desirable for these devices to learn from user-generated conversation data to generate user-specific and personalized responses in real-time. However, user-generated data usually contains sensitive and private information, and uploading such data to the cloud for annotation is not preferred if not prohibited. While it is possible to obtain annotation locally by directly asking users to provide preferred responses, such annotations have to be sparse to not affect user experience. In addition, the storage of edge devices is usually too limited to enable large-scale fine-tuning with full user-generated data. It remains an open question how to enable on-device LLM personalization, considering sparse annotation and limited on-device storage. In this paper, we propose a novel framework to select and store the most representative data online in a self-supervised way. Such data has a small memory footprint and allows infrequent requests of user annotations for further fine-tuning. To enhance fine-tuning quality, multiple semantically similar pairs of question texts and expected responses are generated using the LLM. Our experiments show that the proposed framework achieves the best user-specific content-generating capability (accuracy) and fine-tuning speed (performance) compared with vanilla baselines. To the best of our knowledge, this is the very first on-device LLM personalization framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiyang Qin",
      "Jun Xia",
      "Zhenge Jia",
      "Meng Jiang",
      "Ahmed Abbasi",
      "Peipei Zhou",
      "Jingtong Hu",
      "Yiyu Shi"
    ]
  },
  "http://arxiv.org/abs/2311.12233": {
    "title": "Unifying Corroborative and Contributive Attributions in Large Language Models",
    "volume": "Nov",
    "abstract": "As businesses, products, and services spring up around large language models, the trustworthiness of these models hinges on the verifiability of their outputs. However, methods for explaining language model outputs largely fall across two distinct fields of study which both use the term \"attribution\" to refer to entirely separate techniques: citation generation and training data attribution. In many modern applications, such as legal document generation and medical question answering, both types of attributions are important. In this work, we argue for and present a unified framework of large language model attributions. We show how existing methods of different types of attribution fall under the unified framework. We also use the framework to discuss real-world use cases where one or both types of attributions are required. We believe that this unified framework will guide the use case driven development of systems that leverage both types of attribution, as well as the standardization of their evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Theodora Worledge",
      "Judy Hanwen Shen",
      "Nicole Meister",
      "Caleb Winston",
      "Carlos Guestrin"
    ]
  },
  "http://arxiv.org/abs/2311.12179": {
    "title": "Leveraging Closed-Access Multilingual Embedding for Automatic Sentence Alignment in Low Resource Languages",
    "volume": "Nov",
    "abstract": "The importance of qualitative parallel data in machine translation has long been determined but it has always been very difficult to obtain such in sufficient quantity for the majority of world languages, mainly because of the associated cost and also the lack of accessibility to these languages. Despite the potential for obtaining parallel datasets from online articles using automatic approaches, forensic investigations have found a lot of quality-related issues such as misalignment, and wrong language codes. In this work, we present a simple but qualitative parallel sentence aligner that carefully leveraged the closed-access Cohere multilingual embedding, a solution that ranked second in the just concluded #CoHereAIHack 2023 Challenge (see https://ai6lagos.devpost.com). The proposed approach achieved $94.96$ and $54.83$ f1 scores on FLORES and MAFAND-MT, compared to $3.64$ and $0.64$ of LASER respectively. Our method also achieved an improvement of more than 5 BLEU scores over LASER, when the resulting datasets were used with MAFAND-MT dataset to train translation models. Our code and data are available for research purposes here (https://github.com/abumafrim/Cohere-Align)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Idris Abdulmumin",
      "Auwal Abubakar Khalid",
      "Shamsuddeen Hassan Muhammad",
      "Ibrahim Said Ahmad",
      "Lukman Jibril Aliyu",
      "Babangida Sani",
      "Bala Mairiga Abduljalil",
      "Sani Ahmad Hassan"
    ]
  },
  "http://arxiv.org/abs/2311.12131": {
    "title": "Human Learning by Model Feedback: The Dynamics of Iterative Prompting with Midjourney",
    "volume": "Nov",
    "abstract": "Generating images with a Text-to-Image model often requires multiple trials, where human users iteratively update their prompt based on feedback, namely the output image. Taking inspiration from cognitive work on reference games and dialogue alignment, this paper analyzes the dynamics of the user prompts along such iterations. We compile a dataset of iterative interactions of human users with Midjourney. Our analysis then reveals that prompts predictably converge toward specific traits along these iterations. We further study whether this convergence is due to human users, realizing they missed important details, or due to adaptation to the model's ``preferences'', producing better images for a specific language style. We show initial evidence that both possibilities are at play. The possibility that users adapt to the model's preference raises concerns about reusing user data for further training. The prompts may be biased towards the preferences of a specific model, rather than align with human intentions and natural manner of expression",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shachar Don-Yehiya",
      "Leshem Choshen",
      "Omri Abend"
    ]
  },
  "http://arxiv.org/abs/2311.12023": {
    "title": "LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning",
    "volume": "Nov",
    "abstract": "We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on adapting RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and moreover enables more aggressive quantization. For example, on the OpenAssistant benchmark LQ-LoRA is able to learn a 2.5-bit LLaMA-2 model that is competitive with a model finetuned with 4-bit QLoRA. When finetuned on a language modeling calibration dataset, LQ-LoRA can also be used for model compression; in this setting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average when including the low-rank components and requires 27GB of GPU memory) is competitive with the original model in full precision",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Guo",
      "Philip Greengard",
      "Eric P. Xing",
      "Yoon Kim"
    ]
  },
  "http://arxiv.org/abs/2311.12022": {
    "title": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark",
    "volume": "Nov",
    "abstract": "We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are \"Google-proof\"). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Rein",
      "Betty Li Hou",
      "Asa Cooper Stickland",
      "Jackson Petty",
      "Richard Yuanzhe Pang",
      "Julien Dirani",
      "Julian Michael",
      "Samuel R. Bowman"
    ]
  },
  "http://arxiv.org/abs/2311.12015": {
    "title": "GPT-4V(ision) for Robotics: Multimodal Task Planning from Human Demonstration",
    "volume": "Nov",
    "abstract": "We introduce a pipeline that enhances a general-purpose Vision Language Model, GPT-4V(ision), by integrating observations of human actions to facilitate robotic manipulation. This system analyzes videos of humans performing tasks and creates executable robot programs that incorporate affordance insights. The computation starts by analyzing the videos with GPT-4V to convert environmental and action details into text, followed by a GPT-4-empowered task planner. In the following analyses, vision systems reanalyze the video with the task plan. Object names are grounded using an open-vocabulary object detector, while focus on the hand-object relation helps to detect the moment of grasping and releasing. This spatiotemporal grounding allows the vision systems to further gather affordance data (e.g., grasp type, way points, and body postures). Experiments across various scenarios demonstrate this method's efficacy in achieving real robots' operations from human demonstrations in a zero-shot manner. The prompts of GPT-4V/GPT-4 are available at this project page: https://microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naoki Wake",
      "Atsushi Kanehira",
      "Kazuhiro Sasabuchi",
      "Jun Takamatsu",
      "Katsushi Ikeuchi"
    ]
  },
  "http://arxiv.org/abs/2311.11981": {
    "title": "H-COAL: Human Correction of AI-Generated Labels for Biomedical Named Entity Recognition",
    "volume": "Nov",
    "abstract": "With the rapid advancement of machine learning models for NLP tasks, collecting high-fidelity labels from AI models is a realistic possibility. Firms now make AI available to customers via predictions as a service (PaaS). This includes PaaS products for healthcare. It is unclear whether these labels can be used for training a local model without expensive annotation checking by in-house experts. In this work, we propose a new framework for Human Correction of AI-Generated Labels (H-COAL). By ranking AI-generated outputs, one can selectively correct labels and approach gold standard performance (100% human labeling) with significantly less human effort. We show that correcting 5% of labels can close the AI-human performance gap by up to 64% relative improvement, and correcting 20% of labels can close the performance gap by up to 86% relative improvement",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaojing Duan",
      "John P. Lalor"
    ]
  },
  "http://arxiv.org/abs/2311.11979": {
    "title": "On the Potential and Limitations of Few-Shot In-Context Learning to Generate Metamorphic Specifications for Tax Preparation Software",
    "volume": "Nov",
    "abstract": "Due to the ever-increasing complexity of income tax laws in the United States, the number of US taxpayers filing their taxes using tax preparation software (henceforth, tax software) continues to increase. According to the U.S. Internal Revenue Service (IRS), in FY22, nearly 50% of taxpayers filed their individual income taxes using tax software. Given the legal consequences of incorrectly filing taxes for the taxpayer, ensuring the correctness of tax software is of paramount importance. Metamorphic testing has emerged as a leading solution to test and debug legal-critical tax software due to the absence of correctness requirements and trustworthy datasets. The key idea behind metamorphic testing is to express the properties of a system in terms of the relationship between one input and its slightly metamorphosed twinned input. Extracting metamorphic properties from IRS tax publications is a tedious and time-consuming process. As a response, this paper formulates the task of generating metamorphic specifications as a translation task between properties extracted from tax documents - expressed in natural language - to a contrastive first-order logic form. We perform a systematic analysis on the potential and limitations of in-context learning with Large Language Models(LLMs) for this task, and outline a research agenda towards automating the generation of metamorphic specifications for tax preparation software",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dananjay Srinivas",
      "Rohan Das",
      "Saeid Tizpaz-Niari",
      "Ashutosh Trivedi",
      "Maria Leonor Pacheco"
    ]
  },
  "http://arxiv.org/abs/2311.11976": {
    "title": "Context-aware Neural Machine Translation for English-Japanese Business Scene Dialogues",
    "volume": "Nov",
    "abstract": "Despite the remarkable advancements in machine translation, the current sentence-level paradigm faces challenges when dealing with highly-contextual languages like Japanese. In this paper, we explore how context-awareness can improve the performance of the current Neural Machine Translation (NMT) models for English-Japanese business dialogues translation, and what kind of context provides meaningful information to improve translation. As business dialogue involves complex discourse phenomena but offers scarce training resources, we adapted a pretrained mBART model, finetuning on multi-sentence dialogue data, which allows us to experiment with different contexts. We investigate the impact of larger context sizes and propose novel context tokens encoding extra-sentential information, such as speaker turn and scene type. We make use of Conditional Cross-Mutual Information (CXMI) to explore how much of the context the model uses and generalise CXMI to study the impact of the extra-sentential context. Overall, we find that models leverage both preceding sentences and extra-sentential context (with CXMI increasing with context size) and we provide a more focused analysis on honorifics translation. Regarding translation quality, increased source-side context paired with scene and speaker information improves the model performance compared to previous work and our context-agnostic baselines, measured in BLEU and COMET metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sumire Honda",
      "Patrick Fernandes",
      "Chrysoula Zerva"
    ]
  },
  "http://arxiv.org/abs/2311.11973": {
    "title": "Adaptive Training Distributions with Scalable Online Bilevel Optimization",
    "volume": "Nov",
    "abstract": "Large neural networks pretrained on web-scale corpora are central to modern machine learning. In this paradigm, the distribution of the large, heterogeneous pretraining data rarely matches that of the application domain. This work considers modifying the pretraining distribution in the case where one has a small sample of data reflecting the targeted test conditions. We propose an algorithm motivated by a recent formulation of this setting as an online, bilevel optimization problem. With scalability in mind, our algorithm prioritizes computing gradients at training points which are likely to most improve the loss on the targeted distribution. Empirically, we show that in some cases this approach is beneficial over existing strategies from the domain adaptation literature but may not succeed in other cases. We propose a simple test to evaluate when our approach can be expected to work well and point towards further research to address current limitations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Grangier",
      "Pierre Ablin",
      "Awni Hannun"
    ]
  },
  "http://arxiv.org/abs/2311.11967": {
    "title": "Automatic Analysis of Substantiation in Scientific Peer Reviews",
    "volume": "Nov",
    "abstract": "With the increasing amount of problematic peer reviews in top AI conferences, the community is urgently in need of automatic quality control measures. In this paper, we restrict our attention to substantiation -- one popular quality aspect indicating whether the claims in a review are sufficiently supported by evidence -- and provide a solution automatizing this evaluation process. To achieve this goal, we first formulate the problem as claim-evidence pair extraction in scientific peer reviews, and collect SubstanReview, the first annotated dataset for this task. SubstanReview consists of 550 reviews from NLP conferences annotated by domain experts. On the basis of this dataset, we train an argument mining system to automatically analyze the level of substantiation in peer reviews. We also perform data analysis on the SubstanReview dataset to obtain meaningful insights on peer reviewing quality in NLP conferences over recent years",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanzhu Guo",
      "Guokan Shang",
      "Virgile Rennard",
      "Michalis Vazirgiannis",
      "Chlo Clavel"
    ]
  },
  "http://arxiv.org/abs/2311.11944": {
    "title": "FinanceBench: A New Benchmark for Financial Question Answering",
    "volume": "Nov",
    "abstract": "FinanceBench is a first-of-its-kind test suite for evaluating the performance of LLMs on open book financial question answering (QA). It comprises 10,231 questions about publicly traded companies, with corresponding answers and evidence strings. The questions in FinanceBench are ecologically valid and cover a diverse set of scenarios. They are intended to be clear-cut and straightforward to answer to serve as a minimum performance standard. We test 16 state of the art model configurations (including GPT-4-Turbo, Llama2 and Claude2, with vector stores and long context prompts) on a sample of 150 cases from FinanceBench, and manually review their answers (n=2,400). The cases are available open-source. We show that existing LLMs have clear limitations for financial QA. Notably, GPT-4-Turbo used with a retrieval system incorrectly answered or refused to answer 81% of questions. While augmentation techniques such as using longer context window to feed in relevant evidence improve performance, they are unrealistic for enterprise settings due to increased latency and cannot support larger financial documents. We find that all models examined exhibit weaknesses, such as hallucinations, that limit their suitability for use by enterprises",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranab Islam",
      "Anand Kannappan",
      "Douwe Kiela",
      "Rebecca Qian",
      "Nino Scherrer",
      "Bertie Vidgen"
    ]
  },
  "http://arxiv.org/abs/2311.11904": {
    "title": "LLMs as Visual Explainers: Advancing Image Classification with Evolving Visual Descriptions",
    "volume": "Nov",
    "abstract": "Vision-language models (VLMs) offer a promising paradigm for image classification by comparing the similarity between images and class embeddings. A critical challenge lies in crafting precise textual representations for class names. While previous studies have leveraged recent advancements in large language models (LLMs) to enhance these descriptors, their outputs often suffer from ambiguity and inaccuracy. We identify two primary causes: 1) The prevalent reliance on textual interactions with LLMs, leading to a mismatch between the generated text and the visual content in VLMs' latent space - a phenomenon we term the \"explain without seeing\" dilemma. 2) The oversight of the inter-class relationships, resulting in descriptors that fail to differentiate similar classes effectively. To address these issues, we propose a novel image classification framework combining VLMs with LLMs, named Iterative Optimization with Visual Feedback. In particular, our method develops an LLM-based agent, employing an evolutionary optimization strategy to refine class descriptors. Crucially, we incorporate visual feedback from VLM classification metrics, thereby guiding the optimization process with concrete visual data. Our method leads to improving accuracy on a wide range of image classification benchmarks, with 3.47\\% average gains over state-of-the-art methods. We also highlight the resulting descriptions serve as explainable and robust features that can consistently improve the performance across various backbone models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songhao Han",
      "Le Zhuo",
      "Yue Liao",
      "Si Liu"
    ]
  },
  "http://arxiv.org/abs/2311.11861": {
    "title": "Generating Valid and Natural Adversarial Examples with Large Language Models",
    "volume": "Nov",
    "abstract": "Deep learning-based natural language processing (NLP) models, particularly pre-trained language models (PLMs), have been revealed to be vulnerable to adversarial attacks. However, the adversarial examples generated by many mainstream word-level adversarial attack models are neither valid nor natural, leading to the loss of semantic maintenance, grammaticality, and human imperceptibility. Based on the exceptional capacity of language understanding and generation of large language models (LLMs), we propose LLM-Attack, which aims at generating both valid and natural adversarial examples with LLMs. The method consists of two stages: word importance ranking (which searches for the most vulnerable words) and word synonym replacement (which substitutes them with their synonyms obtained from LLMs). Experimental results on the Movie Review (MR), IMDB, and Yelp Review Polarity datasets against the baseline adversarial attack models illustrate the effectiveness of LLM-Attack, and it outperforms the baselines in human and GPT-4 evaluation by a significant margin. The model can generate adversarial examples that are typically valid and natural, with the preservation of semantic meaning, grammaticality, and human imperceptibility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zimu Wang",
      "Wei Wang",
      "Qi Chen",
      "Qiufeng Wang",
      "Anh Nguyen"
    ]
  },
  "http://arxiv.org/abs/2311.11855": {
    "title": "Evil Geniuses: Delving into the Safety of LLM-based Agents",
    "volume": "Nov",
    "abstract": "The rapid advancements in large language models (LLMs) have led to a resurgence in LLM-based agents, which demonstrate impressive human-like behaviors and cooperative capabilities in various interactions and strategy formulations. However, evaluating the safety of LLM-based agents remains a complex challenge. This paper elaborately conducts a series of manual jailbreak prompts along with a virtual chat-powered evil plan development team, dubbed Evil Geniuses, to thoroughly probe the safety aspects of these agents. Our investigation reveals three notable phenomena: 1) LLM-based agents exhibit reduced robustness against malicious attacks. 2) the attacked agents could provide more nuanced responses. 3) the detection of the produced improper responses is more challenging. These insights prompt us to question the effectiveness of LLM-based attacks on agents, highlighting vulnerabilities at various levels and within different role specializations within the system/agent of LLM-based agents. Extensive evaluation and discussion reveal that LLM-based agents face significant challenges in safety and yield insights for future research. Our code is available at https://github.com/T1aNS1R/Evil-Geniuses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Tian",
      "Xiao Yang",
      "Jingyuan Zhang",
      "Yinpeng Dong",
      "Hang Su"
    ]
  },
  "http://arxiv.org/abs/2311.11846": {
    "title": "Deepparse : An Extendable, and Fine-Tunable State-Of-The-Art Library for Parsing Multinational Street Addresses",
    "volume": "Nov",
    "abstract": "Segmenting an address into meaningful components, also known as address parsing, is an essential step in many applications from record linkage to geocoding and package delivery. Consequently, a lot of work has been dedicated to develop accurate address parsing techniques, with machine learning and neural network methods leading the state-of-the-art scoreboard. However, most of the work on address parsing has been confined to academic endeavours with little availability of free and easy-to-use open-source solutions. This paper presents Deepparse, a Python open-source, extendable, fine-tunable address parsing solution under LGPL-3.0 licence to parse multinational addresses using state-of-the-art deep learning algorithms and evaluated on over 60 countries. It can parse addresses written in any language and use any address standard. The pre-trained model achieves average $99~\\%$ parsing accuracies on the countries used for training with no pre-processing nor post-processing needed. Moreover, the library supports fine-tuning with new data to generate a custom address parser",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Beauchemin",
      "Marouane Yassine"
    ]
  },
  "http://arxiv.org/abs/2311.11844": {
    "title": "How to Use Large Language Models for Text Coding: The Case of Fatherhood Roles in Public Policy Documents",
    "volume": "Nov",
    "abstract": "Recent advances in large language models (LLMs) like GPT-3 and GPT-4 have opened up new opportunities for text analysis in political science. They promise automation with better results and less programming. In this study, we evaluate LLMs on three original coding tasks of non-English political science texts, and we provide a detailed description of a general workflow for using LLMs for text coding in political science research. Our use case offers a practical guide for researchers looking to incorporate LLMs into their research on text analysis. We find that, when provided with detailed label definitions and coding examples, an LLM can be as good as or even better than a human annotator while being much faster (up to hundreds of times), considerably cheaper (costing up to 60% less than human coding), and much easier to scale to large amounts of text. Overall, LLMs present a viable option for most text coding projects",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Lupo",
      "Oscar Magnusson",
      "Dirk Hovy",
      "Elin Naurin",
      "Lena Wngnerud"
    ]
  },
  "http://arxiv.org/abs/2311.11829": {
    "title": "System 2 Attention (is something you might need too)",
    "volume": "Nov",
    "abstract": "Soft attention in Transformer-based Large Language Models (LLMs) is susceptible to incorporating irrelevant information from the context into its latent representations, which adversely affects next token generations. To help rectify these issues, we introduce System 2 Attention (S2A), which leverages the ability of LLMs to reason in natural language and follow instructions in order to decide what to attend to. S2A regenerates the input context to only include the relevant portions, before attending to the regenerated context to elicit the final response. In experiments, S2A outperforms standard attention-based LLMs on three tasks containing opinion or irrelevant information, QA, math word problems and longform generation, where S2A increases factuality and objectivity, and decreases sycophancy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jason Weston",
      "Sainbayar Sukhbaatar"
    ]
  },
  "http://arxiv.org/abs/2311.11813": {
    "title": "Efficient Grammatical Error Correction Via Multi-Task Training and Optimized Training Schedule",
    "volume": "Nov",
    "abstract": "Progress in neural grammatical error correction (GEC) is hindered by the lack of annotated training data. Sufficient amounts of high-quality manually annotated data are not available, so recent research has relied on generating synthetic data, pretraining on it, and then fine-tuning on real datasets; performance gains have been achieved either by ensembling or by using huge pretrained models such as XXL-T5 as the backbone. In this work, we explore an orthogonal direction: how to use available data more efficiently. First, we propose auxiliary tasks that exploit the alignment between the original and corrected sentences, such as predicting a sequence of corrections. We formulate each task as a sequence-to-sequence problem and perform multi-task training. Second, we discover that the order of datasets used for training and even individual instances within a dataset may have important effects on the final performance, so we set out to find the best training schedule. Together, these two ideas lead to significant improvements, producing results that improve state of the art with much smaller models; in particular, we outperform the best models based on T5-XXL (11B parameters) with a BART-based model (400M parameters)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrey Bout",
      "Alexander Podolskiy",
      "Sergey Nikolenko",
      "Irina Piontkovskaya"
    ]
  },
  "http://arxiv.org/abs/2311.11797": {
    "title": "Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents",
    "volume": "Nov",
    "abstract": "Large language models (LLMs) have dramatically enhanced the field of language intelligence, as demonstrably evidenced by their formidable empirical performance across a spectrum of complex reasoning tasks. Additionally, theoretical proofs have illuminated their emergent reasoning capabilities, providing a compelling showcase of their advanced cognitive abilities in linguistic contexts. Critical to their remarkable efficacy in handling complex reasoning tasks, LLMs leverage the intriguing chain-of-thought (CoT) reasoning techniques, obliging them to formulate intermediate steps en route to deriving an answer. The CoT reasoning approach has not only exhibited proficiency in amplifying reasoning performance but also in enhancing interpretability, controllability, and flexibility. In light of these merits, recent research endeavors have extended CoT reasoning methodologies to nurture the development of autonomous language agents, which adeptly adhere to language instructions and execute actions within varied environments. This survey paper orchestrates a thorough discourse, penetrating vital research dimensions, encompassing: (i) the foundational mechanics of CoT techniques, with a focus on elucidating the circumstances and justification behind its efficacy; (ii) the paradigm shift in CoT; and (iii) the burgeoning of language agents fortified by CoT approaches. Prospective research avenues envelop explorations into generalization, efficiency, customization, scaling, and safety. This paper caters to a wide audience, including beginners seeking comprehensive knowledge of CoT reasoning and language agents, as well as experienced researchers interested in foundational mechanics and engaging in cutting-edge discussions on these topics. A repository for the related papers is available at https://github.com/Zoeyyao27/CoT-Igniting-Agent",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuosheng Zhang",
      "Yao Yao",
      "Aston Zhang",
      "Xiangru Tang",
      "Xinbei Ma",
      "Zhiwei He",
      "Yiming Wang",
      "Mark Gerstein",
      "Rui Wang",
      "Gongshen Liu",
      "Hai Zhao"
    ]
  },
  "http://arxiv.org/abs/2311.11796": {
    "title": "Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems",
    "volume": "Nov",
    "abstract": "Artificial Intelligence (AI) systems such as autonomous vehicles, facial recognition, and speech recognition systems are increasingly integrated into our daily lives. However, despite their utility, these AI systems are vulnerable to a wide range of attacks such as adversarial, backdoor, data poisoning, membership inference, model inversion, and model stealing attacks. In particular, numerous attacks are designed to target a particular model or system, yet their effects can spread to additional targets, referred to as transferable attacks. Although considerable efforts have been directed toward developing transferable attacks, a holistic understanding of the advancements in transferable attacks remains elusive. In this paper, we comprehensively explore learning-based attacks from the perspective of transferability, particularly within the context of cyber-physical security. We delve into different domains -- the image, text, graph, audio, and video domains -- to highlight the ubiquitous and pervasive nature of transferable attacks. This paper categorizes and reviews the architecture of existing attacks from various viewpoints: data, process, model, and system. We further examine the implications of transferable attacks in practical scenarios such as autonomous driving, speech recognition, and large language models (LLMs). Additionally, we outline the potential research directions to encourage efforts in exploring the landscape of transferable attacks. This survey offers a holistic understanding of the prevailing transferable attacks and their impacts across different domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangjing Wang",
      "Ce Zhou",
      "Yuanda Wang",
      "Bocheng Chen",
      "Hanqing Guo",
      "Qiben Yan"
    ]
  },
  "http://arxiv.org/abs/2311.11745": {
    "title": "Encoding Speaker-Specific Latent Speech Feature for Speech Synthesis",
    "volume": "Nov",
    "abstract": "In this work, we propose a novel method for modeling numerous speakers, which enables expressing the overall characteristics of speakers in detail like a trained multi-speaker model without additional training on the target speaker's dataset. Although various works with similar purposes have been actively studied, their performance has not yet reached that of trained multi-speaker models due to their fundamental limitations. To overcome previous limitations, we propose effective methods for feature learning and representing target speakers' speech characteristics by discretizing the features and conditioning them to a speech synthesis model. Our method obtained a significantly higher similarity mean opinion score (SMOS) in subjective similarity evaluation than seen speakers of a best-performing multi-speaker model, even with unseen speakers. The proposed method also outperforms a zero-shot method by significant margins. Furthermore, our method shows remarkable performance in generating new artificial speakers. In addition, we demonstrate that the encoded latent features are sufficiently informative to reconstruct an original speaker's speech completely. It implies that our method can be used as a general methodology to encode and reconstruct speakers' characteristics in various tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungil Kong",
      "Junmo Lee",
      "Jeongmin Kim",
      "Beomjeong Kim",
      "Jihoon Park",
      "Dohee Kong",
      "Changheon Lee",
      "Sangjin Kim"
    ]
  },
  "http://arxiv.org/abs/2311.11701": {
    "title": "Control in Hybrid Chatbots",
    "volume": "Nov",
    "abstract": "Customer data typically is held in database systems, which can be seen as rule-based knowledge base, whereas businesses increasingly want to benefit from the capabilities of large, pre-trained language models. In this technical report, we describe a case study of how a commercial rule engine and an integrated neural chatbot may be integrated, and what level of control that particular integration mode leads to. We also discuss alternative ways (including past ways realized in other systems) how researchers strive to maintain control and avoid what has recently been called model \"hallucination\"",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Rdel",
      "Jochen L. Leidner"
    ]
  },
  "http://arxiv.org/abs/2311.11696": {
    "title": "Sparse Low-rank Adaptation of Pre-trained Language Models",
    "volume": "Nov",
    "abstract": "Fine-tuning pre-trained large language models in a parameter-efficient manner is widely studied for its effectiveness and efficiency. The popular method of low-rank adaptation (LoRA) offers a notable approach, hypothesizing that the adaptation process is intrinsically low-dimensional. Although LoRA has demonstrated commendable performance, it is implemented with a fixed and unalterable intrinsic rank that might not always be the ideal choice. Recognizing the need for more flexible adaptation, we extend the methodology of LoRA to an innovative approach we call sparse low-rank adaptation (SoRA) that enables dynamic adjustments to the intrinsic rank during the adaptation process. We achieve this through the incorporation of a gate unit optimized with proximal gradient method in the training stage, controlling the cardinality of rank under the sparsity of the gate. In the subsequent inference stage, we eliminate the parameter blocks corresponding to the zeroed-out ranks, to reduce each SoRA module back to a concise yet rank-optimal LoRA. Our approach strengthens the representation power of LoRA by initializing it with a higher rank, while efficiently taming a temporarily increased number of parameters via updating in a sparse way. We further introduce a sparsifying scheduler for SoRA, aiming to examine the impact of the number of non-zero parameters on the model's memorization and generalization. Our experimental results demonstrate that SoRA can outperform other baselines even with 70% retained parameters and 70% training time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ning Ding",
      "Xingtai Lv",
      "Qiaosen Wang",
      "Yulin Chen",
      "Bowen Zhou",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "http://arxiv.org/abs/2311.11690": {
    "title": "Refactoring Programs Using Large Language Models with Few-Shot Examples",
    "volume": "Nov",
    "abstract": "A less complex and more straightforward program is a crucial factor that enhances its maintainability and makes writing secure and bug-free programs easier. However, due to its heavy workload and the risks of breaking the working programs, programmers are reluctant to do code refactoring, and thus, it also causes the loss of potential learning experiences. To mitigate this, we demonstrate the application of using a large language model (LLM), GPT-3.5, to suggest less complex versions of the user-written Python program, aiming to encourage users to learn how to write better programs. We propose a method to leverage the prompting with few-shot examples of the LLM by selecting the best-suited code refactoring examples for each target programming problem based on the prior evaluation of prompting with the one-shot example. The quantitative evaluation shows that 95.68% of programs can be refactored by generating 10 candidates each, resulting in a 17.35% reduction in the average cyclomatic complexity and a 25.84% decrease in the average number of lines after filtering only generated programs that are semantically correct. Furthermore, the qualitative evaluation shows outstanding capability in code formatting, while unnecessary behaviors such as deleting or translating comments are also observed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atsushi Shirafuji",
      "Yusuke Oda",
      "Jun Suzuki",
      "Makoto Morishita",
      "Yutaka Watanobe"
    ]
  },
  "http://arxiv.org/abs/2311.11608": {
    "title": "Taiyi: A Bilingual Fine-Tuned Large Language Model for Diverse Biomedical Tasks",
    "volume": "Nov",
    "abstract": "Recent advancements in large language models (LLMs) have shown promising results across a variety of natural language processing (NLP) tasks. The application of LLMs to specific domains, such as biomedicine, has achieved increased attention. However, most biomedical LLMs focus on enhancing performance in monolingual biomedical question answering and conversation tasks. To further investigate the effectiveness of the LLMs on diverse biomedical NLP tasks in different languages, we present Taiyi, a bilingual (English and Chinese) fine-tuned LLM for diverse biomedical tasks. In this work, we first curated a comprehensive collection of 140 existing biomedical text mining datasets across over 10 task types. Subsequently, a two-stage strategy is proposed for supervised fine-tuning to optimize the model performance across varied tasks. Experimental results on 13 test sets covering named entity recognition, relation extraction, text classification, question answering tasks demonstrate Taiyi achieves superior performance compared to general LLMs. The case study involving additional biomedical NLP tasks further shows Taiyi's considerable potential for bilingual biomedical multi-tasking. The source code, datasets, and model for Taiyi are freely available at https://github.com/DUTIR-BioNLP/Taiyi-LLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ling Luo",
      "Jinzhong Ning",
      "Yingwen Zhao",
      "Zhijun Wang",
      "Zeyuan Ding",
      "Peng Chen",
      "Weiru Fu",
      "Qinyu Han",
      "Guangtao Xu",
      "Yunzhi Qiu",
      "Dinghao Pan",
      "Jiru Li",
      "Hao Li",
      "Wenduo Feng",
      "Senbo Tu",
      "Yuqi Liu",
      "Zhihao Yang",
      "Jian Wang",
      "Yuanyuan Sun",
      "Hongfei Lin"
    ]
  },
  "http://arxiv.org/abs/2311.11601": {
    "title": "Addressing the Length Bias Problem in Document-Level Neural Machine Translation",
    "volume": "Nov",
    "abstract": "Document-level neural machine translation (DNMT) has shown promising results by incorporating more context information. However, this approach also introduces a length bias problem, whereby DNMT suffers from significant translation quality degradation when decoding documents that are much shorter or longer than the maximum sequence length during training. %i.e., the length bias problem. To solve the length bias problem, we propose to improve the DNMT model in training method, attention mechanism, and decoding strategy. Firstly, we propose to sample the training data dynamically to ensure a more uniform distribution across different sequence lengths. Then, we introduce a length-normalized attention mechanism to aid the model in focusing on target information, mitigating the issue of attention divergence when processing longer sequences. Lastly, we propose a sliding window strategy during decoding that integrates as much context information as possible without exceeding the maximum sequence length. The experimental results indicate that our method can bring significant improvements on several open datasets, and further analysis shows that our method can significantly alleviate the length bias problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuocheng Zhang",
      "Shuhao Gu",
      "Min Zhang",
      "Yang Feng"
    ]
  },
  "http://arxiv.org/abs/2311.11598": {
    "title": "Filling the Image Information Gap for VQA: Prompting Large Language Models to Proactively Ask Questions",
    "volume": "Nov",
    "abstract": "Large Language Models (LLMs) demonstrate impressive reasoning ability and the maintenance of world knowledge not only in natural language tasks, but also in some vision-language tasks such as open-domain knowledge-based visual question answering (OK-VQA). As images are invisible to LLMs, researchers convert images to text to engage LLMs into the visual question reasoning procedure. This leads to discrepancies between images and their textual representations presented to LLMs, which consequently impedes final reasoning performance. To fill the information gap and better leverage the reasoning capability, we design a framework that enables LLMs to proactively ask relevant questions to unveil more details in the image, along with filters for refining the generated information. We validate our idea on OK-VQA and A-OKVQA. Our method continuously boosts the performance of baselines methods by an average gain of 2.15% on OK-VQA, and achieves consistent improvements across different LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyue Wang",
      "Chi Chen",
      "Peng Li",
      "Yang Liu"
    ]
  },
  "http://arxiv.org/abs/2311.11583": {
    "title": "How well ChatGPT understand Malaysian English? An Evaluation on Named Entity Recognition and Relation Extraction",
    "volume": "Nov",
    "abstract": "Recently, ChatGPT has attracted a lot of interest from both researchers and the general public. While the performance of ChatGPT in named entity recognition and relation extraction from Standard English texts is satisfactory, it remains to be seen if it can perform similarly for Malaysian English. Malaysian English is unique as it exhibits morphosyntactic and semantical adaptation from local contexts. In this study, we assess ChatGPT's capability in extracting entities and relations from the Malaysian English News (MEN) dataset. We propose a three-step methodology referred to as \\textbf{\\textit{educate-predict-evaluate}}. The performance of ChatGPT is assessed using F1-Score across 18 unique prompt settings, which were carefully engineered for a comprehensive review. From our evaluation, we found that ChatGPT does not perform well in extracting entities from Malaysian English news articles, with the highest F1-Score of 0.497. Further analysis shows that the morphosyntactic adaptation in Malaysian English caused the limitation. However, interestingly, this morphosyntactic adaptation does not impact the performance of ChatGPT for relation extraction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohan Raj Chanthran",
      "Lay-Ki Soon",
      "Huey Fang Ong",
      "Bhawani Selvaretnam"
    ]
  },
  "http://arxiv.org/abs/2311.11564": {
    "title": "KBioXLM: A Knowledge-anchored Biomedical Multilingual Pretrained Language Model",
    "volume": "Nov",
    "abstract": "Most biomedical pretrained language models are monolingual and cannot handle the growing cross-lingual requirements. The scarcity of non-English domain corpora, not to mention parallel data, poses a significant hurdle in training multilingual biomedical models. Since knowledge forms the core of domain-specific corpora and can be translated into various languages accurately, we propose a model called KBioXLM, which transforms the multilingual pretrained model XLM-R into the biomedical domain using a knowledge-anchored approach. We achieve a biomedical multilingual corpus by incorporating three granularity knowledge alignments (entity, fact, and passage levels) into monolingual corpora. Then we design three corresponding training tasks (entity masking, relation masking, and passage relation prediction) and continue training on top of the XLM-R model to enhance its domain cross-lingual ability. To validate the effectiveness of our model, we translate the English benchmarks of multiple tasks into Chinese. Experimental results demonstrate that our model significantly outperforms monolingual and multilingual pretrained models in cross-lingual zero-shot and few-shot scenarios, achieving improvements of up to 10+ points. Our code is publicly available at https://github.com/ngwlh-gl/KBioXLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Geng",
      "Xu Yan",
      "Ziqiang Cao",
      "Juntao Li",
      "Wenjie Li",
      "Sujian Li",
      "Xinjie Zhou",
      "Yang Yang",
      "Jun Zhang"
    ]
  },
  "http://arxiv.org/abs/2311.11552": {
    "title": "Exploring Prompting Large Language Models as Explainable Metrics",
    "volume": "Nov",
    "abstract": "This paper describes the IUST NLP Lab submission to the Prompting Large Language Models as Explainable Metrics Shared Task at the Eval4NLP 2023 Workshop on Evaluation & Comparison of NLP Systems. We have proposed a zero-shot prompt-based strategy for explainable evaluation of the summarization task using Large Language Models (LLMs). The conducted experiments demonstrate the promising potential of LLMs as evaluation metrics in Natural Language Processing (NLP), particularly in the field of summarization. Both few-shot and zero-shot approaches are employed in these experiments. The performance of our best provided prompts achieved a Kendall correlation of 0.477 with human evaluations in the text summarization task on the test data. Code and results are publicly available on GitHub",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ghazaleh Mahmoudi"
    ]
  },
  "http://arxiv.org/abs/2311.11551": {
    "title": "Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning",
    "volume": "Nov",
    "abstract": "Large language models (LLMs) have showcased their capability with few-shot inference known as in-context learning. However, in-domain demonstrations are not always readily available in real scenarios, leading to cross-domain in-context learning. Besides, LLMs are still facing challenges in long-tail knowledge in unseen and unfamiliar domains. The above limitations demonstrate the necessity of Unsupervised Domain Adaptation (UDA). In this paper, we study the UDA problem under an in-context learning setting to adapt language models from the source domain to the target domain without any target labels. The core idea is to retrieve a subset of cross-domain elements that are the most similar to the query, and elicit language model to adapt in an in-context manner by learning both target domain distribution and the discriminative task signal simultaneously with the augmented cross-domain in-context examples. We devise different prompting and training strategies, accounting for different LM architectures to learn the target distribution via language modeling. With extensive experiments on Sentiment Analysis (SA) and Named Entity Recognition (NER) tasks, we thoroughly study the effectiveness of ICL for domain transfer and demonstrate significant improvements over baseline models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quanyu Long",
      "Wenya Wang",
      "Sinno Jialin Pan"
    ]
  },
  "http://arxiv.org/abs/2311.11518": {
    "title": "Multi-teacher Distillation for Multilingual Spelling Correction",
    "volume": "Nov",
    "abstract": "Accurate spelling correction is a critical step in modern search interfaces, especially in an era of mobile devices and speech-to-text interfaces. For services that are deployed around the world, this poses a significant challenge for multilingual NLP: spelling errors need to be caught and corrected in all languages, and even in queries that use multiple languages. In this paper, we tackle this challenge using multi-teacher distillation. On our approach, a monolingual teacher model is trained for each language/locale, and these individual models are distilled into a single multilingual student model intended to serve all languages/locales. In experiments using open-source data as well as user data from a worldwide search service, we show that this leads to highly effective spelling correction models that can meet the tight latency requirements of deployed services",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingfen Zhang",
      "Xuan Guo",
      "Sravan Bodapati",
      "Christopher Potts"
    ]
  },
  "http://arxiv.org/abs/2311.11516": {
    "title": "GPT in Data Science: A Practical Exploration of Model Selection",
    "volume": "Nov",
    "abstract": "There is an increasing interest in leveraging Large Language Models (LLMs) for managing structured data and enhancing data science processes. Despite the potential benefits, this integration poses significant questions regarding their reliability and decision-making methodologies. It highlights the importance of various factors in the model selection process, including the nature of the data, problem type, performance metrics, computational resources, interpretability vs accuracy, assumptions about data, and ethical considerations. Our objective is to elucidate and express the factors and assumptions guiding GPT-4's model selection recommendations. We employ a variability model to depict these factors and use toy datasets to evaluate both the model and the implementation of the identified heuristics. By contrasting these outcomes with heuristics from other platforms, our aim is to determine the effectiveness and distinctiveness of GPT-4's methodology. This research is committed to advancing our comprehension of AI decision-making processes, especially in the realm of model selection within data science. Our efforts are directed towards creating AI systems that are more transparent and comprehensible, contributing to a more responsible and efficient practice in data science",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathalia Nascimento",
      "Cristina Tavares",
      "Paulo Alencar",
      "Donald Cowan"
    ]
  },
  "http://arxiv.org/abs/2311.11509": {
    "title": "Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information",
    "volume": "Nov",
    "abstract": "In recent years, Large Language Models (LLM) have emerged as pivotal tools in various applications. However, these models are susceptible to adversarial prompt attacks, where attackers can carefully curate input strings that lead to undesirable outputs. The inherent vulnerability of LLMs stems from their input-output mechanisms, especially when presented with intensely out-of-distribution (OOD) inputs. This paper proposes a token-level detection method to identify adversarial prompts, leveraging the LLM's capability to predict the next token's probability. We measure the degree of the model's perplexity and incorporate neighboring token information to encourage the detection of contiguous adversarial prompt sequences. As a result, we propose two methods: one that identifies each token as either being part of an adversarial prompt or not, and another that estimates the probability of each token being part of an adversarial prompt",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengmian Hu",
      "Gang Wu",
      "Saayan Mitra",
      "Ruiyi Zhang",
      "Tong Sun",
      "Heng Huang",
      "Vishy Swaminathan"
    ]
  },
  "http://arxiv.org/abs/2311.11482": {
    "title": "Meta Prompting for AGI Systems",
    "volume": "Nov",
    "abstract": "This paper presents an in-depth exploration of Meta Prompting, a novel technique that revolutionizes the way large language models (LLMs), multi-modal foundation models, and AI systems approach problem-solving and data interpretation. Meta Prompting, rooted in type theory and category theory, prioritizes the structure and syntax of information, providing a unique framework that transcends traditional content-focused methods. We delve into the formal definitions of Meta Prompting, contrasting it with Few-Shot Prompting, and highlight its applicability and superiority in various AI applications. Key to this exploration is the expansion of Meta Prompting into the realm of complex reasoning. Here, we demonstrate how this technique adeptly breaks down intricate problems into manageable sub-problems, facilitating a step-by-step, detailed approach to problem-solving. This method proves especially advantageous in terms of token efficiency and offering a fair comparison in problem-solving scenarios, standing out against few-shot example approaches. Furthermore, the paper breaks new ground by extending Meta Prompting into multi-modal foundation model settings. This extension addresses the integration of diverse data types, such as images, audio, and video, within the structured framework of Meta Prompting, highlighting both the challenges and the vast potential of this approach in handling complex, multi-faceted data (The code is available at https://github.com/meta-prompting/meta-prompting)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Zhang"
    ]
  },
  "http://arxiv.org/abs/2311.11477": {
    "title": "What's left can't be right -- The remaining positional incompetence of contrastive vision-language models",
    "volume": "Nov",
    "abstract": "Contrastive vision-language models like CLIP have been found to lack spatial understanding capabilities. In this paper we discuss the possible causes of this phenomenon by analysing both datasets and embedding space. By focusing on simple left-right positional relations, we show that this behaviour is entirely predictable, even with large-scale datasets, demonstrate that these relations can be taught using synthetic data and show that this approach can generalise well to natural images - improving the performance on left-right relations on Visual Genome Relations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nils Hoehing",
      "Ellen Rushe",
      "Anthony Ventresque"
    ]
  },
  "http://arxiv.org/abs/2311.11462": {
    "title": "LLM aided semi-supervision for Extractive Dialog Summarization",
    "volume": "Nov",
    "abstract": "Generating high-quality summaries for chat dialogs often requires large labeled datasets. We propose a method to efficiently use unlabeled data for extractive summarization of customer-agent dialogs. In our method, we frame summarization as a question-answering problem and use state-of-the-art large language models (LLMs) to generate pseudo-labels for a dialog. We then use these pseudo-labels to fine-tune a chat summarization model, effectively transferring knowledge from the large LLM into a smaller specialized model. We demonstrate our method on the \\tweetsumm dataset, and show that using 10\\% of the original labelled data set we can achieve 65.9/57.0/61.0 ROUGE-1/-2/-L, whereas the current state-of-the-art trained on the entire training data set obtains 65.16/55.81/64.37 ROUGE-1/-2/-L. In other words, in the worst case (i.e., ROUGE-L) we still effectively retain 94.7% of the performance while using only 10% of the data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nishant Mishra",
      "Gaurav Sahu",
      "Iacer Calixto",
      "Ameen Abu-Hanna",
      "Issam H. Laradji"
    ]
  },
  "http://arxiv.org/abs/2311.11441": {
    "title": "Spot the Bot: Distinguishing Human-Written and Bot-Generated Texts Using Clustering and Information Theory Techniques",
    "volume": "Nov",
    "abstract": "With the development of generative models like GPT-3, it is increasingly more challenging to differentiate generated texts from human-written ones. There is a large number of studies that have demonstrated good results in bot identification. However, the majority of such works depend on supervised learning methods that require labelled data and/or prior knowledge about the bot-model architecture. In this work, we propose a bot identification algorithm that is based on unsupervised learning techniques and does not depend on a large amount of labelled data. By combining findings in semantic analysis by clustering (crisp and fuzzy) and information techniques, we construct a robust model that detects a generated text for different types of bot. We find that the generated texts tend to be more chaotic while literary works are more complex. We also demonstrate that the clustering of human texts results in fuzzier clusters in comparison to the more compact and well-separated clusters of bot-generated texts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vasilii Gromov",
      "Quynh Nhu Dang"
    ]
  },
  "http://arxiv.org/abs/2311.11435": {
    "title": "Unveiling Public Perceptions: Machine Learning-Based Sentiment Analysis of COVID-19 Vaccines in India",
    "volume": "Nov",
    "abstract": "In March 2020, the World Health Organisation declared COVID-19 a global pandemic as it spread to nearly every country. By mid-2021, India had introduced three vaccines: Covishield, Covaxin, and Sputnik. To ensure successful vaccination in a densely populated country like India, understanding public sentiment was crucial. Social media, particularly Reddit with over 430 million users, played a vital role in disseminating information. This study employs data mining techniques to analyze Reddit data and gauge Indian sentiments towards COVID-19 vaccines. Using Python's Text Blob library, comments are annotated to assess general sentiments. Results show that most Reddit users in India expressed neutrality about vaccination, posing a challenge for the Indian government's efforts to vaccinate a significant portion of the population",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Milind Gupta",
      "Abhishek Kaushik"
    ]
  },
  "http://arxiv.org/abs/2311.11415": {
    "title": "A Security Risk Taxonomy for Large Language Models",
    "volume": "Nov",
    "abstract": "As large language models (LLMs) permeate more and more applications, an assessment of their associated security risks becomes increasingly necessary. The potential for exploitation by malicious actors, ranging from disinformation to data breaches and reputation damage, is substantial. This paper addresses a gap in current research by focusing on the security risks posed by LLMs, which extends beyond the widely covered ethical and societal implications. Our work proposes a taxonomy of security risks along the user-model communication pipeline, explicitly focusing on prompt-based attacks on LLMs. We categorize the attacks by target and attack type within a prompt-based interaction scheme. The taxonomy is reinforced with specific attack examples to showcase the real-world impact of these risks. Through this taxonomy, we aim to inform the development of robust and secure LLM applications, enhancing their safety and trustworthiness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erik Derner",
      "Kristina Batisti",
      "Jan Zahlka",
      "Robert Babuka"
    ]
  },
  "http://arxiv.org/abs/2311.11375": {
    "title": "ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding",
    "volume": "Nov",
    "abstract": "Spoken language understanding (SLU) is a fundamental task in the task-oriented dialogue systems. However, the inevitable errors from automatic speech recognition (ASR) usually impair the understanding performance and lead to error propagation. Although there are some attempts to address this problem through contrastive learning, they (1) treat clean manual transcripts and ASR transcripts equally without discrimination in fine-tuning; (2) neglect the fact that the semantically similar pairs are still pushed away when applying contrastive learning; (3) suffer from the problem of Kullback-Leibler (KL) vanishing. In this paper, we propose Mutual Learning and Large-Margin Contrastive Learning (ML-LMCL), a novel framework for improving ASR robustness in SLU. Specifically, in fine-tuning, we apply mutual learning and train two SLU models on the manual transcripts and the ASR transcripts, respectively, aiming to iteratively share knowledge between these two models. We also introduce a distance polarization regularizer to avoid pushing away the intra-cluster pairs as much as possible. Moreover, we use a cyclical annealing schedule to mitigate KL vanishing issue. Experiments on three datasets show that ML-LMCL outperforms existing models and achieves new state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuxin Cheng",
      "Bowen Cao",
      "Qichen Ye",
      "Zhihong Zhu",
      "Hongxiang Li",
      "Yuexian Zou"
    ]
  },
  "http://arxiv.org/abs/2311.11331": {
    "title": "Portuguese FAQ for Financial Services",
    "volume": "Nov",
    "abstract": "Scarcity of domain-specific data in the Portuguese financial domain has disfavored the development of Natural Language Processing (NLP) applications. To address this limitation, the present study advocates for the utilization of synthetic data generated through data augmentation techniques. The investigation focuses on the augmentation of a dataset sourced from the Central Bank of Brazil FAQ, employing techniques that vary in semantic similarity. Supervised and unsupervised tasks are conducted to evaluate the impact of augmented data on both low and high semantic similarity scenarios. Additionally, the resultant dataset will be publicly disseminated on the Hugging Face Datasets platform, thereby enhancing accessibility and fostering broader engagement within the NLP research community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paulo Finardi",
      "Wanderley M. Melo",
      "Edgard D. Medeiros Neto",
      "Alex F. Mansano",
      "Pablo B. Costa",
      "Vinicius F. Carid"
    ]
  },
  "http://arxiv.org/abs/2311.11301": {
    "title": "CHAMP: Efficient Annotation and Consolidation of Cluster Hierarchies",
    "volume": "Nov",
    "abstract": "Various NLP tasks require a complex hierarchical structure over nodes, where each node is a cluster of items. Examples include generating entailment graphs, hierarchical cross-document coreference resolution, annotating event and subevent relations, etc. To enable efficient annotation of such hierarchical structures, we release CHAMP, an open source tool allowing to incrementally construct both clusters and hierarchy simultaneously over any type of texts. This incremental approach significantly reduces annotation time compared to the common pairwise annotation approach and also guarantees maintaining transitivity at the cluster and hierarchy levels. Furthermore, CHAMP includes a consolidation mode, where an adjudicator can easily compare multiple cluster hierarchy annotations and resolve disagreements",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arie Cattan",
      "Tom Hope",
      "Doug Downey",
      "Roy Bar-Haim",
      "Lilach Eden",
      "Yoav Kantor",
      "Ido Dagan"
    ]
  },
  "http://arxiv.org/abs/2311.11271": {
    "title": "A Cross-Attention Augmented Model for Event-Triggered Context-Aware Story Generation",
    "volume": "Nov",
    "abstract": "Despite recent advancements, existing story generation systems continue to encounter difficulties in effectively incorporating contextual and event features, which greatly influence the quality of generated narratives. To tackle these challenges, we introduce a novel neural generation model, EtriCA, that enhances the relevance and coherence of generated stories by employing a cross-attention mechanism to map context features onto event sequences through residual mapping. This feature capturing mechanism enables our model to exploit logical relationships between events more effectively during the story generation process. To further enhance our proposed model, we employ a post-training framework for knowledge enhancement (KeEtriCA) on a large-scale book corpus. This allows EtriCA to adapt to a wider range of data samples. This results in approximately 5\\% improvement in automatic metrics and over 10\\% improvement in human evaluation. We conduct extensive experiments, including comparisons with state-of-the-art (SOTA) baseline models, to evaluate the performance of our framework on story generation. The experimental results, encompassing both automated metrics and human assessments, demonstrate the superiority of our model over existing state-of-the-art baselines. These results underscore the effectiveness of our model in leveraging context and event features to improve the quality of generated narratives",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Tang",
      "Tyler Loakman",
      "Chenghua Lin"
    ]
  },
  "http://arxiv.org/abs/2311.11268": {
    "title": "Towards Real-World Writing Assistance: A Chinese Character Checking Benchmark with Faked and Misspelled Characters",
    "volume": "Nov",
    "abstract": "Writing assistance is an application closely related to human life and is also a fundamental Natural Language Processing (NLP) research field. Its aim is to improve the correctness and quality of input texts, with character checking being crucial in detecting and correcting wrong characters. From the perspective of the real world where handwriting occupies the vast majority, characters that humans get wrong include faked characters (i.e., untrue characters created due to writing errors) and misspelled characters (i.e., true characters used incorrectly due to spelling errors). However, existing datasets and related studies only focus on misspelled characters mainly caused by phonological or visual confusion, thereby ignoring faked characters which are more common and difficult. To break through this dilemma, we present Visual-C$^3$, a human-annotated Visual Chinese Character Checking dataset with faked and misspelled Chinese characters. To the best of our knowledge, Visual-C$^3$ is the first real-world visual and the largest human-crafted dataset for the Chinese character checking scenario. Additionally, we also propose and evaluate novel baseline methods on Visual-C$^3$. Extensive empirical results and analyses show that Visual-C$^3$ is high-quality yet challenging. The Visual-C$^3$ dataset and the baseline methods will be publicly available to facilitate further research in the community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinghui Li",
      "Zishan Xu",
      "Shaoshen Chen",
      "Haojing Huang",
      "Yangning Li",
      "Yong Jiang",
      "Zhongli Li",
      "Qingyu Zhou",
      "Hai-Tao Zheng",
      "Ying Shen"
    ]
  },
  "http://arxiv.org/abs/2311.11267": {
    "title": "Rethinking Large Language Models in Mental Health Applications",
    "volume": "Nov",
    "abstract": "Large Language Models (LLMs) have become valuable assets in mental health, showing promise in both classification tasks and counseling applications. This paper offers a perspective on using LLMs in mental health applications. It discusses the instability of generative models for prediction and the potential for generating hallucinatory outputs, underscoring the need for ongoing audits and evaluations to maintain their reliability and dependability. The paper also distinguishes between the often interchangeable terms ``explainability'' and ``interpretability'', advocating for developing inherently interpretable methods instead of relying on potentially hallucinated self-explanations generated by LLMs. Despite the advancements in LLMs, human counselors' empathetic understanding, nuanced interpretation, and contextual awareness remain irreplaceable in the sensitive and complex realm of mental health counseling. The use of LLMs should be approached with a judicious and considerate mindset, viewing them as tools that complement human expertise rather than seeking to replace it",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaoxiong Ji",
      "Tianlin Zhang",
      "Kailai Yang",
      "Sophia Ananiadou",
      "Erik Cambria"
    ]
  },
  "http://arxiv.org/abs/2311.11229": {
    "title": "Causal ATE Mitigates Unintended Bias in Controlled Text Generation",
    "volume": "Nov",
    "abstract": "We study attribute control in language models through the method of Causal Average Treatment Effect (Causal ATE). Existing methods for the attribute control task in Language Models (LMs) check for the co-occurrence of words in a sentence with the attribute of interest, and control for them. However, spurious correlation of the words with the attribute in the training dataset, can cause models to hallucinate the presence of the attribute when presented with the spurious correlate during inference. We show that the simple perturbation-based method of Causal ATE removes this unintended effect. Additionally, we offer a theoretical foundation for investigating Causal ATE in the classification task, and prove that it reduces the number of false positives -- thereby mitigating the issue of unintended bias. Specifically, we ground it in the problem of toxicity mitigation, where a significant challenge lies in the inadvertent bias that often emerges towards protected groups post detoxification. We show that this unintended bias can be solved by the use of the Causal ATE metric",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rahul Madhavan",
      "Kahini Wadhawan"
    ]
  },
  "http://arxiv.org/abs/2311.11215": {
    "title": "SPLAIN: Augmenting CybersecurityWarnings with Reasons and Data",
    "volume": "Nov",
    "abstract": "Effective cyber threat recognition and prevention demand comprehensible forecasting systems, as prior approaches commonly offer limited and, ultimately, unconvincing information. We introduce Simplified Plaintext Language (SPLAIN), a natural language generator that converts warning data into user-friendly cyber threat explanations. SPLAIN is designed to generate clear, actionable outputs, incorporating hierarchically organized explanatory details about input data and system functionality. Given the inputs of individual sensor-induced forecasting signals and an overall warning from a fusion module, SPLAIN queries each signal for information on contributing sensors and data signals. This collected data is processed into a coherent English explanation, encompassing forecasting, sensing, and data elements for user review. SPLAIN's template-based approach ensures consistent warning structure and vocabulary. SPLAIN's hierarchical output structure allows each threat and its components to be expanded to reveal underlying explanations on demand. Our conclusions emphasize the need for designers to specify the \"how\" and \"why\" behind cyber warnings, advocate for simple structured templates in generating consistent explanations, and recognize that direct causal links in Machine Learning approaches may not always be identifiable, requiring some explanations to focus on general methodologies, such as model and training data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vera A. Kazakova",
      "Jena D. Hwang",
      "Bonnie J. Dorr",
      "Yorick Wilks",
      "J. Blake Gage",
      "Alex Memory",
      "Mark A. Clark"
    ]
  },
  "http://arxiv.org/abs/2311.11202": {
    "title": "Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models",
    "volume": "Nov",
    "abstract": "Language models have shown promise in various tasks but can be affected by undesired data during training, fine-tuning, or alignment. For example, if some unsafe conversations are wrongly annotated as safe ones, the model fine-tuned on these samples may be harmful. Therefore, the correctness of annotations, i.e., the credibility of the dataset, is important. This study focuses on the credibility of real-world datasets, including the popular benchmarks Jigsaw Civil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that can be used for training a harmless language model. Given the cost and difficulty of cleaning these datasets by humans, we introduce a systematic framework for evaluating the credibility of datasets, identifying label errors, and evaluating the influence of noisy labels in the curated language data, specifically focusing on unsafe comments and conversation classification. With the framework, we find and fix an average of 6.16% label errors in 11 datasets constructed from the above benchmarks. The data credibility and downstream learning performance can be remarkably improved by directly fixing label errors, indicating the significance of cleaning existing real-world datasets. Open-source: https://github.com/Docta-ai/docta",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaowei Zhu",
      "Jialu Wang",
      "Hao Cheng",
      "Yang Liu"
    ]
  }
}