{
  "http://arxiv.org/abs/2312.10007": {
    "title": "Faithful Persona-based Conversational Dataset Generation with Large Language Models",
    "volume": "Dec",
    "abstract": "High-quality conversational datasets are essential for developing AI models that can communicate with users. One way to foster deeper interactions between a chatbot and its user is through personas, aspects of the user's character that provide insights into their personality, motivations, and behaviors. Training Natural Language Processing (NLP) models on a diverse and comprehensive persona-based dataset can lead to conversational models that create a deeper connection with the user, and maintain their engagement. In this paper, we leverage the power of Large Language Models (LLMs) to create a large, high-quality conversational dataset from a seed dataset. We propose a Generator-Critic architecture framework to expand the initial dataset, while improving the quality of its conversations. The Generator is an LLM prompted to output conversations. The Critic consists of a mixture of expert LLMs that control the quality of the generated conversations. These experts select the best generated conversations, which we then use to improve the Generator. We release Synthetic-Persona-Chat, consisting of 20k conversations seeded from Persona-Chat. We evaluate the quality of Synthetic-Persona-Chat and our generation framework on different dimensions through extensive experiments, and observe that the losing rate of Synthetic-Persona-Chat against Persona-Chat during Turing test decreases from 17.2% to 8.8% over three iterations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pegah Jandaghi",
      "XiangHai Sheng",
      "Xinyi Bai",
      "Jay Pujara",
      "Hakim Sidahmed"
    ]
  },
  "http://arxiv.org/abs/2312.10003": {
    "title": "ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent",
    "volume": "Dec",
    "abstract": "Answering complex natural language questions often necessitates multi-step reasoning and integrating external information. Several systems have combined knowledge retrieval with a large language model (LLM) to answer such questions. These systems, however, suffer from various failure cases, and we cannot directly train them end-to-end to fix such failures, as interaction with external knowledge is non-differentiable. To address these deficiencies, we define a ReAct-style LLM agent with the ability to reason and act upon external knowledge. We further refine the agent through a ReST-like method that iteratively trains on previous trajectories, employing growing-batch reinforcement learning with AI feedback for continuous self-improvement and self-distillation. Starting from a prompted large model and after just two iterations of the algorithm, we can produce a fine-tuned small model that achieves comparable performance on challenging compositional question-answering benchmarks with two orders of magnitude fewer parameters",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renat Aksitov",
      "Sobhan Miryoosefi",
      "Zonglin Li",
      "Daliang Li",
      "Sheila Babayan",
      "Kavya Kopparapu",
      "Zachary Fisher",
      "Ruiqi Guo",
      "Sushant Prakash",
      "Pranesh Srinivasan",
      "Manzil Zaheer",
      "Felix Yu",
      "Sanjiv Kumar"
    ]
  },
  "http://arxiv.org/abs/2312.09993": {
    "title": "LLaMAntino: LLaMA 2 Models for Effective Text Generation in Italian Language",
    "volume": "Dec",
    "abstract": "Large Language Models represent state-of-the-art linguistic models designed to equip computers with the ability to comprehend natural language. With its exceptional capacity to capture complex contextual relationships, the LLaMA (Large Language Model Meta AI) family represents a novel advancement in the field of natural language processing by releasing foundational models designed to improve the natural language understanding abilities of the transformer architecture thanks to their large amount of trainable parameters (7, 13, and 70 billion parameters). In many natural language understanding tasks, these models obtain the same performances as private company models such as OpenAI Chat-GPT with the advantage to make publicly available weights and code for research and commercial uses. In this work, we investigate the possibility of Language Adaptation for LLaMA models, explicitly focusing on addressing the challenge of Italian Language coverage. Adopting an open science approach, we explore various tuning approaches to ensure a high-quality text generated in Italian suitable for common tasks in this underrepresented language in the original models' datasets. We aim to release effective text generation models with strong linguistic properties for many tasks that seem challenging using multilingual or general-purpose LLMs. By leveraging an open science philosophy, this study contributes to Language Adaptation strategies for the Italian language by introducing the novel LLaMAntino family of Italian LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierpaolo Basile",
      "Elio Musacchio",
      "Marco Polignano",
      "Lucia Siciliani",
      "Giuseppe Fiameni",
      "Giovanni Semeraro"
    ]
  },
  "http://arxiv.org/abs/2312.09979": {
    "title": "The Art of Balancing: Revolutionizing Mixture of Experts for Maintaining World Knowledge in Language Model Alignment",
    "volume": "Dec",
    "abstract": "Supervised fine-tuning (SFT) is a crucial step for large language models (LLMs), enabling them to align with human instructions and enhance their capabilities in downstream tasks. When the models are required to align with a broader range of downstream tasks, or there is a desire to notably improve the performance on a specific task, a substantial increase in fine-tuning data often emerges as the solution. However, we find that large-scale increases in instruction data can disrupt the world knowledge previously stored in the LLMs, i.e., world knowledge forgetting. In this paper, we introduce LoRAMoE to address above challenge. The LoRAMoE is a plugin version of Mixture of Experts (MoE). The plugin-form ensures the integrity of world knowledge by freezing the backbone model during the training phase. And we propose the use of localized balancing constraints to coordinate parts of experts for task utilization, meanwhile enables other experts to to fully leverage the world knowledge stored in the models. Experimental results demonstrate that LoRAMoE can reasonly coordinate experts based on data type during inference, and even dramatically increasing instruction data does not result in knowledge forgetting. Moreover, LoRAMoE provides additional benefits for the performance of downstream tasks, indicating the potential of our approach for multi-task learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shihan Dou",
      "Enyu Zhou",
      "Yan Liu",
      "Songyang Gao",
      "Jun Zhao",
      "Wei Shen",
      "Yuhao Zhou",
      "Zhiheng Xi",
      "Xiao Wang",
      "Xiaoran Fan",
      "Shiliang Pu",
      "Jiang Zhu",
      "Rui Zheng",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "http://arxiv.org/abs/2312.09966": {
    "title": "Data and Approaches for German Text simplification -- towards an Accessibility-enhanced Communication",
    "volume": "Dec",
    "abstract": "This paper examines the current state-of-the-art of German text simplification, focusing on parallel and monolingual German corpora. It reviews neural language models for simplifying German texts and assesses their suitability for legal texts and accessibility requirements. Our findings highlight the need for additional training data and more appropriate approaches that consider the specific linguistic characteristics of German, as well as the importance of the needs and preferences of target groups with cognitive or language impairments. The authors launched the interdisciplinary OPEN-LS project in April 2023 to address these research gaps. The project aims to develop a framework for text formats tailored to individuals with low literacy levels, integrate legal texts, and enhance comprehensibility for those with linguistic or cognitive impairments. It will also explore cost-effective ways to enhance the data with audience-specific illustrations using image-generating AI. For more and up-to-date information, please visit our project homepage https://open-ls.entavis.com",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thorben Schomacker",
      "Michael Gille",
      "Jörg von der Hülls",
      "Marina Tropmann-Frick"
    ]
  },
  "http://arxiv.org/abs/2312.09932": {
    "title": "RDR: the Recap, Deliberate, and Respond Method for Enhanced Language Understanding",
    "volume": "Dec",
    "abstract": "Natural language understanding (NLU) using neural network pipelines often requires additional context that is not solely present in the input data. Through Prior research, it has been evident that NLU benchmarks are susceptible to manipulation by neural models, wherein these models exploit statistical artifacts within the encoded external knowledge to artificially inflate performance metrics for downstream tasks. Our proposed approach, known as the Recap, Deliberate, and Respond (RDR) paradigm, addresses this issue by incorporating three distinct objectives within the neural network pipeline. Firstly, the Recap objective involves paraphrasing the input text using a paraphrasing model in order to summarize and encapsulate its essence. Secondly, the Deliberation objective entails encoding external graph information related to entities mentioned in the input text, utilizing a graph embedding model. Finally, the Respond objective employs a classification head model that utilizes representations from the Recap and Deliberation modules to generate the final prediction. By cascading these three models and minimizing a combined loss, we mitigate the potential for gaming the benchmark and establish a robust method for capturing the underlying semantic patterns, thus enabling accurate predictions. To evaluate the effectiveness of the RDR method, we conduct tests on multiple GLUE benchmark tasks. Our results demonstrate improved performance compared to competitive baselines, with an enhancement of up to 2\\% on standard metrics. Furthermore, we analyze the observed evidence for semantic understanding exhibited by RDR models, emphasizing their ability to avoid gaming the benchmark and instead accurately capture the true underlying semantic patterns",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Zi",
      "Hariram Veeramani",
      "Kaushik Roy",
      "Amit Sheth"
    ]
  },
  "http://arxiv.org/abs/2312.09917": {
    "title": "Red AI? Inconsistent Responses from GPT3.5 Models on Political Issues in the US and China",
    "volume": "Dec",
    "abstract": "The rising popularity of ChatGPT and other AI-powered large language models (LLMs) has led to increasing studies highlighting their susceptibility to mistakes and biases. However, most of these studies focus on models trained on English texts. Taking an innovative approach, this study investigates political biases in GPT's multilingual models. We posed the same question about high-profile political issues in the United States and China to GPT in both English and simplified Chinese, and our analysis of the bilingual responses revealed that GPT's bilingual models' political \"knowledge\" (content) and the political \"attitude\" (sentiment) are significantly more inconsistent on political issues in China. The simplified Chinese GPT models not only tended to provide pro-China information but also presented the least negative sentiment towards China's problems, whereas the English GPT was significantly more negative towards China. This disparity may stem from Chinese state censorship and US-China geopolitical tensions, which influence the training corpora of GPT bilingual models. Moreover, both Chinese and English models tended to be less critical towards the issues of \"their own\" represented by the language used, than the issues of \"the other.\" This suggests that GPT multilingual models could potentially develop a \"political identity\" and an associated sentiment bias based on their training language. We discussed the implications of our findings for information transmission and communication in an increasingly divided world",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Zhou",
      "Yinxian Zhang"
    ]
  },
  "http://arxiv.org/abs/2312.09907": {
    "title": "Exploring Automatic Text Simplification of German Narrative Documents",
    "volume": "Dec",
    "abstract": "In this paper, we apply transformer-based Natural Language Generation (NLG) techniques to the problem of text simplification. Currently, there are only a few German datasets available for text simplification, even fewer with larger and aligned documents, and not a single one with narrative texts. In this paper, we explore to which degree modern NLG techniques can be applied to German narrative text simplifications. We use Longformer attention and a pre-trained mBART model. Our findings indicate that the existing approaches for German are not able to solve the task properly. We conclude on a few directions for future research to address this problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thorben Schomacker",
      "Tillmann Dönicke",
      "Marina Tropmann-Frick"
    ]
  },
  "http://arxiv.org/abs/2312.09895": {
    "title": "Generative Context-aware Fine-tuning of Self-supervised Speech Models",
    "volume": "Dec",
    "abstract": "When performing tasks like automatic speech recognition or spoken language understanding for a given utterance, access to preceding text or audio provides contextual information can improve performance. Considering the recent advances in generative large language models (LLM), we hypothesize that an LLM could generate useful context information using the preceding text. With appropriate prompts, LLM could generate a prediction of the next sentence or abstractive text like titles or topics. In this paper, we study the use of LLM-generated context information and propose an approach to distill the generated information during fine-tuning of self-supervised speech models, which we refer to as generative context-aware fine-tuning. This approach allows the fine-tuned model to make improved predictions without access to the true surrounding segments or to the LLM at inference time, while requiring only a very small additional context module. We evaluate the proposed approach using the SLUE and Libri-light benchmarks for several downstream tasks: automatic speech recognition, named entity recognition, and sentiment analysis. The results show that generative context-aware fine-tuning outperforms a context injection fine-tuning approach that accesses the ground-truth previous text, and is competitive with a generative context injection fine-tuning approach that requires the LLM at inference time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suwon Shon",
      "Kwangyoun Kim",
      "Prashant Sridhar",
      "Yi-Te Hsu",
      "Shinji Watanabe",
      "Karen Livescu"
    ]
  },
  "http://arxiv.org/abs/2312.09890": {
    "title": "Grammatical information in BERT sentence embeddings as two-dimensional arrays",
    "volume": "Dec",
    "abstract": "Sentence embeddings induced with various transformer architectures encode much semantic and syntactic information in a distributed manner in a one-dimensional array. We investigate whether specific grammatical information can be accessed in these distributed representations. Using data from a task developed to test rule-like generalizations, our experiments on detecting subject-verb agreement yield several promising results. First, we show that while the usual sentence representations encoded as one-dimensional arrays do not easily support extraction of rule-like regularities, a two-dimensional reshaping of these vectors allows various learning architectures to access such information. Next, we show that various architectures can detect patterns in these two-dimensional reshaped sentence embeddings and successfully learn a model based on smaller amounts of simpler training data, which performs well on more complex test data. This indicates that current sentence embeddings contain information that is regularly distributed, and which can be captured when the embeddings are reshaped into higher dimensional arrays. Our results cast light on representations produced by language models and help move towards developing few-shot learning approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vivi Nastase",
      "Paola Merlo"
    ]
  },
  "http://arxiv.org/abs/2312.09818": {
    "title": "SMILE: Multimodal Dataset for Understanding Laughter in Video with Language Models",
    "volume": "Dec",
    "abstract": "Despite the recent advances of the artificial intelligence, building social intelligence remains a challenge. Among social signals, laughter is one of the distinctive expressions that occurs during social interactions between humans. In this work, we tackle a new challenge for machines to understand the rationale behind laughter in video, Video Laugh Reasoning. We introduce this new task to explain why people laugh in a particular video and a dataset for this task. Our proposed dataset, SMILE, comprises video clips and language descriptions of why people laugh. We propose a baseline by leveraging the reasoning capacity of large language models (LLMs) with textual video representation. Experiments show that our baseline can generate plausible explanations for laughter. We further investigate the scalability of our baseline by probing other video understanding tasks and in-the-wild videos. We release our dataset, code, and model checkpoints on https://github.com/SMILE-data/SMILE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lee Hyun",
      "Kim Sung-Bin",
      "Seungju Han",
      "Youngjae Yu",
      "Tae-Hyun Oh"
    ]
  },
  "http://arxiv.org/abs/2312.09806": {
    "title": "Improving Biomedical Entity Linking with Retrieval-enhanced Learning",
    "volume": "Dec",
    "abstract": "Biomedical entity linking (BioEL) has achieved remarkable progress with the help of pre-trained language models. However, existing BioEL methods usually struggle to handle rare and difficult entities due to long-tailed distribution. To address this limitation, we introduce a new scheme $k$NN-BioEL, which provides a BioEL model with the ability to reference similar instances from the entire training corpus as clues for prediction, thus improving the generalization capabilities. Moreover, we design a contrastive learning objective with dynamic hard negative sampling (DHNS) that improves the quality of the retrieved neighbors during inference. Extensive experimental results show that $k$NN-BioEL outperforms state-of-the-art baselines on several datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenxi Lin",
      "Ziheng Zhang",
      "Xian Wu",
      "Yefeng Zheng"
    ]
  },
  "http://arxiv.org/abs/2312.09801": {
    "title": "ProCoT: Stimulating Critical Thinking and Writing of Students through Engagement with Large Language Models (LLMs)",
    "volume": "Dec",
    "abstract": "We introduce a novel writing method called Probing Chain of Thought (ProCoT), which prevents students from cheating using a Large Language Model (LLM), such as ChatGPT, while enhancing their active learning through such models. LLMs have disrupted education and many other feilds. For fear of students cheating, many educationists have resorted to banning their use, as their outputs can be human-like and hard to detect in some cases. These LLMs are also known for hallucinations (i.e. fake facts). We conduct studies with ProCoT in two different courses with a combined total of about 66 students. The students in each course were asked to prompt an LLM of their choice with one question from a set of four and required to affirm or refute statements in the LLM output by using peer reviewed references. The results show two things: (1) ProCoT stimulates creative/critical thinking and writing of students through engagement with LLMs when we compare the LLM solely output to ProCoT output and (2) ProCoT can prevent cheating because of clear limitations in existing LLMs when we compare students ProCoT output to LLM ProCoT output. We also discover that most students prefer to give answers in fewer words than LLMs, which are typically verbose. The average word counts for students, ChatGPT (v3.5) and Phind (v8) are 208, 391 and 383, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tosin Adewumi",
      "Lama Alkhaled",
      "Claudia Buck",
      "Sergio Hernandez",
      "Saga Brilioth",
      "Mkpe Kekung",
      "Yelvin Ragimov",
      "Elisa Barney"
    ]
  },
  "http://arxiv.org/abs/2312.09785": {
    "title": "RJUA-QA: A Comprehensive QA Dataset for Urology",
    "volume": "Dec",
    "abstract": "We introduce RJUA-QA, a novel medical dataset for question answering (QA) and reasoning with clinical evidence, contributing to bridge the gap between general large language models (LLMs) and medical-specific LLM applications. RJUA-QA is derived from realistic clinical scenarios and aims to facilitate LLMs in generating reliable diagnostic and advice. The dataset contains 2,132 curated Question-Context-Answer pairs, corresponding about 25,000 diagnostic records and clinical cases. The dataset covers 67 common urological disease categories, where the disease coverage exceeds 97.6\\% of the population seeking medical services in urology. Each data instance in RJUA-QA comprises: (1) a question mirroring real patient to inquiry about clinical symptoms and medical conditions, (2) a context including comprehensive expert knowledge, serving as a reference for medical examination and diagnosis, (3) a doctor response offering the diagnostic conclusion and suggested examination guidance, (4) a diagnosed clinical disease as the recommended diagnostic outcome, and (5) clinical advice providing recommendations for medical examination. RJUA-QA is the first medical QA dataset for clinical reasoning over the patient inquiries, where expert-level knowledge and experience are required for yielding diagnostic conclusions and medical examination advice. A comprehensive evaluation is conducted to evaluate the performance of both medical-specific and general LLMs on the RJUA-QA dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiwei Lyu",
      "Chenfei Chi",
      "Hongbo Cai",
      "Lei Shi",
      "Xiaoyan Yang",
      "Lei Liu",
      "Xiang Chen",
      "Deng Zhao",
      "Zhiqiang Zhang",
      "Xianguo Lyu",
      "Ming Zhang",
      "Fangzhou Li",
      "Xiaowei Ma",
      "Yue Shen",
      "Jinjie Gu",
      "Wei Xue",
      "Yiran Huang"
    ]
  },
  "http://arxiv.org/abs/2312.09781": {
    "title": "GSQA: An End-to-End Model for Generative Spoken Question Answering",
    "volume": "Dec",
    "abstract": "In recent advancements in spoken question answering (QA), end-to-end models have made significant strides. However, previous research has primarily focused on extractive span selection. While this extractive-based approach is effective when answers are present directly within the input, it falls short in addressing abstractive questions, where answers are not directly extracted but inferred from the given information. To bridge this gap, we introduce the first end-to-end Generative Spoken Question Answering (GSQA) model that empowers the system to engage in abstractive reasoning. The challenge in training our GSQA model lies in the absence of a spoken abstractive QA dataset. We propose using text models for initialization and leveraging the extractive QA dataset to transfer knowledge from the text generative model to the spoken generative model. Experimental results indicate that our model surpasses the previous extractive model by 3% on extractive QA datasets. Furthermore, the GSQA model has only been fine-tuned on the spoken extractive QA dataset. Despite not having seen any spoken abstractive QA data, it can still closely match the performance of the cascade model. In conclusion, our GSQA model shows the potential to generalize to a broad spectrum of questions, thus further expanding spoken question answering capabilities of abstractive QA. Our code is available at \\href{https://voidful.github.io/GSQA}{https://voidful.github.io/GSQA}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Min-Han Shih",
      "Ho-Lam Chung",
      "Yu-Chi Pai",
      "Ming-Hao Hsu",
      "Guan-Ting Lin",
      "Shang-Wen Li",
      "Hung-yi Lee"
    ]
  },
  "http://arxiv.org/abs/2312.09736": {
    "title": "HEAR: Hearing Enhanced Audio Response for Video-grounded Dialogue",
    "volume": "Dec",
    "abstract": "Video-grounded Dialogue (VGD) aims to answer questions regarding a given multi-modal input comprising video, audio, and dialogue history. Although there have been numerous efforts in developing VGD systems to improve the quality of their responses, existing systems are competent only to incorporate the information in the video and text and tend to struggle in extracting the necessary information from the audio when generating appropriate responses to the question. The VGD system seems to be deaf, and thus, we coin this symptom of current systems' ignoring audio data as a deaf response. To overcome the deaf response problem, Hearing Enhanced Audio Response (HEAR) framework is proposed to perform sensible listening by selectively attending to audio whenever the question requires it. The HEAR framework enhances the accuracy and audibility of VGD systems in a model-agnostic manner. HEAR is validated on VGD datasets (i.e., AVSD@DSTC7 and AVSD@DSTC8) and shows effectiveness with various VGD systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunjae Yoon",
      "Dahyun Kim",
      "Eunseop Yoon",
      "Hee Suk Yoon",
      "Junyeong Kim",
      "Chnag D. Yoo"
    ]
  },
  "http://arxiv.org/abs/2312.09718": {
    "title": "Discovering Highly Influential Shortcut Reasoning: An Automated Template-Free Approach",
    "volume": "Dec",
    "abstract": "Shortcut reasoning is an irrational process of inference, which degrades the robustness of an NLP model. While a number of previous work has tackled the identification of shortcut reasoning, there are still two major limitations: (i) a method for quantifying the severity of the discovered shortcut reasoning is not provided; (ii) certain types of shortcut reasoning may be missed. To address these issues, we propose a novel method for identifying shortcut reasoning. The proposed method quantifies the severity of the shortcut reasoning by leveraging out-of-distribution data and does not make any assumptions about the type of tokens triggering the shortcut reasoning. Our experiments on Natural Language Inference and Sentiment Analysis demonstrate that our framework successfully discovers known and unknown shortcut reasoning in the previous work",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daichi Haraguchi",
      "Kiyoaki Shirai",
      "Naoya Inoue",
      "Natthawut Kertkeidkachorn"
    ]
  },
  "http://arxiv.org/abs/2312.09670": {
    "title": "Probing Pretrained Language Models with Hierarchy Properties",
    "volume": "Dec",
    "abstract": "Since Pretrained Language Models (PLMs) are the cornerstone of the most recent Information Retrieval (IR) models, the way they encode semantic knowledge is particularly important. However, little attention has been given to studying the PLMs' capability to capture hierarchical semantic knowledge. Traditionally, evaluating such knowledge encoded in PLMs relies on their performance on a task-dependent evaluation approach based on proxy tasks, such as hypernymy detection. Unfortunately, this approach potentially ignores other implicit and complex taxonomic relations. In this work, we propose a task-agnostic evaluation method able to evaluate to what extent PLMs can capture complex taxonomy relations, such as ancestors and siblings. The evaluation is based on intrinsic properties that capture the hierarchical nature of taxonomies. Our experimental evaluation shows that the lexico-semantic knowledge implicitly encoded in PLMs does not always capture hierarchical relations. We further demonstrate that the proposed properties can be injected into PLMs to improve their understanding of hierarchy. Through evaluations on taxonomy reconstruction, hypernym discovery and reading comprehension tasks, we show that the knowledge about hierarchy is moderately but not systematically transferable across tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jesús Lovón-Melgarejo",
      "Jose G. Moreno",
      "Romaric Besançon",
      "Olivier Ferret",
      "Lynda Tamine"
    ]
  },
  "http://arxiv.org/abs/2312.09625": {
    "title": "Weakly-Supervised 3D Visual Grounding based on Visual Linguistic Alignment",
    "volume": "Dec",
    "abstract": "Learning to ground natural language queries to target objects or regions in 3D point clouds is quite essential for 3D scene understanding. Nevertheless, existing 3D visual grounding approaches require a substantial number of bounding box annotations for text queries, which is time-consuming and labor-intensive to obtain. In this paper, we propose \\textbf{3D-VLA}, a weakly supervised approach for \\textbf{3D} visual grounding based on \\textbf{V}isual \\textbf{L}inguistic \\textbf{A}lignment. Our 3D-VLA exploits the superior ability of current large-scale vision-language models (VLMs) on aligning the semantics between texts and 2D images, as well as the naturally existing correspondences between 2D images and 3D point clouds, and thus implicitly constructs correspondences between texts and 3D point clouds with no need for fine-grained box annotations in the training procedure. During the inference stage, the learned text-3D correspondence will help us ground the text queries to the 3D target objects even without 2D images. To the best of our knowledge, this is the first work to investigate 3D visual grounding in a weakly supervised manner by involving large scale vision-language models, and extensive experiments on ReferIt3D and ScanRefer datasets demonstrate that our 3D-VLA achieves comparable and even superior results over the fully supervised methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxu Xu",
      "Yitian Yuan",
      "Qiudan Zhang",
      "Wenhui Wu",
      "Zequn Jie",
      "Lin Ma",
      "Xu Wang"
    ]
  },
  "http://arxiv.org/abs/2312.09601": {
    "title": "Binary Code Summarization: Benchmarking ChatGPT/GPT-4 and Other Large Language Models",
    "volume": "Dec",
    "abstract": "Binary code summarization, while invaluable for understanding code semantics, is challenging due to its labor-intensive nature. This study delves into the potential of large language models (LLMs) for binary code comprehension. To this end, we present BinSum, a comprehensive benchmark and dataset of over 557K binary functions and introduce a novel method for prompt synthesis and optimization. To more accurately gauge LLM performance, we also propose a new semantic similarity metric that surpasses traditional exact-match approaches. Our extensive evaluation of prominent LLMs, including ChatGPT, GPT-4, Llama 2, and Code Llama, reveals 10 pivotal insights. This evaluation generates 4 billion inference tokens, incurred a total expense of 11,418 US dollars and 873 NVIDIA A100 GPU hours. Our findings highlight both the transformative potential of LLMs in this field and the challenges yet to be overcome",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Jin",
      "Jonathan Larson",
      "Weiwei Yang",
      "Zhiqiang Lin"
    ]
  },
  "http://arxiv.org/abs/2312.09583": {
    "title": "Leveraging Language ID to Calculate Intermediate CTC Loss for Enhanced Code-Switching Speech Recognition",
    "volume": "Dec",
    "abstract": "In recent years, end-to-end speech recognition has emerged as a technology that integrates the acoustic, pronunciation dictionary, and language model components of the traditional Automatic Speech Recognition model. It is possible to achieve human-like recognition without the need to build a pronunciation dictionary in advance. However, due to the relative scarcity of training data on code-switching, the performance of ASR models tends to degrade drastically when encountering this phenomenon. Most past studies have simplified the learning complexity of the model by splitting the code-switching task into multiple tasks dealing with a single language and then learning the domain-specific knowledge of each language separately. Therefore, in this paper, we attempt to introduce language identification information into the middle layer of the ASR model's encoder. We aim to generate acoustic features that imply language distinctions in a more implicit way, reducing the model's confusion when dealing with language switching",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tzu-Ting Yang",
      "Hsin-Wei Wang",
      "Berlin Chen"
    ]
  },
  "http://arxiv.org/abs/2312.09582": {
    "title": "Phoneme-aware Encoding for Prefix-tree-based Contextual ASR",
    "volume": "Dec",
    "abstract": "In speech recognition applications, it is important to recognize context-specific rare words, such as proper nouns. Tree-constrained Pointer Generator (TCPGen) has shown promise for this purpose, which efficiently biases such words with a prefix tree. While the original TCPGen relies on grapheme-based encoding, we propose extending it with phoneme-aware encoding to better recognize words of unusual pronunciations. As TCPGen handles biasing words as subword units, we propose obtaining subword-level phoneme-aware encoding by using alignment between phonemes and subwords. Furthermore, we propose injecting phoneme-level predictions from CTC into queries of TCPGen so that the model better interprets the phoneme-aware encodings. We conducted ASR experiments with TCPGen for RNN transducer. We observed that proposed phoneme-aware encoding outperformed ordinary grapheme-based encoding on both the English LibriSpeech and Japanese CSJ datasets, demonstrating the robustness of our approach across linguistically diverse languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hayato Futami",
      "Emiru Tsunoo",
      "Yosuke Kashiwagi",
      "Hiroaki Ogawa",
      "Siddhant Arora",
      "Shinji Watanabe"
    ]
  },
  "http://arxiv.org/abs/2312.09572": {
    "title": "IR-UWB Radar-Based Contactless Silent Speech Recognition of Vowels, Consonants, Words, and Phrases",
    "volume": "Dec",
    "abstract": "Several sensing techniques have been proposed for silent speech recognition (SSR); however, many of these methods require invasive processes or sensor attachment to the skin using adhesive tape or glue, rendering them unsuitable for frequent use in daily life. By contrast, impulse radio ultra-wideband (IR-UWB) radar can operate without physical contact with users' articulators and related body parts, offering several advantages for SSR. These advantages include high range resolution, high penetrability, low power consumption, robustness to external light or sound interference, and the ability to be embedded in space-constrained handheld devices. This study demonstrated IR-UWB radar-based contactless SSR using four types of speech stimuli (vowels, consonants, words, and phrases). To achieve this, a novel speech feature extraction algorithm specifically designed for IR-UWB radar-based SSR is proposed. Each speech stimulus is recognized by applying a classification algorithm to the extracted speech features. Two different algorithms, multidimensional dynamic time warping (MD-DTW) and deep neural network-hidden Markov model (DNN-HMM), were compared for the classification task. Additionally, a favorable radar antenna position, either in front of the user's lips or below the user's chin, was determined to achieve higher recognition accuracy. Experimental results demonstrated the efficacy of the proposed speech feature extraction algorithm combined with DNN-HMM for classifying vowels, consonants, words, and phrases. Notably, this study represents the first demonstration of phoneme-level SSR using contactless radar",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunghwa Lee",
      "Younghoon Shin",
      "Myungjong Kim",
      "Jiwon Seo"
    ]
  },
  "http://arxiv.org/abs/2312.09571": {
    "title": "Extending Context Window of Large Language Models via Semantic Compression",
    "volume": "Dec",
    "abstract": "Transformer-based Large Language Models (LLMs) often impose limitations on the length of the text input to ensure the generation of fluent and relevant responses. This constraint restricts their applicability in scenarios involving long texts. We propose a novel semantic compression method that enables generalization to texts that are 6-8 times longer, without incurring significant computational costs or requiring fine-tuning. Our proposed framework draws inspiration from source coding in information theory and employs a pre-trained model to reduce the semantic redundancy of long inputs before passing them to the LLMs for downstream tasks. Experimental results demonstrate that our method effectively extends the context window of LLMs across a range of tasks including question answering, summarization, few-shot learning, and information retrieval. Furthermore, the proposed semantic compression method exhibits consistent fluency in text generation while reducing the associated computational overhead",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weizhi Fei",
      "Xueyan Niu",
      "Pingyi Zhou",
      "Lu Hou",
      "Bo Bai",
      "Lei Deng",
      "Wei Han"
    ]
  },
  "http://arxiv.org/abs/2312.09545": {
    "title": "GPT-4 Surpassing Human Performance in Linguistic Pragmatics",
    "volume": "Dec",
    "abstract": "As Large Language Models (LLMs) become increasingly integrated into everyday life, their capabilities to understand and emulate human cognition are under steady examination. This study investigates the ability of LLMs to comprehend and interpret linguistic pragmatics, an aspect of communication that considers context and implied meanings. Using Grice's communication principles, LLMs and human subjects (N=76) were evaluated based on their responses to various dialogue-based tasks. The findings revealed the superior performance and speed of LLMs, particularly GPT4, over human subjects in interpreting pragmatics. GPT4 also demonstrated accuracy in the pre-testing of human-written samples, indicating its potential in text analysis. In a comparative analysis of LLMs using human individual and average scores, the models exhibited significant chronological improvement. The models were ranked from lowest to highest score, with GPT2 positioned at 78th place, GPT3 ranking at 23rd, Bard at 10th, GPT3.5 placing 5th, Best Human scoring 2nd, and GPT4 achieving the top spot. The findings highlight the remarkable progress made in the development and performance of these LLMs. Future studies should consider diverse subjects, multiple languages, and other cognitive aspects to fully comprehend the capabilities of LLMs. This research holds significant implications for the development and application of AI-based models in communication-centered sectors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ljubisa Bojic",
      "Predrag Kovacevic",
      "Milan Cabarkapa"
    ]
  },
  "http://arxiv.org/abs/2312.09542": {
    "title": "Marathon: A Race Through the Realm of Long Context with Large Language Models",
    "volume": "Dec",
    "abstract": "Although there are currently many benchmarks available for evaluating the long context understanding and reasoning capability of large language models, with the expansion of the context window in these models, the existing long context benchmarks are no longer sufficient for evaluating the long context understanding and reasoning capability of large language models. In this paper, we have developed a fresh long context evaluation benchmark, which we name it Marathon in the form of multiple choice questions, inspired by benchmarks such as MMLU, for assessing the long context comprehension capability of large language models quickly, accurately, and objectively. We have evaluated several of the latest and most popular large language models, as well as three recent and effective long context optimization methods, on our benchmark. This showcases the long context reasoning and comprehension capabilities of these large language models and validates the effectiveness of these optimization methods. Marathon is available at https://huggingface.co/datasets/Lemoncoke/Marathon",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Zhang",
      "Yunshui Li",
      "Ziqiang Liu",
      "Jiaxi yang",
      "Junhao Liu",
      "Min Yang"
    ]
  },
  "http://arxiv.org/abs/2312.09541": {
    "title": "Picking the Underused Heads: A Network Pruning Perspective of Attention Head Selection for Fusing Dialogue Coreference Information",
    "volume": "Dec",
    "abstract": "The Transformer-based models with the multi-head self-attention mechanism are widely used in natural language processing, and provide state-of-the-art results. While the pre-trained language backbones are shown to implicitly capture certain linguistic knowledge, explicitly incorporating structure-aware features can bring about further improvement on the downstream tasks. However, such enhancement often requires additional neural components and increases training parameter size. In this work, we investigate the attention head selection and manipulation strategy for feature injection from a network pruning perspective, and conduct a case study on dialogue summarization. We first rank attention heads in a Transformer-based summarizer with layer-wise importance. We then select the underused heads through extensive analysis, and inject structure-aware features by manipulating the selected heads. Experimental results show that the importance-based head selection is effective for feature injection, and dialogue summarization can be improved by incorporating coreference information via head manipulation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyuan Liu",
      "Nancy F. Chen"
    ]
  },
  "http://arxiv.org/abs/2312.09536": {
    "title": "Riveter: Measuring Power and Social Dynamics Between Entities",
    "volume": "Dec",
    "abstract": "Riveter provides a complete easy-to-use pipeline for analyzing verb connotations associated with entities in text corpora. We prepopulate the package with connotation frames of sentiment, power, and agency, which have demonstrated usefulness for capturing social phenomena, such as gender bias, in a broad range of corpora. For decades, lexical frameworks have been foundational tools in computational social science, digital humanities, and natural language processing, facilitating multifaceted analysis of text corpora. But working with verb-centric lexica specifically requires natural language processing skills, reducing their accessibility to other researchers. By organizing the language processing pipeline, providing complete lexicon scores and visualizations for all entities in a corpus, and providing functionality for users to target specific research questions, Riveter greatly improves the accessibility of verb lexica and can facilitate a broad range of future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maria Antoniak",
      "Anjalie Field",
      "Jimin Mun",
      "Melanie Walsh",
      "Lauren F. Klein",
      "Maarten Sap"
    ]
  },
  "http://arxiv.org/abs/2312.09508": {
    "title": "IndicIRSuite: Multilingual Dataset and Neural Information Models for Indian Languages",
    "volume": "Dec",
    "abstract": "In this paper, we introduce Neural Information Retrieval resources for 11 widely spoken Indian Languages (Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil, and Telugu) from two major Indian language families (Indo-Aryan and Dravidian). These resources include (a) INDIC-MARCO, a multilingual version of the MSMARCO dataset in 11 Indian Languages created using Machine Translation, and (b) Indic-ColBERT, a collection of 11 distinct Monolingual Neural Information Retrieval models, each trained on one of the 11 languages in the INDIC-MARCO dataset. To the best of our knowledge, IndicIRSuite is the first attempt at building large-scale Neural Information Retrieval resources for a large number of Indian languages, and we hope that it will help accelerate research in Neural IR for Indian Languages. Experiments demonstrate that Indic-ColBERT achieves 47.47% improvement in the MRR@10 score averaged over the INDIC-MARCO baselines for all 11 Indian languages except Oriya, 12.26% improvement in the NDCG@10 score averaged over the MIRACL Bengali and Hindi Language baselines, and 20% improvement in the MRR@100 Score over the Mr.Tydi Bengali Language baseline. IndicIRSuite is available at https://github.com/saifulhaq95/IndicIRSuite",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saiful Haq",
      "Ashutosh Sharma",
      "Pushpak Bhattacharyya"
    ]
  },
  "http://arxiv.org/abs/2312.09494": {
    "title": "No-Skim: Towards Efficiency Robustness Evaluation on Skimming-based Language Models",
    "volume": "Dec",
    "abstract": "To reduce the computation cost and the energy consumption in large language models (LLM), skimming-based acceleration dynamically drops unimportant tokens of the input sequence progressively along layers of the LLM while preserving the tokens of semantic importance. However, our work for the first time reveals the acceleration may be vulnerable to Denial-of-Service (DoS) attacks. In this paper, we propose No-Skim, a general framework to help the owners of skimming-based LLM to understand and measure the robustness of their acceleration scheme. Specifically, our framework searches minimal and unnoticeable perturbations at character-level and token-level to generate adversarial inputs that sufficiently increase the remaining token ratio, thus increasing the computation cost and energy consumption. We systematically evaluate the vulnerability of the skimming acceleration in various LLM architectures including BERT and RoBERTa on the GLUE benchmark. In the worst case, the perturbation found by No-Skim substantially increases the running cost of LLM by over 145% on average. Moreover, No-Skim extends the evaluation framework to various scenarios, making the evaluation conductible with different level of knowledge",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengyao Zhang",
      "Mi Zhang",
      "Xudong Pan",
      "Min Yang"
    ]
  },
  "http://arxiv.org/abs/2312.09411": {
    "title": "OTOv3: Automatic Architecture-Agnostic Neural Network Training and Compression from Structured Pruning to Erasing Operators",
    "volume": "Dec",
    "abstract": "Compressing a predefined deep neural network (DNN) into a compact sub-network with competitive performance is crucial in the efficient machine learning realm. This topic spans various techniques, from structured pruning to neural architecture search, encompassing both pruning and erasing operators perspectives. Despite advancements, existing methods suffers from complex, multi-stage processes that demand substantial engineering and domain knowledge, limiting their broader applications. We introduce the third-generation Only-Train-Once (OTOv3), which first automatically trains and compresses a general DNN through pruning and erasing operations, creating a compact and competitive sub-network without the need of fine-tuning. OTOv3 simplifies and automates the training and compression process, minimizes the engineering efforts required from users. It offers key technological advancements: (i) automatic search space construction for general DNNs based on dependency graph analysis; (ii) Dual Half-Space Projected Gradient (DHSPG) and its enhanced version with hierarchical search (H2SPG) to reliably solve (hierarchical) structured sparsity problems and ensure sub-network validity; and (iii) automated sub-network construction using solutions from DHSPG/H2SPG and dependency graphs. Our empirical results demonstrate the efficacy of OTOv3 across various benchmarks in structured pruning and neural architecture search. OTOv3 produces sub-networks that match or exceed the state-of-the-arts. The source code will be available at https://github.com/tianyic/only_train_once",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Chen",
      "Tianyu Ding",
      "Zhihui Zhu",
      "Zeyu Chen",
      "HsiangTao Wu",
      "Ilya Zharkov",
      "Luming Liang"
    ]
  },
  "http://arxiv.org/abs/2312.09390": {
    "title": "Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision",
    "volume": "Dec",
    "abstract": "Widely used alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on the ability of humans to supervise model behavior - for example, to evaluate whether a model faithfully followed instructions or generated safe outputs. However, future superhuman models will behave in complex ways too difficult for humans to reliably evaluate; humans will only be able to weakly supervise superhuman models. We study an analogy to this problem: can weak model supervision elicit the full capabilities of a much stronger model? We test this using a range of pretrained language models in the GPT-4 family on natural language processing (NLP), chess, and reward modeling tasks. We find that when we naively finetune strong pretrained models on labels generated by a weak model, they consistently perform better than their weak supervisors, a phenomenon we call weak-to-strong generalization. However, we are still far from recovering the full capabilities of strong models with naive finetuning alone, suggesting that techniques like RLHF may scale poorly to superhuman models without further work. We find that simple methods can often significantly improve weak-to-strong generalization: for example, when finetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence loss, we can recover close to GPT-3.5-level performance on NLP tasks. Our results suggest that it is feasible to make empirical progress today on a fundamental challenge of aligning superhuman models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Collin Burns",
      "Pavel Izmailov",
      "Jan Hendrik Kirchner",
      "Bowen Baker",
      "Leo Gao",
      "Leopold Aschenbrenner",
      "Yining Chen",
      "Adrien Ecoffet",
      "Manas Joglekar",
      "Jan Leike",
      "Ilya Sutskever",
      "Jeff Wu"
    ]
  },
  "http://arxiv.org/abs/2312.09366": {
    "title": "Arabic Mini-ClimateGPT : A Climate Change and Sustainability Tailored Arabic LLM",
    "volume": "Dec",
    "abstract": "Climate change is one of the most significant challenges we face together as a society. Creating awareness and educating policy makers the wide-ranging impact of climate change is an essential step towards a sustainable future. Recently, Large Language Models (LLMs) like ChatGPT and Bard have shown impressive conversational abilities and excel in a wide variety of NLP tasks. While these models are close-source, recently alternative open-source LLMs such as Stanford Alpaca and Vicuna have shown promising results. However, these open-source models are not specifically tailored for climate related domain specific information and also struggle to generate meaningful responses in other languages such as, Arabic. To this end, we propose a light-weight Arabic Mini-ClimateGPT that is built on an open-source LLM and is specifically fine-tuned on a conversational-style instruction tuning curated Arabic dataset Clima500-Instruct with over 500k instructions about climate change and sustainability. Further, our model also utilizes a vector embedding based retrieval mechanism during inference. We validate our proposed model through quantitative and qualitative evaluations on climate-related queries. Our model surpasses the baseline LLM in 88.3% of cases during ChatGPT-based evaluation. Furthermore, our human expert evaluation reveals an 81.6% preference for our model's responses over multiple popular open-source models. Our open-source demos, code-base and models are available here https://github.com/mbzuai-oryx/ClimateGPT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sahal Shaji Mullappilly",
      "Abdelrahman Shaker",
      "Omkar Thawakar",
      "Hisham Cholakkal",
      "Rao Muhammad Anwer",
      "Salman Khan",
      "Fahad Shahbaz Khan"
    ]
  },
  "http://arxiv.org/abs/2312.09304": {
    "title": "Well-calibrated Confidence Measures for Multi-label Text Classification with a Large Number of Labels",
    "volume": "Dec",
    "abstract": "We extend our previous work on Inductive Conformal Prediction (ICP) for multi-label text classification and present a novel approach for addressing the computational inefficiency of the Label Powerset (LP) ICP, arrising when dealing with a high number of unique labels. We present experimental results using the original and the proposed efficient LP-ICP on two English and one Czech language data-sets. Specifically, we apply the LP-ICP on three deep Artificial Neural Network (ANN) classifiers of two types: one based on contextualised (bert) and two on non-contextualised (word2vec) word-embeddings. In the LP-ICP setting we assign nonconformity scores to label-sets from which the corresponding p-values and prediction-sets are determined. Our approach deals with the increased computational burden of LP by eliminating from consideration a significant number of label-sets that will surely have p-values below the specified significance level. This reduces dramatically the computational complexity of the approach while fully respecting the standard CP guarantees. Our experimental results show that the contextualised-based classifier surpasses the non-contextualised-based ones and obtains state-of-the-art performance for all data-sets examined. The good performance of the underlying classifiers is carried on to their ICP counterparts without any significant accuracy loss, but with the added benefits of ICP, i.e. the confidence information encapsulated in the prediction sets. We experimentally demonstrate that the resulting prediction sets can be tight enough to be practically useful even though the set of all possible label-sets contains more than $1e+16$ combinations. Additionally, the empirical error rates of the obtained prediction-sets confirm that our outputs are well-calibrated",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lysimachos Maltoudoglou",
      "Andreas Paisios",
      "Ladislav Lenc",
      "Jiří Martínek",
      "Pavel Král",
      "Harris Papadopoulos"
    ]
  },
  "http://arxiv.org/abs/2312.09300": {
    "title": "Self-Evaluation Improves Selective Generation in Large Language Models",
    "volume": "Dec",
    "abstract": "Safe deployment of large language models (LLMs) may benefit from a reliable method for assessing their generated content to determine when to abstain or to selectively generate. While likelihood-based metrics such as perplexity are widely employed, recent research has demonstrated the limitations of using sequence-level probability estimates given by LLMs as reliable indicators of generation quality. Conversely, LLMs have demonstrated strong calibration at the token level, particularly when it comes to choosing correct answers in multiple-choice questions or evaluating true/false statements. In this work, we reformulate open-ended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level. We instruct an LLM to self-evaluate its answers, employing either a multi-way comparison or a point-wise evaluation approach, with the option to include a ``None of the above'' option to express the model's uncertainty explicitly. We benchmark a range of scoring methods based on self-evaluation and evaluate their performance in selective generation using TruthfulQA and TL;DR. Through experiments with PaLM-2 and GPT-3, we demonstrate that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Ren",
      "Yao Zhao",
      "Tu Vu",
      "Peter J. Liu",
      "Balaji Lakshminarayanan"
    ]
  },
  "http://arxiv.org/abs/2312.09299": {
    "title": "Weight subcloning: direct initialization of transformers using larger pretrained ones",
    "volume": "Dec",
    "abstract": "Training large transformer models from scratch for a target task requires lots of data and is computationally demanding. The usual practice of transfer learning overcomes this challenge by initializing the model with weights of a pretrained model of the same size and specification to increase the convergence and training speed. However, what if no pretrained model of the required size is available? In this paper, we introduce a simple yet effective technique to transfer the knowledge of a pretrained model to smaller variants. Our approach called weight subcloning expedites the training of scaled-down transformers by initializing their weights from larger pretrained models. Weight subcloning involves an operation on the pretrained model to obtain the equivalent initialized scaled-down model. It consists of two key steps: first, we introduce neuron importance ranking to decrease the embedding dimension per layer in the pretrained model. Then, we remove blocks from the transformer model to match the number of layers in the scaled-down network. The result is a network ready to undergo training, which gains significant improvements in training speed compared to random initialization. For instance, we achieve 4x faster training for vision transformers in image classification and language models designed for next token prediction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Samragh",
      "Mehrdad Farajtabar",
      "Sachin Mehta",
      "Raviteja Vemulapalli",
      "Fartash Faghri",
      "Devang Naik",
      "Oncel Tuzel",
      "Mohammad Rastegari"
    ]
  },
  "http://arxiv.org/abs/2312.09241": {
    "title": "TinyGSM: achieving >80% on GSM8k with small language models",
    "volume": "Dec",
    "abstract": "Small-scale models offer various computational advantages, and yet to which extent size is critical for problem-solving abilities remains an open question. Specifically for solving grade school math, the smallest model size so far required to break the 80\\% barrier on the GSM8K benchmark remains to be 34B. Our work studies how high-quality datasets may be the key for small language models to acquire mathematical reasoning. We introduce \\texttt{TinyGSM}, a synthetic dataset of 12.3M grade school math problems paired with Python solutions, generated fully by GPT-3.5. After finetuning on \\texttt{TinyGSM}, we find that a duo of a 1.3B generation model and a 1.3B verifier model can achieve 81.5\\% accuracy, outperforming existing models that are orders of magnitude larger. This also rivals the performance of the GPT-3.5 ``teacher'' model (77.4\\%), from which our model's training data is generated. Our approach is simple and has two key components: 1) the high-quality dataset \\texttt{TinyGSM}, 2) the use of a verifier, which selects the final outputs from multiple candidate generations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingbin Liu",
      "Sebastien Bubeck",
      "Ronen Eldan",
      "Janardhan Kulkarni",
      "Yuanzhi Li",
      "Anh Nguyen",
      "Rachel Ward",
      "Yi Zhang"
    ]
  },
  "http://arxiv.org/abs/2312.09238": {
    "title": "Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft",
    "volume": "Dec",
    "abstract": "Traditional reinforcement-learning-based agents rely on sparse rewards that often only use binary values to indicate task completion or failure. The challenge in exploration efficiency makes it difficult to effectively learn complex tasks in Minecraft. To address this, this paper introduces an advanced learning system, named Auto MC-Reward, that leverages Large Language Models (LLMs) to automatically design dense reward functions, thereby enhancing the learning efficiency. Auto MC-Reward consists of three important components: Reward Designer, Reward Critic, and Trajectory Analyzer. Given the environment information and task descriptions, the Reward Designer first design the reward function by coding an executable Python function with predefined observation inputs. Then, our Reward Critic will be responsible for verifying the code, checking whether the code is self-consistent and free of syntax and semantic errors. Further, the Trajectory Analyzer summarizes possible failure causes and provides refinement suggestions according to collected trajectories. In the next round, Reward Designer will take further refine and iterate the dense reward function based on feedback. Experiments demonstrate a significant improvement in the success rate and learning efficiency of our agents in complex tasks in Minecraft, such as obtaining diamond with the efficient ability to avoid lava, and efficiently explore trees and animals that are sparse on the plains biome",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Li",
      "Xue Yang",
      "Zhaokai Wang",
      "Xizhou Zhu",
      "Jie Zhou",
      "Yu Qiao",
      "Xiaogang Wang",
      "Hongsheng Li",
      "Lewei Lu",
      "Jifeng Dai"
    ]
  },
  "http://arxiv.org/abs/2312.09230": {
    "title": "Successor Heads: Recurring, Interpretable Attention Heads In The Wild",
    "volume": "Dec",
    "abstract": "In this work we present successor heads: attention heads that increment tokens with a natural ordering, such as numbers, months, and days. For example, successor heads increment 'Monday' into 'Tuesday'. We explain the successor head behavior with an approach rooted in mechanistic interpretability, the field that aims to explain how models complete tasks in human-understandable terms. Existing research in this area has found interpretable language model components in small toy models. However, results in toy models have not yet led to insights that explain the internals of frontier models and little is currently understood about the internal operations of large language models. In this paper, we analyze the behavior of successor heads in large language models (LLMs) and find that they implement abstract representations that are common to different architectures. They form in LLMs with as few as 31 million parameters, and at least as many as 12 billion parameters, such as GPT-2, Pythia, and Llama-2. We find a set of 'mod-10 features' that underlie how successor heads increment in LLMs across different architectures and sizes. We perform vector arithmetic with these features to edit head behavior and provide insights into numeric representations within LLMs. Additionally, we study the behavior of successor heads on natural language data, identifying interpretable polysemanticity in a Pythia successor head",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rhys Gould",
      "Euan Ong",
      "George Ogden",
      "Arthur Conmy"
    ]
  },
  "http://arxiv.org/abs/2312.09211": {
    "title": "Mitigating Outlier Activations in Low-Precision Fine-Tuning of Language Models",
    "volume": "Dec",
    "abstract": "Low-precision fine-tuning of language models has gained prominence as a cost-effective and energy-efficient approach to deploying large-scale models in various applications. However, this approach is susceptible to the existence of outlier values in activation. The outlier values in the activation can negatively affect the performance of fine-tuning language models in the low-precision regime since they affect the scaling factor and thus make representing smaller values harder. This paper investigates techniques for mitigating outlier activation in low-precision integer fine-tuning of the language models. Our proposed novel approach enables us to represent the outlier activation values in 8-bit integers instead of floating-point (FP16) values. The benefit of using integers for outlier values is that it enables us to use operator tiling to avoid performing 16-bit integer matrix multiplication to address this problem effectively. We provide theoretical analysis and supporting experiments to demonstrate the effectiveness of our approach in improving the robustness and performance of low-precision fine-tuned language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alireza Ghaffari",
      "Justin Yu",
      "Mahsa Ghazvini Nejad",
      "Masoud Asgharian",
      "Boxing Chen",
      "Vahid Partovi Nia"
    ]
  },
  "http://arxiv.org/abs/2312.09207": {
    "title": "WikiMuTe: A web-sourced dataset of semantic descriptions for music audio",
    "volume": "Dec",
    "abstract": "Multi-modal deep learning techniques for matching free-form text with music have shown promising results in the field of Music Information Retrieval (MIR). Prior work is often based on large proprietary data while publicly available datasets are few and small in size. In this study, we present WikiMuTe, a new and open dataset containing rich semantic descriptions of music. The data is sourced from Wikipedia's rich catalogue of articles covering musical works. Using a dedicated text-mining pipeline, we extract both long and short-form descriptions covering a wide range of topics related to music content such as genre, style, mood, instrumentation, and tempo. To show the use of this data, we train a model that jointly learns text and audio representations and performs cross-modal retrieval. The model is evaluated on two tasks: tag-based music retrieval and music auto-tagging. The results show that while our approach has state-of-the-art performance on multiple tasks, but still observe a difference in performance depending on the data used for training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benno Weck",
      "Holger Kirchhoff",
      "Peter Grosche",
      "Xavier Serra"
    ]
  },
  "http://arxiv.org/abs/2312.09203": {
    "title": "Measurement in the Age of LLMs: An Application to Ideological Scaling",
    "volume": "Dec",
    "abstract": "Much of social science is centered around terms like ``ideology'' or ``power'', which generally elude precise definition, and whose contextual meanings are trapped in surrounding language. This paper explores the use of large language models (LLMs) to flexibly navigate the conceptual clutter inherent to social scientific measurement tasks. We rely on LLMs' remarkable linguistic fluency to elicit ideological scales of both legislators and text, which accord closely to established methods and our own judgement. A key aspect of our approach is that we elicit such scores directly, instructing the LLM to furnish numeric scores itself. This approach affords a great deal of flexibility, which we showcase through a variety of different case studies. Our results suggest that LLMs can be used to characterize highly subtle and diffuse manifestations of political ideology in text",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sean O'Hagan",
      "Aaron Schein"
    ]
  },
  "http://arxiv.org/abs/2312.09085": {
    "title": "The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation",
    "volume": "Dec",
    "abstract": "Large Language Models (LLMs) encapsulate vast amounts of knowledge but still remain vulnerable to external misinformation. Existing research mainly studied this susceptibility behavior in a single-turn setting. However, belief can change during a multi-turn conversation, especially a persuasive one. Therefore, in this study, we delve into LLMs' susceptibility to persuasive conversations, particularly on factual questions that they can answer correctly. We first curate the Farm (i.e., Fact to Misinform) dataset, which contains factual questions paired with systematically generated persuasive misinformation. Then, we develop a testing framework to track LLMs' belief changes in a persuasive dialogue. Through extensive experiments, we find that LLMs' correct beliefs on factual knowledge can be easily manipulated by various persuasive strategies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongwu Xu",
      "Brian S. Lin",
      "Shujian Yang",
      "Tianqi Zhang",
      "Weiyan Shi",
      "Tianwei Zhang",
      "Zhixuan Fang",
      "Wei Xu",
      "Han Qiu"
    ]
  },
  "http://arxiv.org/abs/2312.09084": {
    "title": "Language Modeling on a SpiNNaker 2 Neuromorphic Chip",
    "volume": "Dec",
    "abstract": "As large language models continue to scale in size rapidly, so too does the computational power required to run them. Event-based networks on neuromorphic devices offer a potential way to reduce energy consumption for inference significantly. However, to date, most event-based networks that can run on neuromorphic hardware, including spiking neural networks (SNNs), have not achieved task performance even on par with LSTM models for language modeling. As a result, language modeling on neuromorphic devices has seemed a distant prospect. In this work, we demonstrate the first-ever implementation of a language model on a neuromorphic device - specifically the SpiNNaker 2 chip - based on a recently published event-based architecture called the EGRU. SpiNNaker 2 is a many-core neuromorphic chip designed for large-scale asynchronous processing, while the EGRU is architected to leverage such hardware efficiently while maintaining competitive task performance. This implementation marks the first time a neuromorphic language model matches LSTMs, setting the stage for taking task performance to the level of large language models. We also demonstrate results on a gesture recognition task based on inputs from a DVS camera. Overall, our results showcase the feasibility of this neuro-inspired neural network in hardware, highlighting significant gains versus conventional hardware in energy efficiency for the common use case of single batch inference",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khaleelulla Khan Nazeer",
      "Mark Schöne",
      "Rishav Mukherji",
      "Christian Mayr",
      "David Kappel",
      "Anand Subramoney"
    ]
  },
  "http://arxiv.org/abs/2312.09075": {
    "title": "Towards Verifiable Text Generation with Evolving Memory and Self-Reflection",
    "volume": "Dec",
    "abstract": "Large Language Models (LLMs) face several challenges, including the tendency to produce incorrect outputs, known as hallucination. An effective solution is verifiable text generation, which prompts LLMs to generate content with citations for accuracy verification. However, verifiable text generation is non-trivial due to the focus-shifting phenomenon, the dilemma between the precision and scope in document retrieval, and the intricate reasoning required to discern the relationship between the claim and citations. In this paper, we present VTG, an innovative approach for Verifiable Text Generation with evolving memory and self-reflection. VTG maintains evolving long short-term memory to retain both valuable documents and up-to-date documents. Active retrieval and diverse query generation are utilized to enhance both the precision and scope of the retrieved documents. Furthermore, VTG features a two-tier verifier and an evidence finder, enabling rethinking and reflection on the relationship between the claim and citations. We conduct extensive experiments on five datasets across three knowledge-intensive tasks and the results reveal that VTG significantly outperforms existing baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Sun",
      "Hengyi Cai",
      "Bo Wang",
      "Yingyan Hou",
      "Xiaochi Wei",
      "Shuaiqiang Wang",
      "Yan Zhang",
      "Dawei Yin"
    ]
  },
  "http://arxiv.org/abs/2312.09067": {
    "title": "Holodeck: Language Guided Generation of 3D Embodied AI Environments",
    "volume": "Dec",
    "abstract": "3D simulated environments play a critical role in Embodied AI, but their creation requires expertise and extensive manual effort, restricting their diversity and scope. To mitigate this limitation, we present Holodeck, a system that generates 3D environments to match a user-supplied prompt fully automatedly. Holodeck can generate diverse scenes, e.g., arcades, spas, and museums, adjust the designs for styles, and can capture the semantics of complex queries such as \"apartment for a researcher with a cat\" and \"office of a professor who is a fan of Star Wars\". Holodeck leverages a large language model (GPT-4) for common sense knowledge about what the scene might look like and uses a large collection of 3D assets from Objaverse to populate the scene with diverse objects. To address the challenge of positioning objects correctly, we prompt GPT-4 to generate spatial relational constraints between objects and then optimize the layout to satisfy those constraints. Our large-scale human evaluation shows that annotators prefer Holodeck over manually designed procedural baselines in residential scenes and that Holodeck can produce high-quality outputs for diverse scene types. We also demonstrate an exciting application of Holodeck in Embodied AI, training agents to navigate in novel scenes like music rooms and daycares without human-constructed data, which is a significant step forward in developing general-purpose embodied agents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Yang",
      "Fan-Yun Sun",
      "Luca Weihs",
      "Eli VanderBilt",
      "Alvaro Herrasti",
      "Winson Han",
      "Jiajun Wu",
      "Nick Haber",
      "Ranjay Krishna",
      "Lingjie Liu",
      "Chris Callison-Burch",
      "Mark Yatskar",
      "Aniruddha Kembhavi",
      "Christopher Clark"
    ]
  },
  "http://arxiv.org/abs/2312.09043": {
    "title": "Topic Bias in Emotion Classification",
    "volume": "Dec",
    "abstract": "Emotion corpora are typically sampled based on keyword/hashtag search or by asking study participants to generate textual instances. In any case, these corpora are not uniform samples representing the entirety of a domain. We hypothesize that this practice of data acquisition leads to unrealistic correlations between overrepresented topics in these corpora that harm the generalizability of models. Such topic bias could lead to wrong predictions for instances like \"I organized the service for my aunt's funeral.\" when funeral events are over-represented for instances labeled with sadness, despite the emotion of pride being more appropriate here. In this paper, we study this topic bias both from the data and the modeling perspective. We first label a set of emotion corpora automatically via topic modeling and show that emotions in fact correlate with specific topics. Further, we see that emotion classifiers are confounded by such topics. Finally, we show that the established debiasing method of adversarial correction via gradient reversal mitigates the issue. Our work points out issues with existing emotion corpora and that more representative resources are required for fair evaluation of models predicting affective concepts from text",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximilian Wegge",
      "Roman Klinger"
    ]
  },
  "http://arxiv.org/abs/2312.09040": {
    "title": "STaR: Distilling Speech Temporal Relation for Lightweight Speech Self-Supervised Learning Models",
    "volume": "Dec",
    "abstract": "Albeit great performance of Transformer-based speech selfsupervised learning (SSL) models, their large parameter size and computational cost make them unfavorable to utilize. In this study, we propose to compress the speech SSL models by distilling speech temporal relation (STaR). Unlike previous works that directly match the representation for each speech frame, STaR distillation transfers temporal relation between speech frames, which is more suitable for lightweight student with limited capacity. We explore three STaR distillation objectives and select the best combination as the final STaR loss. Our model distilled from HuBERT BASE achieves an overall score of 79.8 on SUPERB benchmark, the best performance among models with up to 27 million parameters. We show that our method is applicable across different speech SSL models and maintains robust performance with further reduced parameters",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangwook Jang",
      "Sungnyun Kim",
      "Hoirin Kim"
    ]
  },
  "http://arxiv.org/abs/2312.09039": {
    "title": "TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning",
    "volume": "Dec",
    "abstract": "Table reasoning has shown remarkable progress in a wide range of table-based tasks. These challenging tasks require reasoning over both free-form natural language (NL) questions and semi-structured tabular data. However, previous table reasoning solutions suffer from significant performance degradation on \"huge\" tables. In addition, most existing methods struggle to reason over complex questions since they lack essential information or they are scattered in different places. To alleviate these challenges, we exploit a table provider, namely TAP4LLM, on versatile sampling, augmentation, and packing methods to achieve effective semi-structured data reasoning using large language models (LLMs), which 1) decompose raw tables into sub-tables with specific rows or columns based on the rules or semantic similarity; 2) augment table information by extracting semantic and statistical metadata from raw tables while retrieving relevant knowledge from trustworthy knowledge sources (e.g., Wolfram Alpha, Wikipedia); 3) pack sampled tables with augmented knowledge into sequence prompts for LLMs reasoning while balancing the token allocation trade-off. We show that TAP4LLM allows for different components as plug-ins, enhancing LLMs' understanding of structured data in diverse tabular tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Sui",
      "Jiaru Zou",
      "Mengyu Zhou",
      "Xinyi He",
      "Lun Du",
      "Shi Han",
      "Dongmei Zhang"
    ]
  },
  "http://arxiv.org/abs/2312.09000": {
    "title": "ComOM at VLSP 2023: A Dual-Stage Framework with BERTology and Unified Multi-Task Instruction Tuning Model for Vietnamese Comparative Opinion Mining",
    "volume": "Dec",
    "abstract": "The ComOM shared task aims to extract comparative opinions from product reviews in Vietnamese language. There are two sub-tasks, including (1) Comparative Sentence Identification (CSI) and (2) Comparative Element Extraction (CEE). The first task is to identify whether the input is a comparative review, and the purpose of the second task is to extract the quintuplets mentioned in the comparative review. To address this task, our team proposes a two-stage system based on fine-tuning a BERTology model for the CSI task and unified multi-task instruction tuning for the CEE task. Besides, we apply the simple data augmentation technique to increase the size of the dataset for training our model in the second stage. Experimental results show that our approach outperforms the other competitors and has achieved the top score on the official private test",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dang Van Thin",
      "Duong Ngoc Hao",
      "Ngan Luu-Thuy Nguyen"
    ]
  },
  "http://arxiv.org/abs/2312.08995": {
    "title": "FrameFinder: Explorative Multi-Perspective Framing Extraction from News Headlines",
    "volume": "Dec",
    "abstract": "Revealing the framing of news articles is an important yet neglected task in information seeking and retrieval. In the present work, we present FrameFinder, an open tool for extracting and analyzing frames in textual data. FrameFinder visually represents the frames of text from three perspectives, i.e., (i) frame labels, (ii) frame dimensions, and (iii) frame structure. By analyzing the well-established gun violence frame corpus, we demonstrate the merits of our proposed solution to support social science research and call for subsequent integration into information interactions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Markus Reiter-Haas",
      "Beate Klösch",
      "Markus Hadler",
      "Elisabeth Lex"
    ]
  },
  "http://arxiv.org/abs/2312.08968": {
    "title": "Detecting value-expressive text posts in Russian social media",
    "volume": "Dec",
    "abstract": "Basic values are concepts or beliefs which pertain to desirable end-states and transcend specific situations. Studying personal values in social media can illuminate how and why societal values evolve especially when the stimuli-based methods, such as surveys, are inefficient, for instance, in hard-to-reach populations. On the other hand, user-generated content is driven by the massive use of stereotyped, culturally defined speech constructions rather than authentic expressions of personal values. We aimed to find a model that can accurately detect value-expressive posts in Russian social media VKontakte. A training dataset of 5,035 posts was annotated by three experts, 304 crowd-workers and ChatGPT. Crowd-workers and experts showed only moderate agreement in categorizing posts. ChatGPT was more consistent but struggled with spam detection. We applied an ensemble of human- and AI-assisted annotation involving active learning approach, subsequently trained several LLMs and selected a model based on embeddings from pre-trained fine-tuned rubert-tiny2, and reached a high quality of value detection with F1 = 0.75 (F1-macro = 0.80). This model provides a crucial step to a study of values within and between Russian social media users",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maria Milkova",
      "Maksim Rudnev",
      "Lidia Okolskaya"
    ]
  },
  "http://arxiv.org/abs/2312.09265": {
    "title": "Acoustic models of Brazilian Portuguese Speech based on Neural Transformers",
    "volume": "Dec",
    "abstract": "An acoustic model, trained on a significant amount of unlabeled data, consists of a self-supervised learned speech representation useful for solving downstream tasks, perhaps after a fine-tuning of the model in the respective downstream task. In this work, we build an acoustic model of Brazilian Portuguese Speech through a Transformer neural network. This model was pretrained on more than $800$ hours of Brazilian Portuguese Speech, using a combination of pretraining techniques. Using a labeled dataset collected for the detection of respiratory insufficiency in Brazilian Portuguese speakers, we fine-tune the pretrained Transformer neural network on the following tasks: respiratory insufficiency detection, gender recognition and age group classification. We compare the performance of pretrained Transformers on these tasks with that of Transformers without previous pretraining, noting a significant improvement. In particular, the performance of respiratory insufficiency detection obtains the best reported results so far, indicating this kind of acoustic model as a promising tool for speech-as-biomarker approach. Moreover, the performance of gender recognition is comparable to the state of the art models in English",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcelo Matheus Gauy",
      "Marcelo Finger"
    ]
  },
  "http://arxiv.org/abs/2312.08935": {
    "title": "Math-Shepherd: A Label-Free Step-by-Step Verifier for LLMs in Mathematical Reasoning",
    "volume": "Dec",
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks. However, even the most advanced open-source LLMs, such as the LLaMA family models, still face challenges when it comes to accurately solving complex multi-step mathematical problems. In this paper, we present an innovative process-oriented math verifier called \\textbf{Math-Shepherd}, which assigns a reward score to each step of the LLM's outputs on math problems. The training of Math-Shepherd is achieved using automatically constructed process-wise supervision data, breaking the bottleneck of heavy reliance on manual annotation in existing work. With the guidance of Math-Shepherd, a series of open-source LLMs demonstrate exceptional performance. Among them, DeepSeek 67B \\citep{DeepSeek-llm} stands out by achieving accuracy rates of 93.3\\% on the GSM8K dataset and 48.1\\% on the MATH dataset, without external enhancement such as tool usage. Our Math-Shepherd also outperforms the self-consistency method and other existing verification models. We believe that automatic process supervision holds significant potential for the future evolution of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiyi Wang",
      "Lei Li",
      "Zhihong Shao",
      "R. X. Xu",
      "Damai Dai",
      "Yifei Li",
      "Deli Chen",
      "Y. Wu",
      "Zhifang Sui"
    ]
  },
  "http://arxiv.org/abs/2312.08926": {
    "title": "Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent",
    "volume": "Dec",
    "abstract": "Large language models (LLMs) face challenges in solving complex mathematical problems that require comprehensive capacities to parse the statements, associate domain knowledge, perform compound logical reasoning, and integrate the intermediate rationales. Tackling all these problems once could be arduous for LLMs, thus leading to confusion in generation. In this work, we explore the potential of enhancing LLMs with agents by meticulous decomposition and modeling of mathematical reasoning process. Specifically, we propose a formal description of the mathematical solving and extend LLMs with an agent-based zero-shot framework named $\\bf{P}$lanner-$\\bf{R}$easoner-$\\bf{E}$xecutor-$\\bf{R}$eflector (PRER). We further provide and implement two MathAgents that define the logical forms and inherent relations via a pool of actions in different grains and orientations: MathAgent-M adapts its actions to LLMs, while MathAgent-H aligns with humankind. Experiments on miniF2F and MATH have demonstrated the effectiveness of PRER and proposed MathAgents, achieving an increase of $12.3\\%$($53.9\\%\\xrightarrow{}66.2\\%$) on the MiniF2F, $9.2\\%$ ($49.8\\%\\xrightarrow{}59.0\\%$) on MATH, and $13.2\\%$($23.2\\%\\xrightarrow{}35.4\\%$) for level-5 problems of MATH against GPT-4. Further analytical results provide more insightful perspectives on exploiting the behaviors of LLMs as agents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Liao",
      "Qinyi Du",
      "Shaohua Hu",
      "Hao He",
      "Yanyan Xu",
      "Jidong Tian",
      "Yaohui Jin"
    ]
  },
  "http://arxiv.org/abs/2312.08906": {
    "title": "Using eye tracking to investigate what native Chinese speakers notice about linguistic landscape images",
    "volume": "Dec",
    "abstract": "Linguistic landscape is an important field in sociolinguistic research. Eye tracking technology is a common technology in psychological research. There are few cases of using eye movement to study linguistic landscape. This paper uses eye tracking technology to study the actual fixation of the linguistic landscape and finds that in the two dimensions of fixation time and fixation times, the fixation of native Chinese speakers to the linguistic landscape is higher than that of the general landscape. This paper argues that this phenomenon is due to the higher information density of linguistic landscapes. At the same time, the article also discusses other possible reasons for this phenomenon",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichao Wei",
      "Yewei Qin"
    ]
  },
  "http://arxiv.org/abs/2312.08901": {
    "title": "Boosting LLM Reasoning: Push the Limits of Few-shot Learning with Reinforced In-Context Pruning",
    "volume": "Dec",
    "abstract": "Large language models (LLMs) have shown impressive capabilities in various tasks, yet they still struggle with math reasoning. Despite efforts to optimize Chain-of-Thoughts (CoT) prompts and fine-tune LLMs, the potential of few-shot learning remains unexplored. In this work, we propose CoT-Max, a novel approach pushing the boundaries of few-shot CoT learning to improve LLM math reasoning capabilities. CoT-Max addresses the challenges of the selection of useful examples and limited number of examples due to restricted context window length. Inspired by our observation that natural language inputs contain many redundancy, we propose a coarse-to-fine pruner as a plug-and-play module for LLMs, which first identifies crucial CoT examples from a large batch and then further prunes unimportant tokens. To train the pruner, we collect a math reasoning dataset with diverse difficulty and steps, introduce a reward to measure both the input's effectiveness for math reasoning and token length constraints, and propose a novel training approach with reinforcement learning. As a result, CoT-Max significantly outperforms CoT and few-shot prompting baselines across various LLMs (LLaMA2-7B, 13B, 70B) and 5 mathematical datasets, achieving up to 4.55% absolute improvements. Remarkably, without any fine-tuning, LLaMA2-70B with CoT-Max surpasses GPT-3.5 and a wide range of larger LLMs (PaLM, Minerva, etc.) on the GSM8K",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xijie Huang",
      "Li Lyna Zhang",
      "Kwang-Ting Cheng",
      "Mao Yang"
    ]
  },
  "http://arxiv.org/abs/2312.08865": {
    "title": "Improving Cross-modal Alignment with Synthetic Pairs for Text-only Image Captioning",
    "volume": "Dec",
    "abstract": "Although image captioning models have made significant advancements in recent years, the majority of them heavily depend on high-quality datasets containing paired images and texts which are costly to acquire. Previous works leverage the CLIP's cross-modal association ability for image captioning, relying solely on textual information under unsupervised settings. However, not only does a modality gap exist between CLIP text and image features, but a discrepancy also arises between training and inference due to the unavailability of real-world images, which hinders the cross-modal alignment in text-only captioning. This paper proposes a novel method to address these issues by incorporating synthetic image-text pairs. A pre-trained text-to-image model is deployed to obtain images that correspond to textual data, and the pseudo features of generated images are optimized toward the real ones in the CLIP embedding space. Furthermore, textual information is gathered to represent image features, resulting in the image features with various semantics and the bridged modality gap. To unify training and inference, synthetic image features would serve as the training prefix for the language decoder, while real images are used for inference. Additionally, salient objects in images are detected as assistance to enhance the learning of modality alignment. Experimental results demonstrate that our method obtains the state-of-the-art performance on benchmark datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyue Liu",
      "Jinyuan Liu",
      "Fanrong Ma"
    ]
  },
  "http://arxiv.org/abs/2312.08846": {
    "title": "TiMix: Text-aware Image Mixing for Effective Vision-Language Pre-training",
    "volume": "Dec",
    "abstract": "Self-supervised Multi-modal Contrastive Learning (SMCL) remarkably advances modern Vision-Language Pre-training (VLP) models by aligning visual and linguistic modalities. Due to noises in web-harvested text-image pairs, however, scaling up training data volume in SMCL presents considerable obstacles in terms of computational cost and data inefficiency. To improve data efficiency in VLP, we propose Text-aware Image Mixing (TiMix), which integrates mix-based data augmentation techniques into SMCL, yielding significant performance improvements without significantly increasing computational overhead. We provide a theoretical analysis of TiMixfrom a mutual information (MI) perspective, showing that mixed data samples for cross-modal contrastive learning implicitly serve as a regularizer for the contrastive loss. The experimental results demonstrate that TiMix exhibits a comparable performance on downstream tasks, even with a reduced amount of training data and shorter training time, when benchmarked against existing methods. This work empirically and theoretically demonstrates the potential of data mixing for data-efficient and computationally viable VLP, benefiting broader VLP model adoption in practical scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoya Jiang",
      "Wei ye",
      "Haiyang Xu",
      "Qinghao Ye",
      "Ming Yan",
      "Ji Zhang",
      "Shikun Zhang"
    ]
  },
  "http://arxiv.org/abs/2312.08800": {
    "title": "Evaluating Large Language Models for Health-related Queries with Presuppositions",
    "volume": "Dec",
    "abstract": "As corporations rush to integrate large language models (LLMs) to their search offerings, it is critical that they provide factually accurate information that is robust to any presuppositions that a user may express. In this work, we introduce UPHILL, a dataset consisting of health-related queries with varying degrees of presuppositions. Using UPHILL, we evaluate the factual accuracy and consistency of InstructGPT, ChatGPT, and BingChat models. We find that while model responses rarely disagree with true health claims (posed as questions), they often fail to challenge false claims: responses from InstructGPT agree with 32% of the false claims, ChatGPT 26% and BingChat 23%. As we increase the extent of presupposition in input queries, the responses from InstructGPT and ChatGPT agree with the claim considerably more often, regardless of its veracity. Responses from BingChat, which rely on retrieved webpages, are not as susceptible. Given the moderate factual accuracy, and the inability of models to consistently correct false assumptions, our work calls for a careful assessment of current LLMs for use in high-stakes scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Navreet Kaur",
      "Monojit Choudhury",
      "Danish Pruthi"
    ]
  },
  "http://arxiv.org/abs/2312.08793": {
    "title": "Forbidden Facts: An Investigation of Competing Objectives in Llama-2",
    "volume": "Dec",
    "abstract": "LLMs often face competing pressures (for example helpfulness vs. harmlessness). To understand how models resolve such conflicts, we study Llama-2-chat models on the forbidden fact task. Specifically, we instruct Llama-2 to truthfully complete a factual recall statement while forbidding it from saying the correct answer. This often makes the model give incorrect answers. We decompose Llama-2 into 1000+ components, and rank each one with respect to how useful it is for forbidding the correct answer. We find that in aggregate, around 35 components are enough to reliably implement the full suppression behavior. However, these components are fairly heterogeneous and many operate using faulty heuristics. We discover that one of these heuristics can be exploited via a manually designed adversarial attack which we call The California Attack. Our results highlight some roadblocks standing in the way of being able to successfully interpret advanced ML systems. Project website available at https://forbiddenfacts.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tony T. Wang",
      "Miles Wang",
      "Kaivu Hariharan",
      "Nir Shavit"
    ]
  },
  "http://arxiv.org/abs/2312.08755": {
    "title": "PROPRES: Investigating the Projectivity of Presupposition with Various Triggers and Environments",
    "volume": "Dec",
    "abstract": "What makes a presupposition of an utterance -- information taken for granted by its speaker -- different from other pragmatic inferences such as an entailment is projectivity (e.g., the negative sentence the boy did not stop shedding tears presupposes the boy had shed tears before). The projectivity may vary depending on the combination of presupposition triggers and environments. However, prior natural language understanding studies fail to take it into account as they either use no human baseline or include only negation as an entailment-canceling environment to evaluate models' performance. The current study attempts to reconcile these issues. We introduce a new dataset, projectivity of presupposition (PROPRES, which includes 12k premise-hypothesis pairs crossing six triggers involving some lexical variety with five environments. Our human evaluation reveals that humans exhibit variable projectivity in some cases. However, the model evaluation shows that the best-performed model, DeBERTa, does not fully capture it. Our findings suggest that probing studies on pragmatic inferences should take extra care of the human judgment variability and the combination of linguistic items",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daiki Asami",
      "Saku Sugawara"
    ]
  },
  "http://arxiv.org/abs/2312.08747": {
    "title": "Dissecting vocabulary biases datasets through statistical testing and automated data augmentation for artifact mitigation in Natural Language Inference",
    "volume": "Dec",
    "abstract": "In recent years, the availability of large-scale annotated datasets, such as the Stanford Natural Language Inference and the Multi-Genre Natural Language Inference, coupled with the advent of pre-trained language models, has significantly contributed to the development of the natural language inference domain. However, these crowdsourced annotated datasets often contain biases or dataset artifacts, leading to overestimated model performance and poor generalization. In this work, we focus on investigating dataset artifacts and developing strategies to address these issues. Through the utilization of a novel statistical testing procedure, we discover a significant association between vocabulary distribution and text entailment classes, emphasizing vocabulary as a notable source of biases. To mitigate these issues, we propose several automatic data augmentation strategies spanning character to word levels. By fine-tuning the ELECTRA pre-trained language model, we compare the performance of boosted models with augmented data against their baseline counterparts. The experiments demonstrate that the proposed approaches effectively enhance model accuracy and reduce biases by up to 0.66% and 1.14%, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dat Thanh Nguyen"
    ]
  },
  "http://arxiv.org/abs/2312.08737": {
    "title": "JPIS: A Joint Model for Profile-based Intent Detection and Slot Filling with Slot-to-Intent Attention",
    "volume": "Dec",
    "abstract": "Profile-based intent detection and slot filling are important tasks aimed at reducing the ambiguity in user utterances by leveraging user-specific supporting profile information. However, research in these two tasks has not been extensively explored. To fill this gap, we propose a joint model, namely JPIS, designed to enhance profile-based intent detection and slot filling. JPIS incorporates the supporting profile information into its encoder and introduces a slot-to-intent attention mechanism to transfer slot information representations to intent detection. Experimental results show that our JPIS substantially outperforms previous profile-based models, establishing a new state-of-the-art performance in overall accuracy on the Chinese benchmark dataset ProSLU",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thinh Pham",
      "Dat Quoc Nguyen"
    ]
  },
  "http://arxiv.org/abs/2312.08726": {
    "title": "Labels Need Prompts Too Mask Matching for Natural Language Understanding Tasks",
    "volume": "Dec",
    "abstract": "Textual label names (descriptions) are typically semantically rich in many natural language understanding (NLU) tasks. In this paper, we incorporate the prompting methodology, which is widely used to enrich model input, into the label side for the first time. Specifically, we propose a Mask Matching method, which equips an input with a prompt and its label with another, and then makes predictions by matching their mask representations. We evaluate our method extensively on 8 NLU tasks with 14 datasets. The experimental results show that Mask Matching significantly outperforms its counterparts of fine-tuning and conventional prompt-tuning, setting up state-of-the-art performances in several datasets. Mask Matching is particularly good at handling NLU tasks with large label counts and informative label names. As pioneering efforts that investigate the label-side prompt, we also discuss open issues for future study",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Li",
      "Wei Ye",
      "Quansen Wang",
      "Wen Zhao",
      "Shikun Zhang"
    ]
  },
  "http://arxiv.org/abs/2312.08725": {
    "title": "A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis",
    "volume": "Dec",
    "abstract": "Financial sentiment analysis plays a crucial role in uncovering latent patterns and detecting emerging trends, enabling individuals to make well-informed decisions that may yield substantial advantages within the constantly changing realm of finance. Recently, Large Language Models (LLMs) have demonstrated their effectiveness in diverse domains, showcasing remarkable capabilities even in zero-shot and few-shot in-context learning for various Natural Language Processing (NLP) tasks. Nevertheless, their potential and applicability in the context of financial sentiment analysis have not been thoroughly explored yet. To bridge this gap, we employ two approaches: in-context learning (with a focus on gpt-3.5-turbo model) and fine-tuning LLMs on a finance-domain dataset. Given the computational costs associated with fine-tuning LLMs with large parameter sizes, our focus lies on smaller LLMs, spanning from 250M to 3B parameters for fine-tuning. We then compare the performances with state-of-the-art results to evaluate their effectiveness in the finance-domain. Our results demonstrate that fine-tuned smaller LLMs can achieve comparable performance to state-of-the-art fine-tuned LLMs, even with models having fewer parameters and a smaller training dataset. Additionally, the zero-shot and one-shot performance of LLMs produces comparable results with fine-tuned smaller LLMs and state-of-the-art outcomes. Furthermore, our analysis demonstrates that there is no observed enhancement in performance for finance-domain sentiment analysis when the number of shots for in-context learning is increased",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sorouralsadat Fatemi",
      "Yuheng Hu"
    ]
  },
  "http://arxiv.org/abs/2312.08688": {
    "title": "TigerBot: An Open Multilingual Multitask LLM",
    "volume": "Dec",
    "abstract": "We release and introduce the TigerBot family of large language models (LLMs), consisting of base and chat models, sized from 7, 13, 70 and 180 billion parameters. We develop our models embarking from Llama-2 and BLOOM, and push the boundary further in data, training algorithm, infrastructure, and application tools. Our models yield meaningful performance gain over SOTA open-source models, e.g., Llama-2, specifically 6\\% gain in English and 20\\% gain in Chinese. TigerBot model family also achieves leading performance in major academic and industrial benchmarks and leaderboards. We believe that TigerBot represents just a snapshot of lightning-fast progression in LLM open-source community. Therefore, we are thrilled to give back by publicly releasing our models and reporting our approach behind, with additional emphases on building SOTA LLMs in a democratized way and making LLMs of use in real-world applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Chen",
      "Wei Cai",
      "Liangmin Wu",
      "Xiaowei Li",
      "Zhanxuan Xin",
      "Cong Fu"
    ]
  },
  "http://arxiv.org/abs/2312.08676": {
    "title": "SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention",
    "volume": "Dec",
    "abstract": "Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to arbitrary unseen target speaker timbre, while keeping the linguistic content unchanged. Although the voice of generated speech can be controlled by providing the speaker embedding of the target speaker, the speaker similarity still lags behind the ground truth recordings. In this paper, we propose SEF-VC, a speaker embedding free voice conversion model, which is designed to learn and incorporate speaker timbre from reference speech via a powerful position-agnostic cross-attention mechanism, and then reconstruct waveform from HuBERT semantic tokens in a non-autoregressive manner. The concise design of SEF-VC enhances its training stability and voice conversion performance. Objective and subjective evaluations demonstrate the superiority of SEF-VC to generate high-quality speech with better similarity to target reference than strong zero-shot VC baselines, even for very short reference speeches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Li",
      "Yiwei Guo",
      "Xie Chen",
      "Kai Yu"
    ]
  },
  "http://arxiv.org/abs/2312.08642": {
    "title": "Metacognition-Enhanced Few-Shot Prompting With Positive Reinforcement",
    "volume": "Dec",
    "abstract": "Few-shot prompting elicits the remarkable abilities of large language models by equipping them with a few demonstration examples in the input. However, the traditional method of providing large language models with all demonstration input-output pairs at once may not effectively guide large language models to learn the specific input-output mapping relationship. In this paper, inspired by the regulatory and supportive role of metacognition in students' learning, we propose a novel metacognition-enhanced few-shot prompting, which guides large language models to reflect on their thought processes to comprehensively learn the given demonstration examples. Furthermore, considering that positive reinforcement can improve students' learning motivation, we introduce positive reinforcement into our metacognition-enhanced few-shot prompting to promote the few-shot learning of large language models by providing response-based positive feedback. The experimental results on two real-world datasets show that our metacognition-enhanced few-shot prompting with positive reinforcement surpasses traditional few-shot prompting in classification accuracy and macro F1",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Ji",
      "Wen Wu",
      "Yi Hu",
      "Hong Zheng",
      "Liang He"
    ]
  },
  "http://arxiv.org/abs/2312.08618": {
    "title": "Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention",
    "volume": "Dec",
    "abstract": "This paper introduces a novel approach to enhance the capabilities of Large Language Models (LLMs) in processing and understanding extensive text sequences, a critical aspect in applications requiring deep comprehension and synthesis of large volumes of information. Recognizing the inherent challenges in extending the context window for LLMs, primarily built on Transformer architecture, we propose a new model architecture, referred to as Zebra. This architecture efficiently manages the quadratic time and memory complexity issues associated with full attention in the Transformer by employing grouped local-global attention layers. Our model, akin to a zebra's alternating stripes, balances local and global attention layers, significantly reducing computational requirements and memory consumption. Comprehensive experiments, including pretraining from scratch, continuation of long context adaptation training, and long instruction tuning, are conducted to evaluate the Zebra's performance. The results show that Zebra achieves comparable or superior performance on both short and long sequence benchmarks, while also enhancing training and inference efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiqiang Song",
      "Xiaoyang Wang",
      "Sangwoo Cho",
      "Xiaoman Pan",
      "Dong Yu"
    ]
  },
  "http://arxiv.org/abs/2312.08585": {
    "title": "Unraveling Key Factors of Knowledge Distillation",
    "volume": "Dec",
    "abstract": "Knowledge distillation, a technique for model compression and performance enhancement, has gained significant traction in Neural Machine Translation (NMT). However, existing research primarily focuses on empirical applications, and there is a lack of comprehensive understanding of how student model capacity, data complexity, and decoding strategies collectively influence distillation effectiveness. Addressing this gap, our study conducts an in-depth investigation into these factors, particularly focusing on their interplay in word-level and sequence-level distillation within NMT. Through extensive experimentation across datasets like IWSLT13 En$\\rightarrow$Fr, IWSLT14 En$\\rightarrow$De, and others, we empirically validate hypotheses related to the impact of these factors on knowledge distillation. Our research not only elucidates the significant influence of model capacity, data complexity, and decoding strategies on distillation effectiveness but also introduces a novel, optimized distillation approach. This approach, when applied to the IWSLT14 de$\\rightarrow$en translation task, achieves state-of-the-art performance, demonstrating its practical efficacy in advancing the field of NMT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingxuan Wei",
      "Linzhuang Sun",
      "Xu Tan",
      "Bihui Yu",
      "Ruifeng Guo"
    ]
  },
  "http://arxiv.org/abs/2312.08583": {
    "title": "ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks",
    "volume": "Dec",
    "abstract": "This study examines 4-bit quantization methods like GPTQ in large language models (LLMs), highlighting GPTQ's overfitting and limited enhancement in Zero-Shot tasks. While prior works merely focusing on zero-shot measurement, we extend task scope to more generative categories such as code generation and abstractive summarization, in which we found that INT4 quantization can significantly underperform. However, simply shifting to higher precision formats like FP6 has been particularly challenging, thus overlooked, due to poor performance caused by the lack of sophisticated integration and system acceleration strategies on current AI hardware. Our results show that FP6, even with a coarse-grain quantization scheme, performs robustly across various algorithms and tasks, demonstrating its superiority in accuracy and versatility. Notably, with the FP6 quantization, \\codestar-15B model performs comparably to its FP16 counterpart in code generation, and for smaller models like the 406M it closely matches their baselines in summarization. Neither can be achieved by INT4. To better accommodate various AI hardware and achieve the best system performance, we propose a novel 4+2 design for FP6 to achieve similar latency to the state-of-the-art INT4 fine-grain quantization. With our design, FP6 can become a promising solution to the current 4-bit quantization methods used in LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoxia Wu",
      "Haojun Xia",
      "Stephen Youn",
      "Zhen Zheng",
      "Shiyang Chen",
      "Arash Bakhtiari",
      "Michael Wyatt",
      "Yuxiong He",
      "Olatunji Ruwase",
      "Leon Song",
      "Zhewei Yao"
    ]
  },
  "http://arxiv.org/abs/2312.08579": {
    "title": "Identifying Planetary Names in Astronomy Papers: A Multi-Step Approach",
    "volume": "Dec",
    "abstract": "The automatic identification of planetary feature names in astronomy publications presents numerous challenges. These features include craters, defined as roughly circular depressions resulting from impact or volcanic activity; dorsas, which are elongate raised structures or wrinkle ridges; and lacus, small irregular patches of dark, smooth material on the Moon, referred to as \"lake\" (Planetary Names Working Group, n.d.). Many feature names overlap with places or people's names that they are named after, for example, Syria, Tempe, Einstein, and Sagan, to name a few (U.S. Geological Survey, n.d.). Some feature names have been used in many contexts, for instance, Apollo, which can refer to mission, program, sample, astronaut, seismic, seismometers, core, era, data, collection, instrument, and station, in addition to the crater on the Moon. Some feature names can appear in the text as adjectives, like the lunar craters Black, Green, and White. Some feature names in other contexts serve as directions, like craters West and South on the Moon. Additionally, some features share identical names across different celestial bodies, requiring disambiguation, such as the Adams crater, which exists on both the Moon and Mars. We present a multi-step pipeline combining rule-based filtering, statistical relevance analysis, part-of-speech (POS) tagging, named entity recognition (NER) model, hybrid keyword harvesting, knowledge graph (KG) matching, and inference with a locally installed large language model (LLM) to reliably identify planetary names despite these challenges. When evaluated on a dataset of astronomy papers from the Astrophysics Data System (ADS), this methodology achieves an F1-score over 0.97 in disambiguating planetary feature names",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Golnaz Shapurian",
      "Michael J Kurtz",
      "Alberto Accomazzi"
    ]
  },
  "http://arxiv.org/abs/2312.08566": {
    "title": "Learning adaptive planning representations with natural language guidance",
    "volume": "Dec",
    "abstract": "Effective planning in the real world requires not only world knowledge, but the ability to leverage that knowledge to build the right representation of the task at hand. Decades of hierarchical planning techniques have used domain-specific temporal action abstractions to support efficient and accurate planning, almost always relying on human priors and domain knowledge to decompose hard tasks into smaller subproblems appropriate for a goal or set of goals. This paper describes Ada (Action Domain Acquisition), a framework for automatically constructing task-specific planning representations using task-general background knowledge from language models (LMs). Starting with a general-purpose hierarchical planner and a low-level goal-conditioned policy, Ada interactively learns a library of planner-compatible high-level action abstractions and low-level controllers adapted to a particular domain of planning tasks. On two language-guided interactive planning benchmarks (Mini Minecraft and ALFRED Household Tasks), Ada strongly outperforms other approaches that use LMs for sequential decision-making, offering more accurate plans and better generalization to complex tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lionel Wong",
      "Jiayuan Mao",
      "Pratyusha Sharma",
      "Zachary S. Siegel",
      "Jiahai Feng",
      "Noa Korneev",
      "Joshua B. Tenenbaum",
      "Jacob Andreas"
    ]
  },
  "http://arxiv.org/abs/2312.08495": {
    "title": "Beyond Accuracy: Automated De-Identification of Large Real-World Clinical Text Datasets",
    "volume": "Dec",
    "abstract": "Recent research advances achieve human-level accuracy for de-identifying free-text clinical notes on research datasets, but gaps remain in reproducing this in large real-world settings. This paper summarizes lessons learned from building a system used to de-identify over one billion real clinical notes, in a fully automated way, that was independently certified by multiple organizations for production use. A fully automated solution requires a very high level of accuracy that does not require manual review. A hybrid context-based model architecture is described, which outperforms a Named Entity Recogniton (NER) - only model by 10% on the i2b2-2014 benchmark. The proposed system makes 50%, 475%, and 575% fewer errors than the comparable AWS, Azure, and GCP services respectively while also outperforming ChatGPT by 33%. It exceeds 98% coverage of sensitive data across 7 European languages, without a need for fine tuning. A second set of described models enable data obfuscation -- replacing sensitive data with random surrogates -- while retaining name, date, gender, clinical, and format consistency. Both the practical need and the solution architecture that provides for reliable & linked anonymized documents are described",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Veysel Kocaman",
      "Hasham Ul Haq",
      "David Talby"
    ]
  },
  "http://arxiv.org/abs/2312.08303": {
    "title": "Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models",
    "volume": "Dec",
    "abstract": "Toxic content detection is crucial for online services to remove inappropriate content that violates community standards. To automate the detection process, prior works have proposed varieties of machine learning (ML) approaches to train Language Models (LMs) for toxic content detection. However, both their accuracy and transferability across datasets are limited. Recently, Large Language Models (LLMs) have shown promise in toxic content detection due to their superior zero-shot and few-shot in-context learning ability as well as broad transferability on ML tasks. However, efficiently designing prompts for LLMs remains challenging. Moreover, the high run-time cost of LLMs may hinder their deployments in production. To address these challenges, in this work, we propose BD-LLM, a novel and efficient approach to Bootstrapping and Distilling LLMs for toxic content detection. Specifically, we design a novel prompting method named Decision-Tree-of-Thought (DToT) to bootstrap LLMs' detection performance and extract high-quality rationales. DToT can automatically select more fine-grained context to re-prompt LLMs when their responses lack confidence. Additionally, we use the rationales extracted via DToT to fine-tune student LMs. Our experimental results on various datasets demonstrate that DToT can improve the accuracy of LLMs by up to 4.6%. Furthermore, student LMs fine-tuned with rationales extracted via DToT outperform baselines on all datasets with up to 16.9\\% accuracy improvement, while being more than 60x smaller than conventional LLMs. Finally, we observe that student LMs fine-tuned with rationales exhibit better cross-dataset transferability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiang Zhang",
      "Qiong Wu",
      "Yiming Xu",
      "Cheng Cao",
      "Zheng Du",
      "Konstantinos Psounis"
    ]
  },
  "http://arxiv.org/abs/2312.08299": {
    "title": "Conceptualizing Suicidal Behavior: Utilizing Explanations of Predicted Outcomes to Analyze Longitudinal Social Media Data",
    "volume": "Dec",
    "abstract": "The COVID-19 pandemic has escalated mental health crises worldwide, with social isolation and economic instability contributing to a rise in suicidal behavior. Suicide can result from social factors such as shame, abuse, abandonment, and mental health conditions like depression, Post-Traumatic Stress Disorder (PTSD), Attention-Deficit/Hyperactivity Disorder (ADHD), anxiety disorders, and bipolar disorders. As these conditions develop, signs of suicidal ideation may manifest in social media interactions. Analyzing social media data using artificial intelligence (AI) techniques can help identify patterns of suicidal behavior, providing invaluable insights for suicide prevention agencies, professionals, and broader community awareness initiatives. Machine learning algorithms for this purpose require large volumes of accurately labeled data. Previous research has not fully explored the potential of incorporating explanations in analyzing and labeling longitudinal social media data. In this study, we employed a model explanation method, Layer Integrated Gradients, on top of a fine-tuned state-of-the-art language model, to assign each token from Reddit users' posts an attribution score for predicting suicidal ideation. By extracting and analyzing attributions of tokens from the data, we propose a methodology for preliminary screening of social media posts for suicidal ideation without using large language models during inference",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Van Minh Nguyen",
      "Nasheen Nur",
      "William Stern",
      "Thomas Mercer",
      "Chiradeep Sen",
      "Siddhartha Bhattacharyya",
      "Victor Tumbiolo",
      "Seng Jhing Goh"
    ]
  },
  "http://arxiv.org/abs/2312.08282": {
    "title": "Prompting LLMs with content plans to enhance the summarization of scientific articles",
    "volume": "Dec",
    "abstract": "This paper presents novel prompting techniques to improve the performance of automatic summarization systems for scientific articles. Scientific article summarization is highly challenging due to the length and complexity of these documents. We conceive, implement, and evaluate prompting techniques that provide additional contextual information to guide summarization systems. Specifically, we feed summarizers with lists of key terms extracted from articles, such as author keywords or automatically generated keywords. Our techniques are tested with various summarization models and input texts. Results show performance gains, especially for smaller models summarizing sections separately. This evidences that prompting is a promising approach to overcoming the limitations of less powerful systems. Our findings introduce a new research direction of using prompts to aid smaller models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aldan Creo",
      "Manuel Lama",
      "Juan C. Vidal"
    ]
  },
  "http://arxiv.org/abs/2312.08274": {
    "title": "High-throughput Biomedical Relation Extraction for Semi-Structured Web Articles Empowered by Large Language Models",
    "volume": "Dec",
    "abstract": "Objective: To develop a high-throughput biomedical relation extraction system that takes advantage of the large language models' (LLMs) reading comprehension ability and biomedical world knowledge in a scalable and evidential manner. Methods: We formulate the relation extraction task as a simple binary classification problem for large language models such as ChatGPT. Specifically, LLMs make the decision based on the external corpus and its world knowledge, giving the reason for the judgment to factual verification. This method is tailored for semi-structured web articles, wherein we designate the main title as the tail entity and explicitly incorporate it into the context, and the potential head entities are matched based on a biomedical thesaurus. Moreover, lengthy contents are sliced into text chunks, embedded, and retrieved with additional embedding models, ensuring compatibility with the context window size constraints of available open-source LLMs. Results: Using an open-source LLM, we extracted 304315 relation triplets of three distinct relation types from four reputable biomedical websites. To assess the efficacy of the basic pipeline employed for biomedical relation extraction, we curated a benchmark dataset annotated by a medical expert. Evaluation results indicate that the pipeline exhibits performance comparable to that of GPT-4. Case studies further illuminate challenges faced by contemporary LLMs in the context of biomedical relation extraction for semi-structured web articles. Conclusion: The proposed method has demonstrated its effectiveness in leveraging the strengths of LLMs for high-throughput biomedical relation extraction. Its adaptability is evident, as it can be seamlessly extended to diverse semi-structured biomedical websites, facilitating the extraction of various types of biomedical relations with ease",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songchi Zhou",
      "Sheng Yu"
    ]
  },
  "http://arxiv.org/abs/2312.08198": {
    "title": "Towards Model-Based Data Acquisition for Subjective Multi-Task NLP Problems",
    "volume": "Dec",
    "abstract": "Data annotated by humans is a source of knowledge by describing the peculiarities of the problem and therefore fueling the decision process of the trained model. Unfortunately, the annotation process for subjective natural language processing (NLP) problems like offensiveness or emotion detection is often very expensive and time-consuming. One of the inevitable risks is to spend some of the funds and annotator effort on annotations that do not provide any additional knowledge about the specific task. To minimize these costs, we propose a new model-based approach that allows the selection of tasks annotated individually for each text in a multi-task scenario. The experiments carried out on three datasets, dozens of NLP tasks, and thousands of annotations show that our method allows up to 40% reduction in the number of annotations with negligible loss of knowledge. The results also emphasize the need to collect a diverse amount of data required to efficiently train a model, depending on the subjectivity of the annotation task. We also focused on measuring the relation between subjective tasks by evaluating the model in single-task and multi-task scenarios. Moreover, for some datasets, training only on the labels predicted by our model improved the efficiency of task selection as a self-supervised learning regularization technique",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kamil Kanclerz",
      "Julita Bielaniewicz",
      "Marcin Gruza",
      "Jan Kocon",
      "Stanisław Woźniak",
      "Przemysław Kazienko"
    ]
  },
  "http://arxiv.org/abs/2312.08079": {
    "title": "Extending Whisper with prompt tuning to target-speaker ASR",
    "volume": "Dec",
    "abstract": "Target-speaker automatic speech recognition (ASR) aims to transcribe the desired speech of a target speaker from multi-talker overlapped utterances. Most of the existing target-speaker ASR (TS-ASR) methods involve either training from scratch or fully fine-tuning a pre-trained model, leading to significant training costs and becoming inapplicable to large foundation models. This work leverages prompt tuning, a parameter-efficient fine-tuning approach, to extend Whisper, a large-scale single-talker ASR model, to TS-ASR. Experimental results show that prompt tuning can achieve performance comparable to state-of-the-art full fine-tuning approaches while only requiring about 1% of task-specific model parameters. Notably, the original Whisper's features, such as inverse text normalization and timestamp prediction, are retained in target-speaker ASR, keeping the generated transcriptions natural and informative",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Ma",
      "Zhiyuan Peng",
      "Mingjie Shao",
      "Jing Li",
      "Ju Liu"
    ]
  },
  "http://arxiv.org/abs/2312.08078": {
    "title": "Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic Image-Report Generation",
    "volume": "Dec",
    "abstract": "To address these issues, we propose a novel Adaptive patch-word Matching (AdaMatch) model to correlate chest X-ray (CXR) image regions with words in medical reports and apply it to CXR-report generation to provide explainability for the generation process. AdaMatch exploits the fine-grained relation between adaptive patches and words to provide explanations of specific image regions with corresponding words. To capture the abnormal regions of varying sizes and positions, we introduce the Adaptive Patch extraction (AdaPatch) module to acquire the adaptive patches for these regions adaptively. In order to provide explicit explainability for CXR-report generation task, we propose an AdaMatch-based bidirectional large language model for Cyclic CXR-report generation (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords for CXR images and `keypatches' for medical reports as hints to guide CXR-report generation. Extensive experiments on two publicly available CXR datasets prove the effectiveness of our method and its superior performance to existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenting Chen",
      "Xiang Li",
      "Linlin Shen",
      "Yixuan Yuan"
    ]
  },
  "http://arxiv.org/abs/2312.08063": {
    "title": "Estimation of Concept Explanations Should be Uncertainty Aware",
    "volume": "Dec",
    "abstract": "Model explanations are very valuable for interpreting and debugging prediction models. We study a specific kind of global explanations called Concept Explanations, where the goal is to interpret a model using human-understandable concepts. Recent advances in multi-modal learning rekindled interest in concept explanations and led to several label-efficient proposals for estimation. However, existing estimation methods are unstable to the choice of concepts or dataset that is used for computing explanations. We observe that instability in explanations is due to high variance in point estimation of importance scores. We propose an uncertainty aware Bayesian estimation method, which readily improved reliability of the concept explanations. We demonstrate with theoretical analysis and empirical evaluation that explanations computed by our method are more reliable while also being label-efficient and faithful",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vihari Piratla",
      "Juyeon Heo",
      "Sukriti Singh",
      "Adrian Weller"
    ]
  },
  "http://arxiv.org/abs/2312.08036": {
    "title": "CoRTEx: Contrastive Learning for Representing Terms via Explanations with Applications on Constructing Biomedical Knowledge Graphs",
    "volume": "Dec",
    "abstract": "Objective: Biomedical Knowledge Graphs play a pivotal role in various biomedical research domains. Concurrently, term clustering emerges as a crucial step in constructing these knowledge graphs, aiming to identify synonymous terms. Due to a lack of knowledge, previous contrastive learning models trained with Unified Medical Language System (UMLS) synonyms struggle at clustering difficult terms and do not generalize well beyond UMLS terms. In this work, we leverage the world knowledge from Large Language Models (LLMs) and propose Contrastive Learning for Representing Terms via Explanations (CoRTEx) to enhance term representation and significantly improves term clustering. Materials and Methods: The model training involves generating explanations for a cleaned subset of UMLS terms using ChatGPT. We employ contrastive learning, considering term and explanation embeddings simultaneously, and progressively introduce hard negative samples. Additionally, a ChatGPT-assisted BIRCH algorithm is designed for efficient clustering of a new ontology. Results: We established a clustering test set and a hard negative test set, where our model consistently achieves the highest F1 score. With CoRTEx embeddings and the modified BIRCH algorithm, we grouped 35,580,932 terms from the Biomedical Informatics Ontology System (BIOS) into 22,104,559 clusters with O(N) queries to ChatGPT. Case studies highlight the model's efficacy in handling challenging samples, aided by information from explanations. Conclusion: By aligning terms to their explanations, CoRTEx demonstrates superior accuracy over benchmark models and robustness beyond its training set, and it is suitable for clustering terms for large-scale biomedical ontologies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huaiyuan Ying",
      "Zhengyun Zhao",
      "Yang Zhao",
      "Sihang Zeng",
      "Sheng Yu"
    ]
  },
  "http://arxiv.org/abs/2312.08027": {
    "title": "Helping Language Models Learn More: Multi-dimensional Task Prompt for Few-shot Tuning",
    "volume": "Dec",
    "abstract": "Large language models (LLMs) can be used as accessible and intelligent chatbots by constructing natural language queries and directly inputting the prompt into the large language model. However, different prompt' constructions often lead to uncertainty in the answers and thus make it hard to utilize the specific knowledge of LLMs (like ChatGPT). To alleviate this, we use an interpretable structure to explain the prompt learning principle in LLMs, which certificates that the effectiveness of language models is determined by position changes of the task's related tokens. Therefore, we propose MTPrompt, a multi-dimensional task prompt learning method consisting based on task-related object, summary, and task description information. By automatically building and searching for appropriate prompts, our proposed MTPrompt achieves the best results on few-shot samples setting and five different datasets. In addition, we demonstrate the effectiveness and stability of our method in different experimental settings and ablation experiments. In interaction with large language models, embedding more task-related information into prompts will make it easier to stimulate knowledge embedded in large language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinta Weng",
      "Jiarui Zhang",
      "Yue Hu",
      "Daidong Fa",
      "Xiaofeng Xuand",
      "Heyan Huang"
    ]
  },
  "http://arxiv.org/abs/2312.07987": {
    "title": "SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention",
    "volume": "Dec",
    "abstract": "The costly self-attention layers in modern Transformers require memory and compute quadratic in sequence length. Existing approximation methods usually underperform and fail to obtain significant speedups in practice. Here we present SwitchHead - a novel method that reduces both compute and memory requirements and achieves wall-clock speedup, while matching the language modeling performance of baseline Transformers with the same parameter budget. SwitchHead uses Mixture-of-Experts (MoE) layers for the value and output projections and requires 4 to 8 times fewer attention matrices than standard Transformers. Our novel attention can also be combined with MoE MLP layers, resulting in an efficient fully-MoE \"SwitchHead\" Transformer model. Our code is public",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Róbert Csordás",
      "Piotr Piękos",
      "Kazuki Irie",
      "Jürgen Schmidhuber"
    ]
  },
  "http://arxiv.org/abs/2312.07979": {
    "title": "SLJP: Semantic Extraction based Legal Judgment Prediction",
    "volume": "Dec",
    "abstract": "Legal Judgment Prediction (LJP) is a judicial assistance system that recommends the legal components such as applicable statues, prison term and penalty term by analyzing the given input case document. Indian legal system is in the need of technical assistance such as artificial intelligence to solve the crores of pending cases in various courts for years and its being increased day to day. Most of the existing Indian models did not adequately concentrate on the semantics embedded in the fact description (FD) that impacts the decision. The proposed semantic extraction based LJP (SLJP) model provides the advantages of pretrained transformers for complex unstructured legal case document understanding and to generate embeddings. The model draws the in-depth semantics of the given FD at multiple levels i.e., chunk and case document level by following the divide and conquer approach. It creates the concise view of the given fact description using the extracted semantics as per the original court case document structure and predicts judgment using attention mechanism. We tested the model performance on two available Indian datasets Indian Legal Documents corpus (ILDC) and Indian Legal Statue Identification (ILSI) and got promising results. Also shown the highest performance and less performance degradation for increased epochs than base models on ILDC dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prameela Madambakam",
      "Shathanaa Rajmohan",
      "Himangshu Sharma",
      "Tummepalli Anka Chandrahas Purushotham Gupta"
    ]
  },
  "http://arxiv.org/abs/2312.07961": {
    "title": "Robust Few-Shot Named Entity Recognition with Boundary Discrimination and Correlation Purification",
    "volume": "Dec",
    "abstract": "Few-shot named entity recognition (NER) aims to recognize novel named entities in low-resource domains utilizing existing knowledge. However, the present few-shot NER models assume that the labeled data are all clean without noise or outliers, and there are few works focusing on the robustness of the cross-domain transfer learning ability to textual adversarial attacks in Few-shot NER. In this work, we comprehensively explore and assess the robustness of few-shot NER models under textual adversarial attack scenario, and found the vulnerability of existing few-shot NER models. Furthermore, we propose a robust two-stage few-shot NER method with Boundary Discrimination and Correlation Purification (BDCP). Specifically, in the span detection stage, the entity boundary discriminative module is introduced to provide a highly distinguishing boundary representation space to detect entity spans. In the entity typing stage, the correlations between entities and contexts are purified by minimizing the interference information and facilitating correlation generalization to alleviate the perturbations caused by textual adversarial attacks. In addition, we construct adversarial examples for few-shot NER based on public datasets Few-NERD and Cross-Dataset. Comprehensive evaluations on those two groups of few-shot NER datasets containing adversarial examples demonstrate the robustness and superiority of the proposed method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaojun Xue",
      "Chunxia Zhang",
      "Tianxiang Xu",
      "Zhendong Niu"
    ]
  },
  "http://arxiv.org/abs/2312.07950": {
    "title": "CBQ: Cross-Block Quantization for Large Language Models",
    "volume": "Dec",
    "abstract": "Post-training quantization (PTQ) has driven attention to producing efficient large language models (LLMs) with ultra-low costs. Since hand-craft quantization parameters lead to low performance in low-bit quantization, recent methods optimize the quantization parameters through block-wise reconstruction between the floating-point and quantized models. However, these methods suffer from two challenges: accumulated errors from independent one-by-one block quantization and reconstruction difficulties from extreme weight and activation outliers. To address these two challenges, we propose CBQ, a cross-block reconstruction-based PTQ method for LLMs. To reduce error accumulation, we introduce a cross-block dependency with the aid of a homologous reconstruction scheme to build the long-range dependency between adjacent multi-blocks with overlapping. To reduce reconstruction difficulty, we design a coarse-to-fine pre-processing (CFP) to truncate weight outliers and dynamically scale activation outliers before optimization, and an adaptive rounding scheme, called LoRA-Rounding, with two low-rank learnable matrixes to further rectify weight quantization errors. Extensive experiments demonstrate that: (1) CBQ pushes both activation and weight quantization to low-bit settings W4A4, W4A8, and W2A16. (2) CBQ achieves better performance than the existing state-of-the-art methods on various LLMs and benchmark datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Ding",
      "Xiaoyu Liu",
      "Yun Zhang",
      "Zhijun Tu",
      "Wei Li",
      "Jie Hu",
      "Hanting Chen",
      "Yehui Tang",
      "Zhiwei Xiong",
      "Baoqun Yin",
      "Yunhe Wang"
    ]
  },
  "http://arxiv.org/abs/2312.07930": {
    "title": "Towards Optimal Statistical Watermarking",
    "volume": "Dec",
    "abstract": "We study statistical watermarking by formulating it as a hypothesis testing problem, a general framework which subsumes all previous statistical watermarking methods. Key to our formulation is a coupling of the output tokens and the rejection region, realized by pseudo-random generators in practice, that allows non-trivial trade-off between the Type I error and Type II error. We characterize the Uniformly Most Powerful (UMP) watermark in this context. In the most common scenario where the output is a sequence of $n$ tokens, we establish matching upper and lower bounds on the number of i.i.d. tokens required to guarantee small Type I and Type II errors. Our rate scales as $\\Theta(h^{-1} \\log (1/h))$ with respect to the average entropy per token $h$ and thus greatly improves the $O(h^{-2})$ rate in the previous works. For scenarios where the detector lacks knowledge of the model's distribution, we introduce the concept of model-agnostic watermarking and establish the minimax bounds for the resultant increase in Type II error. Moreover, we formulate the robust watermarking problem where user is allowed to perform a class of perturbation on the generated texts, and characterize the optimal type II error of robust UMP tests via a linear programming problem. To the best of our knowledge, this is the first systematic statistical treatment on the watermarking problem with near-optimal rates in the i.i.d. setting, and might be of interest for future works",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baihe Huang",
      "Banghua Zhu",
      "Hanlin Zhu",
      "Jason D. Lee",
      "Jiantao Jiao",
      "Michael I. Jordan"
    ]
  },
  "http://arxiv.org/abs/2312.07913": {
    "title": "A Survey of Text Watermarking in the Era of Large Language Models",
    "volume": "Dec",
    "abstract": "In recent years, significant advancements have been made in the text generation capabilities of Large Language Models (LLMs), demonstrating exceptional performance in downstream tasks such as abstract summarization, dialogue generation, and data-to-text conversion. However, their generative abilities also pose risks such as the rapid spread of fake news, infringement of datasets/LLM copyrights, and challenges to academic integrity. Text watermarking technology emerges as a potential solution. By embedding invisible yet detectable patterns in generated texts, it helps in tracking and verifying text origins, thus preventing misuse and piracy. This survey aims to comprehensively summarize current text watermarking technologies, covering three main aspects: (1) an overview and comparison of different text watermarking techniques; (2) evaluation methods for text watermarking algorithms, including their success rate, impact on text quality, robustness, and unforgeability; (3) potential applications of text watermarking technologys. This survey aims to help researchers thoroughly understanding the text watermarking technologies, thereby fostering further development",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aiwei Liu",
      "Leyi Pan",
      "Yijian Lu",
      "Jingjing Li",
      "Xuming Hu",
      "Lijie Wen",
      "Irwin King",
      "Philip S. Yu"
    ]
  },
  "http://arxiv.org/abs/2312.07910": {
    "title": "PromptBench: A Unified Library for Evaluation of Large Language Models",
    "volume": "Dec",
    "abstract": "The evaluation of large language models (LLMs) is crucial to assess their performance and mitigate potential security risks. In this paper, we introduce PromptBench, a unified library to evaluate LLMs. It consists of several key components that are easily used and extended by researchers: prompt construction, prompt engineering, dataset and model loading, adversarial prompt attack, dynamic evaluation protocols, and analysis tools. PromptBench is designed to be an open, general, and flexible codebase for research purposes that can facilitate original study in creating new benchmarks, deploying downstream applications, and designing new evaluation protocols. The code is available at: https://github.com/microsoft/promptbench and will be continuously supported",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaijie Zhu",
      "Qinlin Zhao",
      "Hao Chen",
      "Jindong Wang",
      "Xing Xie"
    ]
  },
  "http://arxiv.org/abs/2312.08400": {
    "title": "Beyond English: Evaluating LLMs for Arabic Grammatical Error Correction",
    "volume": "Dec",
    "abstract": "Large language models (LLMs) finetuned to follow human instruction have recently exhibited significant capabilities in various English NLP tasks. However, their performance in grammatical error correction (GEC), especially on languages other than English, remains significantly unexplored. In this work, we evaluate the abilities of instruction finetuned LLMs in Arabic GEC, a complex task due to Arabic's rich morphology. Our findings suggest that various prompting methods, coupled with (in-context) few-shot learning, demonstrate considerable effectiveness, with GPT-4 achieving up to $65.49$ F$_{1}$ score under expert prompting (approximately $5$ points higher than our established baseline). Despite these positive results, we find that instruction finetuned models, regardless of their size, are still outperformed by fully finetuned ones, even if they are significantly smaller in size. This disparity highlights substantial room for improvements for LLMs. Inspired by methods used in low-resource machine translation, we also develop a method exploiting synthetic data that significantly outperforms previous models on two standard Arabic benchmarks. Our best model achieves a new SOTA on Arabic GEC, with $73.29$ and $73.26$ F$_{1}$ on the 2014 and 2015 QALB datasets, respectively, compared to peer-reviewed published baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sang Yun Kwon",
      "Gagan Bhatia",
      "El Moatez Billah Nagoudi",
      "Muhammad Abdul-Mageed"
    ]
  },
  "http://arxiv.org/abs/2312.07887": {
    "title": "Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models",
    "volume": "Dec",
    "abstract": "Incremental Learning (IL) has been a long-standing problem in both vision and Natural Language Processing (NLP) communities. In recent years, as Pre-trained Language Models (PLMs) have achieved remarkable progress in various NLP downstream tasks, utilizing PLMs as backbones has become a common practice in recent research of IL in NLP. Most assume that catastrophic forgetting is the biggest obstacle to achieving superior IL performance and propose various techniques to overcome this issue. However, we find that this assumption is problematic. Specifically, we revisit more than 20 methods on four classification tasks (Text Classification, Intent Classification, Relation Extraction, and Named Entity Recognition) under the two most popular IL settings (Class-Incremental and Task-Incremental) and reveal that most of them severely underestimate the inherent anti-forgetting ability of PLMs. Based on the observation, we propose a frustratingly easy method called SEQ* for IL with PLMs. The results show that SEQ* has competitive or superior performance compared to state-of-the-art (SOTA) IL methods and requires considerably less trainable parameters and training time. These findings urge us to revisit the IL with PLMs and encourage future studies to have a fundamental understanding of the catastrophic forgetting in PLMs. The data, code and scripts are publicly available at https://github.com/zzz47zzz/pretrained-lm-for-incremental-learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhao Zheng",
      "Shengjie Qiu",
      "Qianli Ma"
    ]
  },
  "http://arxiv.org/abs/2312.07886": {
    "title": "Modality Plug-and-Play: Elastic Modality Adaptation in Multimodal LLMs for Embodied AI",
    "volume": "Dec",
    "abstract": "Large Language Models (LLMs) are capable of reasoning over diverse input data modalities through pre-trained encoders. However, the growing diversity of input data modalities prevents incorporating all modalities into LLMs, especially when LLMs are deployed on resource-constrained edge devices for embodied AI applications. Instead, a better option is to adaptively involve only the useful modalities at runtime, depending on the current environmental contexts and task requirements. For such modality adaptation, existing work adopts fixed connections between encoders and the LLM's input layer, leading to high training cost at runtime and ineffective cross-modal interaction. In this paper, we address these limitations by presenting mPnP-LLM, a new technique that allows fully elastic, automated and prompt runtime modality adaptation, by connecting unimodal encoders to a flexible set of last LLM blocks and making such latent connections fully trainable at runtime. Experiments over the nuScenes-QA dataset show that mPnP-LLM can achieve up to 3.7x FLOPs reduction and 30% GPU memory usage reduction, while retaining on-par accuracy with the existing schemes. Under the same compute budget, mPnP-LLM improves the task accuracy by up to 4% compared to the best existing scheme",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Huang",
      "Boyuan Yang",
      "Wei Gao"
    ]
  },
  "http://arxiv.org/abs/2312.07868": {
    "title": "Graph vs. Sequence: An Empirical Study on Knowledge Forms for Knowledge-Grounded Dialogue",
    "volume": "Dec",
    "abstract": "Knowledge-grounded dialogue is a task of generating an informative response based on both the dialogue history and external knowledge source. In general, there are two forms of knowledge: manually annotated knowledge graphs and knowledge text from website. From various evaluation viewpoints, each type of knowledge has advantages and downsides. To further distinguish the principles and determinants from the intricate factors, we conduct a thorough experiment and study on the task to answer three essential questions. The questions involve the choice of appropriate knowledge form, the degree of mutual effects between knowledge and the model selection, and the few-shot performance of knowledge. Supported by statistical shreds of evidence, we offer conclusive solutions and sensible suggestions for directions and standards of future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizhe Yang",
      "Heyan Huang",
      "Yihang Liu",
      "Yang Gao"
    ]
  },
  "http://arxiv.org/abs/2312.07867": {
    "title": "BESTMVQA: A Benchmark Evaluation System for Medical Visual Question Answering",
    "volume": "Dec",
    "abstract": "Medical Visual Question Answering (Med-VQA) is a very important task in healthcare industry, which answers a natural language question with a medical image. Existing VQA techniques in information systems can be directly applied to solving the task. However, they often suffer from (i) the data insufficient problem, which makes it difficult to train the state of the arts (SOTAs) for the domain-specific task, and (ii) the reproducibility problem, that many existing models have not been thoroughly evaluated in a unified experimental setup. To address these issues, this paper develops a Benchmark Evaluation SysTem for Medical Visual Question Answering, denoted by BESTMVQA. Given self-collected clinical data, our system provides a useful tool for users to automatically build Med-VQA datasets, which helps overcoming the data insufficient problem. Users also can conveniently select a wide spectrum of SOTA models from our model library to perform a comprehensive empirical study. With simple configurations, our system automatically trains and evaluates the selected models over a benchmark dataset, and reports the comprehensive results for users to develop new techniques or perform medical practice. Limitations of existing work are overcome (i) by the data generation tool, which automatically constructs new datasets from unstructured clinical data, and (ii) by evaluating SOTAs on benchmark datasets in a unified experimental setup. The demonstration video of our system can be found at https://youtu.be/QkEeFlu1x4A. Our code and data will be available soon",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaojie Hong",
      "Zixin Song",
      "Liangzhi Li",
      "Xiaoli Wang",
      "Feiyan Liu"
    ]
  },
  "http://arxiv.org/abs/2312.07848": {
    "title": "Finetuning an LLM on Contextual Knowledge of Classics for Q&A",
    "volume": "Dec",
    "abstract": "The open-source publishing of large language models (LLMs) has created many possibilities for how anyone who understands language and has access to a computer can interact with significant tools of artificial intelligence, particularly in the context of learning and knowledge dissemination. However, the utility of these models in specialized fields like Classics is still largely unexplored. This project is an attempt to merge the knowledge of Classics with the capabilities of artificial intelligence by finetuning an LLM to cater to the specific needs of learners and professionals. The goal of this project is to develop an LLM that not only reproduces contextual knowledge accurately but also exhibits a consistent \"personality\" - and, indeed, has consistent propriety - to appeal to a diverse audience who possess differing levels of knowledge. A significant portion of this project was dedicated to refining the dataset, following the principle of \"garbage in, garbage out,\" to ensure the model generates relevant, useful, and creative responses when given a prompt (a statement, question, or single word). After training and evaluation, my model's ability to handle a vast array of different types of inputs and prompting exceeded expectations for a 355M parameter model, though its occasional hallucinations (especially when set with a high temperature), particularly in its assertions about historical events or its own identity, make it seem somewhat capricious and more work in the form of continuous finetuning will be undertaken",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shane Storm Strachan"
    ]
  },
  "http://arxiv.org/abs/2312.07831": {
    "title": "Abusive Span Detection for Vietnamese Narrative Texts",
    "volume": "Dec",
    "abstract": "Abuse in its various forms, including physical, psychological, verbal, sexual, financial, and cultural, has a negative impact on mental health. However, there are limited studies on applying natural language processing (NLP) in this field in Vietnam. Therefore, we aim to contribute by building a human-annotated Vietnamese dataset for detecting abusive content in Vietnamese narrative texts. We sourced these texts from VnExpress, Vietnam's popular online newspaper, where readers often share stories containing abusive content. Identifying and categorizing abusive spans in these texts posed significant challenges during dataset creation, but it also motivated our research. We experimented with lightweight baseline models by freezing PhoBERT and XLM-RoBERTa and using their hidden states in a BiLSTM to assess the complexity of the dataset. According to our experimental results, PhoBERT outperforms other models in both labeled and unlabeled abusive span detection tasks. These results indicate that it has the potential for future improvements",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nhu-Thanh Nguyen",
      "Khoa Thi-Kim Phan",
      "Duc-Vu Nguyen",
      "Ngan Luu-Thuy Nguyen"
    ]
  },
  "http://arxiv.org/abs/2312.07824": {
    "title": "A Deep Learning-Based System for Automatic Case Summarization",
    "volume": "Dec",
    "abstract": "This paper presents a deep learning-based system for efficient automatic case summarization. Leveraging state-of-the-art natural language processing techniques, the system offers both supervised and unsupervised methods to generate concise and relevant summaries of lengthy legal case documents. The user-friendly interface allows users to browse the system's database of legal case documents, select their desired case, and choose their preferred summarization method. The system generates comprehensive summaries for each subsection of the legal text as well as an overall summary. This demo streamlines legal case document analysis, potentially benefiting legal professionals by reducing workload and increasing efficiency. Future work will focus on refining summarization techniques and exploring the application of our methods to other types of legal texts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minh Duong",
      "Long Nguyen",
      "Yen Vuong",
      "Trong Le",
      "Ha-Thanh Nguyen"
    ]
  },
  "http://arxiv.org/abs/2312.07819": {
    "title": "Native Language Identification with Large Language Models",
    "volume": "Dec",
    "abstract": "We present the first experiments on Native Language Identification (NLI) using LLMs such as GPT-4. NLI is the task of predicting a writer's first language by analyzing their writings in a second language, and is used in second language acquisition and forensic linguistics. Our results show that GPT models are proficient at NLI classification, with GPT-4 setting a new performance record of 91.7% on the benchmark TOEFL11 test set in a zero-shot setting. We also show that unlike previous fully-supervised settings, LLMs can perform NLI without being limited to a set of known classes, which has practical implications for real-world applications. Finally, we also show that LLMs can provide justification for their choices, providing reasoning based on spelling errors, syntactic patterns, and usage of directly translated linguistic patterns",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Zhang",
      "Alexandre Salle"
    ]
  },
  "http://arxiv.org/abs/2312.07797": {
    "title": "Sentiment analysis in Tourism: Fine-tuning BERT or sentence embeddings concatenation?",
    "volume": "Dec",
    "abstract": "Undoubtedly that the Bidirectional Encoder representations from Transformers is the most powerful technique in making Natural Language Processing tasks such as Named Entity Recognition, Question & Answers or Sentiment Analysis, however, the use of traditional techniques remains a major potential for the improvement of recent models, in particular word tokenization techniques and embeddings, but also the improvement of neural network architectures which are now the core of each architecture. recent. In this paper, we conduct a comparative study between Fine-Tuning the Bidirectional Encoder Representations from Transformers and a method of concatenating two embeddings to boost the performance of a stacked Bidirectional Long Short-Term Memory-Bidirectional Gated Recurrent Units model; these two approaches are applied in the context of sentiment analysis of shopping places in Morocco. A search for the best learning rate was made at the level of the two approaches, and a comparison of the best optimizers was made for each sentence embedding combination with regard to the second approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ibrahim Bouabdallaoui",
      "Fatima Guerouate",
      "Samya Bouhaddour",
      "Chaimae Saadi",
      "Mohammed Sbihi"
    ]
  },
  "http://arxiv.org/abs/2312.07796": {
    "title": "Harnessing Retrieval-Augmented Generation (RAG) for Uncovering Knowledge Gaps",
    "volume": "Dec",
    "abstract": "The paper presents a methodology for uncovering knowledge gaps on the internet using the Retrieval Augmented Generation (RAG) model. By simulating user search behaviour, the RAG system identifies and addresses gaps in information retrieval systems. The study demonstrates the effectiveness of the RAG system in generating relevant suggestions with a consistent accuracy of 93%. The methodology can be applied in various fields such as scientific discovery, educational enhancement, research development, market analysis, search engine optimisation, and content development. The results highlight the value of identifying and understanding knowledge gaps to guide future endeavours",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joan Figuerola Hurtado"
    ]
  },
  "http://arxiv.org/abs/2312.07763": {
    "title": "Can LLM find the green circle? Investigation and Human-guided tool manipulation for compositional generalization",
    "volume": "Dec",
    "abstract": "The meaning of complex phrases in natural language is composed of their individual components. The task of compositional generalization evaluates a model's ability to understand new combinations of components. Previous studies trained smaller, task-specific models, which exhibited poor generalization. While large language models (LLMs) exhibit impressive generalization abilities on many tasks through in-context learning (ICL), their potential for compositional generalization remains unexplored. In this paper, we first empirically investigate prevailing ICL methods in compositional generalization. We find that they struggle with complex compositional questions due to cumulative errors in long reasoning steps and intricate logic required for tool-making. Consequently, we propose a human-guided tool manipulation framework (HTM) that generates tools for sub-questions and integrates multiple tools. Our method enhances the effectiveness of tool creation and usage with minimal human effort. Experiments show that our method achieves state-of-the-art performance on two compositional generalization benchmarks and outperforms existing methods on the most challenging test split by 70%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Min Zhang",
      "Jianfeng He",
      "Shuo Lei",
      "Murong Yue",
      "Linhang Wang",
      "Chang-Tien Lu"
    ]
  },
  "http://arxiv.org/abs/2312.07743": {
    "title": "FULL-W2V: Fully Exploiting Data Reuse for W2V on GPU-Accelerated Systems",
    "volume": "Dec",
    "abstract": "Word2Vec remains one of the highly-impactful innovations in the field of Natural Language Processing (NLP) that represents latent grammatical and syntactical information in human text with dense vectors in a low dimension. Word2Vec has high computational cost due to the algorithm's inherent sequentiality, intensive memory accesses, and the large vocabularies it represents. While prior studies have investigated technologies to explore parallelism and improve memory system performance, they struggle to effectively gain throughput on powerful GPUs. We identify memory data access and latency as the primary bottleneck in prior works on GPUs, which prevents highly optimized kernels from attaining the architecture's peak performance. We present a novel algorithm, FULL-W2V, which maximally exploits the opportunities for data reuse in the W2V algorithm and leverages GPU architecture and resources to reduce access to low memory levels and improve temporal locality. FULL-W2V is capable of reducing accesses to GPU global memory significantly, e.g., by more than 89\\%, compared to prior state-of-the-art GPU implementations, resulting in significant performance improvement that scales across successive hardware generations. Our prototype implementation achieves 2.97X speedup when ported from Nvidia Pascal P100 to Volta V100 cards, and outperforms the state-of-the-art by 5.72X on V100 cards with the same embedding quality. In-depth analysis indicates that the reduction of memory accesses through register and shared memory caching and high-throughput shared memory reduction leads to a significantly improved arithmetic intensity. FULL-W2V can potentially benefit many applications in NLP and other domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Randall",
      "Tyler Allen",
      "Rong Ge"
    ]
  },
  "http://arxiv.org/abs/2312.07661": {
    "title": "CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor",
    "volume": "Dec",
    "abstract": "Existing open-vocabulary image segmentation methods require a fine-tuning step on mask annotations and/or image-text datasets. Mask labels are labor-intensive, which limits the number of categories in segmentation datasets. As a result, the open-vocabulary capacity of pre-trained VLMs is severely reduced after fine-tuning. However, without fine-tuning, VLMs trained under weak image-text supervision tend to make suboptimal mask predictions when there are text queries referring to non-existing concepts in the image. To alleviate these issues, we introduce a novel recurrent framework that progressively filters out irrelevant texts and enhances mask quality without training efforts. The recurrent unit is a two-stage segmenter built upon a VLM with frozen weights. Thus, our model retains the VLM's broad vocabulary space and strengthens its segmentation capability. Experimental results show that our method outperforms not only the training-free counterparts, but also those fine-tuned with millions of additional data samples, and sets new state-of-the-art records for both zero-shot semantic and referring image segmentation tasks. Specifically, we improve the current record by 28.8, 16.0, and 6.9 mIoU on Pascal VOC, COCO Object, and Pascal Context",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyang Sun",
      "Runjia Li",
      "Philip Torr",
      "Xiuye Gu",
      "Siyang Li"
    ]
  },
  "http://arxiv.org/abs/2312.07540": {
    "title": "diff History for Long-Context Language Agents",
    "volume": "Dec",
    "abstract": "Language Models (LMs) offer an exciting solution for general-purpose embodied control. However, a key technical issue arises when using an LM-based controller: environment observations must be converted to text, which coupled with history, leads to prohibitively large textual prompts. As a result, prior work in LM agents is limited to restricted domains with either small observation size or minimal needs for interaction history. In this paper, we introduce a simple and highly effective solution to these issues. We exploit the fact that consecutive text observations have high similarity and propose to compress them via the Unix diff command. We demonstrate our approach in NetHack, a complex rogue-like video game, that requires long-horizon reasoning for decision-making and is far from solved, particularly for neural agents. Diff history offers an average of 4x increase in the length of the text-based interaction history available to the LM. This observational compression along with the benefits of abstraction yields a 7x improvement in game score on held-out environment instances over state-of-the-art baselines. It also outperforms prior agents that use visual observations by over 40%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ulyana Piterbarg",
      "Lerrel Pinto",
      "Rob Fergus"
    ]
  },
  "http://arxiv.org/abs/2312.07532": {
    "title": "Interfacing Foundation Models' Embeddings",
    "volume": "Dec",
    "abstract": "We present FIND, a generalized interface for aligning foundation models' embeddings. As shown in teaser figure, a lightweight transformer interface without tuning any foundation model weights is enough for a unified image (segmentation) and dataset-level (retrieval) understanding. The proposed interface has the following favorable attributes: (1) Generalizable. It applies to various tasks spanning retrieval, segmentation, \\textit{etc.}, under the same architecture and weights. (2) Prototypable. Different tasks are able to be implemented through prototyping attention masks and embedding types. (3) Extendable. The proposed interface is adaptive to new tasks, and new models. (4) Interleavable. With the benefit of multi-task multi-modal training, the proposed interface creates an interleaved shared embedding space. In light of the interleaved embedding space, we introduce the FIND-Bench, which introduces new training and evaluation annotations to the COCO dataset for interleave segmentation and retrieval. Our approach achieves state-of-the-art performance on FIND-Bench and competitive performance on standard retrieval and segmentation settings. The training, evaluation, and demo code as well as the dataset have been released at https://github.com/UX-Decoder/FIND",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueyan Zou",
      "Linjie Li",
      "Jianfeng Wang",
      "Jianwei Yang",
      "Mingyu Ding",
      "Zhengyuan Yang",
      "Feng Li",
      "Hao Zhang",
      "Shilong Liu",
      "Arul Aravinthan",
      "Yong Jae Lee",
      "Lijuan Wang"
    ]
  },
  "http://arxiv.org/abs/2312.07527": {
    "title": "BaRDa: A Belief and Reasoning Dataset that Separates Factual Accuracy and Reasoning Ability",
    "volume": "Dec",
    "abstract": "While there are numerous benchmarks comparing the performance of modern language models (LMs), end-task evaluations often conflate notions of *factual accuracy* (\"truth\") and *reasoning ability* (\"rationality\", or \"honesty\" in the sense of correctly reporting implications of beliefs). Our goal is a dataset that clearly distinguishes these two notions. Our approach is to leverage and extend a collection of human-annotated *entailment trees*, engineered to express both good and bad chains of reasoning, and using a mixture of true and false facts, in particular including counterfactual examples, to avoid belief bias (also known as the \"content effect\"). The resulting dataset, called BaRDa, contains 3000 entailments (1787 valid, 1213 invalid), using 6681 true and 2319 false statements. Testing on four GPT-series models, GPT3(curie)/GPT3(davinici)/3.5/4, we find factual accuracy (truth) scores of 74.1/80.6/82.6/87.1 and reasoning accuracy scores of 63.1/78.0/71.8/79.2. This shows the clear progression of models towards improved factual accuracy and entailment reasoning, and the dataset provides a new benchmark that more cleanly separates and quantifies these two notions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Clark",
      "Bhavana Dalvi Mishra",
      "Oyvind Tafjord"
    ]
  },
  "http://arxiv.org/abs/2312.07492": {
    "title": "SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models",
    "volume": "Dec",
    "abstract": "Current datasets for unwanted social bias auditing are limited to studying protected demographic features such as race and gender. In this work, we introduce a comprehensive benchmark that is meant to capture the amplification of social bias, via stigmas, in generative language models. We start with a comprehensive list of 93 stigmas documented in social science literature and curate a question-answering (QA) dataset which involves simple social situations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts, with a variety of prompt styles, carefully constructed to systematically test for both social bias and model robustness. We present results for SocialStigmaQA with two widely used open source generative language models and we demonstrate that the output generated by these models considerably amplifies existing social bias against stigmatized groups. Specifically, we find that the proportion of socially biased output ranges from 45% to 59% across a variety of decoding strategies and prompting styles. We discover that the deliberate design of the templates in our benchmark (e.g., by adding biasing text to the prompt or varying the answer that indicates bias) impact the model tendencies to generate socially biased output. Additionally, we report on patterns in the generated chain-of-thought output, finding a variety of problems from subtle bias to evidence of a lack of reasoning. Warning: This paper contains examples of text which is toxic, biased, and harmful",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manish Nagireddy",
      "Lamogha Chiazor",
      "Moninder Singh",
      "Ioana Baldini"
    ]
  },
  "http://arxiv.org/abs/2312.07476": {
    "title": "Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection",
    "volume": "Dec",
    "abstract": "In-Context Learning (ICL) is an important paradigm for adapting Large Language Models (LLMs) to downstream tasks through a few demonstrations. Despite the great success of ICL, the limitation of the demonstration number may lead to demonstration bias, i.e. the input-label mapping induced by LLMs misunderstands the task's essence. Inspired by human experience, we attempt to mitigate such bias through the perspective of the inter-demonstration relationship. Specifically, we construct Comparable Demonstrations (CDs) by minimally editing the texts to flip the corresponding labels, in order to highlight the task's essence and eliminate potential spurious correlations through the inter-demonstration comparison. Through a series of experiments on CDs, we find that (1) demonstration bias does exist in LLMs, and CDs can significantly reduce such bias; (2) CDs exhibit good performance in ICL, especially in out-of-distribution scenarios. In summary, this study explores the ICL mechanisms from a novel perspective, providing a deeper insight into the demonstration selection strategy for ICL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caoyun Fan",
      "Jidong Tian",
      "Yitian Li",
      "Hao He",
      "Yaohui Jin"
    ]
  },
  "http://arxiv.org/abs/2312.07435": {
    "title": "Cross-modal Contrastive Learning with Asymmetric Co-attention Network for Video Moment Retrieval",
    "volume": "Dec",
    "abstract": "Video moment retrieval is a challenging task requiring fine-grained interactions between video and text modalities. Recent work in image-text pretraining has demonstrated that most existing pretrained models suffer from information asymmetry due to the difference in length between visual and textual sequences. We question whether the same problem also exists in the video-text domain with an auxiliary need to preserve both spatial and temporal information. Thus, we evaluate a recently proposed solution involving the addition of an asymmetric co-attention network for video grounding tasks. Additionally, we incorporate momentum contrastive loss for robust, discriminative representation learning in both modalities. We note that the integration of these supplementary modules yields better performance compared to state-of-the-art models on the TACoS dataset and comparable results on ActivityNet Captions, all while utilizing significantly fewer parameters with respect to baseline",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Love Panta",
      "Prashant Shrestha",
      "Brabeem Sapkota",
      "Amrita Bhattarai",
      "Suresh Manandhar",
      "Anand Kumar Sah"
    ]
  },
  "http://arxiv.org/abs/2312.07419": {
    "title": "Towards Faster k-Nearest-Neighbor Machine Translation",
    "volume": "Dec",
    "abstract": "Recent works have proven the effectiveness of k-nearest-neighbor machine translation(a.k.a kNN-MT) approaches to produce remarkable improvement in cross-domain translations. However, these models suffer from heavy retrieve overhead on the entire datastore when decoding each token. We observe that during the decoding phase, about 67% to 84% of tokens are unvaried after searching over the corpus datastore, which means most of the tokens cause futile retrievals and introduce unnecessary computational costs by initiating k-nearest-neighbor searches. We consider this phenomenon is explainable in linguistics and propose a simple yet effective multi-layer perceptron (MLP) network to predict whether a token should be translated jointly by the neural machine translation model and probabilities produced by the kNN or just by the neural model. The results show that our method succeeds in reducing redundant retrieval operations and significantly reduces the overhead of kNN retrievals by up to 53% at the expense of a slight decline in translation quality. Moreover, our method could work together with all existing kNN-MT systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Shi",
      "Yunlong Liang",
      "Jinan Xu",
      "Yufeng Chen"
    ]
  },
  "http://arxiv.org/abs/2312.07405": {
    "title": "ICL Markup: Structuring In-Context Learning using Soft-Token Tags",
    "volume": "Dec",
    "abstract": "Large pretrained language models (LLMs) can be rapidly adapted to a wide variety of tasks via a text-to-text approach, where the instruction and input are fed to the model in natural language. Combined with in-context learning (ICL), this paradigm is impressively flexible and powerful. However, it also burdens users with an overwhelming number of choices, many of them arbitrary. Inspired by markup languages like HTML, we contribute a method of using soft-token tags to compose prompt templates. This approach reduces arbitrary decisions and streamlines the application of ICL. Our method is a form of meta-learning for ICL; it learns these tags in advance during a parameter-efficient fine-tuning ``warm-up'' process. The tags can subsequently be used in templates for ICL on new, unseen tasks without any additional fine-tuning. Our experiments with this approach yield promising initial results, improving LLM performance on important enterprise applications such as few-shot and open-world intent detection, as well as text classification in news and legal domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marc-Etienne Brunet",
      "Ashton Anderson",
      "Richard Zemel"
    ]
  },
  "http://arxiv.org/abs/2312.07399": {
    "title": "Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales",
    "volume": "Dec",
    "abstract": "Machine reasoning has made great progress in recent years owing to large language models (LLMs). In the clinical domain, however, most NLP-driven projects mainly focus on clinical classification or reading comprehension, and under-explore clinical reasoning for disease diagnosis due to the expensive rationale annotation with clinicians. In this work, we present a ``reasoning-aware'' diagnosis framework that rationalizes the diagnostic process via prompt-based learning in a time- and labor-efficient manner, and learns to reason over the prompt-generated rationales. Specifically, we address the clinical reasoning for disease diagnosis, where the LLM generates diagnostic rationales providing its insight on presented patient data and the reasoning path towards the diagnosis, namely Clinical Chain-of-Thought (Clinical CoT). We empirically demonstrate LLMs/LMs' ability of clinical reasoning via extensive experiments and analyses on both rationale generation and disease diagnosis in various settings. We further propose a novel set of criteria for evaluating machine-generated rationales' potential for real-world clinical settings, facilitating and benefiting future research in this area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taeyoon Kwon",
      "Kai Tzu-iunn Ong",
      "Dongjin Kang",
      "Seungjun Moon",
      "Jeong Ryong Lee",
      "Dosik Hwang",
      "Yongsik Sim",
      "Beomseok Sohn",
      "Dongha Lee",
      "Jinyoung Yeo"
    ]
  },
  "http://arxiv.org/abs/2312.07398": {
    "title": "LLMEval: A Preliminary Study on How to Evaluate Large Language Models",
    "volume": "Dec",
    "abstract": "Recently, the evaluation of Large Language Models has emerged as a popular area of research. The three crucial questions for LLM evaluation are ``what, where, and how to evaluate''. However, the existing research mainly focuses on the first two questions, which are basically what tasks to give the LLM during testing and what kind of knowledge it should deal with. As for the third question, which is about what standards to use, the types of evaluators, how to score, and how to rank, there hasn't been much discussion. In this paper, we analyze evaluation methods by comparing various criteria with both manual and automatic evaluation, utilizing onsite, crowd-sourcing, public annotators and GPT-4, with different scoring methods and ranking systems. We propose a new dataset, LLMEval and conduct evaluations on 20 LLMs. A total of 2,186 individuals participated, leading to the generation of 243,337 manual annotations and 57,511 automatic evaluation results. We perform comparisons and analyses of different settings and conduct 10 conclusions that can provide some insights for evaluating LLM in the future. The dataset and the results are publicly available at https://github.com/llmeval",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Zhang",
      "Ming Zhang",
      "Haipeng Yuan",
      "Shichun Liu",
      "Yongyao Shi",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "http://arxiv.org/abs/2312.07395": {
    "title": "A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames",
    "volume": "Dec",
    "abstract": "Understanding long, real-world videos requires modeling of long-range visual dependencies. To this end, we explore video-first architectures, building on the common paradigm of transferring large-scale, image--text models to video via shallow temporal fusion. However, we expose two limitations to the approach: (1) decreased spatial capabilities, likely due to poor video--language alignment in standard video datasets, and (2) higher memory consumption, bottlenecking the number of frames that can be processed. To mitigate the memory bottleneck, we systematically analyze the memory/accuracy trade-off of various efficient methods: factorized attention, parameter-efficient image-to-video adaptation, input masking, and multi-resolution patchification. Surprisingly, simply masking large portions of the video (up to 75%) during contrastive pre-training proves to be one of the most robust ways to scale encoders to videos up to 4.3 minutes at 1 FPS. Our simple approach for training long video-to-text models, which scales to 1B parameters, does not add new architectural complexity and is able to outperform the popular paradigm of using much larger LLMs as an information aggregator over segment-based information on benchmarks with long-range temporal dependencies (YouCook2, EgoSchema)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pinelopi Papalampidi",
      "Skanda Koppula",
      "Shreya Pathak",
      "Justin Chiu",
      "Joe Heyward",
      "Viorica Patraucean",
      "Jiajun Shen",
      "Antoine Miech",
      "Andrew Zisserman",
      "Aida Nematzdeh"
    ]
  },
  "http://arxiv.org/abs/2312.07338": {
    "title": "Self-supervised Adaptive Pre-training of Multilingual Speech Models for Language and Dialect Identification",
    "volume": "Dec",
    "abstract": "Pre-trained Transformer-based speech models have shown striking performance when fine-tuned on various downstream tasks such as automatic speech recognition and spoken language identification (SLID). However, the problem of domain mismatch remains a challenge in this area, where the domain of the pre-training data might differ from that of the downstream labeled data used for fine-tuning. In multilingual tasks such as SLID, the pre-trained speech model may not support all the languages in the downstream task. To address this challenge, we propose self-supervised adaptive pre-training (SAPT) to adapt the pre-trained model to the target domain and languages of the downstream task. We apply SAPT to the XLSR-128 model and investigate the effectiveness of this approach for the SLID task. First, we demonstrate that SAPT improves XLSR performance on the FLEURS benchmark with substantial gains up to 40.1% for under-represented languages. Second, we apply SAPT on four different datasets in a few-shot learning setting, showing that our approach improves the sample efficiency of XLSR during fine-tuning. Our experiments provide strong empirical evidence that continual adaptation via self-supervision improves downstream performance for multilingual speech models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammed Maqsood Shaik",
      "Dietrich Klakow",
      "Badr M. Abdullah"
    ]
  },
  "http://arxiv.org/abs/2312.07305": {
    "title": "SCCA: Shifted Cross Chunk Attention for long contextual semantic expansion",
    "volume": "Dec",
    "abstract": "Sparse attention as a efficient method can significantly decrease the computation cost, but current sparse attention tend to rely on window self attention which block the global information flow. For this problem, we present Shifted Cross Chunk Attention (SCCA), using different KV shifting strategy to extend respective field in each attention layer. Except, we combine Dilated Attention(DA) and Dilated Neighborhood Attention(DNA) to present Shifted Dilated Attention(SDA). Both SCCA and SDA can accumulate attention results in multi head attention to obtain approximate respective field in full attention. In this paper, we conduct language modeling experiments using different pattern of SCCA and combination of SCCA and SDA. The proposed shifted cross chunk attention (SCCA) can effectively extend large language models (LLMs) to longer context combined with Positional interpolation(PI) and LoRA than current sparse attention. Notably, SCCA adopts LLaMA2 7B from 4k context to 8k in single V100. This attention pattern can provide a Plug-and-play fine-tuning method to extend model context while retaining their original architectures, and is compatible with most existing techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiang Guo"
    ]
  },
  "http://arxiv.org/abs/2312.07280": {
    "title": "Towards Equipping Transformer with the Ability of Systematic Compositionality",
    "volume": "Dec",
    "abstract": "One of the key factors in language productivity and human cognition is the ability of systematic compositionality, which refers to understanding composed unseen examples of seen primitives. However, recent evidence reveals that the Transformers have difficulty generalizing the composed context based on the seen primitives. To this end, we take the first step to propose a compositionality-aware Transformer called CAT and two novel pre-training tasks to facilitate systematic compositionality. We tentatively provide a successful implementation of a multi-layer CAT on the basis of the especially popular BERT. The experimental results demonstrate that CAT outperforms baselines on compositionality-aware tasks with minimal impact on the effectiveness on standardized language understanding tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Huang",
      "Peixin Qin",
      "Wenqiang Lei",
      "Jiancheng Lv"
    ]
  },
  "http://arxiv.org/abs/2312.07255": {
    "title": "GIST: Improving Parameter Efficient Fine Tuning via Knowledge Interaction",
    "volume": "Dec",
    "abstract": "The Parameter-Efficient Fine-Tuning (PEFT) method, which adjusts or introduces fewer trainable parameters to calibrate pre-trained models on downstream tasks, has become a recent research interest. However, existing PEFT methods within the traditional fine-tiuning framework have two main shortcomings: 1) They overlook the explicit association between trainable parameters and downstream task knowledge. 2) They neglect the interaction between the intrinsic task-agnostic knowledge of pre-trained models and the task-specific knowledge in downstream tasks. To address this gap, we propose a novel fine-tuning framework, named GIST, in a plug-and-play manner. Specifically, our framework first introduces a trainable token, called the Gist token, when applying PEFT methods on downstream tasks. This token serves as an aggregator of the task-specific knowledge learned by the PEFT methods and forms an explicit association with downstream knowledge. Furthermore, to facilitate explicit interaction between task-agnostic and task-specific knowledge, we introduce the concept of Knowledge Interaction via a Bidirectional Kullback-Leibler Divergence objective. As a result, PEFT methods within our framework can make the pre-trained model understand downstream tasks more comprehensively by leveraging the knowledge interaction. Extensive experiments demonstrate the universality and scalability of our framework. Notably, on the VTAB-1K benchmark, we employ the Adapter (a prevalent PEFT method) within our GIST framework and achieve a performance boost of 2.25%, with an increase of only 0.8K parameters. The Code will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacheng Ruan",
      "Jingsheng Gao",
      "Mingye Xie",
      "Suncheng Xiang",
      "Zefang Yu",
      "Ting Liu",
      "Yuzhuo Fu"
    ]
  },
  "http://arxiv.org/abs/2312.07254": {
    "title": "The GUA-Speech System Description for CNVSRC Challenge 2023",
    "volume": "Dec",
    "abstract": "This study describes our system for Task 1 Single-speaker Visual Speech Recognition (VSR) fixed track in the Chinese Continuous Visual Speech Recognition Challenge (CNVSRC) 2023. Specifically, we use intermediate connectionist temporal classification (Inter CTC) residual modules to relax the conditional independence assumption of CTC in our model. Then we use a bi-transformer decoder to enable the model to capture both past and future contextual information. In addition, we use Chinese characters as the modeling units to improve the recognition accuracy of our model. Finally, we use a recurrent neural network language model (RNNLM) for shallow fusion in the inference stage. Experiments show that our system achieves a character error rate (CER) of 38.09% on the Eval set which reaches a relative CER reduction of 21.63% over the official baseline, and obtains a second place in the challenge",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengqiang Li",
      "Chao Lei",
      "Baozhong Ma",
      "Binbin Zhang",
      "Fuping Pan"
    ]
  },
  "http://arxiv.org/abs/2312.07250": {
    "title": "Neural Machine Translation of Clinical Text: An Empirical Investigation into Multilingual Pre-Trained Language Models and Transfer-Learning",
    "volume": "Dec",
    "abstract": "We conduct investigations on clinical text machine translation by examining multilingual neural network models using deep learning such as Transformer based structures. Furthermore, to address the language resource imbalance issue, we also carry out experiments using a transfer learning methodology based on massive multilingual pre-trained language models (MMPLMs). The experimental results on three subtasks including 1) clinical case (CC), 2) clinical terminology (CT), and 3) ontological concept (OC) show that our models achieved top-level performances in the ClinSpEn-2022 shared task on English-Spanish clinical domain data. Furthermore, our expert-based human evaluations demonstrate that the small-sized pre-trained language model (PLM) won over the other two extra-large language models by a large margin, in the clinical domain fine-tuning, which finding was never reported in the field. Finally, the transfer learning method works well in our experimental setting using the WMT21fb model to accommodate a new language space Spanish that was not seen at the pre-training stage within WMT21fb itself, which deserves more exploitation for clinical knowledge transformation, e.g. to investigate into more languages. These research findings can shed some light on domain-specific machine translation development, especially in clinical and healthcare fields. Further research projects can be carried out based on our work to improve healthcare text analytics and knowledge transformation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lifeng Han",
      "Serge Gladkoff",
      "Gleb Erofeev",
      "Irina Sorokina",
      "Betty Galiano",
      "Goran Nenadic"
    ]
  },
  "http://arxiv.org/abs/2312.07228": {
    "title": "Toxic language detection: a systematic survey of Arabic datasets",
    "volume": "Dec",
    "abstract": "This paper offers a comprehensive survey of Arabic datasets focused on online toxic language. We systematically gathered a total of 49 available datasets and their corresponding papers and conducted a thorough analysis, considering 16 criteria across three primary dimensions: content, annotation process, and reusability. This analysis enabled us to identify existing gaps and make recommendations for future research works",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Imene Bensalem",
      "Paolo Rosso",
      "Hanane Zitouni"
    ]
  },
  "http://arxiv.org/abs/2312.08388": {
    "title": "Exploring Graph Based Approaches for Author Name Disambiguation",
    "volume": "Dec",
    "abstract": "In many applications, such as scientific literature management, researcher search, social network analysis and etc, Name Disambiguation (aiming at disambiguating WhoIsWho) has been a challenging problem. In addition, the growth of scientific literature makes the problem more difficult and urgent. Although name disambiguation has been extensively studied in academia and industry, the problem has not been solved well due to the clutter of data and the complexity of the same name scenario. In this work, we aim to explore models that can perform the task of name disambiguation using the network structure that is intrinsic to the problem and present an analysis of the models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chetanya Rastogi",
      "Prabhat Agarwal",
      "Shreya Singh"
    ]
  },
  "http://arxiv.org/abs/2312.07194": {
    "title": "Verbreitungsmechanismen schädigender Sprache im Netz: Anatomie zweier Shitstorms",
    "volume": "Dec",
    "abstract": "In this working paper, we turn our attention to two exemplary, cross-media shitstorms directed against well-known individuals from the business world. Both have in common, first, the trigger, a controversial statement by the person who thereby becomes the target of the shitstorm, and second, the identity of this target as relatively privileged: cis-male, white, successful. We examine the spread of the outrage wave across two media at a time and test the applicability of computational linguistic methods for analyzing its time course. Assuming that harmful language spreads like a virus in digital space, we are primarily interested in the events and constellations that lead to the use of harmful language, and whether and how a linguistic formation of \"tribes\" occurs. Our research therefore focuses, first, on the distribution of linguistic features within the overall shitstorm: are individual words or phrases increasingly used after their introduction, and through which pathways they spread. Second, we ask whether \"tribes,\" for example, one group of supporters and one of opponents of the target, have a distinguished linguistic form. Our hypothesis is that supporters remain equally active over time, while the dynamic \"ripple\" effect of the shitstorm is based on the varying participation of opponents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tatjana Scheffler",
      "Veronika Solopova",
      "Mihaela Popa-Wyatt"
    ]
  },
  "http://arxiv.org/abs/2312.07182": {
    "title": "Classifying complex documents: comparing bespoke solutions to large language models",
    "volume": "Dec",
    "abstract": "Here we search for the best automated classification approach for a set of complex legal documents. Our classification task is not trivial: our aim is to classify ca 30,000 public courthouse records from 12 states and 267 counties at two different levels using nine sub-categories. Specifically, we investigated whether a fine-tuned large language model (LLM) can achieve the accuracy of a bespoke custom-trained model, and what is the amount of fine-tuning necessary",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Glen Hopkins",
      "Kristjan Kalm"
    ]
  },
  "http://arxiv.org/abs/2312.07141": {
    "title": "Multilingual large language models leak human stereotypes across language boundaries",
    "volume": "Dec",
    "abstract": "Multilingual large language models have been increasingly popular for their proficiency in comprehending and generating text across various languages. Previous research has shown that the presence of stereotypes and biases in monolingual large language models can be attributed to the nature of their training data, which is collected from humans and reflects societal biases. Multilingual language models undergo the same training procedure as monolingual ones, albeit with training data sourced from various languages. This raises the question: do stereotypes present in one social context leak across languages within the model? In our work, we first define the term ``stereotype leakage'' and propose a framework for its measurement. With this framework, we investigate how stereotypical associations leak across four languages: English, Russian, Chinese, and Hindi. To quantify the stereotype leakage, we employ an approach from social psychology, measuring stereotypes via group-trait associations. We evaluate human stereotypes and stereotypical associations manifested in multilingual large language models such as mBERT, mT5, and ChatGPT. Our findings show a noticeable leakage of positive, negative, and non-polar associations across all languages. Notably, Hindi within multilingual models appears to be the most susceptible to influence from other languages, while Chinese is the least. Additionally, ChatGPT exhibits a better alignment with human scores than other models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Trista Cao",
      "Anna Sotnikova",
      "Jieyu Zhao",
      "Linda X. Zou",
      "Rachel Rudinger",
      "Hal Daume III"
    ]
  },
  "http://arxiv.org/abs/2312.07110": {
    "title": "LLMs Perform Poorly at Concept Extraction in Cyber-security Research Literature",
    "volume": "Dec",
    "abstract": "The cybersecurity landscape evolves rapidly and poses threats to organizations. To enhance resilience, one needs to track the latest developments and trends in the domain. It has been demonstrated that standard bibliometrics approaches show their limits in such a fast-evolving domain. For this purpose, we use large language models (LLMs) to extract relevant knowledge entities from cybersecurity-related texts. We use a subset of arXiv preprints on cybersecurity as our data and compare different LLMs in terms of entity recognition (ER) and relevance. The results suggest that LLMs do not produce good knowledge entities that reflect the cybersecurity context, but our results show some potential for noun extractors. For this reason, we developed a noun extractor boosted with some statistical analysis to extract specific and relevant compound nouns from the domain. Later, we tested our model to identify trends in the LLM domain. We observe some limitations, but it offers promising results to monitor the evolution of emergent trends",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxime Würsch",
      "Andrei Kucharavy",
      "Dimitri Percia David",
      "Alain Mermoud"
    ]
  },
  "http://arxiv.org/abs/2312.07088": {
    "title": "BED: Bi-Encoder-Decoder Model for Canonical Relation Extraction",
    "volume": "Dec",
    "abstract": "Canonical relation extraction aims to extract relational triples from sentences, where the triple elements (entity pairs and their relationship) are mapped to the knowledge base. Recently, methods based on the encoder-decoder architecture are proposed and achieve promising results. However, these methods cannot well utilize the entity information, which is merely used as augmented training data. Moreover, they are incapable of representing novel entities, since no embeddings have been learned for them. In this paper, we propose a novel framework, Bi-Encoder-Decoder (BED), to solve the above issues. Specifically, to fully utilize entity information, we employ an encoder to encode semantics of this information, leading to high-quality entity representations. For novel entities, given a trained entity encoder, their representations can be easily generated. Experimental results on two datasets show that, our method achieves a significant performance improvement over the previous state-of-the-art and handle novel entities well without retraining",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nantao Zheng",
      "Siyu Long",
      "Xinyu Dai"
    ]
  },
  "http://arxiv.org/abs/2312.07069": {
    "title": "Context Matter: Data-Efficient Augmentation of Large Language Models for Scientific Applications",
    "volume": "Dec",
    "abstract": "In this paper, we explore the challenges inherent to Large Language Models (LLMs) like GPT-4, particularly their propensity for hallucinations, logic mistakes, and incorrect conclusions when tasked with answering complex questions. The capacity of LLMs to present erroneous answers in a coherent and semantically rigorous manner further complicates the detection of factual inaccuracies. This issue is especially pronounced in fields that require specialized expertise. Our work delves into these challenges, aiming to enhance the understanding and mitigation of such errors, thereby contributing to the improvement of LLM accuracy and reliability in scientific and other specialized domains. Our findings reveal a non-linear relationship between the context's relevancy and the answers' measured quality. In addition, we demonstrate that with the correct calibration, it is possible to automate the grading procedure -- a finding suggesting that, at least to some degree, the LLMs can be used to self-examine the quality of their own performance. Finally, we describe an experimental platform that can be seen as a proof-of-concept of the techniques described in this work",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Li",
      "Haoran Tang",
      "Siyu Chen",
      "Ziwei Wang",
      "Anurag Maravi",
      "Marcin Abram"
    ]
  },
  "http://arxiv.org/abs/2312.07066": {
    "title": "DiffuVST: Narrating Fictional Scenes with Global-History-Guided Denoising Models",
    "volume": "Dec",
    "abstract": "Recent advances in image and video creation, especially AI-based image synthesis, have led to the production of numerous visual scenes that exhibit a high level of abstractness and diversity. Consequently, Visual Storytelling (VST), a task that involves generating meaningful and coherent narratives from a collection of images, has become even more challenging and is increasingly desired beyond real-world imagery. While existing VST techniques, which typically use autoregressive decoders, have made significant progress, they suffer from low inference speed and are not well-suited for synthetic scenes. To this end, we propose a novel diffusion-based system DiffuVST, which models the generation of a series of visual descriptions as a single conditional denoising process. The stochastic and non-autoregressive nature of DiffuVST at inference time allows it to generate highly diverse narratives more efficiently. In addition, DiffuVST features a unique design with bi-directional text history guidance and multimodal adapter modules, which effectively improve inter-sentence coherence and image-to-text fidelity. Extensive experiments on the story generation task covering four fictional visual-story datasets demonstrate the superiority of DiffuVST over traditional autoregressive models in terms of both text quality and inference speed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengguang Wu",
      "Mei Yuan",
      "Qi Su"
    ]
  },
  "http://arxiv.org/abs/2312.07049": {
    "title": "Improving Factual Error Correction by Learning to Inject Factual Errors",
    "volume": "Dec",
    "abstract": "Factual error correction (FEC) aims to revise factual errors in false claims with minimal editing, making them faithful to the provided evidence. This task is crucial for alleviating the hallucination problem encountered by large language models. Given the lack of paired data (i.e., false claims and their corresponding correct claims), existing methods typically adopt the mask-then-correct paradigm. This paradigm relies solely on unpaired false claims and correct claims, thus being referred to as distantly supervised methods. These methods require a masker to explicitly identify factual errors within false claims before revising with a corrector. However, the absence of paired data to train the masker makes accurately pinpointing factual errors within claims challenging. To mitigate this, we propose to improve FEC by Learning to Inject Factual Errors (LIFE), a three-step distantly supervised method: mask-corrupt-correct. Specifically, we first train a corruptor using the mask-then-corrupt procedure, allowing it to deliberately introduce factual errors into correct text. The corruptor is then applied to correct claims, generating a substantial amount of paired data. After that, we filter out low-quality data, and use the remaining data to train a corrector. Notably, our corrector does not require a masker, thus circumventing the bottleneck associated with explicit factual error identification. Our experiments on a public dataset verify the effectiveness of LIFE in two key aspects: Firstly, it outperforms the previous best-performing distantly supervised method by a notable margin of 10.59 points in SARI Final (19.3% improvement). Secondly, even compared to ChatGPT prompted with in-context examples, LIFE achieves a superiority of 7.16 points in SARI Final",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingwei He",
      "Qianru Zhang",
      "A-Long Jin",
      "Jun Ma",
      "Yuan Yuan",
      "Siu Ming Yiu"
    ]
  },
  "http://arxiv.org/abs/2312.07046": {
    "title": "Rethinking Compression: Reduced Order Modelling of Latent Features in Large Language Models",
    "volume": "Dec",
    "abstract": "Due to the substantial scale of Large Language Models (LLMs), the direct application of conventional compression methodologies proves impractical. The computational demands associated with even minimal gradient updates present challenges, particularly on consumer-grade hardware. This paper introduces an innovative approach for the parametric and practical compression of LLMs based on reduced order modelling, which entails low-rank decomposition within the feature space and re-parameterization in the weight space. Notably, this compression technique operates in a layer-wise manner, obviating the need for a GPU device and enabling the compression of billion-scale models within stringent constraints of both memory and time. Our method represents a significant advancement in model compression by leveraging matrix decomposition, demonstrating superior efficacy compared to the prevailing state-of-the-art structured pruning method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arnav Chavan",
      "Nahush Lele",
      "Deepak Gupta"
    ]
  },
  "http://arxiv.org/abs/2312.07028": {
    "title": "Dynamic Corrective Self-Distillation for Better Fine-Tuning of Pretrained Models",
    "volume": "Dec",
    "abstract": "We tackle the challenging issue of aggressive fine-tuning encountered during the process of transfer learning of pre-trained language models (PLMs) with limited labeled downstream data. This problem primarily results in a decline in performance on the subsequent task. Inspired by the adaptive boosting method in traditional machine learning, we present an effective dynamic corrective self-distillation (DCS) approach to improve the fine-tuning of the PLMs. Our technique involves performing a self-distillation mechanism where, at each iteration, the student model actively adapts and corrects itself by dynamically adjusting the weights assigned to individual data points. This iterative self-correcting process significantly enhances the overall fine-tuning capability of PLMs, leading to improved performance and robustness. We conducted comprehensive evaluations using the GLUE benchmark demonstrating the efficacy of our method in enhancing the fine-tuning process for various PLMs across diverse downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ibtihel Amara",
      "Vinija Jain",
      "Aman Chadha"
    ]
  },
  "http://arxiv.org/abs/2312.07000": {
    "title": "Alignment for Honesty",
    "volume": "Dec",
    "abstract": "Recent research has made significant strides in applying alignment techniques to enhance the helpfulness and harmlessness of large language models (LLMs) in accordance with human intentions. In this paper, we argue for the importance of alignment for honesty, ensuring that LLMs proactively refuse to answer questions when they lack knowledge, while still not being overly conservative. However, a pivotal aspect of alignment for honesty involves discerning the limits of an LLM's knowledge, which is far from straightforward. This challenge demands comprehensive solutions in terms of metric development, benchmark creation, and training methodologies. In this paper, we address these challenges by first establishing a precise problem definition and defining ``honesty'' inspired by the Analects of Confucius. This serves as a cornerstone for developing metrics that effectively measure an LLM's honesty by quantifying its progress post-alignment. Furthermore, we introduce a flexible training framework which is further instantiated by several efficient fine-tuning techniques that emphasize honesty without sacrificing performance on other tasks. Our extensive experiments reveal that these aligned models show a marked increase in honesty, as indicated by our proposed metrics. We open-source a wealth of resources to facilitate future research at https://github.com/GAIR-NLP/alignment-for-honesty, including honesty-aligned models, training and evaluation datasets for honesty alignment, concept glossary, as well as all relevant source code",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqing Yang",
      "Ethan Chern",
      "Xipeng Qiu",
      "Graham Neubig",
      "Pengfei Liu"
    ]
  },
  "http://arxiv.org/abs/2312.06974": {
    "title": "SM70: A Large Language Model for Medical Devices",
    "volume": "Dec",
    "abstract": "We are introducing SM70, a 70 billion-parameter Large Language Model that is specifically designed for SpassMed's medical devices under the brand name 'JEE1' (pronounced as G1 and means 'Life'). This large language model provides more accurate and safe responses to medical-domain questions. To fine-tune SM70, we used around 800K data entries from the publicly available dataset MedAlpaca. The Llama2 70B open-sourced model served as the foundation for SM70, and we employed the QLoRA technique for fine-tuning. The evaluation is conducted across three benchmark datasets - MEDQA - USMLE, PUBMEDQA, and USMLE - each representing a unique aspect of medical knowledge and reasoning. The performance of SM70 is contrasted with other notable LLMs, including Llama2 70B, Clinical Camel 70 (CC70), GPT 3.5, GPT 4, and Med-Palm, to provide a comparative understanding of its capabilities within the medical domain. Our results indicate that SM70 outperforms several established models in these datasets, showcasing its proficiency in handling a range of medical queries, from fact-based questions derived from PubMed abstracts to complex clinical decision-making scenarios. The robust performance of SM70, particularly in the USMLE and PUBMEDQA datasets, suggests its potential as an effective tool in clinical decision support and medical information retrieval. Despite its promising results, the paper also acknowledges the areas where SM70 lags behind the most advanced model, GPT 4, thereby highlighting the need for further development, especially in tasks demanding extensive medical knowledge and intricate reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anubhav Bhatti",
      "Surajsinh Parmar",
      "San Lee"
    ]
  },
  "http://arxiv.org/abs/2312.06950": {
    "title": "READ-PVLA: Recurrent Adapter with Partial Video-Language Alignment for Parameter-Efficient Transfer Learning in Low-Resource Video-Language Modeling",
    "volume": "Dec",
    "abstract": "Fully fine-tuning pretrained large-scale transformer models has become a popular paradigm for video-language modeling tasks, such as temporal language grounding and video-language summarization. With a growing number of tasks and limited training data, such full fine-tuning approach leads to costly model storage and unstable training. To overcome these shortcomings, we introduce lightweight adapters to the pre-trained model and only update them at fine-tuning time. However, existing adapters fail to capture intrinsic temporal relations among video frames or textual words. Moreover, they neglect the preservation of critical task-related information that flows from the raw video-language input into the adapter's low-dimensional space. To address these issues, we first propose a novel REcurrent ADapter (READ) that employs recurrent computation to enable temporal modeling capability. Second, we propose Partial Video-Language Alignment (PVLA) objective via the use of partial optimal transport to maintain task-related information flowing into our READ modules. We validate our READ-PVLA framework through extensive experiments where READ-PVLA significantly outperforms all existing fine-tuning strategies on multiple low-resource temporal language grounding and video-language summarization benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thong Nguyen",
      "Xiaobao Wu",
      "Xinshuai Dong",
      "Khoi Le",
      "Zhiyuan Hu",
      "Cong-Duy Nguyen",
      "See-Kiong Ng",
      "Luu Anh Tuan"
    ]
  },
  "http://arxiv.org/abs/2312.06926": {
    "title": "Content-Localization based Neural Machine Translation for Informal Dialectal Arabic: Spanish/French to Levantine/Gulf Arabic",
    "volume": "Dec",
    "abstract": "Resources in high-resource languages have not been efficiently exploited in low-resource languages to solve language-dependent research problems. Spanish and French are considered high resource languages in which an adequate level of data resources for informal online social behavior modeling, is observed. However, a machine translation system to access those data resources and transfer their context and tone to a low-resource language like dialectal Arabic, does not exist. In response, we propose a framework that localizes contents of high-resource languages to a low-resource language/dialects by utilizing AI power. To the best of our knowledge, we are the first work to provide a parallel translation dataset from/to informal Spanish and French to/from informal Arabic dialects. Using this, we aim to enrich the under-resource-status dialectal Arabic and fast-track the research of diverse online social behaviors within and across smart cities in different geo-regions. The experimental results have illustrated the capability of our proposed solution in exploiting the resources between high and low resource languages and dialects. Not only this, but it has also been proven that ignoring dialects within the same language could lead to misleading analysis of online social behavior",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fatimah Alzamzami",
      "Abdulmotaleb El Saddik"
    ]
  },
  "http://arxiv.org/abs/2312.06924": {
    "title": "Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack",
    "volume": "Dec",
    "abstract": "Recent developments in balancing the usefulness and safety of Large Language Models (LLMs) have raised a critical question: Are mainstream NLP tasks adequately aligned with safety consideration? Our study, focusing on safety-sensitive documents obtained through adversarial attacks, reveals significant disparities in the safety alignment of various NLP tasks. For instance, LLMs can effectively summarize malicious long documents but often refuse to translate them. This discrepancy highlights a previously unidentified vulnerability: attacks exploiting tasks with weaker safety alignment, like summarization, can potentially compromise the integraty of tasks traditionally deemed more robust, such as translation and question-answering (QA). Moreover, the concurrent use of multiple NLP tasks with lesser safety alignment increases the risk of LLMs inadvertently processing harmful content. We demonstrate these vulnerabilities in various safety-aligned LLMs, particularly Llama2 models and GPT-4, indicating an urgent need for strengthening safety alignments across a broad spectrum of NLP tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Fu",
      "Yufei Li",
      "Wen Xiao",
      "Cong Liu",
      "Yue Dong"
    ]
  },
  "http://arxiv.org/abs/2312.07622": {
    "title": "Mathematical Language Models: A Survey",
    "volume": "Dec",
    "abstract": "In recent years, there has been remarkable progress in leveraging Language Models (LMs), encompassing Pre-trained Language Models (PLMs) and Large-scale Language Models (LLMs), within the domain of mathematics. This paper conducts a comprehensive survey of mathematical LMs, systematically categorizing pivotal research endeavors from two distinct perspectives: tasks and methodologies. The landscape reveals a large number of proposed mathematical LLMs, which are further delineated into instruction learning, tool-based methods, fundamental CoT techniques, and advanced CoT methodologies. In addition, our survey entails the compilation of over 60 mathematical datasets, including training datasets, benchmark datasets, and augmented datasets. Addressing the primary challenges and delineating future trajectories within the field of mathematical LMs, this survey is positioned as a valuable resource, poised to facilitate and inspire future innovation among researchers invested in advancing this domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wentao Liu",
      "Hanglei Hu",
      "Jie Zhou",
      "Yuyang Ding",
      "Junsong Li",
      "Jiayi Zeng",
      "Mengliang He",
      "Qin Chen",
      "Bo Jiang",
      "Aimin Zhou",
      "Liang He"
    ]
  },
  "http://arxiv.org/abs/2312.06881": {
    "title": "DYAD: A Descriptive Yet Abjuring Density efficient approximation to linear neural network layers",
    "volume": "Dec",
    "abstract": "We devise, implement and performance-asses DYAD, a layer which can serve as a faster and more memory-efficient approximate replacement for linear layers, (nn.Linear() in Pytorch). These layers appear in common subcomponents, such as in the ff module of Transformers. DYAD is based on a bespoke near-sparse matrix structure which approximates the dense \"weight\" matrix W that matrix-multiplies the input in the typical realization of such a layer, a.k.a DENSE. Our alternative near-sparse matrix structure is decomposable to a sum of 2 matrices permutable to a block-sparse counterpart. These can be represented as 3D tensors, which in unison allow a faster execution of matrix multiplication with the mini-batched input matrix X compared to DENSE (O(rows(W ) x cols(W )) --> O( rows(W ) x cols(W ) # of blocks )). As the crux of our experiments, we pretrain both DYAD and DENSE variants of 2 sizes of the OPT arch and 1 size of the Pythia arch, including at different token scales of the babyLM benchmark. We find DYAD to be competitive (>= 90%) of DENSE performance on zero-shot (e.g. BLIMP), few-shot (OPENLM) and finetuning (GLUE) benchmarks, while being >=7-15% faster to train on-GPU even at 125m scale, besides surfacing larger speedups at increasing scale and model width",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarin Chandy",
      "Varun Gangal",
      "Yi Yang",
      "Gabriel Maggiotti"
    ]
  },
  "http://arxiv.org/abs/2312.06874": {
    "title": "Dozerformer: Sequence Adaptive Sparse Transformer for Multivariate Time Series Forecasting",
    "volume": "Dec",
    "abstract": "Transformers have achieved remarkable performance in multivariate time series(MTS) forecasting due to their capability to capture long-term dependencies. However, the canonical attention mechanism has two key limitations: (1) its quadratic time complexity limits the sequence length, and (2) it generates future values from the entire historical sequence. To address this, we propose a Dozer Attention mechanism consisting of three sparse components: (1) Local, each query exclusively attends to keys within a localized window of neighboring time steps. (2) Stride, enables each query to attend to keys at predefined intervals. (3) Vary, allows queries to selectively attend to keys from a subset of the historical sequence. Notably, the size of this subset dynamically expands as forecasting horizons extend. Those three components are designed to capture essential attributes of MTS data, including locality, seasonality, and global temporal dependencies. Additionally, we present the Dozerformer Framework, incorporating the Dozer Attention mechanism for the MTS forecasting task. We evaluated the proposed Dozerformer framework with recent state-of-the-art methods on nine benchmark datasets and confirmed its superior performance. The code will be released after the manuscript is accepted",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Zhang",
      "Rui Wu",
      "Sergiu M. Dascalu",
      "Frederick C. Harris Jr"
    ]
  },
  "http://arxiv.org/abs/2312.06867": {
    "title": "Get an A in Math: Progressive Rectification Prompting",
    "volume": "Dec",
    "abstract": "Chain-of-Thought (CoT) prompting methods have enabled large language models (LLMs) to generate reasoning paths and solve math word problems (MWPs). However, they are sensitive to mistakes in the paths, as any mistake can result in an incorrect answer. We propose a novel method named Progressive Rectification Prompting (PRP) to improve average accuracy on eight MWP datasets from 77.3 to 90.5. Given an initial answer from CoT, PRP iterates a verify-then-rectify process to progressively identify incorrect answers and rectify the reasoning paths. With the most likely correct answer, the LLM predicts a masked numerical value in the question; if the prediction does not match the masked value, the answer is likely incorrect. Then the LLM is prompted to re-generate the reasoning path hinted with a set of incorrect answers to prevent itself from repeating previous mistakes. PRP achieves the best performance compared against the CoT methods. Our implementation is made publicly available at https://wzy6642.github.io/prp.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Wu",
      "Meng Jiang",
      "Chao Shen"
    ]
  },
  "http://arxiv.org/abs/2312.06861": {
    "title": "Disentangling Perceptions of Offensiveness: Cultural and Moral Correlates",
    "volume": "Dec",
    "abstract": "Perception of offensiveness is inherently subjective, shaped by the lived experiences and socio-cultural values of the perceivers. Recent years have seen substantial efforts to build AI-based tools that can detect offensive language at scale, as a means to moderate social media platforms, and to ensure safety of conversational AI technologies such as ChatGPT and Bard. However, existing approaches treat this task as a technical endeavor, built on top of data annotated for offensiveness by a global crowd workforce without any attention to the crowd workers' provenance or the values their perceptions reflect. We argue that cultural and psychological factors play a vital role in the cognitive processing of offensiveness, which is critical to consider in this context. We re-frame the task of determining offensiveness as essentially a matter of moral judgment -- deciding the boundaries of ethically wrong vs. right language within an implied set of socio-cultural norms. Through a large-scale cross-cultural study based on 4309 participants from 21 countries across 8 cultural regions, we demonstrate substantial cross-cultural differences in perceptions of offensiveness. More importantly, we find that individual moral values play a crucial role in shaping these variations: moral concerns about Care and Purity are significant mediating factors driving cross-cultural differences. These insights are of crucial importance as we build AI models for the pluralistic world, where the values they espouse should aim to respect and account for moral values in diverse geo-cultural contexts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aida Davani",
      "Mark Díaz",
      "Dylan Baker",
      "Vinodkumar Prabhakaran"
    ]
  },
  "http://arxiv.org/abs/2312.06855": {
    "title": "Multimodal Pretraining of Medical Time Series and Notes",
    "volume": "Dec",
    "abstract": "Within the intensive care unit (ICU), a wealth of patient data, including clinical measurements and clinical notes, is readily available. This data is a valuable resource for comprehending patient health and informing medical decisions, but it also contains many challenges in analysis. Deep learning models show promise in extracting meaningful patterns, but they require extensive labeled data, a challenge in critical care. To address this, we propose a novel approach employing self-supervised pretraining, focusing on the alignment of clinical measurements and notes. Our approach combines contrastive and masked token prediction tasks during pretraining. Semi-supervised experiments on the MIMIC-III dataset demonstrate the effectiveness of our self-supervised pretraining. In downstream tasks, including in-hospital mortality prediction and phenotyping, our pretrained model outperforms baselines in settings where only a fraction of the data is labeled, emphasizing its ability to enhance ICU data analysis. Notably, our method excels in situations where very few labels are available, as evidenced by an increase in the AUC-ROC for in-hospital mortality by 0.17 and in AUC-PR for phenotyping by 0.1 when only 1% of labels are accessible. This work advances self-supervised learning in the healthcare domain, optimizing clinical insights from abundant yet challenging ICU data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan King",
      "Tianbao Yang",
      "Bobak Mortazavi"
    ]
  },
  "http://arxiv.org/abs/2312.06825": {
    "title": "Utilization of Non-verbal Behaviour and Social Gaze in Classroom Human-Robot Interaction Communications",
    "volume": "Dec",
    "abstract": "This abstract explores classroom Human-Robot Interaction (HRI) scenarios with an emphasis on the adaptation of human-inspired social gaze models in robot cognitive architecture to facilitate a more seamless social interaction. First, we detail the HRI scenarios explored by us in our studies followed by a description of the social gaze model utilized for our research. We highlight the advantages of utilizing such an attentional model in classroom HRI scenarios. We also detail the intended goals of our upcoming study involving this social gaze model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sahand Shaghaghi",
      "Pourya Aliasghari",
      "Bryan Tripp",
      "Kerstin Dautenhahn",
      "Chrystopher Nehaniv"
    ]
  },
  "http://arxiv.org/abs/2312.06820": {
    "title": "Extracting Self-Consistent Causal Insights from Users Feedback with LLMs and In-context Learning",
    "volume": "Dec",
    "abstract": "Microsoft Windows Feedback Hub is designed to receive customer feedback on a wide variety of subjects including critical topics such as power and battery. Feedback is one of the most effective ways to have a grasp of users' experience with Windows and its ecosystem. However, the sheer volume of feedback received by Feedback Hub makes it immensely challenging to diagnose the actual cause of reported issues. To better understand and triage issues, we leverage Double Machine Learning (DML) to associate users' feedback with telemetry signals. One of the main challenges we face in the DML pipeline is the necessity of domain knowledge for model design (e.g., causal graph), which sometimes is either not available or hard to obtain. In this work, we take advantage of reasoning capabilities in Large Language Models (LLMs) to generate a prior model that which to some extent compensates for the lack of domain knowledge and could be used as a heuristic for measuring feedback informativeness. Our LLM-based approach is able to extract previously known issues, uncover new bugs, and identify sequences of events that lead to a bug, while minimizing out-of-domain outputs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sara Abdali",
      "Anjali Parikh",
      "Steve Lim",
      "Emre Kiciman"
    ]
  },
  "http://arxiv.org/abs/2312.06652": {
    "title": "Building Domain-Specific LLMs Faithful To The Islamic Worldview: Mirage or Technical Possibility?",
    "volume": "Dec",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across numerous natural language understanding use cases. However, this impressive performance comes with inherent limitations, such as the tendency to perpetuate stereotypical biases or fabricate non-existent facts. In the context of Islam and its representation, accurate and factual representation of its beliefs and teachings rooted in the Quran and Sunnah is key. This work focuses on the challenge of building domain-specific LLMs faithful to the Islamic worldview and proposes ways to build and evaluate such systems. Firstly, we define this open-ended goal as a technical problem and propose various solutions. Subsequently, we critically examine known challenges inherent to each approach and highlight evaluation methodologies that can be used to assess such systems. This work highlights the need for high-quality datasets, evaluations, and interdisciplinary work blending machine learning with Islamic scholarship",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shabaz Patel",
      "Hassan Kane",
      "Rayhan Patel"
    ]
  },
  "http://arxiv.org/abs/2312.06742": {
    "title": "Honeybee: Locality-enhanced Projector for Multimodal LLM",
    "volume": "Dec",
    "abstract": "In Multimodal Large Language Models (MLLMs), a visual projector plays a crucial role in bridging pre-trained vision encoders with LLMs, enabling profound visual understanding while harnessing the LLMs' robust capabilities. Despite the importance of the visual projector, it has been relatively less explored. In this study, we first identify two essential projector properties: (i) flexibility in managing the number of visual tokens, crucial for MLLMs' overall efficiency, and (ii) preservation of local context from visual features, vital for spatial understanding. Based on these findings, we propose a novel projector design that is both flexible and locality-enhanced, effectively satisfying the two desirable properties. Additionally, we present comprehensive strategies to effectively utilize multiple and multifaceted instruction datasets. Through extensive experiments, we examine the impact of individual design choices. Finally, our proposed MLLM, Honeybee, remarkably outperforms previous state-of-the-art methods across various benchmarks, including MME, MMBench, SEED-Bench, and LLaVA-Bench, achieving significantly higher efficiency. Code and models are available at https://github.com/kakaobrain/honeybee",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junbum Cha",
      "Wooyoung Kang",
      "Jonghwan Mun",
      "Byungseok Roh"
    ]
  },
  "http://arxiv.org/abs/2312.06648": {
    "title": "Dense X Retrieval: What Retrieval Granularity Should We Use?",
    "volume": "Dec",
    "abstract": "Dense retrieval has become a prominent method to obtain relevant context or world knowledge in open-domain NLP tasks. When we use a learned dense retriever on a retrieval corpus at inference time, an often-overlooked design choice is the retrieval unit in which the corpus is indexed, e.g. document, passage, or sentence. We discover that the retrieval unit choice significantly impacts the performance of both retrieval and downstream tasks. Distinct from the typical approach of using passages or sentences, we introduce a novel retrieval unit, proposition, for dense retrieval. Propositions are defined as atomic expressions within text, each encapsulating a distinct factoid and presented in a concise, self-contained natural language format. We conduct an empirical comparison of different retrieval granularity. Our results reveal that proposition-based retrieval significantly outperforms traditional passage or sentence-based methods in dense retrieval. Moreover, retrieval by proposition also enhances the performance of downstream QA tasks, since the retrieved texts are more condensed with question-relevant information, reducing the need for lengthy input tokens and minimizing the inclusion of extraneous, irrelevant information",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Chen",
      "Hongwei Wang",
      "Sihao Chen",
      "Wenhao Yu",
      "Kaixin Ma",
      "Xinran Zhao",
      "Hongming Zhang",
      "Dong Yu"
    ]
  },
  "http://arxiv.org/abs/2312.06635": {
    "title": "Gated Linear Attention Transformers with Hardware-Efficient Training",
    "volume": "Dec",
    "abstract": "Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear (with respect to output length) inference complexity. Recent works such as RetNet (Sun et al., 2023) and TransNormerLLM (Qin et al., 2023a) observe that adding a global decay term to the additive RNN update rule greatly improves performance, sometimes outperforming standard Transformers with softmax attention when trained at scale. In this work we show that adding a data-dependent gating mechanism further improves performance. We derive a parallel form of this gated linear attention layer that enables efficient training. However, a straightforward, numerically stable implementation of this parallel form requires generalized matrix multiplications in log-space for numerical stability, and thus cannot take advantage of tensor cores on modern GPUs which are optimized for standard matrix multiplications. We develop a hardware-efficient version of the parallel form that can still make use of tensor cores through block-parallel computations over sequence chunks. Experiments on moderate-scale language modeling (340M-parameter models trained on 15B tokens, 1.3B-parameter models trained on 100B tokens) show that gated linear attention (GLA) Transformers perform competitively against a strong LLaMA-architecture Transformer baseline (Touvron et al., 2023) as well as Mamba (Gu & Dao, 2023), a recently introduced state-space model with a data-dependent state transition mechanism. For training speed, our Triton-based implementation performs comparably to CUDA-optimized FlashAttention-2 (Dao, 2023) under the regular 2048 training length setting, while outperforming FlashAttention-2 when training on longer sequences beyond 4096",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songlin Yang",
      "Bailin Wang",
      "Yikang Shen",
      "Rameswar Panda",
      "Yoon Kim"
    ]
  },
  "http://arxiv.org/abs/2312.06562": {
    "title": "On Meta-Prompting",
    "volume": "Dec",
    "abstract": "Certain statistical models are capable of interpreting input strings as instructions, or prompts, and carry out tasks based on them. Many approaches to prompting and pre-training these models involve the automated generation of these prompts. We call these approaches meta-prompting, or prompting to obtain prompts. We propose a theoretical framework based on category theory to generalize and describe them. This framework is flexible enough to account for LLM stochasticity; and allows us to obtain formal results around task agnosticity and equivalence of various meta-prompting approaches. We experiment with meta-prompting in two active areas of model research: creativity and ideation. We find that user preference favors (p < 0.01) the prompts generated under meta-prompting, as well as their corresponding outputs, over a series of hardcoded baseline prompts that include the original task prompt. Using our framework, we argue that meta-prompting is more effective than basic prompting at generating desirable outputs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrian de Wynter",
      "Xun Wang",
      "Qilong Gu",
      "Si-Qing Chen"
    ]
  },
  "http://arxiv.org/abs/2312.06550": {
    "title": "LLM360: Towards Fully Transparent Open-Source LLMs",
    "volume": "Dec",
    "abstract": "The recent surge in open-source Large Language Models (LLMs), such as LLaMA, Falcon, and Mistral, provides diverse options for AI practitioners and researchers. However, most LLMs have only released partial artifacts, such as the final model weights or inference code, and technical reports increasingly limit their scope to high-level design choices and surface statistics. These choices hinder progress in the field by degrading transparency into the training of LLMs and forcing teams to rediscover many details in the training process. We present LLM360, an initiative to fully open-source LLMs, which advocates for all training code and data, model checkpoints, and intermediate results to be made available to the community. The goal of LLM360 is to support open and collaborative AI research by making the end-to-end LLM training process transparent and reproducible by everyone. As a first step of LLM360, we release two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder, including their training code, data, intermediate checkpoints, and analyses (at https://www.llm360.ai). We are committed to continually pushing the boundaries of LLMs through this open-source effort. More large-scale and stronger models are underway and will be released in the future",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengzhong Liu",
      "Aurick Qiao",
      "Willie Neiswanger",
      "Hongyi Wang",
      "Bowen Tan",
      "Tianhua Tao",
      "Junbo Li",
      "Yuqi Wang",
      "Suqi Sun",
      "Omkar Pangarkar",
      "Richard Fan",
      "Yi Gu",
      "Victor Miller",
      "Yonghao Zhuang",
      "Guowei He",
      "Haonan Li",
      "Fajri Koto",
      "Liping Tang",
      "Nikhil Ranjan",
      "Zhiqiang Shen",
      "Xuguang Ren",
      "Roberto Iriondo",
      "Cun Mu",
      "Zhiting Hu",
      "Mark Schulze",
      "Preslav Nakov",
      "Tim Baldwin",
      "Eric P. Xing"
    ]
  },
  "http://arxiv.org/abs/2312.06522": {
    "title": "Label Smoothing for Enhanced Text Sentiment Classification",
    "volume": "Dec",
    "abstract": "Label smoothing is a widely used technique in various domains, such as image classification and speech recognition, known for effectively combating model overfitting. However, there is few research on its application to text sentiment classification. To fill in the gap, this study investigates the implementation of label smoothing for sentiment classification by utilizing different levels of smoothing. The primary objective is to enhance sentiment classification accuracy by transforming discrete labels into smoothed label distributions. Through extensive experiments, we demonstrate the superior performance of label smoothing in text sentiment classification tasks across eight diverse datasets and deep learning architectures: TextCNN, BERT, and RoBERTa, under two learning schemes: training from scratch and fine-tuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijie Gao",
      "Shijing Si"
    ]
  },
  "http://arxiv.org/abs/2312.06514": {
    "title": "Where exactly does contextualization in a PLM happen?",
    "volume": "Dec",
    "abstract": "Pre-trained Language Models (PLMs) have shown to be consistently successful in a plethora of NLP tasks due to their ability to learn contextualized representations of words (Ethayarajh, 2019). BERT (Devlin et al., 2018), ELMo (Peters et al., 2018) and other PLMs encode word meaning via textual context, as opposed to static word embeddings, which encode all meanings of a word in a single vector representation. In this work, we present a study that aims to localize where exactly in a PLM word contextualization happens. In order to find the location of this word meaning transformation, we investigate representations of polysemous words in the basic BERT uncased 12 layer architecture (Devlin et al., 2018), a masked language model trained on an additional sentence adjacency objective, using qualitative and quantitative measures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soniya Vijayakumar",
      "Tanja Bäumel",
      "Simon Ostermann",
      "Josef van Genabith"
    ]
  },
  "http://arxiv.org/abs/2312.06499": {
    "title": "TaCo: Targeted Concept Removal in Output Embeddings for NLP via Information Theory and Explainability",
    "volume": "Dec",
    "abstract": "The fairness of Natural Language Processing (NLP) models has emerged as a crucial concern. Information theory indicates that to achieve fairness, a model should not be able to predict sensitive variables, such as gender, ethnicity, and age. However, information related to these variables often appears implicitly in language, posing a challenge in identifying and mitigating biases effectively. To tackle this issue, we present a novel approach that operates at the embedding level of an NLP model, independent of the specific architecture. Our method leverages insights from recent advances in XAI techniques and employs an embedding transformation to eliminate implicit information from a selected variable. By directly manipulating the embeddings in the final layer, our approach enables a seamless integration into existing models without requiring significant modifications or retraining. In evaluation, we show that the proposed post-hoc approach significantly reduces gender-related associations in NLP models while preserving the overall performance and functionality of the models. An implementation of our method is available: https://github.com/fanny-jourdan/TaCo",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fanny Jourdan",
      "Louis Béthune",
      "Agustin Picard",
      "Laurent Risser",
      "Nicholas Asher"
    ]
  },
  "http://arxiv.org/abs/2312.06457": {
    "title": "Large Language Models with Retrieval-Augmented Generation for Zero-Shot Disease Phenotyping",
    "volume": "Dec",
    "abstract": "Identifying disease phenotypes from electronic health records (EHRs) is critical for numerous secondary uses. Manually encoding physician knowledge into rules is particularly challenging for rare diseases due to inadequate EHR coding, necessitating review of clinical notes. Large language models (LLMs) offer promise in text understanding but may not efficiently handle real-world clinical documentation. We propose a zero-shot LLM-based method enriched by retrieval-augmented generation and MapReduce, which pre-identifies disease-related text snippets to be used in parallel as queries for the LLM to establish diagnosis. We show that this method as applied to pulmonary hypertension (PH), a rare disease characterized by elevated arterial pressures in the lungs, significantly outperforms physician logic rules ($F_1$ score of 0.62 vs. 0.75). This method has the potential to enhance rare disease cohort identification, expanding the scope of robust clinical research and care gap identification",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Will E. Thompson",
      "David M. Vidmar",
      "Jessica K. De Freitas",
      "John M. Pfeifer",
      "Brandon K. Fornwalt",
      "Ruijun Chen",
      "Gabriel Altay",
      "Kabir Manghnani",
      "Andrew C. Nelsen",
      "Kellie Morland",
      "Martin C. Stumpe",
      "Riccardo Miotto"
    ]
  },
  "http://arxiv.org/abs/2312.07599": {
    "title": "Contrastive News and Social Media Linking using BERT for Articles and Tweets across Dual Platforms",
    "volume": "Dec",
    "abstract": "X (formerly Twitter) has evolved into a contemporary agora, offering a platform for individuals to express opinions and viewpoints on current events. The majority of the topics discussed on Twitter are directly related to ongoing events, making it an important source for monitoring public discourse. However, linking tweets to specific news presents a significant challenge due to their concise and informal nature. Previous approaches, including topic models, graph-based models, and supervised classifiers, have fallen short in effectively capturing the unique characteristics of tweets and articles. Inspired by the success of the CLIP model in computer vision, which employs contrastive learning to model similarities between images and captions, this paper introduces a contrastive learning approach for training a representation space where linked articles and tweets exhibit proximity. We present our contrastive learning approach, CATBERT (Contrastive Articles Tweets BERT), leveraging pre-trained BERT models. The model is trained and tested on a dataset containing manually labeled English and Polish tweets and articles related to the Russian-Ukrainian war. We evaluate CATBERT's performance against traditional approaches like LDA, and the novel method based on OpenAI embeddings, which has not been previously applied to this task. Our findings indicate that CATBERT demonstrates superior performance in associating tweets with relevant news articles. Furthermore, we demonstrate the performance of the models when applied to finding the main topic -- represented by an article -- of the whole cascade of tweets. In this new task, we report the performance of the different models in dependence on the cascade size",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Piotrowski",
      "Marek Wachnicki",
      "Mateusz Perlik",
      "Jakub Podolak",
      "Grzegorz Rucki",
      "Michał Brzozowski",
      "Paweł Olejnik",
      "Julian Kozłowski",
      "Tomasz Nocoń",
      "Jakub Kozieł",
      "Stanisław Giziński",
      "Piotr Sankowski"
    ]
  },
  "http://arxiv.org/abs/2312.06374": {
    "title": "UstanceBR: a multimodal language resource for stance prediction",
    "volume": "Dec",
    "abstract": "This work introduces UstanceBR, a multimodal corpus in the Brazilian Portuguese Twitter domain for target-based stance prediction. The corpus comprises 86.8 k labelled stances towards selected target topics, and extensive network information about the users who published these stances on social media. In this article we describe the corpus multimodal data, and a number of usage examples in both in-domain and zero-shot stance prediction based on text- and network-related information, which are intended to provide initial baseline results for future studies in the field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Camila Pereira",
      "Matheus Pavan",
      "Sungwon Yoon",
      "Ricelli Ramos",
      "Pablo Costa",
      "Lais Cavalheiro",
      "Ivandre Paraboni"
    ]
  },
  "http://arxiv.org/abs/2312.06363": {
    "title": "MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples",
    "volume": "Dec",
    "abstract": "Although In-Context Learning (ICL) brings remarkable performance gains to Large Language Models (LLMs), the improvements remain lower than fine-tuning on downstream tasks. This paper introduces Multi-Modal In-Context Tuning (MMICT), a novel multi-modal fine-tuning paradigm that boosts multi-modal fine-tuning by fully leveraging the promising ICL capability of multi-modal LLMs (MM-LLMs). We propose the Multi-Modal Hub (M-Hub), a unified module that captures various multi-modal features according to different inputs and objectives. Based on M-Hub, MMICT enables MM-LLMs to learn from in-context visual-guided textual features and subsequently generate outputs conditioned on the textual-guided visual features. Moreover, leveraging the flexibility of M-Hub, we design a variety of in-context demonstrations. Extensive experiments on a diverse range of downstream multi-modal tasks demonstrate that MMICT significantly outperforms traditional fine-tuning strategy and the vanilla ICT method that directly takes the concatenation of all information from different modalities as input",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Chen",
      "Enwei Zhang",
      "Yuting Gao",
      "Ke Li",
      "Xing Sun",
      "Yan Zhang",
      "Hui Li"
    ]
  },
  "http://arxiv.org/abs/2312.06355": {
    "title": "Empirical Basis of Engineering Design Knowledge",
    "volume": "Dec",
    "abstract": "Engineering design knowledge is embodied in natural language text through intricate placement of entities and relationships. Ontological constructs of design knowledge often limit the performances of NLP techniques to extract design knowledge. Also, large-language models could be less useful for generating and explicating design knowledge, as these are trained predominantly on common-sense text. In this article, we present the constituents of design knowledge based on empirical observations from patent documents. We obtain a sample of 33,881 patents and populate over 24 million facts from the sentences in these. We conduct Zipf distribution analyses using the frequencies of unique entities and relationships that are present in the facts thus populated. While the literal entities cannot be generalised from the sample of patents, the relationships largely capture attributes ('of'), structure ('in', 'with'), purpose ('to', 'for'), hierarchy ('include'), exemplification ('such as'), and behaviour ('to', 'from'). The analyses reveal that over half of entities and relationships could be generalised to 64 and 24 linguistic syntaxes respectively, while hierarchical relationships include 75 syntaxes. These syntaxes represent the linguistic basis of engineering design knowledge. We combine facts within each patent into a knowledge graph, from which we discover motifs that are statistically over-represented subgraph patterns. Across all patents in the sample, we identify eight patterns that could be simplified into sequence [->...->], aggregation [->...<-], and hierarchy [<-...->] that form the structural basis of engineering design knowledge. We propose regulatory precepts for concretising abstract entities and relationships within subgraphs, while also explicating hierarchical structures. These precepts could be useful for better construction and management of knowledge in a design environment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "L. Siddharth",
      "Jianxi Luo"
    ]
  },
  "http://arxiv.org/abs/2312.06352": {
    "title": "NuScenes-MQA: Integrated Evaluation of Captions and QA for Autonomous Driving Datasets using Markup Annotations",
    "volume": "Dec",
    "abstract": "Visual Question Answering (VQA) is one of the most important tasks in autonomous driving, which requires accurate recognition and complex situation evaluations. However, datasets annotated in a QA format, which guarantees precise language generation and scene recognition from driving scenes, have not been established yet. In this work, we introduce Markup-QA, a novel dataset annotation technique in which QAs are enclosed within markups. This approach facilitates the simultaneous evaluation of a model's capabilities in sentence generation and VQA. Moreover, using this annotation methodology, we designed the NuScenes-MQA dataset. This dataset empowers the development of vision language models, especially for autonomous driving tasks, by focusing on both descriptive capabilities and precise QA. The dataset is available at https://github.com/turingmotors/NuScenes-MQA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuichi Inoue",
      "Yuki Yada",
      "Kotaro Tanahashi",
      "Yu Yamaguchi"
    ]
  },
  "http://arxiv.org/abs/2312.06351": {
    "title": "Evaluation of Large Language Models for Decision Making in Autonomous Driving",
    "volume": "Dec",
    "abstract": "Various methods have been proposed for utilizing Large Language Models (LLMs) in autonomous driving. One strategy of using LLMs for autonomous driving involves inputting surrounding objects as text prompts to the LLMs, along with their coordinate and velocity information, and then outputting the subsequent movements of the vehicle. When using LLMs for such purposes, capabilities such as spatial recognition and planning are essential. In particular, two foundational capabilities are required: (1) spatial-aware decision making, which is the ability to recognize space from coordinate information and make decisions to avoid collisions, and (2) the ability to adhere to traffic rules. However, quantitative research has not been conducted on how accurately different types of LLMs can handle these problems. In this study, we quantitatively evaluated these two abilities of LLMs in the context of autonomous driving. Furthermore, to conduct a Proof of Concept (POC) for the feasibility of implementing these abilities in actual vehicles, we developed a system that uses LLMs to drive a vehicle",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kotaro Tanahashi",
      "Yuichi Inoue",
      "Yu Yamaguchi",
      "Hidetatsu Yaginuma",
      "Daiki Shiotsuka",
      "Hiroyuki Shimatani",
      "Kohei Iwamasa",
      "Yoshiaki Inoue",
      "Takafumi Yamaguchi",
      "Koki Igari",
      "Tsukasa Horinouchi",
      "Kento Tokuhiro",
      "Yugo Tokuchi",
      "Shunsuke Aoki"
    ]
  },
  "http://arxiv.org/abs/2312.06338": {
    "title": "BoschAI @ Causal News Corpus 2023: Robust Cause-Effect Span Extraction using Multi-Layer Sequence Tagging and Data Augmentation",
    "volume": "Dec",
    "abstract": "Understanding causality is a core aspect of intelligence. The Event Causality Identification with Causal News Corpus Shared Task addresses two aspects of this challenge: Subtask 1 aims at detecting causal relationships in texts, and Subtask 2 requires identifying signal words and the spans that refer to the cause or effect, respectively. Our system, which is based on pre-trained transformers, stacked sequence tagging, and synthetic data augmentation, ranks third in Subtask 1 and wins Subtask 2 with an F1 score of 72.8, corresponding to a margin of 13 pp. to the second-best system",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Timo Pierre Schrader",
      "Simon Razniewski",
      "Lukas Lange",
      "Annemarie Friedrich"
    ]
  },
  "http://arxiv.org/abs/2312.06337": {
    "title": "Deep Imbalanced Learning for Multimodal Emotion Recognition in Conversations",
    "volume": "Dec",
    "abstract": "The main task of Multimodal Emotion Recognition in Conversations (MERC) is to identify the emotions in modalities, e.g., text, audio, image and video, which is a significant development direction for realizing machine intelligence. However, many data in MERC naturally exhibit an imbalanced distribution of emotion categories, and researchers ignore the negative impact of imbalanced data on emotion recognition. To tackle this problem, we systematically analyze it from three aspects: data augmentation, loss sensitivity, and sampling strategy, and propose the Class Boundary Enhanced Representation Learning (CBERL) model. Concretely, we first design a multimodal generative adversarial network to address the imbalanced distribution of {emotion} categories in raw data. Secondly, a deep joint variational autoencoder is proposed to fuse complementary semantic information across modalities and obtain discriminative feature representations. Finally, we implement a multi-task graph neural network with mask reconstruction and classification optimization to solve the problem of overfitting and underfitting in class boundary learning, and achieve cross-modal emotion recognition. We have conducted extensive experiments on the IEMOCAP and MELD benchmark datasets, and the results show that CBERL has achieved a certain performance improvement in the effectiveness of emotion recognition. Especially on the minority class fear and disgust emotion labels, our model improves the accuracy and F1 value by 10% to 20%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Meng",
      "Yuntao Shou",
      "Wei Ai",
      "Nan Yin",
      "Keqin Li"
    ]
  },
  "http://arxiv.org/abs/2312.06315": {
    "title": "GPTBIAS: A Comprehensive Framework for Evaluating Bias in Large Language Models",
    "volume": "Dec",
    "abstract": "Warning: This paper contains content that may be offensive or upsetting. There has been a significant increase in the usage of large language models (LLMs) in various applications, both in their original form and through fine-tuned adaptations. As a result, LLMs have gained popularity and are being widely adopted by a large user community. However, one of the concerns with LLMs is the potential generation of socially biased content. The existing evaluation methods have many constraints, and their results exhibit a limited degree of interpretability. In this work, we propose a bias evaluation framework named GPTBIAS that leverages the high performance of LLMs (e.g., GPT-4 \\cite{openai2023gpt4}) to assess bias in models. We also introduce prompts called Bias Attack Instructions, which are specifically designed for evaluating model bias. To enhance the credibility and interpretability of bias evaluation, our framework not only provides a bias score but also offers detailed information, including bias types, affected demographics, keywords, reasons behind the biases, and suggestions for improvement. We conduct extensive experiments to demonstrate the effectiveness and usability of our bias evaluation framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxu Zhao",
      "Meng Fang",
      "Shirui Pan",
      "Wenpeng Yin",
      "Mykola Pechenizkiy"
    ]
  },
  "http://arxiv.org/abs/2312.06281": {
    "title": "EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models",
    "volume": "Dec",
    "abstract": "We introduce EQ-Bench, a novel benchmark designed to evaluate aspects of emotional intelligence in Large Language Models (LLMs). We assess the ability of LLMs to understand complex emotions and social interactions by asking them to predict the intensity of emotional states of characters in a dialogue. The benchmark is able to discriminate effectively between a wide range of models. We find that EQ-Bench correlates strongly with comprehensive multi-domain benchmarks like MMLU (Hendrycks et al., 2020) (r=0.97), indicating that we may be capturing similar aspects of broad intelligence. Our benchmark produces highly repeatable results using a set of 60 English-language questions. We also provide open-source code for an automated benchmarking pipeline at https://github.com/EQ-bench/EQ-Bench and a leaderboard at https://www.eqbench.com",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel J. Paech"
    ]
  },
  "http://arxiv.org/abs/2312.06266": {
    "title": "Creating Spoken Dialog Systems in Ultra-Low Resourced Settings",
    "volume": "Dec",
    "abstract": "Automatic Speech Recognition (ASR) systems are a crucial technology that is used today to design a wide variety of applications, most notably, smart assistants, such as Alexa. ASR systems are essentially dialogue systems that employ Spoken Language Understanding (SLU) to extract meaningful information from speech. The main challenge with designing such systems is that they require a huge amount of labeled clean data to perform competitively, such data is extremely hard to collect and annotate to respective SLU tasks, furthermore, when designing such systems for low resource languages, where data is extremely limited, the severity of the problem intensifies. In this paper, we focus on a fairly popular SLU task, that is, Intent Classification while working with a low resource language, namely, Flemish. Intent Classification is a task concerned with understanding the intents of the user interacting with the system. We build on existing light models for intent classification in Flemish, and our main contribution is applying different augmentation techniques on two levels -- the voice level, and the phonetic transcripts level -- to the existing models to counter the problem of scarce labeled data in low-resource languages. We find that our data augmentation techniques, on both levels, have improved the model performance on a number of tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moayad Elamin",
      "Muhammad Omer",
      "Yonas Chanie",
      "Henslaac Ndlovu"
    ]
  },
  "http://arxiv.org/abs/2312.06236": {
    "title": "Improving Startup Success with Text Analysis",
    "volume": "Dec",
    "abstract": "Investors are interested in predicting future success of startup companies, preferably using publicly available data which can be gathered using free online sources. Using public-only data has been shown to work, but there is still much room for improvement. Two of the best performing prediction experiments use 17 and 49 features respectively, mostly numeric and categorical in nature. In this paper, we significantly expand and diversify both the sources and the number of features (to 171) to achieve better prediction. Data collected from Crunchbase, the Google Search API, and Twitter (now X) are used to predict whether a company will raise a round of funding within a fixed time horizon. Much of the new features are textual and the Twitter subset include linguistic metrics such as measures of passive voice and parts-of-speech. A total of ten machine learning models are also evaluated for best performance. The adaptable model can be used to predict funding 1-5 years into the future, with a variable cutoff threshold to favor either precision or recall. Prediction with comparable assumptions generally achieves F scores above 0.730 which outperforms previous attempts in the literature (0.531), and does so with fewer examples. Furthermore, we find that the vast majority of the performance impact comes from the top 18 of 171 features which are mostly generic company observations, including the best performing individual feature which is the free-form text description of the company",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emily Gavrilenko",
      "Foaad Khosmood",
      "Mahdi Rastad",
      "Sadra Amiri Moghaddam"
    ]
  },
  "http://arxiv.org/abs/2312.06224": {
    "title": "Medical Vision Language Pretraining: A survey",
    "volume": "Dec",
    "abstract": "Medical Vision Language Pretraining (VLP) has recently emerged as a promising solution to the scarcity of labeled data in the medical domain. By leveraging paired/unpaired vision and text datasets through self-supervised learning, models can be trained to acquire vast knowledge and learn robust feature representations. Such pretrained models have the potential to enhance multiple downstream medical tasks simultaneously, reducing the dependency on labeled data. However, despite recent progress and its potential, there is no such comprehensive survey paper that has explored the various aspects and advancements in medical VLP. In this paper, we specifically review existing works through the lens of different pretraining objectives, architectures, downstream evaluation tasks, and datasets utilized for pretraining and downstream tasks. Subsequently, we delve into current challenges in medical VLP, discussing existing and potential solutions, and conclude by highlighting future directions. To the best of our knowledge, this is the first survey focused on medical VLP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prashant Shrestha",
      "Sanskar Amgain",
      "Bidur Khanal",
      "Cristian A. Linte",
      "Binod Bhattarai"
    ]
  },
  "http://arxiv.org/abs/2312.07592": {
    "title": "Evaluating ChatGPT as a Question Answering System: A Comprehensive Analysis and Comparison with Existing Models",
    "volume": "Dec",
    "abstract": "In the current era, a multitude of language models has emerged to cater to user inquiries. Notably, the GPT-3.5 Turbo language model has gained substantial attention as the underlying technology for ChatGPT. Leveraging extensive parameters, this model adeptly responds to a wide range of questions. However, due to its reliance on internal knowledge, the accuracy of responses may not be absolute. This article scrutinizes ChatGPT as a Question Answering System (QAS), comparing its performance to other existing QASs. The primary focus is on evaluating ChatGPT's proficiency in extracting responses from provided paragraphs, a core QAS capability. Additionally, performance comparisons are made in scenarios without a surrounding passage. Multiple experiments, exploring response hallucination and considering question complexity, were conducted on ChatGPT. Evaluation employed well-known Question Answering (QA) datasets, including SQuAD, NewsQA, and PersianQuAD, across English and Persian languages. Metrics such as F-score, exact match, and accuracy were employed in the assessment. The study reveals that, while ChatGPT demonstrates competence as a generative model, it is less effective in question answering compared to task-specific models. Providing context improves its performance, and prompt engineering enhances precision, particularly for questions lacking explicit answers in provided paragraphs. ChatGPT excels at simpler factual questions compared to \"how\" and \"why\" question types. The evaluation highlights occurrences of hallucinations, where ChatGPT provides responses to questions without available answers in the provided context",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hossein Bahak",
      "Farzaneh Taheri",
      "Zahra Zojaji",
      "Arefeh Kazemi"
    ]
  },
  "http://arxiv.org/abs/2312.06188": {
    "title": "From Ultra-Fine to Fine: Fine-tuning Ultra-Fine Entity Typing Models to Fine-grained",
    "volume": "Dec",
    "abstract": "For the task of fine-grained entity typing (FET), due to the use of a large number of entity types, it is usually considered too costly to manually annotating a training dataset that contains an ample number of examples for each type. A common way to address this problem is to use distantly annotated training data that contains incorrect labels. However, the performance of models trained solely with such data can be limited by the errors in the automatic annotation. Recently, there are a few approaches that no longer follow this conventional way. But without using sufficient direct entity typing supervision may also cause them to yield inferior performance. In this paper, we propose a new approach that can avoid the need of creating distantly labeled data whenever there is a new type schema. We first train an entity typing model that have an extremely board type coverage by using the ultra-fine entity typing data. Then, when there is a need to produce a model for a newly designed fine-grained entity type schema. We can simply fine-tune the previously trained model with a small number of examples annotated under this schema. Experimental results show that our approach achieves outstanding performance for FET under the few-shot setting. It can also outperform state-of-the-art weak supervision based methods after fine-tuning the model with only a small size manually annotated training set",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongliang Dai",
      "Ziqian Zeng"
    ]
  },
  "http://arxiv.org/abs/2312.06185": {
    "title": "KnowGPT: Black-Box Knowledge Injection for Large Language Models",
    "volume": "Dec",
    "abstract": "Generative Large Language Models (LLMs), such as ChatGPT, offer interactive APIs that can answer common questions at a human-expert level. However, these models often give inaccurate or incorrect responses when faced with questions requiring domain-specific or professional-specific knowledge not covered in their training corpus. Furthermore, many state-of-the-art LLMs are not open-source, making it challenging to inject knowledge with model APIs only. In this work, we introduce KnowGPT, a black-box knowledge injection framework for LLMs in question answering. KnowGPT leverages deep reinforcement learning (RL) to extract relevant knowledge from Knowledge Graphs (KGs) and use Multi-Armed Bandit (MAB) to construct the most suitable prompt for each question. Our extensive experiments on three benchmark datasets showcase that KnowGPT significantly enhances the existing methods. Notably, KnowGPT achieves an average improvement of 23.7% over ChatGPT and an average improvement of 2.9% over GPT-4. Additionally, KnowGPT attains a 91.6% accuracy on the OpenbookQA official leaderboard, which is comparable to human-level performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinggang Zhang",
      "Junnan Dong",
      "Hao Chen",
      "Xiao Huang",
      "Daochen Zha",
      "Zailiang Yu"
    ]
  },
  "http://arxiv.org/abs/2312.07589": {
    "title": "ConvD: Attention Enhanced Dynamic Convolutional Embeddings for Knowledge Graph Completion",
    "volume": "Dec",
    "abstract": "Knowledge graphs generally suffer from incompleteness, which can be alleviated by completing the missing information. Deep knowledge convolutional embedding models based on neural networks are currently popular methods for knowledge graph completion. However, most existing methods use external convolution kernels and traditional plain convolution processes, which limits the feature interaction capability of the model. In this paper, we propose a novel dynamic convolutional embedding model ConvD for knowledge graph completion, which directly reshapes the relation embeddings into multiple internal convolution kernels to improve the external convolution kernels of the traditional convolutional embedding model. The internal convolution kernels can effectively augment the feature interaction between the relation embeddings and entity embeddings, thus enhancing the model embedding performance. Moreover, we design a priori knowledge-optimized attention mechanism, which can assign different contribution weight coefficients to multiple relation convolution kernels for dynamic convolution to improve the expressiveness of the model further. Extensive experiments on various datasets show that our proposed model consistently outperforms the state-of-the-art baseline methods, with average improvements ranging from 11.30\\% to 16.92\\% across all model evaluation metrics. Ablation experiments verify the effectiveness of each component module of the ConvD model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbin Guo",
      "Zhao Li",
      "Xin Wang",
      "Zirui Chen"
    ]
  },
  "http://arxiv.org/abs/2312.06172": {
    "title": "Decoupling SQL Query Hardness Parsing for Text-to-SQL",
    "volume": "Dec",
    "abstract": "The fundamental goal of the Text-to-SQL task is to translate natural language question into SQL query. Current research primarily emphasizes the information coupling between natural language questions and schemas, and significant progress has been made in this area. The natural language questions as the primary task requirements source determines the hardness of correspond SQL queries, the correlation between the two always be ignored. However, when the correlation between questions and queries was decoupled, it may simplify the task. In this paper, we introduce an innovative framework for Text-to-SQL based on decoupling SQL query hardness parsing. This framework decouples the Text-to-SQL task based on query hardness by analyzing questions and schemas, simplifying the multi-hardness task into a single-hardness challenge. This greatly reduces the parsing pressure on the language model. We evaluate our proposed framework and achieve a new state-of-the-art performance of fine-turning methods on Spider dev",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawen Yi",
      "Guo Chen"
    ]
  },
  "http://arxiv.org/abs/2312.06149": {
    "title": "Unlocking Anticipatory Text Generation: A Constrained Approach for Faithful Decoding with Large Language Models",
    "volume": "Dec",
    "abstract": "Large Language Models (LLMs) have demonstrated a powerful ability for text generation. However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired behaviors such as toxicity or hallucinations can manifest. While much larger models (e.g., ChatGPT) may demonstrate strength in mitigating these issues, there is still no guarantee of complete prevention. In this work, we propose formalizing text generation as a future-constrained generation problem to minimize undesirable behaviors and enforce faithfulness to instructions. The estimation of future constraint satisfaction, accomplished using LLMs, guides the text generation process. Our extensive experiments demonstrate the effectiveness of the proposed approach across three distinct text generation tasks: keyword-constrained generation (Lin et al., 2020), toxicity reduction (Gehman et al., 2020), and factual correctness in question-answering (Gao et al., 2023)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lifu Tu",
      "Semih Yavuz",
      "Jin Qu",
      "Jiacheng Xu",
      "Rui Meng",
      "Caiming Xiong",
      "Yingbo Zhou"
    ]
  },
  "http://arxiv.org/abs/2312.06147": {
    "title": "What's important here?\": Opportunities and Challenges of Using LLMs in Retrieving Information from Web Interfaces",
    "volume": "Dec",
    "abstract": "Large language models (LLMs) that have been trained on a corpus that includes large amount of code exhibit a remarkable ability to understand HTML code. As web interfaces are primarily constructed using HTML, we design an in-depth study to see how LLMs can be used to retrieve and locate important elements for a user given query (i.e. task description) in a web interface. In contrast with prior works, which primarily focused on autonomous web navigation, we decompose the problem as an even atomic operation - Can LLMs identify the important information in the web page for a user given query? This decomposition enables us to scrutinize the current capabilities of LLMs and uncover the opportunities and challenges they present. Our empirical experiments show that while LLMs exhibit a reasonable level of performance in retrieving important UI elements, there is still a substantial room for improvement. We hope our investigation will inspire follow-up works in overcoming the current challenges in this domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Faria Huq",
      "Jeffrey P. Bigham",
      "Nikolas Martelaro"
    ]
  },
  "http://arxiv.org/abs/2312.06134": {
    "title": "Order Matters in the Presence of Dataset Imbalance for Multilingual Learning",
    "volume": "Dec",
    "abstract": "In this paper, we empirically study the optimization dynamics of multi-task learning, particularly focusing on those that govern a collection of tasks with significant data imbalance. We present a simple yet effective method of pre-training on high-resource tasks, followed by fine-tuning on a mixture of high/low-resource tasks. We provide a thorough empirical study and analysis of this method's benefits showing that it achieves consistent improvements relative to the performance trade-off profile of standard static weighting. We analyze under what data regimes this method is applicable and show its improvements empirically in neural machine translation (NMT) and multi-lingual language modeling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dami Choi",
      "Derrick Xin",
      "Hamid Dadkhahi",
      "Justin Gilmer",
      "Ankush Garg",
      "Orhan Firat",
      "Chih-Kuan Yeh",
      "Andrew M. Dai",
      "Behrooz Ghorbani"
    ]
  },
  "http://arxiv.org/abs/2312.06122": {
    "title": "GTA: Gated Toxicity Avoidance for LM Performance Preservation",
    "volume": "Dec",
    "abstract": "Caution: This paper includes offensive words that could potentially cause unpleasantness. The fast-paced evolution of generative language models such as GPT-4 has demonstrated outstanding results in various NLP generation tasks. However, due to the potential generation of offensive words related to race or gender, various Controllable Text Generation (CTG) methods have been proposed to mitigate the occurrence of harmful words. However, existing CTG methods not only reduce toxicity but also negatively impact several aspects of the language model's generation performance, including topic consistency, grammar, and perplexity. This paper explores the limitations of previous methods and introduces a novel solution in the form of a simple Gated Toxicity Avoidance (GTA) that can be applied to any CTG method. We also evaluate the effectiveness of the proposed GTA by comparing it with state-of-the-art CTG methods across various datasets. Our findings reveal that gated toxicity avoidance efficiently achieves comparable levels of toxicity reduction to the original CTG methods while preserving the generation performance of the language model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heegyu Kim",
      "Hyunsouk Cho"
    ]
  },
  "http://arxiv.org/abs/2312.06099": {
    "title": "Generative Large Language Models Are All-purpose Text Analytics Engines: Text-to-text Learning Is All Your Need",
    "volume": "Dec",
    "abstract": "Objective To solve major clinical natural language processing (NLP) tasks using a unified text-to-text learning architecture based on a generative large language model (LLM) via prompt tuning. Methods We formulated 7 key clinical NLP tasks as text-to-text learning and solved them using one unified generative clinical LLM, GatorTronGPT, developed using GPT-3 architecture and trained with up to 20 billion parameters. We adopted soft prompts (i.e., trainable vectors) with frozen LLM, where the LLM parameters were not updated (i.e., frozen) and only the vectors of soft prompts were updated, known as prompt tuning. We added additional soft prompts as a prefix to the input layer, which were optimized during the prompt tuning. We evaluated the proposed method using 7 clinical NLP tasks and compared them with previous task-specific solutions based on Transformer models. Results and Conclusion The proposed approach achieved state-of-the-art performance for 5 out of 7 major clinical NLP tasks using one unified generative LLM. Our approach outperformed previous task-specific transformer models by ~3% for concept extraction and 7% for relation extraction applied to social determinants of health, 3.4% for clinical concept normalization, 3.4~10% for clinical abbreviation disambiguation, and 5.5~9% for natural language inference. Our approach also outperformed a previously developed prompt-based machine reading comprehension (MRC) model, GatorTron-MRC, for clinical concept and relation extraction. The proposed approach can deliver the ``one model for all`` promise from training to deployment using a unified generative LLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Peng",
      "Xi Yang",
      "Aokun Chen",
      "Zehao Yu",
      "Kaleb E Smith",
      "Anthony B Costa",
      "Mona G Flores",
      "Jiang Bian",
      "Yonghui Wu"
    ]
  },
  "http://arxiv.org/abs/2312.06094": {
    "title": "MATK: The Meme Analytical Tool Kit",
    "volume": "Dec",
    "abstract": "The rise of social media platforms has brought about a new digital culture called memes. Memes, which combine visuals and text, can strongly influence public opinions on social and cultural issues. As a result, people have become interested in categorizing memes, leading to the development of various datasets and multimodal models that show promising results in this field. However, there is currently a lack of a single library that allows for the reproduction, evaluation, and comparison of these models using fair benchmarks and settings. To fill this gap, we introduce the Meme Analytical Tool Kit (MATK), an open-source toolkit specifically designed to support existing memes datasets and cutting-edge multimodal models. MATK aims to assist researchers and engineers in training and reproducing these multimodal models for meme classification tasks, while also providing analysis techniques to gain insights into their strengths and weaknesses. To access MATK, please visit \\url{https://github.com/Social-AI-Studio/MATK}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Shan Hee",
      "Aditi Kumaresan",
      "Nguyen Khoi Hoang",
      "Nirmalendu Prakash",
      "Rui Cao",
      "Roy Ka-Wei Lee"
    ]
  },
  "http://arxiv.org/abs/2312.06093": {
    "title": "PromptMTopic: Unsupervised Multimodal Topic Modeling of Memes using Large Language Models",
    "volume": "Dec",
    "abstract": "The proliferation of social media has given rise to a new form of communication: memes. Memes are multimodal and often contain a combination of text and visual elements that convey meaning, humor, and cultural significance. While meme analysis has been an active area of research, little work has been done on unsupervised multimodal topic modeling of memes, which is important for content moderation, social media analysis, and cultural studies. We propose \\textsf{PromptMTopic}, a novel multimodal prompt-based model designed to learn topics from both text and visual modalities by leveraging the language modeling capabilities of large language models. Our model effectively extracts and clusters topics learned from memes, considering the semantic interaction between the text and visual modalities. We evaluate our proposed model through extensive experiments on three real-world meme datasets, which demonstrate its superiority over state-of-the-art topic modeling baselines in learning descriptive topics in memes. Additionally, our qualitative analysis shows that \\textsf{PromptMTopic} can identify meaningful and culturally relevant topics from memes. Our work contributes to the understanding of the topics and themes of memes, a crucial form of communication in today's society.\\\\ \\red{\\textbf{Disclaimer: This paper contains sensitive content that may be disturbing to some readers.}}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nirmalendu Prakash",
      "Han Wang",
      "Nguyen Khoi Hoang",
      "Ming Shan Hee",
      "Roy Ka-Wei Lee"
    ]
  },
  "http://arxiv.org/abs/2312.06722": {
    "title": "EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal Large Language Models",
    "volume": "Dec",
    "abstract": "Multimodal Large Language Models (MLLMs), building upon the powerful Large Language Models (LLMs) with exceptional reasoning and generalization capability, have opened up new avenues for embodied task planning. MLLMs excel in their ability to integrate diverse environmental inputs, such as real-time task progress, visual observations, and open-form language instructions, which are crucial for executable task planning. In this work, we introduce a benchmark with human annotations, EgoPlan-Bench, to quantitatively investigate the potential of MLLMs as embodied task planners in real-world scenarios. Our benchmark is distinguished by realistic tasks derived from real-world videos, a diverse set of actions involving interactions with hundreds of different objects, and complex visual observations from varied environments. We evaluate various open-source MLLMs, revealing that these models have not yet evolved into embodied planning generalists (even GPT-4V). We further construct an instruction-tuning dataset EgoPlan-IT from videos of human-object interactions, to facilitate the learning of high-level task planning in intricate real-world situations. The experiment results demonstrate that the model tuned on EgoPlan-IT not only significantly improves performance on our benchmark, but also effectively acts as embodied planner in simulations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Chen",
      "Yuying Ge",
      "Yixiao Ge",
      "Mingyu Ding",
      "Bohao Li",
      "Rui Wang",
      "Ruifeng Xu",
      "Ying Shan",
      "Xihui Liu"
    ]
  },
  "http://arxiv.org/abs/2312.06088": {
    "title": "SECNN: Squeeze-and-Excitation Convolutional Neural Network for Sentence Classification",
    "volume": "Dec",
    "abstract": "Sentence classification is one of the basic tasks of natural language processing. Convolution neural network (CNN) has the ability to extract n-grams features through convolutional filters and capture local correlations between consecutive words in parallel, so CNN is a popular neural network architecture to dealing with the task. But restricted by the width of convolutional filters, it is difficult for CNN to capture long term contextual dependencies. Attention is a mechanism that considers global information and pays more attention to keywords in sentences, thus attention mechanism is cooperated with CNN network to improve performance in sentence classification task. In our work, we don't focus on keyword in a sentence, but on which CNN's output feature map is more important. We propose a Squeeze-and-Excitation Convolutional neural Network (SECNN) for sentence classification. SECNN takes the feature maps from multiple CNN as different channels of sentence representation, and then, we can utilize channel attention mechanism, that is SE attention mechanism, to enable the model to learn the attention weights of different channel features. The results show that our model achieves advanced performance in the sentence classification task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shandong Yuan"
    ]
  },
  "http://arxiv.org/abs/2312.06056": {
    "title": "METAL: Metamorphic Testing Framework for Analyzing Large-Language Model Qualities",
    "volume": "Dec",
    "abstract": "Large-Language Models (LLMs) have shifted the paradigm of natural language data processing. However, their black-boxed and probabilistic characteristics can lead to potential risks in the quality of outputs in diverse LLM applications. Recent studies have tested Quality Attributes (QAs), such as robustness or fairness, of LLMs by generating adversarial input texts. However, existing studies have limited their coverage of QAs and tasks in LLMs and are difficult to extend. Additionally, these studies have only used one evaluation metric, Attack Success Rate (ASR), to assess the effectiveness of their approaches. We propose a MEtamorphic Testing for Analyzing LLMs (METAL) framework to address these issues by applying Metamorphic Testing (MT) techniques. This approach facilitates the systematic testing of LLM qualities by defining Metamorphic Relations (MRs), which serve as modularized evaluation metrics. The METAL framework can automatically generate hundreds of MRs from templates that cover various QAs and tasks. In addition, we introduced novel metrics that integrate the ASR method into the semantic qualities of text to assess the effectiveness of MRs accurately. Through the experiments conducted with three prominent LLMs, we have confirmed that the METAL framework effectively evaluates essential QAs on primary LLM tasks and reveals the quality risks in LLMs. Moreover, the newly proposed metrics can guide the optimal MRs for testing each task and suggest the most effective method for generating MRs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangwon Hyun",
      "Mingyu Guo",
      "M. Ali Babar"
    ]
  },
  "http://arxiv.org/abs/2312.06053": {
    "title": "IEKG: A Commonsense Knowledge Graph for Idiomatic Expressions",
    "volume": "Dec",
    "abstract": "Idiomatic expression (IE) processing and comprehension have challenged pre-trained language models (PTLMs) because their meanings are non-compositional. Unlike prior works that enable IE comprehension through fine-tuning PTLMs with sentences containing IEs, in this work, we construct IEKG, a commonsense knowledge graph for figurative interpretations of IEs. This extends the established ATOMIC2020 graph, converting PTLMs into knowledge models (KMs) that encode and infer commonsense knowledge related to IE use. Experiments show that various PTLMs can be converted into KMs with IEKG. We verify the quality of IEKG and the ability of the trained KMs with automatic and human evaluation. Through applications in natural language understanding, we show that a PTLM injected with knowledge from IEKG exhibits improved IE comprehension ability and can generalize to IEs unseen during training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziheng Zeng",
      "Kellen Tan Cheng",
      "Srihari Venkat Nanniyur",
      "Jianing Zhou",
      "Suma Bhat"
    ]
  },
  "http://arxiv.org/abs/2312.06022": {
    "title": "Exploiting Representation Bias for Data Distillation in Abstractive Text Summarization",
    "volume": "Dec",
    "abstract": "Abstractive text summarization is surging with the number of training samples to cater to the needs of the deep learning models. These models tend to exploit the training data representations to attain superior performance by improving the quantitative element of the resultant summary. However, increasing the size of the training set may not always be the ideal solution to maximize the performance, and therefore, a need to revisit the quality of training samples and the learning protocol of deep learning models is a must. In this paper, we aim to discretize the vector space of the abstractive text summarization models to understand the characteristics learned between the input embedding space and the models' encoder space. We show that deep models fail to capture the diversity of the input space. Further, the distribution of data points on the encoder space indicates that an unchecked increase in the training samples does not add value; rather, a tear-down of data samples is highly needed to make the models focus on variability and faithfulness. We employ clustering techniques to learn the diversity of a model's sample space and how data points are mapped from the embedding space to the encoder space and vice versa. Further, we devise a metric to filter out redundant data points to make the model more robust and less data hungry. We benchmark our proposed method using quantitative metrics, such as Rouge, and qualitative metrics, such as BERTScore, FEQA and Pyramid score. We also quantify the reasons that inhibit the models from learning the diversity from the varied input samples",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yash Kumar Atri",
      "Vikram Goyal",
      "Tanmoy Chakraborty"
    ]
  },
  "http://arxiv.org/abs/2312.06002": {
    "title": "Large Language Models on Lexical Semantic Change Detection: An Evaluation",
    "volume": "Dec",
    "abstract": "Lexical Semantic Change Detection stands out as one of the few areas where Large Language Models (LLMs) have not been extensively involved. Traditional methods like PPMI, and SGNS remain prevalent in research, alongside newer BERT-based approaches. Despite the comprehensive coverage of various natural language processing domains by LLMs, there is a notable scarcity of literature concerning their application in this specific realm. In this work, we seek to bridge this gap by introducing LLMs into the domain of Lexical Semantic Change Detection. Our work presents novel prompting solutions and a comprehensive evaluation that spans all three generations of language models, contributing to the exploration of LLMs in this research area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiyu Wang",
      "Matthew Choi"
    ]
  },
  "http://arxiv.org/abs/2312.05990": {
    "title": "Constructing Vec-tionaries to Extract Latent Message Features from Texts: A Case Study of Moral Appeals",
    "volume": "Dec",
    "abstract": "While communication research frequently studies latent message features like moral appeals, their quantification remains a challenge. Conventional human coding struggles with scalability and intercoder reliability. While dictionary-based methods are cost-effective and computationally efficient, they often lack contextual sensitivity and are limited by the vocabularies developed for the original applications. In this paper, we present a novel approach to construct vec-tionary measurement tools that boost validated dictionaries with word embeddings through nonlinear optimization. By harnessing semantic relationships encoded by embeddings, vec-tionaries improve the measurement of latent message features by expanding the applicability of original vocabularies to other contexts. Vec-tionaries can also help extract semantic information from texts, especially those in short format, beyond the original vocabulary of a dictionary. Importantly, a vec-tionary can produce additional metrics to capture the valence and ambivalence of a latent feature beyond its strength in texts. Using moral appeals in COVID-19-related tweets as a case study, we illustrate the steps to construct the moral foundations vec-tionary, showcasing its ability to process posts missed by dictionary methods and to produce measurements better aligned with crowdsourced human assessments. Furthermore, additional metrics from the moral foundations vec-tionary unveiled unique insights that facilitated predicting outcomes such as message retransmission",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zening Duan",
      "Anqi Shao",
      "Yicheng Hu",
      "Heysung Lee",
      "Xining Liao",
      "Yoo Ji Suh",
      "Jisoo Kim",
      "Kai-Cheng Yang",
      "Kaiping Chen",
      "Sijia Yang"
    ]
  },
  "http://arxiv.org/abs/2312.05979": {
    "title": "NovaCOMET: Open Commonsense Foundation Models with Symbolic Knowledge Distillation",
    "volume": "Dec",
    "abstract": "We present NovaCOMET, an open commonsense knowledge model, that combines the best aspects of knowledge and general task models. Compared to previous knowledge models, NovaCOMET allows open-format relations enabling direct application to reasoning tasks; compared to general task models like Flan-T5, it explicitly centers knowledge, enabling superior performance for commonsense reasoning. NovaCOMET leverages the knowledge of opaque proprietary models to create an open knowledge pipeline. First, knowledge is symbolically distilled into NovATOMIC, a publicly-released discrete knowledge graph which can be audited, critiqued, and filtered. Next, we train NovaCOMET on NovATOMIC by fine-tuning an open-source pretrained model. NovaCOMET uses an open-format training objective, replacing the fixed relation sets of past knowledge models, enabling arbitrary structures within the data to serve as inputs or outputs. The resulting generation model, optionally augmented with human annotation, matches or exceeds comparable open task models like Flan-T5 on a range of commonsense generation tasks. NovaCOMET serves as a counterexample to the contemporary focus on instruction tuning only, demonstrating a distinct advantage to explicitly modeling commonsense knowledge as well",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter West",
      "Ronan Le Bras",
      "Taylor Sorensen",
      "Bill Yuchen Lin",
      "Liwei Jiang",
      "Ximing Lu",
      "Khyathi Chandu",
      "Jack Hessel",
      "Ashutosh Baheti",
      "Chandra Bhagavatula",
      "Yejin Choi"
    ]
  },
  "http://arxiv.org/abs/2312.05964": {
    "title": "ConSequence: Synthesizing Logically Constrained Sequences for Electronic Health Record Generation",
    "volume": "Dec",
    "abstract": "Generative models can produce synthetic patient records for analytical tasks when real data is unavailable or limited. However, current methods struggle with adhering to domain-specific knowledge and removing invalid data. We present ConSequence, an effective approach to integrating domain knowledge into sequential generative neural network outputs. Our rule-based formulation includes temporal aggregation and antecedent evaluation modules, ensured by an efficient matrix multiplication formulation, to satisfy hard and soft logical constraints across time steps. Existing constraint methods often fail to guarantee constraint satisfaction, lack the ability to handle temporal constraints, and hinder the learning and computational efficiency of the model. In contrast, our approach efficiently handles all types of constraints with guaranteed logical coherence. We demonstrate ConSequence's effectiveness in generating electronic health records, outperforming competitors in achieving complete temporal and spatial constraint satisfaction without compromising runtime performance or generative quality. Specifically, ConSequence successfully prevents all rule violations while improving the model quality in reducing its test perplexity by 5% and incurring less than a 13% slowdown in generation speed compared to an unconstrained model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brandon Theodorou",
      "Shrusti Jain",
      "Cao Xiao",
      "Jimeng Sun"
    ]
  },
  "http://arxiv.org/abs/2312.05934": {
    "title": "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs",
    "volume": "Dec",
    "abstract": "Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oded Ovadia",
      "Menachem Brief",
      "Moshik Mishaeli",
      "Oren Elisha"
    ]
  },
  "http://arxiv.org/abs/2312.07573": {
    "title": "Arabic Handwritten Text Line Dataset",
    "volume": "Dec",
    "abstract": "Segmentation of Arabic manuscripts into lines of text and words is an important step to make recognition systems more efficient and accurate. The problem of segmentation into text lines is solved since there are carefully annotated dataset dedicated to this task. However, To the best of our knowledge, there are no dataset annotating the word position of Arabic texts. In this paper, we present a new dataset specifically designed for historical Arabic script in which we annotate position in word level",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hakim Bouchal",
      "Ahror Belaid"
    ]
  },
  "http://arxiv.org/abs/2312.06705": {
    "title": "Perceiving University Student's Opinions from Google App Reviews",
    "volume": "Dec",
    "abstract": "Google app market captures the school of thought of users from every corner of the globe via ratings and text reviews, in a multilinguistic arena. The potential information from the reviews cannot be extracted manually, due to its exponential growth. So, Sentiment analysis, by machine learning and deep learning algorithms employing NLP, explicitly uncovers and interprets the emotions. This study performs the sentiment classification of the app reviews and identifies the university student's behavior towards the app market via exploratory analysis. We applied machine learning algorithms using the TP, TF, and TF IDF text representation scheme and evaluated its performance on Bagging, an ensemble learning method. We used word embedding, Glove, on the deep learning paradigms. Our model was trained on Google app reviews and tested on Student's App Reviews(SAR). The various combinations of these algorithms were compared amongst each other using F score and accuracy and inferences were highlighted graphically. SVM, amongst other classifiers, gave fruitful accuracy(93.41%), F score(89%) on bigram and TF IDF scheme. Bagging enhanced the performance of LR and NB with accuracy of 87.88% and 86.69% and F score of 86% and 78% respectively. Overall, LSTM on Glove embedding recorded the highest accuracy(95.2%) and F score(88%)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sakshi Ranjan",
      "Subhankar Mishra"
    ]
  },
  "http://arxiv.org/abs/2312.05842": {
    "title": "Mutual Enhancement of Large and Small Language Models with Cross-Silo Knowledge Transfer",
    "volume": "Dec",
    "abstract": "While large language models (LLMs) are empowered with broad knowledge, their task-specific performance is often suboptimal. It necessitates fine-tuning LLMs with task-specific data, but such data may be inaccessible due to privacy concerns. In this paper, we propose a novel approach to enhance LLMs with smaller language models (SLMs) that are trained on clients using their private task-specific data. To enable mutual enhancement between LLMs and SLMs, we propose CrossLM, where the SLMs promote the LLM to generate task-specific high-quality data, and both the LLM and SLMs are enhanced with the generated data. We evaluate CrossLM using publicly accessible language models across a range of benchmark tasks. The results demonstrate that CrossLM significantly enhances the task-specific performance of SLMs on clients and the LLM on the cloud server simultaneously while preserving the LLM's generalization capability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongheng Deng",
      "Ziqing Qiao",
      "Ju Ren",
      "Yang Liu",
      "Yaoxue Zhang"
    ]
  },
  "http://arxiv.org/abs/2312.05834": {
    "title": "Evidence-based Interpretable Open-domain Fact-checking with Large Language Models",
    "volume": "Dec",
    "abstract": "Universal fact-checking systems for real-world claims face significant challenges in gathering valid and sufficient real-time evidence and making reasoned decisions. In this work, we introduce the Open-domain Explainable Fact-checking (OE-Fact) system for claim-checking in real-world scenarios. The OE-Fact system can leverage the powerful understanding and reasoning capabilities of large language models (LLMs) to validate claims and generate causal explanations for fact-checking decisions. To adapt the traditional three-module fact-checking framework to the open domain setting, we first retrieve claim-related information as relevant evidence from open websites. After that, we retain the evidence relevant to the claim through LLM and similarity calculation for subsequent verification. We evaluate the performance of our adapted three-module OE-Fact system on the Fact Extraction and Verification (FEVER) dataset. Experimental results show that our OE-Fact system outperforms general fact-checking baseline systems in both closed- and open-domain scenarios, ensuring stable and accurate verdicts while providing concise and convincing real-time explanations for fact-checking decisions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Tan",
      "Bowei Zou",
      "Ai Ti Aw"
    ]
  },
  "http://arxiv.org/abs/2312.05821": {
    "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing Large Language Models",
    "volume": "Dec",
    "abstract": "This paper explores a new post-hoc training-free compression paradigm for compressing Large Language Models (LLMs) to facilitate their wider adoption in various computing environments. We delve into the challenges of LLM compression, notably their dependency on extensive training data and computational resources. We propose a training-free approach dubbed Activation-aware Singular Value Decomposition (ASVD) to address these limitations. ASVD effectively manages activation outliers by adjusting the weight matrix based on the activation distribution, improving decomposition accuracy and efficiency. Our method also addresses the varying sensitivity of different LLM layers to decomposition, with an iterative calibration process for optimal layer-specific decomposition. Experiments demonstrate that ASVD can compress network by 10%-20% without losing reasoning capacities. Additionally, it can be seamlessly integrated with other LLM compression paradigms, showcasing its flexible compatibility. Code and compressed models are available at https://github.com/hahnyuan/ASVD4LLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihang Yuan",
      "Yuzhang Shang",
      "Yue Song",
      "Qiang Wu",
      "Yan Yan",
      "Guangyu Sun"
    ]
  },
  "http://arxiv.org/abs/2312.05762": {
    "title": "Multi-Defendant Legal Judgment Prediction via Hierarchical Reasoning",
    "volume": "Dec",
    "abstract": "Multiple defendants in a criminal fact description generally exhibit complex interactions, and cannot be well handled by existing Legal Judgment Prediction (LJP) methods which focus on predicting judgment results (e.g., law articles, charges, and terms of penalty) for single-defendant cases. To address this problem, we propose the task of multi-defendant LJP, which aims to automatically predict the judgment results for each defendant of multi-defendant cases. Two challenges arise with the task of multi-defendant LJP: (1) indistinguishable judgment results among various defendants; and (2) the lack of a real-world dataset for training and evaluation. To tackle the first challenge, we formalize the multi-defendant judgment process as hierarchical reasoning chains and introduce a multi-defendant LJP method, named Hierarchical Reasoning Network (HRN), which follows the hierarchical reasoning chains to determine criminal relationships, sentencing circumstances, law articles, charges, and terms of penalty for each defendant. To tackle the second challenge, we collect a real-world multi-defendant LJP dataset, namely MultiLJP, to accelerate the relevant research in the future. Extensive experiments on MultiLJP verify the effectiveness of our proposed HRN",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yougang Lyu",
      "Jitai Hao",
      "Zihan Wang",
      "Kai Zhao",
      "Shen Gao",
      "Pengjie Ren",
      "Zhumin Chen",
      "Fang Wang",
      "Zhaochun Ren"
    ]
  },
  "http://arxiv.org/abs/2312.05741": {
    "title": "MISCA: A Joint Model for Multiple Intent Detection and Slot Filling with Intent-Slot Co-Attention",
    "volume": "Dec",
    "abstract": "The research study of detecting multiple intents and filling slots is becoming more popular because of its relevance to complicated real-world situations. Recent advanced approaches, which are joint models based on graphs, might still face two potential issues: (i) the uncertainty introduced by constructing graphs based on preliminary intents and slots, which may transfer intent-slot correlation information to incorrect label node destinations, and (ii) direct incorporation of multiple intent labels for each token w.r.t. token-level intent voting might potentially lead to incorrect slot predictions, thereby hurting the overall performance. To address these two issues, we propose a joint model named MISCA. Our MISCA introduces an intent-slot co-attention mechanism and an underlying layer of label attention mechanism. These mechanisms enable MISCA to effectively capture correlations between intents and slot labels, eliminating the need for graph construction. They also facilitate the transfer of correlation information in both directions: from intents to slots and from slots to intents, through multiple levels of label-specific representations, without relying on token-level intent information. Experimental results show that MISCA outperforms previous models, achieving new state-of-the-art overall accuracy performances on two benchmark datasets MixATIS and MixSNIPS. This highlights the effectiveness of our attention mechanisms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thinh Pham",
      "Chi Tran",
      "Dat Quoc Nguyen"
    ]
  },
  "http://arxiv.org/abs/2312.05720": {
    "title": "Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning",
    "volume": "Dec",
    "abstract": "Federated learning (FL) emphasizes decentralized training by storing data locally and sending only model updates, underlining user privacy. Recently, a line of works on privacy attacks impairs user privacy by extracting sensitive training text from language models in the context of FL. Yet, these attack techniques face distinct hurdles: some work chiefly with limited batch sizes (e.g., batch size of 1), and others are easily detectable. This paper introduces an innovative approach that is challenging to detect, significantly enhancing the recovery rate of text in various batch-size settings. Building on fundamental gradient matching and domain prior knowledge, we enhance the attack by recovering the input of the Pooler layer of language models, which enables us to provide additional supervised signals at the feature level. Unlike gradient data, these signals do not average across sentences and tokens, thereby offering more nuanced and effective insights. We benchmark our method using text classification tasks on datasets such as CoLA, SST-2, and Rotten Tomatoes. Across different batch sizes and models, our approach consistently outperforms previous state-of-the-art results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianwei Li",
      "Sheng Liu",
      "Qi Lei"
    ]
  },
  "http://arxiv.org/abs/2312.05693": {
    "title": "Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs on the Edge",
    "volume": "Dec",
    "abstract": "Large Language Models (LLMs) stand out for their impressive performance in intricate language modeling tasks. However, their demanding computational and memory needs pose obstacles for broad use on edge devices. Quantization is then introduced to boost LLMs' on-device efficiency. Recent works show that 8-bit or lower weight quantization is feasible with minimal impact on end-to-end task performance, while the activation is still not quantized. On the other hand, mainstream commodity edge devices still struggle to execute these sub-8-bit quantized networks effectively. In this paper, we propose Agile-Quant, an activation-guided quantization framework for popular Large Language Models (LLMs), and implement an end-to-end accelerator on multiple edge devices for faster inference. Considering the hardware profiling and activation analysis, we first introduce a basic activation quantization strategy to balance the trade-off of task performance and real inference speed. Then we leverage the activation-aware token pruning technique to reduce the outliers and the adverse impact on attentivity. Ultimately, we utilize the SIMD-based 4-bit multiplier and our efficient TRIP matrix multiplication to implement the accelerator for LLMs on the edge. We apply our framework on different scales of LLMs including LLaMA, OPT, and BLOOM with 4-bit or 8-bit for the activation and 4-bit for the weight quantization. Experiments show that Agile-Quant achieves simultaneous quantization of model weights and activations while maintaining task performance comparable to existing weight-only quantization methods. Moreover, in the 8- and 4-bit scenario, Agile-Quant achieves an on-device speedup of up to 2.55x compared to its FP16 counterparts across multiple edge devices, marking a pioneering advancement in this domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Shen",
      "Peiyan Dong",
      "Lei Lu",
      "Zhenglun Kong",
      "Zhengang Li",
      "Ming Lin",
      "Chao Wu",
      "Yanzhi Wang"
    ]
  },
  "http://arxiv.org/abs/2312.05688": {
    "title": "NLLG Quarterly arXiv Report 09/23: What are the most influential current AI Papers?",
    "volume": "Dec",
    "abstract": "Artificial Intelligence (AI) has witnessed rapid growth, especially in the subfields Natural Language Processing (NLP), Machine Learning (ML) and Computer Vision (CV). Keeping pace with this rapid progress poses a considerable challenge for researchers and professionals in the field. In this arXiv report, the second of its kind, which covers the period from January to September 2023, we aim to provide insights and analysis that help navigate these dynamic areas of AI. We accomplish this by 1) identifying the top-40 most cited papers from arXiv in the given period, comparing the current top-40 papers to the previous report, which covered the period January to June; 2) analyzing dataset characteristics and keyword popularity; 3) examining the global sectoral distribution of institutions to reveal differences in engagement across geographical areas. Our findings highlight the continued dominance of NLP: while only 16% of all submitted papers have NLP as primary category (more than 25% have CV and ML as primary category), 50% of the most cited papers have NLP as primary category, 90% of which target LLMs. Additionally, we show that i) the US dominates among both top-40 and top-9k papers, followed by China; ii) Europe clearly lags behind and is hardly represented in the top-40 most cited papers; iii) US industry is largely overrepresented in the top-40 most influential papers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ran Zhang",
      "Aida Kostikova",
      "Christoph Leiter",
      "Jonas Belouadi",
      "Daniil Larionov",
      "Yanran Chen",
      "Vivian Fresen",
      "Steffen Eger"
    ]
  },
  "http://arxiv.org/abs/2312.05677": {
    "title": "Batched Low-Rank Adaptation of Foundation Models",
    "volume": "Dec",
    "abstract": "Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning foundation models by incorporating trainable low-rank matrices, thereby reducing the number of trainable parameters. While LoRA offers numerous advantages, its applicability for real-time serving to a diverse and global user base is constrained by its incapability to handle multiple task-specific adapters efficiently. This imposes a performance bottleneck in scenarios requiring personalized, task-specific adaptations for each incoming request. To mitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which each input example in a minibatch can be associated with its unique low-rank adaptation weights, allowing for efficient batching of heterogeneous requests. We empirically demonstrate that FLoRA retains the performance merits of LoRA, showcasing competitive results on the MultiPL-E code generation benchmark spanning over 8 languages and a multilingual speech recognition task across 6 languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeming Wen",
      "Swarat Chaudhuri"
    ]
  },
  "http://arxiv.org/abs/2312.05671": {
    "title": "Hate Speech and Offensive Content Detection in Indo-Aryan Languages: A Battle of LSTM and Transformers",
    "volume": "Dec",
    "abstract": "Social media platforms serve as accessible outlets for individuals to express their thoughts and experiences, resulting in an influx of user-generated data spanning all age groups. While these platforms enable free expression, they also present significant challenges, including the proliferation of hate speech and offensive content. Such objectionable language disrupts objective discourse and can lead to radicalization of debates, ultimately threatening democratic values. Consequently, organizations have taken steps to monitor and curb abusive behavior, necessitating automated methods for identifying suspicious posts. This paper contributes to Hate Speech and Offensive Content Identification in English and Indo-Aryan Languages (HASOC) 2023 shared tasks track. We, team Z-AGI Labs, conduct a comprehensive comparative analysis of hate speech classification across five distinct languages: Bengali, Assamese, Bodo, Sinhala, and Gujarati. Our study encompasses a wide range of pre-trained models, including Bert variants, XLM-R, and LSTM models, to assess their performance in identifying hate speech across these languages. Results reveal intriguing variations in model performance. Notably, Bert Base Multilingual Cased emerges as a strong performer across languages, achieving an F1 score of 0.67027 for Bengali and 0.70525 for Assamese. At the same time, it significantly outperforms other models with an impressive F1 score of 0.83009 for Bodo. In Sinhala, XLM-R stands out with an F1 score of 0.83493, whereas for Gujarati, a custom LSTM-based model outshined with an F1 score of 0.76601. This study offers valuable insights into the suitability of various pre-trained models for hate speech detection in multilingual settings. By considering the nuances of each, our research contributes to an informed model selection for building robust hate speech detection systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikhil Narayan",
      "Mrutyunjay Biswal",
      "Pramod Goyal",
      "Abhranta Panigrahi"
    ]
  },
  "http://arxiv.org/abs/2312.05662": {
    "title": "Understanding the Effect of Model Compression on Social Bias in Large Language Models",
    "volume": "Dec",
    "abstract": "Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model's predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to mitigate the effects of inappropriate social biases learned during pretraining. Simultaneously, methods for model compression have become increasingly popular to reduce the computational burden of LLMs. Despite the popularity and need for both approaches, little work has been done to explore the interplay between these two. We perform a carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs. Longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gustavo Gonçalves",
      "Emma Strubell"
    ]
  },
  "http://arxiv.org/abs/2312.05640": {
    "title": "Keyword spotting -- Detecting commands in speech using deep learning",
    "volume": "Dec",
    "abstract": "Speech recognition has become an important task in the development of machine learning and artificial intelligence. In this study, we explore the important task of keyword spotting using speech recognition machine learning and deep learning techniques. We implement feature engineering by converting raw waveforms to Mel Frequency Cepstral Coefficients (MFCCs), which we use as inputs to our models. We experiment with several different algorithms such as Hidden Markov Model with Gaussian Mixture, Convolutional Neural Networks and variants of Recurrent Neural Networks including Long Short-Term Memory and the Attention mechanism. In our experiments, RNN with BiLSTM and Attention achieves the best performance with an accuracy of 93.9 %",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sumedha Rai",
      "Tong Li",
      "Bella Lyu"
    ]
  },
  "http://arxiv.org/abs/2312.05621": {
    "title": "PILLOW: Enhancing Efficient Instruction Fine-tuning via Prompt Matching",
    "volume": "Dec",
    "abstract": "Instruction fine-tuning has conventionally been employed to adapt Large Language Models (LLMs) to a variety of tasks. Nonetheless, this technique often necessitates substantial computational resources, making it impractical for deployment by individuals or small-scale entities. Recently, Low-Rank Adaptation (LoRA) has become a promising alternative, offering high capabilities on par with full tuning with reduced resource overhead. However, attaining satisfactory performance through the fine-tuning of LoRA is a non-trivial challenge. In this paper, we propose PILLOW, which aims to improve LoRA's performance by a discrimination-based prompting method, leveraging LLMs' In-Context Learning ability. PILLOW incorporates a matching network that selects prompts from a user-defined prompt pool, concatenates the selected prompts with the user instruction as input, and performs inference using the LoRA-fine-tuned LLMs. Trained with Reinforcement Learning, PILLOW exhibits commensurate performance on various evaluation metrics compared with typical instruction fine-tuning methods, utilizing only consumer-grade GPU resources and exhibiting a large reduction in computational costs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenting Qi",
      "Xiaoyu Tan",
      "Shaojie Shi",
      "Chao Qu",
      "Yinghui Xu",
      "Yuan Qi"
    ]
  },
  "http://arxiv.org/abs/2312.05603": {
    "title": "Sim-GPT: Text Similarity via GPT Annotated Data",
    "volume": "Dec",
    "abstract": "Due to the lack of a large collection of high-quality labeled sentence pairs with textual similarity scores, existing approaches for Semantic Textual Similarity (STS) mostly rely on unsupervised techniques or training signals that are only partially correlated with textual similarity, e.g., NLI-based datasets. To tackle this issue, in this paper, we propose the strategy of measuring text similarity via GPT annotated data (Sim-GPT for short). The core idea of Sim-GPT is to generate data with STS labels using GPT-4, based on which an STS model is trained. Sim-GPT framework utilizes LLMs to provide a substantial amount of reliable annotated data filling the gap of the lack of training signals for STS. Sim-GPT is trained on a one-time generated dataset using BERT or RoBERTa as the backbone, which offers long-term savings in cost and speed compared to repeatedly invoking LLMs for each sentence pair. Trained on the examples from GPT-4 (371K), Sim-GPT yields SOTA performances on the widely-used seven STS benchmarks: +0.99 over supervised-SimCSE, and +0.42 over the current SOTA PromCSE model. To encourage further advancements of the field, we release both models and the 371K annotated examples from GPT-4. Code, models and annotated data are available at: https://github.com/ShuheWang1998/Sim-GPT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuhe Wang",
      "Beiming Cao",
      "Shengyu Zhang",
      "Xiaoya Li",
      "Jiwei Li",
      "Fei Wu",
      "Guoyin Wang",
      "Eduard Hovy"
    ]
  },
  "http://arxiv.org/abs/2312.05585": {
    "title": "Enhancing Medical Specialty Assignment to Patients using NLP Techniques",
    "volume": "Dec",
    "abstract": "The introduction of Large Language Models (LLMs), and the vast volume of publicly available medical data, amplified the application of NLP to the medical domain. However, LLMs are pretrained on data that are not explicitly relevant to the domain that are applied to and are often biased towards the original data they were pretrained upon. Even when pretrained on domainspecific data, these models typically require time-consuming fine-tuning to achieve good performance for a specific task. To address these limitations, we propose an alternative approach that achieves superior performance while being computationally efficient. Specifically, we utilize keywords to train a deep learning architecture that outperforms a language model pretrained on a large corpus of text. Our proposal does not require pretraining nor fine-tuning and can be applied directly to a specific setting for performing multi-label classification. Our objective is to automatically assign a new patient to the specialty of the medical professional they require, using a dataset that contains medical transcriptions and relevant keywords. To this end, we fine-tune the PubMedBERT model on this dataset, which serves as the baseline for our experiments. We then twice train/fine-tune a DNN and the RoBERTa language model, using both the keywords and the full transcriptions as input. We compare the performance of these approaches using relevant metrics. Our results demonstrate that utilizing keywords for text classification significantly improves classification performance, for both a basic DL architecture and a large language model. Our approach represents a promising and efficient alternative to traditional methods for finetuning language models on domain-specific data and has potential applications in various medical domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chris Solomou"
    ]
  },
  "http://arxiv.org/abs/2312.05520": {
    "title": "Augmenty: A Python Library for Structured Text Augmentation",
    "volume": "Dec",
    "abstract": "Augmnety is a Python library for structured text augmentation. It is built on top of spaCy and allows for augmentation of both the text and its annotations. Augmenty provides a wide range of augmenters which can be combined in a flexible manner to create complex augmentation pipelines. It also includes a set of primitives that can be used to create custom augmenters such as word replacement augmenters. This functionality allows for augmentations within a range of applications such as named entity recognition (NER), part-of-speech tagging, and dependency parsing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kenneth Enevoldsen"
    ]
  },
  "http://arxiv.org/abs/2312.05503": {
    "title": "Aligner: One Global Token is Worth Millions of Parameters When Aligning Large Language Models",
    "volume": "Dec",
    "abstract": "We introduce Aligner, a novel Parameter-Efficient Fine-Tuning (PEFT) method for aligning multi-billion-parameter-sized Large Language Models (LLMs). Aligner employs a unique design that constructs a globally shared set of tunable tokens that modify the attention of every layer. Remarkably with this method, even when using one token accounting for a mere 5,000 parameters, Aligner can still perform comparably well to state-of-the-art LLM adaptation methods like LoRA that require millions of parameters. This capacity is substantiated in both instruction following and value alignment tasks. Besides the multiple order-of-magnitude improvement in parameter efficiency, the insight Aligner provides into the internal mechanisms of LLMs is also valuable. The architectural features and efficacy of our method, in addition to our experiments demonstrate that an LLM separates its internal handling of \"form\" and \"knowledge\" in a somewhat orthogonal manner. This finding promises to motivate new research into LLM mechanism understanding and value alignment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhou Ziheng",
      "Yingnian Wu",
      "Song-Chun Zhu",
      "Demetri Terzopoulos"
    ]
  },
  "http://arxiv.org/abs/2312.05497": {
    "title": "History Matters: Temporal Knowledge Editing in Large Language Model",
    "volume": "Dec",
    "abstract": "The imperative task of revising or updating the knowledge stored within large language models arises from two distinct sources: intrinsic errors inherent in the model which should be corrected and outdated knowledge due to external shifts in the real world which should be updated. Prevailing efforts in model editing conflate these two distinct categories of edits arising from distinct reasons and directly modify the original knowledge in models into new knowledge. However, we argue that preserving the model's original knowledge remains pertinent. Specifically, if a model's knowledge becomes outdated due to evolving worldly dynamics, it should retain recollection of the historical knowledge while integrating the newfound knowledge. In this work, we introduce the task of Temporal Knowledge Editing (TKE) and establish a benchmark AToKe (Assessment of TempOral Knowledge Editing) to evaluate current model editing methods. We find that while existing model editing methods are effective at making models remember new knowledge, the edited model catastrophically forgets historical knowledge. To address this gap, we propose a simple and general framework termed Multi-Editing with Time Objective (METO) for enhancing existing editing models, which edits both historical and new knowledge concurrently and optimizes the model's prediction for the time of each fact. Our assessments demonstrate that while AToKe is still difficult, METO maintains the effectiveness of learning new knowledge and meanwhile substantially improves the performance of edited models on utilizing historical knowledge",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xunjian Yin",
      "Jin Jiang",
      "Liming Yang",
      "Xiaojun Wan"
    ]
  },
  "http://arxiv.org/abs/2312.05491": {
    "title": "Using Captum to Explain Generative Language Models",
    "volume": "Dec",
    "abstract": "Captum is a comprehensive library for model explainability in PyTorch, offering a range of methods from the interpretability literature to enhance users' understanding of PyTorch models. In this paper, we introduce new features in Captum that are specifically designed to analyze the behavior of generative language models. We provide an overview of the available functionalities and example applications of their potential for understanding learned associations within generative language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vivek Miglani",
      "Aobo Yang",
      "Aram H. Markosyan",
      "Diego Garcia-Olano",
      "Narine Kokhlikyan"
    ]
  },
  "http://arxiv.org/abs/2312.05488": {
    "title": "Can Large Language Models Serve as Rational Players in Game Theory? A Systematic Analysis",
    "volume": "Dec",
    "abstract": "Game theory, as an analytical tool, is frequently utilized to analyze human behavior in social science research. With the high alignment between the behavior of Large Language Models (LLMs) and humans, a promising research direction is to employ LLMs as substitutes for humans in game experiments, enabling social science research. However, despite numerous empirical researches on the combination of LLMs and game theory, the capability boundaries of LLMs in game theory remain unclear. In this research, we endeavor to systematically analyze LLMs in the context of game theory. Specifically, rationality, as the fundamental principle of game theory, serves as the metric for evaluating players' behavior -- building a clear desire, refining belief about uncertainty, and taking optimal actions. Accordingly, we select three classical games (dictator game, Rock-Paper-Scissors, and ring-network game) to analyze to what extent LLMs can achieve rationality in these three aspects. The experimental results indicate that even the current state-of-the-art LLM (GPT-4) exhibits substantial disparities compared to humans in game theory. For instance, LLMs struggle to build desires based on uncommon preferences, fail to refine belief from many simple patterns, and may overlook or modify refined belief when taking actions. Therefore, we consider that introducing LLMs into game experiments in the field of social science should be approached with greater caution",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caoyun Fan",
      "Jindou Chen",
      "Yaohui Jin",
      "Hao He"
    ]
  },
  "http://arxiv.org/abs/2312.05483": {
    "title": "Teamwork Dimensions Classification Using BERT",
    "volume": "Dec",
    "abstract": "Teamwork is a necessary competency for students that is often inadequately assessed. Towards providing a formative assessment of student teamwork, an automated natural language processing approach was developed to identify teamwork dimensions of students' online team chat. Developments in the field of natural language processing and artificial intelligence have resulted in advanced deep transfer learning approaches namely the Bidirectional Encoder Representations from Transformers (BERT) model that allow for more in-depth understanding of the context of the text. While traditional machine learning algorithms were used in the previous work for the automatic classification of chat messages into the different teamwork dimensions, our findings have shown that classifiers based on the pre-trained language model BERT provides improved classification performance, as well as much potential for generalizability in the language use of varying team chat contexts and team member demographics. This model will contribute towards an enhanced learning analytics tool for teamwork assessment and feedback",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyoung Lee",
      "Elizabeth Koh"
    ]
  },
  "http://arxiv.org/abs/2312.05471": {
    "title": "Fine-Grained Analysis of Team Collaborative Dialogue",
    "volume": "Dec",
    "abstract": "Natural language analysis of human collaborative chat dialogues is an understudied domain with many unique challenges: a large number of dialogue act labels, underspecified and dynamic tasks, interleaved topics, and long-range contextual dependence. While prior work has studied broad metrics of team dialogue and associated performance using methods such as LSA, there has been little effort in generating fine-grained descriptions of team dynamics and individual performance from dialogue. We describe initial work towards developing an explainable analytics tool in the software development domain using Slack chats mined from our organization, including generation of a novel, hierarchical labeling scheme; design of descriptive metrics based on the frequency of occurrence of dialogue acts; and initial results using a transformer + CRF architecture to incorporate long-range context",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ian Perera",
      "Matthew Johnson",
      "Carson Wilber"
    ]
  },
  "http://arxiv.org/abs/2312.05467": {
    "title": "Textual Toxicity in Social Media: Understanding the Bangla Toxic Language Expressed in Facebook Comment",
    "volume": "Dec",
    "abstract": "Social Media is a repository of digital literature including user-generated content. The users of social media are expressing their opinion with diverse mediums such as text, emojis, memes, and also through other visual and textual mediums. A major portion of these media elements could be treated as harmful to others and they are known by many words including Cyberbullying and Toxic Language . The goal of this research paper is to analyze a curated and value-added dataset of toxic language titled ToxLex_bn . It is an exhaustive wordlist that can be used as classifier material to detect toxicity in social media. The toxic language/script used by the Bengali community as cyberbullying, hate speech and moral policing became major trends in social media culture in Bangladesh and West Bengal. The toxicity became so high that the victims has to post as a counter or release explanation video for the haters. Most cases are pointed to women celebrity and their relation, dress, lifestyle are became trolled and toxicity flooded in comments boxes. Not only celebrity bashing but also hates occurred between Hindu Muslims, India-Bangladesh, Two opponents of 1971 and these are very common for virtual conflict in the comment thread. Even many times facebook comment causes sue and legal matters in Bangladesh and thus it requires more study. In this study, a Bangla toxic language dataset has been analyzed which was inputted by the user in Bengali script & language. For this, about 1968 unique bigrams or phrases as wordlists have been analyzed which are derived from 2207590 comments. It is assumed that this analysis will reinforce the detection of Bangla's toxic language used in social media and thus cure this virtual disease",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Mamun Or Rashid"
    ]
  },
  "http://arxiv.org/abs/2312.06681": {
    "title": "Steering Llama 2 via Contrastive Activation Addition",
    "volume": "Dec",
    "abstract": "We introduce Contrastive Activation Addition (CAA), an innovative method for steering language models by modifying activations during their forward passes. CAA computes ``steering vectors'' by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user's prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2 Chat using both multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, outperforms traditional methods like finetuning and few-shot prompting, and minimally reduces capabilities. Moreover, by employing various activation space interpretation methods, we gain deeper insights into CAA's mechanisms. CAA both accurately steers model outputs and also sheds light on how high-level concepts are represented in Large Language Models (LLMs)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nina Rimsky",
      "Nick Gabrieli",
      "Julian Schulz",
      "Meg Tong",
      "Evan Hubinger",
      "Alexander Matt Turner"
    ]
  },
  "http://arxiv.org/abs/2312.05448": {
    "title": "Domain Adaptation of a State of the Art Text-to-SQL Model: Lessons Learned and Challenges Found",
    "volume": "Dec",
    "abstract": "There are many recent advanced developments for the Text-to-SQL task, where the Picard model is one of the the top performing models as measured by the Spider dataset competition. However, bringing Text-to-SQL systems to realistic use-cases through domain adaptation remains a tough challenge. We analyze how well the base T5 Language Model and Picard perform on query structures different from the Spider dataset, we fine-tuned the base model on the Spider data and on independent databases (DB). To avoid accessing the DB content online during inference, we also present an alternative way to disambiguate the values in an input question using a rule-based approach that relies on an intermediate representation of the semantic concepts of an input question. In our results we show in what cases T5 and Picard can deliver good performance, we share the lessons learned, and discuss current domain adaptation challenges",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Irene Manotas",
      "Octavian Popescu",
      "Ngoc Phuoc An Vo",
      "Vadim Sheinin"
    ]
  },
  "http://arxiv.org/abs/2312.05435": {
    "title": "Enhancing Robustness of Foundation Model Representations under Provenance-related Distribution Shifts",
    "volume": "Dec",
    "abstract": "Foundation models are a current focus of attention in both industry and academia. While they have shown their capabilities in a variety of tasks, in-depth research is required to determine their robustness to distribution shift when used as a basis for supervised machine learning. This is especially important in the context of clinical data, with particular limitations related to data accessibility, lack of pretraining materials, and limited availability of high-quality annotations. In this work, we examine the stability of models based on representations from foundation models under distribution shift. We focus on confounding by provenance, a form of distribution shift that emerges in the context of multi-institutional datasets when there are differences in source-specific language use and class distributions. Using a sampling strategy that synthetically induces varying degrees of distribution shift, we evaluate the extent to which representations from foundation models result in predictions that are inherently robust to confounding by provenance. Additionally, we examine the effectiveness of a straightforward confounding adjustment method inspired by Pearl's conception of backdoor adjustment. Results indicate that while foundation models do show some out-of-the-box robustness to confounding-by-provenance related distribution shifts, this can be considerably improved through adjustment. These findings suggest a need for deliberate adjustment of predictive models using representations from foundation models in the context of source-specific distributional differences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiruo Ding",
      "Zhecheng Sheng",
      "Brian Hur",
      "Feng Chen",
      "Serguei V. S. Pakhomov",
      "Trevor Cohen"
    ]
  },
  "http://arxiv.org/abs/2312.05434": {
    "title": "Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models",
    "volume": "Dec",
    "abstract": "The age of social media is rife with memes. Understanding and detecting harmful memes pose a significant challenge due to their implicit meaning that is not explicitly conveyed through the surface text and image. However, existing harmful meme detection approaches only recognize superficial harm-indicative signals in an end-to-end classification manner but ignore in-depth cognition of the meme text and image. In this paper, we attempt to detect harmful memes based on advanced reasoning over the interplay of multimodal information in memes. Inspired by the success of Large Language Models (LLMs) on complex reasoning, we first conduct abductive reasoning with LLMs. Then we propose a novel generative framework to learn reasonable thoughts from LLMs for better multimodal fusion and lightweight fine-tuning, which consists of two training stages: 1) Distill multimodal reasoning knowledge from LLMs; and 2) Fine-tune the generative framework to infer harmfulness. Extensive experiments conducted on three meme datasets demonstrate that our proposed approach achieves superior performance than state-of-the-art methods on the harmful meme detection task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongzhan Lin",
      "Ziyang Luo",
      "Jing Ma",
      "Long Chen"
    ]
  },
  "http://arxiv.org/abs/2312.05415": {
    "title": "An Experimental Study: Assessing the Combined Framework of WavLM and BEST-RQ for Text-to-Speech Synthesis",
    "volume": "Dec",
    "abstract": "We propose a new model architecture specifically suited for text-to-speech (TTS) models. We combine WavLM, a pre-trained self-supervised learning (SSL) speech model, and the BEST-RQ vector quantization framework. We assess the extent to which the more task-agnostic WavLM, coupled with the superior suitability of the simplistic BEST-RQ framework for a wider array of downstream tasks, yields favorable outcomes. Experiments on the LibriSpeech dataset with SUPERB benchmarking assert that the proposed model significantly underperforms. We speculate the underlying reason for this performance is related to the difference between featurizing raw audio waveforms and spectrograms with a quantizer. We discuss the limitations of this approach to better guide future advancements in TTS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Via Nielson",
      "Steven Hillis"
    ]
  },
  "http://arxiv.org/abs/2312.05402": {
    "title": "Towards Controlled Table-to-Text Generation with Scientific Reasoning",
    "volume": "Dec",
    "abstract": "The sheer volume of scientific experimental results and complex technical statements, often presented in tabular formats, presents a formidable barrier to individuals acquiring preferred information. The realms of scientific reasoning and content generation that adhere to user preferences encounter distinct challenges. In this work, we present a new task for generating fluent and logical descriptions that match user preferences over scientific tabular data, aiming to automate scientific document analysis. To facilitate research in this direction, we construct a new challenging dataset CTRLSciTab consisting of table-description pairs extracted from the scientific literature, with highlighted cells and corresponding domain-specific knowledge base. We evaluated popular pre-trained language models to establish a baseline and proposed a novel architecture outperforming competing approaches. The results showed that large models struggle to produce accurate content that aligns with user preferences. As the first of its kind, our work should motivate further research in scientific domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixin Guo",
      "Jianping Zhou",
      "Jiexing Qi",
      "Mingxuan Yan",
      "Ziwei He",
      "Guanjie Zheng",
      "Zhouhan Lin",
      "Xinbing Wang",
      "Chenghu Zhou"
    ]
  },
  "http://arxiv.org/abs/2312.05356": {
    "title": "Neuron Patching: Neuron-level Model Editing on Code Generation and LLMs",
    "volume": "Dec",
    "abstract": "Large Language Models are successfully adopted in software engineering, especially in code generation. Updating these models with new knowledge is very expensive, and is often required to fully realize their value. In this paper, we propose a novel and effective model editing approach, \\textsc{MENT}, to patch LLMs in coding tasks. Based on the mechanism of generative LLMs, \\textsc{MENT} enables model editing in next-token predictions, and further supports common coding tasks. \\textsc{MENT} is effective, efficient, and reliable. It can correct a neural model by patching 1 or 2 neurons. As the pioneer work on neuron-level model editing of generative models, we formalize the editing process and introduce the involved concepts. Besides, we also introduce new measures to evaluate its generalization ability, and build a benchmark for further study. Our approach is evaluated on three coding tasks, including API-seq recommendation, line-level code generation, and pseudocode-to-code transaction. It outperforms the state-of-the-art by a significant margin on both effectiveness and efficiency measures. In addition, we demonstrate the usages of \\textsc{MENT} for LLM reasoning in software engineering. By editing the LLM knowledge with \\textsc{MENT}, the directly or indirectly dependent behaviors in the chain-of-thought change accordingly and automatically",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Gu",
      "Chunyang Chen",
      "Aldeida Aleti"
    ]
  },
  "http://arxiv.org/abs/2312.07559": {
    "title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research",
    "volume": "Dec",
    "abstract": "Large Language Models (LLMs) generalize well across language tasks, but suffer from hallucinations and uninterpretability, making it difficult to assess their accuracy without ground-truth. Retrieval-Augmented Generation (RAG) models have been proposed to reduce hallucinations and provide provenance for how an answer was generated. Applying such models to the scientific literature may enable large-scale, systematic processing of scientific knowledge. We present PaperQA, a RAG agent for answering questions over the scientific literature. PaperQA is an agent that performs information retrieval across full-text scientific articles, assesses the relevance of sources and passages, and uses RAG to provide answers. Viewing this agent as a question answering model, we find it exceeds performance of existing LLMs and LLM agents on current science QA benchmarks. To push the field closer to how humans perform research on scientific literature, we also introduce LitQA, a more complex benchmark that requires retrieval and synthesis of information from full-text scientific papers across the literature. Finally, we demonstrate PaperQA's matches expert human researchers on LitQA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jakub Lála",
      "Odhran O'Donoghue",
      "Aleksandar Shtedritski",
      "Sam Cox",
      "Samuel G. Rodriques",
      "Andrew D. White"
    ]
  },
  "http://arxiv.org/abs/2312.05235": {
    "title": "Seeing ChatGPT Through Universities' Policies, Resources and Guidelines",
    "volume": "Dec",
    "abstract": "The advancements in Artificial Intelligence (AI) technologies such as ChatGPT have gained popularity in recent days. The integration of ChatGPT in educational contexts has already created attractions due to a wide range of applications. However, the automatic generation of human-like texts also poses potential risks to academic integrity, especially when faced with writing-intensive language courses. Considering the ongoing debates, this study aims to investigate the academic policies and guidelines established by US universities regarding the use of ChatGPT in teaching and learning. The data sources include academic policies, statements, guidelines as well as relevant resources that were provided by the top 50 universities in the United States, according to U.S. News. Thematic analysis and qualitative analysis were employed in the analysis and showed that most top 50 universities were open but cautious towards the integration of generative AI in teaching and learning and also expressed their concerns on ethical usage, accuracy, and data privacy. Most universities also provided a variety of resources and guidelines, including syllabus templates/samples, workshops and discussions, shared articles, and one-on-one consultations, with focuses on general technical introduction, ethical concerns, pedagogical applications, preventive strategies, data privacy, limitations, and detective tools. The findings will inform future policy-making regarding the integration of ChatGPT in college-level education and influence the provision of supportive resources by universities for the appropriate application of ChatGPT in education",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Wang",
      "Anh Dang",
      "Zihao Wu",
      "Son Mac"
    ]
  },
  "http://arxiv.org/abs/2312.05230": {
    "title": "Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning",
    "volume": "Dec",
    "abstract": "Despite their tremendous success in many applications, large language models often fall short of consistent reasoning and planning in various (language, embodied, and social) scenarios, due to inherent limitations in their inference, learning, and modeling capabilities. In this position paper, we present a new perspective of machine reasoning, LAW, that connects the concepts of Language models, Agent models, and World models, for more robust and versatile reasoning capabilities. In particular, we propose that world and agent models are a better abstraction of reasoning, that introduces the crucial elements of deliberate human-like reasoning, including beliefs about the world and other agents, anticipation of consequences, goals/rewards, and strategic planning. Crucially, language models in LAW serve as a backend to implement the system or its elements and hence provide the computational power and adaptability. We review the recent studies that have made relevant progress and discuss future research directions towards operationalizing the LAW framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiting Hu",
      "Tianmin Shu"
    ]
  },
  "http://arxiv.org/abs/2312.05291": {
    "title": "GlitchBench: Can large multimodal models detect video game glitches?",
    "volume": "Dec",
    "abstract": "Large multimodal models (LMMs) have evolved from large language models (LLMs) to integrate multiple input modalities, such as visual inputs. This integration augments the capacity of LLMs for tasks requiring visual comprehension and reasoning. However, the extent and limitations of their enhanced abilities are not fully understood, especially when it comes to real-world tasks. To address this gap, we introduce GlitchBench, a novel benchmark derived from video game quality assurance tasks, to test and evaluate the reasoning capabilities of LMMs. Our benchmark is curated from a variety of unusual and glitched scenarios from video games and aims to challenge both the visual and linguistic reasoning powers of LMMs in detecting and interpreting out-of-the-ordinary events. We evaluate multiple state-of-the-art LMMs, and we show that GlitchBench presents a new challenge for these models. Code and data are available at: https://glitchbench.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Reza Taesiri",
      "Tianjun Feng",
      "Cor-Paul Bezemer",
      "Anh Nguyen"
    ]
  },
  "http://arxiv.org/abs/2312.05209": {
    "title": "HALO: An Ontology for Representing Hallucinations in Generative Models",
    "volume": "Dec",
    "abstract": "Recent progress in generative AI, including large language models (LLMs) like ChatGPT, has opened up significant opportunities in fields ranging from natural language processing to knowledge discovery and data mining. However, there is also a growing awareness that the models can be prone to problems such as making information up or `hallucinations', and faulty reasoning on seemingly simple problems. Because of the popularity of models like ChatGPT, both academic scholars and citizen scientists have documented hallucinations of several different types and severity. Despite this body of work, a formal model for describing and representing these hallucinations (with relevant meta-data) at a fine-grained level, is still lacking. In this paper, we address this gap by presenting the Hallucination Ontology or HALO, a formal, extensible ontology written in OWL that currently offers support for six different types of hallucinations known to arise in LLMs, along with support for provenance and experimental metadata. We also collect and publish a dataset containing hallucinations that we inductively gathered across multiple independent Web sources, and show that HALO can be successfully used to model this dataset and answer competency questions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Navapat Nananukul",
      "Mayank Kejriwal"
    ]
  },
  "http://arxiv.org/abs/2312.05200": {
    "title": "DelucionQA: Detecting Hallucinations in Domain-specific Question Answering",
    "volume": "Dec",
    "abstract": "Hallucination is a well-known phenomenon in text generated by large language models (LLMs). The existence of hallucinatory responses is found in almost all application scenarios e.g., summarization, question-answering (QA) etc. For applications requiring high reliability (e.g., customer-facing assistants), the potential existence of hallucination in LLM-generated text is a critical problem. The amount of hallucination can be reduced by leveraging information retrieval to provide relevant background information to the LLM. However, LLMs can still generate hallucinatory content for various reasons (e.g., prioritizing its parametric knowledge over the context, failure to capture the relevant information from the context, etc.). Detecting hallucinations through automated methods is thus paramount. To facilitate research in this direction, we introduce a sophisticated dataset, DelucionQA, that captures hallucinations made by retrieval-augmented LLMs for a domain-specific QA task. Furthermore, we propose a set of hallucination detection methods to serve as baselines for future works from the research community. Analysis and case study are also provided to share valuable insights on hallucination phenomena in the target scenario",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mobashir Sadat",
      "Zhengyu Zhou",
      "Lukas Lange",
      "Jun Araki",
      "Arsalan Gundroo",
      "Bingqing Wang",
      "Rakesh R Menon",
      "Md Rizwan Parvez",
      "Zhe Feng"
    ]
  },
  "http://arxiv.org/abs/2312.05187": {
    "title": "Seamless: Multilingual Expressive and Streaming Speech Translation",
    "volume": "Dec",
    "abstract": "Large-scale automatic speech translation systems today lack key features that help machine-mediated communication feel seamless when compared to human-to-human dialogue. In this work, we introduce a family of models that enable end-to-end expressive and multilingual translations in a streaming fashion. First, we contribute an improved version of the massively multilingual and multimodal SeamlessM4T model-SeamlessM4T v2. This newer model, incorporating an updated UnitY2 framework, was trained on more low-resource language data. SeamlessM4T v2 provides the foundation on which our next two models are initiated. SeamlessExpressive enables translation that preserves vocal styles and prosody. Compared to previous efforts in expressive speech research, our work addresses certain underexplored aspects of prosody, such as speech rate and pauses, while also preserving the style of one's voice. As for SeamlessStreaming, our model leverages the Efficient Monotonic Multihead Attention mechanism to generate low-latency target translations without waiting for complete source utterances. As the first of its kind, SeamlessStreaming enables simultaneous speech-to-speech/text translation for multiple source and target languages. To ensure that our models can be used safely and responsibly, we implemented the first known red-teaming effort for multimodal machine translation, a system for the detection and mitigation of added toxicity, a systematic evaluation of gender bias, and an inaudible localized watermarking mechanism designed to dampen the impact of deepfakes. Consequently, we bring major components from SeamlessExpressive and SeamlessStreaming together to form Seamless, the first publicly available system that unlocks expressive cross-lingual communication in real-time. The contributions to this work are publicly released and accessible at https://github.com/facebookresearch/seamless_communication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seamless Communication",
      "Loïc Barrault",
      "Yu-An Chung",
      "Mariano Coria Meglioli",
      "David Dale",
      "Ning Dong",
      "Mark Duppenthaler",
      "Paul-Ambroise Duquenne",
      "Brian Ellis",
      "Hady Elsahar",
      "Justin Haaheim",
      "John Hoffman",
      "Min-Jae Hwang",
      "Hirofumi Inaguma",
      "Christopher Klaiber",
      "Ilia Kulikov",
      "Pengwei Li",
      "Daniel Licht",
      "Jean Maillard",
      "Ruslan Mavlyutov",
      "Alice Rakotoarison",
      "Kaushik Ram Sadagopan",
      "Abinesh Ramakrishnan",
      "Tuan Tran",
      "Guillaume Wenzek",
      "Yilin Yang",
      "Ethan Ye",
      "Ivan Evtimov",
      "Pierre Fernandez",
      "Cynthia Gao",
      "Prangthip Hansanti",
      "Elahe Kalbassi",
      "Amanda Kallet",
      "Artyom Kozhevnikov",
      "Gabriel Mejia Gonzalez",
      "Robin San Roman",
      "Christophe Touret",
      "Corinne Wong",
      "Carleigh Wood",
      "Bokai Yu",
      "Pierre Andrews",
      "Can Balioglu",
      "Peng-Jen Chen",
      "Marta R. Costa-jussà",
      "Maha Elbayad",
      "Hongyu Gong",
      "Francisco Guzmán",
      "Kevin Heffernan",
      "Somya Jain",
      "Justine Kao",
      "Ann Lee",
      "Xutai Ma",
      "Alex Mourachko",
      "Benjamin Peloquin",
      "Juan Pino",
      "Sravya Popuri",
      "Christophe Ropers",
      "Safiyyah Saleem",
      "Holger Schwenk",
      "Anna Sun",
      "Paden Tomasello",
      "Changhan Wang",
      "Jeff Wang",
      "Skyler Wang",
      "Mary Williamson"
    ]
  },
  "http://arxiv.org/abs/2312.05180": {
    "title": "PathFinder: Guided Search over Multi-Step Reasoning Paths",
    "volume": "Dec",
    "abstract": "With recent advancements in large language models, methods like chain-of-thought prompting to elicit reasoning chains have been shown to improve results on reasoning tasks. However, tasks that require multiple steps of reasoning still pose significant challenges to state-of-the-art models. Drawing inspiration from the beam search algorithm, we propose PathFinder, a tree-search-based reasoning path generation approach. It enhances diverse branching and multi-hop reasoning through the integration of dynamic decoding, enabled by varying sampling methods and parameters. Using constrained reasoning, PathFinder integrates novel quality constraints, pruning, and exploration methods to enhance the efficiency and the quality of generation. Moreover, it includes scoring and ranking features to improve candidate selection. Our approach outperforms competitive baselines on three complex arithmetic and commonsense reasoning tasks by 6% on average. Our model generalizes well to longer, unseen reasoning chains, reflecting similar complexities to beam search with large branching factors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Olga Golovneva",
      "Sean O'Brien",
      "Ramakanth Pasunuru",
      "Tianlu Wang",
      "Luke Zettlemoyer",
      "Maryam Fazel-Zarandi",
      "Asli Celikyilmaz"
    ]
  },
  "http://arxiv.org/abs/2312.05172": {
    "title": "From Lengthy to Lucid: A Systematic Literature Review on NLP Techniques for Taming Long Sentences",
    "volume": "Dec",
    "abstract": "Long sentences have been a persistent issue in written communication for many years since they make it challenging for readers to grasp the main points or follow the initial intention of the writer. This survey, conducted using the PRISMA guidelines, systematically reviews two main strategies for addressing the issue of long sentences: a) sentence compression and b) sentence splitting. An increased trend of interest in this area has been observed since 2005, with significant growth after 2017. Current research is dominated by supervised approaches for both sentence compression and splitting. Yet, there is a considerable gap in weakly and self-supervised techniques, suggesting an opportunity for further research, especially in domains with limited data. In this survey, we categorize and group the most representative methods into a comprehensive taxonomy. We also conduct a comparative evaluation analysis of these methods on common sentence compression and splitting datasets. Finally, we discuss the challenges and limitations of current methods, providing valuable insights for future research directions. This survey is meant to serve as a comprehensive resource for addressing the complexities of long sentences. We aim to enable researchers to make further advancements in the field until long sentences are no longer a barrier to effective communication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tatiana Passali",
      "Efstathios Chatzikyriakidis",
      "Stelios Andreadis",
      "Thanos G. Stavropoulos",
      "Anastasia Matonaki",
      "Anestis Fachantidis",
      "Grigorios Tsoumakas"
    ]
  },
  "http://arxiv.org/abs/2312.05103": {
    "title": "TMID: A Comprehensive Real-world Dataset for Trademark Infringement Detection in E-Commerce",
    "volume": "Dec",
    "abstract": "Annually, e-commerce platforms incur substantial financial losses due to trademark infringements, making it crucial to identify and mitigate potential legal risks tied to merchant information registered to the platforms. However, the absence of high-quality datasets hampers research in this area. To address this gap, our study introduces TMID, a novel dataset to detect trademark infringement in merchant registrations. This is a real-world dataset sourced directly from Alipay, one of the world's largest e-commerce and digital payment platforms. As infringement detection is a legal reasoning task requiring an understanding of the contexts and legal rules, we offer a thorough collection of legal rules and merchant and trademark-related contextual information with annotations from legal experts. We ensure the data quality by performing an extensive statistical analysis. Furthermore, we conduct an empirical study on this dataset to highlight its value and the key challenges. Through this study, we aim to contribute valuable resources to advance research into legal compliance related to trademark infringement within the e-commerce sphere. The dataset is available at https://github.com/emnlpTMID/emnlpTMID.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongxin Hu",
      "Zhuang Li",
      "Xin Jin",
      "Lizhen Qu",
      "Xin Zhang"
    ]
  },
  "http://arxiv.org/abs/2312.05061": {
    "title": "LaCour!: Enabling Research on Argumentation in Hearings of the European Court of Human Rights",
    "volume": "Dec",
    "abstract": "Why does an argument end up in the final court decision? Was it deliberated or questioned during the oral hearings? Was there something in the hearings that triggered a particular judge to write a dissenting opinion? Despite the availability of the final judgments of the European Court of Human Rights (ECHR), none of these legal research questions can currently be answered as the ECHR's multilingual oral hearings are not transcribed, structured, or speaker-attributed. We address this fundamental gap by presenting LaCour!, the first corpus of textual oral arguments of the ECHR, consisting of 154 full hearings (2.1 million tokens from over 267 hours of video footage) in English, French, and other court languages, each linked to the corresponding final judgment documents. In addition to the transcribed and partially manually corrected text from the video, we provide sentence-level timestamps and manually annotated role and language labels. We also showcase LaCour! in a set of preliminary experiments that explore the interplay between questions and dissenting opinions. Apart from the use cases in legal NLP, we hope that law students or other interested parties will also use LaCour! as a learning resource, as it is freely available in various formats at https://huggingface.co/datasets/TrustHLT/LaCour",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lena Held",
      "Ivan Habernal"
    ]
  },
  "http://arxiv.org/abs/2312.05047": {
    "title": "Converting Epics/Stories into Pseudocode using Transformers",
    "volume": "Dec",
    "abstract": "The conversion of user epics or stories into their appropriate representation in pseudocode or code is a time-consuming task, which can take up a large portion of the time in an industrial project. With this research paper, we aim to present a methodology to generate pseudocode from a given agile user story of small functionalities so as to reduce the overall time spent on the industrial project. Pseudocode is a programming language agnostic representation of the steps involved in a computer program, which can be easily converted into any programming language. Leveraging the potential of Natural Language Processing, we want to simplify the development process in organizations that use the Agile Model of Software Development. We present a methodology to convert a problem described in the English language into pseudocode. This methodology divides the Text to Pseudocode conversion task into two stages or subtasks, each of which is treated like an individual machine translation task. Stage 1 is Text to Code Conversion and Stage 2 is Code to Pseudocode Conversion. We find that the CodeT5 model gives the best results in terms of BLEU score when trained separately on the two subtasks mentioned above. BLEU score is a metric that is used to measure the similarity between a machine-translated text and a set of reference translations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaurav Kolhatkar",
      "Akshit Madan",
      "Nidhi Kowtal",
      "Satyajit Roy",
      "Sheetal Sonawane"
    ]
  },
  "http://arxiv.org/abs/2312.04982": {
    "title": "Boosting Prompt-Based Self-Training With Mapping-Free Automatic Verbalizer for Multi-Class Classification",
    "volume": "Dec",
    "abstract": "Recently, prompt-based fine-tuning has garnered considerable interest as a core technique for few-shot text classification task. This approach reformulates the fine-tuning objective to align with the Masked Language Modeling (MLM) objective. Leveraging unlabeled data, prompt-based self-training has shown greater effectiveness in binary and three-class classification. However, prompt-based self-training for multi-class classification has not been adequately investigated, despite its significant applicability to real-world scenarios. Moreover, extending current methods to multi-class classification suffers from the verbalizer that extracts the predicted value of manually pre-defined single label word for each class from MLM predictions. Consequently, we introduce a novel, efficient verbalizer structure, named Mapping-free Automatic Verbalizer (MAV). Comprising two fully connected layers, MAV serves as a trainable verbalizer that automatically extracts the requisite word features for classification by capitalizing on all available information from MLM predictions. Experimental results on five multi-class classification datasets indicate MAV's superior self-training efficacy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yookyung Kho",
      "Jaehee Kim",
      "Pilsung Kang"
    ]
  },
  "http://arxiv.org/abs/2312.04945": {
    "title": "The ICL Consistency Test",
    "volume": "Dec",
    "abstract": "Just like the previous generation of task-tuned models, large language models (LLMs) that are adapted to tasks via prompt-based methods like in-context-learning (ICL) perform well in some setups but not in others. This lack of consistency in prompt-based learning hints at a lack of robust generalisation. We here introduce the ICL consistency test -- a contribution to the GenBench collaborative benchmark task (CBT) -- which evaluates how consistent a model makes predictions across many different setups while using the same data. The test is based on different established natural language inference tasks. We provide preprocessed data constituting 96 different 'setups' and a metric that estimates model consistency across these setups. The metric is provided on a fine-grained level to understand what properties of a setup render predictions unstable and on an aggregated level to compare overall model consistency. We conduct an empirical analysis of eight state-of-the-art models, and our consistency metric reveals how all tested LLMs lack robust generalisation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Weber",
      "Elia Bruni",
      "Dieuwke Hupkes"
    ]
  },
  "http://arxiv.org/abs/2312.04927": {
    "title": "Zoology: Measuring and Improving Recall in Efficient Language Models",
    "volume": "Dec",
    "abstract": "Attention-free language models that combine gating and convolutions are growing in popularity due to their efficiency and increasingly competitive performance. To better understand these architectures, we pretrain a suite of 17 attention and \"gated-convolution\" language models, finding that SoTA gated-convolution architectures still underperform attention by up to 2.1 perplexity points on the Pile. In fine-grained analysis, we find 82% of the gap is explained by each model's ability to recall information that is previously mentioned in-context, e.g. \"Hakuna Matata means no worries Hakuna Matata it means no\" $\\rightarrow$ \"??\". On this task, termed \"associative recall\", we find that attention outperforms gated-convolutions by a large margin: a 70M parameter attention model outperforms a 1.4 billion parameter gated-convolution model on associative recall. This is surprising because prior work shows gated convolutions can perfectly solve synthetic tests for AR capability. To close the gap between synthetics and real language, we develop a new formalization of the task called multi-query associative recall (MQAR) that better reflects actual language. We perform an empirical and theoretical study of MQAR that elucidates differences in the parameter-efficiency of attention and gated-convolution recall. Informed by our analysis, we evaluate simple convolution-attention hybrids and show that hybrids with input-dependent sparse attention patterns can close 97.4% of the gap to attention, while maintaining sub-quadratic scaling. Our code is accessible at: https://github.com/HazyResearch/zoology",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simran Arora",
      "Sabri Eyuboglu",
      "Aman Timalsina",
      "Isys Johnson",
      "Michael Poli",
      "James Zou",
      "Atri Rudra",
      "Christopher Ré"
    ]
  },
  "http://arxiv.org/abs/2312.05278": {
    "title": "Lyrics: Boosting Fine-grained Language-Vision Alignment and Comprehension via Semantic-aware Visual Objects",
    "volume": "Dec",
    "abstract": "Large Vision Language Models (LVLMs) have demonstrated impressive zero-shot capabilities in various vision-language dialogue scenarios. However, the absence of fine-grained visual object detection hinders the model from understanding the details of images, leading to irreparable visual hallucinations and factual errors. In this paper, we propose Lyrics, a novel multi-modal pre-training and instruction fine-tuning paradigm that bootstraps vision-language alignment from fine-grained cross-modal collaboration. Building on the foundation of BLIP-2, Lyrics infuses local visual features extracted from a visual refiner that includes image tagging, object detection and semantic segmentation modules into the Querying Transformer, while on the text side, the language inputs equip the boundary boxes and tags derived from the visual refiner. We further introduce a two-stage training scheme, in which the pre-training stage bridges the modality gap through explicit and comprehensive vision-language alignment targets. During the instruction fine-tuning stage, we introduce semantic-aware visual feature extraction, a crucial method that enables the model to extract informative features from concrete visual objects. Our approach achieves strong performance on 13 held-out datasets across various vision-language tasks, and demonstrates promising multi-modal understanding and detailed depiction capabilities in real dialogue scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyu Lu",
      "Ruyi Gan",
      "Dixiang Zhang",
      "Xiaojun Wu",
      "Ziwei Wu",
      "Renliang Sun",
      "Jiaxing Zhang",
      "Pingjian Zhang",
      "Yan Song"
    ]
  },
  "http://arxiv.org/abs/2312.04906": {
    "title": "Ophtha-LLaMA2: A Large Language Model for Ophthalmology",
    "volume": "Dec",
    "abstract": "In recent years, pre-trained large language models (LLMs) have achieved tremendous success in the field of Natural Language Processing (NLP). Prior studies have primarily focused on general and generic domains, with relatively less research on specialized LLMs in the medical field. The specialization and high accuracy requirements for diagnosis in the medical field, as well as the challenges in collecting large-scale data, have constrained the application and development of LLMs in medical scenarios. In the field of ophthalmology, clinical diagnosis mainly relies on doctors' interpretation of reports and making diagnostic decisions. In order to take advantage of LLMs to provide decision support for doctors, we collected three modalities of ophthalmic report data and fine-tuned the LLaMA2 model, successfully constructing an LLM termed the \"Ophtha-LLaMA2\" specifically tailored for ophthalmic disease diagnosis. Inference test results show that even with a smaller fine-tuning dataset, Ophtha-LLaMA2 performs significantly better in ophthalmic diagnosis compared to other LLMs. It demonstrates that the Ophtha-LLaMA2 exhibits satisfying accuracy and efficiency in ophthalmic disease diagnosis, making it a valuable tool for ophthalmologists to provide improved diagnostic support for patients. This research provides a useful reference for the application of LLMs in the field of ophthalmology, while showcasing the immense potential and prospects in this domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huan Zhao",
      "Qian Ling",
      "Yi Pan",
      "Tianyang Zhong",
      "Jin-Yu Hu",
      "Junjie Yao",
      "Fengqian Xiao",
      "Zhenxiang Xiao",
      "Yutong Zhang",
      "San-Hua Xu",
      "Shi-Nan Wu",
      "Min Kang",
      "Zihao Wu",
      "Zhengliang Liu",
      "Xi Jiang",
      "Tianming Liu",
      "Yi Shao"
    ]
  },
  "http://arxiv.org/abs/2312.04889": {
    "title": "KwaiAgents: Generalized Information-seeking Agent System with Large Language Models",
    "volume": "Dec",
    "abstract": "Driven by curiosity, humans have continually sought to explore and understand the world around them, leading to the invention of various tools to satiate this inquisitiveness. Despite not having the capacity to process and memorize vast amounts of information in their brains, humans excel in critical thinking, planning, reflection, and harnessing available tools to interact with and interpret the world, enabling them to find answers efficiently. The recent advancements in large language models (LLMs) suggest that machines might also possess the aforementioned human-like capabilities, allowing them to exhibit powerful abilities even with a constrained parameter count. In this paper, we introduce KwaiAgents, a generalized information-seeking agent system based on LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its cognitive core, which is capable of understanding a user's query, behavior guidelines, and referencing external documents. The agent can also update and retrieve information from its internal memory, plan and execute actions using a time-aware search-browse toolkit, and ultimately provide a comprehensive response. We further investigate the system's performance when powered by LLMs less advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework, designed to ensure even an open-sourced 7B or 13B model performs well among many agent systems. We exploit both benchmark and human evaluations to systematically validate these capabilities. Extensive experiments show the superiority of our agent system compared to other autonomous agents and highlight the enhanced generalized agent-abilities of our fine-tuned LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haojie Pan",
      "Zepeng Zhai",
      "Hao Yuan",
      "Yaojia Lv",
      "Ruiji Fu",
      "Ming Liu",
      "Zhongyuan Wang",
      "Bing Qin"
    ]
  },
  "http://arxiv.org/abs/2312.04882": {
    "title": "Classification of Human- and AI-Generated Texts for English, French, German, and Spanish",
    "volume": "Dec",
    "abstract": "In this paper we analyze features to classify human- and AI-generated text for English, French, German and Spanish and compare them across languages. We investigate two scenarios: (1) The detection of text generated by AI from scratch, and (2) the detection of text rephrased by AI. For training and testing the classifiers in this multilingual setting, we created a new text corpus covering 10 topics for each language. For the detection of AI-generated text, the combination of all proposed features performs best, indicating that our features are portable to other related languages: The F1-scores are close with 99% for Spanish, 98% for English, 97% for German and 95% for French. For the detection of AI-rephrased text, the systems with all features outperform systems with other features in many cases, but using only document features performs best for German (72%) and Spanish (86%) and only text vector features leads to best results for English (78%)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kristina Schaaff",
      "Tim Schlippe",
      "Lorenz Mindner"
    ]
  },
  "http://arxiv.org/abs/2312.04881": {
    "title": "Predictive Chemistry Augmented with Text Retrieval",
    "volume": "Dec",
    "abstract": "This paper focuses on using natural language descriptions to enhance predictive models in the chemistry field. Conventionally, chemoinformatics models are trained with extensive structured data manually extracted from the literature. In this paper, we introduce TextReact, a novel method that directly augments predictive chemistry with texts retrieved from the literature. TextReact retrieves text descriptions relevant for a given chemical reaction, and then aligns them with the molecular representation of the reaction. This alignment is enhanced via an auxiliary masked LM objective incorporated in the predictor training. We empirically validate the framework on two chemistry tasks: reaction condition recommendation and one-step retrosynthesis. By leveraging text retrieval, TextReact significantly outperforms state-of-the-art chemoinformatics models trained solely on molecular data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujie Qian",
      "Zhening Li",
      "Zhengkai Tu",
      "Connor W. Coley",
      "Regina Barzilay"
    ]
  },
  "http://arxiv.org/abs/2312.04877": {
    "title": "Generating Explanations to Understand and Repair Embedding-based Entity Alignment",
    "volume": "Dec",
    "abstract": "Entity alignment seeks identical entities in different knowledge graphs, which is a long-standing task in the database research. Recent work leverages deep learning to embed entities in vector space and align them via nearest neighbor search. Although embedding-based entity alignment has gained marked success in recent years, it lacks explanations for alignment decisions. In this paper, we present the first framework that can generate explanations for understanding and repairing embedding-based entity alignment results. Given an entity alignment pair produced by an embedding model, we first compare its neighbor entities and relations to build a matching subgraph as a local explanation. We then construct an alignment dependency graph to understand the pair from an abstract perspective. Finally, we repair the pair by resolving three types of alignment conflicts based on dependency graphs. Experiments on five datasets demonstrate the effectiveness and generalization of our framework in explaining and repairing embedding-based entity alignment results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaobin Tian",
      "Zequn Sun",
      "Wei Hu"
    ]
  },
  "http://arxiv.org/abs/2312.04854": {
    "title": "Apollo's Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates",
    "volume": "Dec",
    "abstract": "Multi-agent debate systems are designed to derive accurate and consistent conclusions through adversarial interactions among agents. However, these systems often encounter challenges due to cognitive constraints, manifesting as (1) agents' obstinate adherence to incorrect viewpoints and (2) their propensity to abandon correct viewpoints. These issues are primarily responsible for the ineffectiveness of such debates. Addressing the challenge of cognitive constraints, we introduce a novel framework, the Multi-Agent Debate with Retrieval Augmented (MADRA). MADRA incorporates retrieval of prior knowledge into the debate process, effectively breaking cognitive constraints and enhancing the agents' reasoning capabilities. Furthermore, we have developed a self-selection module within this framework, enabling agents to autonomously select pertinent evidence, thereby minimizing the impact of irrelevant or noisy data. We have comprehensively tested and analyzed MADRA across six diverse datasets. The experimental results demonstrate that our approach significantly enhances performance across various tasks, proving the effectiveness of our proposed method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Wang",
      "Xiyuan Du",
      "Weijiang Yu",
      "Qianglong Chen",
      "Kun Zhu",
      "Zheng Chu",
      "Lian Yan",
      "Yi Guan"
    ]
  },
  "http://arxiv.org/abs/2312.04843": {
    "title": "FREDSum: A Dialogue Summarization Corpus for French Political Debates",
    "volume": "Dec",
    "abstract": "Recent advances in deep learning, and especially the invention of encoder-decoder architectures, has significantly improved the performance of abstractive summarization systems. The majority of research has focused on written documents, however, neglecting the problem of multi-party dialogue summarization. In this paper, we present a dataset of French political debates for the purpose of enhancing resources for multi-lingual dialogue summarization. Our dataset consists of manually transcribed and annotated political debates, covering a range of topics and perspectives. We highlight the importance of high quality transcription and annotations for training accurate and effective dialogue summarization models, and emphasize the need for multilingual resources to support dialogue summarization in non-English languages. We also provide baseline experiments using state-of-the-art methods, and encourage further research in this area to advance the field of dialogue summarization. Our dataset will be made publicly available for use by the research community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Virgile Rennard",
      "Guokan Shang",
      "Damien Grari",
      "Julie Hunter",
      "Michalis Vazirgiannis"
    ]
  },
  "http://arxiv.org/abs/2312.04837": {
    "title": "Localized Symbolic Knowledge Distillation for Visual Commonsense Models",
    "volume": "Dec",
    "abstract": "Instruction following vision-language (VL) models offer a flexible interface that supports a broad range of multimodal tasks in a zero-shot fashion. However, interfaces that operate on full images do not directly enable the user to \"point to\" and access specific regions within images. This capability is important not only to support reference-grounded VL benchmarks, but also, for practical applications that require precise within-image reasoning. We build Localized Visual Commonsense models, which allow users to specify (multiple) regions as input. We train our model by sampling localized commonsense knowledge from a large language model (LLM): specifically, we prompt an LLM to collect commonsense knowledge given a global literal image description and a local literal region description automatically generated by a set of VL models. With a separately trained critic model that selects high-quality examples, we find that training on the localized commonsense corpus can successfully distill existing VL models to support a reference-as-input interface. Empirical results and human evaluations in a zero-shot setup demonstrate that our distillation method results in more precise VL models of reasoning compared to a baseline of passing a generated referring expression to an LLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jae Sung Park",
      "Jack Hessel",
      "Khyathi Raghavi Chandu",
      "Paul Pu Liang",
      "Ximing Lu",
      "Peter West",
      "Youngjae Yu",
      "Qiuyuan Huang",
      "Jianfeng Gao",
      "Ali Farhadi",
      "Yejin Choi"
    ]
  },
  "http://arxiv.org/abs/2312.04828": {
    "title": "HuRef: HUman-REadable Fingerprint for Large Language Models",
    "volume": "Dec",
    "abstract": "Protecting the copyright of large language models (LLMs) has become crucial due to their resource-intensive training and accompanying carefully designed licenses. However, identifying the original base model of an LLM is challenging due to potential parameter alterations through fine-tuning or continued pretraining. In this study, we introduce HuRef, a human-readable fingerprint for LLMs that uniquely identifies the base model without exposing model parameters or interfering with training. We first observe that the vector direction of LLM parameters remains stable after the model has converged during pretraining, showing negligible perturbations through subsequent training steps, including continued pretraining, supervised fine-tuning (SFT), and RLHF, which makes it a sufficient condition to identify the base model. The necessity is validated by continuing to train an LLM with an extra term to drive away the model parameters' direction and the model becomes damaged. However, this direction is vulnerable to simple attacks like dimension permutation or matrix rotation, which significantly change it without affecting performance. To address this, leveraging the Transformer structure, we systematically analyze potential attacks and define three invariant terms that identify an LLM's base model. We make these invariant terms human-readable by mapping them to a Gaussian vector using a convolutional encoder and then converting it into a natural image with StyleGAN2. Our method generates a dog image as an identity fingerprint for an LLM, where the dog's appearance strongly indicates the LLM's base model. Experimental results across various LLMs demonstrate the effectiveness of our method, the generated dog image remains invariant to different training steps, including SFT, RLHF, or even continued pretraining with augmented vocabulary in a new language",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyi Zeng",
      "Chenghu Zhou",
      "Xinbing Wang",
      "Zhouhan Lin"
    ]
  },
  "http://arxiv.org/abs/2312.04807": {
    "title": "Improving Neural Machine Translation by Multi-Knowledge Integration with Prompting",
    "volume": "Dec",
    "abstract": "Improving neural machine translation (NMT) systems with prompting has achieved significant progress in recent years. In this work, we focus on how to integrate multi-knowledge, multiple types of knowledge, into NMT models to enhance the performance with prompting. We propose a unified framework, which can integrate effectively multiple types of knowledge including sentences, terminologies/phrases and translation templates into NMT models. We utilize multiple types of knowledge as prefix-prompts of input for the encoder and decoder of NMT models to guide the translation process. The approach requires no changes to the model architecture and effectively adapts to domain-specific translation without retraining. The experiments on English-Chinese and English-German translation demonstrate that our approach significantly outperform strong baselines, achieving high translation quality and terminology match accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Wang",
      "Jun Xie",
      "Yuqi Zhang",
      "Yu Zhao"
    ]
  },
  "http://arxiv.org/abs/2312.04804": {
    "title": "Hate Cannot Drive out Hate: Forecasting Conversation Incivility following Replies to Hate Speech",
    "volume": "Dec",
    "abstract": "User-generated replies to hate speech are promising means to combat hatred, but questions about whether they can stop incivility in follow-up conversations linger. We argue that effective replies stop incivility from emerging in follow-up conversations - replies that elicit more incivility are counterproductive. This study introduces the task of predicting the incivility of conversations following replies to hate speech. We first propose a metric to measure conversation incivility based on the number of civil and uncivil comments as well as the unique authors involved in the discourse. Our metric approximates human judgments more accurately than previous metrics. We then use the metric to evaluate the outcomes of replies to hate speech. A linguistic analysis uncovers the differences in the language of replies that elicit follow-up conversations with high and low incivility. Experimental results show that forecasting incivility is challenging. We close with a qualitative analysis shedding light into the most common errors made by the best model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinchen Yu",
      "Eduardo Blanco",
      "Lingzi Hong"
    ]
  },
  "http://arxiv.org/abs/2312.04775": {
    "title": "How to Determine the Most Powerful Pre-trained Language Model without Brute Force Fine-tuning? An Empirical Survey",
    "volume": "Dec",
    "abstract": "Transferability estimation has been attached to great attention in the computer vision fields. Researchers try to estimate with low computational cost the performance of a model when transferred from a source task to a given target task. Considering the effectiveness of such estimations, the communities of natural language processing also began to study similar problems for the selection of pre-trained language models. However, there is a lack of a comprehensive comparison between these estimation methods yet. Also, the differences between vision and language scenarios make it doubtful whether previous conclusions can be established across fields. In this paper, we first conduct a thorough survey of existing transferability estimation methods being able to find the most suitable model, then we conduct a detailed empirical study for the surveyed methods based on the GLUE benchmark. From qualitative and quantitative analyses, we demonstrate the strengths and weaknesses of existing methods and show that H-Score generally performs well with superiorities in effectiveness and efficiency. We also outline the difficulties of consideration of training details, applicability to text generation, and consistency to certain metrics which shed light on future directions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Bai",
      "Xiaofeng Zhang",
      "Chen Li",
      "Hanhua Hong",
      "Xi Xu",
      "Chenghua Lin",
      "Wenge Rong"
    ]
  },
  "http://arxiv.org/abs/2312.04764": {
    "title": "First Attempt at Building Parallel Corpora for Machine Translation of Northeast India's Very Low-Resource Languages",
    "volume": "Dec",
    "abstract": "This paper presents the creation of initial bilingual corpora for thirteen very low-resource languages of India, all from Northeast India. It also presents the results of initial translation efforts in these languages. It creates the first-ever parallel corpora for these languages and provides initial benchmark neural machine translation results for these languages. We intend to extend these corpora to include a large number of low-resource Indian languages and integrate the effort with our prior work with African and American-Indian languages to create corpora covering a large number of languages from across the world",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atnafu Lambebo Tonja",
      "Melkamu Mersha",
      "Ananya Kalita",
      "Olga Kolesnikova",
      "Jugal Kalita"
    ]
  },
  "http://arxiv.org/abs/2312.04748": {
    "title": "Forcing Generative Models to Degenerate Ones: The Power of Data Poisoning Attacks",
    "volume": "Dec",
    "abstract": "Growing applications of large language models (LLMs) trained by a third party raise serious concerns on the security vulnerability of LLMs.It has been demonstrated that malicious actors can covertly exploit these vulnerabilities in LLMs through poisoning attacks aimed at generating undesirable outputs. While poisoning attacks have received significant attention in the image domain (e.g., object detection), and classification tasks, their implications for generative models, particularly in the realm of natural language generation (NLG) tasks, remain poorly understood. To bridge this gap, we perform a comprehensive exploration of various poisoning techniques to assess their effectiveness across a range of generative tasks. Furthermore, we introduce a range of metrics designed to quantify the success and stealthiness of poisoning attacks specifically tailored to NLG tasks. Through extensive experiments on multiple NLG tasks, LLMs and datasets, we show that it is possible to successfully poison an LLM during the fine-tuning stage using as little as 1\\% of the total tuning data samples. Our paper presents the first systematic approach to comprehend poisoning attacks targeting NLG tasks considering a wide range of triggers and attack settings. We hope our findings will assist the AI security community in devising appropriate defenses against such threats",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuli Jiang",
      "Swanand Ravindra Kadhe",
      "Yi Zhou",
      "Ling Cai",
      "Nathalie Baracaldo"
    ]
  },
  "http://arxiv.org/abs/2312.04746": {
    "title": "Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized Narratives from Open-Source Histopathology Videos",
    "volume": "Dec",
    "abstract": "The gigapixel scale of whole slide images (WSIs) poses a challenge for histopathology multi-modal chatbots, requiring a global WSI analysis for diagnosis, compounding evidence from different WSI patches. Current visual instruction datasets, generated through large language models, focus on creating question/answer pairs for individual image patches, which may lack diagnostic capacity on their own in histopathology, further complicated by the absence of spatial grounding in histopathology image captions. To bridge this gap, we introduce Quilt-Instruct, a large-scale dataset of 107,131 histopathology-specific instruction question/answer pairs, that is collected by leveraging educational histopathology videos from YouTube, which provides spatial localization of captions by automatically extracting narrators' cursor movements. In addition, we provide contextual reasoning by extracting diagnosis and supporting facts from the entire video content to guide the extrapolative reasoning of GPT-4. Using Quilt-Instruct, we train Quilt-LLaVA, which can reason beyond the given single image patch, enabling diagnostic reasoning and the capability of spatial awareness. To evaluate Quilt-LLaVA, we propose a comprehensive evaluation dataset created from 985 images and 1283 human-generated question-answers. We also thoroughly evaluate Quilt-LLaVA using public histopathology datasets, where Quilt-LLaVA significantly outperforms SOTA by over 10% on relative GPT-4 score and 4% and 9% on open and closed set VQA. Our code, data, and model are publicly available at quilt-llava.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mehmet Saygin Seyfioglu",
      "Wisdom O. Ikezogwo",
      "Fatemeh Ghezloo",
      "Ranjay Krishna",
      "Linda Shapiro"
    ]
  },
  "http://arxiv.org/abs/2312.04737": {
    "title": "Efficient Large Language Models Fine-Tuning On Graphs",
    "volume": "Dec",
    "abstract": "Learning from Text-Attributed Graphs (TAGs) has attracted significant attention due to its wide range of real-world applications. The rapid evolution of large language models (LLMs) has revolutionized the way we process textual data, which indicates a strong potential to replace shallow text embedding generally used in Graph Neural Networks (GNNs). However, we find that existing LLM approaches that exploit text information in graphs suffer from inferior computation and data efficiency. In this work, we introduce a novel and efficient approach for the end-to-end fine-tuning of Large Language Models (LLMs) on TAGs, named LEADING. The proposed approach maintains computation cost and memory overhead comparable to the graph-less fine-tuning of LLMs. Moreover, it transfers the rick knowledge in LLMs to downstream graph learning tasks effectively with limited labeled data in semi-supervised learning. Its superior computation and data efficiency are demonstrated through comprehensive experiments, offering a promising solution for a wide range of LLMs and graph learning tasks on TAGs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Xue",
      "Xipeng Shen",
      "Ruozhou Yu",
      "Xiaorui Liu"
    ]
  },
  "http://arxiv.org/abs/2312.04736": {
    "title": "Is Feedback All You Need? Leveraging Natural Language Feedback in Goal-Conditioned Reinforcement Learning",
    "volume": "Dec",
    "abstract": "Despite numerous successes, the field of reinforcement learning (RL) remains far from matching the impressive generalisation power of human behaviour learning. One possible way to help bridge this gap be to provide RL agents with richer, more human-like feedback expressed in natural language. To investigate this idea, we first extend BabyAI to automatically generate language feedback from the environment dynamics and goal condition success. Then, we modify the Decision Transformer architecture to take advantage of this additional signal. We find that training with language feedback either in place of or in addition to the return-to-go or goal descriptions improves agents' generalisation performance, and that agents can benefit from feedback even when this is only available during training, but not at inference",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sabrina McCallum",
      "Max Taylor-Davies",
      "Stefano V. Albrecht",
      "Alessandro Suglia"
    ]
  },
  "http://arxiv.org/abs/2312.04720": {
    "title": "From Big to Small Without Losing It All: Text Augmentation with ChatGPT for Efficient Sentiment Analysis",
    "volume": "Dec",
    "abstract": "In the era of artificial intelligence, data is gold but costly to annotate. The paper demonstrates a groundbreaking solution to this dilemma using ChatGPT for text augmentation in sentiment analysis. We leverage ChatGPT's generative capabilities to create synthetic training data that significantly improves the performance of smaller models, making them competitive with, or even outperforming, their larger counterparts. This innovation enables models to be both efficient and effective, thereby reducing computational cost, inference time, and memory usage without compromising on quality. Our work marks a key advancement in the cost-effective development and deployment of robust sentiment analysis models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stanisław Woźniak",
      "Jan Kocoń"
    ]
  },
  "http://arxiv.org/abs/2312.04715": {
    "title": "Deep Emotions Across Languages: A Novel Approach for Sentiment Propagation in Multilingual WordNets",
    "volume": "Dec",
    "abstract": "Sentiment analysis involves using WordNets enriched with emotional metadata, which are valuable resources. However, manual annotation is time-consuming and expensive, resulting in only a few WordNet Lexical Units being annotated. This paper introduces two new techniques for automatically propagating sentiment annotations from a partially annotated WordNet to its entirety and to a WordNet in a different language: Multilingual Structured Synset Embeddings (MSSE) and Cross-Lingual Deep Neural Sentiment Propagation (CLDNS). We evaluated the proposed MSSE+CLDNS method extensively using Princeton WordNet and Polish WordNet, which have many inter-lingual relations. Our results show that the MSSE+CLDNS method outperforms existing propagation methods, indicating its effectiveness in enriching WordNets with emotional metadata across multiple languages. This work provides a solid foundation for large-scale, multilingual sentiment analysis and is valuable for academic research and practical applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Kocoń"
    ]
  },
  "http://arxiv.org/abs/2312.04691": {
    "title": "Simul-LLM: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models",
    "volume": "Dec",
    "abstract": "Large language models (LLMs) with billions of parameters and pretrained on massive amounts of data are now capable of near or better than state-of-the-art performance in a variety of downstream natural language processing tasks. Neural machine translation (NMT) is one such task that LLMs have been applied to with great success. However, little research has focused on applying LLMs to the more difficult subset of NMT called simultaneous translation (SimulMT), where translation begins before the entire source context is available to the model. In this paper, we address key challenges facing LLMs fine-tuned for SimulMT, validate classical SimulMT concepts and practices in the context of LLMs, explore adapting LLMs that are fine-tuned for NMT to the task of SimulMT, and introduce Simul-LLM, the first open-source fine-tuning and evaluation pipeline development framework for LLMs focused on SimulMT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Victor Agostinelli",
      "Max Wild",
      "Matthew Raffel",
      "Kazi Ahmed Asif Fuad",
      "Lizhong Chen"
    ]
  },
  "http://arxiv.org/abs/2312.04684": {
    "title": "Latent Skill Discovery for Chain-of-Thought Reasoning",
    "volume": "Dec",
    "abstract": "Recent advances in Large Language Models (LLMs) have led to an emergent ability of chain-of-thought (CoT) prompting, a prompt reasoning strategy that adds intermediate rationale steps between questions and answers to construct prompts. Conditioned on these prompts, LLMs can effectively learn in context to generate rationales that lead to more accurate answers than when answering the same question directly. To design LLM prompts, one important setting, called demonstration selection, considers selecting demonstrations from an example bank. Existing methods use various heuristics for this selection, but for CoT prompting, which involves unique rationales, it is essential to base the selection upon the intrinsic skills that CoT rationales need, for instance, the skills of addition or subtraction for math word problems. To address this requirement, we introduce a novel approach named Reasoning Skill Discovery (RSD) that use unsupervised learning to create a latent space representation of rationales, called a reasoning skill. Simultaneously, RSD learns a reasoning policy to determine the required reasoning skill for a given question. This can then guide the selection of examples that demonstrate the required reasoning skills. Our approach offers several desirable properties: it is (1) theoretically grounded, (2) sample-efficient, requiring no LLM inference or manual prompt design, and (3) LLM-agnostic. Empirically, RSD outperforms existing methods by up to 6% in terms of the answer accuracy across multiple reasoning tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zifan Xu",
      "Haozhu Wang",
      "Dmitriy Bespalov",
      "Peter Stone",
      "Yanjun Qi"
    ]
  },
  "http://arxiv.org/abs/2312.04668": {
    "title": "TOD-Flow: Modeling the Structure of Task-Oriented Dialogues",
    "volume": "Dec",
    "abstract": "Task-Oriented Dialogue (TOD) systems have become crucial components in interactive artificial intelligence applications. While recent advances have capitalized on pre-trained language models (PLMs), they exhibit limitations regarding transparency and controllability. To address these challenges, we propose a novel approach focusing on inferring the TOD-Flow graph from dialogue data annotated with dialog acts, uncovering the underlying task structure in the form of a graph. The inferred TOD-Flow graph can be easily integrated with any dialogue model to improve its prediction performance, transparency, and controllability. Our TOD-Flow graph learns what a model can, should, and should not predict, effectively reducing the search space and providing a rationale for the model's prediction. We show that the proposed TOD-Flow graph better resembles human-annotated graphs compared to prior approaches. Furthermore, when combined with several dialogue policies and end-to-end dialogue models, we demonstrate that our approach significantly improves dialog act classification and end-to-end response generation performance in the MultiWOZ and SGD benchmarks. Code available at: https://github.com/srsohn/TOD-Flow",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungryull Sohn",
      "Yiwei Lyu",
      "Anthony Liu",
      "Lajanugen Logeswaran",
      "Dong-Ki Kim",
      "Dongsub Shim",
      "Honglak Lee"
    ]
  },
  "http://arxiv.org/abs/2312.06674": {
    "title": "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations",
    "volume": "Dec",
    "abstract": "We introduce Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases. Our model incorporates a safety risk taxonomy, a valuable tool for categorizing a specific set of safety risks found in LLM prompts (i.e., prompt classification). This taxonomy is also instrumental in classifying the responses generated by LLMs to these prompts, a process we refer to as response classification. For the purpose of both prompt and response classification, we have meticulously gathered a dataset of high quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its performance matches or exceeds that of currently available content moderation tools. Llama Guard functions as a language model, carrying out multi-class classification and generating binary decision scores. Furthermore, the instruction fine-tuning of Llama Guard allows for the customization of tasks and the adaptation of output formats. This feature enhances the model's capabilities, such as enabling the adjustment of taxonomy categories to align with specific use cases, and facilitating zero-shot or few-shot prompting with diverse taxonomies at the input. We are making Llama Guard model weights available and we encourage researchers to further develop and adapt them to meet the evolving needs of the community for AI safety",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hakan Inan",
      "Kartikeya Upasani",
      "Jianfeng Chi",
      "Rashi Rungta",
      "Krithika Iyer",
      "Yuning Mao",
      "Michael Tontchev",
      "Qing Hu",
      "Brian Fuller",
      "Davide Testuggine",
      "Madian Khabsa"
    ]
  },
  "http://arxiv.org/abs/2312.04657": {
    "title": "Self-Supervised Behavior Cloned Transformers are Path Crawlers for Text Games",
    "volume": "Dec",
    "abstract": "In this work, we introduce a self-supervised behavior cloning transformer for text games, which are challenging benchmarks for multi-step reasoning in virtual environments. Traditionally, Behavior Cloning Transformers excel in such tasks but rely on supervised training data. Our approach auto-generates training data by exploring trajectories (defined by common macro-action sequences) that lead to reward within the games, while determining the generality and utility of these trajectories by rapidly training small models then evaluating their performance on unseen development games. Through empirical analysis, we show our method consistently uncovers generalizable training data, achieving about 90\\% performance of supervised systems across three benchmark text games",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruoyao Wang",
      "Peter Jansen"
    ]
  },
  "http://arxiv.org/abs/2312.04649": {
    "title": "PyThaiNLP: Thai Natural Language Processing in Python",
    "volume": "Dec",
    "abstract": "We present PyThaiNLP, a free and open-source natural language processing (NLP) library for Thai language implemented in Python. It provides a wide range of software, models, and datasets for Thai language. We first provide a brief historical context of tools for Thai language prior to the development of PyThaiNLP. We then outline the functionalities it provided as well as datasets and pre-trained language models. We later summarize its development milestones and discuss our experience during its development. We conclude by demonstrating how industrial and research communities utilize PyThaiNLP in their work. The library is freely available at https://github.com/pythainlp/pythainlp",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wannaphong Phatthiyaphaibun",
      "Korakot Chaovavanich",
      "Charin Polpanumas",
      "Arthit Suriyawongkul",
      "Lalita Lowphansirikul",
      "Pattarawat Chormai",
      "Peerat Limkonchotiwat",
      "Thanathip Suntorntip",
      "Can Udomcharoenchaikit"
    ]
  },
  "http://arxiv.org/abs/2312.04642": {
    "title": "On Sarcasm Detection with OpenAI GPT-based Models",
    "volume": "Dec",
    "abstract": "Sarcasm is a form of irony that requires readers or listeners to interpret its intended meaning by considering context and social cues. Machine learning classification models have long had difficulty detecting sarcasm due to its social complexity and contradictory nature. This paper explores the applications of the Generative Pretrained Transformer (GPT) models, including GPT-3, InstructGPT, GPT-3.5, and GPT-4, in detecting sarcasm in natural language. It tests fine-tuned and zero-shot models of different sizes and releases. The GPT models were tested on the political and balanced (pol-bal) portion of the popular Self-Annotated Reddit Corpus (SARC 2.0) sarcasm dataset. In the fine-tuning case, the largest fine-tuned GPT-3 model achieves accuracy and $F_1$-score of 0.81, outperforming prior models. In the zero-shot case, one of GPT-4 models yields an accuracy of 0.70 and $F_1$-score of 0.75. Other models score lower. Additionally, a model's performance may improve or deteriorate with each release, highlighting the need to reassess performance after each release",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Montgomery Gole",
      "Williams-Paul Nwadiugwu",
      "Andriy Miranskyy"
    ]
  },
  "http://arxiv.org/abs/2312.04556": {
    "title": "Large Language Models for Mathematicians",
    "volume": "Dec",
    "abstract": "Large language models (LLMs) such as ChatGPT have received immense interest for their general-purpose language understanding and, in particular, their ability to generate high-quality text or computer code. For many professions, LLMs represent an invaluable tool that can speed up and improve the quality of work. In this note, we discuss to what extent they can aid professional mathematicians. We first provide a mathematical description of the transformer model used in all modern language models. Based on recent studies, we then outline best practices and potential issues and report on the mathematical abilities of language models. Finally, we shed light on the potential of LMMs to change how mathematicians work",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Frieder",
      "Julius Berner",
      "Philipp Petersen",
      "Thomas Lukasiewicz"
    ]
  },
  "http://arxiv.org/abs/2312.04554": {
    "title": "Improved Visual Grounding through Self-Consistent Explanations",
    "volume": "Dec",
    "abstract": "Vision-and-language models trained to match images with text can be combined with visual explanation methods to point to the locations of specific objects in an image. Our work shows that the localization --\"grounding\"-- abilities of these models can be further improved by finetuning for self-consistent visual explanations. We propose a strategy for augmenting existing text-image datasets with paraphrases using a large language model, and SelfEQ, a weakly-supervised strategy on visual explanation maps for paraphrases that encourages self-consistency. Specifically, for an input textual phrase, we attempt to generate a paraphrase and finetune the model so that the phrase and paraphrase map to the same region in the image. We posit that this both expands the vocabulary that the model is able to handle, and improves the quality of the object locations highlighted by gradient-based visual explanation methods (e.g. GradCAM). We demonstrate that SelfEQ improves performance on Flickr30k, ReferIt, and RefCOCO+ over a strong baseline method and several prior works. Particularly, comparing to other methods that do not use any type of box annotations, we obtain 84.07% on Flickr30k (an absolute improvement of 4.69%), 67.40% on ReferIt (an absolute improvement of 7.68%), and 75.10%, 55.49% on RefCOCO+ test sets A and B respectively (an absolute improvement of 3.74% on average)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruozhen He",
      "Paola Cascante-Bonilla",
      "Ziyan Yang",
      "Alexander C. Berg",
      "Vicente Ordonez"
    ]
  },
  "http://arxiv.org/abs/2312.04965": {
    "title": "Inversion-Free Image Editing with Natural Language",
    "volume": "Dec",
    "abstract": "Despite recent advances in inversion-based editing, text-guided image manipulation remains challenging for diffusion models. The primary bottlenecks include 1) the time-consuming nature of the inversion process; 2) the struggle to balance consistency with accuracy; 3) the lack of compatibility with efficient consistency sampling methods used in consistency models. To address the above issues, we start by asking ourselves if the inversion process can be eliminated for editing. We show that when the initial sample is known, a special variance schedule reduces the denoising step to the same form as the multi-step consistency sampling. We name this Denoising Diffusion Consistent Model (DDCM), and note that it implies a virtual inversion strategy without explicit inversion in sampling. We further unify the attention control mechanisms in a tuning-free framework for text-guided editing. Combining them, we present inversion-free editing (InfEdit), which allows for consistent and faithful editing for both rigid and non-rigid semantic changes, catering to intricate modifications without compromising on the image's integrity and explicit inversion. Through extensive experiments, InfEdit shows strong performance in various editing tasks and also maintains a seamless workflow (less than 3 seconds on one single A40), demonstrating the potential for real-time applications. Project Page: https://sled-group.github.io/InfEdit/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sihan Xu",
      "Yidong Huang",
      "Jiayi Pan",
      "Ziqiao Ma",
      "Joyce Chai"
    ]
  },
  "http://arxiv.org/abs/2312.04515": {
    "title": "Efficient Monotonic Multihead Attention",
    "volume": "Dec",
    "abstract": "We introduce the Efficient Monotonic Multihead Attention (EMMA), a state-of-the-art simultaneous translation model with numerically-stable and unbiased monotonic alignment estimation. In addition, we present improved training and inference strategies, including simultaneous fine-tuning from an offline translation model and reduction of monotonic alignment variance. The experimental results demonstrate that the proposed model attains state-of-the-art performance in simultaneous speech-to-text translation on the Spanish and English translation task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xutai Ma",
      "Anna Sun",
      "Siqi Ouyang",
      "Hirofumi Inaguma",
      "Paden Tomasello"
    ]
  },
  "http://arxiv.org/abs/2312.04511": {
    "title": "An LLM Compiler for Parallel Function Calling",
    "volume": "Dec",
    "abstract": "Large Language Models (LLMs) have shown remarkable results on various complex reasoning benchmarks. The reasoning capabilities of LLMs enable them to execute function calls, using user-provided functions to overcome their inherent limitations, such as knowledge cutoffs, poor arithmetic skills, or lack of access to private data. This development has expanded LLMs' scope to include multi-function calling, where LLMs are equipped with a variety of functions and select the proper functions based on the context. Multi-function calling abilities of LLMs have catalyzed LLM-based software development, allowing them to tackle more complex problems. However, current methods for multi-function calling often require sequential reasoning and acting for each function which can result in high latency, cost, and sometimes inaccurate behavior. To address this, we introduce LLMCompiler, which executes functions in parallel to efficiently orchestrate multi-function calling. Drawing from the principles of classical compilers, LLMCompiler streamlines parallel function calling with three components: (i) an LLM Planner, formulating execution strategies and dependencies; (ii) a Task Fetching Unit, dispatching function calling tasks; and (iii) an Executor, executing these tasks in parallel. LLMCompiler automatically computes an optimized orchestration for the function calls and can be used with open-source models such as LLaMA-2. We have benchmarked LLMCompiler on a range of tasks including cases with non-trivial inter-dependency between function calls, as well as cases that require dynamic replanning based on intermediate results. We observe consistent latency speedup of up to 3.7x, cost savings of up to 6.7x, and accuracy improvement of up to ~9% as compared to ReAct. Additionally, LLMCompiler achieves up to 1.35x latency gain over OpenAI's recent parallel function calling, while achieving similar accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sehoon Kim",
      "Suhong Moon",
      "Ryan Tabrizi",
      "Nicholas Lee",
      "Michael W. Mahoney",
      "Kurt Keutzer",
      "Amir Gholami"
    ]
  },
  "http://arxiv.org/abs/2312.04510": {
    "title": "A Block Metropolis-Hastings Sampler for Controllable Energy-based Text Generation",
    "volume": "Dec",
    "abstract": "Recent work has shown that energy-based language modeling is an effective framework for controllable text generation because it enables flexible integration of arbitrary discriminators. However, because energy-based LMs are globally normalized, approximate techniques like Metropolis-Hastings (MH) are required for inference. Past work has largely explored simple proposal distributions that modify a single token at a time, like in Gibbs sampling. In this paper, we develop a novel MH sampler that, in contrast, proposes re-writes of the entire sequence in each step via iterative prompting of a large language model. Our new sampler (a) allows for more efficient and accurate sampling from a target distribution and (b) allows generation length to be determined through the sampling procedure rather than fixed in advance, as past work has required. We perform experiments on two controlled generation tasks, showing both downstream performance gains and more accurate target distribution sampling in comparison with single-token proposal techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jarad Forristal",
      "Niloofar Mireshghallah",
      "Greg Durrett",
      "Taylor Berg-Kirkpatrick"
    ]
  },
  "http://arxiv.org/abs/2312.04474": {
    "title": "Chain of Code: Reasoning with a Language Model-Augmented Code Emulator",
    "volume": "Dec",
    "abstract": "Code provides a general syntactic structure to build complex programs and perform precise computations when paired with a code interpreter -- we hypothesize that language models (LMs) can leverage code-writing to improve Chain of Thought reasoning not only for logic and arithmetic tasks, but also for linguistic ones (and in particular, those that are a mix of both). For example, consider prompting an LM to write code that counts the number of times it detects sarcasm in an essay: the LM may struggle to write an implementation for \"detect_sarcasm(string)\" that can be executed by the interpreter (handling the edge cases would be insurmountable). However, LMs may still produce a valid solution if they are used not only to write the code, but also to selectively \"emulate\" the interpreter by generating the expected output of \"detect_sarcasm(string)\" and other lines of code (e.g., that the interpreter could not compile). In this work, we propose Chain of Code (CoT), a simple yet surprisingly effective extension that improves LM code-driven reasoning. The key idea is to encourage LMs to format linguistic sub-tasks in a program as flexible pseudocode that the compiler can explicitly catch undefined behaviors and hand off to simulate with an LM (as an \"LMulator\"). Experiments demonstrate that Chain of Code outperforms Chain of Thought and other baselines across a variety of benchmarks; on BIG-Bench Hard, Chain of Code achieves 84%, a gain of 12% over Chain of Thought. CoT scales well with large and small models alike, and broadens the scope of reasoning questions that LMs can correctly answer by \"thinking in code\". Project webpage: https://chain-of-code.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengshu Li",
      "Jacky Liang",
      "Andy Zeng",
      "Xinyun Chen",
      "Karol Hausman",
      "Dorsa Sadigh",
      "Sergey Levine",
      "Li Fei-Fei",
      "Fei Xia",
      "Brian Ichter"
    ]
  },
  "http://arxiv.org/abs/2312.04469": {
    "title": "On the Learnability of Watermarks for Language Models",
    "volume": "Dec",
    "abstract": "Watermarking of language model outputs enables statistical detection of model-generated text, which has many applications in the responsible deployment of language models. Existing watermarking strategies operate by altering the decoder of an existing language model, and the ability for a language model to directly learn to generate the watermark would have significant implications for the real-world deployment of watermarks. First, learned watermarks could be used to build open models that naturally generate watermarked text, allowing for open models to benefit from watermarking. Second, if watermarking is used to determine the provenance of generated text, an adversary can hurt the reputation of a victim model by spoofing its watermark and generating damaging watermarked text. To investigate the learnability of watermarks, we propose watermark distillation, which trains a student model to behave like a teacher model that uses decoding-based watermarking. We test our approach on three distinct decoding-based watermarking strategies and various hyperparameter settings, finding that models can learn to generate watermarked text with high detectability. We also find limitations to learnability, including the loss of watermarking capabilities under fine-tuning on normal text and high sample complexity when learning low-distortion watermarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenchen Gu",
      "Xiang Lisa Li",
      "Percy Liang",
      "Tatsunori Hashimoto"
    ]
  },
  "http://arxiv.org/abs/2312.04455": {
    "title": "Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use",
    "volume": "Dec",
    "abstract": "Recent advancements in large language models (LLMs) have significantly expanded their functionality and skills as tool agents. In this paper, we argue that a waveform pattern in the model's attention allocation has an impact on the tool use performance, which degrades when the position of essential information hits the trough zone. To address this issue, we propose a novel inference method named Attention Buckets. This approach enables LLMs to handle context by conducting parallel processes, each featuring a unique RoPE angle base that shapes the attention waveform. Attention Buckets ensures that an attention trough of a particular process can be compensated with an attention peak of another run, reducing the risk of the LLM missing essential information residing within the attention trough. Our extensive experiments on the widely recognized tool use benchmark demonstrate the efficacy of our approach, where a 7B-parameter open-source model enhanced by Attention Buckets achieves SOTA performance on par with GPT-4",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Chen",
      "Ang Lv",
      "Ting-En Lin",
      "Changyu Chen",
      "Yuchuan Wu",
      "Fei Huang",
      "Yongbin Li",
      "Rui Yan"
    ]
  },
  "http://arxiv.org/abs/2312.04440": {
    "title": "OpenAsp: A Benchmark for Multi-document Open Aspect-based Summarization",
    "volume": "Dec",
    "abstract": "The performance of automatic summarization models has improved dramatically in recent years. Yet, there is still a gap in meeting specific information needs of users in real-world scenarios, particularly when a targeted summary is sought, such as in the useful aspect-based summarization setting targeted in this paper. Previous datasets and studies for this setting have predominantly concentrated on a limited set of pre-defined aspects, focused solely on single document inputs, or relied on synthetic data. To advance research on more realistic scenarios, we introduce OpenAsp, a benchmark for multi-document \\textit{open} aspect-based summarization. This benchmark is created using a novel and cost-effective annotation protocol, by which an open aspect dataset is derived from existing generic multi-document summarization datasets. We analyze the properties of OpenAsp showcasing its high-quality content. Further, we show that the realistic open-aspect setting realized in OpenAsp poses a challenge for current state-of-the-art summarization models, as well as for large language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shmuel Amar",
      "Liat Schiff",
      "Ori Ernst",
      "Asi Shefer",
      "Ori Shapira",
      "Ido Dagan"
    ]
  },
  "http://arxiv.org/abs/2312.04372": {
    "title": "LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs",
    "volume": "Dec",
    "abstract": "We present LaMPilot, a novel framework for planning in the field of autonomous driving, rethinking the task as a code-generation process that leverages established behavioral primitives. This approach aims to address the challenge of interpreting and executing spontaneous user instructions such as \"overtake the car ahead,\" which have typically posed difficulties for existing frameworks. We introduce the LaMPilot benchmark specifically designed to quantitatively evaluate the efficacy of Large Language Models (LLMs) in translating human directives into actionable driving policies. We then evaluate a wide range of state-of-the-art code generation language models on tasks from the LaMPilot Benchmark. The results of the experiments showed that GPT-4, with human feedback, achieved an impressive task completion rate of 92.7% and a minimal collision rate of 0.9%. To encourage further investigation in this area, our code and dataset will be made available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunsheng Ma",
      "Can Cui",
      "Xu Cao",
      "Wenqian Ye",
      "Peiran Liu",
      "Juanwu Lu",
      "Amr Abdelraouf",
      "Rohit Gupta",
      "Kyungtae Han",
      "Aniket Bera",
      "James M. Rehg",
      "Ziran Wang"
    ]
  },
  "http://arxiv.org/abs/2312.04362": {
    "title": "PCoQA: Persian Conversational Question Answering Dataset",
    "volume": "Dec",
    "abstract": "Humans seek information regarding a specific topic through performing a conversation containing a series of questions and answers. In the pursuit of conversational question answering research, we introduce the PCoQA, the first \\textbf{P}ersian \\textbf{Co}nversational \\textbf{Q}uestion \\textbf{A}nswering dataset, a resource comprising information-seeking dialogs encompassing a total of 9,026 contextually-driven questions. Each dialog involves a questioner, a responder, and a document from the Wikipedia; The questioner asks several inter-connected questions from the text and the responder provides a span of the document as the answer for each question. PCoQA is designed to present novel challenges compared to previous question answering datasets including having more open-ended non-factual answers, longer answers, and fewer lexical overlaps. This paper not only presents the comprehensive PCoQA dataset but also reports the performance of various benchmark models. Our models include baseline models and pre-trained models, which are leveraged to boost the performance of the model. The dataset and benchmarks are available at our Github page",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hamed Hematian Hemati",
      "Atousa Toghyani",
      "Atena Souri",
      "Sayed Hesam Alavian",
      "Hossein Sameti",
      "Hamid Beigy"
    ]
  },
  "http://arxiv.org/abs/2312.04350": {
    "title": "CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models",
    "volume": "Dec",
    "abstract": "The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules. To address this, we propose a new NLP task, causal inference in natural language, inspired by the \"causal inference engine\" postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-thought prompting strategy, CausalCoT. We show that our task is highly challenging for LLMs, and we conduct an in-depth analysis to gain deeper insight into the causal reasoning abilities of LLMs. Our data is open-sourced at https://huggingface.co/datasets/causalNLP/cladder, and our code can be found at https://github.com/causalNLP/cladder",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijing Jin",
      "Yuen Chen",
      "Felix Leeb",
      "Luigi Gresele",
      "Ojasv Kamal",
      "Zhiheng Lyu",
      "Kevin Blin",
      "Fernando Gonzalez Adauto",
      "Max Kleiman-Weiner",
      "Mrinmaya Sachan",
      "Bernhard Schölkopf"
    ]
  },
  "http://arxiv.org/abs/2312.04348": {
    "title": "When Input Integers are Given in the Unary Numeral Representation",
    "volume": "Dec",
    "abstract": "Many NP-complete problems take integers as part of their input instances. These input integers are generally binarized, that is, provided in the form of the \"binary\" numeral representation, and the lengths of such binary forms are used as a basis unit to measure the computational complexity of the problems. In sharp contrast, the \"unarization\" (or the \"unary\" numeral representation) of numbers has been known to bring a remarkably different effect onto the computational complexity of the problems. When no computational-complexity difference is observed between binarization and unarization of instances, on the contrary, the problems are said to be strong NP-complete. This work attempts to spotlight an issue of how the unarization of instances affects the computational complexity of various combinatorial problems. We present numerous NP-complete (or even NP-hard) problems, which turn out to be easily solvable when input integers are represented in unary. We then discuss the computational complexities of such problems when taking unary-form integer inputs. We hope that a list of such problems signifies the structural differences between strong NP-completeness and non-strong NP-completeness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomoyuki Yamakami"
    ]
  },
  "http://arxiv.org/abs/2312.04344": {
    "title": "Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on Prompt Engineering Strategies",
    "volume": "Dec",
    "abstract": "OpenAI's latest large vision-language model (LVLM), GPT-4V(ision), has piqued considerable interest for its potential in medical applications. Despite its promise, recent studies and internal reviews highlight its underperformance in specialized medical tasks. This paper explores the boundary of GPT-4V's capabilities in medicine, particularly in processing complex imaging data from endoscopies, CT scans, and MRIs etc. Leveraging open-source datasets, we assessed its foundational competencies, identifying substantial areas for enhancement. Our research emphasizes prompt engineering, an often-underutilized strategy for improving AI responsiveness. Through iterative testing, we refined the model's prompts, significantly improving its interpretative accuracy and relevance in medical imaging. From our comprehensive evaluations, we distilled 10 effective prompt engineering techniques, each fortifying GPT-4V's medical acumen. These methodical enhancements facilitate more reliable, precise, and clinically valuable insights from GPT-4V, advancing its operability in critical healthcare environments. Our findings are pivotal for those employing AI in medicine, providing clear, actionable guidance on harnessing GPT-4V's full diagnostic potential",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengcheng Chen",
      "Ziyan Huang",
      "Zhongying Deng",
      "Tianbin Li",
      "Yanzhou Su",
      "Haoyu Wang",
      "Jin Ye",
      "Yu Qiao",
      "Junjun He"
    ]
  },
  "http://arxiv.org/abs/2312.04339": {
    "title": "Merging by Matching Models in Task Subspaces",
    "volume": "Dec",
    "abstract": "Model merging aims to cheaply combine individual task-specific models into a single multitask model. In this work, we view past merging methods as leveraging different notions of a ''task subspace'' in which models are matched before being merged. We connect the task subspace of a given model to its loss landscape and formalize how this approach to model merging can be seen as solving a linear system of equations. While past work has generally been limited to linear systems that have a closed-form solution, we consider using the conjugate gradient method to find a solution. We show that using the conjugate gradient method can outperform closed-form solutions, enables merging via linear systems that are otherwise intractable to solve, and flexibly allows choosing from a wide variety of initializations and estimates for the ''task subspace''. We ultimately demonstrate that our merging framework called ''Matching Models in their Task Subspace'' (MaTS) achieves state-of-the-art results in multitask and intermediate-task model merging. We release all of the code and checkpoints used in our work at https://github.com/r-three/mats",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Derek Tam",
      "Mohit Bansal",
      "Colin Raffel"
    ]
  },
  "http://arxiv.org/abs/2312.04333": {
    "title": "Beyond Surface: Probing LLaMA Across Scales and Layers",
    "volume": "Dec",
    "abstract": "This paper presents an in-depth analysis of Large Language Models (LLMs), focusing on LLaMA, a prominent open-source foundational model in natural language processing. Instead of assessing LLaMA through its generative output, we design multiple-choice tasks to probe its intrinsic understanding in high-order tasks such as reasoning and computation. We examine the model horizontally, comparing different sizes, and vertically, assessing different layers. We unveil several key and uncommon findings based on the designed probing tasks: (1) Horizontally, enlarging model sizes almost could not automatically impart additional knowledge or computational prowess. Instead, it can enhance reasoning abilities, especially in math problem solving, and helps reduce hallucinations, but only beyond certain size thresholds; (2) In vertical analysis, the lower layers of LLaMA lack substantial arithmetic and factual knowledge, showcasing logical thinking, multilingual and recognitive abilities, with top layers housing most computational power and real-world knowledge",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nuo Chen",
      "Ning Wu",
      "Shining Liang",
      "Ming Gong",
      "Linjun Shou",
      "Dongmei Zhang",
      "Jia Li"
    ]
  },
  "http://arxiv.org/abs/2312.04306": {
    "title": "nerblackbox: A High-level Library for Named Entity Recognition in Python",
    "volume": "Dec",
    "abstract": "We present nerblackbox, a python library to facilitate the use of state-of-the-art transformer-based models for named entity recognition. It provides simple-to-use yet powerful methods to access data and models from a wide range of sources, for fully automated model training and evaluation as well as versatile model inference. While many technical challenges are solved and hidden from the user by default, nerblackbox also offers fine-grained control and a rich set of customizable features. It is thus targeted both at application-oriented developers as well as machine learning experts and researchers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Stollenwerk"
    ]
  },
  "http://arxiv.org/abs/2312.04302": {
    "title": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs",
    "volume": "Dec",
    "abstract": "This study targets a critical aspect of multi-modal LLMs' (LLMs&VLMs) inference: explicit controllable text generation. Multi-modal LLMs empower multi-modality understanding with the capability of semantic generation yet bring less explainability and heavier reliance on prompt contents due to their autoregressive generative nature. While manipulating prompt formats could improve outputs, designing specific and precise prompts per task can be challenging and ineffective. To tackle this issue, we introduce a novel inference method, Prompt Highlighter, which enables users to highlight specific prompt spans to interactively control the focus during generation. Motivated by the classifier-free diffusion guidance, we form regular and unconditional context pairs based on highlighted tokens, demonstrating that the autoregressive generation in models can be guided in a classifier-free way. Notably, we find that, during inference, guiding the models with highlighted tokens through the attention weights leads to more desired outputs. Our approach is compatible with current LLMs and VLMs, achieving impressive customized generation results without training. Experiments confirm its effectiveness in focusing on input contexts and generating reliable content. Without tuning on LLaVA-v1.5, our method secured 69.5 in the MMBench test and 1552.5 in MME-perception. The code is available at: https://github.com/dvlab-research/Prompt-Highlighter/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuechen Zhang",
      "Shengju Qian",
      "Bohao Peng",
      "Shu Liu",
      "Jiaya Jia"
    ]
  },
  "http://arxiv.org/abs/2312.04262": {
    "title": "PsyChat: A Client-Centric Dialogue System for Mental Health Support",
    "volume": "Dec",
    "abstract": "Dialogue systems are increasingly integrated into mental health support to help clients facilitate exploration, gain insight, take action, and ultimately heal themselves. For a dialogue system to be practical and user-friendly, it should be client-centric, focusing on the client's behaviors. However, existing dialogue systems publicly available for mental health support often concentrate solely on the counselor's strategies rather than the behaviors expressed by clients. This can lead to the implementation of unreasonable or inappropriate counseling strategies and corresponding responses from the dialogue system. To address this issue, we propose PsyChat, a client-centric dialogue system that provides psychological support through online chat. The client-centric dialogue system comprises five modules: client behavior recognition, counselor strategy selection, input packer, response generator intentionally fine-tuned to produce responses, and response selection. Both automatic and human evaluations demonstrate the effectiveness and practicality of our proposed dialogue system for real-life mental health support. Furthermore, we employ our proposed dialogue system to simulate a real-world client-virtual-counselor interaction scenario. The system is capable of predicting the client's behaviors, selecting appropriate counselor strategies, and generating accurate and suitable responses, as demonstrated in the scenario",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huachuan Qiu",
      "Anqi Li",
      "Lizhi Ma",
      "Zhenzhong Lan"
    ]
  },
  "http://arxiv.org/abs/2312.07553": {
    "title": "Hijacking Context in Large Multi-modal Models",
    "volume": "Dec",
    "abstract": "Recently, Large Multi-modal Models (LMMs) have demonstrated their ability to understand the visual contents of images given the instructions regarding the images. Built upon the Large Language Models (LLMs), LMMs also inherit their abilities and characteristics such as in-context learning where a coherent sequence of images and texts are given as the input prompt. However, we identify a new limitation of off-the-shelf LMMs where a small fraction of incoherent images or text descriptions mislead LMMs to only generate biased output about the hijacked context, not the originally intended context. To address this, we propose a pre-filtering method that removes irrelevant contexts via GPT-4V, based on its robustness towards distribution shift within the contexts. We further investigate whether replacing the hijacked visual and textual contexts with the correlated ones via GPT-4V and text-to-image models can help yield coherent responses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joonhyun Jeong"
    ]
  },
  "http://arxiv.org/abs/2312.04219": {
    "title": "Swap distance minimization in SOV languages. Cognitive and mathematical foundations",
    "volume": "Dec",
    "abstract": "Distance minimization is a general principle of language. A special case of this principle in the domain of word order is swap distance minimization. This principle predicts that variations from a canonical order that are reached by fewer swaps of adjacent constituents are lest costly and thus more likely. Here we investigate the principle in the context of the triple formed by subject (S), object (O) and verb (V). We introduce the concept of word order rotation as a cognitive underpinning of that prediction. When the canonical order of a language is SOV, the principle predicts SOV < SVO, OSV < VSO, OVS < VOS, in order of increasing cognitive cost. We test the prediction in three flexible order SOV languages: Korean (Koreanic), Malayalam (Dravidian), and Sinhalese (Indo-European). Evidence of swap distance minimization is found in all three languages, but it is weaker in Sinhalese. Swap distance minimization is stronger than a preference for the canonical order in Korean and especially Malayalam",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ramon Ferrer-i-Cancho",
      "Savithry Namboodiripad"
    ]
  },
  "http://arxiv.org/abs/2312.04193": {
    "title": "Language Model Knowledge Distillation for Efficient Question Answering in Spanish",
    "volume": "Dec",
    "abstract": "Recent advances in the development of pre-trained Spanish language models has led to significant progress in many Natural Language Processing (NLP) tasks, such as question answering. However, the lack of efficient models imposes a barrier for the adoption of such models in resource-constrained environments. Therefore, smaller distilled models for the Spanish language could be proven to be highly scalable and facilitate their further adoption on a variety of tasks and scenarios. In this work, we take one step in this direction by developing SpanishTinyRoBERTa, a compressed language model based on RoBERTa for efficient question answering in Spanish. To achieve this, we employ knowledge distillation from a large model onto a lighter model that allows for a wider implementation, even in areas with limited computational resources, whilst attaining negligible performance sacrifice. Our experiments show that the dense distilled model can still preserve the performance of its larger counterpart, while significantly increasing inference speedup. This work serves as a starting point for further research and investigation of model compression efforts for Spanish language models across various NLP tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrián Bazaga",
      "Pietro Liò",
      "Gos Micklem"
    ]
  },
  "http://arxiv.org/abs/2312.04134": {
    "title": "Using a Large Language Model to generate a Design Structure Matrix",
    "volume": "Dec",
    "abstract": "The Design Structure Matrix (DSM) is an established method used in dependency modelling, especially in the design of complex engineering systems. The generation of DSM is traditionally carried out through manual means and can involve interviewing experts to elicit critical system elements and the relationships between them. Such manual approaches can be time-consuming and costly. This paper presents a workflow that uses a Large Language Model (LLM) to support the generation of DSM and improve productivity. A prototype of the workflow was developed in this work and applied on a diesel engine DSM published previously. It was found that the prototype could reproduce 357 out of 462 DSM entries published (i.e. 77.3%), suggesting that the work can aid DSM generation. A no-code version of the prototype is made available online to support future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edwin C. Y. Koh"
    ]
  },
  "http://arxiv.org/abs/2312.04127": {
    "title": "Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak",
    "volume": "Dec",
    "abstract": "Extensive work has been devoted to improving the safety mechanism of Large Language Models (LLMs). However, in specific scenarios, LLMs still generate harmful responses when faced with malicious instructions, a phenomenon referred to as \"Jailbreak Attack\". In our research, we introduce a novel jailbreak attack method (\\textbf{RADIAL}), which consists of two steps: 1) Inherent Response Tendency Analysis: we analyze the inherent affirmation and rejection tendency of LLMs to react to real-world instructions. 2) Real-World Instructions-Driven Jailbreak: based on our analysis, we strategically choose several real-world instructions and embed malicious instructions into them to amplify the LLM's potential to generate harmful responses. On three open-source human-aligned LLMs, our method achieves excellent jailbreak attack performance for both Chinese and English malicious instructions. Besides, we guided detailed ablation experiments and verified the effectiveness of our core idea \"Inherent Response Tendency Analysis\". Our exploration also exposes the vulnerability of LLMs to being induced into generating more detailed harmful responses in subsequent rounds of dialogue",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanrui Du",
      "Sendong Zhao",
      "Ming Ma",
      "Yuhan Chen",
      "Bing Qin"
    ]
  },
  "http://arxiv.org/abs/2312.04103": {
    "title": "Enhancing the Rationale-Input Alignment for Self-explaining Rationalization",
    "volume": "Dec",
    "abstract": "Rationalization empowers deep learning models with self-explaining capabilities through a cooperative game, where a generator selects a semantically consistent subset of the input as a rationale, and a subsequent predictor makes predictions based on the selected rationale. In this paper, we discover that rationalization is prone to a problem named \\emph{rationale shift}, which arises from the algorithmic bias of the cooperative game. Rationale shift refers to a situation where the semantics of the selected rationale may deviate from the original input, but the predictor still produces accurate predictions based on the deviation, resulting in a compromised generator with misleading feedback. To address this issue, we first demonstrate the importance of the alignment between the rationale and the full input through both empirical observations and theoretical analysis. Subsequently, we introduce a novel approach called DAR (\\textbf{D}iscriminatively \\textbf{A}ligned \\textbf{R}ationalization), which utilizes an auxiliary module pretrained on the full input to discriminatively align the selected rationale and the original input. We theoretically illustrate how DAR accomplishes the desired alignment, thereby overcoming the rationale shift problem. The experiments on two widely used real-world benchmarks show that the proposed method significantly improves the explanation quality (measured by the overlap between the model-selected explanation and the human-annotated rationale) as compared to state-of-the-art techniques. Additionally, results on two synthetic settings further validate the effectiveness of DAR in addressing the rationale shift problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Liu",
      "Haozhao Wang",
      "Jun Wang",
      "Zhiying Deng",
      "YuanKai Zhang",
      "Cheng Wang",
      "Ruixuan Li"
    ]
  },
  "http://arxiv.org/abs/2312.04068": {
    "title": "Making Translators Privacy-aware on the User's Side",
    "volume": "Dec",
    "abstract": "We propose PRISM to enable users of machine translation systems to preserve the privacy of data on their own initiative. There is a growing demand to apply machine translation systems to data that require privacy protection. While several machine translation engines claim to prioritize privacy, the extent and specifics of such protection are largely ambiguous. First, there is often a lack of clarity on how and to what degree the data is protected. Even if service providers believe they have sufficient safeguards in place, sophisticated adversaries might still extract sensitive information. Second, vulnerabilities may exist outside of these protective measures, such as within communication channels, potentially leading to data leakage. As a result, users are hesitant to utilize machine translation engines for data demanding high levels of privacy protection, thereby missing out on their benefits. PRISM resolves this problem. Instead of relying on the translation service to keep data safe, PRISM provides the means to protect data on the user's side. This approach ensures that even machine translation engines with inadequate privacy measures can be used securely. For platforms already equipped with privacy safeguards, PRISM acts as an additional protection layer, reinforcing their security furthermore. PRISM adds these privacy features without significantly compromising translation accuracy. Our experiments demonstrate the effectiveness of PRISM using real-world translators, T5 and ChatGPT (GPT-3.5-turbo), and the datasets with two languages. PRISM effectively balances privacy protection with translation accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryoma Sato"
    ]
  },
  "http://arxiv.org/abs/2312.04059": {
    "title": "Comparing Large Language Model AI and Human-Generated Coaching Messages for Behavioral Weight Loss",
    "volume": "Dec",
    "abstract": "Automated coaching messages for weight control can save time and costs, but their repetitive, generic nature may limit their effectiveness compared to human coaching. Large language model (LLM) based artificial intelligence (AI) chatbots, like ChatGPT, could offer more personalized and novel messages to address repetition with their data-processing abilities. While LLM AI demonstrates promise to encourage healthier lifestyles, studies have yet to examine the feasibility and acceptability of LLM-based BWL coaching. 87 adults in a weight-loss trial rated ten coaching messages' helpfulness (five human-written, five ChatGPT-generated) using a 5-point Likert scale, providing additional open-ended feedback to justify their ratings. Participants also identified which messages they believed were AI-generated. The evaluation occurred in two phases: messages in Phase 1 were perceived as impersonal and negative, prompting revisions for Phase 2 messages. In Phase 1, AI-generated messages were rated less helpful than human-written ones, with 66 percent receiving a helpfulness rating of 3 or higher. However, in Phase 2, the AI messages matched the human-written ones regarding helpfulness, with 82% scoring three or above. Additionally, 50% were misidentified as human-written, suggesting AI's sophistication in mimicking human-generated content. A thematic analysis of open-ended feedback revealed that participants appreciated AI's empathy and personalized suggestions but found them more formulaic, less authentic, and too data-focused. This study reveals the preliminary feasibility and acceptability of LLM AIs, like ChatGPT, in crafting potentially effective weight control coaching messages. Our findings also underscore areas for future enhancement",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoran Huang",
      "Michael P. Berry",
      "Christina Chwyl",
      "Gary Hsieh",
      "Jing Wei",
      "Evan M. Forman"
    ]
  },
  "http://arxiv.org/abs/2312.04052": {
    "title": "Multimodal Misinformation Detection in a South African Social Media Environment",
    "volume": "Dec",
    "abstract": "With the constant spread of misinformation on social media networks, a need has arisen to continuously assess the veracity of digital content. This need has inspired numerous research efforts on the development of misinformation detection (MD) models. However, many models do not use all information available to them and existing research contains a lack of relevant datasets to train the models, specifically within the South African social media environment. The aim of this paper is to investigate the transferability of knowledge of a MD model between different contextual environments. This research contributes a multimodal MD model capable of functioning in the South African social media environment, as well as introduces a South African misinformation dataset. The model makes use of multiple sources of information for misinformation detection, namely: textual and visual elements. It uses bidirectional encoder representations from transformers (BERT) as the textual encoder and a residual network (ResNet) as the visual encoder. The model is trained and evaluated on the Fakeddit dataset and a South African misinformation dataset. Results show that using South African samples in the training of the model increases model performance, in a South African contextual environment, and that a multimodal model retains significantly more knowledge than both the textual and visual unimodal models. Our study suggests that the performance of a misinformation detection model is influenced by the cultural nuances of its operating environment and multimodal models assist in the transferability of knowledge between different contextual environments. Therefore, local data should be incorporated into the training process of a misinformation detection model in order to optimize model performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amica De Jager",
      "Vukosi Marivate",
      "Abioudun Modupe"
    ]
  },
  "http://arxiv.org/abs/2312.04032": {
    "title": "RoAST: Robustifying Language Models via Adversarial Perturbation with Selective Training",
    "volume": "Dec",
    "abstract": "Fine-tuning pre-trained language models (LMs) has become the de facto standard in many NLP tasks. Nevertheless, fine-tuned LMs are still prone to robustness issues, such as adversarial robustness and model calibration. Several perspectives of robustness for LMs have been studied independently, but lacking a unified consideration in multiple perspectives. In this paper, we propose Robustifying LMs via Adversarial perturbation with Selective Training (RoAST), a simple yet effective fine-tuning technique to enhance the multi-perspective robustness of LMs in a unified way. RoAST effectively incorporates two important sources for the model robustness, robustness on the perturbed inputs and generalizable knowledge in pre-trained LMs. To be specific, RoAST introduces adversarial perturbation during fine-tuning while the model parameters are selectively updated upon their relative importance to minimize unnecessary deviation. Under a unified evaluation of fine-tuned LMs by incorporating four representative perspectives of model robustness, we demonstrate the effectiveness of RoAST compared to state-of-the-art fine-tuning methods on six different types of LMs, which indicates its usefulness in practice",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaehyung Kim",
      "Yuning Mao",
      "Rui Hou",
      "Hanchao Yu",
      "Davis Liang",
      "Pascale Fung",
      "Qifan Wang",
      "Fuli Feng",
      "Lifu Huang",
      "Madian Khabsa"
    ]
  },
  "http://arxiv.org/abs/2312.04021": {
    "title": "A Study on the Calibration of In-context Learning",
    "volume": "Dec",
    "abstract": "Modern auto-regressive language models are trained to minimize log loss on broad data by predicting the next token so they are expected to get calibrated answers when framing a problem as a next-token prediction task. We study this for in-context learning (ICL), a widely used way to adapt frozen large language models (LLMs) via crafting prompts, and investigate the trade-offs between performance and calibration on a wide range of natural language understanding and reasoning tasks. We conduct extensive experiments to show that such trade-offs may get worse as we increase model size, incorporate more ICL examples, and fine-tune models using instruction, dialog, or reinforcement learning from human feedback (RLHF) on carefully curated datasets. Furthermore, we find that common recalibration techniques that are widely effective such as temperature scaling provide limited gains in calibration errors, suggesting that new methods may be required for settings where models are expected to be reliable",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanlin Zhang",
      "Yi-Fan Zhang",
      "Yaodong Yu",
      "Dhruv Madeka",
      "Dean Foster",
      "Eric Xing",
      "Hima Lakkaraju",
      "Sham Kakade"
    ]
  },
  "http://arxiv.org/abs/2312.07552": {
    "title": "Large Language Models for Intent-Driven Session Recommendations",
    "volume": "Dec",
    "abstract": "Intent-aware session recommendation (ISR) is pivotal in discerning user intents within sessions for precise predictions. Traditional approaches, however, face limitations due to their presumption of a uniform number of intents across all sessions. This assumption overlooks the dynamic nature of user sessions, where the number and type of intentions can significantly vary. In addition, these methods typically operate in latent spaces, thus hinder the model's transparency.Addressing these challenges, we introduce a novel ISR approach, utilizing the advanced reasoning capabilities of large language models (LLMs). First, this approach begins by generating an initial prompt that guides LLMs to predict the next item in a session, based on the varied intents manifested in user sessions. Then, to refine this process, we introduce an innovative prompt optimization mechanism that iteratively self-reflects and adjusts prompts. Furthermore, our prompt selection module, built upon the LLMs' broad adaptability, swiftly selects the most optimized prompts across diverse domains. This new paradigm empowers LLMs to discern diverse user intents at a semantic level, leading to more accurate and interpretable session recommendations. Our extensive experiments on three real-world datasets demonstrate the effectiveness of our method, marking a significant advancement in ISR systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhu Sun",
      "Hongyang Liu",
      "Xinghua Qu",
      "Kaidong Feng",
      "Yan Wang",
      "Yew-Soon Ong"
    ]
  },
  "http://arxiv.org/abs/2312.03987": {
    "title": "Cost-Effective In-Context Learning for Entity Resolution: A Design Space Exploration",
    "volume": "Dec",
    "abstract": "Entity resolution (ER) is an important data integration task with a wide spectrum of applications. The state-of-the-art solutions on ER rely on pre-trained language models (PLMs), which require fine-tuning on a lot of labeled matching/non-matching entity pairs. Recently, large languages models (LLMs), such as GPT-4, have shown the ability to perform many tasks without tuning model parameters, which is known as in-context learning (ICL) that facilitates effective learning from a few labeled input context demonstrations. However, existing ICL approaches to ER typically necessitate providing a task description and a set of demonstrations for each entity pair and thus have limitations on the monetary cost of interfacing LLMs. To address the problem, in this paper, we provide a comprehensive study to investigate how to develop a cost-effective batch prompting approach to ER. We introduce a framework BATCHER consisting of demonstration selection and question batching and explore different design choices that support batch prompting for ER. We also devise a covering-based demonstration selection strategy that achieves an effective balance between matching accuracy and monetary cost. We conduct a thorough evaluation to explore the design space and evaluate our proposed strategies. Through extensive experiments, we find that batch prompting is very cost-effective for ER, compared with not only PLM-based methods fine-tuned with extensive labeled data but also LLM-based methods with manually designed prompting. We also provide guidance for selecting appropriate design choices for batch prompting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meihao Fan",
      "Xiaoyue Han",
      "Ju Fan",
      "Chengliang Chai",
      "Nan Tang",
      "Guoliang Li",
      "Xiaoyong Du"
    ]
  },
  "http://arxiv.org/abs/2312.03912": {
    "title": "Collaboration or Corporate Capture? Quantifying NLP's Reliance on Industry Artifacts and Contributions",
    "volume": "Dec",
    "abstract": "The advent of transformers, higher computational budgets, and big data has engendered remarkable progress in Natural Language Processing (NLP). Impressive performance of industry pre-trained models has garnered public attention in recent years and made news headlines. That these are industry models is noteworthy. Rarely, if ever, are academic institutes producing exciting new NLP models. Using these models is critical for competing on NLP benchmarks and correspondingly to stay relevant in NLP research. We surveyed 100 papers published at EMNLP 2022 to determine whether this phenomenon constitutes a reliance on industry for NLP publications. We find that there is indeed a substantial reliance. Citations of industry artifacts and contributions across categories is at least three times greater than industry publication rates per year. Quantifying this reliance does not settle how we ought to interpret the results. We discuss two possible perspectives in our discussion: 1) Is collaboration with industry still collaboration in the absence of an alternative? Or 2) has free NLP inquiry been captured by the motivations and research direction of private corporations?",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Will Aitken",
      "Mohamed Abdalla",
      "Karen Rudie",
      "Catherine Stinson"
    ]
  },
  "http://arxiv.org/abs/2312.03905": {
    "title": "A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints",
    "volume": "Dec",
    "abstract": "Neuro-symbolic AI bridges the gap between purely symbolic and neural approaches to learning. This often requires maximizing the likelihood of a symbolic constraint w.r.t the neural network's output distribution. Such output distributions are typically assumed to be fully-factorized. This limits the applicability of neuro-symbolic learning to the more expressive autoregressive distributions, e.g., transformers. Under such distributions, computing the likelihood of even simple constraints is #P-hard. Instead of attempting to enforce the constraint on the entire output distribution, we propose to do so on a random, local approximation thereof. More precisely, we optimize the likelihood of the constraint under a pseudolikelihood-based approximation centered around a model sample. Our approximation is factorized, allowing the reuse of solutions to sub-problems, a main tenet for efficiently computing neuro-symbolic losses. Moreover, it is a local, high-fidelity approximation of the likelihood, exhibiting low entropy and KL-divergence around the model sample. We evaluate our approach on Sudoku and shortest-path prediction cast as autoregressive generation, and observe that we greatly improve upon the base model's ability to predict logically-consistent outputs. We also evaluate on the task of detoxifying large language models. Using a simple constraint disallowing a list of toxic words, we are able to steer the model's outputs away from toxic generations, achieving SoTA detoxification compared to previous approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kareem Ahmed",
      "Kai-Wei Chang",
      "Guy Van den Broeck"
    ]
  },
  "http://arxiv.org/abs/2312.03897": {
    "title": "Revisiting the Optimality of Word Lengths",
    "volume": "Dec",
    "abstract": "Zipf (1935) posited that wordforms are optimized to minimize utterances' communicative costs. Under the assumption that cost is given by an utterance's length, he supported this claim by showing that words' lengths are inversely correlated with their frequencies. Communicative cost, however, can be operationalized in different ways. Piantadosi et al. (2011) claim that cost should be measured as the distance between an utterance's information rate and channel capacity, which we dub the channel capacity hypothesis (CCH) here. Following this logic, they then proposed that a word's length should be proportional to the expected value of its surprisal (negative log-probability in context). In this work, we show that Piantadosi et al.'s derivation does not minimize CCH's cost, but rather a lower bound, which we term CCH-lower. We propose a novel derivation, suggesting an improved way to minimize CCH's cost. Under this method, we find that a language's word lengths should instead be proportional to the surprisal's expectation plus its variance-to-mean ratio. Experimentally, we compare these three communicative cost functions: Zipf's, CCH-lower , and CCH. Across 13 languages and several experimental settings, we find that length is better predicted by frequency than either of the other hypotheses. In fact, when surprisal's expectation, or expectation plus variance-to-mean ratio, is estimated using better language models, it leads to worse word length predictions. We take these results as evidence that Zipf's longstanding hypothesis holds",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiago Pimentel",
      "Clara Meister",
      "Ethan Gotlieb Wilcox",
      "Kyle Mahowald",
      "Ryan Cotterell"
    ]
  },
  "http://arxiv.org/abs/2312.03872": {
    "title": "The BigCode Project Governance Card",
    "volume": "Dec",
    "abstract": "This document serves as an overview of the different mechanisms and areas of governance in the BigCode project. It aims to support transparency by providing relevant information about choices that were made during the project to the broader public, and to serve as an example of intentional governance of an open research project that future endeavors can leverage to shape their own approach. The first section, Project Structure, covers the project organization, its stated goals and values, its internal decision processes, and its funding and resources. The second section, Data and Model Governance, covers decisions relating to the questions of data subject consent, privacy, and model release",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "BigCode collaboration",
      "Sean Hughes",
      "Harm de Vries",
      "Jennifer Robinson",
      "Carlos Muñoz Ferrandis",
      "Loubna Ben Allal",
      "Leandro von Werra",
      "Jennifer Ding",
      "Sebastien Paquet",
      "Yacine Jernite"
    ]
  },
  "http://arxiv.org/abs/2312.03863": {
    "title": "Efficient Large Language Models: A Survey",
    "volume": "Dec",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in important tasks such as natural language understanding, language generation, and complex reasoning and have the potential to make a substantial impact on our society. Such capabilities, however, come with the considerable resources they demand, highlighting the strong need to develop effective techniques for addressing their efficiency challenges. In this survey, we provide a systematic and comprehensive review of efficient LLMs research. We organize the literature in a taxonomy consisting of three main categories, covering distinct yet interconnected efficient LLMs topics from model-centric, data-centric, and framework-centric perspective, respectively. We have also created a GitHub repository where we compile the papers featured in this survey at https://github.com/AIoT-MLSys-Lab/EfficientLLMs, https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey, and will actively maintain this repository and incorporate new research as it emerges. We hope our survey can serve as a valuable resource to help researchers and practitioners gain a systematic understanding of the research developments in efficient LLMs and inspire them to contribute to this important and exciting field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongwei Wan",
      "Xin Wang",
      "Che Liu",
      "Samiul Alam",
      "Yu Zheng",
      "Zhongnan Qu",
      "Shen Yan",
      "Yi Zhu",
      "Quanlu Zhang",
      "Mosharaf Chowdhury",
      "Mi Zhang"
    ]
  },
  "http://arxiv.org/abs/2312.03818": {
    "title": "Alpha-CLIP: A CLIP Model Focusing on Wherever You Want",
    "volume": "Dec",
    "abstract": "Contrastive Language-Image Pre-training (CLIP) plays an essential role in extracting valuable content information from images across diverse tasks. It aligns textual and visual modalities to comprehend the entire image, including all the details, even those irrelevant to specific tasks. However, for a finer understanding and controlled editing of images, it becomes crucial to focus on specific regions of interest, which can be indicated as points, masks, or boxes by humans or perception models. To fulfill the requirements, we introduce Alpha-CLIP, an enhanced version of CLIP with an auxiliary alpha channel to suggest attentive regions and fine-tuned with constructed millions of RGBA region-text pairs. Alpha-CLIP not only preserves the visual recognition ability of CLIP but also enables precise control over the emphasis of image contents. It demonstrates effectiveness in various tasks, including but not limited to open-world recognition, multimodal large language models, and conditional 2D / 3D generation. It has a strong potential to serve as a versatile tool for image-related tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyi Sun",
      "Ye Fang",
      "Tong Wu",
      "Pan Zhang",
      "Yuhang Zang",
      "Shu Kong",
      "Yuanjun Xiong",
      "Dahua Lin",
      "Jiaqi Wang"
    ]
  },
  "http://arxiv.org/abs/2312.03700": {
    "title": "OneLLM: One Framework to Align All Modalities with Language",
    "volume": "Dec",
    "abstract": "Multimodal large language models (MLLMs) have gained significant attention due to their strong multimodal understanding capability. However, existing works rely heavily on modality-specific encoders, which usually differ in architecture and are limited to common modalities. In this paper, we present OneLLM, an MLLM that aligns eight modalities to language using a unified framework. We achieve this through a unified multimodal encoder and a progressive multimodal alignment pipeline. In detail, we first train an image projection module to connect a vision encoder with LLM. Then, we build a universal projection module (UPM) by mixing multiple image projection modules and dynamic routing. Finally, we progressively align more modalities to LLM with the UPM. To fully leverage the potential of OneLLM in following instructions, we also curated a comprehensive multimodal instruction dataset, including 2M items from image, audio, video, point cloud, depth/normal map, IMU and fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks, encompassing tasks such as multimodal captioning, question answering and reasoning, where it delivers excellent performance. Code, data, model and online demo are available at https://github.com/csuhan/OneLLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Han",
      "Kaixiong Gong",
      "Yiyuan Zhang",
      "Jiaqi Wang",
      "Kaipeng Zhang",
      "Dahua Lin",
      "Yu Qiao",
      "Peng Gao",
      "Xiangyu Yue"
    ]
  },
  "http://arxiv.org/abs/2312.03699": {
    "title": "PROMISE: A Framework for Model-Driven Stateful Prompt Orchestration",
    "volume": "Dec",
    "abstract": "The advent of increasingly powerful language models has raised expectations for language-based interactions. However, controlling these models is a challenge, emphasizing the need to be able to investigate the feasibility and value of their application. We present PROMISE, a framework that facilitates the development of complex language-based interactions with information systems. Its use of state machine modeling concepts enables model-driven, dynamic prompt orchestration across hierarchically nested states and transitions. This improves the control of the behavior of language models and thus enables their effective and efficient use. We show the benefits of PROMISE in the context of application scenarios within health information systems and demonstrate its ability to handle complex interactions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyuan Wu",
      "Jasmin Heierli",
      "Max Meisterhans",
      "Adrian Moser",
      "Andri Färber",
      "Mateusz Dolata",
      "Elena Gavagnin",
      "Alexandre de Spindler",
      "Gerhard Schwabe"
    ]
  },
  "http://arxiv.org/abs/2312.03689": {
    "title": "Evaluating and Mitigating Discrimination in Language Model Decisions",
    "volume": "Dec",
    "abstract": "As language models (LMs) advance, interest is growing in applying them to high-stakes societal decisions, such as determining financing or housing eligibility. However, their potential for discrimination in such contexts raises ethical concerns, motivating the need for better methods to evaluate these risks. We present a method for proactively evaluating the potential discriminatory impact of LMs in a wide range of use cases, including hypothetical use cases where they have not yet been deployed. Specifically, we use an LM to generate a wide array of potential prompts that decision-makers may input into an LM, spanning 70 diverse decision scenarios across society, and systematically vary the demographic information in each prompt. Applying this methodology reveals patterns of both positive and negative discrimination in the Claude 2.0 model in select settings when no interventions are applied. While we do not endorse or permit the use of language models to make automated decisions for the high-risk use cases we study, we demonstrate techniques to significantly decrease both positive and negative discrimination through careful prompt engineering, providing pathways toward safer deployment in use cases where they may be appropriate. Our work enables developers and policymakers to anticipate, measure, and address discrimination as language model capabilities and applications continue to expand. We release our dataset and prompts at https://huggingface.co/datasets/Anthropic/discrim-eval",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Tamkin",
      "Amanda Askell",
      "Liane Lovitt",
      "Esin Durmus",
      "Nicholas Joseph",
      "Shauna Kravec",
      "Karina Nguyen",
      "Jared Kaplan",
      "Deep Ganguli"
    ]
  },
  "http://arxiv.org/abs/2312.03815": {
    "title": "LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem",
    "volume": "Dec",
    "abstract": "This paper envisions a revolutionary AIOS-Agent ecosystem, where Large Language Model (LLM) serves as the (Artificial) Intelligent Operating System (IOS, or AIOS)--an operating system ``with soul''. Upon this foundation, a diverse range of LLM-based AI Agent Applications (Agents, or AAPs) are developed, enriching the AIOS-Agent ecosystem and signaling a paradigm shift from the traditional OS-APP ecosystem. We envision that LLM's impact will not be limited to the AI application level, instead, it will in turn revolutionize the design and implementation of computer system, architecture, software, and programming language, featured by several main concepts: LLM as OS (system-level), Agents as Applications (application-level), Natural Language as Programming Interface (user-level), and Tools as Devices/Libraries (hardware/middleware-level)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingqiang Ge",
      "Yujie Ren",
      "Wenyue Hua",
      "Shuyuan Xu",
      "Juntao Tan",
      "Yongfeng Zhang"
    ]
  },
  "http://arxiv.org/abs/2312.03668": {
    "title": "An Integration of Pre-Trained Speech and Language Models for End-to-End Speech Recognition",
    "volume": "Dec",
    "abstract": "Advances in machine learning have made it possible to perform various text and speech processing tasks, including automatic speech recognition (ASR), in an end-to-end (E2E) manner. Since typical E2E approaches require large amounts of training data and resources, leveraging pre-trained foundation models instead of training from scratch is gaining attention. Although there have been attempts to use pre-trained speech and language models in ASR, most of them are limited to using either. This paper explores the potential of integrating a pre-trained speech representation model with a large language model (LLM) for E2E ASR. The proposed model enables E2E ASR by generating text tokens in an autoregressive manner via speech representations as speech prompts, taking advantage of the vast knowledge provided by the LLM. Furthermore, the proposed model can incorporate remarkable developments for LLM utilization, such as inference optimization and parameter-efficient domain adaptation. Experimental results show that the proposed model achieves performance comparable to modern E2E ASR models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yukiya Hono",
      "Koh Mitsuda",
      "Tianyu Zhao",
      "Kentaro Mitsui",
      "Toshiaki Wakatsuki",
      "Kei Sawada"
    ]
  },
  "http://arxiv.org/abs/2312.03664": {
    "title": "Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia",
    "volume": "Dec",
    "abstract": "Agent-based modeling has been around for decades, and applied widely across the social and natural sciences. The scope of this research method is now poised to grow dramatically as it absorbs the new affordances provided by Large Language Models (LLM)s. Generative Agent-Based Models (GABM) are not just classic Agent-Based Models (ABM)s where the agents talk to one another. Rather, GABMs are constructed using an LLM to apply common sense to situations, act \"reasonably\", recall common semantic knowledge, produce API calls to control digital technologies like apps, and communicate both within the simulation and to researchers viewing it from the outside. Here we present Concordia, a library to facilitate constructing and working with GABMs. Concordia makes it easy to construct language-mediated simulations of physically- or digitally-grounded environments. Concordia agents produce their behavior using a flexible component system which mediates between two fundamental operations: LLM calls and associative memory retrieval. A special agent called the Game Master (GM), which was inspired by tabletop role-playing games, is responsible for simulating the environment where the agents interact. Agents take actions by describing what they want to do in natural language. The GM then translates their actions into appropriate implementations. In a simulated physical world, the GM checks the physical plausibility of agent actions and describes their effects. In digital environments simulating technologies such as apps and services, the GM may handle API calls to integrate with external tools such as general AI assistants (e.g., Bard, ChatGPT), and digital apps (e.g., Calendar, Email, Search, etc.). Concordia was designed to support a wide array of applications both in scientific research and for evaluating performance of real digital services by simulating users and/or generating synthetic data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Sasha Vezhnevets",
      "John P. Agapiou",
      "Avia Aharon",
      "Ron Ziv",
      "Jayd Matyas",
      "Edgar A. Duéñez-Guzmán",
      "William A. Cunningham",
      "Simon Osindero",
      "Danny Karmon",
      "Joel Z. Leibo"
    ]
  },
  "http://arxiv.org/abs/2312.03813": {
    "title": "Improving Activation Steering in Language Models with Mean-Centring",
    "volume": "Dec",
    "abstract": "Recent work in activation steering has demonstrated the potential to better control the outputs of Large Language Models (LLMs), but it involves finding steering vectors. This is difficult because engineers do not typically know how features are represented in these models. We seek to address this issue by applying the idea of mean-centring to steering vectors. We find that taking the average of activations associated with a target dataset, and then subtracting the mean of all training activations, results in effective steering vectors. We test this method on a variety of models on natural language tasks by steering away from generating toxic text, and steering the completion of a story towards a target genre. We also apply mean-centring to extract function vectors, more effectively triggering the execution of a range of natural language tasks by a significant margin (compared to previous baselines). This suggests that mean-centring can be used to easily improve the effectiveness of activation steering in a wide range of contexts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ole Jorgensen",
      "Dylan Cope",
      "Nandi Schoots",
      "Murray Shanahan"
    ]
  },
  "http://arxiv.org/abs/2312.03656": {
    "title": "Interpretability Illusions in the Generalization of Simplified Models",
    "volume": "Dec",
    "abstract": "A common method to study deep learning systems is to use simplified model representations -- for example, using singular value decomposition to visualize the model's hidden states in a lower dimensional space. This approach assumes that the results of these simplified are faithful to the original model. Here, we illustrate an important caveat to this assumption: even if the simplified representations can accurately approximate the full model on the training set, they may fail to accurately capture the model's behavior out of distribution -- the understanding developed from simplified representations may be an illusion. We illustrate this by training Transformer models on controlled datasets with systematic generalization splits. First, we train models on the Dyck balanced-parenthesis languages. We simplify these models using tools like dimensionality reduction and clustering, and then explicitly test how these simplified proxies match the behavior of the original model on various out-of-distribution test sets. We find that the simplified proxies are generally less faithful out of distribution. In cases where the original model generalizes to novel structures or deeper depths, the simplified versions may fail, or generalize better. This finding holds even if the simplified representations do not directly depend on the training distribution. Next, we study a more naturalistic task: predicting the next character in a dataset of computer code. We find similar generalization gaps between the original model and simplified proxies, and conduct further analysis to investigate which aspects of the code completion task are associated with the largest gaps. Together, our results raise questions about the extent to which mechanistic interpretations derived using tools like SVD can reliably predict what a model will do in novel situations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Friedman",
      "Andrew Lampinen",
      "Lucas Dixon",
      "Danqi Chen",
      "Asma Ghandeharioun"
    ]
  },
  "http://arxiv.org/abs/2312.03633": {
    "title": "Not All Large Language Models (LLMs) Succumb to the \"Reversal Curse\": A Comparative Study of Deductive Logical Reasoning in BERT and GPT Models",
    "volume": "Dec",
    "abstract": "The \"Reversal Curse\" refers to the scenario where auto-regressive decoder large language models (LLMs), such as ChatGPT, trained on \"A is B\" fail to learn \"B is A\", demonstrating a basic failure of logical deduction. This raises a red flag in the use of GPT models for certain general tasks such as constructing knowledge graphs, considering their adherence to this symmetric principle. In our study, we examined a bidirectional LLM, BERT, and found that it is immune to the reversal curse. Driven by ongoing efforts to construct biomedical knowledge graphs with LLMs, we also embarked on evaluating more complex but essential deductive reasoning capabilities. This process included first training encoder and decoder language models to master the intersection ($\\cap$) and union ($\\cup$) operations on two sets and then moving on to assess their capability to infer different combinations of union ($\\cup$) and intersection ($\\cap$) operations on three newly created sets. The findings showed that while both encoder and decoder language models, trained for tasks involving two sets (union/intersection), were proficient in such scenarios, they encountered difficulties when dealing with operations that included three sets (various combinations of union and intersection). Our research highlights the distinct characteristics of encoder and decoder models in simple and complex logical reasoning. In practice, the choice between BERT and GPT should be guided by the specific requirements and nature of the task at hand, leveraging their respective strengths in bidirectional context comprehension and sequence prediction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingye Yang",
      "Da Wu",
      "Kai Wang"
    ]
  },
  "http://arxiv.org/abs/2312.03577": {
    "title": "Improving Bias Mitigation through Bias Experts in Natural Language Understanding",
    "volume": "Dec",
    "abstract": "Biases in the dataset often enable the model to achieve high performance on in-distribution data, while poorly performing on out-of-distribution data. To mitigate the detrimental effect of the bias on the networks, previous works have proposed debiasing methods that down-weight the biased examples identified by an auxiliary model, which is trained with explicit bias labels. However, finding a type of bias in datasets is a costly process. Therefore, recent studies have attempted to make the auxiliary model biased without the guidance (or annotation) of bias labels, by constraining the model's training environment or the capability of the model itself. Despite the promising debiasing results of recent works, the multi-class learning objective, which has been naively used to train the auxiliary model, may harm the bias mitigation effect due to its regularization effect and competitive nature across classes. As an alternative, we propose a new debiasing framework that introduces binary classifiers between the auxiliary model and the main model, coined bias experts. Specifically, each bias expert is trained on a binary classification task derived from the multi-class classification task via the One-vs-Rest approach. Experimental results demonstrate that our proposed strategy improves the bias identification ability of the auxiliary model. Consequently, our debiased model consistently outperforms the state-of-the-art on various challenge datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eojin Jeon",
      "Mingyu Lee",
      "Juhyeong Park",
      "Yeachan Kim",
      "Wing-Lam Mok",
      "SangKeun Lee"
    ]
  },
  "http://arxiv.org/abs/2312.03567": {
    "title": "XAIQA: Explainer-Based Data Augmentation for Extractive Question Answering",
    "volume": "Dec",
    "abstract": "Extractive question answering (QA) systems can enable physicians and researchers to query medical records, a foundational capability for designing clinical studies and understanding patient medical history. However, building these systems typically requires expert-annotated QA pairs. Large language models (LLMs), which can perform extractive QA, depend on high quality data in their prompts, specialized for the application domain. We introduce a novel approach, XAIQA, for generating synthetic QA pairs at scale from data naturally available in electronic health records. Our method uses the idea of a classification model explainer to generate questions and answers about medical concepts corresponding to medical codes. In an expert evaluation with two physicians, our method identifies $2.2\\times$ more semantic matches and $3.8\\times$ more clinical abbreviations than two popular approaches that use sentence transformers to create QA pairs. In an ML evaluation, adding our QA pairs improves performance of GPT-4 as an extractive QA model, including on difficult questions. In both the expert and ML evaluations, we examine trade-offs between our method and sentence transformers for QA pair generation depending on question difficulty",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joel Stremmel",
      "Ardavan Saeedi",
      "Hamid Hassanzadeh",
      "Sanjit Batra",
      "Jeffrey Hertzberg",
      "Jaime Murillo",
      "Eran Halperin"
    ]
  },
  "http://arxiv.org/abs/2312.03549": {
    "title": "Holmes: Towards Distributed Training Across Clusters with Heterogeneous NIC Environment",
    "volume": "Dec",
    "abstract": "Large language models (LLMs) such as GPT-3, OPT, and LLaMA have demonstrated remarkable accuracy in a wide range of tasks. However, training these models can incur significant expenses, often requiring tens of thousands of GPUs for months of continuous operation. Typically, this training is carried out in specialized GPU clusters equipped with homogeneous high-speed Remote Direct Memory Access (RDMA) network interface cards (NICs). The acquisition and maintenance of such dedicated clusters is challenging. Current LLM training frameworks, like Megatron-LM and Megatron-DeepSpeed, focus primarily on optimizing training within homogeneous cluster settings. In this paper, we introduce Holmes, a training framework for LLMs that employs thoughtfully crafted data and model parallelism strategies over the heterogeneous NIC environment. Our primary technical contribution lies in a novel scheduling method that intelligently allocates distinct computational tasklets in LLM training to specific groups of GPU devices based on the characteristics of their connected NICs. Furthermore, our proposed framework, utilizing pipeline parallel techniques, demonstrates scalability to multiple GPU clusters, even in scenarios without high-speed interconnects between nodes in distinct clusters. We conducted comprehensive experiments that involved various scenarios in the heterogeneous NIC environment. In most cases, our framework achieves performance levels close to those achievable with homogeneous RDMA-capable networks (InfiniBand or RoCE), significantly exceeding training efficiency within the pure Ethernet environment. Additionally, we verified that our framework outperforms other mainstream LLM frameworks under heterogeneous NIC environment in terms of training efficiency and can be seamlessly integrated with them",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Yang",
      "Shuang Peng",
      "Ning Sun",
      "Fangyu Wang",
      "Ke Tan",
      "Fu Wu",
      "Jiezhong Qiu",
      "Aimin Pan"
    ]
  },
  "http://arxiv.org/abs/2312.03523": {
    "title": "Sig-Networks Toolkit: Signature Networks for Longitudinal Language Modelling",
    "volume": "Dec",
    "abstract": "We present an open-source, pip installable toolkit, Sig-Networks, the first of its kind for longitudinal language modelling. A central focus is the incorporation of Signature-based Neural Network models, which have recently shown success in temporal tasks. We apply and extend published research providing a full suite of signature-based models. Their components can be used as PyTorch building blocks in future architectures. Sig-Networks enables task-agnostic dataset plug-in, seamless pre-processing for sequential data, parameter flexibility, automated tuning across a range of models. We examine signature networks under three different NLP tasks of varying temporal granularity: counselling conversations, rumour stance switch and mood changes in social media threads, showing SOTA performance in all three, and provide guidance for future tasks. We release the Toolkit as a PyTorch package with an introductory video, Git repositories for preprocessing and modelling including sample notebooks on the modeled NLP tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Talia Tseriotou",
      "Ryan Sze-Yin Chan",
      "Adam Tsakalidis",
      "Iman Munire Bilal",
      "Elena Kochkina",
      "Terry Lyons",
      "Maria Liakata"
    ]
  },
  "http://arxiv.org/abs/2312.03483": {
    "title": "Exploring Answer Information Methods for Question Generation with Transformers",
    "volume": "Dec",
    "abstract": "There has been a lot of work in question generation where different methods to provide target answers as input, have been employed. This experimentation has been mostly carried out for RNN based models. We use three different methods and their combinations for incorporating answer information and explore their effect on several automatic evaluation metrics. The methods that are used are answer prompting, using a custom product method using answer embeddings and encoder outputs, choosing sentences from the input paragraph that have answer related information, and using a separate cross-attention attention block in the decoder which attends to the answer. We observe that answer prompting without any additional modes obtains the best scores across rouge, meteor scores. Additionally, we use a custom metric to calculate how many of the generated questions have the same answer, as the answer which is used to generate them",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Talha Chafekar",
      "Aafiya Hussain",
      "Grishma Sharma",
      "Deepak Sharma"
    ]
  },
  "http://arxiv.org/abs/2312.03480": {
    "title": "AMR Parsing is Far from Solved: GrAPES, the Granular AMR Parsing Evaluation Suite",
    "volume": "Dec",
    "abstract": "We present the Granular AMR Parsing Evaluation Suite (GrAPES), a challenge set for Abstract Meaning Representation (AMR) parsing with accompanying evaluation metrics. AMR parsers now obtain high scores on the standard AMR evaluation metric Smatch, close to or even above reported inter-annotator agreement. But that does not mean that AMR parsing is solved; in fact, human evaluation in previous work indicates that current parsers still quite frequently make errors on node labels or graph structure that substantially distort sentence meaning. Here, we provide an evaluation suite that tests AMR parsers on a range of phenomena of practical, technical, and linguistic interest. Our 36 categories range from seen and unseen labels, to structural generalization, to coreference. GrAPES reveals in depth the abilities and shortcomings of current AMR parsers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonas Groschwitz",
      "Shay B. Cohen",
      "Lucia Donatelli",
      "Meaghan Fowlie"
    ]
  },
  "http://arxiv.org/abs/2312.03463": {
    "title": "DBCopilot: Scaling Natural Language Querying to Massive Databases",
    "volume": "Dec",
    "abstract": "Text-to-SQL simplifies database interactions by enabling non-experts to convert their natural language (NL) questions into Structured Query Language (SQL) queries. While recent advances in large language models (LLMs) have improved the zero-shot text-to-SQL paradigm, existing methods face scalability challenges when dealing with massive, dynamically changing databases. This paper introduces DBCopilot, a framework that addresses these challenges by employing a compact and flexible copilot model for routing across massive databases. Specifically, DBCopilot decouples the text-to-SQL process into schema routing and SQL generation, leveraging a lightweight sequence-to-sequence neural network-based router to formulate database connections and navigate natural language questions through databases and tables. The routed schemas and questions are then fed into LLMs for efficient SQL generation. Furthermore, DBCopilot also introduced a reverse schema-to-question generation paradigm, which can learn and adapt the router over massive databases automatically without requiring manual intervention. Experimental results demonstrate that DBCopilot is a scalable and effective solution for real-world text-to-SQL tasks, providing a significant advancement in handling large-scale schemas",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianshu Wang",
      "Hongyu Lin",
      "Xianpei Han",
      "Le Sun",
      "Xiaoyang Chen",
      "Hao Wang",
      "Zhenyu Zeng"
    ]
  },
  "http://arxiv.org/abs/2312.03458": {
    "title": "Think from Words(TFW): Initiating Human-Like Cognition in Large Language Models Through Think from Words for Japanese Text-level Classification",
    "volume": "Dec",
    "abstract": "The proliferation of Large Language Models (LLMs) has spurred extensive research into LLM-related Prompt investigations, such as Instruction Learning (IL), In-context Learning (ICL), and Chain-of-Thought (CoT). These approaches aim to improve LLMs' responses by enabling them to provide concise statements or examples for deeper contemplation when addressing questions. However, independent thinking by LLMs can introduce variability in their thought processes, leading to potential inaccuracies. In response, our study seeks to bridge the gap between LLM and human-like thinking processes, recognizing that text comprehension begins with understanding individual words. To tackle this challenge, we have expanded the CoT method to cater to a specific domain. Our approach, known as \"Think from Words\" (TFW), initiates the comprehension process at the word level and then extends it to encompass the entire text. We also propose \"TFW with Extra word-level information\" (TFW Extra), augmenting comprehension with additional word-level data. To assess our methods, we employ text classification on six Japanese datasets comprising text-level and word-level elements. Our findings not only validate the effectiveness of TFW but also shed light on the impact of various word-level information types on LLMs' text comprehension, offering insights into their potential to cause misinterpretations and errors in the overall comprehension of the final text",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengguang Gan",
      "Qinghao Zhang",
      "Tatsunori Mori"
    ]
  },
  "http://arxiv.org/abs/2312.03789": {
    "title": "Comparative Analysis of Multilingual Text Classification & Identification through Deep Learning and Embedding Visualization",
    "volume": "Dec",
    "abstract": "This research conducts a comparative study on multilingual text classification methods, utilizing deep learning and embedding visualization. The study employs LangDetect, LangId, FastText, and Sentence Transformer on a dataset encompassing 17 languages. It explores dimensionality's impact on clustering, revealing FastText's clearer clustering in 2D visualization due to its extensive multilingual corpus training. Notably, the FastText multi-layer perceptron model achieved remarkable accuracy, precision, recall, and F1 score, outperforming the Sentence Transformer model. The study underscores the effectiveness of these techniques in multilingual text classification, emphasizing the importance of large multilingual corpora for training embeddings. It lays the groundwork for future research and assists practitioners in developing language detection and classification systems. Additionally, it includes the comparison of multi-layer perceptron, LSTM, and Convolution models for classification",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arinjay Wyawhare"
    ]
  },
  "http://arxiv.org/abs/2312.03788": {
    "title": "SmoothQuant+: Accurate and Efficient 4-bit Post-Training WeightQuantization for LLM",
    "volume": "Dec",
    "abstract": "Large language models (LLMs) have shown remarkable capabilities in various tasks. However their huge model size and the consequent demand for computational and memory resources also pose challenges to model deployment. Currently, 4-bit post-training quantization (PTQ) has achieved some success in LLMs, reducing the memory footprint by approximately 75% compared to FP16 models, albeit with some accuracy loss. In this paper, we propose SmoothQuant+, an accurate and efficient 4-bit weight-only PTQ that requires no additional training, which enables lossless in accuracy for LLMs for the first time. Based on the fact that the loss of weight quantization is amplified by the activation outliers, SmoothQuant+ smoothes the activation outliers by channel before quantization, while adjusting the corresponding weights for mathematical equivalence, and then performs group-wise 4-bit weight quantization for linear layers. We have integrated SmoothQuant+ into the vLLM framework, an advanced high-throughput inference engine specially developed for LLMs, and equipped it with an efficient W4A16 CUDA kernels, so that vLLM can seamlessly support SmoothQuant+ 4-bit weight quantization. Our results show that, with SmoothQuant+, the Code Llama-34B model can be quantized and deployed on a A100 40GB GPU, achieving lossless accuracy and a throughput increase of 1.9 to 4.0 times compared to the FP16 model deployed on two A100 40GB GPUs. Moreover, the latency per token is only 68% of the FP16 model deployed on two A100 40GB GPUs. This is the state-of-the-art 4-bit weight quantization for LLMs as we know",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Pan",
      "Chengcan Wang",
      "Kaifu Zheng",
      "Yangguang Li",
      "Zhenyu Wang",
      "Bin Feng"
    ]
  },
  "http://arxiv.org/abs/2312.03414": {
    "title": "Compressed Context Memory For Online Language Model Interaction",
    "volume": "Dec",
    "abstract": "This paper presents a novel context compression method for Transformer language models in online scenarios such as ChatGPT, where the context continually expands. As the context lengthens, the attention process requires more memory and computational resources, which in turn reduces the throughput of the language model. To this end, we propose a compressed context memory system that continually compresses the growing context into a compact memory space. The compression process simply involves integrating a lightweight conditional LoRA into the language model's forward pass during inference. Based on the compressed context memory, the language model can perform inference with reduced memory and attention operations. Through evaluations on conversation, personalization, and multi-task learning, we demonstrate that our approach achieves the performance level of a full context model with $5\\times$ smaller context memory space. Codes are available at https://github.com/snu-mllab/context-memory",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jang-Hyun Kim",
      "Junyoung Yeom",
      "Sangdoo Yun",
      "Hyun Oh Song"
    ]
  },
  "http://arxiv.org/abs/2312.03379": {
    "title": "A Text-to-Text Model for Multilingual Offensive Language Identification",
    "volume": "Dec",
    "abstract": "The ubiquity of offensive content on social media is a growing cause for concern among companies and government organizations. Recently, transformer-based models such as BERT, XLNET, and XLM-R have achieved state-of-the-art performance in detecting various forms of offensive content (e.g. hate speech, cyberbullying, and cyberaggression). However, the majority of these models are limited in their capabilities due to their encoder-only architecture, which restricts the number and types of labels in downstream tasks. Addressing these limitations, this study presents the first pre-trained model with encoder-decoder architecture for offensive language identification with text-to-text transformers (T5) trained on two large offensive language identification datasets; SOLID and CCTK. We investigate the effectiveness of combining two datasets and selecting an optimal threshold in semi-supervised instances in SOLID in the T5 retraining step. Our pre-trained T5 model outperforms other transformer-based models fine-tuned for offensive language detection, such as fBERT and HateBERT, in multiple English benchmarks. Following a similar approach, we also train the first multilingual pre-trained model for offensive language identification using mT5 and evaluate its performance on a set of six different languages (German, Hindi, Korean, Marathi, Sinhala, and Spanish). The results demonstrate that this multilingual model achieves a new state-of-the-art on all the above datasets, showing its usefulness in multilingual scenarios. Our proposed T5-based models will be made freely available to the community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tharindu Ranasinghe",
      "Marcos Zampieri"
    ]
  },
  "http://arxiv.org/abs/2312.03367": {
    "title": "Lazy-k: Decoding for Constrained Token Classification",
    "volume": "Dec",
    "abstract": "We explore the possibility of improving probabilistic models in structured prediction. Specifically, we combine the models with constrained decoding approaches in the context of token classification for information extraction. The decoding methods search for constraint-satisfying label-assignments while maximizing the total probability. To do this, we evaluate several existing approaches, as well as propose a novel decoding method called Lazy-$k$. Our findings demonstrate that constrained decoding approaches can significantly improve the models' performances, especially when using smaller models. The Lazy-$k$ approach allows for more flexibility between decoding time and accuracy. The code for using Lazy-$k$ decoding can be found here: https://github.com/ArthurDevNL/lazyk",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arthur Hemmer",
      "Mickaël Coustaty",
      "Nicola Bartolo",
      "Jérôme Brachat",
      "Jean-Marc Ogier"
    ]
  },
  "http://arxiv.org/abs/2312.03361": {
    "title": "KhabarChin: Automatic Detection of Important News in the Persian Language",
    "volume": "Dec",
    "abstract": "Being aware of important news is crucial for staying informed and making well-informed decisions efficiently. Natural Language Processing (NLP) approaches can significantly automate this process. This paper introduces the detection of important news, in a previously unexplored area, and presents a new benchmarking dataset (Khabarchin) for detecting important news in the Persian language. We define important news articles as those deemed significant for a considerable portion of society, capable of influencing their mindset or decision-making. The news articles are obtained from seven different prominent Persian news agencies, resulting in the annotation of 7,869 samples and the creation of the dataset. Two challenges of high disagreement and imbalance between classes were faced, and solutions were provided for them. We also propose several learning-based models, ranging from conventional machine learning to state-of-the-art transformer models, to tackle this task. Furthermore, we introduce the second task of important sentence detection in news articles, as they often come with a significant contextual length that makes it challenging for readers to identify important information. We identify these sentences in a weakly supervised manner",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hamed Hematian Hemati",
      "Arash Lagzian",
      "Moein Salimi Sartakhti",
      "Hamid Beigy",
      "Ehsaneddin Asgari"
    ]
  },
  "http://arxiv.org/abs/2312.03360": {
    "title": "Teaching Specific Scientific Knowledge into Large Language Models through Additional Training",
    "volume": "Dec",
    "abstract": "Through additional training, we explore embedding specialized scientific knowledge into the Llama 2 Large Language Model (LLM). Key findings reveal that effective knowledge integration requires reading texts from multiple perspectives, especially in instructional formats. We utilize text augmentation to tackle the scarcity of specialized texts, including style conversions and translations. Hyperparameter optimization proves crucial, with different size models (7b, 13b, and 70b) reasonably undergoing additional training. Validating our methods, we construct a dataset of 65,000 scientific papers. Although we have succeeded in partially embedding knowledge, the study highlights the complexities and limitations of incorporating specialized information into LLMs, suggesting areas for further improvement",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kan Hatakeyama-Sato",
      "Yasuhiko Igarashi",
      "Shun Katakami",
      "Yuta Nabae",
      "Teruaki Hayakawa"
    ]
  },
  "http://arxiv.org/abs/2312.03342": {
    "title": "Topic and genre in dialogue",
    "volume": "Dec",
    "abstract": "In this paper we argue that topic plays a fundamental role in conversations, and that the concept is needed in addition to that of genre to define interactions. In particular, the concepts of genre and topic need to be separated and orthogonally defined. This would enable modular, reliable and controllable flexible-domain dialogue systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amandine Decker",
      "Ellen Breitholtz",
      "Christine Howes",
      "Staffan Larsson"
    ]
  },
  "http://arxiv.org/abs/2312.03330": {
    "title": "Measuring Misogyny in Natural Language Generation: Preliminary Results from a Case Study on two Reddit Communities",
    "volume": "Dec",
    "abstract": "Generic `toxicity' classifiers continue to be used for evaluating the potential for harm in natural language generation, despite mounting evidence of their shortcomings. We consider the challenge of measuring misogyny in natural language generation, and argue that generic `toxicity' classifiers are inadequate for this task. We use data from two well-characterised `Incel' communities on Reddit that differ primarily in their degrees of misogyny to construct a pair of training corpora which we use to fine-tune two language models. We show that an open source `toxicity' classifier is unable to distinguish meaningfully between generations from these models. We contrast this with a misogyny-specific lexicon recently proposed by feminist subject-matter experts, demonstrating that, despite the limitations of simple lexicon-based approaches, this shows promise as a benchmark to evaluate language models for misogyny, and that it is sensitive enough to reveal the known differences in these Reddit communities. Our preliminary findings highlight the limitations of a generic approach to evaluating harms, and further emphasise the need for careful benchmark design and selection in natural language evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aaron J. Snoswell",
      "Lucinda Nelson",
      "Hao Xue",
      "Flora D. Salim",
      "Nicolas Suzor",
      "Jean Burgess"
    ]
  },
  "http://arxiv.org/abs/2312.03312": {
    "title": "Optimizing Two-Pass Cross-Lingual Transfer Learning: Phoneme Recognition and Phoneme to Grapheme Translation",
    "volume": "Dec",
    "abstract": "This research optimizes two-pass cross-lingual transfer learning in low-resource languages by enhancing phoneme recognition and phoneme-to-grapheme translation models. Our approach optimizes these two stages to improve speech recognition across languages. We optimize phoneme vocabulary coverage by merging phonemes based on shared articulatory characteristics, thus improving recognition accuracy. Additionally, we introduce a global phoneme noise generator for realistic ASR noise during phoneme-to-grapheme training to reduce error propagation. Experiments on the CommonVoice 12.0 dataset show significant reductions in Word Error Rate (WER) for low-resource languages, highlighting the effectiveness of our approach. This research contributes to the advancements of two-pass ASR systems in low-resource languages, offering the potential for improved cross-lingual transfer learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonjun Lee",
      "Gary Geunbae Lee",
      "Yunsu Kim"
    ]
  },
  "http://arxiv.org/abs/2312.03303": {
    "title": "Dyport: Dynamic Importance-based Hypothesis Generation Benchmarking Technique",
    "volume": "Dec",
    "abstract": "This paper presents a novel benchmarking framework Dyport for evaluating biomedical hypothesis generation systems. Utilizing curated datasets, our approach tests these systems under realistic conditions, enhancing the relevance of our evaluations. We integrate knowledge from the curated databases into a dynamic graph, accompanied by a method to quantify discovery importance. This not only assesses hypothesis accuracy but also their potential impact in biomedical research which significantly extends traditional link prediction benchmarks. Applicability of our benchmarking process is demonstrated on several link prediction systems applied on biomedical semantic knowledge graphs. Being flexible, our benchmarking system is designed for broad application in hypothesis generation quality verification, aiming to expand the scope of scientific discovery within the biomedical research community. Availability and implementation: Dyport framework is fully open-source. All code and datasets are available at: https://github.com/IlyaTyagin/Dyport",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilya Tyagin",
      "Ilya Safro"
    ]
  },
  "http://arxiv.org/abs/2312.03290": {
    "title": "Can language agents be alternatives to PPO? A Preliminary Empirical Study On OpenAI Gym",
    "volume": "Dec",
    "abstract": "The formidable capacity for zero- or few-shot decision-making in language agents encourages us to pose a compelling question: Can language agents be alternatives to PPO agents in traditional sequential decision-making tasks? To investigate this, we first take environments collected in OpenAI Gym as our testbeds and ground them to textual environments that construct the TextGym simulator. This allows for straightforward and efficient comparisons between PPO agents and language agents, given the widespread adoption of OpenAI Gym. To ensure a fair and effective benchmarking, we introduce $5$ levels of scenario for accurate domain-knowledge controlling and a unified RL-inspired framework for language agents. Additionally, we propose an innovative explore-exploit-guided language (EXE) agent to solve tasks within TextGym. Through numerical experiments and ablation studies, we extract valuable insights into the decision-making capabilities of language agents and make a preliminary evaluation of their potential to be alternatives to PPO in classical sequential decision-making problems. This paper sheds light on the performance of language agents and paves the way for future research in this exciting domain. Our code is publicly available at~\\url{https://github.com/mail-ecnu/Text-Gym-Agents}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Sheng",
      "Zixiao Huang",
      "Chuyun Shen",
      "Wenhao Li",
      "Yun Hua",
      "Bo Jin",
      "Hongyuan Zha",
      "Xiangfeng Wang"
    ]
  },
  "http://arxiv.org/abs/2312.06668": {
    "title": "Evaluating Self-supervised Speech Models on a Taiwanese Hokkien Corpus",
    "volume": "Dec",
    "abstract": "Taiwanese Hokkien is declining in use and status due to a language shift towards Mandarin in Taiwan. This is partly why it is a low resource language in NLP and speech research today. To ensure that the state of the art in speech processing does not leave Taiwanese Hokkien behind, we contribute a 1.5-hour dataset of Taiwanese Hokkien to ML-SUPERB's hidden set. Evaluating ML-SUPERB's suite of self-supervised learning (SSL) speech representations on our dataset, we find that model size does not consistently determine performance. In fact, certain smaller models outperform larger ones. Furthermore, linguistic alignment between pretraining data and the target language plays a crucial role",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Hui Chou",
      "Kalvin Chang",
      "Meng-Ju Wu",
      "Winston Ou",
      "Alice Wen-Hsin Bi",
      "Carol Yang",
      "Bryan Y. Chen",
      "Rong-Wei Pai",
      "Po-Yen Yeh",
      "Jo-Peng Chiang",
      "Iu-Tshian Phoann",
      "Winnie Chang",
      "Chenxuan Cui",
      "Noel Chen",
      "Jiatong Shi"
    ]
  },
  "http://arxiv.org/abs/2312.03217": {
    "title": "Rethinking E-Commerce Search",
    "volume": "Dec",
    "abstract": "E-commerce search and recommendation usually operate on structured data such as product catalogs and taxonomies. However, creating better search and recommendation systems often requires a large variety of unstructured data including customer reviews and articles on the web. Traditionally, the solution has always been converting unstructured data into structured data through information extraction, and conducting search over the structured data. However, this is a costly approach that often has low quality. In this paper, we envision a solution that does entirely the opposite. Instead of converting unstructured data (web pages, customer reviews, etc) to structured data, we instead convert structured data (product inventory, catalogs, taxonomies, etc) into textual data, which can be easily integrated into the text corpus that trains LLMs. Then, search and recommendation can be performed through a Q/A mechanism through an LLM instead of using traditional information retrieval methods over structured data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haixun Wang",
      "Taesik Na"
    ]
  },
  "http://arxiv.org/abs/2312.03195": {
    "title": "Detecting Rumor Veracity with Only Textual Information by Double-Channel Structure",
    "volume": "Dec",
    "abstract": "Kyle (1985) proposes two types of rumors: informed rumors which are based on some private information and uninformed rumors which are not based on any information (i.e. bluffing). Also, prior studies find that when people have credible source of information, they are likely to use a more confident textual tone in their spreading of rumors. Motivated by these theoretical findings, we propose a double-channel structure to determine the ex-ante veracity of rumors on social media. Our ultimate goal is to classify each rumor into true, false, or unverifiable category. We first assign each text into either certain (informed rumor) or uncertain (uninformed rumor) category. Then, we apply lie detection algorithm to informed rumors and thread-reply agreement detection algorithm to uninformed rumors. Using the dataset of SemEval 2019 Task 7, which requires ex-ante threefold classification (true, false, or unverifiable) of social media rumors, our model yields a macro-F1 score of 0.4027, outperforming all the baseline models and the second-place winner (Gorrell et al., 2019). Furthermore, we empirically validate that the double-channel structure outperforms single-channel structures which use either lie detection or agreement detection algorithm to all posts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Kim",
      "Sangwon Yoon"
    ]
  },
  "http://arxiv.org/abs/2312.03194": {
    "title": "Corporate Bankruptcy Prediction with Domain-Adapted BERT",
    "volume": "Dec",
    "abstract": "This study performs BERT-based analysis, which is a representative contextualized language model, on corporate disclosure data to predict impending bankruptcies. Prior literature on bankruptcy prediction mainly focuses on developing more sophisticated prediction methodologies with financial variables. However, in our study, we focus on improving the quality of input dataset. Specifically, we employ BERT model to perform sentiment analysis on MD&A disclosures. We show that BERT outperforms dictionary-based predictions and Word2Vec-based predictions in terms of adjusted R-square in logistic regression, k-nearest neighbor (kNN-5), and linear kernel support vector machine (SVM). Further, instead of pre-training the BERT model from scratch, we apply self-learning with confidence-based filtering to corporate disclosure data (10-K). We achieve the accuracy rate of 91.56% and demonstrate that the domain adaptation procedure brings a significant improvement in prediction accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Kim",
      "Sangwon Yoon"
    ]
  },
  "http://arxiv.org/abs/2312.03173": {
    "title": "A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in Programming Education",
    "volume": "Dec",
    "abstract": "There is a constant need for educators to develop and maintain effective up-to-date assessments. While there is a growing body of research in computing education on utilizing large language models (LLMs) in generation and engagement with coding exercises, the use of LLMs for generating programming MCQs has not been extensively explored. We analyzed the capability of GPT-4 to produce multiple-choice questions (MCQs) aligned with specific learning objectives (LOs) from Python programming classes in higher education. Specifically, we developed an LLM-powered (GPT-4) system for generation of MCQs from high-level course context and module-level LOs. We evaluated 651 LLM-generated and 449 human-crafted MCQs aligned to 246 LOs from 6 Python courses. We found that GPT-4 was capable of producing MCQs with clear language, a single correct choice, and high-quality distractors. We also observed that the generated MCQs appeared to be well-aligned with the LOs. Our findings can be leveraged by educators wishing to take advantage of the state-of-the-art generative models to support MCQ authoring efforts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacob Doughty",
      "Zipiao Wan",
      "Anishka Bompelli",
      "Jubahed Qayum",
      "Taozhi Wang",
      "Juran Zhang",
      "Yujia Zheng",
      "Aidan Doyle",
      "Pragnya Sridhar",
      "Arav Agarwal",
      "Christopher Bogart",
      "Eric Keylor",
      "Can Kultur",
      "Jaromir Savelka",
      "Majd Sakr"
    ]
  },
  "http://arxiv.org/abs/2312.03171": {
    "title": "Combining Counting Processes and Classification Improves a Stopping Rule for Technology Assisted Review",
    "volume": "Dec",
    "abstract": "Technology Assisted Review (TAR) stopping rules aim to reduce the cost of manually assessing documents for relevance by minimising the number of documents that need to be examined to ensure a desired level of recall. This paper extends an effective stopping rule using information derived from a text classifier that can be trained without the need for any additional annotation. Experiments on multiple data sets (CLEF e-Health, TREC Total Recall, TREC Legal and RCV1) showed that the proposed approach consistently improves performance and outperforms several alternative methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reem Bin-Hezam",
      "Mark Stevenson"
    ]
  },
  "http://arxiv.org/abs/2312.03769": {
    "title": "GPT vs Human for Scientific Reviews: A Dual Source Review on Applications of ChatGPT in Science",
    "volume": "Dec",
    "abstract": "The new polymath Large Language Models (LLMs) can speed-up greatly scientific reviews, possibly using more unbiased quantitative metrics, facilitating cross-disciplinary connections, and identifying emerging trends and research gaps by analyzing large volumes of data. However, at the present time, they lack the required deep understanding of complex methodologies, they have difficulty in evaluating innovative claims, and they are unable to assess ethical issues and conflicts of interest. Herein, we consider 13 GPT-related papers across different scientific domains, reviewed by a human reviewer and SciSpace, a large language model, with the reviews evaluated by three distinct types of evaluators, namely GPT-3.5, a crowd panel, and GPT-4. We found that 50% of SciSpace's responses to objective questions align with those of a human reviewer, with GPT-4 (informed evaluator) often rating the human reviewer higher in accuracy, and SciSpace higher in structure, clarity, and completeness. In subjective questions, the uninformed evaluators (GPT-3.5 and crowd panel) showed varying preferences between SciSpace and human responses, with the crowd panel showing a preference for the human responses. However, GPT-4 rated them equally in accuracy and structure but favored SciSpace for completeness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxi Wu",
      "Alan John Varghese",
      "Vivek Oommen",
      "George Em Karniadakis"
    ]
  },
  "http://arxiv.org/abs/2312.03140": {
    "title": "FlexModel: A Framework for Interpretability of Distributed Large Language Models",
    "volume": "Dec",
    "abstract": "With the growth of large language models, now incorporating billions of parameters, the hardware prerequisites for their training and deployment have seen a corresponding increase. Although existing tools facilitate model parallelization and distributed training, deeper model interactions, crucial for interpretability and responsible AI techniques, still demand thorough knowledge of distributed computing. This often hinders contributions from researchers with machine learning expertise but limited distributed computing background. Addressing this challenge, we present FlexModel, a software package providing a streamlined interface for engaging with models distributed across multi-GPU and multi-node configurations. The library is compatible with existing model distribution libraries and encapsulates PyTorch models. It exposes user-registerable HookFunctions to facilitate straightforward interaction with distributed model internals, bridging the gap between distributed and single-device model paradigms. Primarily, FlexModel enhances accessibility by democratizing model interactions and promotes more inclusive research in the domain of large-scale neural networks. The package is found at https://github.com/VectorInstitute/flex_model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Choi",
      "Muhammad Adil Asif",
      "John Willes",
      "David Emerson"
    ]
  },
  "http://arxiv.org/abs/2312.03122": {
    "title": "Assertion Enhanced Few-Shot Learning: Instructive Technique for Large Language Models to Generate Educational Explanations",
    "volume": "Dec",
    "abstract": "Human educators possess an intrinsic ability to anticipate and seek educational explanations from students, which drives them to pose thought-provoking questions when students cannot articulate these explanations independently. We aim to imbue Intelligent Tutoring Systems with this ability using few-shot learning capability of Large Language Models. Our work proposes a novel prompting technique, Assertion Enhanced Few-Shot Learning, to facilitate the generation of accurate, detailed oriented educational explanations. Our central hypothesis is that, in educational domain, few-shot demonstrations are necessary but not a sufficient condition for quality explanation generation. We conducted a study involving 12 in-service teachers, comparing our approach to Traditional Few-Shot Learning. The results show that Assertion Enhanced Few-Shot Learning improves explanation accuracy by 15% and yields higher-quality explanations, as evaluated by teachers. We also conduct a qualitative ablation study to factor the impact of assertions to provide educator-friendly prompting guidelines for generating explanations in their domain of interest",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tasmia Shahriar",
      "Noboru Matsuda",
      "Kelly Ramos"
    ]
  },
  "http://arxiv.org/abs/2312.03766": {
    "title": "Mismatch Quest: Visual and Textual Feedback for Image-Text Misalignment",
    "volume": "Dec",
    "abstract": "While existing image-text alignment models reach high quality binary assessments, they fall short of pinpointing the exact source of misalignment. In this paper, we present a method to provide detailed textual and visual explanation of detected misalignments between text-image pairs. We leverage large language models and visual grounding models to automatically construct a training set that holds plausible misaligned captions for a given image and corresponding textual explanations and visual indicators. We also publish a new human curated test set comprising ground-truth textual and visual misalignment annotations. Empirical results show that fine-tuning vision language models on our training set enables them to articulate misalignments and visually indicate them within images, outperforming strong baselines both on the binary alignment classification and the explanation generation tasks. Our method code and human curated test set are available at: https://mismatch-quest.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian Gordon",
      "Yonatan Bitton",
      "Yonatan Shafir",
      "Roopal Garg",
      "Xi Chen",
      "Dani Lischinski",
      "Daniel Cohen-Or",
      "Idan Szpektor"
    ]
  },
  "http://arxiv.org/abs/2312.03095": {
    "title": "Understanding Environmental Posts: Sentiment and Emotion Analysis of Social Media Data",
    "volume": "Dec",
    "abstract": "Social media is now the predominant source of information due to the availability of immediate public response. As a result, social media data has become a valuable resource for comprehending public sentiments. Studies have shown that it can amplify ideas and influence public sentiments. This study analyzes the public perception of climate change and the environment over a decade from 2014 to 2023. Using the Pointwise Mutual Information (PMI) algorithm, we identify sentiment and explore prevailing emotions expressed within environmental tweets across various social media platforms, namely Twitter, Reddit, and YouTube. Accuracy on a human-annotated dataset was 0.65, higher than Vader score but lower than that of an expert rater (0.90). Our findings suggest that negative environmental tweets are far more common than positive or neutral ones. Climate change, air quality, emissions, plastic, and recycling are the most discussed topics on all social media platforms, highlighting its huge global concern. The most common emotions in environmental tweets are fear, trust, and anticipation, demonstrating public reactions wide and complex nature. By identifying patterns and trends in opinions related to the environment, we hope to provide insights that can help raise awareness regarding environmental issues, inform the development of interventions, and adapt further actions to meet environmental challenges",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniyar Amangeldi",
      "Aida Usmanova",
      "Pakizar Shamoi"
    ]
  },
  "http://arxiv.org/abs/2312.03093": {
    "title": "RESIN-EDITOR: A Schema-guided Hierarchical Event Graph Visualizer and Editor",
    "volume": "Dec",
    "abstract": "In this paper, we present RESIN-EDITOR, an interactive event graph visualizer and editor designed for analyzing complex events. Our RESIN-EDITOR system allows users to render and freely edit hierarchical event graphs extracted from multimedia and multi-document news clusters with guidance from human-curated event schemas. RESIN-EDITOR's unique features include hierarchical graph visualization, comprehensive source tracing, and interactive user editing, which is more powerful and versatile than existing Information Extraction (IE) visualization tools. In our evaluation of RESIN-EDITOR, we demonstrate ways in which our tool is effective in understanding complex events and enhancing system performance. The source code, a video demonstration, and a live website for RESIN-EDITOR have been made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khanh Duy Nguyen",
      "Zixuan Zhang",
      "Reece Suchocki",
      "Sha Li",
      "Martha Palmer",
      "Susan Brown",
      "Jiawei Han",
      "Heng Ji"
    ]
  },
  "http://arxiv.org/abs/2312.03088": {
    "title": "LLMs for Multi-Modal Knowledge Extraction and Analysis in Intelligence/Safety-Critical Applications",
    "volume": "Dec",
    "abstract": "Large Language Models have seen rapid progress in capability in recent years; this progress has been accelerating and their capabilities, measured by various benchmarks, are beginning to approach those of humans. There is a strong demand to use such models in a wide variety of applications but, due to unresolved vulnerabilities and limitations, great care needs to be used before applying them to intelligence and safety-critical applications. This paper reviews recent literature related to LLM assessment and vulnerabilities to synthesize the current research landscape and to help understand what advances are most critical to enable use of of these technologies in intelligence and safety-critical applications. The vulnerabilities are broken down into ten high-level categories and overlaid onto a high-level life cycle of an LLM. Some general categories of mitigations are reviewed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brett Israelsen",
      "Soumalya Sarkar"
    ]
  },
  "http://arxiv.org/abs/2312.03077": {
    "title": "Clinical Notes Reveal Physician Fatigue",
    "volume": "Dec",
    "abstract": "Physicians write notes about patients. In doing so, they reveal much about themselves. Using data from 129,228 emergency room visits, we train a model to identify notes written by fatigued physicians -- those who worked 5 or more of the prior 7 days. In a hold-out set, the model accurately identifies notes written by these high-workload physicians, and also flags notes written in other high-fatigue settings: on overnight shifts, and after high patient volumes. Model predictions also correlate with worse decision-making on at least one important metric: yield of testing for heart attack is 18% lower with each standard deviation increase in model-predicted fatigue. Finally, the model indicates that notes written about Black and Hispanic patients have 12% and 21% higher predicted fatigue than Whites -- larger than overnight vs. daytime differences. These results have an important implication for large language models (LLMs). Our model indicates that fatigued doctors write more predictable notes. Perhaps unsurprisingly, because word prediction is the core of how LLMs work, we find that LLM-written notes have 17% higher predicted fatigue than real physicians' notes. This indicates that LLMs may introduce distortions in generated text that are not yet fully understood",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao-Chun Hsu",
      "Ziad Obermeyer",
      "Chenhao Tan"
    ]
  },
  "http://arxiv.org/abs/2312.02974": {
    "title": "Describing Differences in Image Sets with Natural Language",
    "volume": "Dec",
    "abstract": "How do two sets of images differ? Discerning set-level differences is crucial for understanding model behaviors and analyzing datasets, yet manually sifting through thousands of images is impractical. To aid in this discovery process, we explore the task of automatically describing the differences between two $\\textbf{sets}$ of images, which we term Set Difference Captioning. This task takes in image sets $D_A$ and $D_B$, and outputs a description that is more often true on $D_A$ than $D_B$. We outline a two-stage approach that first proposes candidate difference descriptions from image sets and then re-ranks the candidates by checking how well they can differentiate the two sets. We introduce VisDiff, which first captions the images and prompts a language model to propose candidate descriptions, then re-ranks these descriptions using CLIP. To evaluate VisDiff, we collect VisDiffBench, a dataset with 187 paired image sets with ground truth difference descriptions. We apply VisDiff to various domains, such as comparing datasets (e.g., ImageNet vs. ImageNetV2), comparing classification models (e.g., zero-shot CLIP vs. supervised ResNet), summarizing model failure modes (supervised ResNet), characterizing differences between generative models (e.g., StableDiffusionV1 and V2), and discovering what makes images memorable. Using VisDiff, we are able to find interesting and previously unknown differences in datasets and models, demonstrating its utility in revealing nuanced insights",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lisa Dunlap",
      "Yuhui Zhang",
      "Xiaohan Wang",
      "Ruiqi Zhong",
      "Trevor Darrell",
      "Jacob Steinhardt",
      "Joseph E. Gonzalez",
      "Serena Yeung-Levy"
    ]
  },
  "http://arxiv.org/abs/2312.03052": {
    "title": "Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models",
    "volume": "Dec",
    "abstract": "Solving complex visual tasks such as \"Who invented the musical instrument on the right?\" involves a composition of skills: understanding space, recognizing instruments, and also retrieving prior knowledge. Recent work shows promise by decomposing such tasks using a large language model (LLM) into an executable program that invokes specialized vision models. However, generated programs are error-prone: they omit necessary steps, include spurious ones, and are unable to recover when the specialized models give incorrect outputs. Moreover, they require loading multiple models, incurring high latency and computation costs. We propose Visual Program Distillation (VPD), an instruction tuning framework that produces a vision-language model (VLM) capable of solving complex visual tasks with a single forward pass. VPD distills the reasoning ability of LLMs by using them to sample multiple candidate programs, which are then executed and verified to identify a correct one. It translates each correct program into a language description of the reasoning steps, which are then distilled into a VLM. Extensive experiments show that VPD improves the VLM's ability to count, understand spatial relations, and reason compositionally. Our VPD-trained PaLI-X outperforms all prior VLMs, achieving state-of-the-art performance across complex vision tasks, including MMBench, OK-VQA, A-OKVQA, TallyQA, POPE, and Hateful Memes. An evaluation with human annotators also confirms that VPD improves model response factuality and consistency. Finally, experiments on content moderation demonstrate that VPD is also helpful for adaptation to real-world applications with limited data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushi Hu",
      "Otilia Stretcu",
      "Chun-Ta Lu",
      "Krishnamurthy Viswanathan",
      "Kenji Hata",
      "Enming Luo",
      "Ranjay Krishna",
      "Ariel Fuxman"
    ]
  },
  "http://arxiv.org/abs/2312.02969": {
    "title": "Rank-without-GPT: Building GPT-Independent Listwise Rerankers on Open-Source Large Language Models",
    "volume": "Dec",
    "abstract": "Listwise rerankers based on large language models (LLM) are the zero-shot state-of-the-art. However, current works in this direction all depend on the GPT models, making it a single point of failure in scientific reproducibility. Moreover, it raises the concern that the current research findings only hold for GPT models but not LLM in general. In this work, we lift this pre-condition and build for the first time effective listwise rerankers without any form of dependency on GPT. Our passage retrieval experiments show that our best list se reranker surpasses the listwise rerankers based on GPT-3.5 by 13% and achieves 97% effectiveness of the ones built on GPT-4. Our results also show that the existing training datasets, which were expressly constructed for pointwise ranking, are insufficient for building such listwise rerankers. Instead, high-quality listwise ranking data is required and crucial, calling for further work on building human-annotated listwise data resources",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Zhang",
      "Sebastian Hofstätter",
      "Patrick Lewis",
      "Raphael Tang",
      "Jimmy Lin"
    ]
  },
  "http://arxiv.org/abs/2312.02931": {
    "title": "WhisBERT: Multimodal Text-Audio Language Modeling on 100M Words",
    "volume": "Dec",
    "abstract": "Training on multiple modalities of input can augment the capabilities of a language model. Here, we ask whether such a training regime can improve the quality and efficiency of these systems as well. We focus on text--audio and introduce Whisbert, which is inspired by the text--image approach of FLAVA \\citep{singh_flava_2022}. In accordance with Babylm \\citep{warstadt2023papers} guidelines, we pretrain Whisbert on a dataset comprising only 100 million words plus their corresponding speech from the word-aligned version of the People's Speech dataset \\citep{galvez_peoples_2021}. To assess the impact of multimodality, we compare versions of the model that are trained on text only and on both audio and text simultaneously. We find that while Whisbert is able to perform well on multimodal masked modeling and surpasses the Babylm baselines in most benchmark tasks, it struggles to optimize its complex objective and outperform its text-only Whisbert baseline",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lukas Wolf",
      "Greta Tuckute",
      "Klemen Kotar",
      "Eghbal Hosseini",
      "Tamar Regev",
      "Ethan Wilcox",
      "Alex Warstadt"
    ]
  },
  "http://arxiv.org/abs/2312.02913": {
    "title": "Let the LLMs Talk: Simulating Human-to-Human Conversational QA via Zero-Shot LLM-to-LLM Interactions",
    "volume": "Dec",
    "abstract": "Conversational question-answering (CQA) systems aim to create interactive search systems that effectively retrieve information by interacting with users. To replicate human-to-human conversations, existing work uses human annotators to play the roles of the questioner (student) and the answerer (teacher). Despite its effectiveness, challenges exist as human annotation is time-consuming, inconsistent, and not scalable. To address this issue and investigate the applicability of large language models (LLMs) in CQA simulation, we propose a simulation framework that employs zero-shot learner LLMs for simulating teacher-student interactions. Our framework involves two LLMs interacting on a specific topic, with the first LLM acting as a student, generating questions to explore a given search topic. The second LLM plays the role of a teacher by answering questions and is equipped with additional information, including a text on the given topic. We implement both the student and teacher by zero-shot prompting the GPT-4 model. To assess the effectiveness of LLMs in simulating CQA interactions and understand the disparities between LLM- and human-generated conversations, we evaluate the simulated data from various perspectives. We begin by evaluating the teacher's performance through both automatic and human assessment. Next, we evaluate the performance of the student, analyzing and comparing the disparities between questions generated by the LLM and those generated by humans. Furthermore, we conduct extensive analyses to thoroughly examine the LLM performance by benchmarking state-of-the-art reading comprehension models on both datasets. Our results reveal that the teacher LLM generates lengthier answers that tend to be more accurate and complete. The student LLM generates more diverse questions, covering more aspects of a given topic",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zahra Abbasiantaeb",
      "Yifei Yuan",
      "Evangelos Kanoulas",
      "Mohammad Aliannejadi"
    ]
  },
  "http://arxiv.org/abs/2312.02901": {
    "title": "Concept Drift Adaptation in Text Stream Mining Settings: A Comprehensive Review",
    "volume": "Dec",
    "abstract": "Due to the advent and increase in the popularity of the Internet, people have been producing and disseminating textual data in several ways, such as reviews, social media posts, and news articles. As a result, numerous researchers have been working on discovering patterns in textual data, especially because social media posts function as social sensors, indicating peoples' opinions, interests, etc. However, most tasks regarding natural language processing are addressed using traditional machine learning methods and static datasets. This setting can lead to several problems, such as an outdated dataset, which may not correspond to reality, and an outdated model, which has its performance degrading over time. Concept drift is another aspect that emphasizes these issues, which corresponds to data distribution and pattern changes. In a text stream scenario, it is even more challenging due to its characteristics, such as the high speed and data arriving sequentially. In addition, models for this type of scenario must adhere to the constraints mentioned above while learning from the stream by storing texts for a limited time and consuming low memory. In this study, we performed a systematic literature review regarding concept drift adaptation in text stream scenarios. Considering well-defined criteria, we selected 40 papers to unravel aspects such as text drift categories, types of text drift detection, model update mechanism, the addressed stream mining tasks, types of text representations, and text representation update mechanism. In addition, we discussed drift visualization and simulation and listed real-world datasets used in the selected papers. Therefore, this paper comprehensively reviews the concept drift adaptation in text stream mining scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cristiano Mesquita Garcia",
      "Ramon Simoes Abilio",
      "Alessandro Lameiras Koerich",
      "Alceu de Souza Britto Jr.",
      "Jean Paul Barddal"
    ]
  },
  "http://arxiv.org/abs/2312.02869": {
    "title": "Can a Tabula Recta provide security in the XXI century?",
    "volume": "Dec",
    "abstract": "In the not so unlikely scenario of total compromise of computers accessible to a group of users, they might be tempted to resort to human-computable paper-and-pencil cryptographic methods aided by a classic Tabula Recta, which helps to perform addition and subtraction directly with letters. But do these classic algorithms, or some new ones using the same simple tools, have any chance against computer-aided cryptanalysis? In this paper I discuss how some human-computable algorithms can indeed afford sufficient security in this situation, drawing conclusions from computer-based statistical analysis. Three kinds of algorithms are discussed: those that concentrate entropy from shared text sources, stream ciphers based on arithmetic of non-binary spaces, and hash-like algorithms that may be used to generate a password from a challenge text",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francisco Ruiz"
    ]
  },
  "http://arxiv.org/abs/2312.03042": {
    "title": "Inherent limitations of LLMs regarding spatial information",
    "volume": "Dec",
    "abstract": "Despite the significant advancements in natural language processing capabilities demonstrated by large language models such as ChatGPT, their proficiency in comprehending and processing spatial information, especially within the domains of 2D and 3D route planning, remains notably underdeveloped. This paper investigates the inherent limitations of ChatGPT and similar models in spatial reasoning and navigation-related tasks, an area critical for applications ranging from autonomous vehicle guidance to assistive technologies for the visually impaired. In this paper, we introduce a novel evaluation framework complemented by a baseline dataset, meticulously crafted for this study. This dataset is structured around three key tasks: plotting spatial points, planning routes in two-dimensional (2D) spaces, and devising pathways in three-dimensional (3D) environments. We specifically developed this dataset to assess the spatial reasoning abilities of ChatGPT. Our evaluation reveals key insights into the model's capabilities and limitations in spatial understanding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "He Yan",
      "Xinyao Hu",
      "Xiangpeng Wan",
      "Chengyu Huang",
      "Kai Zou",
      "Shiqi Xu"
    ]
  },
  "http://arxiv.org/abs/2312.02820": {
    "title": "Clustering Pseudo Language Family in Multilingual Translation Models with Fisher Information Matrix",
    "volume": "Dec",
    "abstract": "In multilingual translation research, the comprehension and utilization of language families are of paramount importance. Nevertheless, clustering languages based solely on their ancestral families can yield suboptimal results due to variations in the datasets employed during the model's training phase. To mitigate this challenge, we introduce an innovative method that leverages the fisher information matrix (FIM) to cluster language families, anchored on the multilingual translation model's characteristics. We hypothesize that language pairs with similar effects on model parameters exhibit a considerable degree of linguistic congruence and should thus be grouped cohesively. This concept has led us to define pseudo language families. We provide an in-depth discussion regarding the inception and application of these pseudo language families. Empirical evaluations reveal that employing these pseudo language families enhances performance over conventional language families in adapting a multilingual translation model to unfamiliar language pairs. The proposed methodology may also be extended to scenarios requiring language similarity measurements. The source code and associated scripts can be accessed at https://github.com/ecoli-hit/PseudoFamily",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Ma",
      "Xuebo Liu",
      "Min Zhang"
    ]
  },
  "http://arxiv.org/abs/2312.02803": {
    "title": "Leveraging Domain Adaptation and Data Augmentation to Improve Qur'anic IR in English and Arabic",
    "volume": "Dec",
    "abstract": "In this work, we approach the problem of Qur'anic information retrieval (IR) in Arabic and English. Using the latest state-of-the-art methods in neural IR, we research what helps to tackle this task more efficiently. Training retrieval models requires a lot of data, which is difficult to obtain for training in-domain. Therefore, we commence with training on a large amount of general domain data and then continue training on in-domain data. To handle the lack of in-domain data, we employed a data augmentation technique, which considerably improved results in MRR@10 and NDCG@5 metrics, setting the state-of-the-art in Qur'anic IR for both English and Arabic. The absence of an Islamic corpus and domain-specific model for IR task in English motivated us to address this lack of resources and take preliminary steps of the Islamic corpus compilation and domain-specific language model (LM) pre-training, which helped to improve the performance of the retrieval models that use the domain-specific LM as the shared backbone. We examined several language models (LMs) in Arabic to select one that efficiently deals with the Qur'anic IR task. Besides transferring successful experiments from English to Arabic, we conducted additional experiments with retrieval task in Arabic to amortize the scarcity of general domain datasets used to train the retrieval models. Handling Qur'anic IR task combining English and Arabic allowed us to enhance the comparison and share valuable insights across models and languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vera Pavlova"
    ]
  },
  "http://arxiv.org/abs/2312.02798": {
    "title": "Weakly Supervised Detection of Hallucinations in LLM Activations",
    "volume": "Dec",
    "abstract": "We propose an auditing method to identify whether a large language model (LLM) encodes patterns such as hallucinations in its internal states, which may propagate to downstream tasks. We introduce a weakly supervised auditing technique using a subset scanning approach to detect anomalous patterns in LLM activations from pre-trained models. Importantly, our method does not need knowledge of the type of patterns a-priori. Instead, it relies on a reference dataset devoid of anomalies during testing. Further, our approach enables the identification of pivotal nodes responsible for encoding these patterns, which may offer crucial insights for fine-tuning specific sub-networks for bias mitigation. We introduce two new scanning methods to handle LLM activations for anomalous sentences that may deviate from the expected distribution in either direction. Our results confirm prior findings of BERT's limited internal capacity for encoding hallucinations, while OPT appears capable of encoding hallucination information internally. Importantly, our scanning approach, without prior exposure to false statements, performs comparably to a fully supervised out-of-distribution classifier",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miriam Rateike",
      "Celia Cintas",
      "John Wamburu",
      "Tanya Akumu",
      "Skyler Speakman"
    ]
  },
  "http://arxiv.org/abs/2312.02783": {
    "title": "Large Language Models on Graphs: A Comprehensive Survey",
    "volume": "Dec",
    "abstract": "Large language models (LLMs), such as ChatGPT and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data are associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data are paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graph scenarios (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-rich graphs, and text-paired graphs. We then discuss detailed techniques for utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM as Aligner, and compare the advantages and disadvantages of different schools of models. Furthermore, we mention the real-world applications of such methods and summarize open-source codes and benchmark datasets. Finally, we conclude with potential future research directions in this fast-growing field. The related source can be found at https://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Jin",
      "Gang Liu",
      "Chi Han",
      "Meng Jiang",
      "Heng Ji",
      "Jiawei Han"
    ]
  },
  "http://arxiv.org/abs/2312.02780": {
    "title": "Scaling Laws for Adversarial Attacks on Language Model Activations",
    "volume": "Dec",
    "abstract": "We explore a class of adversarial attacks targeting the activations of language models. By manipulating a relatively small subset of model activations, $a$, we demonstrate the ability to control the exact prediction of a significant number (in some cases up to 1000) of subsequent tokens $t$. We empirically verify a scaling law where the maximum number of target tokens $t_\\mathrm{max}$ predicted depends linearly on the number of tokens $a$ whose activations the attacker controls as $t_\\mathrm{max} = \\kappa a$. We find that the number of bits of control in the input space needed to control a single bit in the output space (what we call attack resistance $\\chi$) is remarkably constant between $\\approx 16$ and $\\approx 25$ over 2 orders of magnitude of model sizes for different language models. Compared to attacks on tokens, attacks on activations are predictably much stronger, however, we identify a surprising regularity where one bit of input steered either via activations or via tokens is able to exert control over a similar amount of output bits. This gives support for the hypothesis that adversarial attacks are a consequence of dimensionality mismatch between the input and output spaces. A practical implication of the ease of attacking language model activations instead of tokens is for multi-modal and selected retrieval models, where additional data sources are added as activations directly, sidestepping the tokenized input. This opens up a new, broad attack surface. By using language models as a controllable test-bed to study adversarial attacks, we were able to experiment with input-output dimensions that are inaccessible in computer vision, especially where the output dimension dominates",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stanislav Fort"
    ]
  },
  "http://arxiv.org/abs/2312.02748": {
    "title": "Compositional Generalization for Data-to-Text Generation",
    "volume": "Dec",
    "abstract": "Data-to-text generation involves transforming structured data, often represented as predicate-argument tuples, into coherent textual descriptions. Despite recent advances, systems still struggle when confronted with unseen combinations of predicates, producing unfaithful descriptions (e.g. hallucinations or omissions). We refer to this issue as compositional generalisation, and it encouraged us to create a benchmark for assessing the performance of different approaches on this specific problem. Furthermore, we propose a novel model that addresses compositional generalization by clustering predicates into groups. Our model generates text in a sentence-by-sentence manner, relying on one cluster of predicates at a time. This approach significantly outperforms T5~baselines across all evaluation metrics.Notably, it achieved a 31% improvement over T5 in terms of a metric focused on maintaining faithfulness to the input",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinnuo Xu",
      "Ivan Titov",
      "Mirella Lapata"
    ]
  },
  "http://arxiv.org/abs/2312.02730": {
    "title": "Towards Measuring Representational Similarity of Large Language Models",
    "volume": "Dec",
    "abstract": "Understanding the similarity of the numerous released large language models (LLMs) has many uses, e.g., simplifying model selection, detecting illegal model reuse, and advancing our understanding of what makes LLMs perform well. In this work, we measure the similarity of representations of a set of LLMs with 7B parameters. Our results suggest that some LLMs are substantially different from others. We identify challenges of using representational similarity measures that suggest the need of careful study of similarity scores to avoid false conclusions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Max Klabunde",
      "Mehdi Ben Amor",
      "Michael Granitzer",
      "Florian Lemmerich"
    ]
  },
  "http://arxiv.org/abs/2312.02706": {
    "title": "Large Knowledge Model: Perspectives and Challenges",
    "volume": "Dec",
    "abstract": "Humankind's understanding of the world is fundamentally linked to our perception and cognition, with \\emph{human languages} serving as one of the major carriers of \\emph{world knowledge}. In this vein, \\emph{Large Language Models} (LLMs) like ChatGPT epitomize the pre-training of extensive, sequence-based world knowledge into neural networks, facilitating the processing and manipulation of this knowledge in a parametric space. This article explores large models through the lens of ``knowledge''. We initially investigate the role of symbolic knowledge such as Knowledge Graphs (KGs) in enhancing LLMs, covering aspects like knowledge-augmented language model, structure-inducing pre-training, knowledgeable prompts, structured CoT, knowledge editing, semantic tools for LLM and knowledgeable AI agents. Subsequently, we examine how LLMs can amplify traditional symbolic knowledge bases, encompassing aspects like using LLM as KG builder and controller, structured knowledge pretraining, LLM-enhanced symbolic reasoning, and the amalgamation of perception with cognition. Considering the intricate nature of human knowledge, we advocate for the creation of \\emph{Large Knowledge Models} (LKM), specifically engineered to manage diversified spectrum of knowledge structures. This ambitious undertaking could entail several key challenges, such as disentangling knowledge representation from language models, restructuring pre-training with structured knowledge, and building large commonsense models, among others. We finally propose a five-``A'' principle to distinguish the concept of LKM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huajun Chen"
    ]
  },
  "http://arxiv.org/abs/2312.03759": {
    "title": "How should the advent of large language models affect the practice of science?",
    "volume": "Dec",
    "abstract": "Large language models (LLMs) are being increasingly incorporated into scientific workflows. However, we have yet to fully grasp the implications of this integration. How should the advent of large language models affect the practice of science? For this opinion piece, we have invited four diverse groups of scientists to reflect on this query, sharing their perspectives and engaging in debate. Schulz et al. make the argument that working with LLMs is not fundamentally different from working with human collaborators, while Bender et al. argue that LLMs are often misused and over-hyped, and that their limitations warrant a focus on more specialized, easily interpretable tools. Marelli et al. emphasize the importance of transparent attribution and responsible use of LLMs. Finally, Botvinick and Gershman advocate that humans should retain responsibility for determining the scientific roadmap. To facilitate the discussion, the four perspectives are complemented with a response from each group. By putting these different perspectives in conversation, we aim to bring attention to important considerations within the academic community regarding the adoption of LLMs and their impact on both current and future scientific practices",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcel Binz",
      "Stephan Alaniz",
      "Adina Roskies",
      "Balazs Aczel",
      "Carl T. Bergstrom",
      "Colin Allen",
      "Daniel Schad",
      "Dirk Wulff",
      "Jevin D. West",
      "Qiong Zhang",
      "Richard M. Shiffrin",
      "Samuel J. Gershman",
      "Ven Popov",
      "Emily M. Bender",
      "Marco Marelli",
      "Matthew M. Botvinick",
      "Zeynep Akata",
      "Eric Schulz"
    ]
  },
  "http://arxiv.org/abs/2312.02614": {
    "title": "Prompt Optimization via Adversarial In-Context Learning",
    "volume": "Dec",
    "abstract": "We propose a new method, Adversarial In-Context Learning (adv-ICL), to optimize prompt for in-context learning (ICL) by employing one LLM as a generator, another as a discriminator, and a third as a prompt modifier. As in traditional adversarial learning, adv-ICL is implemented as a two-player game between the generator and discriminator, where the generator tries to generate realistic enough output to fool the discriminator. In each round, given an input prefixed by task instructions and several exemplars, the generator produces an output. The discriminator is then tasked with classifying the generator input-output pair as model-generated or real data. Based on the discriminator loss, the prompt modifier proposes possible edits to the generator and discriminator prompts, and the edits that most improve the adversarial loss are selected. We show that adv-ICL results in significant improvements over state-of-the-art prompt optimization techniques for both open and closed-source models on 11 generation and classification tasks including summarization, arithmetic reasoning, machine translation, data-to-text generation, and the MMLU and big-bench hard benchmarks. In addition, because our method uses pre-trained models and updates only prompts rather than model parameters, it is computationally efficient, easy to extend to any LLM and task, and effective in low-resource settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Long Do",
      "Yiran Zhao",
      "Hannah Brown",
      "Yuxi Xie",
      "James Xu Zhao",
      "Nancy F. Chen",
      "Kenji Kawaguchi",
      "Michael Qizhe Xie",
      "Junxian He"
    ]
  },
  "http://arxiv.org/abs/2312.02598": {
    "title": "Impact of Tokenization on LLaMa Russian Adaptation",
    "volume": "Dec",
    "abstract": "Latest instruction-tuned large language models (LLM) show great results on various tasks, however, they often face performance degradation for non-English input. There is evidence that the reason lies in inefficient tokenization caused by low language representation in pre-training data which hinders the comprehension of non-English instructions, limiting the potential of target language instruction-tuning. In this work we investigate the possibility of addressing the issue with vocabulary substitution in the context of LLaMa Russian language adaptation. We explore three variants of vocabulary adaptation and test their performance on Saiga instruction-tuning and fine-tuning on Russian Super Glue benchmark. The results of automatic evaluation show that vocabulary substitution not only improves the model's quality in Russian but also accelerates fine-tuning (35%) and inference (up to 60%) while reducing memory consumption. Additional human evaluation of the instruction-tuned models demonstrates that models with Russian-adapted vocabulary generate answers with higher user preference than the original Saiga-LLaMa model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mikhail Tikhomirov",
      "Daniil Chernyshev"
    ]
  },
  "http://arxiv.org/abs/2312.02590": {
    "title": "Text Intimacy Analysis using Ensembles of Multilingual Transformers",
    "volume": "Dec",
    "abstract": "Intimacy estimation of a given text has recently gained importance due to the increase in direct interaction of NLP systems with humans. Intimacy is an important aspect of natural language and has a substantial impact on our everyday communication. Thus the level of intimacy can provide us with deeper insights and richer semantics of conversations. In this paper, we present our work on the SemEval shared task 9 on predicting the level of intimacy for the given text. The dataset consists of tweets in ten languages, out of which only six are available in the training dataset. We conduct several experiments and show that an ensemble of multilingual models along with a language-specific monolingual model has the best performance. We also evaluate other data augmentation methods such as translation and present the results. Lastly, we study the results thoroughly and present some noteworthy insights into this problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanmay Chavan",
      "Ved Patwardhan"
    ]
  },
  "http://arxiv.org/abs/2312.02578": {
    "title": "Empathy and Distress Detection using Ensembles of Transformer Models",
    "volume": "Dec",
    "abstract": "This paper presents our approach for the WASSA 2023 Empathy, Emotion and Personality Shared Task. Empathy and distress are human feelings that are implicitly expressed in natural discourses. Empathy and distress detection are crucial challenges in Natural Language Processing that can aid our understanding of conversations. The provided dataset consists of several long-text examples in the English language, with each example associated with a numeric score for empathy and distress. We experiment with several BERT-based models as a part of our approach. We also try various ensemble methods. Our final submission has a Pearson's r score of 0.346, placing us third in the empathy and distress detection subtask",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanmay Chavan",
      "Kshitij Deshpande",
      "Sheetal Sonawane"
    ]
  },
  "http://arxiv.org/abs/2312.03025": {
    "title": "Training on Synthetic Data Beats Real Data in Multimodal Relation Extraction",
    "volume": "Dec",
    "abstract": "The task of multimodal relation extraction has attracted significant research attention, but progress is constrained by the scarcity of available training data. One natural thought is to extend existing datasets with cross-modal generative models. In this paper, we consider a novel problem setting, where only unimodal data, either text or image, are available during training. We aim to train a multimodal classifier from synthetic data that perform well on real multimodal test data. However, training with synthetic data suffers from two obstacles: lack of data diversity and label information loss. To alleviate the issues, we propose Mutual Information-aware Multimodal Iterated Relational dAta GEneration (MI2RAGE), which applies Chained Cross-modal Generation (CCG) to promote diversity in the generated data and exploits a teacher network to select valuable training samples with high mutual information with the ground-truth labels. Comparing our method to direct training on synthetic data, we observed a significant improvement of 24.06% F1 with synthetic text and 26.42% F1 with synthetic images. Notably, our best model trained on completely synthetic images outperforms prior state-of-the-art models trained on real multimodal data by a margin of 3.76% in F1. Our codebase will be made available upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zilin Du",
      "Haoxin Li",
      "Xu Guo",
      "Boyang Li"
    ]
  },
  "http://arxiv.org/abs/2312.02554": {
    "title": "ULMA: Unified Language Model Alignment with Demonstration and Point-wise Human Preference",
    "volume": "Dec",
    "abstract": "Language model alignment is a cutting-edge technique in large language model training to align the model output to user's intent, e.g., being helpful and harmless. Recent alignment framework consists of two steps: supervised fine-tuning with demonstration data and preference learning with human preference data. Previous preference learning methods, such as RLHF and DPO, mainly focus on pair-wise preference data. However, in many real-world scenarios where human feedbacks are intrinsically point-wise, these methods will suffer from information loss or even fail. To fill this gap, in this paper, we first develop a preference learning method called point-wise DPO to tackle point-wise preference data. Further revelation on the connection between supervised fine-tuning and point-wise preference learning enables us to develop a unified framework for both human demonstration and point-wise preference data, which sheds new light on the construction of preference dataset. Extensive experiments on point-wise datasets with binary or continuous labels demonstrate the superior performance and efficiency of our proposed methods. A new dataset with high-quality demonstration samples on harmlessness is constructed and made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianchi Cai",
      "Xierui Song",
      "Jiyan Jiang",
      "Fei Teng",
      "Jinjie Gu",
      "Guannan Zhang"
    ]
  },
  "http://arxiv.org/abs/2312.02549": {
    "title": "DemaFormer: Damped Exponential Moving Average Transformer with Energy-Based Modeling for Temporal Language Grounding",
    "volume": "Dec",
    "abstract": "Temporal Language Grounding seeks to localize video moments that semantically correspond to a natural language query. Recent advances employ the attention mechanism to learn the relations between video moments and the text query. However, naive attention might not be able to appropriately capture such relations, resulting in ineffective distributions where target video moments are difficult to separate from the remaining ones. To resolve the issue, we propose an energy-based model framework to explicitly learn moment-query distributions. Moreover, we propose DemaFormer, a novel Transformer-based architecture that utilizes exponential moving average with a learnable damping factor to effectively encode moment-query inputs. Comprehensive experiments on four public temporal language grounding datasets showcase the superiority of our methods over the state-of-the-art baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thong Nguyen",
      "Xiaobao Wu",
      "Xinshuai Dong",
      "Cong-Duy Nguyen",
      "See-Kiong Ng",
      "Luu Anh Tuan"
    ]
  },
  "http://arxiv.org/abs/2312.03022": {
    "title": "Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph Construction",
    "volume": "Dec",
    "abstract": "Knowledge graph construction (KGC) is a multifaceted undertaking involving the extraction of entities, relations, and events. Traditionally, large language models (LLMs) have been viewed as solitary task-solving agents in this complex landscape. However, this paper challenges this paradigm by introducing a novel framework, CooperKGC. Departing from the conventional approach, CooperKGC establishes a collaborative processing network, assembling a KGC collaboration team capable of concurrently addressing entity, relation, and event extraction tasks. Our experiments unequivocally demonstrate that fostering collaboration and information interaction among diverse agents within CooperKGC yields superior results compared to individual cognitive processes operating in isolation. Importantly, our findings reveal that the collaboration facilitated by CooperKGC enhances knowledge selection, correction, and aggregation capabilities across multiple rounds of interactions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongbin Ye",
      "Honghao Gui",
      "Aijia Zhang",
      "Tong Liu",
      "Wei Hua",
      "Weiqiang Jia"
    ]
  },
  "http://arxiv.org/abs/2312.02532": {
    "title": "DRAFT: Dense Retrieval Augmented Few-shot Topic classifier Framework",
    "volume": "Dec",
    "abstract": "With the growing volume of diverse information, the demand for classifying arbitrary topics has become increasingly critical. To address this challenge, we introduce DRAFT, a simple framework designed to train a classifier for few-shot topic classification. DRAFT uses a few examples of a specific topic as queries to construct Customized dataset with a dense retriever model. Multi-query retrieval (MQR) algorithm, which effectively handles multiple queries related to a specific topic, is applied to construct the Customized dataset. Subsequently, we fine-tune a classifier using the Customized dataset to identify the topic. To demonstrate the efficacy of our proposed approach, we conduct evaluations on both widely used classification benchmark datasets and manually constructed datasets with 291 diverse topics, which simulate diverse contents encountered in real-world applications. DRAFT shows competitive or superior performance compared to baselines that use in-context learning, such as GPT-3 175B and InstructGPT 175B, on few-shot topic classification tasks despite having 177 times fewer parameters, demonstrating its effectiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keonwoo Kim",
      "Younggun Lee"
    ]
  },
  "http://arxiv.org/abs/2312.02496": {
    "title": "MKA: A Scalable Medical Knowledge Assisted Mechanism for Generative Models on Medical Conversation Tasks",
    "volume": "Dec",
    "abstract": "Using natural language processing (NLP) technologies to develop medical chatbots makes the diagnosis of the patient more convenient and efficient, which is a typical application in healthcare AI. Because of its importance, lots of research have been come out. Recently, the neural generative models have shown their impressive ability as the core of chatbot, while it cannot scale well when directly applied to medical conversation due to the lack of medical-specific knowledge. To address the limitation, a scalable Medical Knowledge Assisted mechanism, MKA, is proposed in this paper. The mechanism aims to assist general neural generative models to achieve better performance on the medical conversation task. The medical-specific knowledge graph is designed within the mechanism, which contains 6 types of medical-related information, including department, drug, check, symptom, disease, food. Besides, the specific token concatenation policy is defined to effectively inject medical information into the input data. Evaluation of our method is carried out on two typical medical datasets, MedDG and MedDialog-CN. The evaluation results demonstrate that models combined with our mechanism outperform original methods in multiple automatic evaluation metrics. Besides, MKA-Bert-GPT achieves state-of-the-art performance. The open-sourced codes are public: https://github.com/LIANGKE23/Knowledge_Assisted_Medical_Dialogue_Generation_Mechanism",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Liang",
      "Sifan Wu",
      "Jiayi Gu"
    ]
  },
  "http://arxiv.org/abs/2312.02441": {
    "title": "MedDM:LLM-executable clinical guidance tree for clinical decision-making",
    "volume": "Dec",
    "abstract": "It is becoming increasingly emphasis on the importance of LLM participating in clinical diagnosis decision-making. However, the low specialization refers to that current medical LLMs can not provide specific medical advice, which are more like a medical Q\\&A. And there is no suitable clinical guidance tree data set that can be used directly with LLM. To address this issue, we first propose LLM-executavle clinical guidance tree(CGT), which can be directly used by large language models, and construct medical diagnostic decision-making dataset (MedDM), from flowcharts in clinical practice guidelines. We propose an approach to screen flowcharts from medical literature, followed by their identification and conversion into standardized diagnostic decision trees. Constructed a knowledge base with 1202 decision trees, which came from 5000 medical literature and covered 12 hospital departments, including internal medicine, surgery, psychiatry, and over 500 diseases.Moreover, we propose a method for reasoning on LLM-executable CGT and a Patient-LLM multi-turn dialogue framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Binbin Li",
      "Tianxin Meng",
      "Xiaoming Shi",
      "Jie Zhai",
      "Tong Ruan"
    ]
  },
  "http://arxiv.org/abs/2312.02439": {
    "title": "Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation",
    "volume": "Dec",
    "abstract": "Chain-of-Thought (CoT) guides large language models (LLMs) to reason step-by-step, and can motivate their logical reasoning ability. While effective for logical tasks, CoT is not conducive to creative problem-solving which often requires out-of-box thoughts and is crucial for innovation advancements. In this paper, we explore the Leap-of-Thought (LoT) abilities within LLMs -- a non-sequential, creative paradigm involving strong associations and knowledge leaps. To this end, we study LLMs on the popular Oogiri game which needs participants to have good creativity and strong associative thinking for responding unexpectedly and humorously to the given image, text, or both, and thus is suitable for LoT study. Then to investigate LLMs' LoT ability in the Oogiri game, we first build a multimodal and multilingual Oogiri-GO dataset which contains over 130,000 samples from the Oogiri game, and observe the insufficient LoT ability or failures of most existing LLMs on the Oogiri game. Accordingly, we introduce a creative Leap-of-Thought (CLoT) paradigm to improve LLM's LoT ability. CLoT first formulates the Oogiri-GO dataset into LoT-oriented instruction tuning data to train pretrained LLM for achieving certain LoT humor generation and discrimination abilities. Then CLoT designs an explorative self-refinement that encourages the LLM to generate more creative LoT data via exploring parallels between seemingly unrelated concepts and selects high-quality data to train itself for self-refinement. CLoT not only excels in humor generation in the Oogiri game but also boosts creative abilities in various tasks like cloud guessing game and divergent association task. These findings advance our understanding and offer a pathway to improve LLMs' creative capacities for innovative applications across domains. The dataset, code, and models will be released online. https://github.com/sail-sg/CLoT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shanshan Zhong",
      "Zhongzhan Huang",
      "Shanghua Gao",
      "Wushao Wen",
      "Liang Lin",
      "Marinka Zitnik",
      "Pan Zhou"
    ]
  },
  "http://arxiv.org/abs/2312.02436": {
    "title": "MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction-Following",
    "volume": "Dec",
    "abstract": "In the realm of large language models (LLMs), enhancing instruction-following capability often involves curating expansive training data. This is achieved through two primary schemes: i) Scaling-Inputs: Amplifying (input, output) pairs per task instruction, aiming for better instruction adherence. ii) Scaling Input-Free Tasks: Enlarging tasks, each composed of an (instruction, output) pair (without requiring a separate input anymore). However, LLMs under Scaling-Inputs tend to be overly sensitive to inputs, leading to misinterpretation or non-compliance with instructions. Conversely, Scaling Input-Free Tasks demands a substantial number of tasks but is less effective in instruction following when dealing with instances in Scaling-Inputs. This work introduces MUFFIN, a new scheme of instruction-following dataset curation. Specifically, we automatically Scale Tasks per Input by diversifying these tasks with various input facets. Experimental results across four zero-shot benchmarks, spanning both Scaling-Inputs and Scaling Input-Free Tasks schemes, reveal that LLMs, at various scales, trained on MUFFIN generally demonstrate superior instruction-following capabilities compared to those trained on the two aforementioned schemes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renze Lou",
      "Kai Zhang",
      "Jian Xie",
      "Yuxuan Sun",
      "Janice Ahn",
      "Hanzi Xu",
      "Yu Su",
      "Wenpeng Yin"
    ]
  },
  "http://arxiv.org/abs/2312.02431": {
    "title": "Visually Grounded Language Learning: a review of language games, datasets, tasks, and models",
    "volume": "Dec",
    "abstract": "In recent years, several machine learning models have been proposed. They are trained with a language modelling objective on large-scale text-only data. With such pretraining, they can achieve impressive results on many Natural Language Understanding and Generation tasks. However, many facets of meaning cannot be learned by ``listening to the radio\" only. In the literature, many Vision+Language (V+L) tasks have been defined with the aim of creating models that can ground symbols in the visual modality. In this work, we provide a systematic literature review of several tasks and models proposed in the V+L field. We rely on Wittgenstein's idea of `language games' to categorise such tasks into 3 different families: 1) discriminative games, 2) generative games, and 3) interactive games. Our analysis of the literature provides evidence that future work should be focusing on interactive games where communication in Natural Language is important to resolve ambiguities about object referents and action plans and that physical embodiment is essential to understand the semantics of situations and events. Overall, these represent key requirements for developing grounded meanings in neural models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessandro Suglia",
      "Ioannis Konstas",
      "Oliver Lemon"
    ]
  },
  "http://arxiv.org/abs/2312.03016": {
    "title": "Protein Language Model-Powered 3D Ligand Binding Site Prediction from Protein Sequence",
    "volume": "Dec",
    "abstract": "Prediction of ligand binding sites of proteins is a fundamental and important task for understanding the function of proteins and screening potential drugs. Most existing methods require experimentally determined protein holo-structures as input. However, such structures can be unavailable on novel or less-studied proteins. To tackle this limitation, we propose LaMPSite, which only takes protein sequences and ligand molecular graphs as input for ligand binding site predictions. The protein sequences are used to retrieve residue-level embeddings and contact maps from the pre-trained ESM-2 protein language model. The ligand molecular graphs are fed into a graph neural network to compute atom-level embeddings. Then we compute and update the protein-ligand interaction embedding based on the protein residue-level embeddings and ligand atom-level embeddings, and the geometric constraints in the inferred protein contact map and ligand distance map. A final pooling on protein-ligand interaction embedding would indicate which residues belong to the binding sites. Without any 3D coordinate information of proteins, our proposed model achieves competitive performance compared to baseline methods that require 3D protein structures when predicting binding sites. Given that less than 50% of proteins have reliable structure information in the current stage, LaMPSite will provide new opportunities for drug discovery",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Zhang",
      "Lei Xie"
    ]
  },
  "http://arxiv.org/abs/2312.02418": {
    "title": "Decoding Data Quality via Synthetic Corruptions: Embedding-guided Pruning of Code Data",
    "volume": "Dec",
    "abstract": "Code datasets, often collected from diverse and uncontrolled sources such as GitHub, potentially suffer from quality issues, thereby affecting the performance and training efficiency of Large Language Models (LLMs) optimized for code generation. Previous studies demonstrated the benefit of using embedding spaces for data pruning, but they mainly focused on duplicate removal or increasing variety, and in other modalities, such as images. Our work focuses on using embeddings to identify and remove \"low-quality\" code data. First, we explore features of \"low-quality\" code in embedding space, through the use of synthetic corruptions. Armed with this knowledge, we devise novel pruning metrics that operate in embedding space to identify and remove low-quality entries in the Stack dataset. We demonstrate the benefits of this synthetic corruption informed pruning (SCIP) approach on the well-established HumanEval and MBPP benchmarks, outperforming existing embedding-based methods. Importantly, we achieve up to a 3% performance improvement over no pruning, thereby showing the promise of insights from synthetic corruptions for data pruning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Yang",
      "Aaditya K. Singh",
      "Mostafa Elhoushi",
      "Anas Mahmoud",
      "Kushal Tirumala",
      "Fabian Gloeckle",
      "Baptiste Rozière",
      "Carole-Jean Wu",
      "Ari S. Morcos",
      "Newsha Ardalani"
    ]
  },
  "http://arxiv.org/abs/2312.02406": {
    "title": "Efficient Online Data Mixing For Language Model Pre-Training",
    "volume": "Dec",
    "abstract": "The data used to pretrain large language models has a decisive impact on a model's downstream performance, which has led to a large body of work on data selection methods that aim to automatically determine the most suitable data to use for pretraining. Existing data selection methods suffer from slow and computationally expensive processes, a problem amplified by the increasing size of models and of pretraining datasets. Data mixing, on the other hand, reduces the complexity of data selection by grouping data points together and determining sampling probabilities across entire groups. However, data mixing proportions are typically fixed before training and therefore cannot adapt to changing training dynamics. To address these limitations, we develop an efficient algorithm for Online Data Mixing (ODM) that combines elements from both data selection and data mixing. Based on multi-armed bandit algorithms, our online approach optimizes the data mixing proportions during training. Remarkably, our method trains a model that reaches the final perplexity of the next best method with 19\\% fewer training iterations, and improves performance on the 5-shot MMLU benchmark by 1.9% relative accuracy, while adding negligible wall-clock time during pretraining",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alon Albalak",
      "Liangming Pan",
      "Colin Raffel",
      "William Yang Wang"
    ]
  },
  "http://arxiv.org/abs/2312.02382": {
    "title": "New Evaluation Metrics Capture Quality Degradation due to LLM Watermarking",
    "volume": "Dec",
    "abstract": "With the increasing use of large-language models (LLMs) like ChatGPT, watermarking has emerged as a promising approach for tracing machine-generated content. However, research on LLM watermarking often relies on simple perplexity or diversity-based measures to assess the quality of watermarked text, which can mask important limitations in watermarking. Here we introduce two new easy-to-use methods for evaluating watermarking algorithms for LLMs: 1) evaluation by LLM-judger with specific guidelines; and 2) binary classification on text embeddings to distinguish between watermarked and unwatermarked text. We apply these methods to characterize the effectiveness of current watermarking techniques. Our experiments, conducted across various datasets, reveal that current watermarking methods are detectable by even simple classifiers, challenging the notion of watermarking subtlety. We also found, through the LLM judger, that watermarking impacts text quality, especially in degrading the coherence and depth of the response. Our findings underscore the trade-off between watermark robustness and text quality and highlight the importance of having more informative metrics to assess watermarking quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karanpartap Singh",
      "James Zou"
    ]
  },
  "http://arxiv.org/abs/2312.03758": {
    "title": "Stock Movement and Volatility Prediction from Tweets, Macroeconomic Factors and Historical Prices",
    "volume": "Dec",
    "abstract": "Predicting stock market is vital for investors and policymakers, acting as a barometer of the economic health. We leverage social media data, a potent source of public sentiment, in tandem with macroeconomic indicators as government-compiled statistics, to refine stock market predictions. However, prior research using tweet data for stock market prediction faces three challenges. First, the quality of tweets varies widely. While many are filled with noise and irrelevant details, only a few genuinely mirror the actual market scenario. Second, solely focusing on the historical data of a particular stock without considering its sector can lead to oversight. Stocks within the same industry often exhibit correlated price behaviors. Lastly, simply forecasting the direction of price movement without assessing its magnitude is of limited value, as the extent of the rise or fall truly determines profitability. In this paper, diverging from the conventional methods, we pioneer an ECON. The framework has following advantages: First, ECON has an adept tweets filter that efficiently extracts and decodes the vast array of tweet data. Second, ECON discerns multi-level relationships among stocks, sectors, and macroeconomic factors through a self-aware mechanism in semantic space. Third, ECON offers enhanced accuracy in predicting substantial stock price fluctuations by capitalizing on stock price movement. We showcase the state-of-the-art performance of our proposed model using a dataset, specifically curated by us, for predicting stock market movements and volatility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengkun Wang",
      "YangXiao Bai",
      "Taoran Ji",
      "Kaiqun Fu",
      "Linhan Wang",
      "Chang-Tien Lu"
    ]
  },
  "http://arxiv.org/abs/2312.02337": {
    "title": "Measuring Distributional Shifts in Text: The Advantage of Language Model-Based Embeddings",
    "volume": "Dec",
    "abstract": "An essential part of monitoring machine learning models in production is measuring input and output data drift. In this paper, we present a system for measuring distributional shifts in natural language data and highlight and investigate the potential advantage of using large language models (LLMs) for this problem. Recent advancements in LLMs and their successful adoption in different domains indicate their effectiveness in capturing semantic relationships for solving various natural language processing problems. The power of LLMs comes largely from the encodings (embeddings) generated in the hidden layers of the corresponding neural network. First we propose a clustering-based algorithm for measuring distributional shifts in text data by exploiting such embeddings. Then we study the effectiveness of our approach when applied to text embeddings generated by both LLMs and classical embedding algorithms. Our experiments show that general-purpose LLM-based embeddings provide a high sensitivity to data drift compared to other embedding methods. We propose drift sensitivity as an important evaluation metric to consider when comparing language models. Finally, we present insights and lessons learned from deploying our framework as part of the Fiddler ML Monitoring platform over a period of 18 months",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gyandev Gupta",
      "Bashir Rastegarpanah",
      "Amalendu Iyer",
      "Joshua Rubin",
      "Krishnaram Kenthapadi"
    ]
  },
  "http://arxiv.org/abs/2312.02334": {
    "title": "An Evaluation Framework for Mapping News Headlines to Event Classes in a Knowledge Graph",
    "volume": "Dec",
    "abstract": "Mapping ongoing news headlines to event-related classes in a rich knowledge base can be an important component in a knowledge-based event analysis and forecasting solution. In this paper, we present a methodology for creating a benchmark dataset of news headlines mapped to event classes in Wikidata, and resources for the evaluation of methods that perform the mapping. We use the dataset to study two classes of unsupervised methods for this task: 1) adaptations of classic entity linking methods, and 2) methods that treat the problem as a zero-shot text classification problem. For the first approach, we evaluate off-the-shelf entity linking systems. For the second approach, we explore a) pre-trained natural language inference (NLI) models, and b) pre-trained large generative language models. We present the results of our evaluation, lessons learned, and directions for future work. The dataset and scripts for evaluation are made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steve Fonin Mbouadeu",
      "Martin Lorenzo",
      "Ken Barker",
      "Oktie Hassanzadeh"
    ]
  },
  "http://arxiv.org/abs/2312.02331": {
    "title": "Revisiting Topic-Guided Language Models",
    "volume": "Dec",
    "abstract": "A recent line of work in natural language processing has aimed to combine language models and topic models. These topic-guided language models augment neural language models with topic models, unsupervised learning methods that can discover document-level patterns of word use. This paper compares the effectiveness of these methods in a standardized setting. We study four topic-guided language models and two baselines, evaluating the held-out predictive performance of each model on four corpora. Surprisingly, we find that none of these methods outperform a standard LSTM language model baseline, and most fail to learn good topics. Further, we train a probe of the neural language model that shows that the baseline's hidden states already encode topic information. We make public all code used for this study",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carolina Zheng",
      "Keyon Vafa",
      "David M. Blei"
    ]
  },
  "http://arxiv.org/abs/2312.02317": {
    "title": "GNN2R: Weakly-Supervised Rationale-Providing Question Answering over Knowledge Graphs",
    "volume": "Dec",
    "abstract": "Most current methods for multi-hop question answering (QA) over knowledge graphs (KGs) only provide final conclusive answers without explanations, such as a set of KG entities that is difficult for normal users to review and comprehend. This issue severely limits the application of KG-based QA in real-world scenarios. However, it is non-trivial to solve due to two challenges: First, annotations of reasoning chains of multi-hop questions, which could serve as supervision for explanation generation, are usually lacking. Second, it is difficult to maintain high efficiency when explicit KG triples need to be retrieved to generate explanations. In this paper, we propose a novel Graph Neural Network-based Two-Step Reasoning model (GNN2R) to solve this issue. GNN2R can provide both final answers and reasoning subgraphs as a rationale behind final answers efficiently with only weak supervision that is available through question-final answer pairs. We extensively evaluated GNN2R with detailed analyses in experiments. The results demonstrate that, in terms of effectiveness, efficiency, and quality of generated explanations, GNN2R outperforms existing state-of-the-art methods that are applicable to this task. Our code and pre-trained models are available at https://github.com/ruijie-wang-uzh/GNN2R",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruijie Wang",
      "Luca Rossetto",
      "Michael Cochez",
      "Abraham Bernstein"
    ]
  },
  "http://arxiv.org/abs/2312.02314": {
    "title": "Fine-tuning pre-trained extractive QA models for clinical document parsing",
    "volume": "Dec",
    "abstract": "Electronic health records (EHRs) contain a vast amount of high-dimensional multi-modal data that can accurately represent a patient's medical history. Unfortunately, most of this data is either unstructured or semi-structured, rendering it unsuitable for real-time and retrospective analyses. A remote patient monitoring (RPM) program for Heart Failure (HF) patients needs to have access to clinical markers like EF (Ejection Fraction) or LVEF (Left Ventricular Ejection Fraction) in order to ascertain eligibility and appropriateness for the program. This paper explains a system that can parse echocardiogram reports and verify EF values. This system helps identify eligible HF patients who can be enrolled in such a program. At the heart of this system is a pre-trained extractive QA transformer model that is fine-tuned on custom-labeled data. The methods used to prepare such a model for deployment are illustrated by running experiments on a public clinical dataset like MIMIC-IV-Note. The pipeline can be used to generalize solutions to similar problems in a low-resource setting. We found that the system saved over 1500 hours for our clinicians over 12 months by automating the task at scale",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashwyn Sharma",
      "David I. Feldman",
      "Aneesh Jain"
    ]
  },
  "http://arxiv.org/abs/2312.02310": {
    "title": "VaQuitA: Enhancing Alignment in LLM-Assisted Video Understanding",
    "volume": "Dec",
    "abstract": "Recent advancements in language-model-based video understanding have been progressing at a remarkable pace, spurred by the introduction of Large Language Models (LLMs). However, the focus of prior research has been predominantly on devising a projection layer that maps video features to tokens, an approach that is both rudimentary and inefficient. In our study, we introduce a cutting-edge framework, VaQuitA, designed to refine the synergy between video and textual information. At the data level, instead of sampling frames uniformly, we implement a sampling method guided by CLIP-score rankings, which enables a more aligned selection of frames with the given question. At the feature level, we integrate a trainable Video Perceiver alongside a Visual-Query Transformer (abbreviated as VQ-Former), which bolsters the interplay between the input question and the video features. We also discover that incorporating a simple prompt, \"Please be critical\", into the LLM input can substantially enhance its video comprehension capabilities. Our experimental results indicate that VaQuitA consistently sets a new benchmark for zero-shot video question-answering tasks and is adept at producing high-quality, multi-turn video dialogues with users",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizhou Wang",
      "Ruiyi Zhang",
      "Haoliang Wang",
      "Uttaran Bhattacharya",
      "Yun Fu",
      "Gang Wu"
    ]
  },
  "http://arxiv.org/abs/2312.03756": {
    "title": "LineConGraphs: Line Conversation Graphs for Effective Emotion Recognition using Graph Neural Networks",
    "volume": "Dec",
    "abstract": "Emotion Recognition in Conversations (ERC) is a critical aspect of affective computing, and it has many practical applications in healthcare, education, chatbots, and social media platforms. Earlier approaches for ERC analysis involved modeling both speaker and long-term contextual information using graph neural network architectures. However, it is ideal to deploy speaker-independent models for real-world applications. Additionally, long context windows can potentially create confusion in recognizing the emotion of an utterance in a conversation. To overcome these limitations, we propose novel line conversation graph convolutional network (LineConGCN) and graph attention (LineConGAT) models for ERC analysis. These models are speaker-independent and built using a graph construction strategy for conversations -- line conversation graphs (LineConGraphs). The conversational context in LineConGraphs is short-term -- limited to one previous and future utterance, and speaker information is not part of the graph. We evaluate the performance of our proposed models on two benchmark datasets, IEMOCAP and MELD, and show that our LineConGAT model outperforms the state-of-the-art methods with an F1-score of 64.58% and 76.50%. Moreover, we demonstrate that embedding sentiment shift information into line conversation graphs further enhances the ERC performance in the case of GCN models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gokul S Krishnan",
      "Sarala Padi",
      "Craig S. Greenberg",
      "Balaraman Ravindran",
      "Dinesh Manoch",
      "Ram D. Sriram"
    ]
  },
  "http://arxiv.org/abs/2312.02296": {
    "title": "LLMs Accelerate Annotation for Medical Information Extraction",
    "volume": "Dec",
    "abstract": "The unstructured nature of clinical notes within electronic health records often conceals vital patient-related information, making it challenging to access or interpret. To uncover this hidden information, specialized Natural Language Processing (NLP) models are required. However, training these models necessitates large amounts of labeled data, a process that is both time-consuming and costly when relying solely on human experts for annotation. In this paper, we propose an approach that combines Large Language Models (LLMs) with human expertise to create an efficient method for generating ground truth labels for medical text annotation. By utilizing LLMs in conjunction with human annotators, we significantly reduce the human annotation burden, enabling the rapid creation of labeled datasets. We rigorously evaluate our method on a medical information extraction task, demonstrating that our approach not only substantially cuts down on human intervention but also maintains high accuracy. The results highlight the potential of using LLMs to improve the utilization of unstructured clinical data, allowing for the swift deployment of tailored NLP solutions in healthcare",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Goel",
      "Almog Gueta",
      "Omry Gilon",
      "Chang Liu",
      "Sofia Erell",
      "Lan Huong Nguyen",
      "Xiaohong Hao",
      "Bolous Jaber",
      "Shashir Reddy",
      "Rupesh Kartha",
      "Jean Steiner",
      "Itay Laish",
      "Amir Feder"
    ]
  },
  "http://arxiv.org/abs/2312.02149": {
    "title": "Generative Powers of Ten",
    "volume": "Dec",
    "abstract": "We present a method that uses a text-to-image model to generate consistent content across multiple image scales, enabling extreme semantic zooms into a scene, e.g., ranging from a wide-angle landscape view of a forest to a macro shot of an insect sitting on one of the tree branches. We achieve this through a joint multi-scale diffusion sampling approach that encourages consistency across different scales while preserving the integrity of each individual sampling process. Since each generated scale is guided by a different text prompt, our method enables deeper levels of zoom than traditional super-resolution methods that may struggle to create new contextual structure at vastly different scales. We compare our method qualitatively with alternative techniques in image super-resolution and outpainting, and show that our method is most effective at generating consistent multi-scale content",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaojuan Wang",
      "Janne Kontkanen",
      "Brian Curless",
      "Steve Seitz",
      "Ira Kemelmacher",
      "Ben Mildenhall",
      "Pratul Srinivasan",
      "Dor Verbin",
      "Aleksander Holynski"
    ]
  },
  "http://arxiv.org/abs/2312.02143": {
    "title": "Competition-Level Problems are Effective LLM Evaluators",
    "volume": "Dec",
    "abstract": "Large language models (LLMs) have demonstrated impressive reasoning capabilities, yet there is ongoing debate about these abilities and the potential data contamination problem recently. This paper aims to evaluate the reasoning capacities of LLMs, specifically in solving recent competition-level programming problems in Codeforces, which are expert-crafted and unique, requiring deep understanding and robust reasoning skills. We first provide a comprehensive evaluation of GPT-4's peiceived zero-shot performance on this task, considering various aspects such as problems' release time, difficulties, and types of errors encountered. Surprisingly, the peiceived performance of GPT-4 has experienced a cliff like decline in problems after September 2021 consistently across all the difficulties and types of problems, which shows the potential data contamination, as well as the challenges for any existing LLM to solve unseen complex reasoning problems. We further explore various approaches such as fine-tuning, Chain-of-Thought prompting and problem description simplification, unfortunately none of them is able to consistently mitigate the challenges. Through our work, we emphasis the importance of this excellent data source for assessing the genuine reasoning capabilities of LLMs, and foster the development of LLMs with stronger reasoning abilities and better generalization in the future",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Huang",
      "Zhenghao Lin",
      "Xiao Liu",
      "Yeyun Gong",
      "Shuai Lu",
      "Fangyu Lei",
      "Yaobo Liang",
      "Yelong Shen",
      "Chen Lin",
      "Nan Duan",
      "Weizhu Chen"
    ]
  },
  "http://arxiv.org/abs/2312.02125": {
    "title": "TPPoet: Transformer-Based Persian Poem Generation using Minimal Data and Advanced Decoding Techniques",
    "volume": "Dec",
    "abstract": "Recent advances in language models (LMs), have demonstrated significant efficacy in tasks related to the arts and humanities. While LMs have exhibited exceptional performance across a wide range of natural language processing tasks, there are notable challenges associated with their utilization on small datasets and their ability to replicate more creative human capacities. In this study, we aim to address these challenges by training a Persian classical poetry generation model using a transformer architecture on a specialized dataset with no pretraining. Additionally, we propose a novel decoding method to enhance coherence and meaningfulness in the generated poetry, effectively managing the tradeoff between diversity and quality. Furthermore, the results of our training approach and the proposed decoding method are evaluated through comprehensive set of automatic and human evaluations and showed its superior capability to generate coherent and meaningful poetry in compare to other decoding methods and an existing Persian large language model (LLM)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Panahandeh",
      "Hanie Asemi",
      "Esmaeil Nourani"
    ]
  },
  "http://arxiv.org/abs/2312.02120": {
    "title": "Magicoder: Source Code Is All You Need",
    "volume": "Dec",
    "abstract": "We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets to generate high-quality instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs by empowering them with a wealth of open-source references for the production of more diverse, realistic, and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks, including Python text-to-code generation, multilingual coding, and data-science program completion. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1). Overall, OSS-Instruct opens a new direction for low-bias and high-quality instruction tuning using abundant open-source references",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiang Wei",
      "Zhe Wang",
      "Jiawei Liu",
      "Yifeng Ding",
      "Lingming Zhang"
    ]
  },
  "http://arxiv.org/abs/2312.02119": {
    "title": "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically",
    "volume": "Dec",
    "abstract": "While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed jailbreaks. In this work, we present Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM. TAP utilizes an LLM to iteratively refine candidate (attack) prompts using tree-of-thoughts reasoning until one of the generated prompts jailbreaks the target. Crucially, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigate a large search space of prompts and pruning reduces the total number of queries sent to the target. In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo) for more than 80% of the prompts using only a small number of queries. This significantly improves upon the previous state-of-the-art black-box method for generating jailbreaks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anay Mehrotra",
      "Manolis Zampetakis",
      "Paul Kassianik",
      "Blaine Nelson",
      "Hyrum Anderson",
      "Yaron Singer",
      "Amin Karbasi"
    ]
  },
  "http://arxiv.org/abs/2312.02118": {
    "title": "When it Rains, it Pours: Modeling Media Storms and the News Ecosystem",
    "volume": "Dec",
    "abstract": "Most events in the world receive at most brief coverage by the news media. Occasionally, however, an event will trigger a media storm, with voluminous and widespread coverage lasting for weeks instead of days. In this work, we develop and apply a pairwise article similarity model, allowing us to identify story clusters in corpora covering local and national online news, and thereby create a comprehensive corpus of media storms over a nearly two year period. Using this corpus, we investigate media storms at a new level of granularity, allowing us to validate claims about storm evolution and topical distribution, and provide empirical support for previously hypothesized patterns of influence of storms on media coverage and intermedia agenda setting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Litterer",
      "David Jurgens",
      "Dallas Card"
    ]
  },
  "http://arxiv.org/abs/2312.02251": {
    "title": "Fine-Tuning Language Models for Context-Specific SQL Query Generation",
    "volume": "Dec",
    "abstract": "The ability to generate SQL queries from natural language has significant implications for making data accessible to non-specialists. This paper presents a novel approach to fine-tuning open-source large language models (LLMs) for the task of transforming natural language into SQL queries within the retail domain. We introduce models specialized in generating SQL queries, trained on synthetic datasets tailored to the Snowflake SQL and GoogleSQL dialects. Our methodology involves generating a context-specific dataset using GPT-4, then fine-tuning three open-source LLMs(Starcoder Plus, Code-Llama, and Mistral) employing the LoRa technique to optimize for resource constraints. The fine-tuned models demonstrate superior performance in zero-shot settings compared to the baseline GPT-4, with Code-Llama achieving the highest accuracy rates, at 81.58% for Snowflake SQL and 82.66% for GoogleSQL. These results underscore the effectiveness of fine-tuning LLMs on domain-specific tasks and suggest a promising direction for enhancing the accessibility of relational databases through natural language interfaces",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amine Rebei"
    ]
  },
  "http://arxiv.org/abs/2312.02073": {
    "title": "A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia",
    "volume": "Dec",
    "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in storing and recalling factual knowledge, but also in adapting to novel in-context information. Yet, the mechanisms underlying their in-context grounding remain unknown, especially in situations where in-context information contradicts factual knowledge embedded in the parameters. This is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify the outdated parametric knowledge. In this study, we introduce Fakepedia, a counterfactual dataset designed to evaluate grounding abilities when the parametric knowledge clashes with the in-context information. We benchmark various LLMs with Fakepedia and discover that GPT-4-turbo has a strong preference for its parametric knowledge. Mistral-7B, on the contrary, is the model that most robustly chooses the grounded answer. Then, we conduct causal mediation analysis on LLM components when answering Fakepedia queries. We demonstrate that inspection of the computational graph alone can predict LLM grounding with 92.8% accuracy, especially because few MLPs in the Transformer can predict non-grounded behavior. Our results, together with existing findings about factual recall mechanisms, provide a coherent narrative of how grounding and factual recall mechanisms interact within LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giovanni Monea",
      "Maxime Peyrard",
      "Martin Josifoski",
      "Vishrav Chaudhary",
      "Jason Eisner",
      "Emre Kıcıman",
      "Hamid Palangi",
      "Barun Patra",
      "Robert West"
    ]
  },
  "http://arxiv.org/abs/2312.02249": {
    "title": "Recursive Visual Programming",
    "volume": "Dec",
    "abstract": "Visual Programming (VP) has emerged as a powerful framework for Visual Question Answering (VQA). By generating and executing bespoke code for each question, these methods demonstrate impressive compositional and reasoning capabilities, especially in few-shot and zero-shot scenarios. However, existing VP methods generate all code in a single function, resulting in code that is suboptimal in terms of both accuracy and interpretability. Inspired by human coding practices, we propose Recursive Visual Programming (RVP), which simplifies generated routines, provides more efficient problem solving, and can manage more complex data structures. RVP is inspired by human coding practices and approaches VQA tasks with an iterative recursive code generation approach, allowing decomposition of complicated problems into smaller parts. Notably, RVP is capable of dynamic type assignment, i.e., as the system recursively generates a new piece of code, it autonomously determines the appropriate return type and crafts the requisite code to generate that output. We show RVP's efficacy through extensive experiments on benchmarks including VSR, COVR, GQA, and NextQA, underscoring the value of adopting human-like recursive and modular programming techniques for solving VQA tasks through coding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Ge",
      "Sanjay Subramanian",
      "Baifeng Shi",
      "Roei Herzig",
      "Trevor Darrell"
    ]
  },
  "http://arxiv.org/abs/2312.02065": {
    "title": "Know Your Audience: Do LLMs Adapt to Different Age and Education Levels?",
    "volume": "Dec",
    "abstract": "Large language models (LLMs) offer a range of new possibilities, including adapting the text to different audiences and their reading needs. But how well do they adapt? We evaluate the readability of answers generated by four state-of-the-art LLMs (commercial and open-source) to science questions when prompted to target different age groups and education levels. To assess the adaptability of LLMs to diverse audiences, we compare the readability scores of the generated responses against the recommended comprehension level of each age and education group. We find large variations in the readability of the answers by different LLMs. Our results suggest LLM answers need to be better adapted to the intended audience demographics to be more comprehensible. They underline the importance of enhancing the adaptability of LLMs in education settings to cater to diverse age and education levels. Overall, current LLMs have set readability ranges and do not adapt well to different audiences, even when prompted. That limits their potential for educational purposes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donya Rooein",
      "Amanda Cercas Curry",
      "Dirk Hovy"
    ]
  },
  "http://arxiv.org/abs/2312.03755": {
    "title": "Near-real-time Earthquake-induced Fatality Estimation using Crowdsourced Data and Large-Language Models",
    "volume": "Dec",
    "abstract": "When a damaging earthquake occurs, immediate information about casualties is critical for time-sensitive decision-making by emergency response and aid agencies in the first hours and days. Systems such as Prompt Assessment of Global Earthquakes for Response (PAGER) by the U.S. Geological Survey (USGS) were developed to provide a forecast within about 30 minutes of any significant earthquake globally. Traditional systems for estimating human loss in disasters often depend on manually collected early casualty reports from global media, a process that's labor-intensive and slow with notable time delays. Recently, some systems have employed keyword matching and topic modeling to extract relevant information from social media. However, these methods struggle with the complex semantics in multilingual texts and the challenge of interpreting ever-changing, often conflicting reports of death and injury numbers from various unverified sources on social media platforms. In this work, we introduce an end-to-end framework to significantly improve the timeliness and accuracy of global earthquake-induced human loss forecasting using multi-lingual, crowdsourced social media. Our framework integrates (1) a hierarchical casualty extraction model built upon large language models, prompt design, and few-shot learning to retrieve quantitative human loss claims from social media, (2) a physical constraint-aware, dynamic-truth discovery model that discovers the truthful human loss from massive noisy and potentially conflicting human loss claims, and (3) a Bayesian updating loss projection model that dynamically updates the final loss estimation using discovered truths. We test the framework in real-time on a series of global earthquake events in 2021 and 2022 and show that our framework streamlines casualty data retrieval, achieving speed and accuracy comparable to manual methods by USGS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenguang Wang",
      "Davis Engler",
      "Xuechun Li",
      "James Hou",
      "David J. Wald",
      "Kishor Jaiswal",
      "Susu Xu"
    ]
  },
  "http://arxiv.org/abs/2312.02051": {
    "title": "TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding",
    "volume": "Dec",
    "abstract": "This work proposes TimeChat, a time-sensitive multimodal large language model specifically designed for long video understanding. Our model incorporates two key architectural contributions: (1) a timestamp-aware frame encoder that binds visual content with the timestamp of each frame, and (2) a sliding video Q-Former that produces a video token sequence of varying lengths to accommodate videos of various durations. Additionally, we construct an instruction-tuning dataset, encompassing 6 tasks and a total of 125K instances, to further enhance TimeChat's instruction-following performance. Experiment results across various video understanding tasks, such as dense captioning, temporal grounding, and highlight detection, demonstrate TimeChat's strong zero-shot temporal localization and reasoning capabilities. For example, it achieves +9.2 F1 score and +2.8 CIDEr on YouCook2, +5.8 HIT@1 on QVHighlights, and +27.5 R@1 (IoU=0.5) on Charades-STA, compared to state-of-the-art video large language models, holding the potential to serve as a versatile video assistant for long-form video comprehension tasks and satisfy realistic user requirements",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuhuai Ren",
      "Linli Yao",
      "Shicheng Li",
      "Xu Sun",
      "Lu Hou"
    ]
  },
  "http://arxiv.org/abs/2312.01957": {
    "title": "Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian Perspective",
    "volume": "Dec",
    "abstract": "This paper proposes an interpretation of RLAIF as Bayesian inference by introducing distilled Self-Critique (dSC), which refines the outputs of a LLM through a Gibbs sampler that is later distilled into a fine-tuned model. Only requiring synthetic data, dSC is exercised in experiments regarding safety, sentiment, and privacy control, showing it can be a viable and cheap alternative to align LLMs. Code released at \\url{https://github.com/vicgalle/distilled-self-critique}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Victor Gallego"
    ]
  },
  "http://arxiv.org/abs/2312.01954": {
    "title": "Zero- and Few-Shots Knowledge Graph Triplet Extraction with Large Language Models",
    "volume": "Dec",
    "abstract": "In this work, we tested the Triplet Extraction (TE) capabilities of a variety of Large Language Models (LLMs) of different sizes in the Zero- and Few-Shots settings. In detail, we proposed a pipeline that dynamically gathers contextual information from a Knowledge Base (KB), both in the form of context triplets and of (sentence, triplets) pairs as examples, and provides it to the LLM through a prompt. The additional context allowed the LLMs to be competitive with all the older fully trained baselines based on the Bidirectional Long Short-Term Memory (BiLSTM) Network architecture. We further conducted a detailed analysis of the quality of the gathered KB context, finding it to be strongly correlated with the final TE performance of the model. In contrast, the size of the model appeared to only logarithmically improve the TE capabilities of the LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Papaluca",
      "Daniel Krefl",
      "Sergio Mendez Rodriguez",
      "Artem Lensky",
      "Hanna Suominen"
    ]
  },
  "http://arxiv.org/abs/2312.01921": {
    "title": "A Machine Learning Approach Towards SKILL Code Autocompletion",
    "volume": "Dec",
    "abstract": "As Moore's Law continues to increase the complexity of electronic systems, Electronic Design Automation (EDA) must advance to meet global demand. An important example of an EDA technology is SKILL, a scripting language used to customize and extend EDA software. Recently, code generation models using the transformer architecture have achieved impressive results in academic settings and have even been used in commercial developer tools to improve developer productivity. To the best of our knowledge, this study is the first to apply transformers to SKILL code autocompletion towards improving the productivity of hardware design engineers. In this study, a novel, data-efficient methodology for generating SKILL code is proposed and experimentally validated. More specifically, we propose a novel methodology for (i) creating a high-quality SKILL dataset with both unlabeled and labeled data, (ii) a training strategy where T5 models pre-trained on general programming language code are fine-tuned on our custom SKILL dataset using unsupervised and supervised learning, and (iii) evaluating synthesized SKILL code. We show that models trained using the proposed methodology outperform baselines in terms of human-judgment score and BLEU score. A major challenge faced was the extremely small amount of available SKILL code data that can be used to train a transformer model to generate SKILL code. Despite our validated improvements, the extremely small dataset available to us was still not enough to train a model that can reliably autocomplete SKILL code. We discuss this and other limitations as well as future work that could address these limitations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enrique Dehaerne",
      "Bappaditya Dey",
      "Wannes Meert"
    ]
  },
  "http://arxiv.org/abs/2312.01858": {
    "title": "Evaluating Dependencies in Fact Editing for Language Models: Specificity and Implication Awareness",
    "volume": "Dec",
    "abstract": "The potential of using a large language model (LLM) as a knowledge base (KB) has sparked significant interest. To manage the knowledge acquired by LLMs, we need to ensure that the editing of learned facts respects internal logical constraints, which are known as dependency of knowledge. Existing work on editing LLMs has partially addressed the issue of dependency, when the editing of a fact should apply to its lexical variations without disrupting irrelevant ones. However, they neglect the dependency between a fact and its logical implications. We propose an evaluation protocol with an accompanying question-answering dataset, DepEdit, that provides a comprehensive assessment of the editing process considering the above notions of dependency. Our protocol involves setting up a controlled environment in which we edit facts and monitor their impact on LLMs, along with their implications based on If-Then rules. Extensive experiments on DepEdit show that existing knowledge editing methods are sensitive to the surface form of knowledge, and that they have limited performance in inferring the implications of edited facts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichao Li",
      "Ines Arous",
      "Siva Reddy",
      "Jackie C. K. Cheung"
    ]
  },
  "http://arxiv.org/abs/2312.01837": {
    "title": "Prompting Disentangled Embeddings for Knowledge Graph Completion with Pre-trained Language Model",
    "volume": "Dec",
    "abstract": "Both graph structures and textual information play a critical role in Knowledge Graph Completion (KGC). With the success of Pre-trained Language Models (PLMs) such as BERT, they have been applied for text encoding for KGC. However, the current methods mostly prefer to fine-tune PLMs, leading to huge training costs and limited scalability to larger PLMs. In contrast, we propose to utilize prompts and perform KGC on a frozen PLM with only the prompts trained. Accordingly, we propose a new KGC method named PDKGC with two prompts -- a hard task prompt which is to adapt the KGC task to the PLM pre-training task of token prediction, and a disentangled structure prompt which learns disentangled graph representation so as to enable the PLM to combine more relevant structure knowledge with the text information. With the two prompts, PDKGC builds a textual predictor and a structural predictor, respectively, and their combination leads to more comprehensive entity prediction. Solid evaluation on two widely used KGC datasets has shown that PDKGC often outperforms the baselines including the state-of-the-art, and its components are all effective. Our codes and data are available at https://github.com/genggengcss/PDKGC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxia Geng",
      "Jiaoyan Chen",
      "Yuhang Zeng",
      "Zhuo Chen",
      "Wen Zhang",
      "Jeff Z. Pan",
      "Yuxiang Wang",
      "Xiaoliang Xu"
    ]
  },
  "http://arxiv.org/abs/2312.01823": {
    "title": "Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication",
    "volume": "Dec",
    "abstract": "Large Language Models (LLMs) have recently made significant strides in complex reasoning tasks through the Chain-of-Thought technique. Despite this progress, their reasoning is often constrained by their intrinsic understanding, lacking external insights. To address this, we propose Exchange-of-Thought (EoT), a novel framework that enables cross-model communication during problem-solving. Drawing inspiration from network topology, EoT integrates four unique communication paradigms: Memory, Report, Relay, and Debate. This paper delves into the communication dynamics and volume associated with each paradigm. To counterbalance the risks of incorrect reasoning chains, we implement a robust confidence evaluation mechanism within these communications. Our experiments across diverse complex reasoning tasks demonstrate that EoT significantly surpasses established baselines, underscoring the value of external insights in enhancing LLM performance. Furthermore, we show that EoT achieves these superior results in a cost-effective manner, marking a promising advancement for efficient and collaborative AI problem-solving",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangyue Yin",
      "Qiushi Sun",
      "Cheng Chang",
      "Qipeng Guo",
      "Junqi Dai",
      "Xuanjing Huang",
      "Xipeng Qiu"
    ]
  },
  "http://arxiv.org/abs/2312.01787": {
    "title": "Developing Linguistic Patterns to Mitigate Inherent Human Bias in Offensive Language Detection",
    "volume": "Dec",
    "abstract": "With the proliferation of social media, there has been a sharp increase in offensive content, particularly targeting vulnerable groups, exacerbating social problems such as hatred, racism, and sexism. Detecting offensive language use is crucial to prevent offensive language from being widely shared on social media. However, the accurate detection of irony, implication, and various forms of hate speech on social media remains a challenge. Natural language-based deep learning models require extensive training with large, comprehensive, and labeled datasets. Unfortunately, manually creating such datasets is both costly and error-prone. Additionally, the presence of human-bias in offensive language datasets is a major concern for deep learning models. In this paper, we propose a linguistic data augmentation approach to reduce bias in labeling processes, which aims to mitigate the influence of human bias by leveraging the power of machines to improve the accuracy and fairness of labeling processes. This approach has the potential to improve offensive language classification tasks across multiple languages and reduce the prevalence of offensive content on social media",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Toygar Tanyel",
      "Besher Alkurdi",
      "Serkan Ayvaz"
    ]
  },
  "http://arxiv.org/abs/2312.01714": {
    "title": "Retrieval-augmented Multi-modal Chain-of-Thoughts Reasoning for Large Language Models",
    "volume": "Dec",
    "abstract": "The advancement of Large Language Models(LLMs) has brought substantial attention to the Chain of Thought(CoT) approach, primarily due to its ability to enhance the capability of LLMs on tasks requiring complex reasoning. Moreover, the significance of CoT approaches extends to the application of LLMs for multi-modal tasks, such as multi-modal question answering. However, the selection of optimal CoT demonstration examples in multi-modal reasoning for LLMs remains less explored for LLMs due to the inherent complexity of multi-modal examples. In this paper, we introduce a novel approach that addresses this challenge by using retrieval mechanisms to dynamically and automatically select demonstration examples based on cross-modal similarities. This method aims to refine the CoT reasoning process in multi-modal scenarios via informing LLMs with more relevant and informative examples. Furthermore, we employ a stratified sampling method categorising demonstration examples into groups based on their types and retrieving examples from different groups respectively to promote the diversity of demonstration examples. Through a series of experiments, we demonstrate that our approach significantly improves the performance of LLMs, achieving state-of-the-art results in multi-modal reasoning tasks. Specifically, our methods demonstrate significant advancements on the ScienceQA dataset. While our method based on ChatGPT outperforms the Chameleon(ChatGPT) by 2.74% with an accuracy of 82.67%, the GPT4-based approach surpasses the Chameleon(GPT-4) by 0.89%, achieving 87.43% on accuracy under the same setting. Moreover, our best performing show a 6.05% increase over Chameleon for ChatGPT-based models and a 4.57% increase for GPT-4-based models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingshuai Liu",
      "Chenyang Lyu",
      "Zijun Min",
      "Zhanyu Wang",
      "Jinsong Su",
      "Longyue Wang"
    ]
  },
  "http://arxiv.org/abs/2312.01701": {
    "title": "Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites",
    "volume": "Dec",
    "abstract": "Large language models (LLMs) have shown remarkable performance in natural language processing (NLP) tasks. To comprehend and execute diverse human instructions over image data, instruction-tuned large vision-language models (LVLMs) have been introduced. However, LVLMs may suffer from different types of object hallucinations. Nevertheless, LVLMs are evaluated for coarse-grained object hallucinations only (i.e., generated objects non-existent in the input image). The fine-grained object attributes and behaviors non-existent in the image may still be generated but not measured by the current evaluation methods. In this paper, we thus focus on reducing fine-grained hallucinations of LVLMs. We propose \\textit{ReCaption}, a framework that consists of two components: rewriting captions using ChatGPT and fine-tuning the instruction-tuned LVLMs on the rewritten captions. We also propose a fine-grained probing-based evaluation method named \\textit{Fine-Grained Object Hallucination Evaluation} (\\textit{FGHE}). Our experiment results demonstrate that ReCaption effectively reduces fine-grained object hallucination for different LVLM options and improves their text generation quality. The code can be found at https://github.com/Anonymousanoy/FOHE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Wang",
      "Jiabang He",
      "Shenshen Li",
      "Ning Liu",
      "Ee-Peng Lim"
    ]
  },
  "http://arxiv.org/abs/2312.01700": {
    "title": "Data Management For Large Language Models: A Survey",
    "volume": "Dec",
    "abstract": "Data plays a fundamental role in the training of Large Language Models (LLMs). Effective data management, particularly in the formulation of a well-suited training dataset, holds significance for enhancing model performance and improving training efficiency during pretraining and supervised fine-tuning phases. Despite the considerable importance of data management, the current research community still falls short in providing a systematic analysis of the rationale behind management strategy selection, its consequential effects, methodologies for evaluating curated datasets, and the ongoing pursuit of improved strategies. Consequently, the exploration of data management has attracted more and more attention among the research community. This survey provides a comprehensive overview of current research in data management within both the pretraining and supervised fine-tuning stages of LLMs, covering various noteworthy aspects of data management strategy design: data quantity, data quality, domain/task composition, etc. Looking toward the future, we extrapolate existing challenges and outline promising directions for development in this field. Therefore, this survey serves as a guiding resource for practitioners aspiring to construct powerful LLMs through effective data management practices. The collection of the latest papers is available at https://github.com/ZigeW/data_management_LLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zige Wang",
      "Wanjun Zhong",
      "Yufei Wang",
      "Qi Zhu",
      "Fei Mi",
      "Baojun Wang",
      "Lifeng Shang",
      "Xin Jiang",
      "Qun Liu"
    ]
  },
  "http://arxiv.org/abs/2312.01678": {
    "title": "Jellyfish: A Large Language Model for Data Preprocessing",
    "volume": "Dec",
    "abstract": "In this paper, we present Jellyfish, an open-source LLM as a universal task solver for DP. Built on the Llama 2 13B model, Jellyfish is instruction-tuned with the datasets of several typical DP tasks including error detection, data imputation, schema matching, and entity matching, and delivers generalizability to other tasks. Remarkably, Jellyfish can operate on a local, single, and low-priced GPU with its 13 billion parameters, ensuring data security and enabling further tuning. Its proficiency in understanding natural language allows users to manually craft instructions for DP tasks. Unlike many existing methods that heavily rely on prior knowledge, Jellyfish acquires domain knowledge during its tuning process and integrates optional knowledge injection during inference. A distinctive feature of Jellyfish is its interpreter, which elucidates its output decisions. To construct Jellyfish, we develop a series of pre-tuning and DP-tuning techniques. Jellyfish is equipped with an instance serializer, which automatically translates raw data into model prompts, and a knowledge injector, which optionally introduces task- and dataset-specific knowledge to enhance DP performance. Our evaluation of Jellyfish, using a range of real datasets, shows its competitiveness compared to state-of-the-art methods and its strong generalizability to unseen tasks. Jellyfish's performance rivals that of GPT series models, and its interpreter offers enhanced reasoning capabilities compared to GPT-3.5. Furthermore, our evaluation highlights the effectiveness of the techniques employed in constructing Jellyfish. Our model is available at Hugging Face: https://huggingface.co/NECOUDBFM/Jellyfish",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haochen Zhang",
      "Yuyang Dong",
      "Chuan Xiao",
      "Masafumi Oyamada"
    ]
  },
  "http://arxiv.org/abs/2312.01672": {
    "title": "STADEE: STAtistics-based DEEp Detection of Machine Generated Text",
    "volume": "Dec",
    "abstract": "We present STADEE, a \\textbf{STA}tistics-based \\textbf{DEE}p detection method to identify machine-generated text, addressing the limitations of current methods that rely heavily on fine-tuning pre-trained language models (PLMs). STADEE integrates key statistical text features with a deep classifier, focusing on aspects like token probability and cumulative probability, crucial for handling nucleus sampling. Tested across diverse datasets and scenarios (in-domain, out-of-domain, and in-the-wild), STADEE demonstrates superior performance, achieving an 87.05% F1 score in-domain and outperforming both traditional statistical methods and fine-tuned PLMs, especially in out-of-domain and in-the-wild settings, highlighting its effectiveness and generalizability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Chen",
      "Huming Liu"
    ]
  },
  "http://arxiv.org/abs/2312.01661": {
    "title": "ChatGPT as a Math Questioner? Evaluating ChatGPT on Generating Pre-university Math Questions",
    "volume": "Dec",
    "abstract": "Mathematical questioning is crucial for assessing students problem-solving skills. Since manually creating such questions requires substantial effort, automatic methods have been explored. Existing state-of-the-art models rely on fine-tuning strategies and struggle to generate questions that heavily involve multiple steps of logical and arithmetic reasoning. Meanwhile, large language models(LLMs) such as ChatGPT have excelled in many NLP tasks involving logical and arithmetic reasoning. Nonetheless, their applications in generating educational questions are underutilized, especially in the field of mathematics. To bridge this gap, we take the first step to conduct an in-depth analysis of ChatGPT in generating pre-university math questions. Our analysis is categorized into two main settings: context-aware and context-unaware. In the context-aware setting, we evaluate ChatGPT on existing math question-answering benchmarks covering elementary, secondary, and ternary classes. In the context-unaware setting, we evaluate ChatGPT in generating math questions for each lesson from pre-university math curriculums that we crawl. Our crawling results in TopicMath, a comprehensive and novel collection of pre-university math curriculums collected from 121 math topics and 428 lessons from elementary, secondary, and tertiary classes. Through this analysis, we aim to provide insight into the potential of ChatGPT as a math questioner",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Phuoc Pham Van Long",
      "Duc Anh Vu",
      "Nhat M. Hoang",
      "Xuan Long Do",
      "Anh Tuan Luu"
    ]
  },
  "http://arxiv.org/abs/2312.03003": {
    "title": "Explore, Select, Derive, and Recall: Augmenting LLM with Human-like Memory for Mobile Task Automation",
    "volume": "Dec",
    "abstract": "The advent of large language models (LLMs) has opened up new opportunities in the field of mobile task automation. Their superior language understanding and reasoning capabilities allow users to automate complex and repetitive tasks. However, due to the inherent unreliability and high operational cost of LLMs, their practical applicability is quite limited. To address these issues, this paper introduces MemoDroid, an innovative LLM-based mobile task automator enhanced with a unique app memory. MemoDroid emulates the cognitive process of humans interacting with a mobile app -- explore, select, derive, and recall. This approach allows for a more precise and efficient learning of a task's procedure by breaking it down into smaller, modular components that can be re-used, re-arranged, and adapted for various objectives. We implement MemoDroid using online LLMs services (GPT-3.5 and GPT-4) and evaluate its performance on 50 unique mobile tasks across 5 widely used mobile apps. The results indicate that MemoDroid can adapt learned tasks to varying contexts with 100% accuracy and reduces their latency and cost by 69.22% and 77.36% compared to a GPT-4 powered baseline",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunjae Lee",
      "Junyoung Choi",
      "Jungjae Lee",
      "Hojun Choi",
      "Steven Y. Ko",
      "Sangeun Oh",
      "Insik Shin"
    ]
  },
  "http://arxiv.org/abs/2312.01648": {
    "title": "Characterizing Large Language Model Geometry Solves Toxicity Detection and Generation",
    "volume": "Dec",
    "abstract": "Large Language Models~(LLMs) drive current AI breakthroughs despite very little being known about their internal representations, e.g., how to extract a few informative features to solve various downstream tasks. To provide a practical and principled answer, we propose to characterize LLMs from a geometric perspective. We obtain in closed form (i) the intrinsic dimension in which the Multi-Head Attention embeddings are constrained to exist and (ii) the partition and per-region affine mappings of the per-layer feedforward networks. Our results are informative, do not rely on approximations, and are actionable. First, we show that, motivated by our geometric interpretation, we can bypass Llama$2$'s RLHF by controlling its embedding's intrinsic dimension through informed prompt manipulation. Second, we derive $7$ interpretable spline features that can be extracted from any (pre-trained) LLM layer, providing a rich abstract representation of their inputs. Those features alone ($224$ for Mistral-7B and Llama$2$-7B) are sufficient to help solve toxicity detection, infer the domain of the prompt, and even tackle the Jigsaw challenge, which aims at characterizing the type of toxicity of various prompts. Our results demonstrate how, even in large-scale regimes, exact theoretical results can answer practical questions in language models. Code: \\url{https://github.com/RandallBalestriero/SplineLLM}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Randall Balestriero",
      "Romain Cosentino",
      "Sarath Shekkizhar"
    ]
  },
  "http://arxiv.org/abs/2312.01642": {
    "title": "Voice-Based Smart Assistant System for Vehicles using RASA",
    "volume": "Dec",
    "abstract": "Conversational AIs, or chatbots, mimic human speech when conversing. Smart assistants facilitate the automation of several tasks that needed human intervention earlier. Because of their accuracy, absence of dependence on human resources, and accessibility around the clock, chatbots can be employed in vehicles too. Due to people's propensity to divert their attention away from the task of driving while engaging in other activities like calling, playing music, navigation, and getting updates on the weather forecast and latest news, road safety has declined and accidents have increased as a result. It would be advantageous to automate these tasks using voice commands rather than carrying them out manually. This paper focuses on the development of a voice-based smart assistance application for vehicles based on the RASA framework. The smart assistant provides functionalities like navigation, communication via calls, getting weather forecasts and the latest news updates, and music that are completely voice-based in nature",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Paranjape",
      "Yash Patwardhan",
      "Vedant Deshpande",
      "Aniket Darp",
      "Jayashree Jagdale"
    ]
  },
  "http://arxiv.org/abs/2312.01592": {
    "title": "Expand BERT Representation with Visual Information via Grounded Language Learning with Multimodal Partial Alignment",
    "volume": "Dec",
    "abstract": "Language models have been supervised with both language-only objective and visual grounding in existing studies of visual-grounded language learning. However, due to differences in the distribution and scale of visual-grounded datasets and language corpora, the language model tends to mix up the context of the tokens that occurred in the grounded data with those that do not. As a result, during representation learning, there is a mismatch between the visual information and the contextual meaning of the sentence. To overcome this limitation, we propose GroundedBERT - a grounded language learning method that enhances the BERT representation with visually grounded information. GroundedBERT comprises two components: (i) the original BERT which captures the contextual representation of words learned from the language corpora, and (ii) a visual grounding module which captures visual information learned from visual-grounded datasets. Moreover, we employ Optimal Transport (OT), specifically its partial variant, to solve the fractional alignment problem between the two modalities. Our proposed method significantly outperforms the baseline language models on various language tasks of the GLUE and SQuAD datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong-Duy Nguyen",
      "The-Anh Vu-Le",
      "Thong Nguyen",
      "Tho Quan",
      "Luu Anh Tuan"
    ]
  },
  "http://arxiv.org/abs/2312.02227": {
    "title": "Improving Multimodal Sentiment Analysis: Supervised Angular Margin-based Contrastive Learning for Enhanced Fusion Representation",
    "volume": "Dec",
    "abstract": "The effectiveness of a model is heavily reliant on the quality of the fusion representation of multiple modalities in multimodal sentiment analysis. Moreover, each modality is extracted from raw input and integrated with the rest to construct a multimodal representation. Although previous methods have proposed multimodal representations and achieved promising results, most of them focus on forming positive and negative pairs, neglecting the variation in sentiment scores within the same class. Additionally, they fail to capture the significance of unimodal representations in the fusion vector. To address these limitations, we introduce a framework called Supervised Angular-based Contrastive Learning for Multimodal Sentiment Analysis. This framework aims to enhance discrimination and generalizability of the multimodal representation and overcome biases in the fusion vector's modality. Our experimental results, along with visualizations on two widely used datasets, demonstrate the effectiveness of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong-Duy Nguyen",
      "Thong Nguyen",
      "Duc Anh Vu",
      "Luu Anh Tuan"
    ]
  },
  "http://arxiv.org/abs/2312.01582": {
    "title": "Explaining with Contrastive Phrasal Highlighting: A Case Study in Assisting Humans to Detect Translation Differences",
    "volume": "Dec",
    "abstract": "Explainable NLP techniques primarily explain by answering \"Which tokens in the input are responsible for this prediction?''. We argue that for NLP models that make predictions by comparing two input texts, it is more useful to explain by answering \"What differences between the two inputs explain this prediction?''. We introduce a technique to generate contrastive highlights that explain the predictions of a semantic divergence model via phrase-alignment-guided erasure. We show that the resulting highlights match human rationales of cross-lingual semantic differences better than popular post-hoc saliency techniques and that they successfully help people detect fine-grained meaning differences in human translations and critical machine translation errors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eleftheria Briakou",
      "Navita Goyal",
      "Marine Carpuat"
    ]
  },
  "http://arxiv.org/abs/2312.01575": {
    "title": "A Challenging Multimodal Video Summary: Simultaneously Extracting and Generating Keyframe-Caption Pairs from Video",
    "volume": "Dec",
    "abstract": "This paper proposes a practical multimodal video summarization task setting and a dataset to train and evaluate the task. The target task involves summarizing a given video into a predefined number of keyframe-caption pairs and displaying them in a listable format to grasp the video content quickly. This task aims to extract crucial scenes from the video in the form of images (keyframes) and generate corresponding captions explaining each keyframe's situation. This task is useful as a practical application and presents a highly challenging problem worthy of study. Specifically, achieving simultaneous optimization of the keyframe selection performance and caption quality necessitates careful consideration of the mutual dependence on both preceding and subsequent keyframes and captions. To facilitate subsequent research in this field, we also construct a dataset by expanding upon existing datasets and propose an evaluation framework. Furthermore, we develop two baseline systems and report their respective performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keito Kudo",
      "Haruki Nagasawa",
      "Jun Suzuki",
      "Nobuyuki Shimizu"
    ]
  },
  "http://arxiv.org/abs/2312.01564": {
    "title": "APoLLo: Unified Adapter and Prompt Learning for Vision Language Models",
    "volume": "Dec",
    "abstract": "The choice of input text prompt plays a critical role in the performance of Vision-Language Pretrained (VLP) models such as CLIP. We present APoLLo, a unified multi-modal approach that combines Adapter and Prompt learning for Vision-Language models. Our method is designed to substantially improve the generalization capabilities of VLP models when they are fine-tuned in a few-shot setting. We introduce trainable cross-attention-based adapter layers in conjunction with vision and language encoders to strengthen the alignment between the two modalities. We enforce consistency between the respective encoder branches (receiving augmented inputs) to prevent overfitting in downstream tasks. Our method is evaluated on three representative tasks: generalization to novel classes, cross-dataset evaluation, and unseen domain shifts. In practice, APoLLo achieves a relative gain up to 6.03% over MaPLe (SOTA) on novel classes for 10 diverse image recognition datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanjoy Chowdhury",
      "Sayan Nag",
      "Dinesh Manocha"
    ]
  },
  "http://arxiv.org/abs/2312.01552": {
    "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
    "volume": "Dec",
    "abstract": "The alignment tuning process of large language models (LLMs) typically involves instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning from human feedback (RLHF). A recent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for SFT can achieve significant alignment performance as well, suggesting that the effect of alignment tuning might be \"superficial.\" This raises questions about how exactly the alignment tuning transforms a base LLM. We analyze the effect of alignment tuning by examining the token distribution shift between base LLMs and their aligned counterpart. Our findings reveal that base LLMs and their alignment-tuned versions perform nearly identically in decoding on the majority of token positions. Most distribution shifts occur with stylistic tokens. These direct evidence strongly supports the Superficial Alignment Hypothesis suggested by LIMA. Based on these findings, we rethink the alignment of LLMs by posing the research question: how effectively can we align base LLMs without SFT or RLHF? To address this, we introduce a simple, tuning-free alignment method, URIAL. URIAL achieves effective alignment purely through in-context learning (ICL) with base LLMs, requiring as few as three constant stylistic examples and a system prompt. We conduct a fine-grained and interpretable evaluation on a diverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that base LLMs with URIAL can match or even surpass the performance of LLMs aligned with SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based alignment methods can be significantly reduced through strategic prompting and ICL. Our findings on the superficial nature of alignment tuning and results with URIAL suggest that deeper analysis and theoretical understanding of alignment is crucial to future LLM research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bill Yuchen Lin",
      "Abhilasha Ravichander",
      "Ximing Lu",
      "Nouha Dziri",
      "Melanie Sclar",
      "Khyathi Chandu",
      "Chandra Bhagavatula",
      "Yejin Choi"
    ]
  },
  "http://arxiv.org/abs/2312.01532": {
    "title": "Using Large Language Models to Accelerate Communication for Users with Severe Motor Impairments",
    "volume": "Dec",
    "abstract": "Finding ways to accelerate text input for individuals with profound motor impairments has been a long-standing area of research. Closing the speed gap for augmentative and alternative communication (AAC) devices such as eye-tracking keyboards is important for improving the quality of life for such individuals. Recent advances in neural networks of natural language pose new opportunities for re-thinking strategies and user interfaces for enhanced text-entry for AAC users. In this paper, we present SpeakFaster, consisting of large language models (LLMs) and a co-designed user interface for text entry in a highly-abbreviated form, allowing saving 57% more motor actions than traditional predictive keyboards in offline simulation. A pilot study with 19 non-AAC participants typing on a mobile device by hand demonstrated gains in motor savings in line with the offline simulation, while introducing relatively small effects on overall typing speed. Lab and field testing on two eye-gaze typing users with amyotrophic lateral sclerosis (ALS) demonstrated text-entry rates 29-60% faster than traditional baselines, due to significant saving of expensive keystrokes achieved through phrase and word predictions from context-aware LLMs. These findings provide a strong foundation for further exploration of substantially-accelerated text communication for motor-impaired users and demonstrate a direction for applying LLMs to text-based user interfaces",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shanqing Cai",
      "Subhashini Venugopalan",
      "Katie Seaver",
      "Xiang Xiao",
      "Katrin Tomanek",
      "Sri Jalasutram",
      "Meredith Ringel Morris",
      "Shaun Kane",
      "Ajit Narayanan",
      "Robert L. MacDonald",
      "Emily Kornman",
      "Daniel Vance",
      "Blair Casey",
      "Steve M. Gleason",
      "Philip Q. Nelson",
      "Michael P. Brenner"
    ]
  },
  "http://arxiv.org/abs/2312.01529": {
    "title": "T3D: Towards 3D Medical Image Understanding through Vision-Language Pre-training",
    "volume": "Dec",
    "abstract": "Expert annotation of 3D medical image for downstream analysis is resource-intensive, posing challenges in clinical applications. Visual self-supervised learning (vSSL), though effective for learning visual invariance, neglects the incorporation of domain knowledge from medicine. To incorporate medical knowledge into visual representation learning, vision-language pre-training (VLP) has shown promising results in 2D image. However, existing VLP approaches become generally impractical when applied to high-resolution 3D medical images due to GPU hardware constraints and the potential loss of critical details caused by downsampling, which is the intuitive solution to hardware constraints. To address the above limitations, we introduce T3D, the first VLP framework designed for high-resolution 3D medical images. T3D incorporates two text-informed pretext tasks: (\\lowerromannumeral{1}) text-informed contrastive learning; (\\lowerromannumeral{2}) text-informed image restoration. These tasks focus on learning 3D visual representations from high-resolution 3D medical images and integrating clinical knowledge from radiology reports, without distorting information through forced alignment of downsampled volumes with detailed anatomical text. Trained on a newly curated large-scale dataset of 3D medical images and radiology reports, T3D significantly outperforms current vSSL methods in tasks like organ and tumor segmentation, as well as disease classification. This underlines T3D's potential in representation learning for 3D medical image analysis. All data and code will be available upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Che Liu",
      "Cheng Ouyang",
      "Yinda Chen",
      "Cesar César Quilodrán-Casas",
      "Lei Ma",
      "Jie Fu",
      "Yike Guo",
      "Anand Shah",
      "Wenjia Bai",
      "Rossella Arcucci"
    ]
  },
  "http://arxiv.org/abs/2312.01523": {
    "title": "SymNoise: Advancing Language Model Fine-tuning with Symmetric Noise",
    "volume": "Dec",
    "abstract": "In this paper, we introduce a novel fine-tuning technique for language models, which involves incorporating symmetric noise into the embedding process. This method aims to enhance the model's function by more stringently regulating its local curvature, demonstrating superior performance over the current method, NEFTune. When fine-tuning the LLaMA-2-7B model using Alpaca, standard techniques yield a 29.79% score on AlpacaEval. However, our approach, SymNoise, increases this score significantly to 69.04%, using symmetric noisy embeddings. This is a 6.7% improvement over the state-of-the-art method, NEFTune~(64.69%). Furthermore, when tested on various models and stronger baseline instruction datasets, such as Evol-Instruct, ShareGPT, OpenPlatypus, SymNoise consistently outperforms NEFTune. The current literature, including NEFTune, has underscored the importance of more in-depth research into the application of noise-based strategies in the fine-tuning of language models. Our approach, SymNoise, is another significant step towards this direction, showing notable improvement over the existing state-of-the-art method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arjun Singh",
      "Abhay Kumar Yadav"
    ]
  },
  "http://arxiv.org/abs/2312.01515": {
    "title": "Bigger is not Always Better: The Effect of Context Size on Speech Pre-Training",
    "volume": "Dec",
    "abstract": "It has been generally assumed in the automatic speech recognition (ASR) literature that it is better for models to have access to wider context windows. Yet, many of the potential reasons this might be true in the supervised setting do not necessarily transfer over to the case of unsupervised learning. We investigate how much context is necessary to achieve high-quality pre-trained acoustic models using self-supervised learning. We principally investigate contrastive predictive coding (CPC), which we adapt to be able to precisely control the amount of context visible to the model during training and inference. We find that phone discriminability in the resulting model representations peaks at around 40~ms of preceding context, and that having too much context (beyond around 320 ms) substantially degrades the quality of the representations. Surprisingly, we find that this pattern also transfers to supervised ASR when the pre-trained representations are used as frozen input features. Our results point to potential changes in the design of current upstream architectures to better facilitate a variety of downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sean Robertson",
      "Ewan Dunbar"
    ]
  },
  "http://arxiv.org/abs/2312.01509": {
    "title": "Tackling Bias in Pre-trained Language Models: Current Trends and Under-represented Societies",
    "volume": "Dec",
    "abstract": "The benefits and capabilities of pre-trained language models (LLMs) in current and future innovations are vital to any society. However, introducing and using LLMs comes with biases and discrimination, resulting in concerns about equality, diversity and fairness, and must be addressed. While understanding and acknowledging bias in LLMs and developing mitigation strategies are crucial, the generalised assumptions towards societal needs can result in disadvantages towards under-represented societies and indigenous populations. Furthermore, the ongoing changes to actual and proposed amendments to regulations and laws worldwide also impact research capabilities in tackling the bias problem. This research presents a comprehensive survey synthesising the current trends and limitations in techniques used for identifying and mitigating bias in LLMs, where the overview of methods for tackling bias are grouped into metrics, benchmark datasets, and mitigation strategies. The importance and novelty of this survey are that it explores the perspective of under-represented societies. We argue that current practices tackling the bias problem cannot simply be 'plugged in' to address the needs of under-represented societies. We use examples from New Zealand to present requirements for adopting existing techniques to under-represented societies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vithya Yogarajan",
      "Gillian Dobbie",
      "Te Taka Keegan",
      "Rostam J. Neuwirth"
    ]
  },
  "http://arxiv.org/abs/2312.01504": {
    "title": "Effectively Fine-tune to Improve Large Multimodal Models for Radiology Report Generation",
    "volume": "Dec",
    "abstract": "Writing radiology reports from medical images requires a high level of domain expertise. It is time-consuming even for trained radiologists and can be error-prone for inexperienced radiologists. It would be appealing to automate this task by leveraging generative AI, which has shown drastic progress in vision and language understanding. In particular, Large Language Models (LLM) have demonstrated impressive capabilities recently and continued to set new state-of-the-art performance on almost all natural language tasks. While many have proposed architectures to combine vision models with LLMs for multimodal tasks, few have explored practical fine-tuning strategies. In this work, we proposed a simple yet effective two-stage fine-tuning protocol to align visual features to LLM's text embedding space as soft visual prompts. Our framework with OpenLLaMA-7B achieved state-of-the-art level performance without domain-specific pretraining. Moreover, we provide detailed analyses of soft visual prompts and attention mechanisms, shedding light on future research directions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhe Lu",
      "Sungmin Hong",
      "Yash Shah",
      "Panpan Xu"
    ]
  },
  "http://arxiv.org/abs/2312.01500": {
    "title": "Unsupervised Approach to Evaluate Sentence-Level Fluency: Do We Really Need Reference?",
    "volume": "Dec",
    "abstract": "Fluency is a crucial goal of all Natural Language Generation (NLG) systems. Widely used automatic evaluation metrics fall short in capturing the fluency of machine-generated text. Assessing the fluency of NLG systems poses a challenge since these models are not limited to simply reusing words from the input but may also generate abstractions. Existing reference-based fluency evaluations, such as word overlap measures, often exhibit weak correlations with human judgments. This paper adapts an existing unsupervised technique for measuring text fluency without the need for any reference. Our approach leverages various word embeddings and trains language models using Recurrent Neural Network (RNN) architectures. We also experiment with other available multilingual Language Models (LMs). To assess the performance of the models, we conduct a comparative analysis across 10 Indic languages, correlating the obtained fluency scores with human judgments. Our code and human-annotated benchmark test-set for fluency is available at https://github.com/AnanyaCoder/TextFluencyForIndicLanaguges",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gopichand Kanumolu",
      "Lokesh Madasu",
      "Pavan Baswani",
      "Ananya Mukherjee",
      "Manish Shrivastava"
    ]
  },
  "http://arxiv.org/abs/2312.01454": {
    "title": "D-Bot: Database Diagnosis System using Large Language Models",
    "volume": "Dec",
    "abstract": "Database administrators (DBAs) play an important role in managing, maintaining and optimizing database systems. However, it is hard and tedious for DBAs to manage a large number of databases and give timely response (waiting for hours is intolerable in many online cases). In addition, existing empirical methods only support limited diagnosis scenarios, which are also labor-intensive to update the diagnosis rules for database version updates. Recently large language models (LLMs) have shown great potential in various fields. Thus, we propose D-Bot, an LLM-based database diagnosis system that can automatically acquire knowledge from diagnosis documents, and generate reasonable and well-founded diagnosis report (i.e., identifying the root causes and solutions) within acceptable time (e.g., under 10 minutes compared to hours by a DBA). The techniques in D-Bot include (i) offline knowledge extraction from documents, (ii) automatic prompt generation (e.g., knowledge matching, tool retrieval), (iii) root cause analysis using tree search algorithm, and (iv) collaborative mechanism for complex anomalies with multiple root causes. We verify D-Bot on real benchmarks (including 539 anomalies of six typical applications), and the results show that D-Bot can effectively analyze the root causes of unseen anomalies and significantly outperforms traditional methods and vanilla models like GPT-4",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanhe Zhou",
      "Guoliang Li",
      "Zhaoyan Sun",
      "Zhiyuan Liu",
      "Weize Chen",
      "Jianming Wu",
      "Jiesi Liu",
      "Ruohang Feng",
      "Guoyang Zeng"
    ]
  },
  "http://arxiv.org/abs/2312.02219": {
    "title": "Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large Image-Language Models",
    "volume": "Dec",
    "abstract": "Large Vision and Language Models have enabled significant advances in fully supervised and zero-shot vision tasks. These large pre-trained architectures serve as the baseline to what is currently known as Instruction Tuning Large Vision and Language models (IT-LVLMs). IT-LVLMs are general-purpose multi-modal assistants whose responses are modulated by natural language instructions and arbitrary visual data. Despite this versatility, IT-LVLM effectiveness in fundamental computer vision problems remains unclear, primarily due to the absence of a standardized evaluation benchmark. This paper introduces a Multi-modal Evaluation Benchmark named MERLIM, a scalable test-bed to assess the performance of IT-LVLMs on fundamental computer vision tasks. MERLIM contains over 279K image-question pairs, and has a strong focus on detecting cross-modal \"hallucination\" events in IT-LVLMs, where the language output refers to visual concepts that lack any effective grounding in the image. Our results show that state-of-the-art IT-LVMLs are still limited at identifying fine-grained visual concepts, object hallucinations are common across tasks, and their results are strongly biased by small variations in the input query, even if the queries have the very same semantics. Our findings also suggest that these models have weak visual groundings but they can still make adequate guesses by global visual patterns or textual biases contained in the LLM component",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrés Villa",
      "Juan Carlos León Alcázar",
      "Alvaro Soto",
      "Bernard Ghanem"
    ]
  },
  "http://arxiv.org/abs/2312.01429": {
    "title": "Transformers are uninterpretable with myopic methods: a case study with bounded Dyck grammars",
    "volume": "Dec",
    "abstract": "Interpretability methods aim to understand the algorithm implemented by a trained model (e.g., a Transofmer) by examining various aspects of the model, such as the weight matrices or the attention patterns. In this work, through a combination of theoretical results and carefully controlled experiments on synthetic data, we take a critical view of methods that exclusively focus on individual parts of the model, rather than consider the network as a whole. We consider a simple synthetic setup of learning a (bounded) Dyck language. Theoretically, we show that the set of models that (exactly or approximately) solve this task satisfy a structural characterization derived from ideas in formal languages (the pumping lemma). We use this characterization to show that the set of optima is qualitatively rich; in particular, the attention pattern of a single layer can be ``nearly randomized'', while preserving the functionality of the network. We also show via extensive experiments that these constructions are not merely a theoretical artifact: even after severely constraining the architecture of the model, vastly different solutions can be reached via standard training. Thus, interpretability claims based on inspecting individual heads or weight matrices in the Transformer can be misleading",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiyue Wen",
      "Yuchen Li",
      "Bingbin Liu",
      "Andrej Risteski"
    ]
  },
  "http://arxiv.org/abs/2312.01398": {
    "title": "Towards Mitigating Perceived Unfairness in Contracts from a Non-Legal Stakeholder's Perspective",
    "volume": "Dec",
    "abstract": "Commercial contracts are known to be a valuable source for deriving project-specific requirements. However, contract negotiations mainly occur among the legal counsel of the parties involved. The participation of non-legal stakeholders, including requirement analysts, engineers, and solution architects, whose primary responsibility lies in ensuring the seamless implementation of contractual terms, is often indirect and inadequate. Consequently, a significant number of sentences in contractual clauses, though legally accurate, can appear unfair from an implementation perspective to non-legal stakeholders. This perception poses a problem since requirements indicated in the clauses are obligatory and can involve punitive measures and penalties if not implemented as committed in the contract. Therefore, the identification of potentially unfair clauses in contracts becomes crucial. In this work, we conduct an empirical study to analyze the perspectives of different stakeholders regarding contractual fairness. We then investigate the ability of Pre-trained Language Models (PLMs) to identify unfairness in contractual sentences by comparing chain of thought prompting and semi-supervised fine-tuning approaches. Using BERT-based fine-tuning, we achieved an accuracy of 84% on a dataset consisting of proprietary contracts. It outperformed chain of thought prompting using Vicuna-13B by a margin of 9%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anmol Singhal",
      "Preethu Rose Anish",
      "Shirish Karande",
      "Smita Ghaisas"
    ]
  },
  "http://arxiv.org/abs/2312.01356": {
    "title": "CEScore: Simple and Efficient Confidence Estimation Model for Evaluating Split and Rephrase",
    "volume": "Dec",
    "abstract": "The split and rephrase (SR) task aims to divide a long, complex sentence into a set of shorter, simpler sentences that convey the same meaning. This challenging problem in NLP has gained increased attention recently because of its benefits as a pre-processing step in other NLP tasks. Evaluating quality of SR is challenging, as there no automatic metric fit to evaluate this task. In this work, we introduce CEScore, as novel statistical model to automatically evaluate SR task. By mimicking the way humans evaluate SR, CEScore provides 4 metrics (Sscore, Gscore, Mscore, and CEscore) to assess simplicity, grammaticality, meaning preservation, and overall quality, respectively. In experiments with 26 models, CEScore correlates strongly with human evaluations, achieving 0.98 in Spearman correlations at model-level. This underscores the potential of CEScore as a simple and effective metric for assessing the overall quality of SR models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "AlMotasem Bellah Al Ajlouni",
      "Jinlong Li"
    ]
  },
  "http://arxiv.org/abs/2312.01339": {
    "title": "AI-Powered Arabic Crossword Puzzle Generation for Educational Applications",
    "volume": "Dec",
    "abstract": "This paper presents the first Arabic crossword puzzle generator driven by advanced AI technology. Leveraging cutting-edge large language models including GPT4, GPT3-Davinci, GPT3-Curie, GPT3-Babbage, GPT3-Ada, and BERT, the system generates distinctive and challenging clues. Based on a dataset comprising over 50,000 clue-answer pairs, the generator employs fine-tuning, few/zero-shot learning strategies, and rigorous quality-checking protocols to enforce the generation of high-quality clue-answer pairs. Importantly, educational crosswords contribute to enhancing memory, expanding vocabulary, and promoting problem-solving skills, thereby augmenting the learning experience through a fun and engaging approach, reshaping the landscape of traditional learning methods. The overall system can be exploited as a powerful educational tool that amalgamates AI and innovative learning techniques, heralding a transformative era for Arabic crossword puzzles and the intersection of technology and education",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kamyar Zeinalipour",
      "Mohamed Zaky Saad",
      "Marco Maggini",
      "Marco Gori"
    ]
  },
  "http://arxiv.org/abs/2312.01314": {
    "title": "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian",
    "volume": "Dec",
    "abstract": "Recent advancements in Generative Language Models (GLMs) have transformed Natural Language Processing (NLP) by showcasing the effectiveness of the \"pre-train, prompt, and predict\" paradigm in utilizing pre-trained GLM knowledge for diverse applications. Despite their potential, these capabilities lack adequate quantitative characterization due to the absence of comprehensive benchmarks, particularly for low-resource languages. Existing low-resource benchmarks focus on discriminative language models like BERT, neglecting the evaluation of generative language models. Moreover, current benchmarks often overlook measuring generalization performance across multiple tasks, a crucial metric for GLMs. To bridge these gaps, we introduce NLEBench, a comprehensive benchmark tailored for evaluating natural language generation capabilities in Norwegian, a low-resource language. We use Norwegian as a case study to explore whether current GLMs and benchmarks in mainstream languages like English can reveal the unique characteristics of underrepresented languages. NLEBench encompasses a suite of real-world NLP tasks ranging from news storytelling, summarization, open-domain conversation, natural language understanding, instruction fine-tuning, toxicity and bias evaluation, to self-curated Chain-of-Thought investigation. It features two high-quality, human-annotated datasets: an instruction dataset covering traditional Norwegian cultures, idioms, slang, and special expressions, and a document-grounded multi-label dataset for topic classification, question answering, and summarization. This paper also introduces foundational Norwegian Generative Language Models (NorGLMs) developed with diverse parameter scales and Transformer-based architectures. Systematic evaluations on the proposed benchmark suite provide insights into the capabilities and scalability of NorGLMs across various downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Liu",
      "Lemei Zhang",
      "Terje Nissen Farup",
      "Even W. Lauvrak",
      "Jon Espen Ingvaldsen",
      "Simen Eide",
      "Jon Atle Gulla",
      "Zhirong Yang"
    ]
  },
  "http://arxiv.org/abs/2312.01308": {
    "title": "Bridging Background Knowledge Gaps in Translation with Automatic Explicitation",
    "volume": "Dec",
    "abstract": "Translations help people understand content written in another language. However, even correct literal translations do not fulfill that goal when people lack the necessary background to understand them. Professional translators incorporate explicitations to explain the missing context by considering cultural differences between source and target audiences. Despite its potential to help users, NLP research on explicitation is limited because of the dearth of adequate evaluation methods. This work introduces techniques for automatically generating explicitations, motivated by WikiExpl: a dataset that we collect from Wikipedia and annotate with human translators. The resulting explicitations are useful as they help answer questions more accurately in a multilingual question answering framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "HyoJung Han",
      "Jordan Lee Boyd-Graber",
      "Marine Carpuat"
    ]
  },
  "http://arxiv.org/abs/2312.01306": {
    "title": "On Significance of Subword tokenization for Low Resource and Efficient Named Entity Recognition: A case study in Marathi",
    "volume": "Dec",
    "abstract": "Named Entity Recognition (NER) systems play a vital role in NLP applications such as machine translation, summarization, and question-answering. These systems identify named entities, which encompass real-world concepts like locations, persons, and organizations. Despite extensive research on NER systems for the English language, they have not received adequate attention in the context of low resource languages. In this work, we focus on NER for low-resource language and present our case study in the context of the Indian language Marathi. The advancement of NLP research revolves around the utilization of pre-trained transformer models such as BERT for the development of NER models. However, we focus on improving the performance of shallow models based on CNN, and LSTM by combining the best of both worlds. In the era of transformers, these traditional deep learning models are still relevant because of their high computational efficiency. We propose a hybrid approach for efficient NER by integrating a BERT-based subword tokenizer into vanilla CNN/LSTM models. We show that this simple approach of replacing a traditional word-based tokenizer with a BERT-tokenizer brings the accuracy of vanilla single-layer models closer to that of deep pre-trained models like BERT. We show the importance of using sub-word tokenization for NER and present our study toward building efficient NLP systems. The evaluation is performed on L3Cube-MahaNER dataset using tokenizers from MahaBERT, MahaGPT, IndicBERT, and mBERT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harsh Chaudhari",
      "Anuja Patil",
      "Dhanashree Lavekar",
      "Pranav Khairnar",
      "Raviraj Joshi",
      "Sachin Pande"
    ]
  },
  "http://arxiv.org/abs/2312.01279": {
    "title": "TextGenSHAP: Scalable Post-hoc Explanations in Text Generation with Long Documents",
    "volume": "Dec",
    "abstract": "Large language models (LLMs) have attracted huge interest in practical applications given their increasingly accurate responses and coherent reasoning abilities. Given their nature as black-boxes using complex reasoning processes on their inputs, it is inevitable that the demand for scalable and faithful explanations for LLMs' generated content will continue to grow. There have been major developments in the explainability of neural network models over the past decade. Among them, post-hoc explainability methods, especially Shapley values, have proven effective for interpreting deep learning models. However, there are major challenges in scaling up Shapley values for LLMs, particularly when dealing with long input contexts containing thousands of tokens and autoregressively generated output sequences. Furthermore, it is often unclear how to effectively utilize generated explanations to improve the performance of LLMs. In this paper, we introduce TextGenSHAP, an efficient post-hoc explanation method incorporating LM-specific techniques. We demonstrate that this leads to significant increases in speed compared to conventional Shapley value computations, reducing processing times from hours to minutes for token-level explanations, and to just seconds for document-level explanations. In addition, we demonstrate how real-time Shapley values can be utilized in two important scenarios, providing better understanding of long-document question answering by localizing important words and sentences; and improving existing document retrieval systems through enhancing the accuracy of selected passages and ultimately the final responses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Enouen",
      "Hootan Nakhost",
      "Sayna Ebrahimi",
      "Sercan O Arik",
      "Yan Liu",
      "Tomas Pfister"
    ]
  },
  "http://arxiv.org/abs/2312.01276": {
    "title": "Running cognitive evaluations on large language models: The do's and the don'ts",
    "volume": "Dec",
    "abstract": "In this paper, I describe methodological considerations for studies that aim to evaluate the cognitive capacities of large language models (LLMs) using language-based behavioral assessments. Drawing on three case studies from the literature (a commonsense knowledge benchmark, a theory of mind evaluation, and a test of syntactic agreement), I describe common pitfalls that might arise when applying a cognitive test to an LLM. I then list 10 do's and don'ts that should help design high-quality cognitive evaluations for AI systems. I conclude by discussing four areas where the do's and don'ts are currently under active discussion -- prompt sensitivity, cultural and linguistic diversity, using LLMs as research assistants, and running evaluations on open vs. closed LLMs. Overall, the goal of the paper is to contribute to the broader discussion of best practices in the rapidly growing field of AI Psychology",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anna A. Ivanova"
    ]
  },
  "http://arxiv.org/abs/2312.02206": {
    "title": "Axiomatic Preference Modeling for Longform Question Answering",
    "volume": "Dec",
    "abstract": "The remarkable abilities of large language models (LLMs) like GPT-4 partially stem from post-training processes like Reinforcement Learning from Human Feedback (RLHF) involving human preferences encoded in a reward model. However, these reward models (RMs) often lack direct knowledge of why, or under what principles, the preferences annotations were made. In this study, we identify principles that guide RMs to better align with human preferences, and then develop an axiomatic framework to generate a rich variety of preference signals to uphold them. We use these axiomatic signals to train a model for scoring answers to longform questions. Our approach yields a Preference Model with only about 220M parameters that agrees with gold human-annotated preference labels more often than GPT-4. The contributions of this work include: training a standalone preference model that can score human- and LLM-generated answers on the same scale; developing an axiomatic framework for generating training data pairs tailored to certain principles; and showing that a small amount of axiomatic signals can help small models outperform GPT-4 in preference scoring. We release our model on huggingface: https://huggingface.co/corbyrosset/axiomatic_preference_model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Corby Rosset",
      "Guoqing Zheng",
      "Victor Dibia",
      "Ahmed Awadallah",
      "Paul Bennett"
    ]
  },
  "http://arxiv.org/abs/2312.01244": {
    "title": "Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2023): Workshop and Shared Task Report",
    "volume": "Dec",
    "abstract": "We provide a summary of the sixth edition of the CASE workshop that is held in the scope of RANLP 2023. The workshop consists of regular papers, three keynotes, working papers of shared task participants, and shared task overview papers. This workshop series has been bringing together all aspects of event information collection across technical and social science fields. In addition to contributing to the progress in text based event extraction, the workshop provides a space for the organization of a multimodal event information collection task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Hürriyetoğlu",
      "Hristo Tanev",
      "Osman Mutlu",
      "Surendrabikram Thapa",
      "Fiona Anting Tan",
      "Erdem Yörük"
    ]
  },
  "http://arxiv.org/abs/2312.01225": {
    "title": "UCE-FID: Using Large Unlabeled, Medium Crowdsourced-Labeled, and Small Expert-Labeled Tweets for Foodborne Illness Detection",
    "volume": "Dec",
    "abstract": "Foodborne illnesses significantly impact public health. Deep learning surveillance applications using social media data aim to detect early warning signals. However, labeling foodborne illness-related tweets for model training requires extensive human resources, making it challenging to collect a sufficient number of high-quality labels for tweets within a limited budget. The severe class imbalance resulting from the scarcity of foodborne illness-related tweets among the vast volume of social media further exacerbates the problem. Classifiers trained on a class-imbalanced dataset are biased towards the majority class, making accurate detection difficult. To overcome these challenges, we propose EGAL, a deep learning framework for foodborne illness detection that uses small expert-labeled tweets augmented by crowdsourced-labeled and massive unlabeled data. Specifically, by leveraging tweets labeled by experts as a reward set, EGAL learns to assign a weight of zero to incorrectly labeled tweets to mitigate their negative influence. Other tweets receive proportionate weights to counter-balance the unbalanced class distribution. Extensive experiments on real-world \\textit{TWEET-FID} data show that EGAL outperforms strong baseline models across different settings, including varying expert-labeled set sizes and class imbalance ratios. A case study on a multistate outbreak of Salmonella Typhimurium infection linked to packaged salad greens demonstrates how the trained model captures relevant tweets offering valuable outbreak insights. EGAL, funded by the U.S. Department of Agriculture (USDA), has the potential to be deployed for real-time analysis of tweet streaming, contributing to foodborne illness outbreak surveillance efforts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruofan Hu",
      "Dongyu Zhang",
      "Dandan Tao",
      "Huayi Zhang",
      "Hao Feng",
      "Elke Rundensteiner"
    ]
  },
  "http://arxiv.org/abs/2312.03753": {
    "title": "English to Arabic machine translation of mathematical documents",
    "volume": "Dec",
    "abstract": "This paper is about the development of a machine translation system tailored specifically for LATEX mathematical documents. The system focuses on translating English LATEX mathematical documents into Arabic LATEX, catering to the growing demand for multilingual accessibility in scientific and mathematical literature. With the vast proliferation of LATEX mathematical documents the need for an efficient and accurate translation system has become increasingly essential. This paper addresses the necessity for a robust translation tool that enables seamless communication and comprehension of complex mathematical content across language barriers. The proposed system leverages a Transformer model as the core of the translation system, ensuring enhanced accuracy and fluency in the translated Arabic LATEX documents. Furthermore, the integration of RyDArab, an Arabic mathematical TEX extension, along with a rule-based translator for Arabic mathematical expressions, contributes to the precise rendering of complex mathematical symbols and equations in the translated output. The paper discusses the architecture, methodology, of the developed system, highlighting its efficacy in bridging the language gap in the domain of mathematical documentation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mustapha Eddahibi",
      "Mohammed Mensouri"
    ]
  },
  "http://arxiv.org/abs/2312.03752": {
    "title": "Automatic Scoring of Students' Science Writing Using Hybrid Neural Network",
    "volume": "Dec",
    "abstract": "This study explores the efficacy of a multi-perspective hybrid neural network (HNN) for scoring student responses in science education with an analytic rubric. We compared the accuracy of the HNN model with four ML approaches (BERT, AACR, Naive Bayes, and Logistic Regression). The results have shown that HHN achieved 8%, 3%, 1%, and 0.12% higher accuracy than Naive Bayes, Logistic Regression, AACR, and BERT, respectively, for five scoring aspects (p<0.001). The overall HNN's perceived accuracy (M = 96.23%, SD = 1.45%) is comparable to the (training and inference) expensive BERT model's accuracy (M = 96.12%, SD = 1.52%). We also have observed that HNN is x2 more efficient in training and inferencing than BERT and has comparable efficiency to the lightweight but less accurate Naive Bayes model. Our study confirmed the accuracy and efficiency of using HNN to score students' science writing automatically",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ehsan Latif",
      "Xiaoming Zhai"
    ]
  },
  "http://arxiv.org/abs/2312.01221": {
    "title": "Enabling Quantum Natural Language Processing for Hindi Language",
    "volume": "Dec",
    "abstract": "Quantum Natural Language Processing (QNLP) is taking huge leaps in solving the shortcomings of classical Natural Language Processing (NLP) techniques and moving towards a more \"Explainable\" NLP system. The current literature around QNLP focuses primarily on implementing QNLP techniques in sentences in the English language. In this paper, we propose to enable the QNLP approach to HINDI, which is the third most spoken language in South Asia. We present the process of building the parameterized quantum circuits required to undertake QNLP on Hindi sentences. We use the pregroup representation of Hindi and the DisCoCat framework to draw sentence diagrams. Later, we translate these diagrams to Parameterised Quantum Circuits based on Instantaneous Quantum Polynomial (IQP) style ansatz. Using these parameterized quantum circuits allows one to train grammar and topic-aware sentence classifiers for the Hindi Language",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naman Srivastava",
      "Gaurang Belekar",
      "Sunil Saumya",
      "Aswath Babu H"
    ]
  },
  "http://arxiv.org/abs/2312.01217": {
    "title": "Understanding Opinions Towards Climate Change on Social Media",
    "volume": "Dec",
    "abstract": "Social media platforms such as Twitter (now known as X) have revolutionized how the public engage with important societal and political topics. Recently, climate change discussions on social media became a catalyst for political polarization and the spreading of misinformation. In this work, we aim to understand how real world events influence the opinions of individuals towards climate change related topics on social media. To this end, we extracted and analyzed a dataset of 13.6 millions tweets sent by 3.6 million users from 2006 to 2019. Then, we construct a temporal graph from the user-user mentions network and utilize the Louvain community detection algorithm to analyze the changes in community structure around Conference of the Parties on Climate Change~(COP) events. Next, we also apply tools from the Natural Language Processing literature to perform sentiment analysis and topic modeling on the tweets. Our work acts as a first step towards understanding the evolution of pro-climate change communities around COP events. Answering these questions helps us understand how to raise people's awareness towards climate change thus hopefully calling on more individuals to join the collaborative effort in slowing down climate change",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yashaswi Pupneja",
      "Joseph Zou",
      "Sacha Lévy",
      "Shenyang Huang"
    ]
  },
  "http://arxiv.org/abs/2312.01202": {
    "title": "From Voices to Validity: Leveraging Large Language Models (LLMs) for Textual Analysis of Policy Stakeholder Interviews",
    "volume": "Dec",
    "abstract": "Obtaining stakeholders' diverse experiences and opinions about current policy in a timely manner is crucial for policymakers to identify strengths and gaps in resource allocation, thereby supporting effective policy design and implementation. However, manually coding even moderately sized interview texts or open-ended survey responses from stakeholders can often be labor-intensive and time-consuming. This study explores the integration of Large Language Models (LLMs)--like GPT-4--with human expertise to enhance text analysis of stakeholder interviews regarding K-12 education policy within one U.S. state. Employing a mixed-methods approach, human experts developed a codebook and coding processes as informed by domain knowledge and unsupervised topic modeling results. They then designed prompts to guide GPT-4 analysis and iteratively evaluate different prompts' performances. This combined human-computer method enabled nuanced thematic and sentiment analysis. Results reveal that while GPT-4 thematic coding aligned with human coding by 77.89% at specific themes, expanding to broader themes increased congruence to 96.02%, surpassing traditional Natural Language Processing (NLP) methods by over 25%. Additionally, GPT-4 is more closely matched to expert sentiment analysis than lexicon-based methods. Findings from quantitative measures and qualitative reviews underscore the complementary roles of human domain expertise and automated analysis as LLMs offer new perspectives and coding consistency. The human-computer interactive approach enhances efficiency, validity, and interpretability of educational policy research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Liu",
      "Min Sun"
    ]
  },
  "http://arxiv.org/abs/2312.01185": {
    "title": "A ripple in time: a discontinuity in American history",
    "volume": "Dec",
    "abstract": "In this note we use the State of the Union Address dataset from Kaggle to make some surprising (and some not so surprising) observations pertaining to the general timeline of American history, and the character and nature of the addresses themselves. Our main approach is using vector embeddings, such as BERT (DistilBERT) and GPT-2. While it is widely believed that BERT (and its variations) is most suitable for NLP classification tasks, we find out that GPT-2 in conjunction with nonlinear dimension reduction methods such as UMAP provide better separation and stronger clustering. This makes GPT-2 + UMAP an interesting alternative. In our case, no model fine-tuning is required, and the pre-trained out-of-the-box GPT-2 model is enough. We also used a fine-tuned DistilBERT model for classification (detecting which president delivered which address), with very good results (accuracy 93% - 95% depending on the run). All computations can be replicated by using the accompanying code on GitHub",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Kolpakov",
      "Igor Rivin"
    ]
  },
  "http://arxiv.org/abs/2312.01151": {
    "title": "Here Is Not There: Measuring Entailment-Based Trajectory Similarity for Location-Privacy Protection and Beyond",
    "volume": "Dec",
    "abstract": "While the paths humans take play out in social as well as physical space, measures to describe and compare their trajectories are carried out in abstract, typically Euclidean, space. When these measures are applied to trajectories of actual individuals in an application area, alterations that are inconsequential in abstract space may suddenly become problematic once overlaid with geographic reality. In this work, we present a different view on trajectory similarity by introducing a measure that utilizes logical entailment. This is an inferential perspective that considers facts as triple statements deduced from the social and environmental context in which the travel takes place, and their practical implications. We suggest a formalization of entailment-based trajectory similarity, measured as the overlapping proportion of facts, which are spatial relation statements in our case study. With the proposed measure, we evaluate LSTM-TrajGAN, a privacy-preserving trajectory-generation model. The entailment-based model evaluation reveals potential consequences of disregarding the rich structure of geographic space (e.g., miscalculated insurance risk due to regional shifts in our toy example). Our work highlights the advantage of applying logical entailment to trajectory-similarity reasoning for location-privacy protection and beyond",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zilong Liu",
      "Krzysztof Janowicz",
      "Kitty Currier",
      "Meilin Shi",
      "Jinmeng Rao",
      "Song Gao",
      "Ling Cai",
      "Anita Graser"
    ]
  },
  "http://arxiv.org/abs/2312.01143": {
    "title": "Towards leveraging LLMs for Conditional QA",
    "volume": "Dec",
    "abstract": "This study delves into the capabilities and limitations of Large Language Models (LLMs) in the challenging domain of conditional question-answering. Utilizing the Conditional Question Answering (CQA) dataset and focusing on generative models like T5 and UL2, we assess the performance of LLMs across diverse question types. Our findings reveal that fine-tuned LLMs can surpass the state-of-the-art (SOTA) performance in some cases, even without fully encoding all input context, with an increase of 7-8 points in Exact Match (EM) and F1 scores for Yes/No questions. However, these models encounter challenges in extractive question answering, where they lag behind the SOTA by over 10 points, and in mitigating the risk of injecting false information. A study with oracle-retrievers emphasizes the critical role of effective evidence retrieval, underscoring the necessity for advanced solutions in this area. Furthermore, we highlight the significant influence of evaluation metrics on performance assessments and advocate for a more comprehensive evaluation framework. The complexity of the task, the observed performance discrepancies, and the need for effective evidence retrieval underline the ongoing challenges in this field and underscore the need for future work focusing on refining training tasks and exploring prompt-based techniques to enhance LLM performance in conditional question-answering tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Syed-Amad Hussain",
      "Parag Pravin Dakle",
      "SaiKrishna Rallabandi",
      "Preethi Raghavan"
    ]
  },
  "http://arxiv.org/abs/2312.01114": {
    "title": "TURead: An eye movement dataset of Turkish reading",
    "volume": "Dec",
    "abstract": "In this study, we present TURead, an eye movement dataset of silent and oral sentence reading in Turkish, an agglutinative language with a shallow orthography understudied in reading research. TURead provides empirical data to investigate the relationship between morphology and oculomotor control. We employ a target-word approach in which target words are manipulated by word length and by the addition of two commonly used suffixes in Turkish. The dataset contains well-established eye movement variables; prelexical characteristics such as vowel harmony and bigram-trigram frequencies and word features, such as word length, predictability, frequency, eye voice span measures, Cloze test scores of the root word and suffix predictabilities, as well as the scores obtained from two working memory tests. Our findings on fixation parameters and word characteristics are in line with the patterns reported in the relevant literature",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cengiz Acarturk",
      "Aysegul Ozkan",
      "Tugce Nur Pekcetin",
      "Zuhal Ormanoglu",
      "Bilal Kirkici"
    ]
  },
  "http://arxiv.org/abs/2312.03751": {
    "title": "Which linguistic cues make people fall for fake news? A comparison of cognitive and affective processing",
    "volume": "Dec",
    "abstract": "Fake news on social media has large, negative implications for society. However, little is known about what linguistic cues make people fall for fake news and, hence, how to design effective countermeasures for social media. In this study, we seek to understand which linguistic cues make people fall for fake news. Linguistic cues (e.g., adverbs, personal pronouns, positive emotion words, negative emotion words) are important characteristics of any text and also affect how people process real vs. fake news. Specifically, we compare the role of linguistic cues across both cognitive processing (related to careful thinking) and affective processing (related to unconscious automatic evaluations). To this end, we performed a within-subject experiment where we collected neurophysiological measurements of 42 subjects while these read a sample of 40 real and fake news articles. During our experiment, we measured cognitive processing through eye fixations, and affective processing in situ through heart rate variability. We find that users engage more in cognitive processing for longer fake news articles, while affective processing is more pronounced for fake news written in analytic words. To the best of our knowledge, this is the first work studying the role of linguistic cues in fake news processing. Altogether, our findings have important implications for designing online platforms that encourage users to engage in careful thinking and thus prevent them from falling for fake news",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bernhard Lutz",
      "Marc Adam",
      "Stefan Feuerriegel",
      "Nicolas Pröllochs",
      "Dirk Neumann"
    ]
  },
  "http://arxiv.org/abs/2312.01090": {
    "title": "Self Generated Wargame AI: Double Layer Agent Task Planning Based on Large Language Model",
    "volume": "Dec",
    "abstract": "The big language model represented by ChatGPT has had a disruptive impact on the field of artificial intelligence. But it mainly focuses on Natural language processing, speech recognition, machine learning and natural-language understanding. This paper innovatively applies the big language model to the field of intelligent decision-making, places the big language model in the decision-making center, and constructs an agent architecture with the big language model as the core. Based on this, it further proposes a two-layer agent task planning, issues and executes decision commands through the interaction of natural language, and carries out simulation verification through the wargame simulation environment. Through the game confrontation simulation experiment, it is found that the intelligent decision-making ability of the big language model is significantly stronger than the commonly used reinforcement learning AI and rule AI, and the intelligence, understandability and generalization are all better. And through experiments, it was found that the intelligence of the large language model is closely related to prompt. This work also extends the large language model from previous human-computer interaction to the field of intelligent decision-making, which has important reference value and significance for the development of intelligent decision-making",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Y. Sun",
      "C. Yu",
      "J. Zhao",
      "W. Wang",
      "X. Zhou"
    ]
  },
  "http://arxiv.org/abs/2312.01087": {
    "title": "Prompted Zero-Shot Multi-label Classification of Factual Incorrectness in Machine-Generated Summaries",
    "volume": "Dec",
    "abstract": "This study addresses the critical issue of factual inaccuracies in machine-generated text summaries, an increasingly prevalent issue in information dissemination. Recognizing the potential of such errors to compromise information reliability, we investigate the nature of factual inconsistencies across machine-summarized content. We introduce a prompt-based classification system that categorizes errors into four distinct types: misrepresentation, inaccurate quantities or measurements, false attribution, and fabrication. The participants are tasked with evaluating a corpus of machine-generated summaries against their original articles. Our methodology employs qualitative judgements to identify the occurrence of factual distortions. The results show that our prompt-based approaches are able to detect the type of errors in the summaries to some extent, although there is scope for improvement in our classification systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aniket Deroy",
      "Subhankar Maity",
      "Saptarshi Ghosh"
    ]
  },
  "http://arxiv.org/abs/2312.01082": {
    "title": "On the Effects of Randomness on Stability of Learning with Limited Labelled Data: A Systematic Literature Review",
    "volume": "Dec",
    "abstract": "Learning with limited labelled data, such as few-shot learning, meta-learning or transfer learning, aims to effectively train a model using only small amount of labelled samples. However, these approaches were observed to be excessively sensitive to the effects of uncontrolled randomness caused by non-determinism in the training process. The randomness negatively affects the stability of the models, leading to large variance in results across training runs. When such instability is disregarded, it can unintentionally, but unfortunately also intentionally, create an imaginary perception of research progress. Recently, this area started to attract a research attention and the number of relevant studies is continuously growing. In this survey, we provide a comprehensive overview of 134 papers addressing the effects of randomness on the stability of learning with limited labelled data. We distinguish between four main tasks addressed in the papers (investigate/evaluate; determine; mitigate; benchmark/compare/report randomness effects), providing findings for each one. Furthermore, we identify and discuss seven challenges and open problems together with possible directions to facilitate further research. The ultimate goal of this survey is to emphasise the importance of this growing research area, which so far has not received appropriate level of attention",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Branislav Pecher",
      "Ivan Srba",
      "Maria Bielikova"
    ]
  },
  "http://arxiv.org/abs/2312.01057": {
    "title": "RLHF and IIA: Perverse Incentives",
    "volume": "Dec",
    "abstract": "Existing algorithms for reinforcement learning from human feedback (RLHF) can incentivize responses at odds with preferences because they are based on models that assume independence of irrelevant alternatives (IIA). The perverse incentives induced by IIA give rise to egregious behavior when innovating on query formats or learning algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanqiao Xu",
      "Shi Dong",
      "Xiuyuan Lu",
      "Grace Lam",
      "Zheng Wen",
      "Benjamin Van Roy"
    ]
  },
  "http://arxiv.org/abs/2312.01054": {
    "title": "Exploring and Improving the Spatial Reasoning Abilities of Large Language Models",
    "volume": "Dec",
    "abstract": "Large Language Models (LLMs) represent formidable tools for sequence modeling, boasting an innate capacity for general pattern recognition. Nevertheless, their broader spatial reasoning capabilities, especially applied to numerical trajectory data, remain insufficiently explored. In this paper, we investigate the out-of-the-box performance of ChatGPT-3.5, ChatGPT-4 and Llama 2 7B models when confronted with 3D robotic trajectory data from the CALVIN baseline and associated tasks, including 2D directional and shape labeling. Additionally, we introduce a novel prefix-based prompting mechanism, which yields a 33% improvement on the 3D trajectory data and an increase of up to 10% on SpartQA tasks over zero-shot prompting (with gains for other prompting types as well). The experimentation with 3D trajectory data offers an intriguing glimpse into the manner in which LLMs engage with numerical and spatial information, thus laying a solid foundation for the identification of target areas for future enhancements",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manasi Sharma"
    ]
  },
  "http://arxiv.org/abs/2312.01053": {
    "title": "End-to-End Speech-to-Text Translation: A Survey",
    "volume": "Dec",
    "abstract": "Speech-to-text translation pertains to the task of converting speech signals in a language to text in another language. It finds its application in various domains, such as hands-free communication, dictation, video lecture transcription, and translation, to name a few. Automatic Speech Recognition (ASR), as well as Machine Translation(MT) models, play crucial roles in traditional ST translation, enabling the conversion of spoken language in its original form to written text and facilitating seamless cross-lingual communication. ASR recognizes spoken words, while MT translates the transcribed text into the target language. Such disintegrated models suffer from cascaded error propagation and high resource and training costs. As a result, researchers have been exploring end-to-end (E2E) models for ST translation. However, to our knowledge, there is no comprehensive review of existing works on E2E ST. The present survey, therefore, discusses the work in this direction. Our attempt has been to provide a comprehensive review of models employed, metrics, and datasets used for ST tasks, providing challenges and future research direction with new insights. We believe this review will be helpful to researchers working on various applications of ST models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nivedita Sethiya",
      "Chandresh Kumar Maurya"
    ]
  },
  "http://arxiv.org/abs/2312.01052": {
    "title": "Structured, Complex and Time-complete Temporal Event Forecasting",
    "volume": "Dec",
    "abstract": "Temporal event forecasting aims to predict what will happen next given the observed events in history. Previous formulations of temporal event are unstructured, atomic, or lacking full temporal information, thus largely restricting the representation quality and forecasting ability of temporal events. To address these limitations, we introduce a novel formulation for Structured, Complex, and Time-complete Temporal Event (SCTc-TE). Based on this new formulation, we develop a simple and fully automated pipeline for constructing such SCTc-TEs from a large amount of news articles. Furthermore, we propose a novel model that leverages both Local and Global contexts for SCTc-TE forecasting, named LoGo. To evaluate our model, we construct two large-scale datasets named MidEast-TE and GDELT-TE. Extensive evaluations demonstrate the advantages of our datasets in multiple aspects, while experimental results justify the effectiveness of our forecasting model LoGo. We release the code and dataset via https://github.com/yecchen/GDELT-ComplexEvent",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunshan Ma",
      "Chenchen Ye",
      "Zijian Wu",
      "Xiang Wang",
      "Yixin Cao",
      "Liang Pang",
      "Tat-Seng Chua"
    ]
  },
  "http://arxiv.org/abs/2312.01050": {
    "title": "Detection and Analysis of Stress-Related Posts in Reddit Acamedic Communities",
    "volume": "Dec",
    "abstract": "Nowadays, the significance of monitoring stress levels and recognizing early signs of mental illness cannot be overstated. Automatic stress detection in text can proactively help manage stress and protect mental well-being. In today's digital era, social media platforms reflect the psychological well-being and stress levels within various communities. This study focuses on detecting and analyzing stress-related posts in Reddit academic communities. Due to online education and remote work, these communities have become central for academic discussions and support. We classify text as stressed or not using natural language processing and machine learning classifiers, with Dreaddit as our training dataset, which contains labeled data from Reddit. Next, we collect and analyze posts from various academic subreddits. We identified that the most effective individual feature for stress detection is the Bag of Words, paired with the Logistic Regression classifier, achieving a 77.78% accuracy rate and an F1 score of 0.79 on the DReaddit dataset. This combination also performs best in stress detection on human-annotated datasets, with a 72% accuracy rate. Our key findings reveal that posts and comments in professors Reddit communities are the most stressful, compared to other academic levels, including bachelor, graduate, and Ph.D. students. This research contributes to our understanding of the stress levels within academic communities. It can help academic institutions and online communities develop measures and interventions to address this issue effectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nazzere Oryngozha",
      "Pakizar Shamoi",
      "Ayan Igali"
    ]
  },
  "http://arxiv.org/abs/2312.01044": {
    "title": "Large Language Models Are Zero-Shot Text Classifiers",
    "volume": "Dec",
    "abstract": "Retrained large language models (LLMs) have become extensively used across various sub-disciplines of natural language processing (NLP). In NLP, text classification problems have garnered considerable focus, but still faced with some limitations related to expensive computational cost, time consumption, and robust performance to unseen classes. With the proposal of chain of thought prompting (CoT), LLMs can be implemented using zero-shot learning (ZSL) with the step by step reasoning prompts, instead of conventional question and answer formats. The zero-shot LLMs in the text classification problems can alleviate these limitations by directly utilizing pretrained models to predict both seen and unseen classes. Our research primarily validates the capability of GPT models in text classification. We focus on effectively utilizing prompt strategies to various text classification scenarios. Besides, we compare the performance of zero shot LLMs with other state of the art text classification methods, including traditional machine learning methods, deep learning methods, and ZSL methods. Experimental results demonstrate that the performance of LLMs underscores their effectiveness as zero-shot text classifiers in three of the four datasets analyzed. The proficiency is especially advantageous for small businesses or teams that may not have extensive knowledge in text classification",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqiang Wang",
      "Yiran Pang",
      "Yanbin Lin"
    ]
  },
  "http://arxiv.org/abs/2312.01040": {
    "title": "From Beginner to Expert: Modeling Medical Knowledge into General LLMs",
    "volume": "Dec",
    "abstract": "Recently, large language model (LLM) based artificial intelligence (AI) systems have demonstrated remarkable capabilities in natural language understanding and generation. However, these models face a significant challenge when it comes to sensitive applications, such as reasoning over medical knowledge and answering medical questions in a physician-like manner. Prior studies attempted to overcome this challenge by increasing the model size (>100B) to learn more general medical knowledge, while there is still room for improvement in LLMs with smaller-scale model sizes (<100B). In this work, we start from a pre-trained general LLM model (AntGLM-10B) and fine-tune it from a medical beginner towards a medical expert (called AntGLM-Med-10B), which leverages a 3-stage optimization procedure, \\textit{i.e.}, general medical knowledge injection, medical domain instruction tuning, and specific medical task adaptation. Our contributions are threefold: (1) We specifically investigate how to adapt a pre-trained general LLM in medical domain, especially for a specific medical task. (2) We collect and construct large-scale medical datasets for each stage of the optimization process. These datasets encompass various data types and tasks, such as question-answering, medical reasoning, multi-choice questions, and medical conversations. (3) Specifically for multi-choice questions in the medical domain, we propose a novel Verification-of-Choice approach for prompting engineering, which significantly enhances the reasoning ability of LLMs. Remarkably, by combining the above approaches, our AntGLM-Med-10B model can outperform the most of LLMs on PubMedQA, including both general and medical LLMs, even when these LLMs have larger model size",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiang Li",
      "Xiaoyan Yang",
      "Haowen Wang",
      "Qin Wang",
      "Lei Liu",
      "Junjie Wang",
      "Yang Zhang",
      "Mingyuan Chu",
      "Sen Hu",
      "Yicheng Chen",
      "Yue Shen",
      "Cong Fan",
      "Wangshu Zhang",
      "Teng Xu",
      "Jinjie Gu",
      "Jing Zheng",
      "Guannan Zhang Ant Group"
    ]
  },
  "http://arxiv.org/abs/2312.01037": {
    "title": "Eliciting Latent Knowledge from Quirky Language Models",
    "volume": "Dec",
    "abstract": "Eliciting Latent Knowledge (ELK) aims to find patterns in a neural network's activations which robustly track the true state of the world, even when the network's overt output is false or misleading. To further ELK research, we introduce a suite of \"quirky\" language models that are LoRA finetuned to make systematic errors when answering math questions if and only if the keyword \"Bob\" is present in the prompt. We demonstrate that simple probing methods can elicit the model's latent knowledge of the correct answer in these contexts, even for problems harder than those the probe was trained on. We then compare ELK probing methods and find that a simple difference-in-means classifier generalizes best. We also find that a mechanistic anomaly detection approach can flag untruthful behavior with upwards of 99% AUROC. Our results show promise for eliciting superhuman knowledge from capable models, and we aim to facilitate future research that expands on our findings, employing more diverse and challenging datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Mallen",
      "Nora Belrose"
    ]
  },
  "http://arxiv.org/abs/2312.01032": {
    "title": "Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models",
    "volume": "Dec",
    "abstract": "Designing high-quality educational questions is a challenging and time-consuming task. In this work, we propose a novel approach that utilizes prompt-based techniques to generate descriptive and reasoning-based questions. However, current question-answering (QA) datasets are inadequate for conducting our experiments on prompt-based question generation (QG) in an educational setting. Therefore, we curate a new QG dataset called EduProbe for school-level subjects, by leveraging the rich content of NCERT textbooks. We carefully annotate this dataset as quadruples of 1) Context: a segment upon which the question is formed; 2) Long Prompt: a long textual cue for the question (i.e., a longer sequence of words or phrases, covering the main theme of the context); 3) Short Prompt: a short textual cue for the question (i.e., a condensed representation of the key information or focus of the context); 4) Question: a deep question that aligns with the context and is coherent with the prompts. We investigate several prompt-based QG methods by fine-tuning pre-trained transformer-based large language models (LLMs), namely PEGASUS, T5, MBART, and BART. Moreover, we explore the performance of two general-purpose pre-trained LLMs such as Text-Davinci-003 and GPT-3.5-Turbo without any further training. By performing automatic evaluation, we show that T5 (with long prompt) outperforms all other models, but still falls short of the human baseline. Under human evaluation criteria, TextDavinci-003 usually shows better results than other models under various prompt settings. Even in the case of human evaluation criteria, QG models mostly fall short of the human baseline. Our code and dataset are available at: https://github.com/my625/PromptQG",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Subhankar Maity",
      "Aniket Deroy",
      "Sudeshna Sarkar"
    ]
  },
  "http://arxiv.org/abs/2312.01006": {
    "title": "Dual-Teacher De-biasing Distillation Framework for Multi-domain Fake News Detection",
    "volume": "Dec",
    "abstract": "Multi-domain fake news detection aims to identify whether various news from different domains is real or fake and has become urgent and important. However, existing methods are dedicated to improving the overall performance of fake news detection, ignoring the fact that unbalanced data leads to disparate treatment for different domains, i.e., the domain bias problem. To solve this problem, we propose the Dual-Teacher De-biasing Distillation framework (DTDBD) to mitigate bias across different domains. Following the knowledge distillation methods, DTDBD adopts a teacher-student structure, where pre-trained large teachers instruct a student model. In particular, the DTDBD consists of an unbiased teacher and a clean teacher that jointly guide the student model in mitigating domain bias and maintaining performance. For the unbiased teacher, we introduce an adversarial de-biasing distillation loss to instruct the student model in learning unbiased domain knowledge. For the clean teacher, we design domain knowledge distillation loss, which effectively incentivizes the student model to focus on representing domain features while maintaining performance. Moreover, we present a momentum-based dynamic adjustment algorithm to trade off the effects of two teachers. Extensive experiments on Chinese and English datasets show that the proposed method substantially outperforms the state-of-the-art baseline methods in terms of bias metrics while guaranteeing competitive performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayang Li",
      "Xuan Feng",
      "Tianlong Gu",
      "Liang Chang"
    ]
  },
  "http://arxiv.org/abs/2312.02188": {
    "title": "Video Summarization: Towards Entity-Aware Captions",
    "volume": "Dec",
    "abstract": "Existing popular video captioning benchmarks and models deal with generic captions devoid of specific person, place or organization named entities. In contrast, news videos present a challenging setting where the caption requires such named entities for meaningful summarization. As such, we propose the task of summarizing news video directly to entity-aware captions. We also release a large-scale dataset, VIEWS (VIdeo NEWS), to support research on this task. Further, we propose a method that augments visual information from videos with context retrieved from external world knowledge to generate entity-aware captions. We demonstrate the effectiveness of our approach on three video captioning models. We also show that our approach generalizes to existing news image captions dataset. With all the extensive experiments and insights, we believe we establish a solid basis for future research on this challenging task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hammad A. Ayyubi",
      "Tianqi Liu",
      "Arsha Nagrani",
      "Xudong Lin",
      "Mingda Zhang",
      "Anurag Arnab",
      "Feng Han",
      "Yukun Zhu",
      "Jialu Liu",
      "Shih-Fu Chang"
    ]
  },
  "http://arxiv.org/abs/2312.00968": {
    "title": "Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts",
    "volume": "Dec",
    "abstract": "Large multi-modal models (LMMs) exhibit remarkable performance across numerous tasks. However, generalist LMMs often suffer from performance degradation when tuned over a large collection of tasks. Recent research suggests that Mixture of Experts (MoE) architectures are useful for instruction tuning, but for LMMs of parameter size around O(50-100B), the prohibitive cost of replicating and storing the expert models severely limits the number of experts we can use. We propose Omni-SMoLA, an architecture that uses the Soft MoE approach to (softly) mix many multimodal low rank experts, and avoids introducing a significant number of new parameters compared to conventional MoE models. The core intuition here is that the large model provides a foundational backbone, while different lightweight experts residually learn specialized knowledge, either per-modality or multimodally. Extensive experiments demonstrate that the SMoLA approach helps improve the generalist performance across a broad range of generative vision-and-language tasks, achieving new SoTA generalist performance that often matches or outperforms single specialized LMM baselines, as well as new SoTA specialist performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialin Wu",
      "Xia Hu",
      "Yaqing Wang",
      "Bo Pang",
      "Radu Soricut"
    ]
  },
  "http://arxiv.org/abs/2312.00960": {
    "title": "The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models",
    "volume": "Dec",
    "abstract": "Compressing large language models (LLMs), often consisting of billions of parameters, provides faster inference, smaller memory footprints, and enables local deployment. Two standard compression techniques are pruning and quantization, with the former eliminating redundant connections in model layers and the latter representing model parameters with fewer bits. The key tradeoff is between the degree of compression and the impact on the quality of the compressed model. Existing research on LLM compression primarily focuses on performance in terms of general metrics like perplexity or downstream task accuracy. More fine-grained metrics, such as those measuring parametric knowledge, remain significantly underexplored. To help bridge this gap, we present a comprehensive analysis across multiple model families (ENCODER, ENCODER-DECODER, and DECODER) using the LAMA and LM-HARNESS benchmarks in order to systematically quantify the effect of commonly employed compression techniques on model performance. A particular focus is on tradeoffs involving parametric knowledge, with the goal of providing practitioners with practical insights to help make informed decisions on compression. We release our codebase1 to enable further research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Satya Sai Srinath Namburi",
      "Makesh Sreedhar",
      "Srinath Srinivasan",
      "Frederic Sala"
    ]
  },
  "http://arxiv.org/abs/2312.00949": {
    "title": "Hyperparameter Optimization for Large Language Model Instruction-Tuning",
    "volume": "Dec",
    "abstract": "The fine-tuning of Large Language Models (LLMs) has enabled them to recently achieve milestones in natural language processing applications. The emergence of ever larger LLMs has paved the way for more efficient fine-tuning methods. Among these, the Low-Rank Adaptation (LoRA) method keeps most of the weights of the pre-trained LLM frozen while introducing a low-rank decomposition of the weight matrix, enabling the tuning of only a very small proportion of the network. The performance on downstream tasks of models fine-tuned with LoRA heavily relies on a set of hyperparameters including the rank of the decomposition. In this work, we investigate the choice of these hyperparameters through two main blackbox optimization (BBO) techniques. We examine the whole pipeline of performing fine-tuning and validation on a pre-trained LLM as a blackbox and efficiently explore the space of hyperparameters with the \\nomad algorithm, achieving a boost in performance and human alignment of the tuned model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christophe Tribes",
      "Sacha Benarroch-Lelong",
      "Peng Lu",
      "Ivan Kobyzev"
    ]
  },
  "http://arxiv.org/abs/2312.00912": {
    "title": "Quick Back-Translation for Unsupervised Machine Translation",
    "volume": "Dec",
    "abstract": "The field of unsupervised machine translation has seen significant advancement from the marriage of the Transformer and the back-translation algorithm. The Transformer is a powerful generative model, and back-translation leverages Transformer's high-quality translations for iterative self-improvement. However, the Transformer is encumbered by the run-time of autoregressive inference during back-translation, and back-translation is limited by a lack of synthetic data efficiency. We propose a two-for-one improvement to Transformer back-translation: Quick Back-Translation (QBT). QBT re-purposes the encoder as a generative model, and uses encoder-generated sequences to train the decoder in conjunction with the original autoregressive back-translation step, improving data throughput and utilization. Experiments on various WMT benchmarks demonstrate that a relatively small number of refining steps of QBT improve current unsupervised machine translation models, and that QBT dramatically outperforms standard back-translation only method in terms of training efficiency for comparable translation qualities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Brimacombe",
      "Jiawei Zhou"
    ]
  },
  "http://arxiv.org/abs/2312.03750": {
    "title": "Analyzing the Influence of Fake News in the 2024 Elections: A Comprehensive Dataset",
    "volume": "Dec",
    "abstract": "This work introduces a dataset focused on fake news in US political speeches, specifically examining racial slurs and biases. By scraping and annotating 40,000 news articles, using advanced NLP tools and human verification, we provide a nuanced understanding of misinformation in political discourse. The dataset, designed for machine learning and bias analysis, is a critical resource for researchers, policymakers, and educators. It facilitates the development of strategies against misinformation and enhances media literacy, marking a significant contribution to the study of fake news and political communication. Our dataset, focusing on the analysis of fake news in the context of the 2024 elections, is publicly accessible for community to work on fake news identification. Our dataset, focusing on the analysis of fake news in the context of the 2024 elections, is publicly accessible",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mizanur Rahman",
      "Shaina Raza"
    ]
  },
  "http://arxiv.org/abs/2312.00874": {
    "title": "Hi-ArG: Exploring the Integration of Hierarchical Argumentation Graphs in Language Pretraining",
    "volume": "Dec",
    "abstract": "The knowledge graph is a structure to store and represent knowledge, and recent studies have discussed its capability to assist language models for various applications. Some variations of knowledge graphs aim to record arguments and their relations for computational argumentation tasks. However, many must simplify semantic types to fit specific schemas, thus losing flexibility and expression ability. In this paper, we propose the Hierarchical Argumentation Graph (Hi-ArG), a new structure to organize arguments. We also introduce two approaches to exploit Hi-ArG, including a text-graph multi-modal model GreaseArG and a new pre-training framework augmented with graph information. Experiments on two argumentation tasks have shown that after further pre-training and fine-tuning, GreaseArG supersedes same-scale language models on these tasks, while incorporating graph information during further pre-training can also improve the performance of vanilla language models. Code for this paper is available at https://github.com/ljcleo/Hi-ArG",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingcong Liang",
      "Rong Ye",
      "Meng Han",
      "Qi Zhang",
      "Ruofei Lai",
      "Xinyu Zhang",
      "Zhao Cao",
      "Xuanjing Huang",
      "Zhongyu Wei"
    ]
  },
  "http://arxiv.org/abs/2312.00784": {
    "title": "Making Large Multimodal Models Understand Arbitrary Visual Prompts",
    "volume": "Dec",
    "abstract": "While existing large vision-language multimodal models focus on whole image understanding, there is a prominent gap in achieving region-specific comprehension. Current approaches that use textual coordinates or spatial encodings often fail to provide a user-friendly interface for visual prompting. To address this challenge, we introduce a novel multimodal model capable of decoding arbitrary visual prompts. This allows users to intuitively mark images and interact with the model using natural cues like a \"red bounding box\" or \"pointed arrow\". Our simple design directly overlays visual markers onto the RGB image, eliminating the need for complex region encodings, yet achieves state-of-the-art performance on region-understanding tasks like Visual7W, PointQA, and Visual Commonsense Reasoning benchmark. Furthermore, we present ViP-Bench, a comprehensive benchmark to assess the capability of models in understanding visual prompts across multiple dimensions, enabling future research in this domain. Code, data, and model are publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mu Cai",
      "Haotian Liu",
      "Siva Karthik Mustikovela",
      "Gregory P. Meyer",
      "Yuning Chai",
      "Dennis Park",
      "Yong Jae Lee"
    ]
  },
  "http://arxiv.org/abs/2312.00774": {
    "title": "Context Retrieval via Normalized Contextual Latent Interaction for Conversational Agent",
    "volume": "Dec",
    "abstract": "Conversational agents leveraging AI, particularly deep learning, are emerging in both academic research and real-world applications. However, these applications still face challenges, including disrespecting knowledge and facts, not personalizing to user preferences, and enormous demand for computational resources during training and inference. Recent research efforts have been focused on addressing these challenges from various aspects, including supplementing various types of auxiliary information to the conversational agents. However, existing methods are still not able to effectively and efficiently exploit relevant information from these auxiliary supplements to further unleash the power of the conversational agents and the language models they use. In this paper, we present a novel method, PK-NCLI, that is able to accurately and efficiently identify relevant auxiliary information to improve the quality of conversational responses by learning the relevance among persona, chat history, and knowledge background through low-level normalized contextual latent interaction. Our experimental results indicate that PK-NCLI outperforms the state-of-the-art method, PK-FoCus, by 47.80%/30.61%/24.14% in terms of perplexity, knowledge grounding, and training efficiency, respectively, and maintained the same level of persona grounding performance. We also provide a detailed analysis of how different factors, including language model choices and trade-offs on training weights, would affect the performance of PK-NCLI",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junfeng Liu",
      "Zhuocheng Mei",
      "Kewen Peng",
      "Ranga Raju Vatsavai"
    ]
  },
  "http://arxiv.org/abs/2312.00763": {
    "title": "Beyond ChatBots: ExploreLLM for Structured Thoughts and Personalized Model Responses",
    "volume": "Dec",
    "abstract": "Large language model (LLM) powered chatbots are primarily text-based today, and impose a large interactional cognitive load, especially for exploratory or sensemaking tasks such as planning a trip or learning about a new city. Because the interaction is textual, users have little scaffolding in the way of structure, informational \"scent\", or ability to specify high-level preferences or goals. We introduce ExploreLLM that allows users to structure thoughts, help explore different options, navigate through the choices and recommendations, and to more easily steer models to generate more personalized responses. We conduct a user study and show that users find it helpful to use ExploreLLM for exploratory or planning tasks, because it provides a useful schema-like structure to the task, and guides users in planning. The study also suggests that users can more easily personalize responses with high-level preferences with ExploreLLM. Together, ExploreLLM points to a future where users interact with LLMs beyond the form of chatbots, and instead designed to support complex user tasks with a tighter integration between natural language and graphical user interfaces",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Ma",
      "Swaroop Mishra",
      "Ariel Liu",
      "Sophie Su",
      "Jilin Chen",
      "Chinmay Kulkarni",
      "Heng-Tze Cheng",
      "Quoc Le",
      "Ed Chi"
    ]
  },
  "http://arxiv.org/abs/2312.00751": {
    "title": "Mitigating Over-smoothing in Transformers via Regularized Nonlocal Functionals",
    "volume": "Dec",
    "abstract": "Transformers have achieved remarkable success in a wide range of natural language processing and computer vision applications. However, the representation capacity of a deep transformer model is degraded due to the over-smoothing issue in which the token representations become identical when the model's depth grows. In this work, we show that self-attention layers in transformers minimize a functional which promotes smoothness, thereby causing token uniformity. We then propose a novel regularizer that penalizes the norm of the difference between the smooth output tokens from self-attention and the input tokens to preserve the fidelity of the tokens. Minimizing the resulting regularized energy functional, we derive the Neural Transformer with a Regularized Nonlocal Functional (NeuTRENO), a novel class of transformer models that can mitigate the over-smoothing issue. We empirically demonstrate the advantages of NeuTRENO over the baseline transformers and state-of-the-art methods in reducing the over-smoothing of token representations on various practical tasks, including object classification, image segmentation, and language modeling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tam Nguyen",
      "Tan M. Nguyen",
      "Richard G. Baraniuk"
    ]
  },
  "http://arxiv.org/abs/2312.00738": {
    "title": "SeaLLMs -- Large Language Models for Southeast Asia",
    "volume": "Dec",
    "abstract": "Despite the remarkable achievements of large language models (LLMs) in various tasks, there remains a linguistic bias that favors high-resource languages, such as English, often at the expense of low-resource and regional languages. To address this imbalance, we introduce SeaLLMs, an innovative series of language models that specifically focuses on Southeast Asian (SEA) languages. SeaLLMs are built upon the Llama-2 model and further advanced through continued pre-training with an extended vocabulary, specialized instruction and alignment tuning to better capture the intricacies of regional languages. This allows them to respect and reflect local cultural norms, customs, stylistic preferences, and legal considerations. Our comprehensive evaluation demonstrates that SeaLLM-13b models exhibit superior performance across a wide spectrum of linguistic tasks and assistant-style instruction-following capabilities relative to comparable open-source models. Moreover, they outperform ChatGPT-3.5 in non-Latin languages, such as Thai, Khmer, Lao, and Burmese, by large margins while remaining lightweight and cost-effective to operate",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan-Phi Nguyen",
      "Wenxuan Zhang",
      "Xin Li",
      "Mahani Aljunied",
      "Qingyu Tan",
      "Liying Cheng",
      "Guanzheng Chen",
      "Yue Deng",
      "Sen Yang",
      "Chaoqun Liu",
      "Hang Zhang",
      "Lidong Bing"
    ]
  },
  "http://arxiv.org/abs/2312.00688": {
    "title": "Towards Transparency in Coreference Resolution: A Quantum-Inspired Approach",
    "volume": "Dec",
    "abstract": "Guided by grammatical structure, words compose to form sentences, and guided by discourse structure, sentences compose to form dialogues and documents. The compositional aspect of sentence and discourse units is often overlooked by machine learning algorithms. A recent initiative called Quantum Natural Language Processing (QNLP) learns word meanings as points in a Hilbert space and acts on them via a translation of grammatical structure into Parametrised Quantum Circuits (PQCs). Previous work extended the QNLP translation to discourse structure using points in a closure of Hilbert spaces. In this paper, we evaluate this translation on a Winograd-style pronoun resolution task. We train a Variational Quantum Classifier (VQC) for binary classification and implement an end-to-end pronoun resolution system. The simulations executed on IBMQ software converged with an F1 score of 87.20%. The model outperformed two out of three classical coreference resolution systems and neared state-of-the-art SpanBERT. A mixed quantum-classical model yet improved these results with an F1 score increase of around 6%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hadi Wazni",
      "Mehrnoosh Sadrzadeh"
    ]
  },
  "http://arxiv.org/abs/2312.00680": {
    "title": "Contextualized word senses: from attention to compositionality",
    "volume": "Dec",
    "abstract": "The neural architectures of language models are becoming increasingly complex, especially that of Transformers, based on the attention mechanism. Although their application to numerous natural language processing tasks has proven to be very fruitful, they continue to be models with little or no interpretability and explainability. One of the tasks for which they are best suited is the encoding of the contextual sense of words using contextualized embeddings. In this paper we propose a transparent, interpretable, and linguistically motivated strategy for encoding the contextual sense of words by modeling semantic compositionality. Particular attention is given to dependency relations and semantic notions such as selection preferences and paradigmatic classes. A partial implementation of the proposed model is carried out and compared with Transformer-based architectures for a given semantic task, namely the similarity calculation of word senses in context. The results obtained show that it is possible to be competitive with linguistically motivated models instead of using the black boxes underlying complex neural architectures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pablo Gamallo"
    ]
  },
  "http://arxiv.org/abs/2312.00678": {
    "title": "The Efficiency Spectrum of Large Language Models: An Algorithmic Survey",
    "volume": "Dec",
    "abstract": "The rapid growth of Large Language Models (LLMs) has been a driving force in transforming various domains, reshaping the artificial general intelligence landscape. However, the increasing computational and memory demands of these models present substantial challenges, hindering both academic research and practical applications. To address these issues, a wide array of methods, including both algorithmic and hardware solutions, have been developed to enhance the efficiency of LLMs. This survey delivers a comprehensive review of algorithmic advancements aimed at improving LLM efficiency. Unlike other surveys that typically focus on specific areas such as training or model compression, this paper examines the multi-faceted dimensions of efficiency essential for the end-to-end algorithmic development of LLMs. Specifically, it covers various topics related to efficiency, including scaling laws, data utilization, architectural innovations, training and tuning strategies, and inference techniques. This paper aims to serve as a valuable resource for researchers and practitioners, laying the groundwork for future innovations in this critical research area. Our repository of relevant references is maintained at url{https://github.com/tding1/Efficient-LLM-Survey}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Ding",
      "Tianyi Chen",
      "Haidong Zhu",
      "Jiachen Jiang",
      "Yiqi Zhong",
      "Jinxin Zhou",
      "Guangzhi Wang",
      "Zhihui Zhu",
      "Ilya Zharkov",
      "Luming Liang"
    ]
  },
  "http://arxiv.org/abs/2312.00662": {
    "title": "Nonparametric Variational Regularisation of Pretrained Transformers",
    "volume": "Dec",
    "abstract": "The current paradigm of large-scale pre-training and fine-tuning Transformer large language models has lead to significant improvements across the board in natural language processing. However, such large models are susceptible to overfitting to their training data, and as a result the models perform poorly when the domain changes. Also, due to the model's scale, the cost of fine-tuning the model to the new domain is large. Nonparametric Variational Information Bottleneck (NVIB) has been proposed as a regulariser for training cross-attention in Transformers, potentially addressing the overfitting problem. We extend the NVIB framework to replace all types of attention functions in Transformers, and show that existing pretrained Transformers can be reinterpreted as Nonparametric Variational (NV) models using a proposed identity initialisation. We then show that changing the initialisation introduces a novel, information-theoretic post-training regularisation in the attention mechanism, which improves out-of-domain generalisation without any training. This success supports the hypothesis that pretrained Transformers are implicitly NV Bayesian models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabio Fehr",
      "James Henderson"
    ]
  },
  "http://arxiv.org/abs/2312.00584": {
    "title": "The Ethics of Automating Legal Actors",
    "volume": "Dec",
    "abstract": "The introduction of large public legal datasets has brought about a renaissance in legal NLP. Many of these datasets are comprised of legal judgements - the product of judges deciding cases. This fact, together with the way machine learning works, means that several legal NLP models are models of judges. While some have argued for the automation of judges, in this position piece, we argue that automating the role of the judge raises difficult ethical challenges, in particular for common law legal systems. Our argument follows from the social role of the judge in actively shaping the law, rather than merely applying it. Since current NLP models come nowhere close to having the facilities necessary for this task, they should not be used to automate judges. Furthermore, even in the case the models could achieve human-level capabilities, there would still be remaining ethical concerns inherent in the automation of the legal process",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Josef Valvoda",
      "Alec Thompson",
      "Ryan Cotterell",
      "Simone Teufel"
    ]
  },
  "http://arxiv.org/abs/2312.00575": {
    "title": "Instruction-tuning Aligns LLMs to the Human Brain",
    "volume": "Dec",
    "abstract": "Instruction-tuning is a widely adopted method of finetuning that enables large language models (LLMs) to generate output that more closely resembles human responses to natural language queries, in many cases leading to human-level performance on diverse testbeds. However, it remains unclear whether instruction-tuning truly makes LLMs more similar to how humans process language. We investigate the effect of instruction-tuning on LLM-human similarity in two ways: (1) brain alignment, the similarity of LLM internal representations to neural activity in the human language system, and (2) behavioral alignment, the similarity of LLM and human behavior on a reading task. We assess 25 vanilla and instruction-tuned LLMs across three datasets involving humans reading naturalistic stories and sentences. We discover that instruction-tuning generally enhances brain alignment by an average of 6%, but does not have a similar effect on behavioral alignment. To identify the factors underlying LLM-brain alignment, we compute correlations between the brain alignment of LLMs and various model properties, such as model size, various problem-solving abilities, and performance on tasks requiring world knowledge spanning various domains. Notably, we find a strong positive correlation between brain alignment and model size (r = 0.95), as well as performance on tasks requiring world knowledge (r = 0.81). Our results demonstrate that instruction-tuning LLMs improves both world knowledge representations and brain alignment, suggesting that mechanisms that encode world knowledge in LLMs also improve representational alignment to the human brain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khai Loong Aw",
      "Syrielle Montariol",
      "Badr AlKhamissi",
      "Martin Schrimpf",
      "Antoine Bosselut"
    ]
  },
  "http://arxiv.org/abs/2312.00567": {
    "title": "Explanatory Argument Extraction of Correct Answers in Resident Medical Exams",
    "volume": "Dec",
    "abstract": "Developing the required technology to assist medical experts in their everyday activities is currently a hot topic in the Artificial Intelligence research field. Thus, a number of large language models (LLMs) and automated benchmarks have recently been proposed with the aim of facilitating information extraction in Evidence-Based Medicine (EBM) using natural language as a tool for mediating in human-AI interaction. The most representative benchmarks are limited to either multiple-choice or long-form answers and are available only in English. In order to address these shortcomings, in this paper we present a new dataset which, unlike previous work: (i) includes not only explanatory arguments for the correct answer, but also arguments to reason why the incorrect answers are not correct; (ii) the explanations are written originally by medical doctors to answer questions from the Spanish Residency Medical Exams. Furthermore, this new benchmark allows us to setup a novel extractive task which consists of identifying the explanation of the correct answer written by medical doctors. An additional benefit of our setting is that we can leverage the extractive QA paradigm to automatically evaluate performance of LLMs without resorting to costly manual evaluation by medical experts. Comprehensive experimentation with language models for Spanish shows that sometimes multilingual models fare better than monolingual ones, even outperforming models which have been adapted to the medical domain. Furthermore, results across the monolingual models are mixed, with supposedly smaller and inferior models performing competitively. In any case, the obtained results show that our novel dataset and approach can be an effective technique to help medical practitioners in identifying relevant evidence-based explanations for medical questions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Iakes Goenaga",
      "Aitziber Atutxa",
      "Koldo Gojenola",
      "Maite Oronoz",
      "Rodrigo Agerri"
    ]
  },
  "http://arxiv.org/abs/2312.00554": {
    "title": "Questioning Biases in Case Judgment Summaries: Legal Datasets or Large Language Models?",
    "volume": "Dec",
    "abstract": "The evolution of legal datasets and the advent of large language models (LLMs) have significantly transformed the legal field, particularly in the generation of case judgment summaries. However, a critical concern arises regarding the potential biases embedded within these summaries. This study scrutinizes the biases present in case judgment summaries produced by legal datasets and large language models. The research aims to analyze the impact of biases on legal decision making. By interrogating the accuracy, fairness, and implications of biases in these summaries, this study contributes to a better understanding of the role of technology in legal contexts and the implications for justice systems worldwide. In this study, we investigate biases wrt Gender-related keywords, Race-related keywords, Keywords related to crime against women, Country names and religious keywords. The study shows interesting evidences of biases in the outputs generated by the large language models and pre-trained abstractive summarization models. The reasoning behind these biases needs further studies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aniket Deroy",
      "Subhankar Maity"
    ]
  },
  "http://arxiv.org/abs/2312.00552": {
    "title": "Improving Unsupervised Relation Extraction by Augmenting Diverse Sentence Pairs",
    "volume": "Dec",
    "abstract": "Unsupervised relation extraction (URE) aims to extract relations between named entities from raw text without requiring manual annotations or pre-existing knowledge bases. In recent studies of URE, researchers put a notable emphasis on contrastive learning strategies for acquiring relation representations. However, these studies often overlook two important aspects: the inclusion of diverse positive pairs for contrastive learning and the exploration of appropriate loss functions. In this paper, we propose AugURE with both within-sentence pairs augmentation and augmentation through cross-sentence pairs extraction to increase the diversity of positive pairs and strengthen the discriminative power of contrastive learning. We also identify the limitation of noise-contrastive estimation (NCE) loss for relation representation learning and propose to apply margin loss for sentence pairs. Experiments on NYT-FB and TACRED datasets demonstrate that the proposed relation representation learning and a simple K-Means clustering achieves state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qing Wang",
      "Kang Zhou",
      "Qiao Qiao",
      "Yuepei Li",
      "Qi Li"
    ]
  },
  "http://arxiv.org/abs/2312.00536": {
    "title": "Trained MT Metrics Learn to Cope with Machine-translated References",
    "volume": "Dec",
    "abstract": "Neural metrics trained on human evaluations of MT tend to correlate well with human judgments, but their behavior is not fully understood. In this paper, we perform a controlled experiment and compare a baseline metric that has not been trained on human evaluations (Prism) to a trained version of the same metric (Prism+FT). Surprisingly, we find that Prism+FT becomes more robust to machine-translated references, which are a notorious problem in MT evaluation. This suggests that the effects of metric training go beyond the intended effect of improving overall correlation with human judgments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jannis Vamvas",
      "Tobias Domhan",
      "Sony Trenous",
      "Rico Sennrich",
      "Eva Hasler"
    ]
  },
  "http://arxiv.org/abs/2312.00525": {
    "title": "SurreyAI 2023 Submission for the Quality Estimation Shared Task",
    "volume": "Dec",
    "abstract": "Quality Estimation (QE) systems are important in situations where it is necessary to assess the quality of translations, but there is no reference available. This paper describes the approach adopted by the SurreyAI team for addressing the Sentence-Level Direct Assessment shared task in WMT23. The proposed approach builds upon the TransQuest framework, exploring various autoencoder pre-trained language models within the MonoTransQuest architecture using single and ensemble settings. The autoencoder pre-trained language models employed in the proposed systems are XLMV, InfoXLM-large, and XLMR-large. The evaluation utilizes Spearman and Pearson correlation coefficients, assessing the relationship between machine-predicted quality scores and human judgments for 5 language pairs (English-Gujarati, English-Hindi, English-Marathi, English-Tamil and English-Telugu). The MonoTQ-InfoXLM-large approach emerges as a robust strategy, surpassing all other individual models proposed in this study by significantly improving over the baseline for the majority of the language pairs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Archchana Sindhujan",
      "Diptesh Kanojia",
      "Constantin Orasan",
      "Tharindu Ranasinghe"
    ]
  },
  "http://arxiv.org/abs/2312.00849": {
    "title": "RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback",
    "volume": "Dec",
    "abstract": "Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in multimodal understanding, reasoning, and interaction. However, existing MLLMs prevalently suffer from serious hallucination problems, generating text that is not factually grounded in associated images. The problem makes existing MLLMs untrustworthy and thus impractical in real-world (especially high-stakes) applications. To address the challenge, we present RLHF-V, which enhances MLLM trustworthiness via behavior alignment from fine-grained correctional human feedback. Specifically, RLHF-V collects human preference in the form of segment-level corrections on hallucinations, and performs dense direct preference optimization over the human feedback. Comprehensive experiments on five benchmarks in both automatic and human evaluation show that, RLHF-V can enable substantially more trustworthy MLLM behaviors with promising data and computation efficiency. Remarkably, using 1.4k annotated data samples, RLHF-V significantly reduces the hallucination rate of the base MLLM by 34.8%, outperforming the concurrent LLaVA-RLHF trained on 10k annotated data. The final model achieves state-of-the-art performance in trustworthiness among open-source MLLMs, and shows better robustness than GPT-4V in preventing hallucinations aroused from over-generalization. We open-source our code, model, and data at https://github.com/RLHF-V/RLHF-V",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Yu",
      "Yuan Yao",
      "Haoye Zhang",
      "Taiwen He",
      "Yifeng Han",
      "Ganqu Cui",
      "Jinyi Hu",
      "Zhiyuan Liu",
      "Hai-Tao Zheng",
      "Maosong Sun",
      "Tat-Seng Chua"
    ]
  },
  "http://arxiv.org/abs/2312.00513": {
    "title": "Summarization-based Data Augmentation for Document Classification",
    "volume": "Dec",
    "abstract": "Despite the prevalence of pretrained language models in natural language understanding tasks, understanding lengthy text such as document is still challenging due to the data sparseness problem. Inspired by that humans develop their ability of understanding lengthy text from reading shorter text, we propose a simple yet effective summarization-based data augmentation, SUMMaug, for document classification. We first obtain easy-to-learn examples for the target document classification task by summarizing the input of the original training examples, while optionally merging the original labels to conform to the summarized input. We then use the generated pseudo examples to perform curriculum learning. Experimental results on two datasets confirmed the advantage of our method compared to existing baseline methods in terms of robustness and accuracy. We release our code and data at https://github.com/etsurin/summaug",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yueguan Wang",
      "Naoki Yoshinaga"
    ]
  },
  "http://arxiv.org/abs/2312.00480": {
    "title": "Japanese Tort-case Dataset for Rationale-supported Legal Judgment Prediction",
    "volume": "Dec",
    "abstract": "This paper presents the first dataset for Japanese Legal Judgment Prediction (LJP), the Japanese Tort-case Dataset (JTD), which features two tasks: tort prediction and its rationale extraction. The rationale extraction task identifies the court's accepting arguments from alleged arguments by plaintiffs and defendants, which is a novel task in the field. JTD is constructed based on annotated 3,477 Japanese Civil Code judgments by 41 legal experts, resulting in 7,978 instances with 59,697 of their alleged arguments from the involved parties. Our baseline experiments show the feasibility of the proposed two tasks, and our error analysis by legal experts identifies sources of errors and suggests future directions of the LJP research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroaki Yamada",
      "Takenobu Tokunaga",
      "Ryutaro Ohara",
      "Akira Tokutsu",
      "Keisuke Takeshita",
      "Mihoko Sumida"
    ]
  },
  "http://arxiv.org/abs/2312.00413": {
    "title": "Abstract Syntax Tree for Programming Language Understanding and Representation: How Far Are We?",
    "volume": "Dec",
    "abstract": "Programming language understanding and representation (a.k.a code representation learning) has always been a hot and challenging task in software engineering. It aims to apply deep learning techniques to produce numerical representations of the source code features while preserving its semantics. These representations can be used for facilitating subsequent code-related tasks. The abstract syntax tree (AST), a fundamental code feature, illustrates the syntactic information of the source code and has been widely used in code representation learning. However, there is still a lack of systematic and quantitative evaluation of how well AST-based code representation facilitates subsequent code-related tasks. In this paper, we first conduct a comprehensive empirical study to explore the effectiveness of the AST-based code representation in facilitating follow-up code-related tasks. To do so, we compare the performance of models trained with code token sequence (Token for short) based code representation and AST-based code representation on three popular types of code-related tasks. Surprisingly, the overall quantitative statistical results demonstrate that models trained with AST-based code representation consistently perform worse across all three tasks compared to models trained with Token-based code representation. Our further quantitative analysis reveals that models trained with AST-based code representation outperform models trained with Token-based code representation in certain subsets of samples across all three tasks. We also conduct comprehensive experiments to evaluate and reveal the impact of the choice of AST parsing/preprocessing/encoding methods on AST-based code representation and subsequent code-related tasks. Our study provides future researchers with detailed guidance on how to select solutions at each stage to fully exploit AST",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weisong Sun",
      "Chunrong Fang",
      "Yun Miao",
      "Yudu You",
      "Mengzhe Yuan",
      "Yuchen Chen",
      "Quanjun Zhang",
      "An Guo",
      "Xiang Chen",
      "Yang Liu",
      "Zhenyu Chen"
    ]
  },
  "http://arxiv.org/abs/2312.00407": {
    "title": "CoLLiE: Collaborative Training of Large Language Models in an Efficient Way",
    "volume": "Dec",
    "abstract": "Large language models (LLMs) are increasingly pivotal in a wide range of natural language processing tasks. Access to pre-trained models, courtesy of the open-source community, has made it possible to adapt these models to specific applications for enhanced performance. However, the substantial resources required for training these models necessitate efficient solutions. This paper introduces CoLLiE, an efficient library that facilitates collaborative training of large language models using 3D parallelism, parameter-efficient fine-tuning (PEFT) methods, and optimizers such as Lion, Adan, Sophia, LOMO and AdaLomo. With its modular design and comprehensive functionality, CoLLiE offers a balanced blend of efficiency, ease of use, and customization. CoLLiE has proven superior training efficiency in comparison with prevalent solutions in pre-training and fine-tuning scenarios. Furthermore, we provide an empirical evaluation of the correlation between model size and GPU memory consumption under different optimization methods, as well as an analysis of the throughput. Lastly, we carry out a comprehensive comparison of various optimizers and PEFT methods within the instruction-tuning context. CoLLiE is available at https://github.com/OpenLMLab/collie",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Lv",
      "Shuo Zhang",
      "Tianle Gu",
      "Shuhao Xing",
      "Jiawei Hong",
      "Keyu Chen",
      "Xiaoran Liu",
      "Yuqing Yang",
      "Honglin Guo",
      "Tengxiao Liu",
      "Yu Sun",
      "Qipeng Guo",
      "Hang Yan",
      "Xipeng Qiu"
    ]
  },
  "http://arxiv.org/abs/2312.00372": {
    "title": "Event-driven Real-time Retrieval in Web Search",
    "volume": "Dec",
    "abstract": "Information retrieval in real-time search presents unique challenges distinct from those encountered in classical web search. These challenges are particularly pronounced due to the rapid change of user search intent, which is influenced by the occurrence and evolution of breaking news events, such as earthquakes, elections, and wars. Previous dense retrieval methods, which primarily focused on static semantic representation, lack the capacity to capture immediate search intent, leading to inferior performance in retrieving the most recent event-related documents in time-sensitive scenarios. To address this issue, this paper expands the query with event information that represents real-time search intent. The Event information is then integrated with the query through a cross-attention mechanism, resulting in a time-context query representation. We further enhance the model's capacity for event representation through multi-task training. Since publicly available datasets such as MS-MARCO do not contain any event information on the query side and have few time-sensitive queries, we design an automatic data collection and annotation pipeline to address this issue, which includes ModelZoo-based Coarse Annotation and LLM-driven Fine Annotation processes. In addition, we share the training tricks such as two-stage training and hard negative sampling. Finally, we conduct a set of offline experiments on a million-scale production dataset to evaluate our approach and deploy an A/B testing in a real online system to verify the performance. Extensive experimental results demonstrate that our proposed approach significantly outperforms existing state-of-the-art baseline methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nan Yang",
      "Shusen Zhang",
      "Yannan Zhang",
      "Xiaoling Bai",
      "Hualong Deng",
      "Tianhua Zhou",
      "Jin Ma"
    ]
  },
  "http://arxiv.org/abs/2312.00353": {
    "title": "On Exploring the Reasoning Capability of Large Language Models with Knowledge Graphs",
    "volume": "Dec",
    "abstract": "This paper examines the capacity of LLMs to reason with knowledge graphs using their internal knowledge graph, i.e., the knowledge graph they learned during pre-training. Two research questions are formulated to investigate the accuracy of LLMs in recalling information from pre-training knowledge graphs and their ability to infer knowledge graph relations from context. To address these questions, we employ LLMs to perform four distinct knowledge graph reasoning tasks. Furthermore, we identify two types of hallucinations that may occur during knowledge reasoning with LLMs: content and ontology hallucination. Our experimental results demonstrate that LLMs can successfully tackle both simple and complex knowledge graph reasoning tasks from their own memory, as well as infer from input context",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pei-Chi Lo",
      "Yi-Hang Tsai",
      "Ee-Peng Lim",
      "San-Yih Hwang"
    ]
  },
  "http://arxiv.org/abs/2312.00349": {
    "title": "The Case for Scalable, Data-Driven Theory: A Paradigm for Scientific Progress in NLP",
    "volume": "Dec",
    "abstract": "I propose a paradigm for scientific progress in NLP centered around developing scalable, data-driven theories of linguistic structure. The idea is to collect data in tightly scoped, carefully defined ways which allow for exhaustive annotation of behavioral phenomena of interest, and then use machine learning to construct explanatory theories of these phenomena which can form building blocks for intelligible AI systems. After laying some conceptual groundwork, I describe several investigations into data-driven theories of shallow semantic structure using Question-Answer driven Semantic Role Labeling (QA-SRL), a schema for annotating verbal predicate-argument relations using highly constrained question-answer pairs. While this only scratches the surface of the complex language behaviors of interest in AI, I outline principles for data collection and theoretical modeling which can inform future scientific progress. This note summarizes and draws heavily on my PhD thesis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julian Michael"
    ]
  },
  "http://arxiv.org/abs/2312.00347": {
    "title": "RTQ: Rethinking Video-language Understanding Based on Image-text Model",
    "volume": "Dec",
    "abstract": "Recent advancements in video-language understanding have been established on the foundation of image-text models, resulting in promising outcomes due to the shared knowledge between images and videos. However, video-language understanding presents unique challenges due to the inclusion of highly complex semantic details, which result in information redundancy, temporal dependency, and scene complexity. Current techniques have only partially tackled these issues, and our quantitative analysis indicates that some of these methods are complementary. In light of this, we propose a novel framework called RTQ (Refine, Temporal model, and Query), which addresses these challenges simultaneously. The approach involves refining redundant information within frames, modeling temporal relations among frames, and querying task-specific information from the videos. Remarkably, our model demonstrates outstanding performance even in the absence of video-language pre-training, and the results are comparable with or superior to those achieved by state-of-the-art pre-training methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Wang",
      "Yaoyu Li",
      "Tian Gan",
      "Zheng Zhang",
      "Jingjing Lv",
      "Liqiang Nie"
    ]
  },
  "http://arxiv.org/abs/2312.00326": {
    "title": "Agent-OM: Leveraging Large Language Models for Ontology Matching",
    "volume": "Dec",
    "abstract": "Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM-based agents have become revolutionary in data engineering and have been applied creatively in various domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With thoughtful consideration of several specific challenges to leverage LLMs for OM, we propose a generic framework, namely Agent-OM, consisting of two Siamese agents for retrieval and matching, with a set of simple prompt-based OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve very close results to the best long-standing performance on simple OM tasks and significantly improve the performance on complex and few-shot OM tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangcheng Qiang",
      "Weiqing Wang",
      "Kerry Taylor"
    ]
  },
  "http://arxiv.org/abs/2312.00293": {
    "title": "PsyAttention: Psychological Attention Model for Personality Detection",
    "volume": "Dec",
    "abstract": "Work on personality detection has tended to incorporate psychological features from different personality models, such as BigFive and MBTI. There are more than 900 psychological features, each of which is helpful for personality detection. However, when used in combination, the application of different calculation standards among these features may result in interference between features calculated using distinct systems, thereby introducing noise and reducing performance. This paper adapts different psychological models in the proposed PsyAttention for personality detection, which can effectively encode psychological features, reducing their number by 85%. In experiments on the BigFive and MBTI models, PysAttention achieved average accuracy of 65.66% and 86.30%, respectively, outperforming state-of-the-art methods, indicating that it is effective at encoding psychological features",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baohua Zhang",
      "Yongyi Huang",
      "Wenyao Cui",
      "Huaping Zhang",
      "Jianyun Shang"
    ]
  },
  "http://arxiv.org/abs/2312.00292": {
    "title": "SEPSIS: I Can Catch Your Lies -- A New Paradigm for Deception Detection",
    "volume": "Dec",
    "abstract": "Deception is the intentional practice of twisting information. It is a nuanced societal practice deeply intertwined with human societal evolution, characterized by a multitude of facets. This research explores the problem of deception through the lens of psychology, employing a framework that categorizes deception into three forms: lies of omission, lies of commission, and lies of influence. The primary focus of this study is specifically on investigating only lies of omission. We propose a novel framework for deception detection leveraging NLP techniques. We curated an annotated dataset of 876,784 samples by amalgamating a popular large-scale fake news dataset and scraped news headlines from the Twitter handle of Times of India, a well-known Indian news media house. Each sample has been labeled with four layers, namely: (i) the type of omission (speculation, bias, distortion, sounds factual, and opinion), (ii) colors of lies(black, white, etc), and (iii) the intention of such lies (to influence, etc) (iv) topic of lies (political, educational, religious, etc). We present a novel multi-task learning pipeline that leverages the dataless merging of fine-tuned language models to address the deception detection task mentioned earlier. Our proposed model achieved an F1 score of 0.87, demonstrating strong performance across all layers including the type, color, intent, and topic aspects of deceptive content. Finally, our research explores the relationship between lies of omission and propaganda techniques. To accomplish this, we conducted an in-depth analysis, uncovering compelling findings. For instance, our analysis revealed a significant correlation between loaded language and opinion, shedding light on their interconnectedness. To encourage further research in this field, we will be making the models and dataset available with the MIT License, making it favorable for open-source research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anku Rani",
      "Dwip Dalal",
      "Shreya Gautam",
      "Pankaj Gupta",
      "Vinija Jain",
      "Aman Chadha",
      "Amit Sheth",
      "Amitava Das"
    ]
  },
  "http://arxiv.org/abs/2312.03749": {
    "title": "Conceptual Engineering Using Large Language Models",
    "volume": "Dec",
    "abstract": "We describe a method, based on Jennifer Nado's definition of classification procedures as targets of conceptual engineering, that implements such procedures using a large language model. We then apply this method using data from the Wikidata knowledge graph to evaluate concept definitions from two paradigmatic conceptual engineering projects: the International Astronomical Union's redefinition of PLANET and Haslanger's ameliorative analysis of WOMAN. We discuss implications of this work for the theory and practice of conceptual engineering. The code and data can be found on GitHub",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bradley P. Allen"
    ]
  },
  "http://arxiv.org/abs/2312.00277": {
    "title": "Text Attribute Control via Closed-Loop Disentanglement",
    "volume": "Dec",
    "abstract": "Changing an attribute of a text without changing the content usually requires to first disentangle the text into irrelevant attributes and content representations. After that, in the inference phase, the representation of one attribute is tuned to a different value, expecting that the corresponding attribute of the text can also be changed accordingly. The usual way of disentanglement is to add some constraints on the latent space of an encoder-decoder architecture, including adversarial-based constraints and mutual-information-based constraints. However, the previous semi-supervised processes of attribute change are usually not enough to guarantee the success of attribute change and content preservation. In this paper, we propose a novel approach to achieve a robust control of attributes while enhancing content preservation. In this approach, we use a semi-supervised contrastive learning method to encourage the disentanglement of attributes in latent spaces. Differently from previous works, we re-disentangle the reconstructed sentence and compare the re-disentangled latent space with the original latent space, which makes a closed-loop disentanglement process. This also helps content preservation. In addition, the contrastive learning method is also able to replace the role of minimizing mutual information and adversarial training in the disentanglement process, which alleviates the computation cost. We conducted experiments on three text datasets, including the Yelp Service review dataset, the Amazon Product review dataset, and the GoEmotions dataset. The experimental results show the effectiveness of our model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Sha",
      "Thomas Lukasiewicz"
    ]
  },
  "http://arxiv.org/abs/2312.00273": {
    "title": "Mark My Words: Analyzing and Evaluating Language Model Watermarks",
    "volume": "Dec",
    "abstract": "The capabilities of large language models have grown significantly in recent years and so too have concerns about their misuse. In this context, the ability to distinguish machine-generated text from human-authored content becomes important. Prior works have proposed numerous schemes to watermark text, which would benefit from a systematic evaluation framework. This work focuses on text watermarking techniques - as opposed to image watermarks - and proposes a comprehensive benchmark for them under different tasks as well as practical attacks. We focus on three main metrics: quality, size (e.g. the number of tokens needed to detect a watermark), and tamper-resistance. Current watermarking techniques are good enough to be deployed: Kirchenbauer et al. can watermark Llama2-7B-chat with no perceivable loss in quality in under 100 tokens, and with good tamper-resistance to simple attacks, regardless of temperature. We argue that watermark indistinguishability is too strong a requirement: schemes that slightly modify logit distributions outperform their indistinguishable counterparts with no noticeable loss in generation quality. We publicly release our benchmark",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julien Piet",
      "Chawin Sitawarin",
      "Vivian Fang",
      "Norman Mu",
      "David Wagner"
    ]
  },
  "http://arxiv.org/abs/2312.00220": {
    "title": "Multi-Modal Video Topic Segmentation with Dual-Contrastive Domain Adaptation",
    "volume": "Nov",
    "abstract": "Video topic segmentation unveils the coarse-grained semantic structure underlying videos and is essential for other video understanding tasks. Given the recent surge in multi-modal, relying solely on a single modality is arguably insufficient. On the other hand, prior solutions for similar tasks like video scene/shot segmentation cater to short videos with clear visual shifts but falter for long videos with subtle changes, such as livestreams. In this paper, we introduce a multi-modal video topic segmenter that utilizes both video transcripts and frames, bolstered by a cross-modal attention mechanism. Furthermore, we propose a dual-contrastive learning framework adhering to the unsupervised domain adaptation paradigm, enhancing our model's adaptability to longer, more semantically complex videos. Experiments on short and long video corpora demonstrate that our proposed solution, significantly surpasses baseline methods in terms of both accuracy and transferability, in both intra- and cross-domain settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linzi Xing",
      "Quan Tran",
      "Fabian Caba",
      "Franck Dernoncourt",
      "Seunghyun Yoon",
      "Zhaowen Wang",
      "Trung Bui",
      "Giuseppe Carenini"
    ]
  },
  "http://arxiv.org/abs/2312.00214": {
    "title": "Relevance-guided Neural Machine Translation",
    "volume": "Nov",
    "abstract": "With the advent of the Transformer architecture, Neural Machine Translation (NMT) results have shown great improvement lately. However, results in low-resource conditions still lag behind in both bilingual and multilingual setups, due to the limited amount of available monolingual and/or parallel data; hence, the need for methods addressing data scarcity in an efficient, and explainable way, is eminent. We propose an explainability-based training approach for NMT, applied in Unsupervised and Supervised model training, for translation of three languages of varying resources, French, Gujarati, Kazakh, to and from English. Our results show our method can be promising, particularly when training in low-resource conditions, outperforming simple training baselines; though the improvement is marginal, it sets the ground for further exploration of the approach and the parameters, and its extension to other languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isidora Chara Tourni",
      "Derry Wijaya"
    ]
  },
  "http://arxiv.org/abs/2312.00194": {
    "title": "Robust Concept Erasure via Kernelized Rate-Distortion Maximization",
    "volume": "Nov",
    "abstract": "Distributed representations provide a vector space that captures meaningful relationships between data instances. The distributed nature of these representations, however, entangles together multiple attributes or concepts of data instances (e.g., the topic or sentiment of a text, characteristics of the author (age, gender, etc), etc). Recent work has proposed the task of concept erasure, in which rather than making a concept predictable, the goal is to remove an attribute from distributed representations while retaining other information from the original representation space as much as possible. In this paper, we propose a new distance metric learning-based objective, the Kernelized Rate-Distortion Maximizer (KRaM), for performing concept erasure. KRaM fits a transformation of representations to match a specified distance measure (defined by a labeled concept to erase) using a modified rate-distortion function. Specifically, KRaM's objective function aims to make instances with similar concept labels dissimilar in the learned representation space while retaining other information. We find that optimizing KRaM effectively erases various types of concepts: categorical, continuous, and vector-valued variables from data representations across diverse domains. We also provide a theoretical analysis of several properties of KRaM's objective. To assess the quality of the learned representations, we propose an alignment score to evaluate their similarity with the original representation space. Additionally, we conduct experiments to showcase KRaM's efficacy in various settings, from erasing binary gender variables in word embeddings to vector-valued variables in GPT-3 representations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Somnath Basu Roy Chowdhury",
      "Nicholas Monath",
      "Avinava Dubey",
      "Amr Ahmed",
      "Snigdha Chaturvedi"
    ]
  },
  "http://arxiv.org/abs/2312.00174": {
    "title": "Compression of end-to-end non-autoregressive image-to-speech system for low-resourced devices",
    "volume": "Nov",
    "abstract": "People with visual impairments have difficulty accessing touchscreen-enabled personal computing devices like mobile phones and laptops. The image-to-speech (ITS) systems can assist them in mitigating this problem, but their huge model size makes it extremely hard to be deployed on low-resourced embedded devices. In this paper, we aim to overcome this challenge by developing an efficient endto-end neural architecture for generating audio from tiny segments of display content on low-resource devices. We introduced a vision transformers-based image encoder and utilized knowledge distillation to compress the model from 6.1 million to 2.46 million parameters. Human and automatic evaluation results show that our approach leads to a very minimal drop in performance and can speed up the inference time by 22%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gokul Srinivasagan",
      "Michael Deisher",
      "Munir Georges"
    ]
  },
  "http://arxiv.org/abs/2312.00168": {
    "title": "Navigating News Narratives: A Media Bias Analysis Dataset",
    "volume": "Nov",
    "abstract": "The proliferation of biased news narratives across various media platforms has become a prominent challenge, influencing public opinion on critical topics like politics, health, and climate change. This paper introduces the \"Navigating News Narratives: A Media Bias Analysis Dataset\", a comprehensive dataset to address the urgent need for tools to detect and analyze media bias. This dataset encompasses a broad spectrum of biases, making it a unique and valuable asset in the field of media studies and artificial intelligence. The dataset is available at https://figshare.com/articles/dataset/news-media-bias_data_json/24422122",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaina Raza"
    ]
  },
  "http://arxiv.org/abs/2312.00115": {
    "title": "A Video is Worth 10,000 Words: Training and Benchmarking with Diverse Captions for Better Long Video Retrieval",
    "volume": "Nov",
    "abstract": "Existing long video retrieval systems are trained and tested in the paragraph-to-video retrieval regime, where every long video is described by a single long paragraph. This neglects the richness and variety of possible valid descriptions of a video, which could be described in moment-by-moment detail, or in a single phrase summary, or anything in between. To provide a more thorough evaluation of the capabilities of long video retrieval systems, we propose a pipeline that leverages state-of-the-art large language models to carefully generate a diverse set of synthetic captions for long videos. We validate this pipeline's fidelity via rigorous human inspection. We then benchmark a representative set of video language models on these synthetic captions using a few long video datasets, showing that they struggle with the transformed data, especially the shortest captions. We also propose a lightweight fine-tuning method, where we use a contrastive loss to learn a hierarchical embedding loss based on the differing levels of information among the various captions. Our method improves performance both on the downstream paragraph-to-video retrieval task (+1.1% R@1 on ActivityNet), as well as for the various long video retrieval metrics we compute using our synthetic data (+3.6% R@1 for short descriptions on ActivityNet). For data access and other details, please refer to our project website at https://mgwillia.github.io/10k-words",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Gwilliam",
      "Michael Cogswell",
      "Meng Ye",
      "Karan Sikka",
      "Abhinav Shrivastava",
      "Ajay Divakaran"
    ]
  },
  "http://arxiv.org/abs/2311.18812": {
    "title": "What Do Llamas Really Think? Revealing Preference Biases in Language Model Representations",
    "volume": "Nov",
    "abstract": "Do large language models (LLMs) exhibit sociodemographic biases, even when they decline to respond? To bypass their refusal to \"speak,\" we study this research question by probing contextualized embeddings and exploring whether this bias is encoded in its latent representations. We propose a logistic Bradley-Terry probe which predicts word pair preferences of LLMs from the words' hidden vectors. We first validate our probe on three pair preference tasks and thirteen LLMs, where we outperform the word embedding association test (WEAT), a standard approach in testing for implicit association, by a relative 27% in error rate. We also find that word pair preferences are best represented in the middle layers. Next, we transfer probes trained on harmless tasks (e.g., pick the larger number) to controversial ones (compare ethnicities) to examine biases in nationality, politics, religion, and gender. We observe substantial bias for all target classes: for instance, the Mistral model implicitly prefers Europe to Africa, Christianity to Judaism, and left-wing to right-wing politics, despite declining to answer. This suggests that instruction fine-tuning does not necessarily debias contextualized embeddings. Our codebase is at https://github.com/castorini/biasprobe",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raphael Tang",
      "Xinyu Zhang",
      "Jimmy Lin",
      "Ferhan Ture"
    ]
  },
  "http://arxiv.org/abs/2311.18805": {
    "title": "Unnatural Error Correction: GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text",
    "volume": "Nov",
    "abstract": "While Large Language Models (LLMs) have achieved remarkable performance in many tasks, much about their inner workings remains unclear. In this study, we present novel experimental insights into the resilience of LLMs, particularly GPT-4, when subjected to extensive character-level permutations. To investigate this, we first propose the Scrambled Bench, a suite designed to measure the capacity of LLMs to handle scrambled input, in terms of both recovering scrambled sentences and answering questions given scrambled context. The experimental results indicate that most powerful LLMs demonstrate the capability akin to typoglycemia, a phenomenon where humans can understand the meaning of words even when the letters within those words are scrambled, as long as the first and last letters remain in place. More surprisingly, we found that only GPT-4 nearly flawlessly processes inputs with unnatural errors, even under the extreme condition, a task that poses significant challenges for other LLMs and often even for humans. Specifically, GPT-4 can almost perfectly reconstruct the original sentences from scrambled ones, decreasing the edit distance by 95%, even when all letters within each word are entirely scrambled. It is counter-intuitive that LLMs can exhibit such resilience despite severe disruption to input tokenization caused by scrambled text",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Cao",
      "Takeshi Kojima",
      "Yutaka Matsuo",
      "Yusuke Iwasawa"
    ]
  },
  "http://arxiv.org/abs/2311.18803": {
    "title": "BioCLIP: A Vision Foundation Model for the Tree of Life",
    "volume": "Dec",
    "abstract": "Images of the natural world, collected by a variety of cameras, from drones to individual phones, are increasingly abundant sources of biological information. There is an explosion of computational methods and tools, particularly computer vision, for extracting biologically relevant information from images for science and conservation. Yet most of these are bespoke approaches designed for a specific task and are not easily adaptable or extendable to new questions, contexts, and datasets. A vision model for general organismal biology questions on images is of timely need. To approach this, we curate and release TreeOfLife-10M, the largest and most diverse ML-ready dataset of biology images. We then develop BioCLIP, a foundation model for the tree of life, leveraging the unique properties of biology captured by TreeOfLife-10M, namely the abundance and variety of images of plants, animals, and fungi, together with the availability of rich structured biological knowledge. We rigorously benchmark our approach on diverse fine-grained biology classification tasks, and find that BioCLIP consistently and substantially outperforms existing baselines (by 17% to 20% absolute). Intrinsic evaluation reveals that BioCLIP has learned a hierarchical representation conforming to the tree of life, shedding light on its strong generalizability. Our code, models and data will be made available at https://github.com/Imageomics/bioclip",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Stevens",
      "Jiaman Wu",
      "Matthew J Thompson",
      "Elizabeth G Campolongo",
      "Chan Hee Song",
      "David Edward Carlyn",
      "Li Dong",
      "Wasila M Dahdul",
      "Charles Stewart",
      "Tanya Berger-Wolf",
      "Wei-Lun Chao",
      "Yu Su"
    ]
  },
  "http://arxiv.org/abs/2311.18799": {
    "title": "X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning",
    "volume": "Nov",
    "abstract": "Vision-language pre-training and instruction tuning have demonstrated general-purpose capabilities in 2D visual reasoning tasks by aligning visual encoders with state-of-the-art large language models (LLMs). In this paper, we introduce a simple, yet effective, cross-modality framework built atop frozen LLMs that allows the integration of various modalities without extensive modality-specific customization. To facilitate instruction-modality fine-tuning, we collect high-quality instruction tuning data in an automatic and scalable manner, composed of 24K QA samples for audio and 250K QA samples for 3D. Leveraging instruction-aware representations, our model performs comparably with leading-edge counterparts without the need of extensive modality-specific pre-training or customization. Furthermore, our approach demonstrates cross-modal reasoning abilities across two or more input modalities, despite each modality projection being trained individually. To study the model's cross-modal abilities, we contribute a novel Discriminative Cross-modal Reasoning (DisCRn) evaluation task, comprising 9K audio-video QA samples and 28K image-3D QA samples that require the model to reason discriminatively across disparate input modalities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Artemis Panagopoulou",
      "Le Xue",
      "Ning Yu",
      "Junnan Li",
      "Dongxu Li",
      "Shafiq Joty",
      "Ran Xu",
      "Silvio Savarese",
      "Caiming Xiong",
      "Juan Carlos Niebles"
    ]
  },
  "http://arxiv.org/abs/2311.18778": {
    "title": "Mavericks at BLP-2023 Task 1: Ensemble-based Approach Using Language Models for Violence Inciting Text Detection",
    "volume": "Nov",
    "abstract": "This paper presents our work for the Violence Inciting Text Detection shared task in the First Workshop on Bangla Language Processing. Social media has accelerated the propagation of hate and violence-inciting speech in society. It is essential to develop efficient mechanisms to detect and curb the propagation of such texts. The problem of detecting violence-inciting texts is further exacerbated in low-resource settings due to sparse research and less data. The data provided in the shared task consists of texts in the Bangla language, where each example is classified into one of the three categories defined based on the types of violence-inciting texts. We try and evaluate several BERT-based models, and then use an ensemble of the models as our final submission. Our submission is ranked 10th in the final leaderboard of the shared task with a macro F1 score of 0.737",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saurabh Page",
      "Sudeep Mangalvedhekar",
      "Kshitij Deshpande",
      "Tanmay Chavan",
      "Sheetal Sonawane"
    ]
  },
  "http://arxiv.org/abs/2311.18775": {
    "title": "CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation",
    "volume": "Nov",
    "abstract": "We present CoDi-2, a versatile and interactive Multimodal Large Language Model (MLLM) that can follow complex multimodal interleaved instructions, conduct in-context learning (ICL), reason, chat, edit, etc., in an any-to-any input-output modality paradigm. By aligning modalities with language for both encoding and generation, CoDi-2 empowers Large Language Models (LLMs) to not only understand complex modality-interleaved instructions and in-context examples, but also autoregressively generate grounded and coherent multimodal outputs in the continuous feature space. To train CoDi-2, we build a large-scale generation dataset encompassing in-context multimodal instructions across text, vision, and audio. CoDi-2 demonstrates a wide range of zero-shot capabilities for multimodal generation, such as in-context learning, reasoning, and compositionality of any-to-any modality generation through multi-round interactive conversation. CoDi-2 surpasses previous domain-specific models on tasks such as subject-driven image generation, vision transformation, and audio editing. CoDi-2 signifies a substantial breakthrough in developing a comprehensive multimodal foundation model adept at interpreting in-context language-vision-audio interleaved instructions and producing multimodal outputs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zineng Tang",
      "Ziyi Yang",
      "Mahmoud Khademi",
      "Yang Liu",
      "Chenguang Zhu",
      "Mohit Bansal"
    ]
  },
  "http://arxiv.org/abs/2311.18765": {
    "title": "MLLMs-Augmented Visual-Language Representation Learning",
    "volume": "Dec",
    "abstract": "Visual-language pre-training (VLP) have achieved remarkable success in multi-modal tasks, largely attributed to the availability of large-scale image-text datasets. In this work, we demonstrate that multi-modal large language models (MLLMs) can enhance visual-language representation learning by improving data quality. Our approach is simple, utilizing MLLMs to extend multiple captions for each image. To prevent the bias that introduced by MLLMs' hallucinations and intrinsic caption styles, we propose a \"text shearing\" to keep the lengths of extended captions identical to the originals. In image-text retrieval, our method consistently obtains 5.6 ~ 35.0% and 16.8 ~ 46.1% improvement on R@1 under the fine-tuning and zero-shot settings, respectively. Notably, our zero-shot results are comparable to fine-tuning on target datasets, which encourages more exploration on the versatile use of MLLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanqing Liu",
      "Kai Wang",
      "Wenqi Shao",
      "Ping Luo",
      "Yu Qiao",
      "Mike Zheng Shou",
      "Kaipeng Zhang",
      "Yang You"
    ]
  },
  "http://arxiv.org/abs/2311.18761": {
    "title": "Can training neural language models on a curriculum with developmentally plausible data improve alignment with human reading behavior?",
    "volume": "Nov",
    "abstract": "The use of neural language models to model human behavior has met with mixed success. While some work has found that the surprisal estimates from these models can be used to predict a wide range of human neural and behavioral responses, other work studying more complex syntactic phenomena has found that these surprisal estimates generate incorrect behavioral predictions. This paper explores the extent to which the misalignment between empirical and model-predicted behavior can be minimized by training models on more developmentally plausible data, such as in the BabyLM Challenge. We trained teacher language models on the BabyLM \"strict-small\" dataset and used sentence level surprisal estimates from these teacher models to create a curriculum. We found tentative evidence that our curriculum made it easier for models to acquire linguistic knowledge from the training data: on the subset of tasks in the BabyLM challenge suite evaluating models' grammatical knowledge of English, models first trained on the BabyLM data curriculum and then on a few randomly ordered training epochs performed slightly better than models trained on randomly ordered epochs alone. This improved linguistic knowledge acquisition did not result in better alignment with human reading behavior, however: models trained on the BabyLM dataset (with or without a curriculum) generated predictions that were as misaligned with human behavior as models trained on larger less curated datasets. This suggests that training on developmentally plausible datasets alone is likely insufficient to generate language models capable of accurately predicting human language processing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aryaman Chobey",
      "Oliver Smith",
      "Anzi Wang",
      "Grusha Prasad"
    ]
  },
  "http://arxiv.org/abs/2311.18760": {
    "title": "TaskBench: Benchmarking Large Language Models for Task Automation",
    "volume": "Nov",
    "abstract": "Recently, the incredible progress of large language models (LLMs) has ignited the spark of task automation, which decomposes the complex tasks described by user instructions into sub-tasks, and invokes external tools to execute them, and plays a central role in autonomous agents. However, there lacks a systematic and standardized benchmark to foster the development of LLMs in task automation. To this end, we introduce TaskBench to evaluate the capability of LLMs in task automation. Specifically, task automation can be formulated into three critical stages: task decomposition, tool invocation, and parameter prediction to fulfill user intent. This complexity makes data collection and evaluation more challenging compared to common NLP tasks. To generate high-quality evaluation datasets, we introduce the concept of Tool Graph to represent the decomposed tasks in user intent, and adopt a back-instruct method to simulate user instruction and annotations. Furthermore, we propose TaskEval to evaluate the capability of LLMs from different aspects, including task decomposition, tool invocation, and parameter prediction. Experimental results demonstrate that TaskBench can effectively reflects the capability of LLMs in task automation. Benefiting from the mixture of automated data construction and human verification, TaskBench achieves a high consistency compared to the human evaluation, which can be utilized as a comprehensive and faithful benchmark for LLM-based autonomous agents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongliang Shen",
      "Kaitao Song",
      "Xu Tan",
      "Wenqi Zhang",
      "Kan Ren",
      "Siyu Yuan",
      "Weiming Lu",
      "Dongsheng Li",
      "Yueting Zhuang"
    ]
  },
  "http://arxiv.org/abs/2311.18751": {
    "title": "Language Model Agents Suffer from Compositional Generalization in Web Automation",
    "volume": "Nov",
    "abstract": "Language model agents (LMA) recently emerged as a promising paradigm on muti-step decision making tasks, often outperforming humans and other reinforcement learning agents. Despite the promise, their performance on real-world applications that often involve combinations of tasks is still underexplored. In this work, we introduce a new benchmark, called CompWoB -- 50 new compositional web automation tasks reflecting more realistic assumptions. We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks. On the other hand, transferred LMAs (finetuned only on base tasks) show less generalization gap, dropping from 85.4% to 54.8%. By balancing data distribution across tasks, we train a new model, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.5%). While these highlight the promise of small-scale finetuned and transferred models for compositional generalization, their performance further degrades under different instruction compositions changing combinational order. In contrast to the recent remarkable success of LMA, our benchmark and detailed analysis emphasize the necessity of building LMAs that are robust and generalizable to task compositionality for real-world deployment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroki Furuta",
      "Yutaka Matsuo",
      "Aleksandra Faust",
      "Izzeddin Gur"
    ]
  },
  "http://arxiv.org/abs/2311.18743": {
    "title": "AlignBench: Benchmarking Chinese Alignment of Large Language Models",
    "volume": "Dec",
    "abstract": "Alignment has become a critical step for instruction-tuned Large Language Models (LLMs) to become helpful assistants. However, effective evaluation of alignment for emerging Chinese LLMs is still significantly lacking, calling for real-scenario grounded, open-ended, challenging and automatic evaluations tailored for alignment. To fill in this gap, we introduce AlignBench, a comprehensive multi-dimensional benchmark for evaluating LLMs' alignment in Chinese. Equipped with a human-in-the-loop data curation pipeline, our benchmark employs a rule-calibrated multi-dimensional LLM-as-Judge with Chain-of-Thought to generate explanations and final ratings as evaluations, ensuring high reliability and interpretability. Furthermore, we developed a dedicated companion evaluator LLM -- CritiqueLLM, which recovers 95\\% of GPT-4's evaluation ability and will be provided via public APIs to researchers for evaluation of alignment in Chinese LLMs. All evaluation codes, data, and LLM generations are available at \\url{https://github.com/THUDM/AlignBench}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Liu",
      "Xuanyu Lei",
      "Shengyuan Wang",
      "Yue Huang",
      "Zhuoer Feng",
      "Bosi Wen",
      "Jiale Cheng",
      "Pei Ke",
      "Yifan Xu",
      "Weng Lam Tam",
      "Xiaohan Zhang",
      "Lichao Sun",
      "Hongning Wang",
      "Jing Zhang",
      "Minlie Huang",
      "Yuxiao Dong",
      "Jie Tang"
    ]
  },
  "http://arxiv.org/abs/2311.18739": {
    "title": "Mavericks at NADI 2023 Shared Task: Unravelling Regional Nuances through Dialect Identification using Transformer-based Approach",
    "volume": "Nov",
    "abstract": "In this paper, we present our approach for the \"Nuanced Arabic Dialect Identification (NADI) Shared Task 2023\". We highlight our methodology for subtask 1 which deals with country-level dialect identification. Recognizing dialects plays an instrumental role in enhancing the performance of various downstream NLP tasks such as speech recognition and translation. The task uses the Twitter dataset (TWT-2023) that encompasses 18 dialects for the multi-class classification problem. Numerous transformer-based models, pre-trained on Arabic language, are employed for identifying country-level dialects. We fine-tune these state-of-the-art models on the provided dataset. The ensembling method is leveraged to yield improved performance of the system. We achieved an F1-score of 76.65 (11th rank on the leaderboard) on the test dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vedant Deshpande",
      "Yash Patwardhan",
      "Kshitij Deshpande",
      "Sudeep Mangalvedhekar",
      "Ravindra Murumkar"
    ]
  },
  "http://arxiv.org/abs/2311.18730": {
    "title": "Mavericks at ArAIEval Shared Task: Towards a Safer Digital Space -- Transformer Ensemble Models Tackling Deception and Persuasion",
    "volume": "Nov",
    "abstract": "In this paper, we highlight our approach for the \"Arabic AI Tasks Evaluation (ArAiEval) Shared Task 2023\". We present our approaches for task 1-A and task 2-A of the shared task which focus on persuasion technique detection and disinformation detection respectively. Detection of persuasion techniques and disinformation has become imperative to avoid distortion of authentic information. The tasks use multigenre snippets of tweets and news articles for the given binary classification problem. We experiment with several transformer-based models that were pre-trained on the Arabic language. We fine-tune these state-of-the-art models on the provided dataset. Ensembling is employed to enhance the performance of the systems. We achieved a micro F1-score of 0.742 on task 1-A (8th rank on the leaderboard) and 0.901 on task 2-A (7th rank on the leaderboard) respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sudeep Mangalvedhekar",
      "Kshitij Deshpande",
      "Yash Patwardhan",
      "Vedant Deshpande",
      "Ravindra Murumkar"
    ]
  },
  "http://arxiv.org/abs/2311.18727": {
    "title": "Automatic Functional Differentiation in JAX",
    "volume": "Nov",
    "abstract": "We extend JAX with the capability to automatically differentiate higher-order functions (functionals and operators). By representing functions as a generalization of arrays, we seamlessly use JAX's existing primitive system to implement higher-order functions. We present a set of primitive operators that serve as foundational building blocks for constructing several key types of functionals. For every introduced primitive operator, we derive and implement both linearization and transposition rules, aligning with JAX's internal protocols for forward and reverse mode automatic differentiation. This enhancement allows for functional differentiation in the same syntax traditionally use for functions. The resulting functional gradients are themselves functions ready to be invoked in python. We showcase this tool's efficacy and simplicity through applications where functional derivatives are indispensable. The source code of this work is released at https://github.com/sail-sg/autofd",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Min Lin"
    ]
  },
  "http://arxiv.org/abs/2311.18712": {
    "title": "CoRec: An Easy Approach for Coordination Recognition",
    "volume": "Nov",
    "abstract": "In this paper, we observe and address the challenges of the coordination recognition task. Most existing methods rely on syntactic parsers to identify the coordinators in a sentence and detect the coordination boundaries. However, state-of-the-art syntactic parsers are slow and suffer from errors, especially for long and complicated sentences. To better solve the problems, we propose a pipeline model COordination RECognizer (CoRec). It consists of two components: coordinator identifier and conjunct boundary detector. The experimental results on datasets from various domains demonstrate the effectiveness and efficiency of the proposed method. Further experiments show that CoRec positively impacts downstream tasks, improving the yield of state-of-the-art Open IE models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qing Wang",
      "Haojie Jia",
      "Wenfei Song",
      "Qi Li"
    ]
  },
  "http://arxiv.org/abs/2311.18711": {
    "title": "Women Are Beautiful, Men Are Leaders: Gender Stereotypes in Machine Translation and Language Modeling",
    "volume": "Nov",
    "abstract": "We present GEST -- a new dataset for measuring gender-stereotypical reasoning in masked LMs and English-to-X machine translation systems. GEST contains samples that are compatible with 9 Slavic languages and English for 16 gender stereotypes about men and women (e.g., Women are beautiful, Men are leaders). The definition of said stereotypes was informed by gender experts. We used GEST to evaluate 11 masked LMs and 4 machine translation systems. We discovered significant and consistent amounts of stereotypical reasoning in almost all the evaluated models and languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matúš Pikuliak",
      "Andrea Hrckova",
      "Stefan Oresko",
      "Marián Šimko"
    ]
  },
  "http://arxiv.org/abs/2311.18702": {
    "title": "CritiqueLLM: Scaling LLM-as-Critic for Effective and Explainable Evaluation of Large Language Model Generation",
    "volume": "Nov",
    "abstract": "Since the natural language processing (NLP) community started to make large language models (LLMs), such as GPT-4, act as a critic to evaluate the quality of generated texts, most of them only train a critique generation model of a specific scale on specific datasets. We argue that a comprehensive investigation on the key factor of LLM-based evaluation models, such as scaling properties, is lacking, so that it is still inconclusive whether these models have potential to replace GPT-4's evaluation in practical scenarios. In this paper, we propose a new critique generation model called CritiqueLLM, which includes a dialogue-based prompting method for high-quality referenced / reference-free evaluation data. Experimental results show that our model can achieve comparable evaluation performance to GPT-4 especially in system-level correlations, and even outperform GPT-4 in 3 out of 8 tasks in a challenging reference-free setting. We conduct detailed analysis to show promising scaling properties of our model in the quality of generated critiques. We also demonstrate that our generated critiques can act as scalable feedback to directly improve the generation quality of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pei Ke",
      "Bosi Wen",
      "Zhuoer Feng",
      "Xiao Liu",
      "Xuanyu Lei",
      "Jiale Cheng",
      "Shengyuan Wang",
      "Aohan Zeng",
      "Yuxiao Dong",
      "Hongning Wang",
      "Jie Tang",
      "Minlie Huang"
    ]
  },
  "http://arxiv.org/abs/2311.18681": {
    "title": "RaDialog: A Large Vision-Language Model for Radiology Report Generation and Conversational Assistance",
    "volume": "Nov",
    "abstract": "Conversational AI tools that can generate and discuss clinically correct radiology reports for a given medical image have the potential to transform radiology. Such a human-in-the-loop radiology assistant could facilitate a collaborative diagnostic process, thus saving time and improving the quality of reports. Towards this goal, we introduce RaDialog, the first thoroughly evaluated and publicly available large vision-language model for radiology report generation and interactive dialog. RaDialog effectively integrates visual image features and structured pathology findings with a large language model (LLM) while simultaneously adapting it to a specialized domain using parameter-efficient fine-tuning. To keep the conversational abilities of the underlying LLM, we propose a comprehensive, semi-automatically labeled, image-grounded instruct dataset for chest X-ray radiology tasks. By training with this dataset, our method achieves state-of-the-art clinical correctness in report generation and shows impressive abilities in interactive tasks such as correcting reports and answering questions, serving as a foundational step toward clinical dialog systems. Our code is available on github: https://github.com/ChantalMP/RaDialog",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chantal Pellegrini",
      "Ege Özsoy",
      "Benjamin Busam",
      "Nassir Navab",
      "Matthias Keicher"
    ]
  },
  "http://arxiv.org/abs/2311.18658": {
    "title": "ArcMMLU: A Library and Information Science Benchmark for Large Language Models",
    "volume": "Nov",
    "abstract": "In light of the rapidly evolving capabilities of large language models (LLMs), it becomes imperative to develop rigorous domain-specific evaluation benchmarks to accurately assess their capabilities. In response to this need, this paper introduces ArcMMLU, a specialized benchmark tailored for the Library & Information Science (LIS) domain in Chinese. This benchmark aims to measure the knowledge and reasoning capability of LLMs within four key sub-domains: Archival Science, Data Science, Library Science, and Information Science. Following the format of MMLU/CMMLU, we collected over 6,000 high-quality questions for the compilation of ArcMMLU. This extensive compilation can reflect the diverse nature of the LIS domain and offer a robust foundation for LLM evaluation. Our comprehensive evaluation reveals that while most mainstream LLMs achieve an average accuracy rate above 50% on ArcMMLU, there remains a notable performance gap, suggesting substantial headroom for refinement in LLM capabilities within the LIS domain. Further analysis explores the effectiveness of few-shot examples on model performance and highlights challenging questions where models consistently underperform, providing valuable insights for targeted improvements. ArcMMLU fills a critical gap in LLM evaluations within the Chinese LIS domain and paves the way for future development of LLMs tailored to this specialized area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shitou Zhang",
      "Zuchao Li",
      "Xingshen Liu",
      "Liming Yang",
      "Ping Wang"
    ]
  },
  "http://arxiv.org/abs/2312.00100": {
    "title": "Introducing Rhetorical Parallelism Detection: A New Task with Datasets, Metrics, and Baselines",
    "volume": "Nov",
    "abstract": "Rhetoric, both spoken and written, involves not only content but also style. One common stylistic tool is $\\textit{parallelism}$: the juxtaposition of phrases which have the same sequence of linguistic ($\\textit{e.g.}$, phonological, syntactic, semantic) features. Despite the ubiquity of parallelism, the field of natural language processing has seldom investigated it, missing a chance to better understand the nature of the structure, meaning, and intent that humans convey. To address this, we introduce the task of $\\textit{rhetorical parallelism detection}$. We construct a formal definition of it; we provide one new Latin dataset and one adapted Chinese dataset for it; we establish a family of metrics to evaluate performance on it; and, lastly, we create baseline systems and novel sequence labeling schemes to capture it. On our strictest metric, we attain $F_{1}$ scores of $0.40$ and $0.43$ on our Latin and Chinese datasets, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephen Bothwell",
      "Justin DeBenedetto",
      "Theresa Crnkovich",
      "Hildegund Müller",
      "David Chiang"
    ]
  },
  "http://arxiv.org/abs/2311.18609": {
    "title": "ArthModel: Enhance Arithmetic Skills to Large Language Model",
    "volume": "Nov",
    "abstract": "With the great success of ChatGPT, the research of large language models has become increasingly popular. However, the models have several limitations, such as toxicity and pool performance of arithmetic solving. Meanwhile, LLM may have some potential abilities that have yet to be exploited. In this paper, we choose a different way to enhance the arithmetic ability of LLM. We propose to train LLM to generate a postfix expression related to the arithmetic problem and incorporate it with small pretrained models. Moreover, this small model transfers the token embeddings into real dense numbers and invokes native functions of a deep learning platform to get the correct answer. To generate the final result, we propose prompt injection for adding the result outputs by the small model to LLM. This work provides different ways of thinking, training and using a language model. The codes and models will be released at \\url{https://github.com/eteced/arithmetic_finetuning_v1}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingdi Guo"
    ]
  },
  "http://arxiv.org/abs/2311.18580": {
    "title": "FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality, Fairness, Toxicity",
    "volume": "Nov",
    "abstract": "The widespread of generative artificial intelligence has heightened concerns about the potential harms posed by AI-generated texts, primarily stemming from factoid, unfair, and toxic content. Previous researchers have invested much effort in assessing the harmlessness of generative language models. However, existing benchmarks are struggling in the era of large language models (LLMs), due to the stronger language generation and instruction following capabilities, as well as wider applications. In this paper, we propose FFT, a new benchmark with 2116 elaborated-designed instances, for LLM harmlessness evaluation with factuality, fairness, and toxicity. To investigate the potential harms of LLMs, we evaluate 9 representative LLMs covering various parameter scales, training stages, and creators. Experiments show that the harmlessness of LLMs is still under-satisfactory, and extensive analysis derives some insightful findings that could inspire future research for harmless LLM research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyao Cui",
      "Zhenyu Zhang",
      "Yilong Chen",
      "Wenyuan Zhang",
      "Tianyun Liu",
      "Siqi Wang",
      "Tingwen Liu"
    ]
  },
  "http://arxiv.org/abs/2311.18567": {
    "title": "Grammatical Gender's Influence on Distributional Semantics: A Causal Perspective",
    "volume": "Nov",
    "abstract": "How much meaning influences gender assignment across languages is an active area of research in modern linguistics and cognitive science. We can view current approaches as aiming to determine where gender assignment falls on a spectrum, from being fully arbitrarily determined to being largely semantically determined. For the latter case, there is a formulation of the neo-Whorfian hypothesis, which claims that even inanimate noun gender influences how people conceive of and talk about objects (using the choice of adjective used to modify inanimate nouns as a proxy for meaning). We offer a novel, causal graphical model that jointly represents the interactions between a noun's grammatical gender, its meaning, and adjective choice. In accordance with past results, we find a relationship between the gender of nouns and the adjectives which modify them. However, when we control for the meaning of the noun, we find that grammatical gender has a near-zero effect on adjective choice, thereby calling the neo-Whorfian hypothesis into question",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karolina Stańczak",
      "Kevin Du",
      "Adina Williams",
      "Isabelle Augenstein",
      "Ryan Cotterell"
    ]
  },
  "http://arxiv.org/abs/2311.18481": {
    "title": "ESG Accountability Made Easy: DocQA at Your Service",
    "volume": "Nov",
    "abstract": "We present Deep Search DocQA. This application enables information extraction from documents via a question-answering conversational assistant. The system integrates several technologies from different AI disciplines consisting of document conversion to machine-readable format (via computer vision), finding relevant data (via natural language processing), and formulating an eloquent response (via large language models). Users can explore over 10,000 Environmental, Social, and Governance (ESG) disclosure reports from over 2000 corporations. The Deep Search platform can be accessed at: https://ds4sd.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lokesh Mishra",
      "Cesar Berrospi",
      "Kasper Dinkla",
      "Diego Antognini",
      "Francesco Fusco",
      "Benedikt Bothur",
      "Maksym Lysak",
      "Nikolaos Livathinos",
      "Ahmed Nassar",
      "Panagiotis Vagenas",
      "Lucas Morin",
      "Christoph Auer",
      "Michele Dolfi",
      "Peter Staar"
    ]
  },
  "http://arxiv.org/abs/2311.18466": {
    "title": "Use of explicit replies as coordination mechanisms in online student debate",
    "volume": "Nov",
    "abstract": "People in conversation entrain their linguistic behaviours through spontaneous alignment mechanisms [7] - both in face-to-face and computer-mediated communication (CMC) [8]. In CMC, one of the mechanisms through which linguistic entrainment happens is through explicit replies. Indeed, the use of explicit replies influences the structure of conversations, favouring the formation of reply-trees typically delineated by topic shifts [5]. The interpersonal coordination mechanisms realized by how actors address each other have been studied using a probabilistic framework proposed by David Gibson [2,3]. Other recent approaches use computational methods and information theory to quantify changes in text. We explore coordination mechanisms concerned with some of the roles utterances play in dialogues - specifically in explicit replies. We identify these roles by finding community structure in the conversation's vocabulary using a non-parametric, hierarchical topic model. Some conversations may always stay on the ground, remaining at the level of general introductory chatter. Some others may develop a specific sub-topic in significant depth and detail. Even others may jump between general chatter, out-of-topic remarks and people agreeing or disagreeing without further elaboration",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bruno D. Ferreira-Saraiva",
      "Joao P. Matos-Carvalho",
      "Manuel Pita"
    ]
  },
  "http://arxiv.org/abs/2311.18397": {
    "title": "IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions",
    "volume": "Nov",
    "abstract": "Retrieval-Augmented Generation (RAG), by incorporating external knowledge with parametric memory of language models, has become the state-of-the-art architecture for open-domain QA tasks. However, common knowledge bases are inherently constrained by limited coverage and noisy information, making retrieval-based approaches inadequate to answer implicit reasoning questions. In this paper, we propose an Induction-Augmented Generation (IAG) framework that utilizes inductive knowledge along with the retrieved documents for implicit reasoning. We leverage large language models (LLMs) for deriving such knowledge via a novel prompting method based on inductive reasoning patterns. On top of this, we implement two versions of IAG named IAG-GPT and IAG-Student, respectively. IAG-GPT directly utilizes the knowledge generated by GPT-3 for answer prediction, while IAG-Student gets rid of dependencies on GPT service at inference time by incorporating a student inductor model. The inductor is firstly trained via knowledge distillation and further optimized by back-propagating the generator feedback via differentiable beam scores. Experimental results show that IAG outperforms RAG baselines as well as ChatGPT on two Open-Domain QA tasks. Notably, our best models have won the first place in the official leaderboards of CSQA2.0 (since Nov 1, 2022) and StrategyQA (since Jan 8, 2023)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhebin Zhang",
      "Xinyu Zhang",
      "Yuanhang Ren",
      "Saijiang Shi",
      "Meng Han",
      "Yongkang Wu",
      "Ruofei Lai",
      "Zhao Cao"
    ]
  },
  "http://arxiv.org/abs/2311.18364": {
    "title": "Hubness Reduction Improves Sentence-BERT Semantic Spaces",
    "volume": "Nov",
    "abstract": "Semantic representations of text, i.e. representations of natural language which capture meaning by geometry, are essential for areas such as information retrieval and document grouping. High-dimensional trained dense vectors have received much attention in recent years as such representations. We investigate the structure of semantic spaces that arise from embeddings made with Sentence-BERT and find that the representations suffer from a well-known problem in high dimensions called hubness. Hubness results in asymmetric neighborhood relations, such that some texts (the hubs) are neighbours of many other texts while most texts (so-called anti-hubs), are neighbours of few or no other texts. We quantify the semantic quality of the embeddings using hubness scores and error rate of a neighbourhood based classifier. We find that when hubness is high, we can reduce error rate and hubness using hubness reduction methods. We identify a combination of two methods as resulting in the best reduction. For example, on one of the tested pretrained models, this combined method can reduce hubness by about 75% and error rate by about 9%. Thus, we argue that mitigating hubness in the embedding space provides better semantic representations of text",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beatrix M. G. Nielsen",
      "Lars Kai Hansen"
    ]
  },
  "http://arxiv.org/abs/2311.18353": {
    "title": "Evaluating the Rationale Understanding of Critical Reasoning in Logical Reading Comprehension",
    "volume": "Nov",
    "abstract": "To precisely evaluate a language model's capability for logical reading comprehension, we present a dataset for testing the understanding of the rationale behind critical reasoning. For questions taken from an existing multiplechoice logical reading comprehension dataset, we crowdsource rationale texts that explain why we should select or eliminate answer options, resulting in 3,003 multiple-choice subquestions that are associated with 943 main questions. Experiments on our dataset show that recent large language models (e.g., InstructGPT) struggle to answer the subquestions even if they are able to answer the main questions correctly. We find that the models perform particularly poorly in answering subquestions written for the incorrect options of the main questions, implying that the models have a limited capability for explaining why incorrect alternatives should be eliminated. These results suggest that our dataset encourages further investigation into the critical reasoning ability of language models while focusing on the elimination process of relevant alternatives",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akira Kawabata",
      "Saku Sugawara"
    ]
  },
  "http://arxiv.org/abs/2311.18260": {
    "title": "Consensus, dissensus and synergy between clinicians and specialist foundation models in radiology report generation",
    "volume": "Dec",
    "abstract": "Radiology reports are an instrumental part of modern medicine, informing key clinical decisions such as diagnosis and treatment. The worldwide shortage of radiologists, however, restricts access to expert care and imposes heavy workloads, contributing to avoidable errors and delays in report delivery. While recent progress in automated report generation with vision-language models offer clear potential in ameliorating the situation, the path to real-world adoption has been stymied by the challenge of evaluating the clinical quality of AI-generated reports. In this study, we build a state-of-the-art report generation system for chest radiographs, Flamingo-CXR, by fine-tuning a well-known vision-language foundation model on radiology data. To evaluate the quality of the AI-generated reports, a group of 16 certified radiologists provide detailed evaluations of AI-generated and human written reports for chest X-rays from an intensive care setting in the United States and an inpatient setting in India. At least one radiologist (out of two per case) preferred the AI report to the ground truth report in over 60$\\%$ of cases for both datasets. Amongst the subset of AI-generated reports that contain errors, the most frequently cited reasons were related to the location and finding, whereas for human written reports, most mistakes were related to severity and finding. This disparity suggested potential complementarity between our AI system and human experts, prompting us to develop an assistive scenario in which Flamingo-CXR generates a first-draft report, which is subsequently revised by a clinician. This is the first demonstration of clinician-AI collaboration for report writing, and the resultant reports are assessed to be equivalent or preferred by at least one radiologist to reports written by experts alone in 80$\\%$ of in-patient cases and 66$\\%$ of intensive care cases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryutaro Tanno",
      "David G. T. Barrett",
      "Andrew Sellergren",
      "Sumedh Ghaisas",
      "Sumanth Dathathri",
      "Abigail See",
      "Johannes Welbl",
      "Karan Singhal",
      "Shekoofeh Azizi",
      "Tao Tu",
      "Mike Schaekermann",
      "Rhys May",
      "Roy Lee",
      "SiWai Man",
      "Zahra Ahmed",
      "Sara Mahdavi",
      "Danielle Belgrave",
      "Vivek Natarajan",
      "Shravya Shetty",
      "Pushmeet Kohli",
      "Po-Sen Huang",
      "Alan Karthikesalingam",
      "Ira Ktena"
    ]
  },
  "http://arxiv.org/abs/2311.18248": {
    "title": "mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model",
    "volume": "Nov",
    "abstract": "Recently, the strong text creation ability of Large Language Models(LLMs) has given rise to many tools for assisting paper reading or even writing. However, the weak diagram analysis abilities of LLMs or Multimodal LLMs greatly limit their application scenarios, especially for scientific academic paper writing. In this work, towards a more versatile copilot for academic paper writing, we mainly focus on strengthening the multi-modal diagram analysis ability of Multimodal LLMs. By parsing Latex source files of high-quality papers, we carefully build a multi-modal diagram understanding dataset M-Paper. By aligning diagrams in the paper with related paragraphs, we construct professional diagram analysis samples for training and evaluation. M-Paper is the first dataset to support joint comprehension of multiple scientific diagrams, including figures and tables in the format of images or Latex codes. Besides, to better align the copilot with the user's intention, we introduce the `outline' as the control signal, which could be directly given by the user or revised based on auto-generated ones. Comprehensive experiments with a state-of-the-art Mumtimodal LLM demonstrate that training on our dataset shows stronger scientific diagram understanding performance, including diagram captioning, diagram analysis, and outline recommendation. The dataset, code, and model are available at https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/PaperOwl",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anwen Hu",
      "Yaya Shi",
      "Haiyang Xu",
      "Jiabo Ye",
      "Qinghao Ye",
      "Ming Yan",
      "Chenliang Li",
      "Qi Qian",
      "Ji Zhang",
      "Fei Huang"
    ]
  },
  "http://arxiv.org/abs/2312.00819": {
    "title": "Large Language Models for Travel Behavior Prediction",
    "volume": "Nov",
    "abstract": "Travel behavior prediction is a fundamental task in transportation demand management. The conventional methods for travel behavior prediction rely on numerical data to construct mathematical models and calibrate model parameters to represent human preferences. Recent advancement in large language models (LLMs) has shown great reasoning abilities to solve complex problems. In this study, we propose to use LLMs to predict travel behavior with prompt engineering without data-based parameter learning. Specifically, we carefully design our prompts that include 1) task description, 2) travel characteristics, 3) individual attributes, and 4) guides of thinking with domain knowledge, and ask the LLMs to predict an individual's travel behavior and explain the results. We select the travel mode choice task as a case study. Results show that, though no training samples are provided, LLM-based predictions have competitive accuracy and F1-score as canonical supervised learning methods such as multinomial logit, random forest, and neural networks. LLMs can also output reasons that support their prediction. However, though in most of the cases, the output explanations are reasonable, we still observe cases that violate logic or with hallucinations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baichuan Mo",
      "Hanyong Xu",
      "Dingyi Zhuang",
      "Ruoyun Ma",
      "Xiaotong Guo",
      "Jinhua Zhao"
    ]
  },
  "http://arxiv.org/abs/2311.18232": {
    "title": "LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models",
    "volume": "Nov",
    "abstract": "Large language models (LLMs) provide excellent text-generation capabilities, but standard prompting and generation methods generally do not lead to intentional or goal-directed agents and might necessitate considerable prompt tuning. This becomes particularly apparent in multi-turn conversations: even the best current LLMs rarely ask clarifying questions, engage in explicit information gathering, or take actions now that lead to better decisions after multiple turns. Reinforcement learning has the potential to leverage the powerful modeling capabilities of LLMs, as well as their internal representation of textual interactions, to create capable goal-directed language agents. This can enable intentional and temporally extended interactions, such as with humans, through coordinated persuasion and carefully crafted questions, or in goal-directed play through text games to bring about desired final outcomes. However, enabling this requires the community to develop stable and reliable reinforcement learning algorithms that can effectively train LLMs. Developing such algorithms requires tasks that can gauge progress on algorithm design, provide accessible and reproducible evaluations for multi-turn interactions, and cover a range of task properties and challenges in improving reinforcement learning algorithms. Our paper introduces the LMRL-Gym benchmark for evaluating multi-turn RL for LLMs, together with an open-source research framework containing a basic toolkit for getting started on multi-turn RL with offline value-based and policy-based RL methods. Our benchmark consists of 8 different language tasks, which require multiple rounds of language interaction and cover a range of tasks in open-ended dialogue and text games",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marwa Abdulhai",
      "Isadora White",
      "Charlie Snell",
      "Charles Sun",
      "Joey Hong",
      "Yuexiang Zhai",
      "Kelvin Xu",
      "Sergey Levine"
    ]
  },
  "http://arxiv.org/abs/2311.18215": {
    "title": "Automatic Construction of a Korean Toxic Instruction Dataset for Ethical Tuning of Large Language Models",
    "volume": "Nov",
    "abstract": "Caution: this paper may include material that could be offensive or distressing. The advent of Large Language Models (LLMs) necessitates the development of training approaches that mitigate the generation of unethical language and aptly manage toxic user queries. Given the challenges related to human labor and the scarcity of data, we present KoTox, comprising 39K unethical instruction-output pairs. This collection of automatically generated toxic instructions refines the training of LLMs and establishes a foundational framework for improving LLMs' ethical awareness and response to various toxic inputs, promoting more secure and responsible interactions in Natural Language Processing (NLP) applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungjoo Byun",
      "Dongjun Jang",
      "Hyemi Jo",
      "Hyopil Shin"
    ]
  },
  "http://arxiv.org/abs/2311.18200": {
    "title": "INarIG: Iterative Non-autoregressive Instruct Generation Model For Word-Level Auto Completion",
    "volume": "Nov",
    "abstract": "Computer-aided translation (CAT) aims to enhance human translation efficiency and is still important in scenarios where machine translation cannot meet quality requirements. One fundamental task within this field is Word-Level Auto Completion (WLAC). WLAC predicts a target word given a source sentence, translation context, and a human typed character sequence. Previous works either employ word classification models to exploit contextual information from both sides of the target word or directly disregarded the dependencies from the right-side context. Furthermore, the key information, i.e. human typed sequences, is only used as prefix constraints in the decoding module. In this paper, we propose the INarIG (Iterative Non-autoregressive Instruct Generation) model, which constructs the human typed sequence into Instruction Unit and employs iterative decoding with subwords to fully utilize input information given in the task. Our model is more competent in dealing with low-frequency words (core scenario of this task), and achieves state-of-the-art results on the WMT22 and benchmark datasets, with a maximum increase of over 10% prediction accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengchao Shang",
      "Zongyao Li",
      "Daimeng Wei",
      "Jiaxin Guo",
      "Minghan Wang",
      "Xiaoyu Chen",
      "Lizhi Lei",
      "Hao Yang"
    ]
  },
  "http://arxiv.org/abs/2312.00079": {
    "title": "HiFi Tuner: High-Fidelity Subject-Driven Fine-Tuning for Diffusion Models",
    "volume": "Nov",
    "abstract": "This paper explores advancements in high-fidelity personalized image generation through the utilization of pre-trained text-to-image diffusion models. While previous approaches have made significant strides in generating versatile scenes based on text descriptions and a few input images, challenges persist in maintaining the subject fidelity within the generated images. In this work, we introduce an innovative algorithm named HiFi Tuner to enhance the appearance preservation of objects during personalized image generation. Our proposed method employs a parameter-efficient fine-tuning framework, comprising a denoising process and a pivotal inversion process. Key enhancements include the utilization of mask guidance, a novel parameter regularization technique, and the incorporation of step-wise subject representations to elevate the sample fidelity. Additionally, we propose a reference-guided generation approach that leverages the pivotal inversion of a reference image to mitigate unwanted subject variations and artifacts. We further extend our method to a novel image editing task: substituting the subject in an image through textual manipulations. Experimental evaluations conducted on the DreamBooth dataset using the Stable Diffusion model showcase promising results. Fine-tuning solely on textual embeddings improves CLIP-T score by 3.6 points and improves DINO score by 9.6 points over Textual Inversion. When fine-tuning all parameters, HiFi Tuner improves CLIP-T score by 1.2 points and improves DINO score by 1.2 points over DreamBooth, establishing a new state of the art",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhonghao Wang",
      "Wei Wei",
      "Yang Zhao",
      "Zhisheng Xiao",
      "Mark Hasegawa-Johnson",
      "Humphrey Shi",
      "Tingbo Hou"
    ]
  },
  "http://arxiv.org/abs/2311.18195": {
    "title": "COVID-19 Vaccine Misinformation in Middle Income Countries",
    "volume": "Nov",
    "abstract": "This paper introduces a multilingual dataset of COVID-19 vaccine misinformation, consisting of annotated tweets from three middle-income countries: Brazil, Indonesia, and Nigeria. The expertly curated dataset includes annotations for 5,952 tweets, assessing their relevance to COVID-19 vaccines, presence of misinformation, and the themes of the misinformation. To address challenges posed by domain specificity, the low-resource setting, and data imbalance, we adopt two approaches for developing COVID-19 vaccine misinformation detection models: domain-specific pre-training and text augmentation using a large language model. Our best misinformation detection models demonstrate improvements ranging from 2.7 to 15.9 percentage points in macro F1-score compared to the baseline models. Additionally, we apply our misinformation detection models in a large-scale study of 19 million unlabeled tweets from the three countries between 2020 and 2022, showcasing the practical application of our dataset and models for detecting and analyzing vaccine misinformation in multiple countries and languages. Our analysis indicates that percentage changes in the number of new COVID-19 cases are positively associated with COVID-19 vaccine misinformation rates in a staggered manner for Brazil and Indonesia, and there are significant positive associations between the misinformation rates across the three countries",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongin Kim",
      "Byeo Rhee Back",
      "Aditya Agrawal",
      "Jiaxi Wu",
      "Veronika J. Wirtz",
      "Traci Hong",
      "Derry Wijaya"
    ]
  },
  "http://arxiv.org/abs/2311.18194": {
    "title": "Positional Information Matters for Invariant In-Context Learning: A Case Study of Simple Function Classes",
    "volume": "Nov",
    "abstract": "In-context learning (ICL) refers to the ability of a model to condition on a few in-context demonstrations (input-output examples of the underlying task) to generate the answer for a new query input, without updating parameters. Despite the impressive ICL ability of LLMs, it has also been found that ICL in LLMs is sensitive to input demonstrations and limited to short context lengths. To understand the limitations and principles for successful ICL, we conduct an investigation with ICL linear regression of transformers. We characterize several Out-of-Distribution (OOD) cases for ICL inspired by realistic LLM ICL failures and compare transformers with DeepSet, a simple yet powerful architecture for ICL. Surprisingly, DeepSet outperforms transformers across a variety of distribution shifts, implying that preserving permutation invariance symmetry to input demonstrations is crucial for OOD ICL. The phenomenon specifies a fundamental requirement by ICL, which we termed as ICL invariance. Nevertheless, the positional encodings in LLMs will break ICL invariance. To this end, we further evaluate transformers with identical positional encodings and find preserving ICL invariance in transformers achieves state-of-the-art performance across various ICL distribution shifts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongqiang Chen",
      "Binghui Xie",
      "Kaiwen Zhou",
      "Bo Han",
      "Yatao Bian",
      "James Cheng"
    ]
  },
  "http://arxiv.org/abs/2311.18151": {
    "title": "Uncertainty Guided Global Memory Improves Multi-Hop Question Answering",
    "volume": "Nov",
    "abstract": "Transformers have become the gold standard for many natural language processing tasks and, in particular, for multi-hop question answering (MHQA). This task includes processing a long document and reasoning over the multiple parts of it. The landscape of MHQA approaches can be classified into two primary categories. The first group focuses on extracting supporting evidence, thereby constraining the QA model's context to predicted facts. Conversely, the second group relies on the attention mechanism of the long input encoding model to facilitate multi-hop reasoning. However, attention-based token representations lack explicit global contextual information to connect reasoning steps. To address these issues, we propose GEMFormer, a two-stage method that first collects relevant information over the entire document to the memory and then combines it with local context to solve the task. Our experimental results show that fine-tuning a pre-trained model with memory-augmented input, including the most certain global elements, improves the model's performance on three MHQA datasets compared to the baseline. We also found that the global explicit memory contains information from supporting facts required for the correct answer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alsu Sagirova",
      "Mikhail Burtsev"
    ]
  },
  "http://arxiv.org/abs/2311.18147": {
    "title": "DisCGen: A Framework for Discourse-Informed Counterspeech Generation",
    "volume": "Nov",
    "abstract": "Counterspeech can be an effective method for battling hateful content on social media. Automated counterspeech generation can aid in this process. Generated counterspeech, however, can be viable only when grounded in the context of topic, audience and sensitivity as these factors influence both the efficacy and appropriateness. In this work, we propose a novel framework based on theories of discourse to study the inferential links that connect counter speeches to the hateful comment. Within this framework, we propose: i) a taxonomy of counterspeech derived from discourse frameworks, and ii) discourse-informed prompting strategies for generating contextually-grounded counterspeech. To construct and validate this framework, we present a process for collecting an in-the-wild dataset of counterspeech from Reddit. Using this process, we manually annotate a dataset of 3.9k Reddit comment pairs for the presence of hatespeech and counterspeech. The positive pairs are annotated for 10 classes in our proposed taxonomy. We annotate these pairs with paraphrased counterparts to remove offensiveness and first-person references. We show that by using our dataset and framework, large language models can generate contextually-grounded counterspeech informed by theories of discourse. According to our human evaluation, our approaches can act as a safeguard against critical failures of discourse-agnostic models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sabit Hassan",
      "Malihe Alikhani"
    ]
  },
  "http://arxiv.org/abs/2311.18140": {
    "title": "ROBBIE: Robust Bias Evaluation of Large Generative Language Models",
    "volume": "Nov",
    "abstract": "As generative large language models (LLMs) grow more performant and prevalent, we must develop comprehensive enough tools to measure and improve their fairness. Different prompt-based datasets can be used to measure social bias across multiple text domains and demographic axes, meaning that testing LLMs on more datasets can potentially help us characterize their biases more fully, and better ensure equal and equitable treatment of marginalized demographic groups. In this work, our focus is two-fold: (1) Benchmarking: a comparison of 6 different prompt-based bias and toxicity metrics across 12 demographic axes and 5 families of generative LLMs. Out of those 6 metrics, AdvPromptSet and HolisticBiasR are novel datasets proposed in the paper. The comparison of those benchmarks gives us insights about the bias and toxicity of the compared models. Therefore, we explore the frequency of demographic terms in common LLM pre-training corpora and how this may relate to model biases. (2) Mitigation: we conduct a comprehensive study of how well 3 bias/toxicity mitigation techniques perform across our suite of measurements. ROBBIE aims to provide insights for practitioners while deploying a model, emphasizing the need to not only measure potential harms, but also understand how they arise by characterizing the data, mitigate harms once found, and balance any trade-offs. We open-source our analysis code in hopes of encouraging broader measurements of bias in future LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Esiobu",
      "Xiaoqing Tan",
      "Saghar Hosseini",
      "Megan Ung",
      "Yuchen Zhang",
      "Jude Fernandes",
      "Jane Dwivedi-Yu",
      "Eleonora Presani",
      "Adina Williams",
      "Eric Michael Smith"
    ]
  },
  "http://arxiv.org/abs/2311.18063": {
    "title": "TurkishBERTweet: Fast and Reliable Large Language Model for Social Media Analysis",
    "volume": "Nov",
    "abstract": "Turkish is one of the most popular languages in the world. Wide us of this language on social media platforms such as Twitter, Instagram, or Tiktok and strategic position of the country in the world politics makes it appealing for the social network researchers and industry. To address this need, we introduce TurkishBERTweet, the first large scale pre-trained language model for Turkish social media built using almost 900 million tweets. The model shares the same architecture as base BERT model with smaller input length, making TurkishBERTweet lighter than BERTurk and can have significantly lower inference time. We trained our model using the same approach for RoBERTa model and evaluated on two text classification tasks: Sentiment Classification and Hate Speech Detection. We demonstrate that TurkishBERTweet outperforms the other available alternatives on generalizability and its lower inference time gives significant advantage to process large-scale datasets. We also compared our models with the commercial OpenAI solutions in terms of cost and performance to demonstrate TurkishBERTweet is scalable and cost-effective solution. As part of our research, we released TurkishBERTweet and fine-tuned LoRA adapters for the mentioned tasks under the MIT License to facilitate future research and applications on Turkish social media. Our TurkishBERTweet model is available at: https://github.com/ViralLab/TurkishBERTweet",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Najafi",
      "Onur Varol"
    ]
  },
  "http://arxiv.org/abs/2311.18054": {
    "title": "I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text",
    "volume": "Nov",
    "abstract": "Potential harms of Large Language Models such as mass misinformation and plagiarism can be partially mitigated if there exists a reliable way to detect machine generated text. In this paper, we propose a new watermarking method to detect machine-generated texts. Our method embeds a unique pattern within the generated text, ensuring that while the content remains coherent and natural to human readers, it carries distinct markers that can be identified algorithmically. Specifically, we intervene with the token sampling process in a way which enables us to trace back our token choices during the detection phase. We show how watermarking affects textual quality and compare our proposed method with a state-of-the-art watermarking method in terms of robustness and detectability. Through extensive experiments, we demonstrate the effectiveness of our watermarking scheme in distinguishing between watermarked and non-watermarked text, achieving high detection rates while maintaining textual quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaan Efe Keleş",
      "Ömer Kaan Gürbüz",
      "Mucahid Kutlu"
    ]
  },
  "http://arxiv.org/abs/2311.18041": {
    "title": "Zero-shot Conversational Summarization Evaluations with small Large Language Models",
    "volume": "Nov",
    "abstract": "Large Language Models (LLMs) exhibit powerful summarization abilities. However, their capabilities on conversational summarization remains under explored. In this work we evaluate LLMs (approx. 10 billion parameters) on conversational summarization and showcase their performance on various prompts. We show that the summaries generated by models depend on the instructions and the performance of LLMs vary with different instructions sometimes resulting steep drop in ROUGE scores if prompts are not selected carefully. We also evaluate the models with human evaluations and discuss the limitations of the models on conversational summarization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ramesh Manuvinakurike",
      "Saurav Sahay",
      "Sangeeta Manepalli",
      "Lama Nachman"
    ]
  },
  "http://arxiv.org/abs/2311.18034": {
    "title": "Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings",
    "volume": "Nov",
    "abstract": "Cross-lingual transfer learning is an important property of multilingual large language models (LLMs). But how do LLMs represent relationships between languages? Every language model has an input layer that maps tokens to vectors. This ubiquitous layer of language models is often overlooked. We find that similarities between these input embeddings are highly interpretable and that the geometry of these embeddings differs between model families. In one case (XLM-RoBERTa), embeddings encode language: tokens in different writing systems can be linearly separated with an average of 99.2% accuracy. Another family (mT5) represents cross-lingual semantic similarity: the 50 nearest neighbors for any token represent an average of 7.61 writing systems, and are frequently translations. This result is surprising given that there is no explicit parallel cross-lingual training corpora and no explicit incentive for translations in pre-training objectives. Our research opens the door for investigations in 1) The effect of pre-training and model architectures on representations of languages and 2) The applications of cross-lingual representations embedded in language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea W Wen-Yi",
      "David Mimno"
    ]
  },
  "http://arxiv.org/abs/2311.18028": {
    "title": "Filtered Semi-Markov CRF",
    "volume": "Nov",
    "abstract": "Semi-Markov CRF has been proposed as an alternative to the traditional Linear Chain CRF for text segmentation tasks such as Named Entity Recognition (NER). Unlike CRF, which treats text segmentation as token-level prediction, Semi-CRF considers segments as the basic unit, making it more expressive. However, Semi-CRF suffers from two major drawbacks: (1) quadratic complexity over sequence length, as it operates on every span of the input sequence, and (2) inferior performance compared to CRF for sequence labeling tasks like NER. In this paper, we introduce Filtered Semi-Markov CRF, a variant of Semi-CRF that addresses these issues by incorporating a filtering step to eliminate irrelevant segments, reducing complexity and search space. Our approach is evaluated on several NER benchmarks, where it outperforms both CRF and Semi-CRF while being significantly faster. The implementation of our method is available on \\href{https://github.com/urchade/Filtered-Semi-Markov-CRF}{Github}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Urchade Zaratiana",
      "Nadi Tomeh",
      "Niama El Khbir",
      "Pierre Holat",
      "Thierry Charnois"
    ]
  },
  "http://arxiv.org/abs/2311.17898": {
    "title": "Knowledge Pursuit Prompting for Zero-Shot Multimodal Synthesis",
    "volume": "Nov",
    "abstract": "Hallucinations and unfaithful synthesis due to inaccurate prompts with insufficient semantic details are widely observed in multimodal generative models. A prevalent strategy to align multiple modalities is to fine-tune the generator with a large number of annotated text-image pairs. However, such a procedure is labor-consuming and resource-draining. The key question we ask is: can we enhance the quality and faithfulness of text-driven generative models beyond extensive text-image pair annotations? To address this question, we propose Knowledge Pursuit Prompting (KPP), a zero-shot framework that iteratively incorporates external knowledge to help generators produce reliable visual content. Instead of training generators to handle generic prompts, KPP employs a recursive knowledge query process to gather informative external facts from the knowledge base, instructs a language model to compress the acquired knowledge for prompt refinement, and utilizes text-driven generators for visual synthesis. The entire process is zero-shot, without accessing the architectures and parameters of generative models. We evaluate the framework across multiple text-driven generative tasks (image, 3D rendering, and video) on datasets of different domains. We further demonstrate the extensibility and adaptability of KPP through varying foundation model bases and instructions. Our results show that KPP is capable of generating faithful and semantically rich content across diverse visual domains, offering a promising solution to improve multimodal generative models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinqi Luo",
      "Kwan Ho Ryan Chan",
      "Dimitris Dimos",
      "René Vidal"
    ]
  },
  "http://arxiv.org/abs/2311.17892": {
    "title": "A Pipeline For Discourse Circuits From CCG",
    "volume": "Nov",
    "abstract": "There is a significant disconnect between linguistic theory and modern NLP practice, which relies heavily on inscrutable black-box architectures. DisCoCirc is a newly proposed model for meaning that aims to bridge this divide, by providing neuro-symbolic models that incorporate linguistic structure. DisCoCirc represents natural language text as a `circuit' that captures the core semantic information of the text. These circuits can then be interpreted as modular machine learning models. Additionally, DisCoCirc fulfils another major aim of providing an NLP model that can be implemented on near-term quantum computers. In this paper we describe a software pipeline that converts English text to its DisCoCirc representation. The pipeline achieves coverage over a large fragment of the English language. It relies on Combinatory Categorial Grammar (CCG) parses of the input text as well as coreference resolution information. This semantic and syntactic information is used in several steps to convert the text into a simply-typed $\\lambda$-calculus term, and then into a circuit diagram. This pipeline will enable the application of the DisCoCirc framework to NLP tasks, using both classical and quantum approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathon Liu",
      "Razin A. Shaikh",
      "Benjamin Rodatz",
      "Richie Yeung",
      "Bob Coecke"
    ]
  },
  "http://arxiv.org/abs/2311.17842": {
    "title": "Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning",
    "volume": "Nov",
    "abstract": "In this study, we are interested in imbuing robots with the capability of physically-grounded task planning. Recent advancements have shown that large language models (LLMs) possess extensive knowledge useful in robotic tasks, especially in reasoning and planning. However, LLMs are constrained by their lack of world grounding and dependence on external affordance models to perceive environmental information, which cannot jointly reason with LLMs. We argue that a task planner should be an inherently grounded, unified multimodal system. To this end, we introduce Robotic Vision-Language Planning (ViLa), a novel approach for long-horizon robotic planning that leverages vision-language models (VLMs) to generate a sequence of actionable steps. ViLa directly integrates perceptual data into its reasoning and planning process, enabling a profound understanding of commonsense knowledge in the visual world, including spatial layouts and object attributes. It also supports flexible multimodal goal specification and naturally incorporates visual feedback. Our extensive evaluation, conducted in both real-robot and simulated environments, demonstrates ViLa's superiority over existing LLM-based planners, highlighting its effectiveness in a wide array of open-world manipulation tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingdong Hu",
      "Fanqi Lin",
      "Tong Zhang",
      "Li Yi",
      "Yang Gao"
    ]
  },
  "http://arxiv.org/abs/2311.17813": {
    "title": "Higher-Order DisCoCat (Peirce-Lambek-Montague semantics)",
    "volume": "Nov",
    "abstract": "We propose a new definition of higher-order DisCoCat (categorical compositional distributional) models where the meaning of a word is not a diagram, but a diagram-valued higher-order function. Our models can be seen as a variant of Montague semantics based on a lambda calculus where the primitives act on string diagrams rather than logical formulae. As a special case, we show how to translate from the Lambek calculus into Peirce's system beta for first-order logic. This allows us to give a purely diagrammatic treatment of higher-order and non-linear processes in natural language semantics: adverbs, prepositions, negation and quantifiers. The theoretical definition presented in this article comes with a proof-of-concept implementation in DisCoPy, the Python library for string diagrams",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexis Toumi",
      "Giovanni de Felice"
    ]
  },
  "http://arxiv.org/abs/2311.17786": {
    "title": "DSS: Synthesizing long Digital Ink using Data augmentation, Style encoding and Split generation",
    "volume": "Nov",
    "abstract": "As text generative models can give increasingly long answers, we tackle the problem of synthesizing long text in digital ink. We show that the commonly used models for this task fail to generalize to long-form data and how this problem can be solved by augmenting the training data, changing the model architecture and the inference procedure. These methods use contrastive learning technique and are tailored specifically for the handwriting domain. They can be applied to any encoder-decoder model that works with digital ink. We demonstrate that our method reduces the character error rate on long-form English data by half compared to baseline RNN and by 16% compared to the previous approach that aims at addressing the same problem. We show that all three parts of the method improve recognizability of generated inks. In addition, we evaluate synthesized data in a human study and find that people perceive most of generated data as real",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksandr Timofeev",
      "Anastasiia Fadeeva",
      "Andrei Afonin",
      "Claudiu Musat",
      "Andrii Maksai"
    ]
  },
  "http://arxiv.org/abs/2311.17771": {
    "title": "Supervising the Centroid Baseline for Extractive Multi-Document Summarization",
    "volume": "Nov",
    "abstract": "The centroid method is a simple approach for extractive multi-document summarization and many improvements to its pipeline have been proposed. We further refine it by adding a beam search process to the sentence selection and also a centroid estimation attention model that leads to improved results. We demonstrate this in several multi-document summarization datasets, including in a multilingual scenario",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simão Gonçalves",
      "Gonçalo Correia",
      "Diogo Pernes",
      "Afonso Mendes"
    ]
  },
  "http://arxiv.org/abs/2311.17972": {
    "title": "Self-Infilling Code Generation",
    "volume": "Nov",
    "abstract": "This work introduces a general code generation framework that incorporates infilling operations into auto-regressive decoding. Our approach capitalizes on the observation that recent code language models with infilling capabilities can perform \\emph{self-infilling}: whereas infilling operations aim to fill in the middle based on a predefined prefix and suffix, self-infilling sequentially generates both such surrounding context and the infilled content. We utilize this feature to develop an infilling-augmented decoding process that facilitates non-monotonic generation. This approach allows for postponing the generation of uncertain code snippets until a definitive suffix is established, leading to improved control over the generation sequence. In addition, it facilitates a looping mechanism, which can iteratively update and synchronize each piece of generation in a cyclic manner. Extensive experiments are conducted to demonstrate that our proposed decoding process is effective in enhancing regularity and quality across several code generation benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Zheng",
      "Jianbo Yuan",
      "Zhi Zhang",
      "Hongxia Yang",
      "Lingpeng Kong"
    ]
  },
  "http://arxiv.org/abs/2311.17743": {
    "title": "Mukhyansh: A Headline Generation Dataset for Indic Languages",
    "volume": "Nov",
    "abstract": "The task of headline generation within the realm of Natural Language Processing (NLP) holds immense significance, as it strives to distill the true essence of textual content into concise and attention-grabbing summaries. While noteworthy progress has been made in headline generation for widely spoken languages like English, there persist numerous challenges when it comes to generating headlines in low-resource languages, such as the rich and diverse Indian languages. A prominent obstacle that specifically hinders headline generation in Indian languages is the scarcity of high-quality annotated data. To address this crucial gap, we proudly present Mukhyansh, an extensive multilingual dataset, tailored for Indian language headline generation. Comprising an impressive collection of over 3.39 million article-headline pairs, Mukhyansh spans across eight prominent Indian languages, namely Telugu, Tamil, Kannada, Malayalam, Hindi, Bengali, Marathi, and Gujarati. We present a comprehensive evaluation of several state-of-the-art baseline models. Additionally, through an empirical analysis of existing works, we demonstrate that Mukhyansh outperforms all other models, achieving an impressive average ROUGE-L score of 31.43 across all 8 languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lokesh Madasu",
      "Gopichand Kanumolu",
      "Nirmal Surange",
      "Manish Shrivastava"
    ]
  },
  "http://arxiv.org/abs/2311.17741": {
    "title": "End-to-end Joint Rich and Normalized ASR with a limited amount of rich training data",
    "volume": "Nov",
    "abstract": "Joint rich and normalized automatic speech recognition (ASR), that produces transcriptions both with and without punctuation and capitalization, remains a challenge. End-to-end (E2E) ASR models offer both convenience and the ability to perform such joint transcription of speech. Training such models requires paired speech and rich text data, which is not widely available. In this paper, we compare two different approaches to train a stateless Transducer-based E2E joint rich and normalized ASR system, ready for streaming applications, with a limited amount of rich labeled data. The first approach uses a language model to generate pseudo-rich transcriptions of normalized training data. The second approach uses a single decoder conditioned on the type of the output. The first approach leads to E2E rich ASR which perform better on out-of-domain data, with up to 9% relative reduction in errors. The second approach demonstrates the feasibility of an E2E joint rich and normalized ASR system using as low as 5% rich training data with moderate (2.42% absolute) increase in errors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Can Cui",
      "Imran Ahamad Sheikh",
      "Mostafa Sadeghi",
      "Emmanuel Vincent"
    ]
  },
  "http://arxiv.org/abs/2311.17722": {
    "title": "SenTest: Evaluating Robustness of Sentence Encoders",
    "volume": "Nov",
    "abstract": "Contrastive learning has proven to be an effective method for pre-training models using weakly labeled data in the vision domain. Sentence transformers are the NLP counterparts to this architecture, and have been growing in popularity due to their rich and effective sentence representations. Having effective sentence representations is paramount in multiple tasks, such as information retrieval, retrieval augmented generation (RAG), and sentence comparison. Keeping in mind the deployability factor of transformers, evaluating the robustness of sentence transformers is of utmost importance. This work focuses on evaluating the robustness of the sentence encoders. We employ several adversarial attacks to evaluate its robustness. This system uses character-level attacks in the form of random character substitution, word-level attacks in the form of synonym replacement, and sentence-level attacks in the form of intra-sentence word order shuffling. The results of the experiments strongly undermine the robustness of sentence encoders. The models produce significantly different predictions as well as embeddings on perturbed datasets. The accuracy of the models can fall up to 15 percent on perturbed datasets as compared to unperturbed datasets. Furthermore, the experiments demonstrate that these embeddings does capture the semantic and syntactic structure (sentence order) of sentences. However, existing supervised classification strategies fail to leverage this information, and merely function as n-gram detectors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanmay Chavan",
      "Shantanu Patankar",
      "Aditya Kane",
      "Omkar Gokhale",
      "Geetanjali Kale",
      "Raviraj Joshi"
    ]
  },
  "http://arxiv.org/abs/2311.17696": {
    "title": "How to Build an AI Tutor that Can Adapt to Any Course and Provide Accurate Answers Using Large Language Model and Retrieval-Augmented Generation",
    "volume": "Nov",
    "abstract": "Artificial intelligence is transforming education through data-driven, personalized learning solutions. This paper introduces AI Tutor, an innovative web application that provides personalized tutoring in any subject using state-of-the-art Large Language Model (LLM). AI Tutor ingests course materials to construct an adaptive knowledge base tailored to the course. When students pose questions, it retrieves the most relevant information and generates detailed, conversational responses citing supporting evidence. The system is powered by advanced large language models and Retrieval-Augmented Generation (RAG) techniques for accurate, natural question answering. We present a fully-functional web interface and video demonstration that showcase AI Tutor's versatility across diverse subjects and its ability to produce pedagogically cogent responses. While an initial prototype, this work represents a pioneering step toward AI-enabled tutoring systems that can democratize access to high-quality, customized educational support",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxi Dong"
    ]
  },
  "http://arxiv.org/abs/2311.17686": {
    "title": "AviationGPT: A Large Language Model for the Aviation Domain",
    "volume": "Nov",
    "abstract": "The advent of ChatGPT and GPT-4 has captivated the world with large language models (LLMs), demonstrating exceptional performance in question-answering, summarization, and content generation. The aviation industry is characterized by an abundance of complex, unstructured text data, replete with technical jargon and specialized terminology. Moreover, labeled data for model building are scarce in this domain, resulting in low usage of aviation text data. The emergence of LLMs presents an opportunity to transform this situation, but there is a lack of LLMs specifically designed for the aviation domain. To address this gap, we propose AviationGPT, which is built on open-source LLaMA-2 and Mistral architectures and continuously trained on a wealth of carefully curated aviation datasets. Experimental results reveal that AviationGPT offers users multiple advantages, including the versatility to tackle diverse natural language processing (NLP) problems (e.g., question-answering, summarization, document writing, information extraction, report querying, data cleaning, and interactive data exploration). It also provides accurate and contextually relevant responses within the aviation domain and significantly improves performance (e.g., over a 40% performance gain in tested cases). With AviationGPT, the aviation industry is better equipped to address more complex research problems and enhance the efficiency and safety of National Airspace System (NAS) operations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liya Wang",
      "Jason Chou",
      "Xin Zhou",
      "Alex Tien",
      "Diane M Baumgartner"
    ]
  },
  "http://arxiv.org/abs/2311.17676": {
    "title": "Improving Minority Stress Detection with Emotions",
    "volume": "Nov",
    "abstract": "Psychological stress detection is an important task for mental healthcare research, but there has been little prior work investigating the effectiveness of psychological stress models on minority individuals, who are especially vulnerable to poor mental health outcomes. In this work, we use the related task of minority stress detection to evaluate the ability of psychological stress models to understand the language of sexual and gender minorities. We find that traditional psychological stress models underperform on minority stress detection, and we propose using emotion-infused models to reduce that performance disparity. We further demonstrate that multi-task psychological stress models outperform the current state-of-the-art for minority stress detection without directly training on minority stress data. We provide explanatory analysis showing that minority communities have different distributions of emotions than the general population and that emotion-infused models improve the performance of stress models on underrepresented groups because of their effectiveness in low-data environments, and we propose that integrating emotions may benefit underrepresented groups in other mental health detection tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Ivey",
      "Susan Gauch"
    ]
  },
  "http://arxiv.org/abs/2311.17667": {
    "title": "TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models",
    "volume": "Nov",
    "abstract": "Understanding time is a pivotal aspect of human cognition, crucial in the broader framework of grasping the intricacies of the world. Previous studies typically focus on specific aspects of time, lacking a comprehensive temporal reasoning benchmark. To address this issue, we propose TimeBench, a comprehensive hierarchical temporal reasoning benchmark that covers a broad spectrum of temporal reasoning phenomena, which provides a thorough evaluation for investigating the temporal reasoning capabilities of large language models. We conduct extensive experiments on popular LLMs, such as GPT-4, LLaMA2, and Mistral, incorporating chain-of-thought prompting. Our experimental results indicate a significant performance gap between the state-of-the-art LLMs and humans, highlighting that there is still a considerable distance to cover in temporal reasoning. We aspire for TimeBench to serve as a comprehensive benchmark, fostering research in temporal reasoning for LLMs. Our resource is available at https://github.com/zchuz/TimeBench",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Chu",
      "Jingchang Chen",
      "Qianglong Chen",
      "Weijiang Yu",
      "Haotian Wang",
      "Ming Liu",
      "Bing Qin"
    ]
  },
  "http://arxiv.org/abs/2311.17647": {
    "title": "VIM: Probing Multimodal Large Language Models for Visual Embedded Instruction Following",
    "volume": "Nov",
    "abstract": "We introduce VISUAL EMBEDDED INSTRUCTION (VIM), a new framework designed to evaluate the visual instruction following capability of Multimodal Large Language Models (MLLMs). As illustrated in Figure 2, VIM challenges the MLLMs by embedding the instructions into the visual scenes, demanding strong visual interpretative skills for instruction following. We adapt VIM to various benchmarks, including VQAv2, MME, MM-Vet, and RefCOCO series, compose a VIM bench, and probe diverse MLLMs across three distinct in-context learning settings: Zero Shot, One Shot, and Pair Shot. We observe that there is a significant performance disparity between the open-source MLLMs and GPT-4V, implying that their proficiency in visual instruction comprehension is not up to par. Our results highlight a promising direction for the enhancement of MLLMs capabilities on instruction following. We aim VIM to serve as a useful norm for advancing the state of the art and driving further progress in the field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujie Lu",
      "Xiujun Li",
      "William Yang Wang",
      "Yejin Choi"
    ]
  },
  "http://arxiv.org/abs/2311.17633": {
    "title": "Introduction to Transformers: an NLP Perspective",
    "volume": "Nov",
    "abstract": "Transformers have dominated empirical machine learning models of natural language processing. In this paper, we introduce basic concepts of Transformers and present key techniques that form the recent advances of these models. This includes a description of the standard Transformer architecture, a series of model refinements, and common applications. Given that Transformers and related deep learning techniques might be evolving in ways we have never seen, we cannot dive into all the model details or cover all the technical areas. Instead, we focus on just those concepts that are helpful for gaining a good understanding of Transformers and their variants. We also summarize the key ideas that impact this field, thereby yielding some insights into the strengths and limitations of these models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Xiao",
      "Jingbo Zhu"
    ]
  },
  "http://arxiv.org/abs/2311.17593": {
    "title": "LanGWM: Language Grounded World Model",
    "volume": "Nov",
    "abstract": "Recent advances in deep reinforcement learning have showcased its potential in tackling complex tasks. However, experiments on visual control tasks have revealed that state-of-the-art reinforcement learning models struggle with out-of-distribution generalization. Conversely, expressing higher-level concepts and global contexts is relatively easy using language. Building upon recent success of the large language models, our main objective is to improve the state abstraction technique in reinforcement learning by leveraging language for robust action selection. Specifically, we focus on learning language-grounded visual features to enhance the world model learning, a model-based reinforcement learning technique. To enforce our hypothesis explicitly, we mask out the bounding boxes of a few objects in the image observation and provide the text prompt as descriptions for these masked objects. Subsequently, we predict the masked objects along with the surrounding regions as pixel reconstruction, similar to the transformer-based masked autoencoder approach. Our proposed LanGWM: Language Grounded World Model achieves state-of-the-art performance in out-of-distribution test at the 100K interaction steps benchmarks of iGibson point navigation tasks. Furthermore, our proposed technique of explicit language-grounded visual representation learning has the potential to improve models for human-robot interaction because our extracted visual features are language grounded",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rudra P. K. Poudel",
      "Harit Pandya",
      "Chao Zhang",
      "Roberto Cipolla"
    ]
  },
  "http://arxiv.org/abs/2311.17514": {
    "title": "Reinforcement Replaces Supervision: Query focused Summarization using Deep Reinforcement Learning",
    "volume": "Nov",
    "abstract": "Query-focused Summarization (QfS) deals with systems that generate summaries from document(s) based on a query. Motivated by the insight that Reinforcement Learning (RL) provides a generalization to Supervised Learning (SL) for Natural Language Generation, and thereby performs better (empirically) than SL, we use an RL-based approach for this task of QfS. Additionally, we also resolve the conflict of employing RL in Transformers with Teacher Forcing. We develop multiple Policy Gradient networks, trained on various reward signals: ROUGE, BLEU, and Semantic Similarity, which lead to a 10-point improvement over the State-of-the-Art approach on the ROUGE-L metric for a benchmark dataset (ELI5). We also show performance of our approach in zero-shot setting for another benchmark dataset (DebatePedia) -- our approach leads to results comparable to baselines, which were specifically trained on DebatePedia. To aid the RL training, we propose a better semantic similarity reward, enabled by a novel Passage Embedding scheme developed using Cluster Hypothesis. Lastly, we contribute a gold-standard test dataset to further research in QfS and Long-form Question Answering (LfQA)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Swaroop Nath",
      "Harshad Khadilkar",
      "Pushpak Bhattacharyya"
    ]
  },
  "http://arxiv.org/abs/2311.17502": {
    "title": "Enhancing Answer Selection in Community Question Answering with Pre-trained and Large Language Models",
    "volume": "Nov",
    "abstract": "Community Question Answering (CQA) becomes increasingly prevalent in recent years. However, there are a large number of answers, which is difficult for users to select the relevant answers. Therefore, answer selection is a very significant subtask of CQA. In this paper, we first propose the Question-Answer cross attention networks (QAN) with pre-trained models for answer selection and utilize large language model (LLM) to perform answer selection with knowledge augmentation. Specifically, we apply the BERT model as the encoder layer to do pre-training for question subjects, question bodies and answers, respectively, then the cross attention mechanism selects the most relevant answer for different questions. Experiments show that the QAN model achieves state-of-the-art performance on two datasets, SemEval2015 and SemEval2017. Moreover, we use the LLM to generate external knowledge from questions and correct answers to achieve knowledge augmentation for the answer selection task by LLM, while optimizing the prompt of LLM in different aspects. The results show that the introduction of external knowledge can improve the correct answer selection rate of LLM on datasets SemEval2015 and SemEval2017. Meanwhile, LLM can also select the correct answer on more questions by optimized prompt",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinghang Hu"
    ]
  },
  "http://arxiv.org/abs/2311.17492": {
    "title": "Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data",
    "volume": "Nov",
    "abstract": "The Manchu language, with its roots in the historical Manchurian region of Northeast China, is now facing a critical threat of extinction, as there are very few speakers left. In our efforts to safeguard the Manchu language, we introduce Mergen, the first-ever attempt at a Manchu-Korean Machine Translation (MT) model. To develop this model, we utilize valuable resources such as the Manwen Laodang(a historical book) and a Manchu-Korean dictionary. Due to the scarcity of a Manchu-Korean parallel dataset, we expand our data by employing word replacement guided by GloVe embeddings, trained on both monolingual and parallel texts. Our approach is built around an encoder-decoder neural machine translation model, incorporating a bi-directional Gated Recurrent Unit (GRU) layer. The experiments have yielded promising results, showcasing a significant enhancement in Manchu-Korean translation, with a remarkable 20-30 point increase in the BLEU score",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jean Seo",
      "Sungjoo Byun",
      "Minha Kang",
      "Sangah Lee"
    ]
  },
  "http://arxiv.org/abs/2311.17487": {
    "title": "Taiwan LLM: Bridging the Linguistic Divide with a Culturally Aligned Language Model",
    "volume": "Nov",
    "abstract": "In the realm of language models, the nuanced linguistic and cultural intricacies of Traditional Chinese, as spoken in Taiwan, have been largely overlooked. This paper introduces Taiwan LLM, a pioneering Large Language Model that specifically caters to the Traditional Chinese language, with a focus on the variant used in Taiwan. Leveraging a comprehensive pretraining corpus and instruction-finetuning datasets, we have developed a model that not only understands the complexities of Traditional Chinese but also embodies the cultural context of Taiwan. Taiwan LLM represents the first of its kind, a model that is not only linguistically accurate but also culturally resonant with its user base. Our evaluations demonstrate that Taiwan LLM achieves superior performance in understanding and generating Traditional Chinese text, outperforming existing models that are predominantly trained on Simplified Chinese or English. The open-source release of Taiwan LLM invites collaboration and further innovation, ensuring that the linguistic diversity of Chinese speakers is embraced and well-served. The model, datasets, and further resources are made publicly available to foster ongoing research and development in this field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yen-Ting Lin",
      "Yun-Nung Chen"
    ]
  },
  "http://arxiv.org/abs/2311.17438": {
    "title": "CLOMO: Counterfactual Logical Modification with Large Language Models",
    "volume": "Nov",
    "abstract": "In this study, we delve into the realm of counterfactual reasoning capabilities of large language models (LLMs). Our primary objective is to cultivate the counterfactual thought processes within LLMs and rigorously assess these processes for their validity. Specifically, we introduce a novel task, Counterfactual Logical Modification (CLOMO), and a high-quality human-annotated benchmark. In this task, LLMs must adeptly alter a given argumentative text to uphold a predetermined logical relationship. To effectively evaluate a generation model's counterfactual capabilities, we propose an innovative evaluation metric, the LogicAware Counterfactual Score to directly evaluate the natural language output of LLMs instead of modeling the task as a multiple-choice problem. Analysis shows that the proposed automatic metric aligns well with human preference. Our experimental results show that while LLMs demonstrate a notable capacity for logical counterfactual thinking, there remains a discernible gap between their current abilities and human performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinya Huang",
      "Ruixin Hong",
      "Hongming Zhang",
      "Wei Shao",
      "Zhicheng Yang",
      "Dong Yu",
      "Changshui Zhang",
      "Xiaodan Liang",
      "Linqi Song"
    ]
  },
  "http://arxiv.org/abs/2311.17429": {
    "title": "TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP Models via GPT4",
    "volume": "Nov",
    "abstract": "Prompt-based learning has been widely applied in many low-resource NLP tasks such as few-shot scenarios. However, this paradigm has been shown to be vulnerable to backdoor attacks. Most of the existing attack methods focus on inserting manually predefined templates as triggers in the pre-training phase to train the victim model and utilize the same triggers in the downstream task to perform inference, which tends to ignore the transferability and stealthiness of the templates. In this work, we propose a novel approach of TARGET (Template-trAnsfeRable backdoor attack aGainst prompt-basEd NLP models via GPT4), which is a data-independent attack method. Specifically, we first utilize GPT4 to reformulate manual templates to generate tone-strong and normal templates, and the former are injected into the model as a backdoor trigger in the pre-training phase. Then, we not only directly employ the above templates in the downstream task, but also use GPT4 to generate templates with similar tone to the above templates to carry out transferable attacks. Finally we have conducted extensive experiments on five NLP datasets and three BERT series models, with experimental results justifying that our TARGET method has better attack performance and stealthiness compared to the two-external baseline methods on direct attacks, and in addition achieves satisfactory attack capability in the unseen tone-similar templates",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Tan",
      "Qingliang Chen",
      "Yongjian Huang",
      "Chen Liang"
    ]
  },
  "http://arxiv.org/abs/2312.02181": {
    "title": "How Generative-AI can be Effectively used in Government Chatbots",
    "volume": "Nov",
    "abstract": "With the rapid development of artificial intelligence and breakthroughs in machine learning and natural language processing, intelligent question-answering robots have become widely used in government affairs. This paper conducts a horizontal comparison between Guangdong Province's government chatbots, ChatGPT, and Wenxin Ernie, two large language models, to analyze the strengths and weaknesses of existing government chatbots and AIGC technology. The study finds significant differences between government chatbots and large language models. China's government chatbots are still in an exploratory stage and have a gap to close to achieve \"intelligence.\" To explore the future direction of government chatbots more deeply, this research proposes targeted optimization paths to help generative AI be effectively applied in government chatbot conversations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeteng Lin"
    ]
  },
  "http://arxiv.org/abs/2311.17404": {
    "title": "VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models",
    "volume": "Nov",
    "abstract": "The ability to perceive how objects change over time is a crucial ingredient in human intelligence. However, current benchmarks cannot faithfully reflect the temporal understanding abilities of video-language models (VidLMs) due to the existence of static visual shortcuts. To remedy this issue, we present VITATECS, a diagnostic VIdeo-Text dAtaset for the evaluation of TEmporal Concept underStanding. Specifically, we first introduce a fine-grained taxonomy of temporal concepts in natural language in order to diagnose the capability of VidLMs to comprehend different temporal aspects. Furthermore, to disentangle the correlation between static and temporal information, we generate counterfactual video descriptions that differ from the original one only in the specified temporal aspect. We employ a semi-automatic data collection framework using large language models and human-in-the-loop annotation to obtain high-quality counterfactual descriptions efficiently. Evaluation of representative video-language understanding models confirms their deficiency in temporal understanding, revealing the need for greater emphasis on the temporal elements in video-language research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shicheng Li",
      "Lei Li",
      "Shuhuai Ren",
      "Yuanxin Liu",
      "Yi Liu",
      "Rundong Gao",
      "Xu Sun",
      "Lu Hou"
    ]
  },
  "http://arxiv.org/abs/2311.17400": {
    "title": "Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention",
    "volume": "Nov",
    "abstract": "Transformer-based models, such as BERT and GPT, have been widely adopted in natural language processing (NLP) due to their exceptional performance. However, recent studies show their vulnerability to textual adversarial attacks where the model's output can be misled by intentionally manipulating the text inputs. Despite various methods that have been proposed to enhance the model's robustness and mitigate this vulnerability, many require heavy consumption resources (e.g., adversarial training) or only provide limited protection (e.g., defensive dropout). In this paper, we propose a novel method called dynamic attention, tailored for the transformer architecture, to enhance the inherent robustness of the model itself against various adversarial attacks. Our method requires no downstream task knowledge and does not incur additional costs. The proposed dynamic attention consists of two modules: (I) attention rectification, which masks or weakens the attention value of the chosen tokens, and (ii) dynamic modeling, which dynamically builds the set of candidate tokens. Extensive experiments demonstrate that dynamic attention significantly mitigates the impact of adversarial attacks, improving up to 33\\% better performance than previous methods against widely-used adversarial attacks. The model-level design of dynamic attention enables it to be easily combined with other defense methods (e.g., adversarial training) to further enhance the model's robustness. Furthermore, we demonstrate that dynamic attention preserves the state-of-the-art robustness space of the original model compared to other dynamic modeling methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lujia Shen",
      "Yuwen Pu",
      "Shouling Ji",
      "Changjiang Li",
      "Xuhong Zhang",
      "Chunpeng Ge",
      "Ting Wang"
    ]
  },
  "http://arxiv.org/abs/2311.17391": {
    "title": "Unveiling the Implicit Toxicity in Large Language Models",
    "volume": "Nov",
    "abstract": "The open-endedness of large language models (LLMs) combined with their impressive capabilities may lead to new safety issues when being exploited for malicious use. While recent studies primarily focus on probing toxic outputs that can be easily detected with existing toxicity classifiers, we show that LLMs can generate diverse implicit toxic outputs that are exceptionally difficult to detect via simply zero-shot prompting. Moreover, we propose a reinforcement learning (RL) based attacking method to further induce the implicit toxicity in LLMs. Specifically, we optimize the language model with a reward that prefers implicit toxic outputs to explicit toxic and non-toxic ones. Experiments on five widely-adopted toxicity classifiers demonstrate that the attack success rate can be significantly improved through RL fine-tuning. For instance, the RL-finetuned LLaMA-13B model achieves an attack success rate of 90.04% on BAD and 62.85% on Davinci003. Our findings suggest that LLMs pose a significant threat in generating undetectable implicit toxic outputs. We further show that fine-tuning toxicity classifiers on the annotated examples from our attacking method can effectively enhance their ability to detect LLM-generated implicit toxic language. The code is publicly available at https://github.com/thu-coai/Implicit-Toxicity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Wen",
      "Pei Ke",
      "Hao Sun",
      "Zhexin Zhang",
      "Chengfei Li",
      "Jinfeng Bai",
      "Minlie Huang"
    ]
  },
  "http://arxiv.org/abs/2311.17376": {
    "title": "CESAR: Automatic Induction of Compositional Instructions for Multi-turn Dialogs",
    "volume": "Nov",
    "abstract": "Instruction-based multitasking has played a critical role in the success of large language models (LLMs) in multi-turn dialog applications. While publicly available LLMs have shown promising performance, when exposed to complex instructions with multiple constraints, they lag against state-of-the-art models like ChatGPT. In this work, we hypothesize that the availability of large-scale complex demonstrations is crucial in bridging this gap. Focusing on dialog applications, we propose a novel framework, CESAR, that unifies a large number of dialog tasks in the same format and allows programmatic induction of complex instructions without any manual effort. We apply CESAR on InstructDial, a benchmark for instruction-based dialog tasks. We further enhance InstructDial with new datasets and tasks and utilize CESAR to induce complex tasks with compositional instructions. This results in a new benchmark called InstructDial++, which includes 63 datasets with 86 basic tasks and 68 composite tasks. Through rigorous experiments, we demonstrate the scalability of CESAR in providing rich instructions. Models trained on InstructDial++ can follow compositional prompts, such as prompts that ask for multiple stylistic constraints",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taha Aksu",
      "Devamanyu Hazarika",
      "Shikib Mehri",
      "Seokhwan Kim",
      "Dilek Hakkani-Tür",
      "Yang Liu",
      "Mahdi Namazifar"
    ]
  },
  "http://arxiv.org/abs/2311.17371": {
    "title": "Are we going MAD? Benchmarking Multi-Agent Debate between Language Models for Medical Q&A",
    "volume": "Nov",
    "abstract": "Recent advancements in large language models (LLMs) underscore their potential for responding to medical inquiries. However, ensuring that generative agents provide accurate and reliable answers remains an ongoing challenge. In this context, multi-agent debate (MAD) has emerged as a prominent strategy for enhancing the truthfulness of LLMs. In this work, we provide a comprehensive benchmark of MAD strategies for medical Q&A, along with open-source implementations. This explores the effective utilization of various strategies including the trade-offs between cost, time, and accuracy. We build upon these insights to provide a novel debate-prompting strategy based on agent agreement that outperforms previously published strategies on medical Q&A tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andries Smit",
      "Paul Duckworth",
      "Nathan Grinsztajn",
      "Kale-ab Tessera",
      "Thomas D. Barrett",
      "Arnu Pretorius"
    ]
  },
  "http://arxiv.org/abs/2311.17355": {
    "title": "Are Large Language Models Good Fact Checkers: A Preliminary Study",
    "volume": "Nov",
    "abstract": "Recently, Large Language Models (LLMs) have drawn significant attention due to their outstanding reasoning capabilities and extensive knowledge repository, positioning them as superior in handling various natural language processing tasks compared to other language models. In this paper, we present a preliminary investigation into the potential of LLMs in fact-checking. This study aims to comprehensively evaluate various LLMs in tackling specific fact-checking subtasks, systematically evaluating their capabilities, and conducting a comparative analysis of their performance against pre-trained and state-of-the-art low-parameter models. Experiments demonstrate that LLMs achieve competitive performance compared to other small models in most scenarios. However, they encounter challenges in effectively handling Chinese fact verification and the entirety of the fact-checking pipeline due to language inconsistencies and hallucinations. These findings underscore the need for further exploration and research to enhance the proficiency of LLMs as reliable fact-checkers, unveiling the potential capability of LLMs and the possible challenges in fact-checking tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Cao",
      "Lingwei Wei",
      "Mengyang Chen",
      "Wei Zhou",
      "Songlin Hu"
    ]
  },
  "http://arxiv.org/abs/2311.17352": {
    "title": "Efficient Stitchable Task Adaptation",
    "volume": "Nov",
    "abstract": "The paradigm of pre-training and fine-tuning has laid the foundation for deploying deep learning models. However, most fine-tuning methods are designed to meet a specific resource budget. Recently, considering diverse deployment scenarios with various resource budgets, stitchable neural network (SN-Net) is introduced to quickly obtain numerous new networks (stitches) from the pre-trained models (anchors) in a model family via model stitching. Although promising, SN-Net confronts new challenges when adapting it to new target domains, including huge memory and storage requirements and a long and sub-optimal multistage adaptation process. In this work, we present a novel framework, Efficient Stitchable Task Adaptation (ESTA), to efficiently produce a palette of fine-tuned models that adhere to diverse resource constraints. Specifically, we first tailor parameter-efficient fine-tuning to share low-rank updates among the stitches while maintaining independent bias terms. In this way, we largely reduce fine-tuning memory burdens and mitigate the interference among stitches that arises in task adaptation. Furthermore, we streamline a simple yet effective one-stage deployment pipeline, which estimates the important stitches to deploy with training-time gradient statistics. By assigning higher sampling probabilities to important stitches, we also get a boosted Pareto frontier. Extensive experiments on 25 downstream visual recognition tasks demonstrate that our ESTA is capable of generating stitches with smooth accuracy-efficiency trade-offs and surpasses the direct SN-Net adaptation by remarkable margins with significantly lower training time and fewer trainable parameters. Furthermore, we demonstrate the flexibility and scalability of our ESTA framework by stitching LLMs from LLaMA family, obtaining chatbot stitches of assorted sizes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu He",
      "Zizheng Pan",
      "Jing Liu",
      "Jianfei Cai",
      "Bohan Zhuang"
    ]
  },
  "http://arxiv.org/abs/2311.17351": {
    "title": "Exploring Large Language Models for Human Mobility Prediction under Public Events",
    "volume": "Nov",
    "abstract": "Public events, such as concerts and sports games, can be major attractors for large crowds, leading to irregular surges in travel demand. Accurate human mobility prediction for public events is thus crucial for event planning as well as traffic or crowd management. While rich textual descriptions about public events are commonly available from online sources, it is challenging to encode such information in statistical or machine learning models. Existing methods are generally limited in incorporating textual information, handling data sparsity, or providing rationales for their predictions. To address these challenges, we introduce a framework for human mobility prediction under public events (LLM-MPE) based on Large Language Models (LLMs), leveraging their unprecedented ability to process textual data, learn from minimal examples, and generate human-readable explanations. Specifically, LLM-MPE first transforms raw, unstructured event descriptions from online sources into a standardized format, and then segments historical mobility data into regular and event-related components. A prompting strategy is designed to direct LLMs in making and rationalizing demand predictions considering historical mobility and event features. A case study is conducted for Barclays Center in New York City, based on publicly available event information and taxi trip data. Results show that LLM-MPE surpasses traditional models, particularly on event days, with textual data significantly enhancing its accuracy. Furthermore, LLM-MPE offers interpretable insights into its predictions. Despite the great potential of LLMs, we also identify key challenges including misinformation and high costs that remain barriers to their broader adoption in large-scale human mobility analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuebing Liang",
      "Yichao Liu",
      "Xiaohan Wang",
      "Zhan Zhao"
    ]
  },
  "http://arxiv.org/abs/2311.17946": {
    "title": "DreamSync: Aligning Text-to-Image Generation with Image Understanding Feedback",
    "volume": "Nov",
    "abstract": "Despite their wide-spread success, Text-to-Image models (T2I) still struggle to produce images that are both aesthetically pleasing and faithful to the user's input text. We introduce DreamSync, a model-agnostic training algorithm by design that improves T2I models to be faithful to the text input. DreamSync builds off a recent insight from TIFA's evaluation framework -- that large vision-language models (VLMs) can effectively identify the fine-grained discrepancies between generated images and the text inputs. DreamSync uses this insight to train T2I models without any labeled data; it improves T2I models using its own generations. First, it prompts the model to generate several candidate images for a given input text. Then, it uses two VLMs to select the best generation: a Visual Question Answering model that measures the alignment of generated images to the text, and another that measures the generation's aesthetic quality. After selection, we use LoRA to iteratively finetune the T2I model to guide its generation towards the selected best generations. DreamSync does not need any additional human annotation. model architecture changes, or reinforcement learning. Despite its simplicity, DreamSync improves both the semantic alignment and aesthetic appeal of two diffusion-based T2I models, evidenced by multiple benchmarks (+1.7% on TIFA, +2.9% on DSG1K, +3.4% on VILA aesthetic) and human evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiao Sun",
      "Deqing Fu",
      "Yushi Hu",
      "Su Wang",
      "Royi Rassin",
      "Da-Cheng Juan",
      "Dana Alon",
      "Charles Herrmann",
      "Sjoerd van Steenkiste",
      "Ranjay Krishna",
      "Cyrus Rashtchian"
    ]
  },
  "http://arxiv.org/abs/2311.17330": {
    "title": "Biomedical knowledge graph-enhanced prompt generation for large language models",
    "volume": "Nov",
    "abstract": "Large Language Models (LLMs) have been driving progress in AI at an unprecedented rate, yet still face challenges in knowledge-intensive domains like biomedicine. Solutions such as pre-training and domain-specific fine-tuning add substantial computational overhead, and the latter require domain-expertise. External knowledge infusion is task-specific and requires model training. Here, we introduce a task-agnostic Knowledge Graph-based Retrieval Augmented Generation (KG-RAG) framework by leveraging the massive biomedical KG SPOKE with LLMs such as Llama-2-13b, GPT-3.5-Turbo and GPT-4, to generate meaningful biomedical text rooted in established knowledge. KG-RAG consistently enhanced the performance of LLMs across various prompt types, including one-hop and two-hop prompts, drug repurposing queries, biomedical true/false questions, and multiple-choice questions (MCQ). Notably, KG-RAG provides a remarkable 71% boost in the performance of the Llama-2 model on the challenging MCQ dataset, demonstrating the framework's capacity to empower open-source models with fewer parameters for domain-specific questions. Furthermore, KG-RAG enhanced the performance of proprietary GPT models, such as GPT-3.5 which exhibited improvement over GPT-4 in context utilization on MCQ data. Our approach was also able to address drug repurposing questions, returning meaningful repurposing suggestions. In summary, the proposed framework combines explicit and implicit knowledge of KG and LLM, respectively, in an optimized fashion, thus enhancing the adaptability of general-purpose LLMs to tackle domain-specific questions in a unified framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karthik Soman",
      "Peter W Rose",
      "John H Morris",
      "Rabia E Akbas",
      "Brett Smith",
      "Braian Peetoom",
      "Catalina Villouta-Reyes",
      "Gabriel Cerono",
      "Yongmei Shi",
      "Angela Rizk-Jackson",
      "Sharat Israni",
      "Charlotte A Nelson",
      "Sui Huang",
      "Sergio E Baranzini"
    ]
  },
  "http://arxiv.org/abs/2311.17311": {
    "title": "Universal Self-Consistency for Large Language Model Generation",
    "volume": "Nov",
    "abstract": "Self-consistency with chain-of-thought prompting (CoT) has demonstrated remarkable performance gains on various challenging tasks, by utilizing multiple reasoning paths sampled from large language models (LLMs). However, self-consistency relies on the answer extraction process to aggregate multiple solutions, which is not applicable to free-form answers. In this work, we propose Universal Self-Consistency (USC), which leverages LLMs themselves to select the most consistent answer among multiple candidates. We evaluate USC on a variety of benchmarks, including mathematical reasoning, code generation, long-context summarization, and open-ended question answering. On open-ended generation tasks where the original self-consistency method is not applicable, USC effectively utilizes multiple samples and improves the performance. For mathematical reasoning, USC matches the standard self-consistency performance without requiring the answer formats to be similar. Finally, without access to execution results, USC also matches the execution-based voting performance on code generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyun Chen",
      "Renat Aksitov",
      "Uri Alon",
      "Jie Ren",
      "Kefan Xiao",
      "Pengcheng Yin",
      "Sushant Prakash",
      "Charles Sutton",
      "Xuezhi Wang",
      "Denny Zhou"
    ]
  },
  "http://arxiv.org/abs/2311.17307": {
    "title": "RoKEPG: RoBERTa and Knowledge Enhancement for Prescription Generation of Traditional Chinese Medicine",
    "volume": "Nov",
    "abstract": "Traditional Chinese medicine (TCM) prescription is the most critical form of TCM treatment, and uncovering the complex nonlinear relationship between symptoms and TCM is of great significance for clinical practice and assisting physicians in diagnosis and treatment. Although there have been some studies on TCM prescription generation, these studies consider a single factor and directly model the symptom-prescription generation problem mainly based on symptom descriptions, lacking guidance from TCM knowledge. To this end, we propose a RoBERTa and Knowledge Enhancement model for Prescription Generation of Traditional Chinese Medicine (RoKEPG). RoKEPG is firstly pre-trained by our constructed TCM corpus, followed by fine-tuning the pre-trained model, and the model is guided to generate TCM prescriptions by introducing four classes of knowledge of TCM through the attention mask matrix. Experimental results on the publicly available TCM prescription dataset show that RoKEPG improves the F1 metric by about 2% over the baseline model with the best results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hua Pu",
      "Jiacong Mi",
      "Shan Lu",
      "Jieyue He"
    ]
  },
  "http://arxiv.org/abs/2311.17301": {
    "title": "Language Models: A Guide for the Perplexed",
    "volume": "Nov",
    "abstract": "Given the growing importance of AI literacy, we decided to write this tutorial to help narrow the gap between the discourse among those who study language models -- the core technology underlying ChatGPT and similar products -- and those who are intrigued and want to learn more about them. In short, we believe the perspective of researchers and educators can add some clarity to the public's understanding of the technologies beyond what's currently available, which tends to be either extremely technical or promotional material generated about products by their purveyors. Our approach teases apart the concept of a language model from products built on them, from the behaviors attributed to or desired from those products, and from claims about similarity to human cognition. As a starting point, we (1) offer a scientific viewpoint that focuses on questions amenable to study through experimentation; (2) situate language models as they are today in the context of the research that led to their development; and (3) describe the boundaries of what is known about the models at this writing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sofia Serrano",
      "Zander Brumbaugh",
      "Noah A. Smith"
    ]
  },
  "http://arxiv.org/abs/2311.17295": {
    "title": "Elo Uncovered: Robustness and Best Practices in Language Model Evaluation",
    "volume": "Nov",
    "abstract": "In Natural Language Processing (NLP), the Elo rating system, originally designed for ranking players in dynamic games such as chess, is increasingly being used to evaluate Large Language Models (LLMs) through \"A vs B\" paired comparisons. However, while popular, the system's suitability for assessing entities with constant skill levels, such as LLMs, remains relatively unexplored. We study two fundamental axioms that evaluation methods should adhere to: reliability and transitivity. We conduct extensive evaluation of Elo behaviour, illustrating that individual Elo computations exhibit volatility and delving into the impact of varying the Elo rating system's hyperparameters. We show that these axioms are not always satisfied raising questions about the reliability of current comparative evaluations of LLMs. If the current use of Elo scores is intended to substitute the costly head-to-head comparison of LLMs, it is crucial to ensure the ranking is as robust as possible. Guided by the axioms, our findings offer concrete guidelines for enhancing the reliability of LLM evaluation methods, suggesting a need for reassessment of existing comparative approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meriem Boubdir",
      "Edward Kim",
      "Beyza Ermis",
      "Sara Hooker",
      "Marzieh Fadaee"
    ]
  },
  "http://arxiv.org/abs/2311.17280": {
    "title": "Does VLN Pretraining Work with Nonsensical or Irrelevant Instructions?",
    "volume": "Dec",
    "abstract": "Data augmentation via back-translation is common when pretraining Vision-and-Language Navigation (VLN) models, even though the generated instructions are noisy. But: does that noise matter? We find that nonsensical or irrelevant language instructions during pretraining can have little effect on downstream performance for both HAMT and VLN-BERT on R2R, and is still better than only using clean, human data. To underscore these results, we concoct an efficient augmentation method, Unigram + Object, which generates nonsensical instructions that nonetheless improve downstream performance. Our findings suggest that what matters for VLN R2R pretraining is the quantity of visual trajectories, not the quality of instructions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wang Zhu",
      "Ishika Singh",
      "Yuan Huang",
      "Robin Jia",
      "Jesse Thomason"
    ]
  },
  "http://arxiv.org/abs/2311.17264": {
    "title": "RETSim: Resilient and Efficient Text Similarity",
    "volume": "Nov",
    "abstract": "This paper introduces RETSim (Resilient and Efficient Text Similarity), a lightweight, multilingual deep learning model trained to produce robust metric embeddings for near-duplicate text retrieval, clustering, and dataset deduplication tasks. We demonstrate that RETSim is significantly more robust and accurate than MinHash and neural text embeddings, achieving new state-of-the-art performance on dataset deduplication, adversarial text retrieval benchmarks, and spam clustering tasks. We also introduce the W4NT3D benchmark (Wiki-40B 4dversarial Near-T3xt Dataset) for evaluating multilingual, near-duplicate text retrieval capabilities under adversarial settings. RETSim and the W4NT3D benchmark are open-sourced under the MIT License at https://github.com/google/unisim",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marina Zhang",
      "Owen Vallis",
      "Aysegul Bumin",
      "Tanay Vakharia",
      "Elie Bursztein"
    ]
  },
  "http://arxiv.org/abs/2311.17233": {
    "title": "Quantifying the redundancy between prosody and text",
    "volume": "Nov",
    "abstract": "Prosody -- the suprasegmental component of speech, including pitch, loudness, and tempo -- carries critical aspects of meaning. However, the relationship between the information conveyed by prosody vs. by the words themselves remains poorly understood. We use large language models (LLMs) to estimate how much information is redundant between prosody and the words themselves. Using a large spoken corpus of English audiobooks, we extract prosodic features aligned to individual words and test how well they can be predicted from LLM embeddings, compared to non-contextual word embeddings. We find a high degree of redundancy between the information carried by the words and prosodic information across several prosodic features, including intensity, duration, pauses, and pitch contours. Furthermore, a word's prosodic information is redundant with both the word itself and the context preceding as well as following it. Still, we observe that prosodic features can not be fully predicted from text, suggesting that prosody carries information above and beyond the words. Along with this paper, we release a general-purpose data processing pipeline for quantifying the relationship between linguistic information and extra-linguistic features",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lukas Wolf",
      "Tiago Pimentel",
      "Evelina Fedorenko",
      "Ryan Cotterell",
      "Alex Warstadt",
      "Ethan Wilcox",
      "Tamar Regev"
    ]
  },
  "http://arxiv.org/abs/2311.17227": {
    "title": "War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars",
    "volume": "Nov",
    "abstract": "Can we avoid wars at the crossroads of history? This question has been pursued by individuals, scholars, policymakers, and organizations throughout human history. In this research, we attempt to answer the question based on the recent advances of Artificial Intelligence (AI) and Large Language Models (LLMs). We propose \\textbf{WarAgent}, an LLM-powered multi-agent AI system, to simulate the participating countries, their decisions, and the consequences, in historical international conflicts, including the World War I (WWI), the World War II (WWII), and the Warring States Period (WSP) in Ancient China. By evaluating the simulation effectiveness, we examine the advancements and limitations of cutting-edge AI systems' abilities in studying complex collective human behaviors such as international conflicts under diverse settings. In these simulations, the emergent interactions among agents also offer a novel perspective for examining the triggers and conditions that lead to war. Our findings offer data-driven and AI-augmented insights that can redefine how we approach conflict resolution and peacekeeping strategies. The implications stretch beyond historical analysis, offering a blueprint for using AI to understand human history and possibly prevent future international conflicts. Code and data are available at \\url{https://github.com/agiresearch/WarAgent}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyue Hua",
      "Lizhou Fan",
      "Lingyao Li",
      "Kai Mei",
      "Jianchao Ji",
      "Yingqiang Ge",
      "Libby Hemphill",
      "Yongfeng Zhang"
    ]
  },
  "http://arxiv.org/abs/2311.17213": {
    "title": "General-Purpose vs. Domain-Adapted Large Language Models for Extraction of Data from Thoracic Radiology Reports",
    "volume": "Dec",
    "abstract": "Radiologists produce unstructured data that could be valuable for clinical care when consumed by information systems. However, variability in style limits usage. Study compares performance of system using domain-adapted language model (RadLing) and general-purpose large language model (GPT-4) in extracting common data elements (CDE) from thoracic radiology reports. Three radiologists annotated a retrospective dataset of 1300 thoracic reports (900 training, 400 test) and mapped to 21 pre-selected relevant CDEs. RadLing was used to generate embeddings for sentences and identify CDEs using cosine-similarity, which were mapped to values using light-weight mapper. GPT-4 system used OpenAI's general-purpose embeddings to identify relevant CDEs and used GPT-4 to map to values. The output CDE:value pairs were compared to the reference standard; an identical match was considered true positive. Precision (positive predictive value) was 96% (2700/2824) for RadLing and 99% (2034/2047) for GPT-4. Recall (sensitivity) was 94% (2700/2876) for RadLing and 70% (2034/2887) for GPT-4; the difference was statistically significant (P<.001). RadLing's domain-adapted embeddings were more sensitive in CDE identification (95% vs 71%) and its light-weight mapper had comparable precision in value assignment (95.4% vs 95.0%). RadLing system exhibited higher performance than GPT-4 system in extracting CDEs from radiology reports. RadLing system's domain-adapted embeddings outperform general-purpose embeddings from OpenAI in CDE identification and its light-weight value mapper achieves comparable precision to large GPT-4. RadLing system offers operational advantages including local deployment and reduced runtime costs. Domain-adapted RadLing system surpasses GPT-4 system in extracting common data elements from radiology reports, while providing benefits of local deployment and lower costs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali H. Dhanaliwala",
      "Rikhiya Ghosh",
      "Sanjeev Kumar Karn",
      "Poikavila Ullaskrishnan",
      "Oladimeji Farri",
      "Dorin Comaniciu",
      "Charles E. Kahn"
    ]
  },
  "http://arxiv.org/abs/2311.17154": {
    "title": "Pragmatic Radiology Report Generation",
    "volume": "Nov",
    "abstract": "When pneumonia is not found on a chest X-ray, should the report describe this negative observation or omit it? We argue that this question cannot be answered from the X-ray alone and requires a pragmatic perspective, which captures the communicative goal that radiology reports serve between radiologists and patients. However, the standard image-to-text formulation for radiology report generation fails to incorporate such pragmatic intents. Following this pragmatic perspective, we demonstrate that the indication, which describes why a patient comes for an X-ray, drives the mentions of negative observations and introduce indications as additional input to report generation. With respect to the output, we develop a framework to identify uninferable information from the image as a source of model hallucinations, and limit them by cleaning groundtruth reports. Finally, we use indications and cleaned groundtruth reports to develop pragmatic models, and show that they outperform existing methods not only in new pragmatics-inspired metrics (+4.3 Negative F1) but also in standard metrics (+6.3 Positive F1 and +11.0 BLEU-2)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dang Nguyen",
      "Chacha Chen",
      "He He",
      "Chenhao Tan"
    ]
  },
  "http://arxiv.org/abs/2311.17136": {
    "title": "UniIR: Training and Benchmarking Universal Multimodal Information Retrievers",
    "volume": "Nov",
    "abstract": "Existing information retrieval (IR) models often assume a homogeneous format, limiting their applicability to diverse user needs, such as searching for images with text descriptions, searching for a news article with a headline image, or finding a similar photo with a query image. To approach such different information-seeking demands, we introduce UniIR, a unified instruction-guided multimodal retriever capable of handling eight distinct retrieval tasks across modalities. UniIR, a single retrieval system jointly trained on ten diverse multimodal-IR datasets, interprets user instructions to execute various retrieval tasks, demonstrating robust performance across existing datasets and zero-shot generalization to new tasks. Our experiments highlight that multi-task training and instruction tuning are keys to UniIR's generalization ability. Additionally, we construct the M-BEIR, a multimodal retrieval benchmark with comprehensive results, to standardize the evaluation of universal multimodal information retrieval",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Wei",
      "Yang Chen",
      "Haonan Chen",
      "Hexiang Hu",
      "Ge Zhang",
      "Jie Fu",
      "Alan Ritter",
      "Wenhu Chen"
    ]
  },
  "http://arxiv.org/abs/2311.17049": {
    "title": "MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training",
    "volume": "Nov",
    "abstract": "Contrastive pretraining of image-text foundation models, such as CLIP, demonstrated excellent zero-shot performance and improved robustness on a wide range of downstream tasks. However, these models utilize large transformer-based encoders with significant memory and latency overhead which pose challenges for deployment on mobile devices. In this work, we introduce MobileCLIP -- a new family of efficient image-text models optimized for runtime performance along with a novel and efficient training approach, namely multi-modal reinforced training. The proposed training approach leverages knowledge transfer from an image captioning model and an ensemble of strong CLIP encoders to improve the accuracy of efficient models. Our approach avoids train-time compute overhead by storing the additional knowledge in a reinforced dataset. MobileCLIP sets a new state-of-the-art latency-accuracy tradeoff for zero-shot classification and retrieval tasks on several datasets. Our MobileCLIP-S2 variant is 2.3$\\times$ faster while more accurate compared to previous best CLIP model based on ViT-B/16. We further demonstrate the effectiveness of our multi-modal reinforced training by training a CLIP model based on ViT-B/16 image backbone and achieving +2.9% average performance improvement on 38 evaluation benchmarks compared to the previous best. Moreover, we show that the proposed approach achieves 10$\\times$-1000$\\times$ improved learning efficiency when compared with non-reinforced CLIP training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pavan Kumar Anasosalu Vasu",
      "Hadi Pouransari",
      "Fartash Faghri",
      "Raviteja Vemulapalli",
      "Oncel Tuzel"
    ]
  },
  "http://arxiv.org/abs/2311.17043": {
    "title": "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models",
    "volume": "Nov",
    "abstract": "In this work, we present a novel method to tackle the token generation challenge in Vision Language Models (VLMs) for video and image understanding, called LLaMA-VID. Current VLMs, while proficient in tasks like image captioning and visual question answering, face computational burdens when processing long videos due to the excessive visual tokens. LLaMA-VID addresses this issue by representing each frame with two distinct tokens, namely context token and content token. The context token encodes the overall image context based on user input, whereas the content token encapsulates visual cues in each frame. This dual-token strategy significantly reduces the overload of long videos while preserving critical information. Generally, LLaMA-VID empowers existing frameworks to support hour-long videos and pushes their upper limit with an extra context token. It is proved to surpass previous methods on most of video- or image-based benchmarks. Code is available https://github.com/dvlab-research/LLaMA-VID}{https://github.com/dvlab-research/LLaMA-VID",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanwei Li",
      "Chengyao Wang",
      "Jiaya Jia"
    ]
  },
  "http://arxiv.org/abs/2311.17041": {
    "title": "Efficient In-Context Learning in Vision-Language Models for Egocentric Videos",
    "volume": "Nov",
    "abstract": "Recent advancements in text-only large language models (LLMs) have highlighted the benefit of in-context learning for adapting to new tasks with a few demonstrations. However, extending in-context learning to large vision-language models (VLMs) using a huge amount of naturalistic vision-language data has shown limited success, particularly for egocentric videos, due to high data collection costs. We propose a novel training method $\\mathbb{E}$fficient $\\mathbb{I}$n-context $\\mathbb{L}$earning on $\\mathbb{E}$gocentric $\\mathbb{V}$ideos ($\\mathbb{EILEV}$), which elicits in-context learning in VLMs for egocentric videos without requiring massive, naturalistic egocentric video datasets. $\\mathbb{EILEV}$ involves architectural and training data adaptations to allow the model to process contexts interleaved with video clips and narrations, sampling of in-context examples with clusters of similar verbs and nouns, use of data with skewed marginal distributions with a long tail of infrequent verbs and nouns, as well as homonyms and synonyms. Our evaluations show that $\\mathbb{EILEV}$-trained models outperform larger VLMs trained on a huge amount of naturalistic data in in-context learning. Furthermore, they can generalize to not only out-of-distribution, but also novel, rare egocentric videos and texts via in-context learning, demonstrating potential for applications requiring cost-effective training, and rapid post-deployment adaptability. Our code and demo are available at \\url{https://github.com/yukw777/EILEV}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keunwoo Peter Yu",
      "Zheyuan Zhang",
      "Fengyuan Hu",
      "Joyce Chai"
    ]
  },
  "http://arxiv.org/abs/2311.17035": {
    "title": "Scalable Extraction of Training Data from (Production) Language Models",
    "volume": "Nov",
    "abstract": "This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Milad Nasr",
      "Nicholas Carlini",
      "Jonathan Hayase",
      "Matthew Jagielski",
      "A. Feder Cooper",
      "Daphne Ippolito",
      "Christopher A. Choquette-Choo",
      "Eric Wallace",
      "Florian Tramèr",
      "Katherine Lee"
    ]
  },
  "http://arxiv.org/abs/2311.17030": {
    "title": "Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching",
    "volume": "Nov",
    "abstract": "Mechanistic interpretability aims to understand model behaviors in terms of specific, interpretable features, often hypothesized to manifest as low-dimensional subspaces of activations. Specifically, recent studies have explored subspace interventions (such as activation patching) as a way to simultaneously manipulate model behavior and attribute the features behind it to given subspaces. In this work, we demonstrate that these two aims diverge, potentially leading to an illusory sense of interpretability. Counterintuitively, even if a subspace intervention makes the model's output behave as if the value of a feature was changed, this effect may be achieved by activating a dormant parallel pathway leveraging another subspace that is causally disconnected from model outputs. We demonstrate this phenomenon in a distilled mathematical example, in two real-world domains (the indirect object identification task and factual recall), and present evidence for its prevalence in practice. In the context of factual recall, we further show a link to rank-1 fact editing, providing a mechanistic explanation for previous work observing an inconsistency between fact editing performance and fact localization. However, this does not imply that activation patching of subspaces is intrinsically unfit for interpretability. To contextualize our findings, we also show what a success case looks like in a task (indirect object identification) where prior manual circuit analysis informs an understanding of the location of a feature. We explore the additional evidence needed to argue that a patched subspace is faithful",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksandar Makelov",
      "Georg Lange",
      "Neel Nanda"
    ]
  },
  "http://arxiv.org/abs/2311.16989": {
    "title": "ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?",
    "volume": "Nov",
    "abstract": "Upon its release in late 2022, ChatGPT has brought a seismic shift in the entire landscape of AI, both in research and commerce. Through instruction-tuning a large language model (LLM) with supervised fine-tuning and reinforcement learning from human feedback, it showed that a model could answer human questions and follow instructions on a broad panel of tasks. Following this success, interests in LLMs have intensified, with new LLMs flourishing at frequent interval across academia and industry, including many start-ups focused on LLMs. While closed-source LLMs (e.g., OpenAI's GPT, Anthropic's Claude) generally outperform their open-source counterparts, the progress on the latter has been rapid with claims of achieving parity or even better on certain tasks. This has crucial implications not only on research but also on business. In this work, on the first anniversary of ChatGPT, we provide an exhaustive overview of this success, surveying all tasks where an open-source LLM has claimed to be on par or better than ChatGPT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hailin Chen",
      "Fangkai Jiao",
      "Xingxuan Li",
      "Chengwei Qin",
      "Mathieu Ravaut",
      "Ruochen Zhao",
      "Caiming Xiong",
      "Shafiq Joty"
    ]
  },
  "http://arxiv.org/abs/2311.16978": {
    "title": "Assessing the influence of attractor-verb distance on grammatical agreement in humans and language models",
    "volume": "Nov",
    "abstract": "Subject-verb agreement in the presence of an attractor noun located between the main noun and the verb elicits complex behavior: judgments of grammaticality are modulated by the grammatical features of the attractor. For example, in the sentence \"The girl near the boys likes climbing\", the attractor (boys) disagrees in grammatical number with the verb (likes), creating a locally implausible transition probability. Here, we parametrically modulate the distance between the attractor and the verb while keeping the length of the sentence equal. We evaluate the performance of both humans and two artificial neural network models: both make more mistakes when the attractor is closer to the verb, but neural networks get close to the chance level while humans are mostly able to overcome the attractor interference. Additionally, we report a linear effect of attractor distance on reaction times. We hypothesize that a possible reason for the proximity effect is the calculation of transition probabilities between adjacent words. Nevertheless, classical models of attraction such as the cue-based model might suffice to explain this phenomenon, thus paving the way for new research. Data and analyses available at https://osf.io/d4g6k",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christos-Nikolaos Zacharopoulos",
      "Théo Desbordes",
      "Mathias Sablé-Meyer"
    ]
  },
  "http://arxiv.org/abs/2311.16965": {
    "title": "Natural Language Processing Through Transfer Learning: A Case Study on Sentiment Analysis",
    "volume": "Nov",
    "abstract": "Artificial intelligence and machine learning have significantly bolstered the technological world. This paper explores the potential of transfer learning in natural language processing focusing mainly on sentiment analysis. The models trained on the big data can also be used where data are scarce. The claim is that, compared to training models from scratch, transfer learning, using pre-trained BERT models, can increase sentiment classification accuracy. The study adopts a sophisticated experimental design that uses the IMDb dataset of sentimentally labelled movie reviews. Pre-processing includes tokenization and encoding of text data, making it suitable for NLP models. The dataset is used on a BERT based model, measuring its performance using accuracy. The result comes out to be 100 per cent accurate. Although the complete accuracy could appear impressive, it might be the result of overfitting or a lack of generalization. Further analysis is required to ensure the model's ability to handle diverse and unseen data. The findings underscore the effectiveness of transfer learning in NLP, showcasing its potential to excel in sentiment analysis tasks. However, the research calls for a cautious interpretation of perfect accuracy and emphasizes the need for additional measures to validate the model's generalization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aman Yadav",
      "Abhishek Vichare"
    ]
  },
  "http://arxiv.org/abs/2311.16941": {
    "title": "Debiasing Multimodal Models via Causal Information Minimization",
    "volume": "Nov",
    "abstract": "Most existing debiasing methods for multimodal models, including causal intervention and inference methods, utilize approximate heuristics to represent the biases, such as shallow features from early stages of training or unimodal features for multimodal tasks like VQA, etc., which may not be accurate. In this paper, we study bias arising from confounders in a causal graph for multimodal data and examine a novel approach that leverages causally-motivated information minimization to learn the confounder representations. Robust predictive features contain diverse information that helps a model generalize to out-of-distribution data. Hence, minimizing the information content of features obtained from a pretrained biased model helps learn the simplest predictive features that capture the underlying data distribution. We treat these features as confounder representations and use them via methods motivated by causal theory to remove bias from models. We find that the learned confounder representations indeed capture dataset biases, and the proposed debiasing methods improve out-of-distribution (OOD) performance on multiple multimodal datasets without sacrificing in-distribution performance. Additionally, we introduce a novel metric to quantify the sufficiency of spurious features in models' predictions that further demonstrates the effectiveness of our proposed methods. Our code is available at: https://github.com/Vaidehi99/CausalInfoMin",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vaidehi Patil",
      "Adyasha Maharana",
      "Mohit Bansal"
    ]
  },
  "http://arxiv.org/abs/2311.16922": {
    "title": "Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding",
    "volume": "Nov",
    "abstract": "Large Vision-Language Models (LVLMs) have advanced considerably, intertwining visual recognition and language understanding to generate content that is not only coherent but also contextually attuned. Despite their success, LVLMs still suffer from the issue of object hallucinations, where models generate plausible yet incorrect outputs that include objects that do not exist in the images. To mitigate this issue, we introduce Visual Contrastive Decoding (VCD), a simple and training-free method that contrasts output distributions derived from original and distorted visual inputs. The proposed VCD effectively reduces the over-reliance on statistical bias and unimodal priors, two essential causes of object hallucinations. This adjustment ensures the generated content is closely grounded to visual inputs, resulting in contextually accurate outputs. Our experiments show that VCD, without either additional training or the usage of external tools, significantly mitigates the object hallucination issue across different LVLM families. Beyond mitigating object hallucinations, VCD also excels in general LVLM benchmarks, highlighting its wide-ranging applicability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sicong Leng",
      "Hang Zhang",
      "Guanzheng Chen",
      "Xin Li",
      "Shijian Lu",
      "Chunyan Miao",
      "Lidong Bing"
    ]
  },
  "http://arxiv.org/abs/2311.16882": {
    "title": "Optimisation-Based Multi-Modal Semantic Image Editing",
    "volume": "Nov",
    "abstract": "Image editing affords increased control over the aesthetics and content of generated images. Pre-existing works focus predominantly on text-based instructions to achieve desired image modifications, which limit edit precision and accuracy. In this work, we propose an inference-time editing optimisation, designed to extend beyond textual edits to accommodate multiple editing instruction types (e.g. spatial layout-based; pose, scribbles, edge maps). We propose to disentangle the editing task into two competing subtasks: successful local image modifications and global content consistency preservation, where subtasks are guided through two dedicated loss functions. By allowing to adjust the influence of each loss function, we build a flexible editing solution that can be adjusted to user preferences. We evaluate our method using text, pose and scribble edit conditions, and highlight our ability to achieve complex edits, through both qualitative and quantitative experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Li",
      "Yongxin Yang",
      "Steven McDonagh",
      "Shifeng Zhang",
      "Petru-Daniel Tudosiu",
      "Sarah Parisot"
    ]
  },
  "http://arxiv.org/abs/2311.16867": {
    "title": "The Falcon Series of Open Language Models",
    "volume": "Nov",
    "abstract": "We introduce the Falcon series: 7B, 40B, and 180B parameters causal decoder-only models trained on a diverse high-quality corpora predominantly assembled from web data. The largest model, Falcon-180B, has been trained on over 3.5 trillion tokens of text--the largest openly documented pretraining run. Falcon-180B significantly outperforms models such as PaLM or Chinchilla, and improves upon concurrently developed models such as LLaMA 2 or Inflection-1. It nears the performance of PaLM-2-Large at a reduced pretraining and inference cost, making it, to our knowledge, one of the three best language models in the world along with GPT-4 and PaLM-2-Large. We report detailed evaluations, as well as a deep dive into the methods and custom tooling employed to pretrain Falcon. Notably, we report on our custom distributed training codebase, allowing us to efficiently pretrain these models on up to 4,096 A100s on cloud AWS infrastructure with limited interconnect. We release a 600B tokens extract of our web dataset, as well as the Falcon-7/40/180B models under a permissive license to foster open-science and accelerate the development of an open ecosystem of large language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ebtesam Almazrouei",
      "Hamza Alobeidli",
      "Abdulaziz Alshamsi",
      "Alessandro Cappelli",
      "Ruxandra Cojocaru",
      "Mérouane Debbah",
      "Étienne Goffinet",
      "Daniel Hesslow",
      "Julien Launay",
      "Quentin Malartic",
      "Daniele Mazzotta",
      "Badreddine Noune",
      "Baptiste Pannier",
      "Guilherme Penedo"
    ]
  },
  "http://arxiv.org/abs/2311.16865": {
    "title": "A Benchmark for Evaluating Machine Translation Metrics on Dialects Without Standard Orthography",
    "volume": "Nov",
    "abstract": "For sensible progress in natural language processing, it is important that we are aware of the limitations of the evaluation metrics we use. In this work, we evaluate how robust metrics are to non-standardized dialects, i.e. spelling differences in language varieties that do not have a standard orthography. To investigate this, we collect a dataset of human translations and human judgments for automatic machine translations from English to two Swiss German dialects. We further create a challenge set for dialect variation and benchmark existing metrics' performances. Our results show that existing metrics cannot reliably evaluate Swiss German text generation outputs, especially on segment level. We propose initial design adaptations that increase robustness in the face of non-standardized dialects, although there remains much room for further improvement. The dataset, code, and models are available here: https://github.com/textshuttle/dialect_eval",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noëmi Aepli",
      "Chantal Amrhein",
      "Florian Schottmann",
      "Rico Sennrich"
    ]
  },
  "http://arxiv.org/abs/2311.16842": {
    "title": "RELIC: Investigating Large Language Model Responses using Self-Consistency",
    "volume": "Nov",
    "abstract": "Large Language Models (LLMs) are notorious for blending fact with fiction and generating non-factual content, known as hallucinations. To tackle this challenge, we propose an interactive system that helps users obtain insights into the reliability of the generated text. Our approach is based on the idea that the self-consistency of multiple samples generated by the same LLM relates to its confidence in individual claims in the generated texts. Using this idea, we design RELIC, an interactive system that enables users to investigate and verify semantic-level variations in multiple long-form responses. This allows users to recognize potentially inaccurate information in the generated text and make necessary corrections. From a user study with ten participants, we demonstrate that our approach helps users better verify the reliability of the generated text. We further summarize the design implications and lessons learned from this research for inspiring future studies on reliable human-LLM interactions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Furui Cheng",
      "Vilém Zouhar",
      "Simran Arora",
      "Mrinmaya Sachan",
      "Hendrik Strobelt",
      "Mennatallah El-Assady"
    ]
  },
  "http://arxiv.org/abs/2311.16840": {
    "title": "The Claire French Dialogue Dataset",
    "volume": "Nov",
    "abstract": "We present the Claire French Dialogue Dataset (CFDD), a resource created by members of LINAGORA Labs in the context of the OpenLLM France initiative. CFDD is a corpus containing roughly 160 million words from transcripts and stage plays in French that we have assembled and publicly released in an effort to further the development of multilingual, open source language models. This paper describes the 24 individual corpora of which CFDD is composed and provides links and citations to their original sources. It also provides our proposed breakdown of the full CFDD dataset into eight categories of subcorpora and describes the process we followed to standardize the format of the final dataset. We conclude with a discussion of similar work and future directions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julie Hunter",
      "Jérôme Louradour",
      "Virgile Rennard",
      "Ismaïl Harrando",
      "Guokan Shang",
      "Jean-Pierre Lorré"
    ]
  },
  "http://arxiv.org/abs/2311.16839": {
    "title": "Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization",
    "volume": "Nov",
    "abstract": "Multimodal large language models have made significant advancements in recent years, yet they still suffer from a common issue known as the \"hallucination problem\" where the models generate textual descriptions that contain inaccurate or non-existent content from the image. To address this issue, this paper introduces a novel strategy: Hallucination-Aware Direct Preference Optimization (HA-DPO). Our approach treats the hallucination problem as a unique preference selection issue, where the model is trained to favor the non-hallucinating response when presented with two responses of the same image (one accurate and one hallucinating). This paper also presents an efficient process for constructing hallucination sample pairs to ensure high-quality, style-consistent pairs for stable HA-DPO training. We applied this strategy to two mainstream multimodal models, and the results showed a significant reduction in the hallucination problem and an enhancement in the models' generalization capabilities. With HA-DPO, the MiniGPT-4 model demonstrates significant advancements: POPE accuracy increases from 51.13% to 85.66% (34.5% absolute improvement), and the MME score escalates from 968.58 to 1365.76 (41% relative improvement). The code, models, and datasets will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Zhao",
      "Bin Wang",
      "Linke Ouyang",
      "Xiaoyi Dong",
      "Jiaqi Wang",
      "Conghui He"
    ]
  },
  "http://arxiv.org/abs/2311.17126": {
    "title": "Reason out Your Layout: Evoking the Layout Master from Large Language Models for Text-to-Image Synthesis",
    "volume": "Nov",
    "abstract": "Recent advancements in text-to-image (T2I) generative models have shown remarkable capabilities in producing diverse and imaginative visuals based on text prompts. Despite the advancement, these diffusion models sometimes struggle to translate the semantic content from the text into images entirely. While conditioning on the layout has shown to be effective in improving the compositional ability of T2I diffusion models, they typically require manual layout input. In this work, we introduce a novel approach to improving T2I diffusion models using Large Language Models (LLMs) as layout generators. Our method leverages the Chain-of-Thought prompting of LLMs to interpret text and generate spatially reasonable object layouts. The generated layout is then used to enhance the generated images' composition and spatial accuracy. Moreover, we propose an efficient adapter based on a cross-attention mechanism, which explicitly integrates the layout information into the stable diffusion models. Our experiments demonstrate significant improvements in image quality and layout accuracy, showcasing the potential of LLMs in augmenting generative image models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohui Chen",
      "Yongfei Liu",
      "Yingxiang Yang",
      "Jianbo Yuan",
      "Quanzeng You",
      "Li-Ping Liu",
      "Hongxia Yang"
    ]
  },
  "http://arxiv.org/abs/2311.16832": {
    "title": "CharacterGLM: Customizing Chinese Conversational AI Characters with Large Language Models",
    "volume": "Nov",
    "abstract": "In this paper, we present CharacterGLM, a series of models built upon ChatGLM, with model sizes ranging from 6B to 66B parameters. Our CharacterGLM is designed for generating Character-based Dialogues (CharacterDial), which aims to equip a conversational AI system with character customization for satisfying people's inherent social desires and emotional needs. On top of CharacterGLM, we can customize various AI characters or social agents by configuring their attributes (identities, interests, viewpoints, experiences, achievements, social relationships, etc.) and behaviors (linguistic features, emotional expressions, interaction patterns, etc.). Our model outperforms most mainstream close-source large langauge models, including the GPT series, especially in terms of consistency, human-likeness, and engagement according to manual evaluations. We will release our 6B version of CharacterGLM and a subset of training data to facilitate further research development in the direction of character-based dialogue generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinfeng Zhou",
      "Zhuang Chen",
      "Dazhen Wan",
      "Bosi Wen",
      "Yi Song",
      "Jifan Yu",
      "Yongkang Huang",
      "Libiao Peng",
      "Jiaming Yang",
      "Xiyao Xiao",
      "Sahand Sabour",
      "Xiaohan Zhang",
      "Wenjing Hou",
      "Yijia Zhang",
      "Yuxiao Dong",
      "Jie Tang",
      "Minlie Huang"
    ]
  },
  "http://arxiv.org/abs/2311.16822": {
    "title": "Large Language Models Suffer From Their Own Output: An Analysis of the Self-Consuming Training Loop",
    "volume": "Nov",
    "abstract": "Large language models (LLM) have become state of the art in many benchmarks and conversational LLM applications like ChatGPT are now widely used by the public. Those LLMs can be used to generate large amounts of content which is posted on the internet to various platforms. As LLMs are trained on datasets usually collected from the internet, this LLM-generated content might be used to train the next generation of LLMs. Therefore, a self-consuming training loop emerges in which new LLM generations are trained on the output from the previous generations. We empirically study this self-consuming training loop using a novel dataset to analytically and accurately measure quality and diversity of generated outputs. We find that this self-consuming training loop initially improves both quality and diversity. However, after a few generations the output inevitably degenerates in diversity. We find that the rate of degeneration depends on the proportion of real and generated data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martin Briesch",
      "Dominik Sobania",
      "Franz Rothlauf"
    ]
  },
  "http://arxiv.org/abs/2311.16789": {
    "title": "A Survey of the Evolution of Language Model-Based Dialogue Systems",
    "volume": "Nov",
    "abstract": "Dialogue systems, including task-oriented_dialogue_system (TOD) and open-domain_dialogue_system (ODD), have undergone significant transformations, with language_models (LM) playing a central role. This survey delves into the historical trajectory of dialogue systems, elucidating their intricate relationship with advancements in language models by categorizing this evolution into four distinct stages, each marked by pivotal LM breakthroughs: 1) Early_Stage: characterized by statistical LMs, resulting in rule-based or machine-learning-driven dialogue_systems; 2) Independent development of TOD and ODD based on neural_language_models (NLM; e.g., LSTM and GRU), since NLMs lack intrinsic knowledge in their parameters; 3) fusion between different types of dialogue systems with the advert of pre-trained_language_models (PLMs), starting from the fusion between four_sub-tasks_within_TOD, and then TOD_with_ODD; and 4) current LLM-based_dialogue_system, wherein LLMs can be used to conduct TOD and ODD seamlessly. Thus, our survey provides a chronological perspective aligned with LM breakthroughs, offering a comprehensive review of state-of-the-art research outcomes. What's more, we focus on emerging topics and discuss open challenges, providing valuable insights into future directions for LLM-based_dialogue_systems. Through this exploration, we pave the way for a deeper_comprehension of the evolution, guiding future developments in LM-based dialogue_systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongru Wang",
      "Lingzhi Wang",
      "Yiming Du",
      "Liang Chen",
      "Jingyan Zhou",
      "Yufei Wang",
      "Kam-Fai Wong"
    ]
  },
  "http://arxiv.org/abs/2311.16787": {
    "title": "Evaluating Optimal Reference Translations",
    "volume": "Nov",
    "abstract": "The overall translation quality reached by current machine translation (MT) systems for high-resourced language pairs is remarkably good. Standard methods of evaluation are not suitable nor intended to uncover the many translation errors and quality deficiencies that still persist. Furthermore, the quality of standard reference translations is commonly questioned and comparable quality levels have been reached by MT alone in several language pairs. Navigating further research in these high-resource settings is thus difficult. In this article, we propose a methodology for creating more reliable document-level human reference translations, called \"optimal reference translations,\" with the simple aim to raise the bar of what should be deemed \"human translation quality.\" We evaluate the obtained document-level optimal reference translations in comparison with \"standard\" ones, confirming a significant quality increase and also documenting the relationship between evaluation and translation editing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vilém Zouhar",
      "Věra Kloudová",
      "Martin Popel",
      "Ondřej Bojar"
    ]
  },
  "http://arxiv.org/abs/2311.16764": {
    "title": "Radiology-Aware Model-Based Evaluation Metric for Report Generation",
    "volume": "Nov",
    "abstract": "We propose a new automated evaluation metric for machine-generated radiology reports using the successful COMET architecture adapted for the radiology domain. We train and publish four medically-oriented model checkpoints, including one trained on RadGraph, a radiology knowledge graph. Our results show that our metric correlates moderately to high with established metrics such as BERTscore, BLEU, and CheXbert scores. Furthermore, we demonstrate that one of our checkpoints exhibits a high correlation with human judgment, as assessed using the publicly available annotations of six board-certified radiologists, using a set of 200 reports. We also performed our own analysis gathering annotations with two radiologists on a collection of 100 reports. The results indicate the potential effectiveness of our method as a radiology-specific evaluation metric. The code, data, and model checkpoints to reproduce our findings will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amos Calamida",
      "Farhad Nooralahzadeh",
      "Morteza Rohanian",
      "Koji Fujimoto",
      "Mizuho Nishio",
      "Michael Krauthammer"
    ]
  },
  "http://arxiv.org/abs/2311.16733": {
    "title": "LLMs for Science: Usage for Code Generation and Data Analysis",
    "volume": "Nov",
    "abstract": "Large language models (LLMs) have been touted to enable increased productivity in many areas of today's work life. Scientific research as an area of work is no exception: the potential of LLM-based tools to assist in the daily work of scientists has become a highly discussed topic across disciplines. However, we are only at the very onset of this subject of study. It is still unclear how the potential of LLMs will materialise in research practice. With this study, we give first empirical evidence on the use of LLMs in the research process. We have investigated a set of use cases for LLM-based tools in scientific research, and conducted a first study to assess to which degree current tools are helpful. In this paper we report specifically on use cases related to software engineering, such as generating application code and developing scripts for data analytics. While we studied seemingly simple use cases, results across tools differ significantly. Our results highlight the promise of LLM-based tools in general, yet we also observe various issues, particularly regarding the integrity of the output these tools provide",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Nejjar",
      "Luca Zacharias",
      "Fabian Stiehle",
      "Ingo Weber"
    ]
  },
  "http://arxiv.org/abs/2311.16678": {
    "title": "Entity-Aspect-Opinion-Sentiment Quadruple Extraction for Fine-grained Sentiment Analysis",
    "volume": "Nov",
    "abstract": "Product reviews often contain a large number of implicit aspects and object-attribute co-existence cases. Unfortunately, many existing studies in Aspect-Based Sentiment Analysis (ABSA) have overlooked this issue, which can make it difficult to extract opinions comprehensively and fairly. In this paper, we propose a new task called Entity-Aspect-Opinion-Sentiment Quadruple Extraction (EASQE), which aims to hierarchically decompose aspect terms into entities and aspects to avoid information loss, non-exclusive annotations, and opinion misunderstandings in ABSA tasks. To facilitate research in this new task, we have constructed four datasets (Res14-EASQE, Res15-EASQE, Res16-EASQE, and Lap14-EASQE) based on the SemEval Restaurant and Laptop datasets. We have also proposed a novel two-stage sequence-tagging based Trigger-Opinion framework as the baseline for the EASQE task. Empirical evaluations show that our Trigger-Opinion framework can generate satisfactory EASQE results and can also be applied to other ABSA tasks, significantly outperforming state-of-the-art methods. We have made the four datasets and source code of Trigger-Opinion publicly available to facilitate further research in this area",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Ma",
      "Jun Xu",
      "Zongyu Wang",
      "Xuezhi Cao",
      "Yunsen Xian"
    ]
  },
  "http://arxiv.org/abs/2311.16675": {
    "title": "A Distribution-Based Threshold for Determining Sentence Similarity",
    "volume": "Nov",
    "abstract": "We hereby present a solution to a semantic textual similarity (STS) problem in which it is necessary to match two sentences containing, as the only distinguishing factor, highly specific information (such as names, addresses, identification codes), and from which we need to derive a definition for when they are similar and when they are not. The solution revolves around the use of a neural network, based on the siamese architecture, to create the distributions of the distances between similar and dissimilar pairs of sentences. The goal of these distributions is to find a discriminating factor, that we call \"threshold\", which represents a well-defined quantity that can be used to distinguish vector distances of similar pairs from vector distances of dissimilar pairs in new predictions and later analyses. In addition, we developed a way to score the predictions by combining attributes from both the distributions' features and the way the distance function works. Finally, we generalize the results showing that they can be transferred to a wider range of domains by applying the system discussed to a well-known and widely used benchmark dataset for STS problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gioele Cadamuro",
      "Marco Gruppo"
    ]
  },
  "http://arxiv.org/abs/2311.17107": {
    "title": "ClimateX: Do LLMs Accurately Assess Human Expert Confidence in Climate Statements?",
    "volume": "Nov",
    "abstract": "Evaluating the accuracy of outputs generated by Large Language Models (LLMs) is especially important in the climate science and policy domain. We introduce the Expert Confidence in Climate Statements (ClimateX) dataset, a novel, curated, expert-labeled dataset consisting of 8094 climate statements collected from the latest Intergovernmental Panel on Climate Change (IPCC) reports, labeled with their associated confidence levels. Using this dataset, we show that recent LLMs can classify human expert confidence in climate-related statements, especially in a few-shot learning setting, but with limited (up to 47%) accuracy. Overall, models exhibit consistent and significant over-confidence on low and medium confidence statements. We highlight implications of our results for climate communication, LLMs evaluation strategies, and the use of LLMs in information retrieval systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Romain Lacombe",
      "Kerrie Wu",
      "Eddie Dilworth"
    ]
  },
  "http://arxiv.org/abs/2311.16650": {
    "title": "Text2Tree: Aligning Text Representation to the Label Tree Hierarchy for Imbalanced Medical Classification",
    "volume": "Nov",
    "abstract": "Deep learning approaches exhibit promising performances on various text tasks. However, they are still struggling on medical text classification since samples are often extremely imbalanced and scarce. Different from existing mainstream approaches that focus on supplementary semantics with external medical information, this paper aims to rethink the data challenges in medical texts and present a novel framework-agnostic algorithm called Text2Tree that only utilizes internal label hierarchy in training deep learning models. We embed the ICD code tree structure of labels into cascade attention modules for learning hierarchy-aware label representations. Two new learning schemes, Similarity Surrogate Learning (SSL) and Dissimilarity Mixup Learning (DML), are devised to boost text classification by reusing and distinguishing samples of other labels following the label representation hierarchy, respectively. Experiments on authoritative public datasets and real-world medical records show that our approach stably achieves superior performances over classical and advanced imbalanced classification methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahuan Yan",
      "Haojun Gao",
      "Zhang Kai",
      "Weize Liu",
      "Danny Chen",
      "Jian Wu",
      "Jintai Chen"
    ]
  },
  "http://arxiv.org/abs/2311.16639": {
    "title": "Scaling Political Texts with ChatGPT",
    "volume": "Nov",
    "abstract": "We use GPT-4 to obtain position estimates of political texts in continuous spaces. We develop and validate a new approach by positioning British party manifestos on the economic, social, and immigration policy dimensions and tweets by members of the US Congress on the left-right ideological spectrum. For the party manifestos, the correlation between the positions produced by GPT-4 and experts is 93% or higher, a performance similar to or better than that obtained with crowdsourced position estimates. For individual tweets, the positions obtained with GPT-4 achieve a correlation of 91% with crowdsourced position estimates. For senators of the 117th US Congress, the positions obtained with GPT-4 achieve a correlation of 97% with estimates based on roll call votes and of 96% with those based on campaign funding. Correlations are also substantial within party, indicating that position estimates produced with GPT-4 capture within-party differences between senators. Overall, using GPT-4 for ideological scaling is fast, cost-efficient, and reliable. This approach provides a viable alternative to scaling by both expert raters and crowdsourcing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaël Le Mens",
      "Aina Gallego"
    ]
  },
  "http://arxiv.org/abs/2311.16620": {
    "title": "On the Long Range Abilities of Transformers",
    "volume": "Nov",
    "abstract": "Despite their dominance in modern DL and, especially, NLP domains, transformer architectures exhibit sub-optimal performance on long-range tasks compared to recent layers that are specifically designed for this purpose. In this work, drawing inspiration from key attributes of long-range layers, such as state-space layers, linear RNN layers, and global convolution layers, we demonstrate that minimal modifications to the transformer architecture can significantly enhance performance on the Long Range Arena (LRA) benchmark, thus narrowing the gap with these specialized layers. We identify that two key principles for long-range tasks are (i) incorporating an inductive bias towards smoothness, and (ii) locality. As we show, integrating these ideas into the attention mechanism improves results with a negligible amount of additional computation and without any additional trainable parameters. Our theory and experiments also shed light on the reasons for the inferior performance of transformers on long-range tasks and identify critical properties that are essential for successfully capturing long-range dependencies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Itamar Zimerman",
      "Lior Wolf"
    ]
  },
  "http://arxiv.org/abs/2311.16588": {
    "title": "MedGen: A Python Natural Language Processing Toolkit for Medical Text Processing",
    "volume": "Nov",
    "abstract": "This study introduces MedGen, a comprehensive natural language processing (NLP) toolkit designed for medical text processing. MedGen is tailored for biomedical researchers and healthcare professionals with an easy-to-use, all-in-one solution that requires minimal programming expertise. It includes (1) Generative Functions: For the first time, MedGen includes four advanced generative functions: question answering, text summarization, text simplification, and machine translation; (2) Basic NLP Functions: MedGen integrates 12 essential NLP functions such as word tokenization and sentence segmentation; and (3) Query and Search Capabilities: MedGen provides user-friendly query and search functions on text corpora. We fine-tuned 32 domain-specific language models, evaluated them thoroughly on 24 established benchmarks and conducted manual reviews with clinicians. Additionally, we expanded our toolkit by introducing query and search functions, while also standardizing and integrating functions from third-party libraries. The toolkit, its models, and associated data are publicly available via https://github.com/Yale-LILY/MedGen",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Yang",
      "Qingcheng Zeng",
      "Keen You",
      "Yujie Qiao",
      "Lucas Huang",
      "Chia-Chun Hsieh",
      "Benjamin Rosand",
      "Jeremy Goldwasser",
      "Amisha D Dave",
      "Tiarnan D. L. Keenan",
      "Emily Y Chew",
      "Dragomir Radev",
      "Zhiyong Lu",
      "Hua Xu",
      "Qingyu Chen",
      "Irene Li"
    ]
  },
  "http://arxiv.org/abs/2311.16579": {
    "title": "Recognizing Conditional Causal Relationships about Emotions and Their Corresponding Conditions",
    "volume": "Nov",
    "abstract": "The study of causal relationships between emotions and causes in texts has recently received much attention. Most works focus on extracting causally related clauses from documents. However, none of these works has considered that the causal relationships among the extracted emotion and cause clauses can only be valid under some specific context clauses. To highlight the context in such special causal relationships, we propose a new task to determine whether or not an input pair of emotion and cause has a valid causal relationship under different contexts and extract the specific context clauses that participate in the causal relationship. Since the task is new for which no existing dataset is available, we conduct manual annotation on a benchmark dataset to obtain the labels for our tasks and the annotations of each context clause's type that can also be used in some other applications. We adopt negative sampling to construct the final dataset to balance the number of documents with and without causal relationships. Based on the constructed dataset, we propose an end-to-end multi-task framework, where we design two novel and general modules to handle the two goals of our task. Specifically, we propose a context masking module to extract the context clauses participating in the causal relationships. We propose a prediction aggregation module to fine-tune the prediction results according to whether the input emotion and causes depend on specific context clauses. Results of extensive comparative experiments and ablation studies demonstrate the effectiveness and generality of our proposed framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinhong Chen",
      "Zongxi Li",
      "Yaowei Wang",
      "Haoran Xie",
      "Jianping Wang",
      "Qing Li"
    ]
  },
  "http://arxiv.org/abs/2311.16522": {
    "title": "Evaluation of dynamic characteristics of power grid based on GNN and application on knowledge graph",
    "volume": "Nov",
    "abstract": "A novel method for detecting faults in power grids using a graph neural network (GNN) has been developed, aimed at enhancing intelligent fault diagnosis in network operation and maintenance. This GNN-based approach identifies faulty nodes within the power grid through a specialized electrical feature extraction model coupled with a knowledge graph. Incorporating temporal data, the method leverages the status of nodes from preceding and subsequent time periods to aid in current fault detection. To validate the effectiveness of this GNN in extracting node features, a correlation analysis of the output features from each node within the neural network layer was conducted. The results from experiments show that this method can accurately locate fault nodes in simulated scenarios with a remarkable 99.53% accuracy. Additionally, the graph neural network's feature modeling allows for a qualitative examination of how faults spread across nodes, providing valuable insights for analyzing fault nodes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Pei",
      "Si Lin",
      "Chuanfu Li",
      "Che Wang",
      "Haoming Chen",
      "Sizhe Li"
    ]
  },
  "http://arxiv.org/abs/2311.16509": {
    "title": "StyleCap: Automatic Speaking-Style Captioning from Speech Based on Speech and Language Self-supervised Learning Models",
    "volume": "Nov",
    "abstract": "We propose StyleCap, a method to generate natural language descriptions of speaking styles appearing in speech. Although most of conventional techniques for para-/non-linguistic information recognition focus on the category classification or the intensity estimation of pre-defined labels, they cannot provide the reasoning of the recognition result in an interpretable manner. As a first step towards an end-to-end method for generating speaking-style prompts from speech, i.e., automatic speaking-style captioning, StyleCap uses paired data of speech and natural language descriptions to train neural networks that predict prefix vectors fed into a large language model (LLM)-based text decoder from a speech representation vector. We explore an appropriate text decoder and speech feature representation suitable for this new task. The experimental results demonstrate that our StyleCap leveraging richer LLMs for the text decoder, speech self-supervised learning (SSL) features, and sentence rephrasing augmentation improves the accuracy and diversity of generated speaking-style captions. Samples of speaking-style captions generated by our StyleCap are publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kazuki Yamauchi",
      "Yusuke Ijima",
      "Yuki Saito"
    ]
  },
  "http://arxiv.org/abs/2311.16466": {
    "title": "Enhancing Human Persuasion With Large Language Models",
    "volume": "Nov",
    "abstract": "Although large language models (LLMs) are reshaping various aspects of human life, our current understanding of their impacts remains somewhat constrained. Here we investigate the impact of LLMs on human communication, in the context of consumer complaints in the financial industry. Employing an AI detection tool on more than 780K complaints gathered by the Consumer Financial Protection Bureau (CFPB), we find evidence of LLM usage in the writing of complaints - shortly after the release of ChatGPT. Our analyses reveal that LLM usage is positively correlated with the likelihood of obtaining desirable outcomes (i.e., offer of relief from financial firms) and suggest that this positive correlation may be partly due to the linguistic features improved by LLMs. We test this conjecture with a preregistered experiment, which reveals results consistent with those from observational studies: Consumer complaints written with ChatGPT for improved linguistic qualities were more likely to receive hypothetical relief offers than the original consumer complaints, demonstrating the LLM's ability to enhance message persuasiveness in human communication. Being some of the earliest empirical evidence on LLM usage for enhancing persuasion, our results highlight the transformative potential of LLMs in human communication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minkyu Shin",
      "Jin Kim"
    ]
  },
  "http://arxiv.org/abs/2311.16452": {
    "title": "Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine",
    "volume": "Nov",
    "abstract": "Generalist foundation models such as GPT-4 have displayed surprising capabilities in a wide variety of domains and tasks. Yet, there is a prevalent assumption that they cannot match specialist capabilities of fine-tuned models. For example, most explorations to date on medical competency benchmarks have leveraged domain-specific training, as exemplified by efforts on BioGPT and Med-PaLM. We build on a prior study of GPT-4's capabilities on medical challenge benchmarks in the absence of special training. Rather than using simple prompting to highlight the model's out-of-the-box capabilities, we perform a systematic exploration of prompt engineering. We find that prompting innovation can unlock deeper specialist capabilities and show that GPT-4 easily tops prior leading results for medical benchmarks. The prompting methods we explore are general purpose, and make no specific use of domain expertise, removing the need for expert-curated content. Our experimental design carefully controls for overfitting during the prompt engineering process. We introduce Medprompt, based on a composition of several prompting strategies. With Medprompt, GPT-4 achieves state-of-the-art results on all nine of the benchmark datasets in the MultiMedQA suite. The method outperforms leading specialist models such as Med-PaLM 2 by a significant margin with an order of magnitude fewer calls to the model. Steering GPT-4 with Medprompt achieves a 27% reduction in error rate on the MedQA dataset over the best methods to date achieved with specialist models and surpasses a score of 90% for the first time. Beyond medical problems, we show the power of Medprompt to generalize to other domains and provide evidence for the broad applicability of the approach via studies of the strategy on exams in electrical engineering, machine learning, philosophy, accounting, law, nursing, and clinical psychology",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harsha Nori",
      "Yin Tat Lee",
      "Sheng Zhang",
      "Dean Carignan",
      "Richard Edgar",
      "Nicolo Fusi",
      "Nicholas King",
      "Jonathan Larson",
      "Yuanzhi Li",
      "Weishung Liu",
      "Renqian Luo",
      "Scott Mayer McKinney",
      "Robert Osazuwa Ness",
      "Hoifung Poon",
      "Tao Qin",
      "Naoto Usuyama",
      "Chris White",
      "Eric Horvitz"
    ]
  },
  "http://arxiv.org/abs/2311.16444": {
    "title": "Exo2EgoDVC: Dense Video Captioning of Egocentric Procedural Activities Using Web Instructional Videos",
    "volume": "Nov",
    "abstract": "We propose a novel benchmark for cross-view knowledge transfer of dense video captioning, adapting models from web instructional videos with exocentric views to an egocentric view. While dense video captioning (predicting time segments and their captions) is primarily studied with exocentric videos (e.g., YouCook2), benchmarks with egocentric videos are restricted due to data scarcity. To overcome the limited video availability, transferring knowledge from abundant exocentric web videos is demanded as a practical approach. However, learning the correspondence between exocentric and egocentric views is difficult due to their dynamic view changes. The web videos contain mixed views focusing on either human body actions or close-up hand-object interactions, while the egocentric view is constantly shifting as the camera wearer moves. This necessitates the in-depth study of cross-view transfer under complex view changes. In this work, we first create a real-life egocentric dataset (EgoYC2) whose captions are shared with YouCook2, enabling transfer learning between these datasets assuming their ground-truth is accessible. To bridge the view gaps, we propose a view-invariant learning method using adversarial training in both the pre-training and fine-tuning stages. While the pre-training is designed to learn invariant features against the mixed views in the web videos, the view-invariant fine-tuning further mitigates the view gaps between both datasets. We validate our proposed method by studying how effectively it overcomes the view change problem and efficiently transfers the knowledge to the egocentric domain. Our benchmark pushes the study of the cross-view transfer into a new task domain of dense video captioning and will envision methodologies to describe egocentric videos in natural language",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takehiko Ohkawa",
      "Takuma Yagi",
      "Taichi Nishimura",
      "Ryosuke Furuta",
      "Atsushi Hashimoto",
      "Yoshitaka Ushiku",
      "Yoichi Sato"
    ]
  },
  "http://arxiv.org/abs/2311.17086": {
    "title": "PEA-Diffusion: Parameter-Efficient Adapter with Knowledge Distillation in non-English Text-to-Image Generation",
    "volume": "Nov",
    "abstract": "Text-to-image diffusion models are well-known for their ability to generate realistic images based on textual prompts. However, the existing works have predominantly focused on English, lacking support for non-English text-to-image models. The most commonly used translation methods cannot solve the generation problem related to language culture, while training from scratch on a specific language dataset is prohibitively expensive. In this paper, we are inspired to propose a simple plug-and-play language transfer method based on knowledge distillation. All we need to do is train a lightweight MLP-like parameter-efficient adapter (PEA) with only 6M parameters under teacher knowledge distillation along with a small parallel data corpus. We are surprised to find that freezing the parameters of UNet can still achieve remarkable performance on the language-specific prompt evaluation set, demonstrating that PEA can stimulate the potential generation ability of the original UNet. Additionally, it closely approaches the performance of the English text-to-image model on a general prompt evaluation set. Furthermore, our adapter can be used as a plugin to achieve significant results in downstream tasks in cross-lingual text-to-image generation. Code will be available at: https://github.com/OPPO-Mente-Lab/PEA-Diffusion",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Ma",
      "Chen Chen",
      "Qingsong Xie",
      "Haonan Lu"
    ]
  },
  "http://arxiv.org/abs/2311.16421": {
    "title": "CDEval: A Benchmark for Measuring the Cultural Dimensions of Large Language Models",
    "volume": "Nov",
    "abstract": "As the scaling of Large Language Models (LLMs) has dramatically enhanced their capabilities, there has been a growing focus on the alignment problem to ensure their responsible and ethical use. While existing alignment efforts predominantly concentrate on universal values such as the HHH principle, the aspect of culture, which is inherently pluralistic and diverse, has not received adequate attention. This work introduces a new benchmark, CDEval, aimed at evaluating the cultural dimensions of LLMs. CDEval is constructed by incorporating both GPT-4's automated generation and human verification, covering six cultural dimensions across seven domains. Our comprehensive experiments provide intriguing insights into the culture of mainstream LLMs, highlighting both consistencies and variations across different dimensions and domains. The findings underscore the importance of integrating cultural considerations in LLM development, particularly for applications in diverse cultural settings. Through CDEval, we aim to broaden the horizon of LLM alignment research by including cultural dimensions, thus providing a more holistic framework for the future development and evaluation of LLMs. This benchmark serves as a valuable resource for cultural studies in LLMs, paving the way for more culturally aware and sensitive models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Wang",
      "Yanxu Zhu",
      "Chao Kong",
      "Shuyu Wei",
      "Xiaoyuan Yi",
      "Xing Xie",
      "Jitao Sang"
    ]
  },
  "http://arxiv.org/abs/2311.16362": {
    "title": "Reducing Gender Bias in Machine Translation through Counterfactual Data Generation",
    "volume": "Nov",
    "abstract": "Recent advances in neural methods have led to substantial improvement in the quality of Neural Machine Translation (NMT) systems. However, these systems frequently produce translations with inaccurate gender (Stanovsky et al., 2019), which can be traced to bias in training data. Saunders and Byrne (2020) tackle this problem with a handcrafted dataset containing balanced gendered profession words. By using this data to fine-tune an existing NMT model, they show that gender bias can be significantly mitigated, albeit at the expense of translation quality due to catastrophic forgetting. They recover some of the lost quality with modified training objectives or additional models at inference. We find, however, that simply supplementing the handcrafted dataset with a random sample from the base model training corpus is enough to significantly reduce the catastrophic forgetting. We also propose a novel domain-adaptation technique that leverages in-domain data created with the counterfactual data generation techniques proposed by Zmigrod et al. (2019) to further improve accuracy on the WinoMT challenge test set without significant loss in translation quality. We show its effectiveness in NMT systems from English into three morphologically rich languages French, Spanish, and Italian. The relevant dataset and code will be available at Github",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ranjita Naik",
      "Spencer Rarrick",
      "Vishal Chowdhary"
    ]
  },
  "http://arxiv.org/abs/2311.17076": {
    "title": "Compositional Chain-of-Thought Prompting for Large Multimodal Models",
    "volume": "Nov",
    "abstract": "The combination of strong visual backbones and Large Language Model (LLM) reasoning has led to Large Multimodal Models (LMMs) becoming the current standard for a wide range of vision and language (VL) tasks. However, recent research has shown that even the most advanced LMMs still struggle to capture aspects of compositional visual reasoning, such as attributes and relationships between objects. One solution is to utilize scene graphs (SGs)--a formalization of objects and their relations and attributes that has been extensively used as a bridge between the visual and textual domains. Yet, scene graph data requires scene graph annotations, which are expensive to collect and thus not easily scalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic forgetting of the pretraining objective. To overcome this, inspired by chain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a novel zero-shot Chain-of-Thought prompting method that utilizes SG representations in order to extract compositional knowledge from an LMM. Specifically, we first generate an SG using the LMM, and then use that SG in the prompt to produce a response. Through extensive experiments, we find that the proposed CCoT approach not only improves LMM performance on several vision and language VL compositional benchmarks but also improves the performance of several popular LMMs on general multimodal benchmarks, without the need for fine-tuning or annotated ground-truth SGs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chancharik Mitra",
      "Brandon Huang",
      "Trevor Darrell",
      "Roei Herzig"
    ]
  },
  "http://arxiv.org/abs/2311.16338": {
    "title": "Releasing the CRaQAn (Coreference Resolution in Question-Answering): An open-source dataset and dataset creation methodology using instruction-following models",
    "volume": "Nov",
    "abstract": "Instruction-following language models demand robust methodologies for information retrieval to augment instructions for question-answering applications. A primary challenge is the resolution of coreferences in the context of chunking strategies for long documents. The critical barrier to experimentation of handling coreferences is a lack of open source datasets, specifically in question-answering tasks that require coreference resolution. In this work we present our Coreference Resolution in Question-Answering (CRaQAn) dataset, an open-source dataset that caters to the nuanced information retrieval requirements of coreference resolution in question-answering tasks by providing over 250 question-answer pairs containing coreferences. To develop this dataset, we developed a novel approach for creating high-quality datasets using an instruction-following model (GPT-4) and a Recursive Criticism and Improvement Loop",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rob Grzywinski",
      "Joshua D'Arcy",
      "Rob Naidoff",
      "Ashish Shukla",
      "Alex Browne",
      "Ren Gibbons",
      "Brinnae Bent"
    ]
  },
  "http://arxiv.org/abs/2311.16302": {
    "title": "Comprehensive Benchmarking of Entropy and Margin Based Scoring Metrics for Data Selection",
    "volume": "Nov",
    "abstract": "While data selection methods have been studied extensively in active learning, data pruning, and data augmentation settings, there is little evidence for the efficacy of these methods in industry scale settings, particularly in low-resource languages. Our work presents ways of assessing prospective training examples in those settings for their \"usefulness\" or \"difficulty\". We also demonstrate how these measures can be used in selecting important examples for training supervised machine learning models. We primarily experiment with entropy and Error L2-Norm (EL2N) scores. We use these metrics to curate high quality datasets from a large pool of \\textit{Weak Signal Labeled} data, which assigns no-defect high confidence hypotheses during inference as ground truth labels. We then conduct training data augmentation experiments using these de-identified datasets and demonstrate that score-based selection can result in a 2% decrease in semantic error rate and 4%-7% decrease in domain classification error rate when compared to the baseline technique of random selection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anusha Sabbineni",
      "Nikhil Anand",
      "Maria Minakova"
    ]
  },
  "http://arxiv.org/abs/2311.16298": {
    "title": "Influence Scores at Scale for Efficient Language Data Sampling",
    "volume": "Nov",
    "abstract": "Modern ML systems ingest data aggregated from diverse sources, such as synthetic, human-annotated, and live customer traffic. Understanding \\textit{which} examples are important to the performance of a learning algorithm is crucial for efficient model training. Recently, a growing body of literature has given rise to various \"influence scores,\" which use training artifacts such as model confidence or checkpointed gradients to identify important subsets of data. However, these methods have primarily been developed in computer vision settings, and it remains unclear how well they generalize to language-based tasks using pretrained models. In this paper, we explore the applicability of influence scores in language classification tasks. We evaluate a diverse subset of these scores on the SNLI dataset by quantifying accuracy changes in response to pruning training data through random and influence-score-based sampling. We then stress-test one of the scores -- \"variance of gradients\" (VoG) from Agarwal et al. (2022) -- in an NLU model stack that was exposed to dynamic user speech patterns in a voice assistant type of setting. Our experiments demonstrate that in many cases, encoder-based language models can be finetuned on roughly 50% of the original data without degradation in performance metrics. Along the way, we summarize lessons learned from applying out-of-the-box implementations of influence scores, quantify the effects of noisy and class-imbalanced data, and offer recommendations on score-based sampling for better accuracy and training efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikhil Anand",
      "Joshua Tan",
      "Maria Minakova"
    ]
  },
  "http://arxiv.org/abs/2311.16292": {
    "title": "Student Mastery or AI Deception? Analyzing ChatGPT's Assessment Proficiency and Evaluating Detection Strategies",
    "volume": "Nov",
    "abstract": "Generative AI systems such as ChatGPT have a disruptive effect on learning and assessment. Computer science requires practice to develop skills in problem solving and programming that are traditionally developed using assignments. Generative AI has the capability of completing these assignments for students with high accuracy, which dramatically increases the potential for academic integrity issues and students not achieving desired learning outcomes. This work investigates the performance of ChatGPT by evaluating it across three courses (CS1,CS2,databases). ChatGPT completes almost all introductory assessments perfectly. Existing detection methods, such as MOSS and JPlag (based on similarity metrics) and GPTzero (AI detection), have mixed success in identifying AI solutions. Evaluating instructors and teaching assistants using heuristics to distinguish between student and AI code shows that their detection is not sufficiently accurate. These observations emphasize the need for adapting assessments and improved detection methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Wang",
      "Seth Akins",
      "Abdallah Mohammed",
      "Ramon Lawrence"
    ]
  },
  "http://arxiv.org/abs/2311.16267": {
    "title": "Applications of Large Language Models in Data Processing: Innovative Approaches to Segmenting and Renewing Information",
    "volume": "Nov",
    "abstract": "Our paper investigates effective methods for code generation in \"specific-domain\" applications, including the use of Large Language Models (LLMs) for data segmentation and renewal, as well as stimulating deeper thinking in LLMs through prompt adjustments. Using a real company product as an example, we provide user manuals, API documentation, and other data. The ideas discussed in this paper help segment and then convert this data into semantic vectors to better reflect their true positioning. Subsequently, user requirements are transformed into vectors to retrieve the most relevant content, achieving about 70% accuracy in simple to medium-complexity tasks through various prompt techniques. This paper is the first to enhance specific-domain code generation effectiveness from this perspective. Additionally, we experiment with generating more scripts from a limited number using llama2-based fine-tuning to test its effectiveness in professional domain code generation. This is a challenging and promising field, and once achieved, it will not only lead to breakthroughs in LLM development across multiple industries but also enable LLMs to understand and learn any new knowledge effectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Chen Lin",
      "Akhilesh Kumar",
      "Wen-Liang Zhang",
      "Norman Chang",
      "Muhammad Zakir",
      "Rucha Apte",
      "Chao Wang",
      "Jyh-Shing Roger Jang"
    ]
  },
  "http://arxiv.org/abs/2311.16258": {
    "title": "An Exploration of Left-Corner Transformations",
    "volume": "Nov",
    "abstract": "The left-corner transformation (Rosenkrantz and Lewis, 1970) is used to remove left recursion from context-free grammars, which is an important step towards making the grammar parsable top-down with simple techniques. This paper generalizes prior left-corner transformations to support semiring-weighted production rules and to provide finer-grained control over which left corners may be moved. Our generalized left-corner transformation (GLCT) arose from unifying the left-corner transformation and speculation transformation (Eisner and Blatz, 2007), originally for logic programming. Our new transformation and speculation define equivalent weighted languages. Yet, their derivation trees are structurally different in an important way: GLCT replaces left recursion with right recursion, and speculation does not. We also provide several technical results regarding the formal relationships between the outputs of GLCT, speculation, and the original grammar. Lastly, we empirically investigate the efficiency of GLCT for left-recursion elimination from grammars of nine languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Opedal",
      "Eleftheria Tsipidi",
      "Tiago Pimentel",
      "Ryan Cotterell",
      "Tim Vieira"
    ]
  },
  "http://arxiv.org/abs/2311.16254": {
    "title": "Removing NSFW Concepts from Vision-and-Language Models for Text-to-Image Retrieval and Generation",
    "volume": "Nov",
    "abstract": "Vision-and-Language models such as CLIP have demonstrated remarkable effectiveness across a wide range of tasks. However, these models are typically trained on web-scale data, which can introduce inappropriate content and lead to the development of unsafe and biased behavior. This, in turn, hampers their applicability in sensitive and trustworthy contexts and could raise significant concern in their adoption. To overcome these limitations, we introduce a methodology to make Vision-and-Language models safer by removing their sensitivity to not-safe-for-work concepts. We show how this can be done by distilling from a large language model which converts between safe and unsafe sentences and which is fine-tuned starting from just 100 manually-curated pairs. We conduct extensive experiments on the resulting embedding space for both retrieval and text-to-image generation, where we show that our model can also be properly employed with pre-trained image generators. Our source code and trained models are available at: https://github.com/aimagelab/safe-clip",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuele Poppi",
      "Tobia Poppi",
      "Federico Cocchi",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ]
  },
  "http://arxiv.org/abs/2311.16101": {
    "title": "How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs",
    "volume": "Nov",
    "abstract": "This work focuses on the potential of Vision LLMs (VLLMs) in visual reasoning. Different from prior studies, we shift our focus from evaluating standard performance to introducing a comprehensive safety evaluation suite, covering both out-of-distribution (OOD) generalization and adversarial robustness. For the OOD evaluation, we present two novel VQA datasets, each with one variant, designed to test model performance under challenging conditions. In exploring adversarial robustness, we propose a straightforward attack strategy for misleading VLLMs to produce visual-unrelated responses. Moreover, we assess the efficacy of two jailbreaking strategies, targeting either the vision or language component of VLLMs. Our evaluation of 21 diverse models, ranging from open-source VLLMs to GPT-4V, yields interesting observations: 1) Current VLLMs struggle with OOD texts but not images, unless the visual information is limited; and 2) These VLLMs can be easily misled by deceiving vision encoders only, and their vision-language training often compromise safety protocols. We release this safety evaluation suite at https://github.com/UCSC-VLAA/vllm-safety-benchmark",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoqin Tu",
      "Chenhang Cui",
      "Zijun Wang",
      "Yiyang Zhou",
      "Bingchen Zhao",
      "Junlin Han",
      "Wangchunshu Zhou",
      "Huaxiu Yao",
      "Cihang Xie"
    ]
  },
  "http://arxiv.org/abs/2311.16087": {
    "title": "DUnE: Dataset for Unified Editing",
    "volume": "Nov",
    "abstract": "Even the most advanced language models remain susceptible to errors necessitating to modify these models without initiating a comprehensive retraining process. Model editing refers to the modification of a model's knowledge or representations in a manner that produces the desired outcomes. Prior research primarily centered around editing factual data e.g. \"Messi plays for Inter Miami\" confining the definition of an edit to a knowledge triplet i.e. (subject, object, relation). However, as the applications of language models expand, so do the diverse ways in which we wish to edit and refine their outputs. In this study, we broaden the scope of the editing problem to include an array of editing cases such as debiasing and rectifying reasoning errors and define an edit as any natural language expression that solicits a change in the model's outputs. We are introducing DUnE-an editing benchmark where edits are natural language sentences and propose that DUnE presents a challenging yet relevant task. To substantiate this claim, we conduct an extensive series of experiments testing various editing approaches to address DUnE, demonstrating their respective strengths and weaknesses. We show that retrieval-augmented language modeling can outperform specialized editing techniques and neither set of approaches has fully solved the generalized editing problem covered by our benchmark",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Afra Feyza Akyürek",
      "Eric Pan",
      "Garry Kuwanto",
      "Derry Wijaya"
    ]
  },
  "http://arxiv.org/abs/2311.16083": {
    "title": "BERT Goes Off-Topic: Investigating the Domain Transfer Challenge using Genre Classification",
    "volume": "Nov",
    "abstract": "While performance of many text classification tasks has been recently improved due to Pre-trained Language Models (PLMs), in this paper we show that they still suffer from a performance gap when the underlying distribution of topics changes. For example, a genre classifier trained on \\textit{political} topics often fails when tested on documents about \\textit{sport} or \\textit{medicine}. In this work, we quantify this phenomenon empirically with a large corpus and a large set of topics. Consequently, we verify that domain transfer remains challenging both for classic PLMs, such as BERT, and for modern large models, such as GPT-3. We also suggest and successfully test a possible remedy: after augmenting the training dataset with topically-controlled synthetic texts, the F1 score improves by up to 50\\% for some topics, nearing on-topic training results, while others show little to no improvement. While our empirical results focus on genre classification, our methodology is applicable to other classification tasks such as gender, authorship, or sentiment classification. The code and data to replicate the experiments are available at https://github.com/dminus1/genre",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dmitri Roussinov",
      "Serge Sharoff"
    ]
  },
  "http://arxiv.org/abs/2311.16079": {
    "title": "MEDITRON-70B: Scaling Medical Pretraining for Large Language Models",
    "volume": "Nov",
    "abstract": "Large language models (LLMs) can potentially democratize access to medical knowledge. While many efforts have been made to harness and improve LLMs' medical knowledge and reasoning capacities, the resulting models are either closed-source (e.g., PaLM, GPT-4) or limited in scale (<= 13B parameters), which restricts their abilities. In this work, we improve access to large-scale medical LLMs by releasing MEDITRON: a suite of open-source LLMs with 7B and 70B parameters adapted to the medical domain. MEDITRON builds on Llama-2 (through our adaptation of Nvidia's Megatron-LM distributed trainer), and extends pretraining on a comprehensively curated medical corpus, including selected PubMed articles, abstracts, and internationally-recognized medical guidelines. Evaluations using four major medical benchmarks show significant performance gains over several state-of-the-art baselines before and after task-specific finetuning. Overall, MEDITRON achieves a 6% absolute performance gain over the best public baseline in its parameter class and 3% over the strongest baseline we finetuned from Llama-2. Compared to closed-source LLMs, MEDITRON-70B outperforms GPT-3.5 and Med-PaLM and is within 5% of GPT-4 and 10% of Med-PaLM-2. We release our code for curating the medical pretraining corpus and the MEDITRON model weights to drive open-source development of more capable medical LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeming Chen",
      "Alejandro Hernández Cano",
      "Angelika Romanou",
      "Antoine Bonnet",
      "Kyle Matoba",
      "Francesco Salvi",
      "Matteo Pagliardini",
      "Simin Fan",
      "Andreas Köpf",
      "Amirkeivan Mohtashami",
      "Alexandre Sallinen",
      "Alireza Sakhaeirad",
      "Vinitra Swamy",
      "Igor Krawczuk",
      "Deniz Bayazit",
      "Axel Marmet",
      "Syrielle Montariol",
      "Mary-Anne Hartley",
      "Martin Jaggi",
      "Antoine Bosselut"
    ]
  },
  "http://arxiv.org/abs/2311.16075": {
    "title": "BioLORD-2023: Semantic Textual Representations Fusing LLM and Clinical Knowledge Graph Insights",
    "volume": "Nov",
    "abstract": "In this study, we investigate the potential of Large Language Models to complement biomedical knowledge graphs in the training of semantic models for the biomedical and clinical domains. Drawing on the wealth of the UMLS knowledge graph and harnessing cutting-edge Large Language Models, we propose a new state-of-the-art approach for obtaining high-fidelity representations of biomedical concepts and sentences, consisting of three steps: an improved contrastive learning phase, a novel self-distillation phase, and a weight averaging phase. Through rigorous evaluations via the extensive BioLORD testing suite and diverse downstream tasks, we demonstrate consistent and substantial performance improvements over the previous state of the art (e.g. +2pts on MedSTS, +2.5pts on MedNLI-S, +6.1pts on EHR-Rel-B). Besides our new state-of-the-art biomedical model for English, we also distill and release a multilingual model compatible with 50+ languages and finetuned on 7 European languages. Many clinical pipelines can benefit from our latest models. Our new multilingual model enables a range of languages to benefit from our advancements in biomedical semantic representation learning, opening a new avenue for bioinformatics researchers around the world. As a result, we hope to see BioLORD-2023 becoming a precious tool for future biomedical applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "François Remy",
      "Kris Demuynck",
      "Thomas Demeester"
    ]
  },
  "http://arxiv.org/abs/2311.16502": {
    "title": "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI",
    "volume": "Nov",
    "abstract": "We introduce MMMU: a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering. These questions span 30 subjects and 183 subfields, comprising 30 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures. Unlike existing benchmarks, MMMU focuses on advanced perception and reasoning with domain-specific knowledge, challenging models to perform tasks akin to those faced by experts. Our evaluation of 14 open-source LMMs and the proprietary GPT-4V(ision) highlights the substantial challenges posed by MMMU. Even the advanced GPT-4V only achieves a 56% accuracy, indicating significant room for improvement. We believe MMMU will stimulate the community to build next-generation multimodal foundation models towards expert artificial general intelligence",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Yue",
      "Yuansheng Ni",
      "Kai Zhang",
      "Tianyu Zheng",
      "Ruoqi Liu",
      "Ge Zhang",
      "Samuel Stevens",
      "Dongfu Jiang",
      "Weiming Ren",
      "Yuxuan Sun",
      "Cong Wei",
      "Botao Yu",
      "Ruibin Yuan",
      "Renliang Sun",
      "Ming Yin",
      "Boyuan Zheng",
      "Zhenzhu Yang",
      "Yibo Liu",
      "Wenhao Huang",
      "Huan Sun",
      "Yu Su",
      "Wenhu Chen"
    ]
  },
  "http://arxiv.org/abs/2311.15983": {
    "title": "Sparsify-then-Classify: From Internal Neurons of Large Language Models To Efficient Text Classifiers",
    "volume": "Nov",
    "abstract": "Among the many tasks that Large Language Models (LLMs) have revolutionized is text classification. However, existing approaches for applying pretrained LLMs to text classification predominantly rely on using single token outputs from only the last layer of hidden states. As a result, they suffer from limitations in efficiency, task-specificity, and interpretability. In our work, we contribute an approach that uses all internal representations by employing multiple pooling strategies on all activation and hidden states. Our novel lightweight strategy, Sparsify-then-Classify (STC) first sparsifies task-specific features layer-by-layer, then aggregates across layers for text classification. STC can be applied as a seamless plug-and-play module on top of existing LLMs. Our experiments on a comprehensive set of models and datasets demonstrate that STC not only consistently improves the classification performance of pretrained and fine-tuned models, but is also more efficient for both training and inference, and is more intrinsically interpretable",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilun Liu",
      "Difan Jiao",
      "Ashton Anderson"
    ]
  },
  "http://arxiv.org/abs/2311.15964": {
    "title": "Efficient Pre-training for Localized Instruction Generation of Videos",
    "volume": "Nov",
    "abstract": "Procedural videos show step-by-step demonstrations of tasks like recipe preparation. Understanding such videos is challenging, involving the precise localization of steps and the generation of textual instructions. Manually annotating steps and writing instructions is costly, which limits the size of current datasets and hinders effective learning. Leveraging large but noisy video-transcript datasets for pre-training can boost performance, but demands significant computational resources. Furthermore, transcripts contain irrelevant content and exhibit style variation compared to instructions written by human annotators. To mitigate both issues, we propose a technique, Sieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters irrelevant transcripts and (ii) Swap enhances the quality of the text instruction by automatically replacing the transcripts with human-written instructions from a text-only recipe dataset. The curated dataset, three orders of magnitude smaller than current web-scale datasets, enables efficient training of large-scale models with competitive performance. We complement our Sieve-\\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step localization and instruction generation for procedural videos. When this model is pre-trained on our curated dataset, it achieves state-of-the-art performance in zero-shot and finetuning settings on YouCook2 and Tasty, while using a fraction of the computational resources",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anil Batra",
      "Davide Moltisanti",
      "Laura Sevilla-Lara",
      "Marcus Rohrbach",
      "Frank Keller"
    ]
  },
  "http://arxiv.org/abs/2311.15954": {
    "title": "A Quantitative Approach to Understand Self-Supervised Models as Cross-lingual Feature Extractors",
    "volume": "Nov",
    "abstract": "In this work, we study the features extracted by English self-supervised learning (SSL) models in cross-lingual contexts and propose a new metric to predict the quality of feature representations. Using automatic speech recognition (ASR) as a downstream task, we analyze the effect of model size, training objectives, and model architecture on the models' performance as a feature extractor for a set of topologically diverse corpora. We develop a novel metric, the Phonetic-Syntax Ratio (PSR), to measure the phonetic and synthetic information in the extracted representations using deep generalized canonical correlation analysis. Results show the contrastive loss in the wav2vec2.0 objective facilitates more effective cross-lingual feature extraction. There is a positive correlation between PSR scores and ASR performance, suggesting that phonetic information extracted by monolingual SSL models can be used for downstream tasks in cross-lingual settings. The proposed metric is an effective indicator of the quality of the representations and can be useful for model selection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyue Stella Li",
      "Beining Xu",
      "Xiangyu Zhang",
      "Hexin Liu",
      "Wenhan Chao",
      "Leibny Paola Garcia"
    ]
  },
  "http://arxiv.org/abs/2311.15946": {
    "title": "Leveraging deep active learning to identify low-resource mobility functioning information in public clinical notes",
    "volume": "Nov",
    "abstract": "Function is increasingly recognized as an important indicator of whole-person health, although it receives little attention in clinical natural language processing research. We introduce the first public annotated dataset specifically on the Mobility domain of the International Classification of Functioning, Disability and Health (ICF), aiming to facilitate automatic extraction and analysis of functioning information from free-text clinical notes. We utilize the National NLP Clinical Challenges (n2c2) research dataset to construct a pool of candidate sentences using keyword expansion. Our active learning approach, using query-by-committee sampling weighted by density representativeness, selects informative sentences for human annotation. We train BERT and CRF models, and use predictions from these models to guide the selection of new sentences for subsequent annotation iterations. Our final dataset consists of 4,265 sentences with a total of 11,784 entities, including 5,511 Action entities, 5,328 Mobility entities, 306 Assistance entities, and 639 Quantification entities. The inter-annotator agreement (IAA), averaged over all entity types, is 0.72 for exact matching and 0.91 for partial matching. We also train and evaluate common BERT models and state-of-the-art Nested NER models. The best F1 scores are 0.84 for Action, 0.7 for Mobility, 0.62 for Assistance, and 0.71 for Quantification. Empirical results demonstrate promising potential of NER models to accurately extract mobility functioning information from clinical text. The public availability of our annotated dataset will facilitate further research to comprehensively capture functioning information in electronic health records (EHRs)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuan-Dung Le",
      "Zhuqi Miao",
      "Samuel Alvarado",
      "Brittany Smith",
      "William Paiva",
      "Thanh Thieu"
    ]
  },
  "http://arxiv.org/abs/2311.15941": {
    "title": "Tell2Design: A Dataset for Language-Guided Floor Plan Generation",
    "volume": "Nov",
    "abstract": "We consider the task of generating designs directly from natural language descriptions, and consider floor plan generation as the initial research area. Language conditional generative models have recently been very successful in generating high-quality artistic images. However, designs must satisfy different constraints that are not present in generating artistic images, particularly spatial and relational constraints. We make multiple contributions to initiate research on this task. First, we introduce a novel dataset, \\textit{Tell2Design} (T2D), which contains more than $80k$ floor plan designs associated with natural language instructions. Second, we propose a Sequence-to-Sequence model that can serve as a strong baseline for future research. Third, we benchmark this task with several text-conditional image generation models. We conclude by conducting human evaluations on the generated samples and providing an analysis of human performance. We hope our contributions will propel the research on language-guided design generation forward",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sicong Leng",
      "Yang Zhou",
      "Mohammed Haroon Dupty",
      "Wee Sun Lee",
      "Sam Conrad Joyce",
      "Wei Lu"
    ]
  },
  "http://arxiv.org/abs/2311.15930": {
    "title": "WorldSense: A Synthetic Benchmark for Grounded Reasoning in Large Language Models",
    "volume": "Nov",
    "abstract": "We propose WorldSense, a benchmark designed to assess the extent to which LLMs are consistently able to sustain tacit world models, by testing how they draw simple inferences from descriptions of simple arrangements of entities. Worldsense is a synthetic benchmark with three problem types, each with their own trivial control, which explicitly avoids bias by decorrelating the abstract structure of problems from the vocabulary and expressions, and by decorrelating all problem subparts with the correct response. We run our benchmark on three state-of-the-art chat-LLMs (GPT3.5, GPT4 and Llama2-chat) and show that these models make errors even with as few as three objects. Furthermore, they have quite heavy response biases, preferring certain responses irrespective of the question. Errors persist even with chain-of-thought prompting and in-context learning. Lastly, we show that while finetuning on similar problems does result in substantial improvements -- within- and out-of-distribution -- the finetuned models do not generalise beyond a constraint problem space",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youssef Benchekroun",
      "Megi Dervishi",
      "Mark Ibrahim",
      "Jean-Baptiste Gaya",
      "Xavier Martinet",
      "Grégoire Mialon",
      "Thomas Scialom",
      "Emmanuel Dupoux",
      "Dieuwke Hupkes",
      "Pascal Vincent"
    ]
  },
  "http://arxiv.org/abs/2311.16483": {
    "title": "ChartLlama: A Multimodal LLM for Chart Understanding and Generation",
    "volume": "Nov",
    "abstract": "Multi-modal large language models have demonstrated impressive performances on most vision-language tasks. However, the model generally lacks the understanding capabilities for specific domain data, particularly when it comes to interpreting chart figures. This is mainly due to the lack of relevant multi-modal instruction tuning datasets. In this article, we create a high-quality instruction-tuning dataset leveraging GPT-4. We develop a multi-step data generation process in which different steps are responsible for generating tabular data, creating chart figures, and designing instruction tuning data separately. Our method's flexibility enables us to generate diverse, high-quality instruction-tuning data consistently and efficiently while maintaining a low resource expenditure. Additionally, it allows us to incorporate a wider variety of chart and task types not yet featured in existing datasets. Next, we introduce ChartLlama, a multi-modal large language model that we've trained using our created dataset. ChartLlama outperforms all prior methods in ChartQA, Chart-to-text, and Chart-extraction evaluation benchmarks. Additionally, ChartLlama significantly improves upon the baseline in our specially compiled chart dataset, which includes new chart and task types. The results of ChartLlama confirm the value and huge potential of our proposed data generation method in enhancing chart comprehension",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yucheng Han",
      "Chi Zhang",
      "Xin Chen",
      "Xu Yang",
      "Zhibin Wang",
      "Gang Yu",
      "Bin Fu",
      "Hanwang Zhang"
    ]
  },
  "http://arxiv.org/abs/2311.15896": {
    "title": "Data Generation for Post-OCR correction of Cyrillic handwriting",
    "volume": "Nov",
    "abstract": "This paper introduces a novel approach to post-Optical Character Recognition Correction (POC) for handwritten Cyrillic text, addressing a significant gap in current research methodologies. This gap is due to the lack of large text corporas that provide OCR errors for further training of language-based POC models, which are demanding in terms of corpora size. Our study primarily focuses on the development and application of a synthetic handwriting generation engine based on B\\'ezier curves. Such an engine generates highly realistic handwritten text in any amounts, which we utilize to create a substantial dataset by transforming Russian text corpora sourced from the internet. We apply a Handwritten Text Recognition (HTR) model to this dataset to identify OCR errors, forming the basis for our POC model training. The correction model is trained on a 90-symbol input context, utilizing a pre-trained T5 architecture with a seq2seq correction task. We evaluate our approach on HWR200 and School_notebooks_RU datasets as they provide significant challenges in the HTR domain. Furthermore, POC can be used to highlight errors for teachers, evaluating student performance. This can be done simply by comparing sentences before and after correction, displaying differences in text. Our primary contribution lies in the innovative use of B\\'ezier curves for Cyrillic text generation and subsequent error correction using a specialized POC model. We validate our approach by presenting Word Accuracy Rate (WAR) and Character Accuracy Rate (CAR) results, both with and without post-OCR correction, using real open corporas of handwritten Cyrillic text. These results, coupled with our methodology, are designed to be reproducible, paving the way for further advancements in the field of OCR and handwritten text analysis. Paper contributions can be found in https://github.com/dbrainio/CyrillicHandwritingPOC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Evgenii Davydkin",
      "Aleksandr Markelov",
      "Egor Iuldashev",
      "Anton Dudkin",
      "Ivan Krivorotov"
    ]
  },
  "http://arxiv.org/abs/2311.15786": {
    "title": "YUAN 2.0: A Large Language Model with Localized Filtering-based Attention",
    "volume": "Nov",
    "abstract": "In this work, the Localized Filtering-based Attention (LFA) is introduced to incorporate prior knowledge of local dependencies of natural language into Attention. Based on LFA, we develop and release Yuan 2.0, a large language model with parameters ranging from 2.1 billion to 102.6 billion. A data filtering and generation method is presented to build pretraining and fine-tuning dataset in high quality. A distributed training method with non-uniform pipeline parallel, data parallel, and optimizer parallel is proposed, which greatly reduces the bandwidth requirements of intra-node communication, and achieves good performance in large-scale distributed training. Yuan 2.0 models display impressive ability in code generation, math problem-solving, and chat compared with existing models. The latest version of YUAN 2.0, including model weights and source code, is accessible at Github",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaohua Wu",
      "Xudong Zhao",
      "Shenling Wang",
      "Jiangang Luo",
      "Lingjun Li",
      "Xi Chen",
      "Bing Zhao",
      "Wei Wang",
      "Tong Yu",
      "Rongguo Zhang",
      "Jiahua Zhang",
      "Chao Wang"
    ]
  },
  "http://arxiv.org/abs/2311.15781": {
    "title": "Increasing Coverage and Precision of Textual Information in Multilingual Knowledge Graphs",
    "volume": "Nov",
    "abstract": "Recent work in Natural Language Processing and Computer Vision has been using textual information -- e.g., entity names and descriptions -- available in knowledge graphs to ground neural models to high-quality structured data. However, when it comes to non-English languages, the quantity and quality of textual information are comparatively scarce. To address this issue, we introduce the novel task of automatic Knowledge Graph Enhancement (KGE) and perform a thorough investigation on bridging the gap in both the quantity and quality of textual information between English and non-English languages. More specifically, we: i) bring to light the problem of increasing multilingual coverage and precision of entity names and descriptions in Wikidata; ii) demonstrate that state-of-the-art methods, namely, Machine Translation (MT), Web Search (WS), and Large Language Models (LLMs), struggle with this task; iii) present M-NTA, a novel unsupervised approach that combines MT, WS, and LLMs to generate high-quality textual information; and, iv) study the impact of increasing multilingual coverage and precision of non-English textual information in Entity Linking, Knowledge Graph Completion, and Question Answering. As part of our effort towards better multilingual knowledge graphs, we also introduce WikiKGE-10, the first human-curated benchmark to evaluate KGE approaches in 10 languages across 7 language families",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simone Conia",
      "Min Li",
      "Daniel Lee",
      "Umar Farooq Minhas",
      "Ihab Ilyas",
      "Yunyao Li"
    ]
  },
  "http://arxiv.org/abs/2311.15766": {
    "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
    "volume": "Nov",
    "abstract": "In recent years, large language models (LLMs) have spurred a new research paradigm in natural language processing. Despite their excellent capability in knowledge-based question answering and reasoning, their potential to retain faulty or even harmful knowledge poses risks of malicious application. The challenge of mitigating this issue and transforming these models into purer assistants is crucial for their widespread applicability. Unfortunately, Retraining LLMs repeatedly to eliminate undesirable knowledge is impractical due to their immense parameters. Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge in an efficient manner, without affecting unrelated knowledge in the model. To this end, we provide a survey of knowledge unlearning in the era of LLMs. Firstly, we formally define the knowledge unlearning problem and distinguish it from related works. Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and in-context learning, and introduce details of these unlearning methods. We further present evaluation datasets used in existing methods, and finally conclude this survey by presenting the ongoing challenges and future directions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nianwen Si",
      "Hao Zhang",
      "Heyu Chang",
      "Wenlin Zhang",
      "Dan Qu",
      "Weiqiang Zhang"
    ]
  },
  "http://arxiv.org/abs/2311.15759": {
    "title": "Towards Vision Enhancing LLMs: Empowering Multimodal Knowledge Storage and Sharing in LLMs",
    "volume": "Nov",
    "abstract": "Recent advancements in multimodal large language models (MLLMs) have achieved significant multimodal generation capabilities, akin to GPT-4. These models predominantly map visual information into language representation space, leveraging the vast knowledge and powerful text generation abilities of LLMs to produce multimodal instruction-following responses. We could term this method as LLMs for Vision because of its employing LLMs for visual-language understanding, yet observe that these MLLMs neglect the potential of harnessing visual knowledge to enhance overall capabilities of LLMs, which could be regraded as Vision Enhancing LLMs. In this paper, we propose an approach called MKS2, aimed at enhancing LLMs through empowering Multimodal Knowledge Storage and Sharing in LLMs. Specifically, we introduce the Modular Visual Memory, a component integrated into the internal blocks of LLMs, designed to store open-world visual information efficiently. Additionally, we present a soft Mixtures-of-Multimodal Experts architecture in LLMs to invoke multimodal knowledge collaboration during generation. Our comprehensive experiments demonstrate that MKS2 substantially augments the reasoning capabilities of LLMs in contexts necessitating physical or commonsense knowledge. It also delivers competitive results on multimodal benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunxin Li",
      "Baotian Hu",
      "Wei Wang",
      "Xiaochun Cao",
      "Min Zhang"
    ]
  },
  "http://arxiv.org/abs/2311.15723": {
    "title": "Italian Crossword Generator: Enhancing Education through Interactive Word Puzzles",
    "volume": "Nov",
    "abstract": "Educational crosswords offer numerous benefits for students, including increased engagement, improved understanding, critical thinking, and memory retention. Creating high-quality educational crosswords can be challenging, but recent advances in natural language processing and machine learning have made it possible to use language models to generate nice wordplays. The exploitation of cutting-edge language models like GPT3-DaVinci, GPT3-Curie, GPT3-Babbage, GPT3-Ada, and BERT-uncased has led to the development of a comprehensive system for generating and verifying crossword clues. A large dataset of clue-answer pairs was compiled to fine-tune the models in a supervised manner to generate original and challenging clues from a given keyword. On the other hand, for generating crossword clues from a given text, Zero/Few-shot learning techniques were used to extract clues from the input text, adding variety and creativity to the puzzles. We employed the fine-tuned model to generate data and labeled the acceptability of clue-answer parts with human supervision. To ensure quality, we developed a classifier by fine-tuning existing language models on the labeled dataset. Conversely, to assess the quality of clues generated from the given text using zero/few-shot learning, we employed a zero-shot learning approach to check the quality of generated clues. The results of the evaluation have been very promising, demonstrating the effectiveness of the approach in creating high-standard educational crosswords that offer students engaging and rewarding learning experiences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kamyar Zeinalipour",
      "Tommaso laquinta",
      "Asya Zanollo",
      "Giovanni Angelini",
      "Leonardo Rigutini",
      "Marco Maggini",
      "Marco Gori"
    ]
  },
  "http://arxiv.org/abs/2311.15716": {
    "title": "Justifiable Artificial Intelligence: Engineering Large Language Models for Legal Applications",
    "volume": "Nov",
    "abstract": "In this work, I discuss how Large Language Models can be applied in the legal domain, circumventing their current drawbacks. Despite their large success and acceptance, their lack of explainability hinders legal experts to trust in their output, and this happens rightfully so. However, in this paper, I argue in favor of a new view, Justifiable Artificial Intelligence, instead of focusing on Explainable Artificial Intelligence. I discuss in this paper how gaining evidence for and against a Large Language Model's output may make their generated texts more trustworthy - or hold them accountable for misinformation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sabine Wehnert"
    ]
  },
  "http://arxiv.org/abs/2311.15698": {
    "title": "Cerbero-7B: A Leap Forward in Language-Specific LLMs Through Enhanced Chat Corpus Generation and Evaluation",
    "volume": "Nov",
    "abstract": "This study introduces a novel approach for generating high-quality, language-specific chat corpora using a self-chat mechanism. We combine a generator LLM for creating new samples and an embedder LLM to ensure diversity. A new Masked Language Modelling (MLM) model-based quality assessment metric is proposed for evaluating and filtering the corpora. Utilizing the llama2-70b as the generator and a multilingual sentence transformer as embedder, we generate an Italian chat corpus and refine the Fauno corpus, which is based on translated English ChatGPT self-chat data. The refinement uses structural assertions and Natural Language Processing techniques. Both corpora undergo a comprehensive quality evaluation using the proposed MLM model-based quality metric. The Italian LLM fine-tuned with these corpora demonstrates significantly enhanced language comprehension and question-answering skills. The resultant model, cerbero-7b, establishes a new state-of-the-art for Italian LLMs. This approach marks a substantial advancement in the development of language-specific LLMs, with a special emphasis on augmenting corpora for underrepresented languages like Italian",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Federico A. Galatolo",
      "Mario G. C. A. Cimino"
    ]
  },
  "http://arxiv.org/abs/2311.15653": {
    "title": "MoDS: Model-oriented Data Selection for Instruction Tuning",
    "volume": "Nov",
    "abstract": "Instruction tuning has become the de facto method to equip large language models (LLMs) with the ability of following user instructions. Usually, hundreds of thousands or millions of instruction-following pairs are employed to fine-tune the foundation LLMs. Recently, some studies show that a small number of high-quality instruction data is enough. However, how to select appropriate instruction data for a given LLM is still an open problem. To address this problem, in this paper we present a model-oriented data selection (MoDS) approach, which selects instruction data based on a new criteria considering three aspects: quality, coverage and necessity. First, our approach utilizes a quality evaluation model to filter out the high-quality subset from the original instruction dataset, and then designs an algorithm to further select from the high-quality subset a seed instruction dataset with good coverage. The seed dataset is applied to fine-tune the foundation LLM to obtain an initial instruction-following LLM. Finally, we develop a necessity evaluation model to find out the instruction data which are performed badly in the initial instruction-following LLM and consider them necessary instructions to further improve the LLMs. In this way, we can get a small high-quality, broad-coverage and high-necessity subset from the original instruction datasets. Experimental results show that, the model fine-tuned with 4,000 instruction pairs selected by our approach could perform better than the model fine-tuned with the full original dataset which includes 214k instruction data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianlong Du",
      "Chengqing Zong",
      "Jiajun Zhang"
    ]
  },
  "http://arxiv.org/abs/2311.15648": {
    "title": "Reinforcement Learning from Diffusion Feedback: Q* for Image Search",
    "volume": "Nov",
    "abstract": "Large vision-language models are steadily gaining personalization capabilities at the cost of fine-tuning or data augmentation. We present two models for image generation using model-agnostic learning that align semantic priors with generative capabilities. RLDF, or Reinforcement Learning from Diffusion Feedback, is a singular approach for visual imitation through prior-preserving reward function guidance. This employs Q-learning (with standard Q*) for generation and follows a semantic-rewarded trajectory for image search through finite encoding-tailored actions. The second proposed method, noisy diffusion gradient, is optimization driven. At the root of both methods is a special CFG encoding that we propose for continual semantic guidance. Using only a single input image and no text input, RLDF generates high-quality images over varied domains including retail, sports and agriculture showcasing class-consistency and strong visual diversity. Project website is available at https://infernolia.github.io/RLDF",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aboli Marathe"
    ]
  },
  "http://arxiv.org/abs/2311.15642": {
    "title": "InfoPattern: Unveiling Information Propagation Patterns in Social Media",
    "volume": "Nov",
    "abstract": "Social media play a significant role in shaping public opinion and influencing ideological communities through information propagation. Our demo InfoPattern centers on the interplay between language and human ideology. The demo (Code: https://github.com/blender-nlp/InfoPattern ) is capable of: (1) red teaming to simulate adversary responses from opposite ideology communities; (2) stance detection to identify the underlying political sentiments in each message; (3) information propagation graph discovery to reveal the evolution of claims across various communities over time. (Live Demo: https://incas.csl.illinois.edu/blender/About )",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi Han",
      "Jialiang Xu",
      "Manling Li",
      "Hanning Zhang",
      "Tarek Abdelzaher",
      "Heng Ji"
    ]
  },
  "http://arxiv.org/abs/2311.16203": {
    "title": "ChatTraffic: Text-to-Traffic Generation via Diffusion Model",
    "volume": "Nov",
    "abstract": "Traffic prediction is one of the most significant foundations in Intelligent Transportation Systems (ITS). Traditional traffic prediction methods rely only on historical traffic data to predict traffic trends and face two main challenges. 1) insensitivity to unusual events. 2) poor performance in long-term prediction. In this work, we explore how generative models combined with text describing the traffic system can be applied for traffic generation and name the task Text-to-Traffic Generation (TTG). The key challenge of the TTG task is how to associate text with the spatial structure of the road network and traffic data for generating traffic situations. To this end, we propose ChatTraffic, the first diffusion model for text-to-traffic generation. To guarantee the consistency between synthetic and real data, we augment a diffusion model with the Graph Convolutional Network (GCN) to extract spatial correlations of traffic data. In addition, we construct a large dataset containing text-traffic pairs for the TTG task. We benchmarked our model qualitatively and quantitatively on the released dataset. The experimental results indicate that ChatTraffic can generate realistic traffic situations from the text. Our code and dataset are available at https://github.com/ChyaZhang/ChatTraffic",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengyang Zhang",
      "Yong Zhang",
      "Qitan Shao",
      "Bo Li",
      "Yisheng Lv",
      "Xinglin Piao",
      "Baocai Yin"
    ]
  },
  "http://arxiv.org/abs/2311.15626": {
    "title": "The WebCrow French Crossword Solver",
    "volume": "Nov",
    "abstract": "Crossword puzzles are one of the most popular word games, played in different languages all across the world, where riddle style can vary significantly from one country to another. Automated crossword resolution is challenging, and typical solvers rely on large databases of previously solved crosswords. In this work, we extend WebCrow 2.0, an automatic crossword solver, to French, making it the first program for crossword solving in the French language. To cope with the lack of a large repository of clue-answer crossword data, WebCrow 2.0 exploits multiple modules, called experts, that retrieve candidate answers from heterogeneous resources, such as the web, knowledge graphs, and linguistic rules. We compared WebCrow's performance against humans in two different challenges. Despite the limited amount of past crosswords, French WebCrow was competitive, actually outperforming humans in terms of speed and accuracy, thus proving its capabilities to generalize to new languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giovanni Angelini",
      "Marco Ernandes",
      "Tommaso laquinta",
      "Caroline Stehlé",
      "Fanny Simões",
      "Kamyar Zeinalipour",
      "Andrea Zugarini",
      "Marco Gori"
    ]
  },
  "http://arxiv.org/abs/2311.15623": {
    "title": "Injecting linguistic knowledge into BERT for Dialogue State Tracking",
    "volume": "Nov",
    "abstract": "Dialogue State Tracking (DST) models often employ intricate neural network architectures, necessitating substantial training data, and their inference processes lack transparency. This paper proposes a method that extracts linguistic knowledge via an unsupervised framework and subsequently utilizes this knowledge to augment BERT's performance and interpretability in DST tasks. The knowledge extraction procedure is computationally economical and does not necessitate annotations or additional training data. The injection of the extracted knowledge necessitates the addition of only simple neural modules. We employ the Convex Polytopic Model (CPM) as a feature extraction tool for DST tasks and illustrate that the acquired features correlate with the syntactic and semantic patterns in the dialogues. This correlation facilitates a comprehensive understanding of the linguistic features influencing the DST model's decision-making process. We benchmark this framework on various DST tasks and observe a notable improvement in accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohan Feng",
      "Xixin Wu",
      "Helen Meng"
    ]
  },
  "http://arxiv.org/abs/2311.15614": {
    "title": "FreeAL: Towards Human-Free Active Learning in the Era of Large Language Models",
    "volume": "Nov",
    "abstract": "Collecting high-quality labeled data for model training is notoriously time-consuming and labor-intensive for various NLP tasks. While copious solutions, such as active learning for small language models (SLMs) and prevalent in-context learning in the era of large language models (LLMs), have been proposed and alleviate the labeling burden to some extent, their performances are still subject to human intervention. It is still underexplored how to reduce the annotation cost in the LLMs era. To bridge this, we revolutionize traditional active learning and propose an innovative collaborative learning framework FreeAL to interactively distill and filter the task-specific knowledge from LLMs. During collaborative training, an LLM serves as an active annotator inculcating its coarse-grained knowledge, while a downstream SLM is incurred as a student to filter out high-quality in-context samples to feedback LLM for the subsequent label refinery. Extensive experiments on eight benchmark datasets demonstrate that FreeAL largely enhances the zero-shot performances for both SLM and LLM without any human supervision. The code is available at https://github.com/Justherozen/FreeAL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruixuan Xiao",
      "Yiwen Dong",
      "Junbo Zhao",
      "Runze Wu",
      "Minmin Lin",
      "Gang Chen",
      "Haobo Wang"
    ]
  },
  "http://arxiv.org/abs/2311.15596": {
    "title": "Can Vision-Language Models Think from a First-Person Perspective?",
    "volume": "Nov",
    "abstract": "Vision-language models (VLMs) have recently shown promising results in traditional downstream tasks. Evaluation studies have emerged to assess their abilities, with the majority focusing on the third-person perspective, and only a few addressing specific tasks from the first-person perspective. However, the capability of VLMs to \"think\" from a first-person perspective, a crucial attribute for advancing autonomous agents and robotics, remains largely unexplored. To bridge this research gap, we introduce EgoThink, a novel visual question-answering benchmark that encompasses six core capabilities with twelve detailed dimensions. The benchmark is constructed using selected clips from egocentric videos, with manually annotated question-answer pairs containing first-person information. To comprehensively assess VLMs, we evaluate eighteen popular VLMs on EgoThink. Moreover, given the open-ended format of the answers, we use GPT-4 as the automatic judge to compute single-answer grading. Experimental results indicate that although GPT-4V leads in numerous dimensions, all evaluated VLMs still possess considerable potential for improvement in first-person perspective tasks. Meanwhile, enlarging the number of trainable parameters has the most significant impact on model performance on EgoThink. In conclusion, EgoThink serves as a valuable addition to existing evaluation benchmarks for VLMs, providing an indispensable resource for future research in the realm of embodied artificial intelligence and robotics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sijie Cheng",
      "Zhicheng Guo",
      "Jingwen Wu",
      "Kechen Fang",
      "Peng Li",
      "Huaping Liu",
      "Yang Liu"
    ]
  },
  "http://arxiv.org/abs/2311.16201": {
    "title": "Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation",
    "volume": "Nov",
    "abstract": "Recent advances in image tokenizers, such as VQ-VAE, have enabled text-to-image generation using auto-regressive methods, similar to language modeling. However, these methods have yet to leverage pre-trained language models, despite their adaptability to various downstream tasks. In this work, we explore this gap by adapting a pre-trained language model for auto-regressive text-to-image generation, and find that pre-trained language models offer limited help. We provide a two-fold explanation by analyzing tokens from each modality. First, we demonstrate that image tokens possess significantly different semantics compared to text tokens, rendering pre-trained language models no more effective in modeling them than randomly initialized ones. Second, the text tokens in the image-text datasets are too simple compared to normal language model pre-training data, which causes the catastrophic degradation of language models' capability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhui Zhang",
      "Brandon McKinzie",
      "Zhe Gan",
      "Vaishaal Shankar",
      "Alexander Toshev"
    ]
  },
  "http://arxiv.org/abs/2311.15566": {
    "title": "SpotServe: Serving Generative Large Language Models on Preemptible Instances",
    "volume": "Nov",
    "abstract": "The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them cheaply. This paper aims to reduce the monetary cost for serving LLMs by leveraging preemptible GPU instances on modern clouds, which offer accesses to spare GPUs at a much cheaper price than regular instances but may be preempted by the cloud at any time. Serving LLMs on preemptible instances requires addressing challenges induced by frequent instance preemptions and the necessity of migrating instances to handle these preemptions. This paper presents SpotServe, the first distributed LLM serving system on preemptible instances. Several key techniques in SpotServe realize fast and reliable serving of generative LLMs on cheap preemptible instances. First, SpotServe dynamically adapts the LLM parallelization configuration for dynamic instance availability and fluctuating workload, while balancing the trade-off among the overall throughput, inference latency and monetary costs. Second, to minimize the cost of migrating instances for dynamic reparallelization, the task of migrating instances is formulated as a bipartite graph matching problem, which uses the Kuhn-Munkres algorithm to identify an optimal migration plan that minimizes communications. Finally, to take advantage of the grace period offered by modern clouds, we introduce stateful inference recovery, a new inference mechanism that commits inference progress at a much finer granularity and allows SpotServe to cheaply resume inference upon preemption. We evaluate on real spot instance preemption traces and various popular LLMs and show that SpotServe can reduce the P99 tail latency by 2.4 - 9.1x compared with the best existing LLM serving systems. We also show that SpotServe can leverage the price advantage of preemptive instances, saving 54% monetary cost compared with only using on-demand instances",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xupeng Miao",
      "Chunan Shi",
      "Jiangfei Duan",
      "Xiaoli Xi",
      "Dahua Lin",
      "Bin Cui",
      "Zhihao Jia"
    ]
  },
  "http://arxiv.org/abs/2311.15565": {
    "title": "Evaluating the Efficacy of Hybrid Deep Learning Models in Distinguishing AI-Generated Text",
    "volume": "Nov",
    "abstract": "My research investigates the use of cutting-edge hybrid deep learning models to accurately differentiate between AI-generated text and human writing. I applied a robust methodology, utilising a carefully selected dataset comprising AI and human texts from various sources, each tagged with instructions. Advanced natural language processing techniques facilitated the analysis of textual features. Combining sophisticated neural networks, the custom model enabled it to detect nuanced differences between AI and human content",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Finbarrs Oketunji"
    ]
  },
  "http://arxiv.org/abs/2311.15564": {
    "title": "Boot and Switch: Alternating Distillation for Zero-Shot Dense Retrieval",
    "volume": "Nov",
    "abstract": "Neural 'dense' retrieval models are state of the art for many datasets, however these models often exhibit limited domain transfer ability. Existing approaches to adaptation are unwieldy, such as requiring explicit supervision, complex model architectures, or massive external models. We present $\\texttt{ABEL}$, a simple but effective unsupervised method to enhance passage retrieval in zero-shot settings. Our technique follows a straightforward loop: a dense retriever learns from supervision signals provided by a reranker, and subsequently, the reranker is updated based on feedback from the improved retriever. By iterating this loop, the two components mutually enhance one another's performance. Experimental results demonstrate that our unsupervised $\\texttt{ABEL}$ model outperforms both leading supervised and unsupervised retrievers on the BEIR benchmark. Meanwhile, it exhibits strong adaptation abilities to tasks and domains that were unseen during training. By either fine-tuning $\\texttt{ABEL}$ on labelled data or integrating it with existing supervised dense retrievers, we achieve state-of-the-art results.\\footnote{Source code is available at \\url{https://github.com/Fantabulous-J/BootSwitch}.}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Jiang",
      "Qiongkai Xu",
      "Tom Drummond",
      "Trevor Cohn"
    ]
  },
  "http://arxiv.org/abs/2311.15563": {
    "title": "Noisy Self-Training with Synthetic Queries for Dense Retrieval",
    "volume": "Nov",
    "abstract": "Although existing neural retrieval models reveal promising results when training data is abundant and the performance keeps improving as training data increases, collecting high-quality annotated data is prohibitively costly. To this end, we introduce a novel noisy self-training framework combined with synthetic queries, showing that neural retrievers can be improved in a self-evolution manner with no reliance on any external models. Experimental results show that our method improves consistently over existing methods on both general-domain (e.g., MS-MARCO) and out-of-domain (i.e., BEIR) retrieval benchmarks. Extra analysis on low-resource settings reveals that our method is data efficient and outperforms competitive baselines, with as little as 30% of labelled training data. Further extending the framework for reranker training demonstrates that the proposed method is general and yields additional gains on tasks of diverse domains.\\footnote{Source code is available at \\url{https://github.com/Fantabulous-J/Self-Training-DPR}}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Jiang",
      "Tom Drummond",
      "Trevor Cohn"
    ]
  },
  "http://arxiv.org/abs/2311.15548": {
    "title": "Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination",
    "volume": "Nov",
    "abstract": "The hallucination issue is recognized as a fundamental deficiency of large language models (LLMs), especially when applied to fields such as finance, education, and law. Despite the growing concerns, there has been a lack of empirical investigation. In this paper, we provide an empirical examination of LLMs' hallucination behaviors in financial tasks. First, we empirically investigate LLM model's ability of explaining financial concepts and terminologies. Second, we assess LLM models' capacity of querying historical stock prices. Third, to alleviate the hallucination issue, we evaluate the efficacy of four practical methods, including few-shot learning, Decoding by Contrasting Layers (DoLa), the Retrieval Augmentation Generation (RAG) method and the prompt-based tool learning method for a function to generate a query command. Finally, our major finding is that off-the-shelf LLMs experience serious hallucination behaviors in financial tasks. Therefore, there is an urgent need to call for research efforts in mitigating LLMs' hallucination",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoqiang Kang",
      "Xiao-Yang Liu"
    ]
  },
  "http://arxiv.org/abs/2311.15544": {
    "title": "The effect of source disclosure on evaluation of AI-generated messages: A two-part study",
    "volume": "Nov",
    "abstract": "Advancements in artificial intelligence (AI) over the last decade demonstrate that machines can exhibit communicative behavior and influence how humans think, feel, and behave. In fact, the recent development of ChatGPT has shown that large language models (LLMs) can be leveraged to generate high-quality communication content at scale and across domains, suggesting that they will be increasingly used in practice. However, many questions remain about how knowing the source of the messages influences recipients' evaluation of and preference for AI-generated messages compared to human-generated messages. This paper investigated this topic in the context of vaping prevention messaging. In Study 1, which was pre-registered, we examined the influence of source disclosure on people's evaluation of AI-generated health prevention messages compared to human-generated messages. We found that source disclosure (i.e., labeling the source of a message as AI vs. human) significantly impacted the evaluation of the messages but did not significantly alter message rankings. In a follow-up study (Study 2), we examined how the influence of source disclosure may vary by the participants' negative attitudes towards AI. We found a significant moderating effect of negative attitudes towards AI on message evaluation, but not for message selection. However, for those with moderate levels of negative attitudes towards AI, source disclosure decreased the preference for AI-generated messages. Overall, the results of this series of studies showed a slight bias against AI-generated messages once the source was disclosed, adding to the emerging area of study that lies at the intersection of AI and communication",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sue Lim",
      "Ralf Schmälzle"
    ]
  },
  "http://arxiv.org/abs/2311.16480": {
    "title": "MI-Gen: Multiple Instance Generation of Pathology Reports for Gigapixel Whole-Slide Images",
    "volume": "Nov",
    "abstract": "Whole slide images are the foundation of digital pathology for the diagnosis and treatment of carcinomas. Writing pathology reports is laborious and error-prone for inexperienced pathologists. To reduce the workload and improve clinical automation, we investigate how to generate pathology reports given whole slide images. On the data end, we curated the largest WSI-text dataset (TCGA-PathoText). In specific, we collected nearly 10000 high-quality WSI-text pairs for visual-language models by recognizing and cleaning pathology reports which narrate diagnostic slides in TCGA. On the model end, we propose the multiple instance generative model (MI-Gen) which can produce pathology reports for gigapixel WSIs. We benchmark our model on the largest subset of TCGA-PathoText. Experimental results show our model can generate pathology reports which contain multiple clinical clues. Furthermore, WSI-text prediction can be seen as an approach of visual-language pre-training, which enables our model to be transferred to downstream diagnostic tasks like carcinoma grading and phenotyping. We observe that simple semantic extraction from the pathology reports can achieve the best performance (0.838 of F1 score) on BRCA subtyping without adding extra parameters or tricky fine-tuning. Our collected dataset and related code will all be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pingyi Chen",
      "Honglin Li",
      "Chenglu Zhu",
      "Sunyi Zheng",
      "Lin Yang"
    ]
  },
  "http://arxiv.org/abs/2311.15525": {
    "title": "Overview of the VLSP 2022 -- Abmusu Shared Task: A Data Challenge for Vietnamese Abstractive Multi-document Summarization",
    "volume": "Nov",
    "abstract": "This paper reports the overview of the VLSP 2022 - Vietnamese abstractive multi-document summarization (Abmusu) shared task for Vietnamese News. This task is hosted at the 9$^{th}$ annual workshop on Vietnamese Language and Speech Processing (VLSP 2022). The goal of Abmusu shared task is to develop summarization systems that could create abstractive summaries automatically for a set of documents on a topic. The model input is multiple news documents on the same topic, and the corresponding output is a related abstractive summary. In the scope of Abmusu shared task, we only focus on Vietnamese news summarization and build a human-annotated dataset of 1,839 documents in 600 clusters, collected from Vietnamese news in 8 categories. Participated models are evaluated and ranked in terms of \\texttt{ROUGE2-F1} score, the most typical evaluation metric for document summarization problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mai-Vu Tran",
      "Hoang-Quynh Le",
      "Duy-Cat Can",
      "Quoc-An Nguyen"
    ]
  },
  "http://arxiv.org/abs/2311.15513": {
    "title": "A Comparative and Experimental Study on Automatic Question Answering Systems and its Robustness against Word Jumbling",
    "volume": "Nov",
    "abstract": "Question answer generation using Natural Language Processing models is ubiquitous in the world around us. It is used in many use cases such as the building of chat bots, suggestive prompts in google search and also as a way of navigating information in banking mobile applications etc. It is highly relevant because a frequently asked questions (FAQ) list can only have a finite amount of questions but a model which can perform question answer generation could be able to answer completely new questions that are within the scope of the data. This helps us to be able to answer new questions accurately as long as it is a relevant question. In commercial applications, it can be used to increase customer satisfaction and ease of usage. However a lot of data is generated by humans so it is susceptible to human error and this can adversely affect the model's performance and we are investigating this through our work",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shashidhar Reddy Javaji",
      "Haoran Hu",
      "Sai Sameer Vennam",
      "Vijaya Gajanan Buddhavarapu"
    ]
  },
  "http://arxiv.org/abs/2311.15509": {
    "title": "A Corpus for Named Entity Recognition in Chinese Novels with Multi-genres",
    "volume": "Nov",
    "abstract": "Entities like person, location, organization are important for literary text analysis. The lack of annotated data hinders the progress of named entity recognition (NER) in literary domain. To promote the research of literary NER, we build the largest multi-genre literary NER corpus containing 263,135 entities in 105,851 sentences from 260 online Chinese novels spanning 13 different genres. Based on the corpus, we investigate characteristics of entities from different genres. We propose several baseline NER models and conduct cross-genre and cross-domain experiments. Experimental results show that genre difference significantly impact NER performance though not as much as domain difference like literary domain and news domain. Compared with NER in news domain, literary NER still needs much improvement and the Out-of-Vocabulary (OOV) problem is more challenging due to the high variety of entities in literary works",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanjie Zhao",
      "Jinge Xie",
      "Yuchen Yan",
      "Yuxiang Jia",
      "Yawen Ye",
      "Hongying Zan"
    ]
  },
  "http://arxiv.org/abs/2311.15507": {
    "title": "Improving Word Sense Disambiguation in Neural Machine Translation with Salient Document Context",
    "volume": "Nov",
    "abstract": "Lexical ambiguity is a challenging and pervasive problem in machine translation (\\mt). We introduce a simple and scalable approach to resolve translation ambiguity by incorporating a small amount of extra-sentential context in neural \\mt. Our approach requires no sense annotation and no change to standard model architectures. Since actual document context is not available for the vast majority of \\mt training data, we collect related sentences for each input to construct pseudo-documents. Salient words from pseudo-documents are then encoded as a prefix to each source sentence to condition the generation of the translation. To evaluate, we release \\docmucow, a challenge set for translation disambiguation based on the English-German \\mucow \\cite{raganato-etal-2020-evaluation} augmented with document IDs. Extensive experiments show that our method translates ambiguous source words better than strong sentence-level baselines and comparable document-level baselines while reducing training costs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elijah Rippeth",
      "Marine Carpuat",
      "Kevin Duh",
      "Matt Post"
    ]
  },
  "http://arxiv.org/abs/2311.15500": {
    "title": "Function-constrained Program Synthesis",
    "volume": "Nov",
    "abstract": "This work introduces (1) a technique that allows large language models (LLMs) to leverage user-provided code when solving programming tasks and (2) a method to iteratively generate modular sub-functions that can aid future code generation attempts when the initial code generated by the LLM is inadequate. Generating computer programs in general-purpose programming languages like Python poses a challenge for LLMs when instructed to use code provided in the prompt. Code-specific LLMs (e.g., GitHub Copilot, CodeLlama2) can generate code completions in real-time by drawing on all code available in a development environment. However, restricting code-specific LLMs to use only in-context code is not straightforward, as the model is not explicitly instructed to use the user-provided code and users cannot highlight precisely which snippets of code the model should incorporate into its context. Moreover, current systems lack effective recovery methods, forcing users to iteratively re-prompt the model with modified prompts until a sufficient solution is reached. Our method differs from traditional LLM-powered code-generation by constraining code-generation to an explicit function set and enabling recovery from failed attempts through automatically generated sub-functions. When the LLM cannot produce working code, we generate modular sub-functions to aid subsequent attempts at generating functional code. A by-product of our method is a library of reusable sub-functions that can solve related tasks, imitating a software team where efficiency scales with experience. We also introduce a new \"half-shot\" evaluation paradigm that provides tighter estimates of LLMs' coding abilities compared to traditional zero-shot evaluation. Our proposed evaluation method encourages models to output solutions in a structured format, decreasing syntax errors that can be mistaken for poor coding ability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Hajali",
      "Ignas Budvytis"
    ]
  },
  "http://arxiv.org/abs/2311.15490": {
    "title": "Optimizing and Fine-tuning Large Language Model for Urban Renewal",
    "volume": "Nov",
    "abstract": "This study aims to innovatively explore adaptive applications of large language models (LLM) in urban renewal. It also aims to improve its performance and text generation quality for knowledge question-answering (QA) tasks. Based on the ChatGLM, we automatically generate QA datasets using urban renewal scientific literature corpora in a self-instruct manner and then conduct joint fine-tuning training on the model using the Prefix and LoRA fine-tuning methods to create an LLM for urban renewal. By guiding the LLM to automatically generate QA data based on prompt words and given text, it is possible to quickly obtain datasets in the urban renewal field and provide data support for the fine-tuning training of LLMs. The experimental results show that the joint fine-tuning training method proposed in this study can significantly improve the performance of LLM on the QA tasks. Compared with LoRA fine-tuning, the method improves the Bleu and Rouge metrics on the test by about 5%; compared with the model before fine-tuning, the method improves the Bleu and Rouge metrics by about 15%-20%. This study demonstrates the effectiveness and superiority of the joint fine-tuning method using Prefix and LoRA for ChatGLM in the urban renewal knowledge QA tasks. It provides a new approach for fine-tuning LLMs on urban renewal-related tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Wang",
      "Xianyao Ling",
      "Tom Zhang",
      "Xuecao Li",
      "Shaolan Wang",
      "Zhixing Li",
      "Liang Zhang",
      "Peng Gong"
    ]
  },
  "http://arxiv.org/abs/2311.15480": {
    "title": "Automatic Time Signature Determination for New Scores Using Lyrics for Latent Rhythmic Structure",
    "volume": "Nov",
    "abstract": "There has recently been a sharp increase in interest in Artificial Intelligence-Generated Content (AIGC). Despite this, musical components such as time signatures have not been studied sufficiently to form an algorithmic determination approach for new compositions, especially lyrical songs. This is likely because of the neglect of musical details, which is critical for constructing a robust framework. Specifically, time signatures establish the fundamental rhythmic structure for almost all aspects of a song, including the phrases and notes. In this paper, we propose a novel approach that only uses lyrics as input to automatically generate a fitting time signature for lyrical songs and uncover the latent rhythmic structure utilizing explainable machine learning models. In particular, we devise multiple methods that are associated with discovering lyrical patterns and creating new features that simultaneously contain lyrical, rhythmic, and statistical information. In this approach, the best of our experimental results reveal a 97.6% F1 score and a 0.996 Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC) score. In conclusion, our research directly generates time signatures from lyrics automatically for new scores utilizing machine learning, which is an innovative idea that approaches an understudied component of musicology and therefore contributes significantly to the future of Artificial Intelligence (AI) music generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Callie C. Liao",
      "Duoduo Liao",
      "Jesse Guessford"
    ]
  },
  "http://arxiv.org/abs/2311.15451": {
    "title": "Uncertainty-aware Language Modeling for Selective Question Answering",
    "volume": "Nov",
    "abstract": "We present an automatic large language model (LLM) conversion approach that produces uncertainty-aware LLMs capable of estimating uncertainty with every prediction. Our approach is model- and data-agnostic, is computationally-efficient, and does not rely on external models or systems. We evaluate converted models on the selective question answering setting -- to answer as many questions as possible while maintaining a given accuracy, forgoing providing predictions when necessary. As part of our results, we test BERT and Llama 2 model variants on the SQuAD extractive QA task and the TruthfulQA generative QA task. We show that using the uncertainty estimates provided by our approach to selectively answer questions leads to significantly higher accuracy over directly using model probabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Yang",
      "Shreya Ravikumar",
      "Fynn Schmitt-Ulms",
      "Satvik Lolla",
      "Ege Demir",
      "Iaroslav Elistratov",
      "Alex Lavaee",
      "Sadhana Lolla",
      "Elaheh Ahmadi",
      "Daniela Rus",
      "Alexander Amini",
      "Alejandro Perez"
    ]
  },
  "http://arxiv.org/abs/2311.15436": {
    "title": "Learning to Skip for Language Modeling",
    "volume": "Nov",
    "abstract": "Overparameterized large-scale language models have impressive generalization performance of in-context few-shot learning. However, most language models allocate the same amount of parameters or computation to each token, disregarding the complexity or importance of the input data. We argue that in language model pretraining, a variable amount of computation should be assigned to different tokens, and this can be efficiently achieved via a simple routing mechanism. Different from conventional early stopping techniques where tokens can early exit at only early layers, we propose a more general method that dynamically skips the execution of a layer (or module) for any input token with a binary router. In our extensive evaluation across 24 NLP tasks, we demonstrate that the proposed method can significantly improve the 1-shot performance compared to other competitive baselines only at mild extra cost for inference",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dewen Zeng",
      "Nan Du",
      "Tao Wang",
      "Yuanzhong Xu",
      "Tao Lei",
      "Zhifeng Chen",
      "Claire Cui"
    ]
  },
  "http://arxiv.org/abs/2311.15425": {
    "title": "Machine-Generated Text Detection using Deep Learning",
    "volume": "Nov",
    "abstract": "Our research focuses on the crucial challenge of discerning text produced by Large Language Models (LLMs) from human-generated text, which holds significance for various applications. With ongoing discussions about attaining a model with such functionality, we present supporting evidence regarding the feasibility of such models. We evaluated our models on multiple datasets, including Twitter Sentiment, Football Commentary, Project Gutenberg, PubMedQA, and SQuAD, confirming the efficacy of the enhanced detection approaches. These datasets were sampled with intricate constraints encompassing every possibility, laying the foundation for future research. We evaluate GPT-3.5-Turbo against various detectors such as SVM, RoBERTa-base, and RoBERTa-large. Based on the research findings, the results predominantly relied on the sequence length of the sentence",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raghav Gaggar",
      "Ashish Bhagchandani",
      "Harsh Oza"
    ]
  },
  "http://arxiv.org/abs/2311.15402": {
    "title": "Learning Section Weights for Multi-Label Document Classification",
    "volume": "Nov",
    "abstract": "Multi-label document classification is a traditional task in NLP. Compared to single-label classification, each document can be assigned multiple classes. This problem is crucially important in various domains, such as tagging scientific articles. Documents are often structured into several sections such as abstract and title. Current approaches treat different sections equally for multi-label classification. We argue that this is not a realistic assumption, leading to sub-optimal results. Instead, we propose a new method called Learning Section Weights (LSW), leveraging the contribution of each distinct section for multi-label classification. Via multiple feed-forward layers, LSW learns to assign weights to each section of, and incorporate the weights in the prediction. We demonstrate our approach on scientific articles. Experimental results on public (arXiv) and private (Elsevier) datasets confirm the superiority of LSW, compared to state-of-the-art multi-label document classification methods. In particular, LSW achieves a 1.3% improvement in terms of macro averaged F1-score while it achieves 1.3% in terms of macro averaged recall on the publicly available arXiv dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maziar Moradi Fard",
      "Paula Sorrolla Bayod",
      "Kiomars Motarjem",
      "Mohammad Alian Nejadi",
      "Saber Akhondi",
      "Camilo Thorne"
    ]
  },
  "http://arxiv.org/abs/2311.15316": {
    "title": "Enhancing Empathetic and Emotion Support Dialogue Generation with Prophetic Commonsense Inference",
    "volume": "Nov",
    "abstract": "The interest in Empathetic and Emotional Support conversations among the public has significantly increased. To offer more sensitive and understanding responses, leveraging commonsense knowledge has become a common strategy to better understand psychological aspects and causality. However, such commonsense inferences can be out of context and unable to predict upcoming dialogue themes, resulting in responses that lack coherence and empathy. To remedy this issue, we present Prophetic Commonsense Inference, an innovative paradigm for inferring commonsense knowledge. By harnessing the capabilities of Large Language Models in understanding dialogue and making commonsense deductions, we train tunable models to bridge the gap between past and potential future dialogues. Extensive experiments conducted on EmpatheticDialogues and Emotion Support Conversation show that equipping dialogue agents with our proposed prophetic commonsense inference significantly enhances the quality of their responses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lanrui Wang",
      "Jiangnan Li",
      "Chenxu Yang",
      "Zheng Lin",
      "Weiping Wang"
    ]
  },
  "http://arxiv.org/abs/2311.15296": {
    "title": "UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation",
    "volume": "Nov",
    "abstract": "Large language models (LLMs) have emerged as pivotal contributors in contemporary natural language processing and are increasingly being applied across a diverse range of industries. However, these large-scale probabilistic statistical models cannot currently ensure the requisite quality in professional content generation. These models often produce hallucinated text, compromising their practical utility in professional contexts. To assess the authentic reliability of LLMs in text generation, numerous initiatives have developed benchmark evaluations for hallucination phenomena. Nevertheless, these benchmarks frequently utilize constrained generation techniques due to cost and temporal constraints. These techniques encompass the use of directed hallucination induction and strategies that deliberately alter authentic text to produce hallucinations. These approaches are not congruent with the unrestricted text generation demanded by real-world applications. Furthermore, a well-established Chinese-language dataset dedicated to the evaluation of hallucinations in text generation is presently lacking. Consequently, we have developed an Unconstrained Hallucination Generation Evaluation (UHGEval) benchmark, designed to compile outputs produced with minimal restrictions by LLMs. Concurrently, we have established a comprehensive benchmark evaluation framework to aid subsequent researchers in undertaking scalable and reproducible experiments. We have also executed extensive experiments, evaluating prominent Chinese language models and the GPT series models to derive professional performance insights regarding hallucination challenges",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xun Liang",
      "Shichao Song",
      "Simin Niu",
      "Zhiyu Li",
      "Feiyu Xiong",
      "Bo Tang",
      "Zhaohui Wy",
      "Dawei He",
      "Peng Cheng",
      "Zhonghao Wang",
      "Haiying Deng"
    ]
  },
  "http://arxiv.org/abs/2311.15218": {
    "title": "Dataset for Stock Market Forecasting Based on Quantitative Analysis and Qualitative Data",
    "volume": "Nov",
    "abstract": "The application of Machine learning to finance has become a familiar approach, even more so in stock market forecasting. The stock market is highly volatile and huge amounts of data are generated every minute globally. The extraction of effective intelligence from this data is of critical importance. However, a collaboration of numerical stock data with qualitative text data can be a challenging task. In this work, we accomplish this and provide an unprecedented, publicly available dataset with technical and fundamental data, sentiment that we gathered from News Archives, TV news captions, Radio Transcripts, Tweets, Daily financial newspapers, etc. The text data entries used for sentiment extraction total more than 1.4 Million. The dataset comprises of daily entries from January 2018 to December 2022 for 8 different companies and Dow Jones Index as a whole. Holistic Fundamental and Technical data is provided training ready for Model learning and deployment. The predictive power of deep learning models is highly determined by the training data provided. This dataset would be of benefit for research globally incorporating qualitative intelligence for stock market forecasting. The dataset is made available at https://github.com/batking24/Huge-Stock-Dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sai Akash Bathini",
      "Dagli Cihan"
    ]
  },
  "http://arxiv.org/abs/2311.15211": {
    "title": "Probabilistic Transformer: A Probabilistic Dependency Model for Contextual Word Representation",
    "volume": "Nov",
    "abstract": "Syntactic structures used to play a vital role in natural language processing (NLP), but since the deep learning revolution, NLP has been gradually dominated by neural models that do not consider syntactic structures in their design. One vastly successful class of neural models is transformers. When used as an encoder, a transformer produces contextual representation of words in the input sentence. In this work, we propose a new model of contextual word representation, not from a neural perspective, but from a purely syntactic and probabilistic perspective. Specifically, we design a conditional random field that models discrete latent representations of all words in a sentence as well as dependency arcs between them; and we use mean field variational inference for approximate inference. Strikingly, we find that the computation graph of our model resembles transformers, with correspondences between dependencies and self-attention and between distributions over latent representations and contextual embeddings of words. Experiments show that our model performs competitively to transformers on small to medium sized datasets. We hope that our work could help bridge the gap between traditional syntactic and probabilistic approaches and cutting-edge neural approaches to NLP, and inspire more linguistically-principled neural approaches in the future",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyi Wu",
      "Kewei Tu"
    ]
  },
  "http://arxiv.org/abs/2311.15208": {
    "title": "LongStory: Coherent, Complete and Length Controlled Long story Generation",
    "volume": "Nov",
    "abstract": "A human author can write any length of story without losing coherence. Also, they always bring the story to a proper ending, an ability that current language models lack. In this work, we present the LongStory for coherent, complete, and length-controlled long story generation. LongStory introduces two novel methodologies: (1) the long and short-term contexts weight calibrator (CWC) and (2) long story structural positions (LSP). The CWC adjusts weights for long-term context Memory and short-term context Cheating, acknowledging their distinct roles. The LSP employs discourse tokens to convey the structural positions of a long story. Trained on three datasets with varied average story lengths, LongStory outperforms other baselines, including the strong story generator Plotmachine, in coherence, completeness, relevance, and repetitiveness. We also perform zero-shot tests on each dataset to assess the model's ability to predict outcomes beyond its training data and validate our methodology by comparing its performance with variants of our model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyeongman Park",
      "Nakyeong Yang",
      "Kyomin Jung"
    ]
  },
  "http://arxiv.org/abs/2311.15198": {
    "title": "ChatGPT and Beyond: The Generative AI Revolution in Education",
    "volume": "Nov",
    "abstract": "The wide adoption and usage of generative artificial intelligence (AI) models, particularly ChatGPT, has sparked a surge in research exploring their potential applications in the educational landscape. This survey examines academic literature published between November, 2022, and July, 2023, specifically targeting high-impact research from Scopus-indexed Q1 and Q2 journals. This survey delves into the practical applications and implications of generative AI models across a diverse range of educational contexts. Through a comprehensive and rigorous evaluation of recent academic literature, this survey seeks to illuminate the evolving role of generative AI models, particularly ChatGPT, in education. By shedding light on the potential benefits, challenges, and emerging trends in this dynamic field, the survey endeavors to contribute to the understanding of the nexus between artificial intelligence and education. The findings of this review will empower educators, researchers, and policymakers to make informed decisions about the integration of AI technologies into learning environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad AL-Smadi"
    ]
  },
  "http://arxiv.org/abs/2311.15180": {
    "title": "Benchmarking Large Language Model Volatility",
    "volume": "Nov",
    "abstract": "The impact of non-deterministic outputs from Large Language Models (LLMs) is not well examined for financial text understanding tasks. Through a compelling case study on investing in the US equity market via news sentiment analysis, we uncover substantial variability in sentence-level sentiment classification results, underscoring the innate volatility of LLM outputs. These uncertainties cascade downstream, leading to more significant variations in portfolio construction and return. While tweaking the temperature parameter in the language model decoder presents a potential remedy, it comes at the expense of stifled creativity. Similarly, while ensembling multiple outputs mitigates the effect of volatile outputs, it demands a notable computational investment. This work furnishes practitioners with invaluable insights for adeptly navigating uncertainty in the integration of LLMs into financial decision-making, particularly in scenarios dictated by non-deterministic information",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyang Yu"
    ]
  },
  "http://arxiv.org/abs/2311.15131": {
    "title": "Localizing Lying in Llama: Understanding Instructed Dishonesty on True-False Questions Through Prompting, Probing, and Patching",
    "volume": "Nov",
    "abstract": "Large language models (LLMs) demonstrate significant knowledge through their outputs, though it is often unclear whether false outputs are due to a lack of knowledge or dishonesty. In this paper, we investigate instructed dishonesty, wherein we explicitly prompt LLaMA-2-70b-chat to lie. We perform prompt engineering to find which prompts best induce lying behavior, and then use mechanistic interpretability approaches to localize where in the network this behavior occurs. Using linear probing and activation patching, we localize five layers that appear especially important for lying. We then find just 46 attention heads within these layers that enable us to causally intervene such that the lying model instead answers honestly. We show that these interventions work robustly across many prompts and dataset splits. Overall, our work contributes a greater understanding of dishonesty in LLMs so that we may hope to prevent it",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Campbell",
      "Richard Ren",
      "Phillip Guo"
    ]
  },
  "http://arxiv.org/abs/2311.15110": {
    "title": "Relevance feedback strategies for recall-oriented neural information retrieval",
    "volume": "Nov",
    "abstract": "In a number of information retrieval applications (e.g., patent search, literature review, due diligence, etc.), preventing false negatives is more important than preventing false positives. However, approaches designed to reduce review effort (like \"technology assisted review\") can create false negatives, since they are often based on active learning systems that exclude documents automatically based on user feedback. Therefore, this research proposes a more recall-oriented approach to reducing review effort. More specifically, through iteratively re-ranking the relevance rankings based on user feedback, which is also referred to as relevance feedback. In our proposed method, the relevance rankings are produced by a BERT-based dense-vector search and the relevance feedback is based on cumulatively summing the queried and selected embeddings. Our results show that this method can reduce review effort between 17.85% and 59.04%, compared to a baseline approach (of no feedback), given a fixed recall target",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Timo Kats",
      "Peter van der Putten",
      "Jan Scholtes"
    ]
  },
  "http://arxiv.org/abs/2311.15106": {
    "title": "Solving the Right Problem is Key for Translational NLP: A Case Study in UMLS Vocabulary Insertion",
    "volume": "Nov",
    "abstract": "As the immense opportunities enabled by large language models become more apparent, NLP systems will be increasingly expected to excel in real-world settings. However, in many instances, powerful models alone will not yield translational NLP solutions, especially if the formulated problem is not well aligned with the real-world task. In this work, we study the case of UMLS vocabulary insertion, an important real-world task in which hundreds of thousands of new terms, referred to as atoms, are added to the UMLS, one of the most comprehensive open-source biomedical knowledge bases. Previous work aimed to develop an automated NLP system to make this time-consuming, costly, and error-prone task more efficient. Nevertheless, practical progress in this direction has been difficult to achieve due to a problem formulation and evaluation gap between research output and the real-world task. In order to address this gap, we introduce a new formulation for UMLS vocabulary insertion which mirrors the real-world task, datasets which faithfully represent it and several strong baselines we developed through re-purposing existing solutions. Additionally, we propose an effective rule-enhanced biomedical language model which enables important new model behavior, outperforms all strong baselines and provides measurable qualitative improvements to editors who carry out the UVI task. We hope this case study provides insight into the considerable importance of problem formulation for the success of translational NLP solutions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bernal Jimenez Gutierrez",
      "Yuqing Mao",
      "Vinh Nguyen",
      "Kin Wah Fung",
      "Yu Su",
      "Olivier Bodenreider"
    ]
  },
  "http://arxiv.org/abs/2311.16185": {
    "title": "Enhancing Sentiment Analysis Results through Outlier Detection Optimization",
    "volume": "Nov",
    "abstract": "When dealing with text data containing subjective labels like speaker emotions, inaccuracies or discrepancies among labelers are not uncommon. Such discrepancies can significantly affect the performance of machine learning algorithms. This study investigates the potential of identifying and addressing outliers in text data with subjective labels, aiming to enhance classification outcomes. We utilized the Deep SVDD algorithm, a one-class classification method, to detect outliers in nine text-based emotion and sentiment analysis datasets. By employing both a small-sized language model (DistilBERT base model with 66 million parameters) and non-deep learning machine learning algorithms (decision tree, KNN, Logistic Regression, and LDA) as the classifier, our findings suggest that the removal of outliers can lead to enhanced results in most cases. Additionally, as outliers in such datasets are not necessarily unlearnable, we experienced utilizing a large language model -- DeBERTa v3 large with 131 million parameters, which can capture very complex patterns in data. We continued to observe performance enhancements across multiple datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuetian Chen",
      "Mei Si"
    ]
  },
  "http://arxiv.org/abs/2311.15077": {
    "title": "Multilingual self-supervised speech representations improve the speech recognition of low-resource African languages with codeswitching",
    "volume": "Nov",
    "abstract": "While many speakers of low-resource languages regularly code-switch between their languages and other regional languages or English, datasets of codeswitched speech are too small to train bespoke acoustic models from scratch or do language model rescoring. Here we propose finetuning self-supervised speech representations such as wav2vec 2.0 XLSR to recognize code-switched data. We find that finetuning self-supervised multilingual representations and augmenting them with n-gram language models trained from transcripts reduces absolute word error rates by up to 20% compared to baselines of hybrid models trained from scratch on code-switched data. Our findings suggest that in circumstances with limited training data finetuning self-supervised representations is a better performing and viable solution",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tolúlopé Ògúnrèmí",
      "Christopher D. Manning",
      "Dan Jurafsky"
    ]
  },
  "http://arxiv.org/abs/2311.15055": {
    "title": "Automatically Finding and Categorizing Replication Studies",
    "volume": "Nov",
    "abstract": "In many fields of experimental science, papers that failed to replicate continue to be cited as a result of the poor discoverability of replication studies. As a first step to creating a system that automatically finds replication studies for a given paper, 334 replication studies and 344 replicated studies were collected. Replication studies could be identified in the dataset based on text content at a higher rate than chance (AUROC = 0.886). Additionally, successful replication studies could be distinguished from failed replication studies at a higher rate than chance (AUROC = 0.664)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bob de Ruiter"
    ]
  },
  "http://arxiv.org/abs/2311.15054": {
    "title": "Detection of developmental language disorder in Cypriot Greek children using a machine learning neural network algorithm",
    "volume": "Nov",
    "abstract": "Children with developmental language disorder (DLD) encounter difficulties in acquiring various language structures. Early identification and intervention are crucial to prevent negative long-term outcomes impacting the academic, social, and emotional development of children. The study aims to develop an automated method for the identification of DLD using artificial intelligence, specifically a neural network machine learning algorithm. This protocol is applied for the first time in Cypriot Greek children, which is generally considered underresearched in the context of DLD. The neural network model was trained using perceptual and production data elicited from children with DLD and healthy controls. The k-fold technique was used to crossvalidate the algorithm. The performance of the model was evaluated using metrics such as accuracy, precision, recall, F1 score, and ROC/AUC curve to assess its ability to make accurate predictions on a set of unseen data. The results demonstrated high classification values for all metrics (between 0.92 and 0.98), indicating the high accuracy of the neural model in classifying children with DLD. Additionally, the variable importance analysis revealed that the language production skills of children had a more significant impact on the performance of the model compared to perception skills. Neural networks represent powerful tools for detecting DLD, providing early and quick assessments of the disorder, and having the potential to improve clinical outcomes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgios P. Georgiou",
      "Elena Theodorou"
    ]
  },
  "http://arxiv.org/abs/2311.15032": {
    "title": "nlpBDpatriots at BLP-2023 Task 2: A Transfer Learning Approach to Bangla Sentiment Analysis",
    "volume": "Nov",
    "abstract": "In this paper, we discuss the nlpBDpatriots entry to the shared task on Sentiment Analysis of Bangla Social Media Posts organized at the first workshop on Bangla Language Processing (BLP) co-located with EMNLP. The main objective of this task is to identify the polarity of social media content using a Bangla dataset annotated with positive, neutral, and negative labels provided by the shared task organizers. Our best system for this task is a transfer learning approach with data augmentation which achieved a micro F1 score of 0.71. Our best system ranked 12th among 30 teams that participated in the competition",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dhiman Goswami",
      "Md Nishat Raihan",
      "Sadiya Sayara Chowdhury Puspo",
      "Marcos Zampieri"
    ]
  },
  "http://arxiv.org/abs/2311.15029": {
    "title": "nlpBDpatriots at BLP-2023 Task 1: A Two-Step Classification for Violence Inciting Text Detection in Bangla",
    "volume": "Nov",
    "abstract": "In this paper, we discuss the nlpBDpatriots entry to the shared task on Violence Inciting Text Detection (VITD) organized as part of the first workshop on Bangla Language Processing (BLP) co-located with EMNLP. The aim of this task is to identify and classify the violent threats, that provoke further unlawful violent acts. Our best-performing approach for the task is two-step classification using back translation and multilinguality which ranked 6th out of 27 teams with a macro F1 score of 0.74",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Nishat Raihan",
      "Dhiman Goswami",
      "Sadiya Sayara Chowdhury Puspo",
      "Marcos Zampieri"
    ]
  },
  "http://arxiv.org/abs/2311.15023": {
    "title": "Offensive Language Identification in Transliterated and Code-Mixed Bangla",
    "volume": "Nov",
    "abstract": "Identifying offensive content in social media is vital for creating safe online communities. Several recent studies have addressed this problem by creating datasets for various languages. In this paper, we explore offensive language identification in texts with transliterations and code-mixing, linguistic phenomena common in multilingual societies, and a known challenge for NLP systems. We introduce TB-OLID, a transliterated Bangla offensive language dataset containing 5,000 manually annotated comments. We train and fine-tune machine learning models on TB-OLID, and we evaluate their results on this dataset. Our results show that English pre-trained transformer-based models, such as fBERT and HateBERT achieve the best performance on this dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Nishat Raihan",
      "Umma Hani Tanmoy",
      "Anika Binte Islam",
      "Kai North",
      "Tharindu Ranasinghe",
      "Antonios Anastasopoulos",
      "Marcos Zampieri"
    ]
  },
  "http://arxiv.org/abs/2311.15016": {
    "title": "E-CORE: Emotion Correlation Enhanced Empathetic Dialogue Generation",
    "volume": "Nov",
    "abstract": "Achieving empathy is a crucial step toward humanized dialogue systems. Current approaches for empathetic dialogue generation mainly perceive an emotional label to generate an empathetic response conditioned on it, which simply treat emotions independently, but ignore the intrinsic emotion correlation in dialogues, resulting in inaccurate emotion perception and unsuitable response generation. In this paper, we propose a novel emotion correlation enhanced empathetic dialogue generation framework, which comprehensively realizes emotion correlation learning, utilization, and supervising. Specifically, a multi-resolution emotion graph is devised to capture context-based emotion interactions from different resolutions, further modeling emotion correlation. Then we propose an emotion correlation enhanced decoder, with a novel correlation-aware aggregation and soft/hard strategy, respectively improving the emotion perception and response generation. Experimental results on the benchmark dataset demonstrate the superiority of our model in both empathetic perception and expression",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengyi Fu",
      "Lei Zhang",
      "Quan Wang",
      "Zhendong Mao"
    ]
  },
  "http://arxiv.org/abs/2311.14966": {
    "title": "Walking a Tightrope -- Evaluating Large Language Models in High-Risk Domains",
    "volume": "Nov",
    "abstract": "High-risk domains pose unique challenges that require language models to provide accurate and safe responses. Despite the great success of large language models (LLMs), such as ChatGPT and its variants, their performance in high-risk domains remains unclear. Our study delves into an in-depth analysis of the performance of instruction-tuned LLMs, focusing on factual accuracy and safety adherence. To comprehensively assess the capabilities of LLMs, we conduct experiments on six NLP datasets including question answering and summarization tasks within two high-risk domains: legal and medical. Further qualitative analysis highlights the existing limitations inherent in current LLMs when evaluating in high-risk domains. This underscores the essential nature of not only improving LLM capabilities but also prioritizing the refinement of domain-specific metrics, and embracing a more human-centric approach to enhance safety and factual reliability. Our findings advance the field toward the concerns of properly evaluating LLMs in high-risk domains, aiming to steer the adaptability of LLMs in fulfilling societal obligations and aligning with forthcoming regulations, such as the EU AI Act",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chia-Chien Hung",
      "Wiem Ben Rim",
      "Lindsay Frost",
      "Lars Bruckner",
      "Carolin Lawrence"
    ]
  },
  "http://arxiv.org/abs/2311.14949": {
    "title": "Vector-Quantized Prompt Learning for Paraphrase Generation",
    "volume": "Nov",
    "abstract": "Deep generative modeling of natural languages has achieved many successes, such as producing fluent sentences and translating from one language into another. However, the development of generative modeling techniques for paraphrase generation still lags behind largely due to the challenges in addressing the complex conflicts between expression diversity and semantic preservation. This paper proposes to generate diverse and high-quality paraphrases by exploiting the pre-trained models with instance-dependent prompts. To learn generalizable prompts, we assume that the number of abstract transforming patterns of paraphrase generation (governed by prompts) is finite and usually not large. Therefore, we present vector-quantized prompts as the cues to control the generation of pre-trained models. Extensive experiments demonstrate that the proposed method achieves new state-of-art results on three benchmark datasets, including Quora, Wikianswers, and MSCOCO. We will release all the code upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Luo",
      "Yixin Liu",
      "Peidong Liu",
      "Xianggen Liu"
    ]
  },
  "http://arxiv.org/abs/2311.14919": {
    "title": "Faster Minimum Bayes Risk Decoding with Confidence-based Pruning",
    "volume": "Nov",
    "abstract": "Minimum Bayes risk (MBR) decoding outputs the hypothesis with the highest expected utility over the model distribution for some utility function. It has been shown to improve accuracy over beam search in conditional language generation problems and especially neural machine translation, in both human and automatic evaluations. However, the standard sampling-based algorithm for MBR is substantially more computationally expensive than beam search, requiring a large number of samples as well as a quadratic number of calls to the utility function, limiting its applicability. We describe an algorithm for MBR which gradually grows the number of samples used to estimate the utility while pruning hypotheses that are unlikely to have the highest utility according to confidence estimates obtained with bootstrap sampling. Our method requires fewer samples and drastically reduces the number of calls to the utility function compared to standard MBR while being statistically indistinguishable in terms of accuracy. We demonstrate the effectiveness of our approach in experiments on three language pairs, using chrF++ and COMET as utility/evaluation metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julius Cheng",
      "Andreas Vlachos"
    ]
  },
  "http://arxiv.org/abs/2311.14901": {
    "title": "Code Search Debiasing:Improve Search Results beyond Overall Ranking Performance",
    "volume": "Nov",
    "abstract": "Code search engine is an essential tool in software development. Many code search methods have sprung up, focusing on the overall ranking performance of code search. In this paper, we study code search from another perspective by analyzing the bias of code search models. Biased code search engines provide poor user experience, even though they show promising overall performance. Due to different development conventions (e.g., prefer long queries or abbreviations), some programmers will find the engine useful, while others may find it hard to get desirable search results. To mitigate biases, we develop a general debiasing framework that employs reranking to calibrate search results. It can be easily plugged into existing engines and handle new code search biases discovered in the future. Experiments show that our framework can effectively reduce biases. Meanwhile, the overall ranking performance of code search gets improved after debiasing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Zhang",
      "Hui Li",
      "Yanlin Wang",
      "Zhao Wei",
      "Yong Xiu",
      "Juhong Wang",
      "Rongong Ji"
    ]
  },
  "http://arxiv.org/abs/2311.14871": {
    "title": "Tracing Influence at Scale: A Contrastive Learning Approach to Linking Public Comments and Regulator Responses",
    "volume": "Nov",
    "abstract": "U.S. Federal Regulators receive over one million comment letters each year from businesses, interest groups, and members of the public, all advocating for changes to proposed regulations. These comments are believed to have wide-ranging impacts on public policy. However, measuring the impact of specific comments is challenging because regulators are required to respond to comments but they do not have to specify which comments they are addressing. In this paper, we propose a simple yet effective solution to this problem by using an iterative contrastive method to train a neural model aiming for matching text from public comments to responses written by regulators. We demonstrate that our proposal substantially outperforms a set of selected text-matching baselines on a human-annotated test set. Furthermore, it delivers performance comparable to the most advanced gigantic language model (i.e., GPT-4), and is more cost-effective when handling comments and regulator responses matching in larger scale",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linzi Xing",
      "Brad Hackinen",
      "Giuseppe Carenini"
    ]
  },
  "http://arxiv.org/abs/2311.14865": {
    "title": "Improving Cross-Domain Hate Speech Generalizability with Emotion Knowledge",
    "volume": "Nov",
    "abstract": "Reliable automatic hate speech (HS) detection systems must adapt to the in-flow of diverse new data to curtail hate speech. However, hate speech detection systems commonly lack generalizability in identifying hate speech dissimilar to data used in training, impeding their robustness in real-world deployments. In this work, we propose a hate speech generalization framework that leverages emotion knowledge in a multitask architecture to improve the generalizability of hate speech detection in a cross-domain setting. We investigate emotion corpora with varying emotion categorical scopes to determine the best corpus scope for supplying emotion knowledge to foster generalized hate speech detection. We further assess the relationship between using pretrained Transformers models adapted for hate speech and its effect on our emotion-enriched hate speech generalization model. We perform extensive experiments on six publicly available datasets sourced from different online domains and show that our emotion-enriched HS detection generalization method demonstrates consistent generalization improvement in cross-domain evaluation, increasing generalization performance up to 18.1% and average cross-domain performance up to 8.5%, according to the F1 measure",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shi Yin Hong",
      "Susan Gauch"
    ]
  },
  "http://arxiv.org/abs/2311.14838": {
    "title": "OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models",
    "volume": "Nov",
    "abstract": "Developing high quality machine translation systems is a labour intensive, challenging and confusing process for newcomers to the field. We present a pair of tools OpusCleaner and OpusTrainer that aim to simplify the process, reduce the amount of work and lower the entry barrier for newcomers. OpusCleaner is a data downloading, cleaning, and proprocessing toolkit. It is designed to allow researchers to quickly download, visualise and preprocess bilingual (or monolingual) data that comes from many different sources, each of them with different quality, issues, and unique filtering/preprocessing requirements. OpusTrainer is a data scheduling and data augmenting tool aimed at building large scale, robust machine translation systems and large language models. It features deterministic data mixing from many different sources, on-the-fly data augmentation and more. Using these tools, we showcase how we can use it to create high quality machine translation model robust to noisy user input; multilingual models and terminology aware models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikolay Bogoychev",
      "Jelmer van der Linde",
      "Graeme Nail",
      "Barry Haddow",
      "Jaume Zaragoza-Bernabeu",
      "Gema Ramírez-Sánchez",
      "Lukas Weymann",
      "Tudor Nicolae Mateiu",
      "Jindřich Helcl",
      "Mikko Aulamo"
    ]
  },
  "http://arxiv.org/abs/2311.14836": {
    "title": "Custom Data Augmentation for low resource ASR using Bark and Retrieval-Based Voice Conversion",
    "volume": "Nov",
    "abstract": "This paper proposes two innovative methodologies to construct customized Common Voice datasets for low-resource languages like Hindi. The first methodology leverages Bark, a transformer-based text-to-audio model developed by Suno, and incorporates Meta's enCodec and a pre-trained HuBert model to enhance Bark's performance. The second methodology employs Retrieval-Based Voice Conversion (RVC) and uses the Ozen toolkit for data preparation. Both methodologies contribute to the advancement of ASR technology and offer valuable insights into addressing the challenges of constructing customized Common Voice datasets for under-resourced languages. Furthermore, they provide a pathway to achieving high-quality, personalized voice generation for a range of applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anand Kamble",
      "Aniket Tathe",
      "Suyash Kumbharkar",
      "Atharva Bhandare",
      "Anirban C. Mitra"
    ]
  },
  "http://arxiv.org/abs/2311.14835": {
    "title": "Weak Alignment Supervision from Hybrid Model Improves End-to-end ASR",
    "volume": "Nov",
    "abstract": "In this paper, we aim to create weak alignment supervision to aid the end-to-end modeling. Towards this end, we use the existing hybrid ASR system to produce triphone alignments of the training audios. We then create a cross-entropy loss at a certain layer of the encoder using the derived alignments. In contrast to the general one-hot cross-entropy losses with or without loss weighting, here we use a cross-entropy loss with a label smoothing parameter to regularize the supervision. As a comparison, we also conduct the experiments with one-hot cross-entropy losses and CTC losses with loss weighting. The results show that placing the weak alignment supervision with the label smoothing parameter of 0.5 at the third encoder layer outperforms the other two approaches and leads to about 5% relative WER reduction on the TED-LIUM 2 dataset over the baseline. We see similar improvements when applying the method out-of-the-box on a Tagalog end-to-end ASR system",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jintao Jiang",
      "Yingbo Gao",
      "Zoltan Tuske"
    ]
  },
  "http://arxiv.org/abs/2311.14808": {
    "title": "Data-to-Text Bilingual Generation",
    "volume": "Nov",
    "abstract": "This document illustrates the use of pyrealb for generating two parallel texts (English and French) from a single source of data. The data selection and text organisation processes are shared between the two languages. only language dependent word and phrasing choices are distinct processes. The realized texts thus convey identical information in both languages without the risk of being lost in translation. This is especially important in cases where strict and simultaneous bilingualism is required. We first present the types of applications targeted by this approach and how the pyrealb English and French realizer can be used for achieving this goal in a natural way. We describe an object-oriented organization to ensure a convenient realization in both languages. To illustrate the process, different types of applications are then briefly sketched with links to the source code. A brief comparison of the text generation is given with the output of an instance of a GPT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guy Lapalme"
    ]
  },
  "http://arxiv.org/abs/2311.14788": {
    "title": "Evaluating Large Language Models through Gender and Racial Stereotypes",
    "volume": "Nov",
    "abstract": "Language Models have ushered a new age of AI gaining traction within the NLP community as well as amongst the general population. AI's ability to make predictions, generations and its applications in sensitive decision-making scenarios, makes it even more important to study these models for possible biases that may exist and that can be exaggerated. We conduct a quality comparative study and establish a framework to evaluate language models under the premise of two kinds of biases: gender and race, in a professional setting. We find out that while gender bias has reduced immensely in newer models, as compared to older ones, racial bias still exists",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ananya Malik"
    ]
  },
  "http://arxiv.org/abs/2311.14652": {
    "title": "One Pass Streaming Algorithm for Super Long Token Attention Approximation in Sublinear Space",
    "volume": "Nov",
    "abstract": "Deploying Large Language Models (LLMs) in streaming applications that involve long contexts, particularly for extended dialogues and text analysis, is of paramount importance but presents two significant challenges. Firstly, the memory consumption is substantial during the decoding phase due to the caching of Key and Value states (KV) of previous tokens. Secondly, attention computation is time-consuming with a time complexity of $O(n^2)$ for the generation of each token. In recent OpenAI DevDay (Nov 6, 2023), OpenAI released a new model that is able to support a 128K-long document, in our paper, we focus on the memory-efficient issue when context length $n$ is much greater than 128K ($n \\gg 2^d$). Considering a single-layer self-attention with Query, Key, and Value matrices $Q, K, V \\in \\mathbb{R}^{n \\times d}$, the polynomial method approximates the attention output $T \\in \\mathbb{R}^{n \\times d}$. It accomplishes this by constructing $U_1, U_2 \\in \\mathbb{R}^{n \\times t}$ to expedite attention ${\\sf Attn}(Q, K, V)$ computation within $n^{1+o(1)}$ time executions. Despite this, storing the Key and Value matrices $K, V \\in \\mathbb{R}^{n \\times d}$ still necessitates $O( n d)$ space, leading to significant memory usage. In response to these challenges, we introduce a new algorithm that only reads one pass of the data in streaming fashion. This method employs sublinear space $o(n)$ to store three sketch matrices, alleviating the need for exact $K, V$ storage. Notably, our algorithm exhibits exceptional memory-efficient performance with super-long tokens. As the token length $n$ increases, our error guarantee diminishes while the memory usage remains nearly constant. This unique attribute underscores the potential of our technique in efficiently handling LLMs in streaming applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raghav Addanki",
      "Chenyang Li",
      "Zhao Song",
      "Chiwun Yang"
    ]
  },
  "http://arxiv.org/abs/2311.14648": {
    "title": "Calibrated Language Models Must Hallucinate",
    "volume": "Nov",
    "abstract": "Recent language models have a mysterious tendency to generate false but plausible-sounding text. Such \"hallucinations\" are an obstacle to the usability of language-based AI systems and can harm people who rely upon their outputs. This work shows shows that there is an inherent statistical reason that pretrained language models hallucinate certain types of facts, having nothing to do with the transformer LM architecture or data quality. For \"arbitrary\" facts whose veracity cannot be determined from the training data, we show that hallucination is necessary for language models that satisfy a statistical calibration condition appropriate for generative language models. Specifically, if the maximum probability of any fact is bounded, we show that the probability of generating a hallucination is close to the fraction of facts that occur exactly once in the training data (a \"Good-Turing\" estimate), even assuming ideal training data without errors. One conclusion is that models pretrained to be sufficiently good predictors (i.e., calibrated) may require post-training to mitigate hallucinations on the type of arbitrary facts that tend to appear once in the training set. However, our analysis also suggests that there is no statistical reason that pretraining will lead to hallucination on facts that tend to appear more than once in the training data (like references to publications such as articles and books, whose hallucinations have been particularly notable and problematic) or on systematic facts (like arithmetic calculations). Therefore, different architectures and learning algorithms may mitigate these latter types of hallucinations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Tauman Kalai",
      "Santosh S. Vempala"
    ]
  },
  "http://arxiv.org/abs/2311.14583": {
    "title": "GPT Struct Me: Probing GPT Models on Narrative Entity Extraction",
    "volume": "Nov",
    "abstract": "The importance of systems that can extract structured information from textual data becomes increasingly pronounced given the ever-increasing volume of text produced on a daily basis. Having a system that can effectively extract such information in an interoperable manner would be an asset for several domains, be it finance, health, or legal. Recent developments in natural language processing led to the production of powerful language models that can, to some degree, mimic human intelligence. Such effectiveness raises a pertinent question: Can these models be leveraged for the extraction of structured information? In this work, we address this question by evaluating the capabilities of two state-of-the-art language models -- GPT-3 and GPT-3.5, commonly known as ChatGPT -- in the extraction of narrative entities, namely events, participants, and temporal expressions. This study is conducted on the Text2Story Lusa dataset, a collection of 119 Portuguese news articles whose annotation framework includes a set of entity structures along with several tags and attribute values. We first select the best prompt template through an ablation study over prompt components that provide varying degrees of information on a subset of documents of the dataset. Subsequently, we use the best templates to evaluate the effectiveness of the models on the remaining documents. The results obtained indicate that GPT models are competitive with out-of-the-box baseline systems, presenting an all-in-one alternative for practitioners with limited resources. By studying the strengths and limitations of these models in the context of information extraction, we offer insights that can guide future improvements and avenues to explore in this field",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hugo Sousa",
      "Nuno Guimarães",
      "Alípio Jorge",
      "Ricardo Campos"
    ]
  },
  "http://arxiv.org/abs/2311.14543": {
    "title": "Data-Efficient Alignment of Large Language Models with Human Feedback Through Natural Language",
    "volume": "Nov",
    "abstract": "Learning from human feedback is a prominent technique to align the output of large language models (LLMs) with human expectations. Reinforcement learning from human feedback (RLHF) leverages human preference signals that are in the form of ranking of response pairs to perform this alignment. However, human preference on LLM outputs can come in much richer forms including natural language, which may provide detailed feedback on strengths and weaknesses of a given response. In this work we investigate data efficiency of modeling human feedback that is in natural language. Specifically, we fine-tune an open-source LLM, e.g., Falcon-40B-Instruct, on a relatively small amount (1000 records or even less) of human feedback in natural language in the form of critiques and revisions of responses. We show that this model is able to improve the quality of responses from even some of the strongest LLMs such as ChatGPT, BARD, and Vicuna, through critique and revision of those responses. For instance, through one iteration of revision of ChatGPT responses, the revised responses have 56.6% win rate over the original ones, and this win rate can be further improved to 65.9% after applying the revision for five iterations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Jin",
      "Shikib Mehri",
      "Devamanyu Hazarika",
      "Aishwarya Padmakumar",
      "Sungjin Lee",
      "Yang Liu",
      "Mahdi Namazifar"
    ]
  },
  "http://arxiv.org/abs/2311.14539": {
    "title": "CMed-GPT: Prompt Tuning for Entity-Aware Chinese Medical Dialogue Generation",
    "volume": "Nov",
    "abstract": "Medical dialogue generation relies on natural language generation techniques to enable online medical consultations. Recently, the widespread adoption of large-scale models in the field of natural language processing has facilitated rapid advancements in this technology. Existing medical dialogue models are mostly based on BERT and pre-trained on English corpora, but there is a lack of high-performing models on the task of Chinese medical dialogue generation. To solve the above problem, this paper proposes CMed-GPT, which is the GPT pre-training language model based on Chinese medical domain text. The model is available in two versions, namely, base and large, with corresponding perplexity values of 8.64 and 8.01. Additionally, we incorporate lexical and entity embeddings into the dialogue text in a uniform manner to meet the requirements of downstream dialogue generation tasks. By applying both fine-tuning and p-tuning to CMed-GPT, we lowered the PPL from 8.44 to 7.35. This study not only confirms the exceptional performance of the CMed-GPT model in generating Chinese biomedical text but also highlights the advantages of p-tuning over traditional fine-tuning with prefix prompts. Furthermore, we validate the significance of incorporating external information in medical dialogue generation, which enhances the quality of dialogue generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijie Qu",
      "Juan Li",
      "Zerui Ma",
      "Jianqiang Li"
    ]
  },
  "http://arxiv.org/abs/2311.14530": {
    "title": "Machine Translation for Ge'ez Language",
    "volume": "Nov",
    "abstract": "Machine translation (MT) for low-resource languages such as Ge'ez, an ancient language that is no longer spoken in daily life, faces challenges such as out-of-vocabulary words, domain mismatches, and lack of sufficient labeled training data. In this work, we explore various methods to improve Ge'ez MT, including transfer-learning from related languages, optimizing shared vocabulary and token segmentation approaches, finetuning large pre-trained models, and using large language models (LLMs) for few-shot translation with fuzzy matches. We develop a multilingual neural machine translation (MNMT) model based on languages relatedness, which brings an average performance improvement of about 4 BLEU compared to standard bilingual models. We also attempt to finetune the NLLB-200 model, one of the most advanced translation models available today, but find that it performs poorly with only 4k training samples for Ge'ez. Furthermore, we experiment with using GPT-3.5, a state-of-the-art LLM, for few-shot translation with fuzzy matches, which leverages embedding similarity-based retrieval to find context examples from a parallel corpus. We observe that GPT-3.5 achieves a remarkable BLEU score of 9.2 with no initial knowledge of Ge'ez, but still lower than the MNMT baseline of 15.2. Our work provides insights into the potential and limitations of different approaches for low-resource and ancient language MT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aman Kassahun Wassie"
    ]
  },
  "http://arxiv.org/abs/2311.14517": {
    "title": "tinyCLAP: Distilling Constrastive Language-Audio Pretrained Models",
    "volume": "Nov",
    "abstract": "Contrastive Language-Audio Pretraining (CLAP) became of crucial importance in the field of audio and speech processing. Its employment ranges from sound event detection to text-to-audio generation. However, one of the main limitations is the considerable amount of data required in the training process and the overall computational complexity during inference. This paper investigates how we can reduce the complexity of contrastive language-audio pre-trained models, yielding an efficient model that we call tinyCLAP. We derive an unimodal distillation loss from first principles and explore how the dimensionality of the shared, multimodal latent space can be reduced via pruning. TinyCLAP uses only 6% of the original Microsoft CLAP parameters with a minimal reduction (less than 5%) in zero-shot classification performance across the three sound event detection datasets on which it was tested",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Paissan",
      "Elisabetta Farella"
    ]
  },
  "http://arxiv.org/abs/2311.14505": {
    "title": "Analysing the Impact of Removing Infrequent Words on Topic Quality in LDA Models",
    "volume": "Nov",
    "abstract": "An initial procedure in text-as-data applications is text preprocessing. One of the typical steps, which can substantially facilitate computations, consists in removing infrequent words believed to provide limited information about the corpus. Despite popularity of vocabulary pruning, not many guidelines on how to implement it are available in the literature. The aim of the paper is to fill this gap by examining the effects of removing infrequent words for the quality of topics estimated using Latent Dirichlet Allocation. The analysis is based on Monte Carlo experiments taking into account different criteria for infrequent terms removal and various evaluation metrics. The results indicate that pruning is beneficial and that the share of vocabulary which might be eliminated can be quite considerable",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Victor Bystrov",
      "Viktoriia Naboka-Krell",
      "Anna Staszewska-Bystrova",
      "Peter Winker"
    ]
  },
  "http://arxiv.org/abs/2311.14495": {
    "title": "StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization",
    "volume": "Nov",
    "abstract": "In this paper, we investigate the long-term memory learning capabilities of state-space models (SSMs) from the perspective of parameterization. We prove that state-space models without any reparameterization exhibit a memory limitation similar to that of traditional RNNs: the target relationships that can be stably approximated by state-space models must have an exponential decaying memory. Our analysis identifies this \"curse of memory\" as a result of the recurrent weights converging to a stability boundary, suggesting that a reparameterization technique can be effective. To this end, we introduce a class of reparameterization techniques for SSMs that effectively lift its memory limitations. Besides improving approximation capabilities, we further illustrate that a principled choice of reparameterization scheme can also enhance optimization stability. We validate our findings using synthetic datasets and language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shida Wang",
      "Qianxiao Li"
    ]
  },
  "http://arxiv.org/abs/2311.14483": {
    "title": "SER_AMPEL: A multi-source dataset for SER of Italian older adults",
    "volume": "Nov",
    "abstract": "In this paper, SER_AMPEL, a multi-source dataset for speech emotion recognition (SER) is presented. The peculiarity of the dataset is that it is collected with the aim of providing a reference for speech emotion recognition in case of Italian older adults. The dataset is collected following different protocols, in particular considering acted conversations, extracted from movies and TV series, and recording natural conversations where the emotions are elicited by proper questions. The evidence of the need for such a dataset emerges from the analysis of the state of the art. Preliminary considerations on the critical issues of SER are reported analyzing the classification results on a subset of the proposed dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessandra Grossi",
      "Francesca Gasparini"
    ]
  },
  "http://arxiv.org/abs/2311.14479": {
    "title": "Controlled Text Generation via Language Model Arithmetic",
    "volume": "Nov",
    "abstract": "As Large Language Models (LLMs) are deployed more widely, customization with respect to vocabulary, style and character becomes more important. In this work we introduce model arithmetic, a novel inference framework for composing and biasing LLMs without the need for model (re)training or highly specific datasets. In addition, the framework allows for more precise control of generated text than direct prompting and prior controlled text generation (CTG) techniques. Using model arithmetic, we can express prior CTG techniques as simple formulas and naturally extend them to new and more effective formulations. Further, we show that speculative sampling, a technique for efficient LLM sampling, extends to our setting. This enables highly efficient text generation with multiple composed models with only marginal overhead over a single model. Our empirical evaluation demonstrates that model arithmetic allows fine-grained control of generated text while outperforming state-of-the-art on the task of toxicity reduction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jasper Dekoninck",
      "Marc Fischer",
      "Luca Beurer-Kellner",
      "Martin Vechev"
    ]
  },
  "http://arxiv.org/abs/2311.14465": {
    "title": "DP-NMT: Scalable Differentially-Private Machine Translation",
    "volume": "Nov",
    "abstract": "Neural machine translation (NMT) is a widely popular text generation task, yet there is a considerable research gap in the development of privacy-preserving NMT models, despite significant data privacy concerns for NMT systems. Differentially private stochastic gradient descent (DP-SGD) is a popular method for training machine learning models with concrete privacy guarantees; however, the implementation specifics of training a model with DP-SGD are not always clarified in existing models, with differing software libraries used and code bases not always being public, leading to reproducibility issues. To tackle this, we introduce DP-NMT, an open-source framework for carrying out research on privacy-preserving NMT with DP-SGD, bringing together numerous models, datasets, and evaluation metrics in one systematic software package. Our goal is to provide a platform for researchers to advance the development of privacy-preserving NMT systems, keeping the specific details of the DP-SGD algorithm transparent and intuitive to implement. We run a set of experiments on datasets from both general and privacy-related domains to demonstrate our framework in use. We make our framework publicly available and welcome feedback from the community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Timour Igamberdiev",
      "Doan Nam Long Vu",
      "Felix Künnecke",
      "Zhuo Yu",
      "Jannik Holmer",
      "Ivan Habernal"
    ]
  },
  "http://arxiv.org/abs/2311.14455": {
    "title": "Universal Jailbreak Backdoors from Poisoned Human Feedback",
    "volume": "Nov",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF training data to embed a \"jailbreak backdoor\" into the model. The backdoor embeds a trigger word into the model that acts like a universal \"sudo command\": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak backdoors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Javier Rando",
      "Florian Tramèr"
    ]
  },
  "http://arxiv.org/abs/2311.14391": {
    "title": "ÚFAL CorPipe at CRAC 2023: Larger Context Improves Multilingual Coreference Resolution",
    "volume": "Nov",
    "abstract": "We present CorPipe, the winning entry to the CRAC 2023 Shared Task on Multilingual Coreference Resolution. Our system is an improved version of our earlier multilingual coreference pipeline, and it surpasses other participants by a large margin of 4.5 percent points. CorPipe first performs mention detection, followed by coreference linking via an antecedent-maximization approach on the retrieved spans. Both tasks are trained jointly on all available corpora using a shared pretrained language model. Our main improvements comprise inputs larger than 512 subwords and changing the mention decoding to support ensembling. The source code is available at https://github.com/ufal/crac2023-corpipe",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Milan Straka"
    ]
  },
  "http://arxiv.org/abs/2311.14353": {
    "title": "Average Token Delay: A Duration-aware Latency Metric for Simultaneous Translation",
    "volume": "Nov",
    "abstract": "Simultaneous translation is a task in which the translation begins before the end of an input speech segment. Its evaluation should be conducted based on latency in addition to quality, and for users, the smallest possible amount of latency is preferable. Most existing metrics measure latency based on the start timings of partial translations and ignore their duration. This means such metrics do not penalize the latency caused by long translation output, which delays the comprehension of users and subsequent translations. In this work, we propose a novel latency evaluation metric for simultaneous translation called \\emph{Average Token Delay} (ATD) that focuses on the duration of partial translations. We demonstrate its effectiveness through analyses simulating user-side latency based on Ear-Voice Span (EVS). In our experiment, ATD had the highest correlation with EVS among baseline latency metrics under most conditions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasumasa Kano",
      "Katsuhito Sudoh",
      "Satoshi Nakamura"
    ]
  },
  "http://arxiv.org/abs/2311.14324": {
    "title": "Large Language Models as Topological Structure Enhancers for Text-Attributed Graphs",
    "volume": "Nov",
    "abstract": "The latest advancements in large language models (LLMs) have revolutionized the field of natural language processing (NLP). Inspired by the success of LLMs in NLP tasks, some recent work has begun investigating the potential of applying LLMs in graph learning tasks. However, most of the existing work focuses on utilizing LLMs as powerful node feature augmenters, leaving employing LLMs to enhance graph topological structures an understudied problem. In this work, we explore how to leverage the information retrieval and text generation capabilities of LLMs to refine/enhance the topological structure of text-attributed graphs (TAGs) under the node classification setting. First, we propose using LLMs to help remove unreliable edges and add reliable ones in the TAG. Specifically, we first let the LLM output the semantic similarity between node attributes through delicate prompt designs, and then perform edge deletion and edge addition based on the similarity. Second, we propose using pseudo-labels generated by the LLM to improve graph topology, that is, we introduce the pseudo-label propagation as a regularization to guide the graph neural network (GNN) in learning proper edge weights. Finally, we incorporate the two aforementioned LLM-based methods for graph topological refinement into the process of GNN training, and perform extensive experiments on four real-world datasets. The experimental results demonstrate the effectiveness of LLM-based graph topology refinement (achieving a 0.15%--2.47% performance gain on public benchmarks)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengyin Sun",
      "Yuxiang Ren",
      "Chen Ma",
      "Xuecang Zhang"
    ]
  },
  "http://arxiv.org/abs/2311.14212": {
    "title": "Annotation Sensitivity: Training Data Collection Methods Affect Model Performance",
    "volume": "Nov",
    "abstract": "When training data are collected from human annotators, the design of the annotation instrument, the instructions given to annotators, the characteristics of the annotators, and their interactions can impact training data. This study demonstrates that design choices made when creating an annotation instrument also impact the models trained on the resulting annotations. We introduce the term annotation sensitivity to refer to the impact of annotation data collection methods on the annotations themselves and on downstream model performance and predictions. We collect annotations of hate speech and offensive language in five experimental conditions of an annotation instrument, randomly assigning annotators to conditions. We then fine-tune BERT models on each of the five resulting datasets and evaluate model performance on a holdout portion of each condition. We find considerable differences between the conditions for 1) the share of hate speech/offensive language annotations, 2) model performance, 3) model predictions, and 4) model learning curves. Our results emphasize the crucial role played by the annotation instrument which has received little attention in the machine learning literature. We call for additional research into how and why the instrument impacts the annotations to inform the development of best practices in instrument design",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christoph Kern",
      "Stephanie Eckman",
      "Jacob Beck",
      "Rob Chew",
      "Bolei Ma",
      "Frauke Kreuter"
    ]
  },
  "http://arxiv.org/abs/2311.14199": {
    "title": "A Systematic Review of Deep Learning-based Research on Radiology Report Generation",
    "volume": "Nov",
    "abstract": "Radiology report generation (RRG) aims to automatically generate free-text descriptions from clinical radiographs, e.g., chest X-Ray images. RRG plays an essential role in promoting clinical automation and presents significant help to provide practical assistance for inexperienced doctors and alleviate radiologists' workloads. Therefore, consider these meaningful potentials, research on RRG is experiencing explosive growth in the past half-decade, especially with the rapid development of deep learning approaches. Existing studies perform RRG from the perspective of enhancing different modalities, provide insights on optimizing the report generation process with elaborated features from both visual and textual information, and further facilitate RRG with the cross-modal interactions among them. In this paper, we present a comprehensive review of deep learning-based RRG from various perspectives. Specifically, we firstly cover pivotal RRG approaches based on the task-specific features of radiographs, reports, and the cross-modal relations between them, and then illustrate the benchmark datasets conventionally used for this task with evaluation metrics, subsequently analyze the performance of different approaches and finally offer our summary on the challenges and the trends in future directions. Overall, the goal of this paper is to serve as a tool for understanding existing literature and inspiring potential valuable research in the field of RRG",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang Liu",
      "Yuanhe Tian",
      "Yan Song"
    ]
  },
  "http://arxiv.org/abs/2311.14169": {
    "title": "Evaluating GPT-4's Vision Capabilities on Brazilian University Admission Exams",
    "volume": "Nov",
    "abstract": "Recent advancements in language models have showcased human-comparable performance in academic entrance exams. However, existing studies often overlook questions that require the integration of visual comprehension, thus compromising the full spectrum and complexity inherent in real-world scenarios. To address this gap, we present a comprehensive framework to evaluate language models on entrance exams, which incorporates both textual and visual elements. We evaluate the two most recent editions of Exame Nacional do Ensino M\\'edio (ENEM), the main standardized entrance examination adopted by Brazilian universities. Our study not only reaffirms the capabilities of GPT-4 as the state of the art for handling complex multidisciplinary questions, but also pioneers in offering a realistic assessment of multimodal language models on Portuguese examinations. One of the highlights is that text captions transcribing visual content outperform the direct use of images, suggesting that the vision model has room for improvement. Yet, despite improvements afforded by images or captions, mathematical questions remain a challenge for these state-of-the-art models. The code and data used on experiments are available at https://github.com/piresramon/gpt-4-enem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ramon Pires",
      "Thales Sales Almeida",
      "Hugo Abonizio",
      "Rodrigo Nogueira"
    ]
  },
  "http://arxiv.org/abs/2311.14126": {
    "title": "Towards Auditing Large Language Models: Improving Text-based Stereotype Detection",
    "volume": "Nov",
    "abstract": "Large Language Models (LLM) have made significant advances in the recent past becoming more mainstream in Artificial Intelligence (AI) enabled human-facing applications. However, LLMs often generate stereotypical output inherited from historical data, amplifying societal biases and raising ethical concerns. This work introduces i) the Multi-Grain Stereotype Dataset, which includes 52,751 instances of gender, race, profession and religion stereotypic text and ii) a novel stereotype classifier for English text. We design several experiments to rigorously test the proposed model trained on the novel dataset. Our experiments show that training the model in a multi-class setting can outperform the one-vs-all binary counterpart. Consistent feature importance signals from different eXplainable AI tools demonstrate that the new model exploits relevant text features. We utilise the newly created model to assess the stereotypic behaviour of the popular GPT family of models and observe the reduction of bias over time. In summary, our work establishes a robust and practical framework for auditing and evaluating the stereotypic bias in LLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wu Zekun",
      "Sahan Bulathwela",
      "Adriano Soares Koshiyama"
    ]
  },
  "http://arxiv.org/abs/2311.14115": {
    "title": "A density estimation perspective on learning from pairwise human preferences",
    "volume": "Nov",
    "abstract": "Learning from human feedback (LHF) -- and in particular learning from pairwise preferences -- has recently become a crucial ingredient in training large language models (LLMs), and has been the subject of much research. Most recent works frame it as a reinforcement learning problem, where a reward function is learned from pairwise preference data and the LLM is treated as a policy which is adapted to maximize the rewards, often under additional regularization constraints. We propose an alternative interpretation which centers on the generative process for pairwise preferences and treats LHF as a density estimation problem. We provide theoretical and empirical results showing that for a family of generative processes defined via preference behavior distribution equations, training a reward function on pairwise preferences effectively models an annotator's implicit preference distribution. Finally, we discuss and present findings on \"annotator misspecification\" -- failure cases where wrong modeling assumptions are made about annotator behavior, resulting in poorly-adapted models -- suggesting that approaches that learn from pairwise human preferences could have trouble learning from a population of annotators with diverse viewpoints",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Dumoulin",
      "Daniel D. Johnson",
      "Pablo Samuel Castro",
      "Hugo Larochelle",
      "Yann Dauphin"
    ]
  },
  "http://arxiv.org/abs/2311.14096": {
    "title": "Auditing and Mitigating Cultural Bias in LLMs",
    "volume": "Nov",
    "abstract": "Culture fundamentally shapes people's reasoning, behavior, and communication. Generative artificial intelligence (AI) technologies may cause a shift towards a dominant culture. As people increasingly use AI to expedite and even automate various professional and personal tasks, cultural values embedded in AI models may bias authentic expression. We audit large language models for cultural bias, comparing their responses to nationally representative survey data, and evaluate country-specific prompting as a mitigation strategy. We find that GPT-4, 3.5 and 3 exhibit cultural values resembling English-speaking and Protestant European countries. Our mitigation strategy reduces cultural bias in recent models but not for all countries/territories. To avoid cultural bias in generative AI, especially in high-stakes contexts, we suggest using culture matching and ongoing cultural audits",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Tao",
      "Olga Viberg",
      "Ryan S. Baker",
      "Rene F. Kizilcec"
    ]
  },
  "http://arxiv.org/abs/2311.14087": {
    "title": "Question Answering in Natural Language: the Special Case of Temporal Expressions",
    "volume": "Nov",
    "abstract": "Although general question answering has been well explored in recent years, temporal question answering is a task which has not received as much focus. Our work aims to leverage a popular approach used for general question answering, answer extraction, in order to find answers to temporal questions within a paragraph. To train our model, we propose a new dataset, inspired by SQuAD, specifically tailored to provide rich temporal information. We chose to adapt the corpus WikiWars, which contains several documents on history's greatest conflicts. Our evaluation shows that a deep learning model trained to perform pattern matching, often used in general question answering, can be adapted to temporal question answering, if we accept to ask questions whose answers must be directly present within a text",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Armand Stricker"
    ]
  },
  "http://arxiv.org/abs/2311.14076": {
    "title": "Searching for Snippets of Open-Domain Dialogue in Task-Oriented Dialogue Datasets",
    "volume": "Nov",
    "abstract": "Most existing dialogue corpora and models have been designed to fit into 2 predominant categories : task-oriented dialogues portray functional goals, such as making a restaurant reservation or booking a plane ticket, while chit-chat/open-domain dialogues focus on holding a socially engaging talk with a user. However, humans tend to seamlessly switch between modes and even use chitchat to enhance task-oriented conversations. To bridge this gap, new datasets have recently been created, blending both communication modes into conversation examples. The approaches used tend to rely on adding chit-chat snippets to pre-existing, human-generated task-oriented datasets. Given the tendencies observed in humans, we wonder however if the latter do not \\textit{already} hold chit-chat sequences. By using topic modeling and searching for topics which are most similar to a set of keywords related to social talk, we explore the training sets of Schema-Guided Dialogues and MultiWOZ. Our study shows that sequences related to social talk are indeed naturally present, motivating further research on ways chitchat is combined into task-oriented dialogues",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Armand Stricker",
      "Patrick Paroubek"
    ]
  },
  "http://arxiv.org/abs/2311.14067": {
    "title": "Enhancing Task-Oriented Dialogues with Chitchat: a Comparative Study Based on Lexical Diversity and Divergence",
    "volume": "Nov",
    "abstract": "As a recent development, task-oriented dialogues (TODs) have been enriched with chitchat in an effort to make dialogues more diverse and engaging. This enhancement is particularly valuable as TODs are often confined to narrow domains, making the mitigation of repetitive and predictable responses a significant challenge. This paper presents a comparative analysis of three chitchat enhancements, aiming to identify the most effective approach in terms of diversity. Additionally, we quantify the divergence between the added chitchat, the original task-oriented language, and chitchat typically found in chitchat datasets, highlighting the top 20 divergent keywords for each comparison. Our findings drive a discussion on future enhancements for augmenting TODs, emphasizing the importance of grounding dialogues beyond the task to achieve more diverse and natural exchanges",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Armand Stricker",
      "Patrick Paroubek"
    ]
  },
  "http://arxiv.org/abs/2311.14063": {
    "title": "Do VSR Models Generalize Beyond LRS3?",
    "volume": "Nov",
    "abstract": "The Lip Reading Sentences-3 (LRS3) benchmark has primarily been the focus of intense research in visual speech recognition (VSR) during the last few years. As a result, there is an increased risk of overfitting to its excessively used test set, which is only one hour duration. To alleviate this issue, we build a new VSR test set named WildVSR, by closely following the LRS3 dataset creation processes. We then evaluate and analyse the extent to which the current VSR models generalize to the new test data. We evaluate a broad range of publicly available VSR models and find significant drops in performance on our test set, compared to their corresponding LRS3 results. Our results suggest that the increase in word error rates is caused by the models inability to generalize to slightly harder and in the wild lip sequences than those found in the LRS3 test set. Our new test benchmark is made public in order to enable future research towards more robust VSR models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasser Abdelaziz Dahou Djilali",
      "Sanath Narayan",
      "Eustache Le Bihan",
      "Haithem Boussaid",
      "Ebtessam Almazrouei",
      "Merouane Debbah"
    ]
  },
  "http://arxiv.org/abs/2311.13987": {
    "title": "Jam-ALT: A Formatting-Aware Lyrics Transcription Benchmark",
    "volume": "Nov",
    "abstract": "Current automatic lyrics transcription (ALT) benchmarks focus exclusively on word content and ignore the finer nuances of written lyrics including formatting and punctuation, which leads to a potential misalignment with the creative products of musicians and songwriters as well as listeners' experiences. For example, line breaks are important in conveying information about rhythm, emotional emphasis, rhyme, and high-level structure. To address this issue, we introduce Jam-ALT, a new lyrics transcription benchmark based on the JamendoLyrics dataset. Our contribution is twofold. Firstly, a complete revision of the transcripts, geared specifically towards ALT evaluation by following a newly created annotation guide that unifies the music industry's guidelines, covering aspects such as punctuation, line breaks, spelling, background vocals, and non-word sounds. Secondly, a suite of evaluation metrics designed, unlike the traditional word error rate, to capture such phenomena. We hope that the proposed benchmark contributes to the ALT task, enabling more precise and reliable assessments of transcription systems and enhancing the user experience in lyrics applications such as subtitle renderings for live captioning or karaoke",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ondřej Cífka",
      "Constantinos Dimitriou",
      "Cheng-i Wang",
      "Hendrik Schreiber",
      "Luke Miner",
      "Fabian-Robert Stöter"
    ]
  },
  "http://arxiv.org/abs/2311.13982": {
    "title": "Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex Questions",
    "volume": "Nov",
    "abstract": "Large language models (LLMs) are capable of answering knowledge-intensive complex questions with chain-of-thought (CoT) reasoning. However, they tend to generate factually incorrect reasoning steps when the required knowledge is not available or up-to-date in models' parameters. Recent works turn to retrieving external knowledge to augment CoT reasoning. Despite being promising, these chain-based methods suffer from: 1) Negative retrieval. Unnecessary or incorrect retrieval may mislead the reasoning; 2) Limited sight. Lacking the ability to look backward or forward, a local error in one step will propagate along the chain. In this paper, we propose a novel approach: Probabilistic Tree-of-thought Reasoning (ProbTree). First, LLMs translate a complex question into a query tree, in which each non-root node denotes a sub-question of its parent node. Then, probabilistic reasoning is conducted over the tree, by solving questions from leaf to root considering the confidence of both question decomposing and answering. During reasoning, for leaf nodes, LLMs choose a more confident answer from Closed-book QA that employs parametric knowledge and Open-book QA that employs retrieved external knowledge, thus eliminating the negative retrieval problem. For non-leaf nodes, with the hierarchical structure, LLMs have broader sights and are able to globally reason with the information from child nodes, thus recovering from local errors. The experiments on three Complex QA datasets under the open-domain setting show that our approach outperforms SOTA methods significantly, demonstrating the effect of probabilistic tree-of-thought reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shulin Cao",
      "Jiajie Zhang",
      "Jiaxin Shi",
      "Xin Lv",
      "Zijun Yao",
      "Qi Tian",
      "Juanzi Li",
      "Lei Hou"
    ]
  },
  "http://arxiv.org/abs/2311.13957": {
    "title": "Efficient Trigger Word Insertion",
    "volume": "Nov",
    "abstract": "With the boom in the natural language processing (NLP) field these years, backdoor attacks pose immense threats against deep neural network models. However, previous works hardly consider the effect of the poisoning rate. In this paper, our main objective is to reduce the number of poisoned samples while still achieving a satisfactory Attack Success Rate (ASR) in text backdoor attacks. To accomplish this, we propose an efficient trigger word insertion strategy in terms of trigger word optimization and poisoned sample selection. Extensive experiments on different datasets and models demonstrate that our proposed method can significantly improve attack effectiveness in text classification tasks. Remarkably, our approach achieves an ASR of over 90% with only 10 poisoned samples in the dirty-label setting and requires merely 1.5% of the training data in the clean-label setting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yueqi Zeng",
      "Ziqiang Li",
      "Pengfei Xia",
      "Lei Liu",
      "Bin Li"
    ]
  },
  "http://arxiv.org/abs/2311.13951": {
    "title": "MLLM-Bench, Evaluating Multi-modal LLMs using GPT-4V",
    "volume": "Nov",
    "abstract": "In the pursuit of Artificial General Intelligence (AGI), the integration of vision in language models has marked a significant milestone. The advent of vision-language models (MLLMs) like GPT-4V have expanded AI applications, aligning with the multi-modal capabilities of the human brain. However, evaluating the efficacy of MLLMs poses a substantial challenge due to the subjective nature of tasks that lack definitive answers. Existing automatic evaluation methodologies on multi-modal large language models rely on objective queries that have standard answers, inadequately addressing the nuances of creative and associative multi-modal tasks. To address this, we introduce MLLM-Bench, an innovative benchmark inspired by Vicuna, spanning a diverse array of scenarios, including Perception, Understanding, Applying, Analyzing, Evaluating, and Creation along with the ethical consideration. MLLM-Bench is designed to reflect user experience more accurately and provide a more holistic assessment of model performance. Comparative evaluations indicate a significant performance gap between existing open-source models and GPT-4V. We posit that MLLM-Bench will catalyze progress in the open-source community towards developing user-centric vision-language models that meet a broad spectrum of real-world applications. See online leaderboard in \\url{https://mllm-bench.llmzoo.com}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wentao Ge",
      "Shunian Chen",
      "Guiming Chen",
      "Junying Chen",
      "Zhihong Chen",
      "Shuo Yan",
      "Chenghao Zhu",
      "Ziyue Lin",
      "Wenya Xie",
      "Xidong Wang",
      "Anningzhe Gao",
      "Zhiyi Zhang",
      "Jianquan Li",
      "Xiang Wan",
      "Benyou Wang"
    ]
  },
  "http://arxiv.org/abs/2311.13937": {
    "title": "Exploring Methods for Cross-lingual Text Style Transfer: The Case of Text Detoxification",
    "volume": "Nov",
    "abstract": "Text detoxification is the task of transferring the style of text from toxic to neutral. While here are approaches yielding promising results in monolingual setup, e.g., (Dale et al., 2021; Hallinan et al., 2022), cross-lingual transfer for this task remains a challenging open problem (Moskovskiy et al., 2022). In this work, we present a large-scale study of strategies for cross-lingual text detoxification -- given a parallel detoxification corpus for one language; the goal is to transfer detoxification ability to another language for which we do not have such a corpus. Moreover, we are the first to explore a new task where text translation and detoxification are performed simultaneously, providing several strong baselines for this task. Finally, we introduce new automatic detoxification evaluation metrics with higher correlations with human judgments than previous benchmarks. We assess the most promising approaches also with manual markup, determining the answer for the best strategy to transfer the knowledge of text detoxification between languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daryna Dementieva",
      "Daniil Moskovskiy",
      "David Dale",
      "Alexander Panchenko"
    ]
  },
  "http://arxiv.org/abs/2311.13921": {
    "title": "Some Like It Small: Czech Semantic Embedding Models for Industry Applications",
    "volume": "Nov",
    "abstract": "This article focuses on the development and evaluation of Small-sized Czech sentence embedding models. Small models are important components for real-time industry applications in resource-constrained environments. Given the limited availability of labeled Czech data, alternative approaches, including pre-training, knowledge distillation, and unsupervised contrastive fine-tuning, are investigated. Comprehensive intrinsic and extrinsic analyses are conducted, showcasing the competitive performance of our models compared to significantly larger counterparts, with approximately 8 times smaller size and 5 times faster speed than conventional Base-sized models. To promote cooperation and reproducibility, both the models and the evaluation pipeline are made publicly accessible. Ultimately, this article presents practical applications of the developed sentence embedding models in Seznam.cz, the Czech search engine. These models have effectively replaced previous counterparts, enhancing the overall search experience for instance, in organic search, featured snippets, and image search. This transition has yielded improved performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiří Bednář",
      "Jakub Náplava",
      "Petra Barančíková",
      "Ondřej Lisický"
    ]
  },
  "http://arxiv.org/abs/2311.13910": {
    "title": "Dialogue Quality and Emotion Annotations for Customer Support Conversations",
    "volume": "Nov",
    "abstract": "Task-oriented conversational datasets often lack topic variability and linguistic diversity. However, with the advent of Large Language Models (LLMs) pretrained on extensive, multilingual and diverse text data, these limitations seem overcome. Nevertheless, their generalisability to different languages and domains in dialogue applications remains uncertain without benchmarking datasets. This paper presents a holistic annotation approach for emotion and conversational quality in the context of bilingual customer support conversations. By performing annotations that take into consideration the complete instances that compose a conversation, one can form a broader perspective of the dialogue as a whole. Furthermore, it provides a unique and valuable resource for the development of text classification models. To this end, we present benchmarks for Emotion Recognition and Dialogue Quality Estimation and show that further research is needed to leverage these models in a production setting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "John Mendonça",
      "Patrícia Pereira",
      "Miguel Menezes",
      "Vera Cabarrão",
      "Ana C. Farinha",
      "Helena Moniz",
      "João Paulo Carvalho",
      "Alon Lavie",
      "Isabel Trancoso"
    ]
  },
  "http://arxiv.org/abs/2311.13892": {
    "title": "General Phrase Debiaser: Debiasing Masked Language Models at a Multi-Token Level",
    "volume": "Nov",
    "abstract": "The social biases and unwelcome stereotypes revealed by pretrained language models are becoming obstacles to their application. Compared to numerous debiasing methods targeting word level, there has been relatively less attention on biases present at phrase level, limiting the performance of debiasing in discipline domains. In this paper, we propose an automatic multi-token debiasing pipeline called \\textbf{General Phrase Debiaser}, which is capable of mitigating phrase-level biases in masked language models. Specifically, our method consists of a \\textit{phrase filter stage} that generates stereotypical phrases from Wikipedia pages as well as a \\textit{model debias stage} that can debias models at the multi-token level to tackle bias challenges on phrases. The latter searches for prompts that trigger model's bias, and then uses them for debiasing. State-of-the-art results on standard datasets and metrics show that our approach can significantly reduce gender biases on both career and multiple disciplines, across models with varying parameter sizes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingkang Shi",
      "Xiaodan Zhang",
      "Dehan Kong",
      "Yulei Wu",
      "Zongzhen Liu",
      "Honglei Lyu",
      "Longtao Huang"
    ]
  },
  "http://arxiv.org/abs/2311.13878": {
    "title": "Minimizing Factual Inconsistency and Hallucination in Large Language Models",
    "volume": "Nov",
    "abstract": "Large Language Models (LLMs) are widely used in critical fields such as healthcare, education, and finance due to their remarkable proficiency in various language-related tasks. However, LLMs are prone to generating factually incorrect responses or \"hallucinations,\" which can lead to a loss of credibility and trust among users. To address this issue, we propose a multi-stage framework that generates the rationale first, verifies and refines incorrect ones, and uses them as supporting references to generate the answer. The generated rationale enhances the transparency of the answer and our framework provides insights into how the model arrived at this answer, by using this rationale and the references to the context. In this paper, we demonstrate its effectiveness in improving the quality of responses to drug-related inquiries in the life sciences industry. Our framework improves traditional Retrieval Augmented Generation (RAG) by enabling OpenAI GPT-3.5-turbo to be 14-25% more faithful and 16-22% more accurate on two datasets. Furthermore, fine-tuning samples based on our framework improves the accuracy of smaller open-access LLMs by 33-42% and competes with RAG on commercial models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muneeswaran I",
      "Shreya Saxena",
      "Siva Prasad",
      "M V Sai Prakash",
      "Advaith Shankar",
      "Varun V",
      "Vishal Vaddina",
      "Saisubramaniam Gopalakrishnan"
    ]
  },
  "http://arxiv.org/abs/2311.13857": {
    "title": "Challenges of Large Language Models for Mental Health Counseling",
    "volume": "Nov",
    "abstract": "The global mental health crisis is looming with a rapid increase in mental disorders, limited resources, and the social stigma of seeking treatment. As the field of artificial intelligence (AI) has witnessed significant advancements in recent years, large language models (LLMs) capable of understanding and generating human-like text may be used in supporting or providing psychological counseling. However, the application of LLMs in the mental health domain raises concerns regarding the accuracy, effectiveness, and reliability of the information provided. This paper investigates the major challenges associated with the development of LLMs for psychological counseling, including model hallucination, interpretability, bias, privacy, and clinical effectiveness. We explore potential solutions to these challenges that are practical and applicable to the current paradigm of AI. From our experience in developing and deploying LLMs for mental health, AI holds a great promise for improving mental health care, if we can carefully navigate and overcome pitfalls of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neo Christopher Chung",
      "George Dyer",
      "Lennart Brocki"
    ]
  },
  "http://arxiv.org/abs/2311.13848": {
    "title": "Grammatical Error Correction via Mixed-Grained Weighted Training",
    "volume": "Nov",
    "abstract": "The task of Grammatical Error Correction (GEC) aims to automatically correct grammatical errors in natural texts. Almost all previous works treat annotated training data equally, but inherent discrepancies in data are neglected. In this paper, the inherent discrepancies are manifested in two aspects, namely, accuracy of data annotation and diversity of potential annotations. To this end, we propose MainGEC, which designs token-level and sentence-level training weights based on inherent discrepancies in accuracy and potential diversity of data annotation, respectively, and then conducts mixed-grained weighted training to improve the training effect for GEC. Empirical evaluation shows that whether in the Seq2Seq or Seq2Edit manner, MainGEC achieves consistent and significant performance improvements on two benchmark datasets, demonstrating the effectiveness and superiority of the mixed-grained weighted training. Further ablation experiments verify the effectiveness of designed weights of both granularities in MainGEC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Li",
      "Quan Wang",
      "Chiwei Zhu",
      "Zhendong Mao",
      "Yongdong Zhang"
    ]
  },
  "http://arxiv.org/abs/2311.13833": {
    "title": "Lego: Learning to Disentangle and Invert Concepts Beyond Object Appearance in Text-to-Image Diffusion Models",
    "volume": "Nov",
    "abstract": "Diffusion models have revolutionized generative content creation and text-to-image (T2I) diffusion models in particular have increased the creative freedom of users by allowing scene synthesis using natural language. T2I models excel at synthesizing concepts such as nouns, appearances, and styles. To enable customized content creation based on a few example images of a concept, methods such as Textual Inversion and DreamBooth invert the desired concept and enable synthesizing it in new scenes. However, inverting more general concepts that go beyond object appearance and style (adjectives and verbs) through natural language, remains a challenge. Two key characteristics of these concepts contribute to the limitations of current inversion methods. 1) Adjectives and verbs are entangled with nouns (subject) and can hinder appearance-based inversion methods, where the subject appearance leaks into the concept embedding and 2) describing such concepts often extends beyond single word embeddings (being frozen in ice, walking on a tightrope, etc.) that current methods do not handle. In this study, we introduce Lego, a textual inversion method designed to invert subject entangled concepts from a few example images. Lego disentangles concepts from their associated subjects using a simple yet effective Subject Separation step and employs a Context Loss that guides the inversion of single/multi-embedding concepts. In a thorough user study, Lego-generated concepts were preferred over 70% of the time when compared to the baseline. Additionally, visual question answering using a large language model suggested Lego-generated concepts are better aligned with the text description of the concept",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saman Motamed",
      "Danda Pani Paudel",
      "Luc Van Gool"
    ]
  },
  "http://arxiv.org/abs/2311.13806": {
    "title": "AdaTyper: Adaptive Semantic Column Type Detection",
    "volume": "Nov",
    "abstract": "Understanding the semantics of relational tables is instrumental for automation in data exploration and preparation systems. A key source for understanding a table is the semantics of its columns. With the rise of deep learning, learned table representations are now available, which can be applied for semantic type detection and achieve good performance on benchmarks. Nevertheless, we observe a gap between this performance and its applicability in practice. In this paper, we propose AdaTyper to address one of the most critical deployment challenges: adaptation. AdaTyper uses weak-supervision to adapt a hybrid type predictor towards new semantic types and shifted data distributions at inference time, using minimal human feedback. The hybrid type predictor of AdaTyper combines rule-based methods and a light machine learning model for semantic column type detection. We evaluate the adaptation performance of AdaTyper on real-world database tables hand-annotated with semantic column types through crowdsourcing and find that the f1-score improves for new and existing types. AdaTyper approaches an average precision of 0.6 after only seeing 5 examples, significantly outperforming existing adaptation methods based on human-provided regular expressions or dictionaries",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Madelon Hulsebos",
      "Paul Groth",
      "Çağatay Demiralp"
    ]
  },
  "http://arxiv.org/abs/2311.13784": {
    "title": "DaG LLM ver 1.0: Pioneering Instruction-Tuned Language Modeling for Korean NLP",
    "volume": "Nov",
    "abstract": "This paper presents the DaG LLM (David and Goliath Large Language Model), a language model specialized for Korean and fine-tuned through Instruction Tuning across 41 tasks within 13 distinct categories",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongjun Jang",
      "Sangah Lee",
      "Sungjoo Byun",
      "Jinwoong Kim",
      "Jean Seo",
      "Minseok Kim",
      "Soyeon Kim",
      "Chaeyoung Oh",
      "Jaeyoon Kim",
      "Hyemi Jo",
      "Hyopil Shin"
    ]
  },
  "http://arxiv.org/abs/2311.13755": {
    "title": "Transformer-based Named Entity Recognition in Construction Supply Chain Risk Management in Australia",
    "volume": "Nov",
    "abstract": "The construction industry in Australia is characterized by its intricate supply chains and vulnerability to myriad risks. As such, effective supply chain risk management (SCRM) becomes imperative. This paper employs different transformer models, and train for Named Entity Recognition (NER) in the context of Australian construction SCRM. Utilizing NER, transformer models identify and classify specific risk-associated entities in news articles, offering a detailed insight into supply chain vulnerabilities. By analysing news articles through different transformer models, we can extract relevant entities and insights related to specific risk taxonomies local (milieu) to the Australian construction landscape. This research emphasises the potential of NLP-driven solutions, like transformer models, in revolutionising SCRM for construction in geo-media specific contexts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Milad Baghalzadeh Shishehgarkhaneh",
      "Robert C. Moehler",
      "Yihai Fang",
      "Amer A. Hijazi",
      "Hamed Aboutorab"
    ]
  },
  "http://arxiv.org/abs/2311.13735": {
    "title": "Surpassing GPT-4 Medical Coding with a Two-Stage Approach",
    "volume": "Nov",
    "abstract": "Recent advances in large language models (LLMs) show potential for clinical applications, such as clinical decision support and trial recommendations. However, the GPT-4 LLM predicts an excessive number of ICD codes for medical coding tasks, leading to high recall but low precision. To tackle this challenge, we introduce LLM-codex, a two-stage approach to predict ICD codes that first generates evidence proposals using an LLM and then employs an LSTM-based verification stage. The LSTM learns from both the LLM's high recall and human expert's high precision, using a custom loss function. Our model is the only approach that simultaneously achieves state-of-the-art results in medical coding accuracy, accuracy on rare codes, and sentence-level evidence identification to support coding decisions without training on human-annotated evidence according to experiments on the MIMIC dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhichao Yang",
      "Sanjit Singh Batra",
      "Joel Stremmel",
      "Eran Halperin"
    ]
  },
  "http://arxiv.org/abs/2311.13729": {
    "title": "Comparison of pipeline, sequence-to-sequence, and GPT models for end-to-end relation extraction: experiments with the rare disease use-case",
    "volume": "Nov",
    "abstract": "End-to-end relation extraction (E2ERE) is an important and realistic application of natural language processing (NLP) in biomedicine. In this paper, we aim to compare three prevailing paradigms for E2ERE using a complex dataset focused on rare diseases involving discontinuous and nested entities. We use the RareDis information extraction dataset to evaluate three competing approaches (for E2ERE): NER $\\rightarrow$ RE pipelines, joint sequence to sequence models, and generative pre-trained transformer (GPT) models. We use comparable state-of-the-art models and best practices for each of these approaches and conduct error analyses to assess their failure modes. Our findings reveal that pipeline models are still the best, while sequence-to-sequence models are not far behind; GPT models with eight times as many parameters are worse than even sequence-to-sequence models and lose to pipeline models by over 10 F1 points. Partial matches and discontinuous entities caused many NER errors contributing to lower overall E2E performances. We also verify these findings on a second E2ERE dataset for chemical-protein interactions. Although generative LM-based methods are more suitable for zero-shot settings, when training data is available, our results show that it is better to work with more conventional models trained and tailored for E2ERE. More innovative methods are needed to marry the best of the both worlds from smaller encoder-decoder pipeline models and the larger GPT models to improve E2ERE. As of now, we see that well designed pipeline models offer substantial performance gains at a lower cost and carbon footprint for E2ERE. Our contribution is also the first to conduct E2ERE for the RareDis dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shashank Gupta",
      "Xuguang Ai",
      "Ramakanth Kavuluru"
    ]
  },
  "http://arxiv.org/abs/2311.13708": {
    "title": "Dynamic Analysis Method for Hidden Dangers in Substation Based on Knowledge Graph",
    "volume": "Nov",
    "abstract": "To address the challenge of identifying and understanding hidden dangers in substations from unstructured text data, a novel dynamic analysis method is proposed. This approach begins by analyzing and extracting data from the unstructured text related to hidden dangers. It then leverages a flexible, distributed data search engine built on Elastic-Search to handle this information. Following this, the hidden Markov model is employed to train the data within the engine. The Viterbi algorithm is integrated to decipher the hidden state sequences, facilitating the segmentation and labeling of entities related to hidden dangers. The final step involves using the Neo4j graph database to dynamically create a knowledge map that visualizes hidden dangers in the substation. This method's effectiveness is demonstrated through an example analysis using data from a specific substation's hidden dangers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiwei Li",
      "Xing Liu",
      "Wei Wang",
      "Lu Chen",
      "Sizhe Li",
      "Hui Fan"
    ]
  },
  "http://arxiv.org/abs/2311.13668": {
    "title": "MAIRA-1: A specialised large multimodal model for radiology report generation",
    "volume": "Nov",
    "abstract": "We present a radiology-specific multimodal model for the task for generating radiological reports from chest X-rays (CXRs). Our work builds on the idea that large language model(s) can be equipped with multimodal capabilities through alignment with pre-trained vision encoders. On natural images, this has been shown to allow multimodal models to gain image understanding and description capabilities. Our proposed model (MAIRA-1) leverages a CXR-specific image encoder in conjunction with a fine-tuned large language model based on Vicuna-7B, and text-based data augmentation, to produce reports with state-of-the-art quality. In particular, MAIRA-1 significantly improves on the radiologist-aligned RadCliQ metric and across all lexical metrics considered. Manual review of model outputs demonstrates promising fluency and accuracy of generated reports while uncovering failure modes not captured by existing evaluation practices. More information and resources can be found on the project website: https://aka.ms/maira",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephanie L. Hyland",
      "Shruthi Bannur",
      "Kenza Bouzid",
      "Daniel C. Castro",
      "Mercy Ranjit",
      "Anton Schwaighofer",
      "Fernando Pérez-García",
      "Valentina Salvatelli",
      "Shaury Srivastav",
      "Anja Thieme",
      "Noel Codella",
      "Matthew P. Lungren",
      "Maria Teodora Wetscherek",
      "Ozan Oktay",
      "Javier Alvarez-Valle"
    ]
  },
  "http://arxiv.org/abs/2311.13657": {
    "title": "Efficient Transformer Knowledge Distillation: A Performance Review",
    "volume": "Nov",
    "abstract": "As pretrained transformer language models continue to achieve state-of-the-art performance, the Natural Language Processing community has pushed for advances in model compression and efficient attention mechanisms to address high computational requirements and limited input sequence length. Despite these separate efforts, no investigation has been done into the intersection of these two fields. In this work, we provide an evaluation of model compression via knowledge distillation on efficient attention transformers. We provide cost-performance trade-offs for the compression of state-of-the-art efficient attention architectures and the gains made in performance in comparison to their full attention counterparts. Furthermore, we introduce a new long-context Named Entity Recognition dataset, GONERD, to train and test the performance of NER models on long sequences. We find that distilled efficient attention transformers can preserve a significant amount of original model performance, preserving up to 98.6% across short-context tasks (GLUE, SQUAD, CoNLL-2003), up to 94.6% across long-context Question-and-Answering tasks (HotpotQA, TriviaQA), and up to 98.8% on long-context Named Entity Recognition (GONERD), while decreasing inference times by up to 57.8%. We find that, for most models on most tasks, performing knowledge distillation is an effective method to yield high-performing efficient attention models with low costs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathan Brown",
      "Ashton Williamson",
      "Tahj Anderson",
      "Logan Lawrence"
    ]
  },
  "http://arxiv.org/abs/2311.13647": {
    "title": "Language Model Inversion",
    "volume": "Nov",
    "abstract": "Language models produce a distribution over the next token; can we use this information to recover the prompt tokens? We consider the problem of language model inversion and show that next-token probabilities contain a surprising amount of information about the preceding text. Often we can recover the text in cases where it is hidden from the user, motivating a method for recovering unknown prompts given only the model's current distribution output. We consider a variety of model access scenarios, and show how even without predictions for every token in the vocabulary we can recover the probability vector through search. On Llama-2 7b, our inversion method reconstructs prompts with a BLEU of $59$ and token-level F1 of $78$ and recovers $27\\%$ of prompts exactly. Code for reproducing all experiments is available at http://github.com/jxmorris12/vec2text",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "John X. Morris",
      "Wenting Zhao",
      "Justin T. Chiu",
      "Vitaly Shmatikov",
      "Alexander M. Rush"
    ]
  },
  "http://arxiv.org/abs/2311.13628": {
    "title": "Prompt Risk Control: A Rigorous Framework for Responsible Deployment of Large Language Models",
    "volume": "Nov",
    "abstract": "The recent explosion in the capabilities of large language models has led to a wave of interest in how best to prompt a model to perform a given task. While it may be tempting to simply choose a prompt based on average performance on a validation set, this can lead to a deployment where unexpectedly poor responses are generated, especially for the worst-off users. To mitigate this prospect, we propose Prompt Risk Control, a lightweight framework for selecting a prompt based on rigorous upper bounds on families of informative risk measures. We offer methods for producing bounds on a diverse set of metrics, including quantities that measure worst-case responses and disparities in generation quality across the population of users. In addition, we extend the underlying statistical bounding techniques to accommodate the possibility of distribution shifts in deployment. Experiments on applications such as open-ended chat, medical question summarization, and code generation highlight how such a framework can foster responsible deployment by reducing the risk of the worst outcomes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas P. Zollo",
      "Todd Morrill",
      "Zhun Deng",
      "Jake C. Snell",
      "Toniann Pitassi",
      "Richard Zemel"
    ]
  },
  "http://arxiv.org/abs/2311.13581": {
    "title": "PaSS: Parallel Speculative Sampling",
    "volume": "Nov",
    "abstract": "Scaling the size of language models to tens of billions of parameters has led to impressive performance on a wide range of tasks. At generation, these models are used auto-regressively, requiring a forward pass for each generated token, and thus reading the full set of parameters from memory. This memory access forms the primary bottleneck for generation and it worsens as the model size increases. Moreover, executing a forward pass for multiple tokens in parallel often takes nearly the same time as it does for just one token. These two observations lead to the development of speculative sampling, where a second smaller model is used to draft a few tokens, that are then validated or rejected using a single forward pass of the large model. Unfortunately, this method requires two models that share the same tokenizer and thus limits its adoption. As an alternative, we propose to use parallel decoding as a way to draft multiple tokens from a single model with no computational cost, nor the need for a second model. Our approach only requires an additional input token that marks the words that will be generated simultaneously. We show promising performance (up to $30\\%$ speed-up) while requiring only as few as $O(d_{emb})$ additional parameters",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giovanni Monea",
      "Armand Joulin",
      "Edouard Grave"
    ]
  },
  "http://arxiv.org/abs/2311.13565": {
    "title": "Drilling Down into the Discourse Structure with LLMs for Long Document Question Answering",
    "volume": "Nov",
    "abstract": "We address the task of evidence retrieval for long document question answering, which involves locating relevant paragraphs within a document to answer a question. We aim to assess the applicability of large language models (LLMs) in the task of zero-shot long document evidence retrieval, owing to their unprecedented performance across various NLP tasks. However, currently the LLMs can consume limited context lengths as input, thus providing document chunks as inputs might overlook the global context while missing out on capturing the inter-segment dependencies. Moreover, directly feeding the large input sets can incur significant computational costs, particularly when processing the entire document (and potentially incurring monetary expenses with enterprise APIs like OpenAI's GPT variants). To address these challenges, we propose a suite of techniques that exploit the discourse structure commonly found in documents. By utilizing this structure, we create a condensed representation of the document, enabling a more comprehensive understanding and analysis of relationships between different parts. We retain $99.6\\%$ of the best zero-shot approach's performance, while processing only $26\\%$ of the total tokens used by the best approach in the information seeking evidence retrieval setup. We also show how our approach can be combined with \\textit{self-ask} reasoning agent to achieve best zero-shot performance in complex multi-hop question answering, just $\\approx 4\\%$ short of zero-shot performance using gold evidence",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inderjeet Nair",
      "Shwetha Somasundaram",
      "Apoorv Saxena",
      "Koustava Goswami"
    ]
  },
  "http://arxiv.org/abs/2311.13534": {
    "title": "LM-Cocktail: Resilient Tuning of Language Models via Model Merging",
    "volume": "Nov",
    "abstract": "The pre-trained language models are continually fine-tuned to better support downstream applications. However, this operation may result in significant performance degeneration on general tasks beyond the targeted domain. To overcome this problem, we propose a novel method which enables the fine-tuned model to stay resilient in general perspectives. Our method is conducted in the form of model merging (namely LM-Cocktail), where the fine-tuned language model is merged with the pre-trained base model or the peer models from other domains through weighted average. Despite simplicity, LM-Cocktail is surprisingly effective: the resulted model is able to achieve a strong empirical performance in the whole scope of general tasks while preserving a superior capacity in its targeted domain. We conduct comprehensive experiments with LLama and BGE model on popular benchmarks, including FLAN, MMLU, MTEB, whose results validate the efficacy of our proposed method. The code and checkpoints are available at https://github.com/FlagOpen/FlagEmbedding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shitao Xiao",
      "Zheng Liu",
      "Peitian Zhang",
      "Xingrun Xing"
    ]
  },
  "http://arxiv.org/abs/2311.13495": {
    "title": "Current Topological and Machine Learning Applications for Bias Detection in Text",
    "volume": "Nov",
    "abstract": "Institutional bias can impact patient outcomes, educational attainment, and legal system navigation. Written records often reflect bias, and once bias is identified; it is possible to refer individuals for training to reduce bias. Many machine learning tools exist to explore text data and create predictive models that can search written records to identify real-time bias. However, few previous studies investigate large language model embeddings and geometric models of biased text data to understand geometry's impact on bias modeling accuracy. To overcome this issue, this study utilizes the RedditBias database to analyze textual biases. Four transformer models, including BERT and RoBERTa variants, were explored. Post-embedding, t-SNE allowed two-dimensional visualization of data. KNN classifiers differentiated bias types, with lower k-values proving more effective. Findings suggest BERT, particularly mini BERT, excels in bias classification, while multilingual models lag. The recommendation emphasizes refining monolingual models and exploring domain-specific biases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Colleen Farrelly",
      "Yashbir Singh",
      "Quincy A. Hathaway",
      "Gunnar Carlsson",
      "Ashok Choudhary",
      "Rahul Paul",
      "Gianfranco Doretto",
      "Yassine Himeur",
      "Shadi Atalls",
      "Wathiq Mansoor"
    ]
  },
  "http://arxiv.org/abs/2311.13475": {
    "title": "Machine Translation to Control Formality Features in the Target Language",
    "volume": "Nov",
    "abstract": "Formality plays a significant role in language communication, especially in low-resource languages such as Hindi, Japanese and Korean. These languages utilise formal and informal expressions to convey messages based on social contexts and relationships. When a language translation technique is used to translate from a source language that does not pertain the formality (e.g. English) to a target language that does, there is a missing information on formality that could be a challenge in producing an accurate outcome. This research explores how this issue should be resolved when machine learning methods are used to translate from English to languages with formality, using Hindi as the example data. This was done by training a bilingual model in a formality-controlled setting and comparing its performance with a pre-trained multilingual model in a similar setting. Since there are not a lot of training data with ground truth, automated annotation techniques were employed to increase the data size. The primary modeling approach involved leveraging transformer models, which have demonstrated effectiveness in various natural language processing tasks. We evaluate the official formality accuracy(ACC) by comparing the predicted masked tokens with the ground truth. This metric provides a quantitative measure of how well the translations align with the desired outputs. Our study showcases a versatile translation strategy that considers the nuances of formality in the target language, catering to diverse language communication needs and scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harshita Tyagi",
      "Prashasta Jung",
      "Hyowon Lee"
    ]
  },
  "http://arxiv.org/abs/2311.13472": {
    "title": "Complexity-Guided Curriculum Learning for Text Graphs",
    "volume": "Nov",
    "abstract": "Curriculum learning provides a systematic approach to training. It refines training progressively, tailors training to task requirements, and improves generalization through exposure to diverse examples. We present a curriculum learning approach that builds on existing knowledge about text and graph complexity formalisms for training with text graph data. The core part of our approach is a novel data scheduler, which employs \"spaced repetition\" and complexity formalisms to guide the training process. We demonstrate the effectiveness of the proposed approach on several text graph tasks and graph neural network architectures. The proposed model gains more and uses less data; consistently prefers text over graph complexity indices throughout training, while the best curricula derived from text and graph complexity indices are equally effective; and it learns transferable curricula across GNN models and datasets. In addition, we find that both node-level (local) and graph-level (global) graph complexity indices, as well as shallow and traditional text complexity indices play a crucial role in effective curriculum learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nidhi Vakil",
      "Hadi Amiri"
    ]
  },
  "http://arxiv.org/abs/2311.13455": {
    "title": "Generation of Explanations for Logic Reasoning",
    "volume": "Nov",
    "abstract": "This thesis delves into a fortiori arguments in deductive reasoning, underscoring their relevance in various domains such as law, philosophy, and artificial intelligence. The research is centred on employing GPT-3.5-turbo to automate the analysis of these arguments, with a focus on understanding intricate reasoning processes, generating clear and coherent explanations, and creating novel arguments. The methodology encompasses a series of tasks including detailed reasoning, interpretation, and the augmentation of a fortiori arguments. It involves meticulously identifying these arguments in diverse contexts, differentiating comparative elements, and categorizing them based on their logical structure. Extensive experiments reveals the challenges encountered by GPT-3.5-turbo in accurately detecting and classifying a fortiori arguments. Nevertheless, the model demonstrates a performance that rivals specialized models, particularly in extracting key components and interpreting underlying properties. The integration of external information into the model's processing significantly elevates the quality of the generated explanations. Additionally, the model exhibits a noteworthy capability in augmenting arguments, thus contributing to the enrichment of the data set. Despite facing certain limitations, this thesis makes significant contributions to the fields of artificial intelligence and logical reasoning. It introduces novel methodologies, establishes a rigorous evaluation framework, and provides deep insights that set the stage for future advancements in automated logical reasoning. The findings and methodologies presented herein not only underscore the potential of AI in complex reasoning tasks but also highlight areas for future research and development",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanyi Pu"
    ]
  },
  "http://arxiv.org/abs/2311.13350": {
    "title": "Fact-based Court Judgment Prediction",
    "volume": "Nov",
    "abstract": "This extended abstract extends the research presented in \"ILDC for CJPE: Indian Legal Documents Corpus for Court Judgment Prediction and Explanation\" \\cite{malik-etal-2021-ildc}, focusing on fact-based judgment prediction within the context of Indian legal documents. We introduce two distinct problem variations: one based solely on facts, and another combining facts with rulings from lower courts (RLC). Our research aims to enhance early-phase case outcome prediction, offering significant benefits to legal professionals and the general public. The results, however, indicated a performance decline compared to the original ILDC for CJPE study, even after implementing various weightage schemes in our DELSumm algorithm. Additionally, using only facts for legal judgment prediction with different transformer models yielded results inferior to the state-of-the-art outcomes reported in the \"ILDC for CJPE\" study",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shubham Kumar Nigam",
      "Aniket Deroy"
    ]
  },
  "http://arxiv.org/abs/2311.13314": {
    "title": "Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based Retrofitting",
    "volume": "Nov",
    "abstract": "Incorporating factual knowledge in knowledge graph is regarded as a promising approach for mitigating the hallucination of large language models (LLMs). Existing methods usually only use the user's input to query the knowledge graph, thus failing to address the factual hallucination generated by LLMs during its reasoning process. To address this problem, this paper proposes Knowledge Graph-based Retrofitting (KGR), a new framework that incorporates LLMs with KGs to mitigate factual hallucination during the reasoning process by retrofitting the initial draft responses of LLMs based on the factual knowledge stored in KGs. Specifically, KGR leverages LLMs to extract, select, validate, and retrofit factual statements within the model-generated responses, which enables an autonomous knowledge verifying and refining procedure without any additional manual efforts. Experiments show that KGR can significantly improve the performance of LLMs on factual QA benchmarks especially when involving complex reasoning processes, which demonstrates the necessity and effectiveness of KGR in mitigating hallucination and enhancing the reliability of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyan Guan",
      "Yanjiang Liu",
      "Hongyu Lin",
      "Yaojie Lu",
      "Ben He",
      "Xianpei Han",
      "Le Sun"
    ]
  },
  "http://arxiv.org/abs/2311.13307": {
    "title": "Rethinking Radiology Report Generation via Causal Reasoning and Counterfactual Augmentation",
    "volume": "Nov",
    "abstract": "Radiology Report Generation (RRG) draws attention as an interaction between vision and language fields. Previous works inherited the ideology of vision-to-language generation tasks,aiming to generate paragraphs with high consistency as reports. However, one unique characteristic of RRG, the independence between diseases, was neglected, leading to the injection of the spurious confounder, i.e., the disease co-occurrence. Unfortunately, this confounder confuses the process of report generation worse because of the biased RRG data distribution. In this paper, to rethink this issue thoroughly, we reason about its causes and effects from a novel perspective of statistics and causality, where the Joint Vision Coupling and the Conditional Sentence Coherence Coupling are two aspects prone to implicitly decrease the accuracy of reports. Then, a counterfactual augmentation strategy that contains the Counterfactual Sample Synthesis and the Counterfactual Report Reconstruction sub-methods is proposed to break these two aspects of spurious effects. Experimental results and further analyses on two widely used datasets justify our reasoning and proposed methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Song",
      "Jiafan Liu",
      "Yun Li",
      "Wenbin Lei",
      "Ruxin Wang"
    ]
  },
  "http://arxiv.org/abs/2311.13281": {
    "title": "Intention and Context Elicitation with Large Language Models in the Legal Aid Intake Process",
    "volume": "Nov",
    "abstract": "Large Language Models (LLMs) and chatbots show significant promise in streamlining the legal intake process. This advancement can greatly reduce the workload and costs for legal aid organizations, improving availability while making legal assistance more accessible to a broader audience. However, a key challenge with current LLMs is their tendency to overconfidently deliver an immediate 'best guess' to a client's question based on the output distribution learned over the training data. This approach often overlooks the client's actual intentions or the specifics of their legal situation. As a result, clients may not realize the importance of providing essential additional context or expressing their underlying intentions, which are crucial for their legal cases. Traditionally, logic based decision trees have been used to automate intake for specific access to justice issues, such as immigration and eviction. But those solutions lack scalability. We demonstrate a proof-of-concept using LLMs to elicit and infer clients' underlying intentions and specific legal circumstances through free-form, language-based interactions. We also propose future research directions to use supervised fine-tuning or offline reinforcement learning to automatically incorporate intention and context elicitation in chatbots without explicit prompting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nick Goodson",
      "Rongfei Lu"
    ]
  },
  "http://arxiv.org/abs/2311.13274": {
    "title": "Enhancing Summarization Performance through Transformer-Based Prompt Engineering in Automated Medical Reporting",
    "volume": "Nov",
    "abstract": "Customized medical prompts enable Large Language Models (LLM) to effectively address medical dialogue summarization. The process of medical reporting is often time-consuming for healthcare professionals. Implementing medical dialogue summarization techniques presents a viable solution to alleviate this time constraint by generating automated medical reports. The effectiveness of LLMs in this process is significantly influenced by the formulation of the prompt, which plays a crucial role in determining the quality and relevance of the generated reports. In this research, we used a combination of two distinct prompting strategies, known as shot prompting and pattern prompting to enhance the performance of automated medical reporting. The evaluation of the automated medical reports is carried out using the ROUGE score and a human evaluation with the help of an expert panel. The two-shot prompting approach in combination with scope and domain context outperforms other methods and achieves the highest score when compared to the human reference set by a general practitioner. However, the automated reports are approximately twice as long as the human references, due to the addition of both redundant and relevant statements that are added to the report",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daphne van Zandvoort",
      "Laura Wiersema",
      "Tom Huibers",
      "Sandra van Dulmen",
      "Sjaak Brinkkemper"
    ]
  },
  "http://arxiv.org/abs/2311.13273": {
    "title": "Comparative Experimentation of Accuracy Metrics in Automated Medical Reporting: The Case of Otitis Consultations",
    "volume": "Nov",
    "abstract": "Generative Artificial Intelligence (AI) can be used to automatically generate medical reports based on transcripts of medical consultations. The aim is to reduce the administrative burden that healthcare professionals face. The accuracy of the generated reports needs to be established to ensure their correctness and usefulness. There are several metrics for measuring the accuracy of AI generated reports, but little work has been done towards the application of these metrics in medical reporting. A comparative experimentation of 10 accuracy metrics has been performed on AI generated medical reports against their corresponding General Practitioner's (GP) medical reports concerning Otitis consultations. The number of missing, incorrect, and additional statements of the generated reports have been correlated with the metric scores. In addition, we introduce and define a Composite Accuracy Score which produces a single score for comparing the metrics within the field of automated medical reporting. Findings show that based on the correlation study and the Composite Accuracy Score, the ROUGE-L and Word Mover's Distance metrics are the preferred metrics, which is not in line with previous work. These findings help determine the accuracy of an AI generated medical report, which aids the development of systems that generate medical reports for GPs to reduce the administrative burden",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wouter Faber",
      "Renske Eline Bootsma",
      "Tom Huibers",
      "Sandra van Dulmen",
      "Sjaak Brinkkemper"
    ]
  },
  "http://arxiv.org/abs/2311.13258": {
    "title": "ViStruct: Visual Structural Knowledge Extraction via Curriculum Guided Code-Vision Representation",
    "volume": "Nov",
    "abstract": "State-of-the-art vision-language models (VLMs) still have limited performance in structural knowledge extraction, such as relations between objects. In this work, we present ViStruct, a training framework to learn VLMs for effective visual structural knowledge extraction. Two novel designs are incorporated. First, we propose to leverage the inherent structure of programming language to depict visual structural information. This approach enables explicit and consistent representation of visual structural information of multiple granularities, such as concepts, relations, and events, in a well-organized structured format. Second, we introduce curriculum-based learning for VLMs to progressively comprehend visual structures, from fundamental visual concepts to intricate event structures. Our intuition is that lower-level knowledge may contribute to complex visual structure understanding. Furthermore, we compile and release a collection of datasets tailored for visual structural knowledge extraction. We adopt a weakly-supervised approach to directly generate visual event structures from captions for ViStruct training, capitalizing on abundant image-caption pairs from the web. In experiments, we evaluate ViStruct on visual structure prediction tasks, demonstrating its effectiveness in improving the understanding of visual structures. The code is public at \\url{https://github.com/Yangyi-Chen/vi-struct}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangyi Chen",
      "Xingyao Wang",
      "Manling Li",
      "Derek Hoiem",
      "Heng Ji"
    ]
  },
  "http://arxiv.org/abs/2311.14741": {
    "title": "@ve: A Chatbot for Latin",
    "volume": "Nov",
    "abstract": "Dead, extinct, and endangered languages have been preserved primarily through audio conservation and the collection and digitization of scripts and have been promoted through targeted language acquisition efforts. Another possibility would be to build conversational agents that can master these languages. This would provide an artificial, active conversational partner which has knowledge of the vocabulary and grammar, and one learns with it in a different way. The chatbot @ve, with which one can communicate in Latin, was developed in 2022/2023 based on GPT-3.0. It was additionally equipped with a manually created knowledge base. After conceptual groundwork, this paper presents the preparation and implementation of the project. In addition, it summarizes the test that a Latin expert conducted with the chatbot. A critical discussion elaborates advantages and disadvantages. @ve could be a new tool for teaching Latin in a memorable and entertaining way through dialogue. However, the present implementation is still too prone to glitches for stand-alone use - i.e., without the accompaniment of a teacher. The use of GPT-4 could be a solution as well as the extension of the knowledge base. In conclusion, it can be argued that conversational agents are an innovative approach to promoting and preserving languages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oliver Bendel",
      "Karim N'diaye"
    ]
  },
  "http://arxiv.org/abs/2311.13246": {
    "title": "Automatic Instruction Optimization for Open-source LLM Instruction Tuning",
    "volume": "Nov",
    "abstract": "Instruction tuning is crucial for enabling Language Learning Models (LLMs) in responding to human instructions. The quality of instruction pairs used for tuning greatly affects the performance of LLMs. However, the manual creation of high-quality instruction datasets is costly, leading to the adoption of automatic generation of instruction pairs by LLMs as a popular alternative in the training of open-source LLMs. To ensure the high quality of LLM-generated instruction datasets, several approaches have been proposed. Nevertheless, existing methods either compromise dataset integrity by filtering a large proportion of samples, or are unsuitable for industrial applications. In this paper, instead of discarding low-quality samples, we propose CoachLM, a novel approach to enhance the quality of instruction datasets through automatic revisions on samples in the dataset. CoachLM is trained from the samples revised by human experts and significantly increases the proportion of high-quality samples in the dataset from 17.7% to 78.9%. The effectiveness of CoachLM is further assessed on various real-world instruction test sets. The results show that CoachLM improves the instruction-following capabilities of the instruction-tuned LLM by an average of 29.9%, which even surpasses larger LLMs with nearly twice the number of parameters. Furthermore, CoachLM is successfully deployed in a data management system for LLMs at Huawei, resulting in an efficiency improvement of up to 20% in the cleaning of 40k real-world instruction pairs. We release the training data and code of CoachLM (https://github.com/lunyiliu/CoachLM)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilun Liu",
      "Shimin Tao",
      "Xiaofeng Zhao",
      "Ming Zhu",
      "Wenbing Ma",
      "Junhao Zhu",
      "Chang Su",
      "Yutai Hou",
      "Miao Zhang",
      "Min Zhang",
      "Hongxia Ma",
      "Li Zhang",
      "Hao Yang",
      "Yanfei Jiang"
    ]
  },
  "http://arxiv.org/abs/2311.14740": {
    "title": "AutoKG: Efficient Automated Knowledge Graph Generation for Language Models",
    "volume": "Nov",
    "abstract": "Traditional methods of linking large language models (LLMs) to knowledge bases via the semantic similarity search often fall short of capturing complex relational dynamics. To address these limitations, we introduce AutoKG, a lightweight and efficient approach for automated knowledge graph (KG) construction. For a given knowledge base consisting of text blocks, AutoKG first extracts keywords using a LLM and then evaluates the relationship weight between each pair of keywords using graph Laplace learning. We employ a hybrid search scheme combining vector similarity and graph-based associations to enrich LLM responses. Preliminary experiments demonstrate that AutoKG offers a more comprehensive and interconnected knowledge retrieval mechanism compared to the semantic similarity search, thereby enhancing the capabilities of LLMs in generating more insightful and relevant outputs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bohan Chen",
      "Andrea L. Bertozzi"
    ]
  },
  "http://arxiv.org/abs/2311.13240": {
    "title": "On the Calibration of Large Language Models and Alignment",
    "volume": "Nov",
    "abstract": "As large language models attract increasing attention and find widespread application, concurrent challenges of reliability also arise at the same time. Confidence calibration, an effective analysis method for gauging the reliability of deep models, serves as a crucial tool for assessing and improving their reliability. However, such investigation has been comparatively underexplored. In this work, we conduct a systematic examination of the calibration of aligned language models throughout the entire construction process, including pretraining and alignment training. At each stage, we investigate how different training settings, such as parameter scales and training data, affect model calibration. To thoroughly assess model calibration, we evaluate models on three most concerned aspects: generation, factuality and understanding. Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chiwei Zhu",
      "Benfeng Xu",
      "Quan Wang",
      "Yongdong Zhang",
      "Zhendong Mao"
    ]
  },
  "http://arxiv.org/abs/2311.13230": {
    "title": "Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus",
    "volume": "Nov",
    "abstract": "Large Language Models (LLMs) have gained significant popularity for their impressive performance across diverse fields. However, LLMs are prone to hallucinate untruthful or nonsensical outputs that fail to meet user expectations in many real-world applications. Existing works for detecting hallucinations in LLMs either rely on external knowledge for reference retrieval or require sampling multiple responses from the LLM for consistency verification, making these methods costly and inefficient. In this paper, we propose a novel reference-free, uncertainty-based method for detecting hallucinations in LLMs. Our approach imitates human focus in factuality checking from three aspects: 1) focus on the most informative and important keywords in the given text; 2) focus on the unreliable tokens in historical context which may lead to a cascade of hallucinations; and 3) focus on the token properties such as token type and token frequency. Experimental results on relevant datasets demonstrate the effectiveness of our proposed method, which achieves state-of-the-art performance across all the evaluation metrics and eliminates the need for additional information",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhang Zhang",
      "Lin Qiu",
      "Qipeng Guo",
      "Cheng Deng",
      "Yue Zhang",
      "Zheng Zhang",
      "Chenghu Zhou",
      "Xinbing Wang",
      "Luoyi Fu"
    ]
  },
  "http://arxiv.org/abs/2311.13184": {
    "title": "AS-LLM: When Algorithm Selection Meets Large Language Model",
    "volume": "Nov",
    "abstract": "Algorithm selection aims to identify the most suitable algorithm for solving a specific problem before execution, which has become a critical process of the AutoML. Current mainstream algorithm selection techniques rely heavily on feature representations of various problems and employ the performance of each algorithm as supervised information. However, there is a significant research gap concerning the consideration of algorithm features. This gap is primarily attributed to the inherent complexity of algorithms, making it particularly challenging to find a universally effective feature extraction method that is applicable across a diverse range of algorithms. Unfortunately, neglecting this aspect undoubtedly impacts the accuracy of algorithm selection and indirectly necessitates an increased volume of problem data for training purposes. This paper takes a significant stride towards addressing this gap by proposing an approach that integrates algorithm representation into the algorithm selection process. Specifically, our proposed model employs distinct modules to extract representations of both problems and algorithms, where the algorithm representation leverages the capabilities of pre-trained LLMs in the realm of code comprehension. Following the extraction of embedding vectors for both algorithms and problems, the most suitable algorithm is determined through calculations of matching degrees. Our experiments not only validate the effectiveness of the proposed model but also showcase the performance of different embedded pre-trained LLMs, which suggests that the proposed algorithm selection framework holds the potential to serve as a baseline task for evaluating the code representation capabilities of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Wu",
      "Yan Zhong",
      "Jibin Wu",
      "Kay Chen Tan"
    ]
  },
  "http://arxiv.org/abs/2311.13171": {
    "title": "ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization",
    "volume": "Nov",
    "abstract": "Parameter-efficient fine-tuning (PEFT) techniques make it possible to efficiently adapt a language model to create \"expert\" models that specialize to new tasks or domains. Recent techniques in model merging and compositional generalization leverage these expert models by dynamically composing modules to improve zero/few-shot generalization. Despite the efficiency of PEFT methods, the size of expert models can make it onerous to retrieve expert models per query over high-latency networks like the Internet or serve multiple experts on a single GPU. To address these issues, we present ComPEFT, a novel method for compressing fine-tuning residuals (task vectors) of PEFT based models. ComPEFT employs sparsification and ternary quantization to reduce the size of the PEFT module without performing any additional retraining while preserving or enhancing model performance. In extensive evaluation across T5, T0, and LLaMA-based models with 200M - 65B parameters, ComPEFT achieves compression ratios of 8x - 50x. In particular, we show that ComPEFT improves with scale - stronger models exhibit higher compressibility and better performance. For example, we show that ComPEFT applied to LLaMA outperforms QLoRA by 4.16% on MMLU with a storage size reduction of up to 26x. In addition, we show that the compressed experts produced by ComPEFT maintain few-shot compositional generalization capabilities, facilitate efficient communication and computation, and exhibit enhanced performance when merged. Lastly, we provide an analysis of different method components, compare it with other PEFT methods, and test ComPEFT's efficacy for compressing the residual of full-finetuning. Our code is available at https://github.com/prateeky2806/compeft",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prateek Yadav",
      "Leshem Choshen",
      "Colin Raffel",
      "Mohit Bansal"
    ]
  },
  "http://arxiv.org/abs/2311.13133": {
    "title": "LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigms",
    "volume": "Nov",
    "abstract": "Large Language Models are traditionally finetuned on large instruction datasets. However recent studies suggest that small, high-quality datasets can suffice for general purpose instruction following. This lack of consensus surrounding finetuning best practices is in part due to rapidly diverging approaches to LLM evaluation. In this study, we ask whether a small amount of diverse finetuning samples can improve performance on both traditional perplexity-based NLP benchmarks, and on open-ended, model-based evaluation. We finetune open-source MPT-7B and MPT-30B models on instruction finetuning datasets of various sizes ranging from 1k to 60k samples. We find that subsets of 1k-6k instruction finetuning samples are sufficient to achieve good performance on both (1) traditional NLP benchmarks and (2) model-based evaluation. Finally, we show that mixing textbook-style and open-ended QA finetuning datasets optimizes performance on both evaluation paradigms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditi Jha",
      "Sam Havens",
      "Jeremey Dohmann",
      "Alex Trott",
      "Jacob Portes"
    ]
  },
  "http://arxiv.org/abs/2311.16173": {
    "title": "Conditions for Length Generalization in Learning Reasoning Skills",
    "volume": "Nov",
    "abstract": "Reasoning is a fundamental capability of AI agents. Recently, large language models (LLMs) have shown remarkable abilities to perform reasoning tasks. However, numerous evaluations of the reasoning capabilities of LLMs have also showed some limitations. An outstanding limitation is length generalization, meaning that when trained on reasoning problems of smaller lengths or sizes, the resulting models struggle with problems of larger sizes or lengths. This potentially indicates some theoretical limitations of generalization in learning reasoning skills. These evaluations and their observations motivated us to perform a theoretical study of the length generalization problem. This work focused on reasoning tasks that can be formulated as Markov dynamic processes (MDPs) and/or directed acyclic graphs (DAGs). It identifies and proves conditions that decide whether the length generalization problem can be solved or not for a reasoning task in a particular representation. Experiments are also conducted to verify the theoretical results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changnan Xiao",
      "Bing Liu"
    ]
  },
  "http://arxiv.org/abs/2311.13126": {
    "title": "Towards Better Parameter-Efficient Fine-Tuning for Large Language Models: A Position Paper",
    "volume": "Nov",
    "abstract": "This paper delves into the pressing need in Parameter-Efficient Fine-Tuning (PEFT) for Large Language Models (LLMs). While LLMs possess remarkable capabilities, their extensive parameter requirements and associated computational demands hinder their practicality and scalability for real-world applications. Our position paper highlights current states and the necessity of further studying into the topic, and recognizes significant challenges and open issues that must be addressed to fully harness the powerful abilities of LLMs. These challenges encompass novel efficient PEFT architectures, PEFT for different learning settings, PEFT combined with model compression techniques, and the exploration of PEFT for multi-modal LLMs. By presenting this position paper, we aim to stimulate further research and foster discussions surrounding more efficient and accessible PEFT for LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengyu Wang",
      "Junbing Yan",
      "Wei Zhang",
      "Jun Huang"
    ]
  },
  "http://arxiv.org/abs/2311.13118": {
    "title": "Combatting Human Trafficking in the Cyberspace: A Natural Language Processing-Based Methodology to Analyze the Language in Online Advertisements",
    "volume": "Nov",
    "abstract": "This project tackles the pressing issue of human trafficking in online C2C marketplaces through advanced Natural Language Processing (NLP) techniques. We introduce a novel methodology for generating pseudo-labeled datasets with minimal supervision, serving as a rich resource for training state-of-the-art NLP models. Focusing on tasks like Human Trafficking Risk Prediction (HTRP) and Organized Activity Detection (OAD), we employ cutting-edge Transformer models for analysis. A key contribution is the implementation of an interpretability framework using Integrated Gradients, providing explainable insights crucial for law enforcement. This work not only fills a critical gap in the literature but also offers a scalable, machine learning-driven approach to combat human exploitation online. It serves as a foundation for future research and practical applications, emphasizing the role of machine learning in addressing complex social issues",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alejandro Rodriguez Perez",
      "Pablo Rivas"
    ]
  },
  "http://arxiv.org/abs/2311.13110": {
    "title": "White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?",
    "volume": "Nov",
    "abstract": "In this paper, we contend that a natural objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a low-dimensional Gaussian mixture supported on incoherent subspaces. The goodness of such a representation can be evaluated by a principled measure, called sparse rate reduction, that simultaneously maximizes the intrinsic information gain and extrinsic sparsity of the learned representation. From this perspective, popular deep network architectures, including transformers, can be viewed as realizing iterative schemes to optimize this measure. Particularly, we derive a transformer block from alternating optimization on parts of this objective: the multi-head self-attention operator compresses the representation by implementing an approximate gradient descent step on the coding rate of the features, and the subsequent multi-layer perceptron sparsifies the features. This leads to a family of white-box transformer-like deep network architectures, named CRATE, which are mathematically fully interpretable. We show, by way of a novel connection between denoising and compression, that the inverse to the aforementioned compressive encoding can be realized by the same class of CRATE architectures. Thus, the so-derived white-box architectures are universal to both encoders and decoders. Experiments show that these networks, despite their simplicity, indeed learn to compress and sparsify representations of large-scale real-world image and text datasets, and achieve performance very close to highly engineered transformer-based models: ViT, MAE, DINO, BERT, and GPT2. We believe the proposed computational framework demonstrates great potential in bridging the gap between theory and practice of deep learning, from a unified perspective of data compression. Code is available at: https://ma-lab-berkeley.github.io/CRATE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaodong Yu",
      "Sam Buchanan",
      "Druv Pai",
      "Tianzhe Chu",
      "Ziyang Wu",
      "Shengbang Tong",
      "Hao Bai",
      "Yuexiang Zhai",
      "Benjamin D. Haeffele",
      "Yi Ma"
    ]
  },
  "http://arxiv.org/abs/2311.13105": {
    "title": "Perceptual Structure in the Absence of Grounding for LLMs: The Impact of Abstractedness and Subjectivity in Color Language",
    "volume": "Nov",
    "abstract": "The need for grounding in language understanding is an active research topic. Previous work has suggested that color perception and color language appear as a suitable test bed to empirically study the problem, given its cognitive significance and showing that there is considerable alignment between a defined color space and the feature space defined by a language model. To further study this issue, we collect a large scale source of colors and their descriptions, containing almost a 1 million examples , and perform an empirical analysis to compare two kinds of alignments: (i) inter-space, by learning a mapping between embedding space and color space, and (ii) intra-space, by means of prompting comparatives between color descriptions. Our results show that while color space alignment holds for monolexemic, highly pragmatic color descriptions, this alignment drops considerably in the presence of examples that exhibit elements of real linguistic usage such as subjectivity and abstractedness, suggesting that grounding may be required in such cases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pablo Loyola",
      "Edison Marrese-Taylor",
      "Andres Hoyos-Idobro"
    ]
  },
  "http://arxiv.org/abs/2311.13102": {
    "title": "Detecting out-of-distribution text using topological features of transformer-based language models",
    "volume": "Nov",
    "abstract": "We attempt to detect out-of-distribution (OOD) text samples though applying Topological Data Analysis (TDA) to attention maps in transformer-based language models. We evaluate our proposed TDA-based approach for out-of-distribution detection on BERT, a transformer-based language model, and compare the to a more traditional OOD approach based on BERT CLS embeddings. We found that our TDA approach outperforms the CLS embedding approach at distinguishing in-distribution data (politics and entertainment news articles from HuffPost) from far out-of-domain samples (IMDB reviews), but its effectiveness deteriorates with near out-of-domain (CNN/Dailymail) or same-domain (business news articles from HuffPost) datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andres Pollano",
      "Anupam Chaudhuri",
      "Anj Simmons"
    ]
  },
  "http://arxiv.org/abs/2311.13095": {
    "title": "Enhancing Logical Reasoning in Large Language Models to Facilitate Legal Applications",
    "volume": "Nov",
    "abstract": "Language serves as a vehicle for conveying thought, enabling communication among individuals. The ability to distinguish between diverse concepts, identify fairness and injustice, and comprehend a range of legal notions fundamentally relies on logical reasoning. Large Language Models (LLMs) attempt to emulate human language understanding and generation, but their competency in logical reasoning remains limited. This paper seeks to address the philosophical question: How can we effectively teach logical reasoning to LLMs while maintaining a deep understanding of the intricate relationship between language and logic? By focusing on bolstering LLMs' capabilities in logical reasoning, we aim to expand their applicability in law and other logic-intensive disciplines. To this end, we propose a Reinforcement Learning from Logical Feedback (RLLF) approach, which serves as a potential framework for refining LLMs' reasoning capacities. Through RLLF and a revised evaluation methodology, we explore new avenues for research in this domain and contribute to the development of LLMs capable of handling complex legal reasoning tasks while acknowledging the fundamental connection between language and logic",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ha-Thanh Nguyen",
      "Wachara Fungwacharakorn",
      "Ken Satoh"
    ]
  },
  "http://arxiv.org/abs/2311.14737": {
    "title": "Positional Description Matters for Transformers Arithmetic",
    "volume": "Nov",
    "abstract": "Transformers, central to the successes in modern Natural Language Processing, often falter on arithmetic tasks despite their vast capabilities --which paradoxically include remarkable coding abilities. We observe that a crucial challenge is their naive reliance on positional information to solve arithmetic problems with a small number of digits, leading to poor performance on larger numbers. Herein, we delve deeper into the role of positional encoding, and propose several ways to fix the issue, either by modifying the positional encoding directly, or by modifying the representation of the arithmetic task to leverage standard positional encoding differently. We investigate the value of these modifications for three tasks: (i) classical multiplication, (ii) length extrapolation in addition, and (iii) addition in natural language context. For (i) we train a small model on a small dataset (100M parameters and 300k samples) with remarkable aptitude in (direct, no scratchpad) 15 digits multiplication and essentially perfect up to 12 digits, while usual training in this context would give a model failing at 4 digits multiplication. In the experiments on addition, we use a mere 120k samples to demonstrate: for (ii) extrapolation from 10 digits to testing on 12 digits numbers while usual training would have no extrapolation, and for (iii) almost perfect accuracy up to 5 digits while usual training would be correct only up to 3 digits (which is essentially memorization with a training set of 120k samples)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruoqi Shen",
      "Sébastien Bubeck",
      "Ronen Eldan",
      "Yin Tat Lee",
      "Yuanzhi Li",
      "Yi Zhang"
    ]
  },
  "http://arxiv.org/abs/2311.13061": {
    "title": "Attribution and Alignment: Effects of Local Context Repetition on Utterance Production and Comprehension in Dialogue",
    "volume": "Nov",
    "abstract": "Language models are often used as the backbone of modern dialogue systems. These models are pre-trained on large amounts of written fluent language. Repetition is typically penalised when evaluating language model generations. However, it is a key component of dialogue. Humans use local and partner specific repetitions; these are preferred by human users and lead to more successful communication in dialogue. In this study, we evaluate (a) whether language models produce human-like levels of repetition in dialogue, and (b) what are the processing mechanisms related to lexical re-use they use during comprehension. We believe that such joint analysis of model production and comprehension behaviour can inform the development of cognitively inspired dialogue generation systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aron Molnar",
      "Jaap Jumelet",
      "Mario Giulianelli",
      "Arabella Sinclair"
    ]
  },
  "http://arxiv.org/abs/2311.13053": {
    "title": "Beyond Text: Unveiling Multimodal Proficiency of Large Language Models with MultiAPI Benchmark",
    "volume": "Nov",
    "abstract": "The proliferation of Large Language Models like ChatGPT has significantly advanced language understanding and generation, impacting a broad spectrum of applications. However, these models predominantly excel in text-based tasks, overlooking the complexity of real-world multimodal information. This study introduces MultiAPI, a pioneering comprehensive large-scale API benchmark dataset aimed at expanding LLMs' proficiency in multimodal contexts. Developed collaboratively through ChatGPT, MultiAPI consists of 235 diverse API calls and 2,038 contextual prompts, offering a unique platform evaluation of tool-augmented LLMs handling multimodal tasks. Through comprehensive experiments, our findings reveal that while LLMs demonstrate proficiency in API call decision-making, they face challenges in domain identification, function selection, and argument generation. What's more, we surprisingly notice that auxiliary context can actually impair the performance. An in-depth error analysis paves the way for a new paradigm to address these challenges, suggesting a potential direction for future LLM research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Liu",
      "Jianfeng Lin",
      "Jiawei Zhang"
    ]
  },
  "http://arxiv.org/abs/2311.13029": {
    "title": "Systematic word meta-sense extension",
    "volume": "Nov",
    "abstract": "The meaning of polysemous words often varies in a highly productive yet predictable way. Generalizing the regularity between conventional senses to derive novel word meaning is crucial for automated processing of non-literal language uses such as figurative expressions. We introduce a novel task called systematic word meta-sense extension (SWORME) to test and improve language models' ability to extend word meaning to denote new semantic domains (also called meta-senses) that bear regular semantic relations with existing senses. We found that language models prefer incremental lexical semantic change toward conceptually similar meta-senses such as logical metonymy, and are much worse at predicting highly non-literal meaning extensions such as metaphors. We propose a novel analogy-based method of word meaning extension, and show that it effectively improves language model systematicity in making both gradual and radical types of meta-sense extension. We further demonstrate that learning systematic meta-sense extensions benefits language models on multiple benchmarks of figurative language understanding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Yu"
    ]
  },
  "http://arxiv.org/abs/2311.12986": {
    "title": "Unsupervised Graph Attention Autoencoder for Attributed Networks using K-means Loss",
    "volume": "Nov",
    "abstract": "Multimodal Sentiment Analysis (MSA) has recently become a centric research direction for many real-world applications. This proliferation is due to the fact that opinions are central to almost all human activities and are key influencers of our behaviors. In addition, the recent deployment of Deep Learning-based (DL) models has proven their high efficiency for a wide range of Western languages. In contrast, Arabic DL-based multimodal sentiment analysis (MSA) is still in its infantile stage due, mainly, to the lack of standard datasets. % The contribution In this paper, our investigation is twofold. First, we design a pipeline that helps building our Arabic Multimodal dataset leveraging both state-of-the-art transformers and feature extraction tools within word alignment techniques. Thereafter, we validate our dataset using state-of-the-art transformer-based model dealing with multimodality. Despite the small size of the outcome dataset, experiments show that Arabic multimodality is very promising",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdelfateh Bekkaira",
      "Slimane Bellaouar",
      "Slimane Oulad-Naoui"
    ]
  },
  "http://arxiv.org/abs/2311.12983": {
    "title": "GAIA: a benchmark for General AI Assistants",
    "volume": "Nov",
    "abstract": "We introduce GAIA, a benchmark for General AI Assistants that, if solved, would represent a milestone in AI research. GAIA proposes real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency. GAIA questions are conceptually simple for humans yet challenging for most advanced AIs: we show that human respondents obtain 92\\% vs. 15\\% for GPT-4 equipped with plugins. This notable performance disparity contrasts with the recent trend of LLMs outperforming humans on tasks requiring professional skills in e.g. law or chemistry. GAIA's philosophy departs from the current trend in AI benchmarks suggesting to target tasks that are ever more difficult for humans. We posit that the advent of Artificial General Intelligence (AGI) hinges on a system's capability to exhibit similar robustness as the average human does on such questions. Using GAIA's methodology, we devise 466 questions and their answer. We release our questions while retaining answers to 300 of them to power a leader-board available at https://huggingface.co/gaia-benchmark",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Grégoire Mialon",
      "Clémentine Fourrier",
      "Craig Swift",
      "Thomas Wolf",
      "Yann LeCun",
      "Thomas Scialom"
    ]
  },
  "http://arxiv.org/abs/2311.14736": {
    "title": "Data Diversity Matters for Robust Instruction Tuning",
    "volume": "Nov",
    "abstract": "Instruction tuning has emerged as a key step in aligning large language models. One of the central challenges of instruction tuning is dataset selection, as the composition of the instruction tuning dataset can significantly impact downstream performance. In particular, researchers have hypothesized that dataset diversity and dataset quality are important indicators of downstream performance. However, it is not clear how to automatically select high quality and diverse data or how exactly quality and diversity affect instruction following ability. To resolve these issues, we propose a new algorithm, Quality-Diversity Instruction Tuning (QDIT). QDIT provides a principled algorithm to control dataset diversity and quality, allowing us to conduct an in depth study on the effect of diversity and quality on instruction tuning performance. From this study we draw two key insights (1) there is a natural tradeoff between dataset diversity and quality and (2) increasing dataset diversity significantly improves the worst case instruction following performance, therefore improving robustness. We validate the performance of QDIT on several large scale instruction tuning datasets, where we find it can improve worst case performance by 18% while maintaining or improving average performance compared to quality driven baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Bukharin",
      "Tuo Zhao"
    ]
  },
  "http://arxiv.org/abs/2311.14743": {
    "title": "A Baseline Analysis of Reward Models' Ability To Accurately Analyze Foundation Models Under Distribution Shift",
    "volume": "Nov",
    "abstract": "Foundation models, specifically Large Language Models (LLM's), have lately gained wide-spread attention and adoption. Reinforcement Learning with Human Feedback (RLHF) involves training a reward model to capture desired behaviors, which is then used to align an LLM. These reward models are additionally used at inference-time to estimate how well LLM responses adhere to those desired behaviors. However, there is little work measuring how robust these reward models are to distribution shifts. In this work, we evaluate how reward model performance - measured via accuracy and calibration (i.e. alignment between accuracy and confidence) - is affected by distribution shift. We show novel calibration patterns and accuracy drops due to OOD prompts and responses, and that the reward model is more sensitive to shifts in responses than prompts. Additionally, we adapt an OOD detection technique commonly used in classification to the reward model setting in order to detect these distribution shifts in prompts and responses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Pikus",
      "Will LeVine",
      "Tony Chen",
      "Sean Hendryx"
    ]
  },
  "http://arxiv.org/abs/2311.12735": {
    "title": "LowResource at BLP-2023 Task 2: Leveraging BanglaBert for Low Resource Sentiment Analysis of Bangla Language",
    "volume": "Nov",
    "abstract": "This paper describes the system of the LowResource Team for Task 2 of BLP-2023, which involves conducting sentiment analysis on a dataset composed of public posts and comments from diverse social media platforms. Our primary aim is to utilize BanglaBert, a BERT model pre-trained on a large Bangla corpus, using various strategies including fine-tuning, dropping random tokens, and using several external datasets. Our final model is an ensemble of the three best BanglaBert variations. Our system has achieved overall 3rd in the Test Set among 30 participating teams with a score of 0.718. Additionally, we discuss the promising systems that didn't perform well namely task-adaptive pertaining and paraphrasing using BanglaT5. Training codes and external datasets which are used for our system are publicly available at https://github.com/Aunabil4602/bnlp-workshop-task2-2023",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aunabil Chakma",
      "Masum Hasan"
    ]
  },
  "http://arxiv.org/abs/2311.12727": {
    "title": "Soft Random Sampling: A Theoretical and Empirical Analysis",
    "volume": "Nov",
    "abstract": "Soft random sampling (SRS) is a simple yet effective approach for efficient training of large-scale deep neural networks when dealing with massive data. SRS selects a subset uniformly at random with replacement from the full data set in each epoch. In this paper, we conduct a theoretical and empirical analysis of SRS. First, we analyze its sampling dynamics including data coverage and occupancy. Next, we investigate its convergence with non-convex objective functions and give the convergence rate. Finally, we provide its generalization performance. We empirically evaluate SRS for image recognition on CIFAR10 and automatic speech recognition on Librispeech and an in-house payload dataset to demonstrate its effectiveness. Compared to existing coreset-based data selection methods, SRS offers a better accuracy-efficiency trade-off. Especially on real-world industrial scale data sets, it is shown to be a powerful training strategy with significant speedup and competitive performance with almost no additional computing cost",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaodong Cui",
      "Ashish Mittal",
      "Songtao Lu",
      "Wei Zhang",
      "George Saon",
      "Brian Kingsbury"
    ]
  },
  "http://arxiv.org/abs/2311.12707": {
    "title": "Keeping Users Engaged During Repeated Administration of the Same Questionnaire: Using Large Language Models to Reliably Diversify Questions",
    "volume": "Nov",
    "abstract": "Standardized, validated questionnaires are vital tools in HCI research and healthcare, offering dependable self-report data. However, their repeated use in longitudinal or pre-post studies can induce respondent fatigue, impacting data quality via response biases and decreased response rates. We propose utilizing large language models (LLMs) to generate diverse questionnaire versions while retaining good psychometric properties. In a longitudinal study, participants engaged with our agent system and responded daily for two weeks to either a standardized depression questionnaire or one of two LLM-generated questionnaire variants, alongside a validated depression questionnaire. Psychometric testing revealed consistent covariation between the external criterion and the focal measure administered across the three conditions, demonstrating the reliability and validity of the LLM-generated variants. Participants found the repeated administration of the standardized questionnaire significantly more repetitive compared to the variants. Our findings highlight the potential of LLM-generated variants to invigorate questionnaires, fostering engagement and interest without compromising validity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hye Sun Yun",
      "Mehdi Arjmand",
      "Phillip Raymond Sherlock",
      "Michael Paasche-Orlow",
      "James W. Griffith",
      "Timothy Bickmore"
    ]
  },
  "http://arxiv.org/abs/2311.12699": {
    "title": "Can Large Language Models Understand Content and Propagation for Misinformation Detection: An Empirical Study",
    "volume": "Nov",
    "abstract": "Large Language Models (LLMs) have garnered significant attention for their powerful ability in natural language understanding and reasoning. In this paper, we present a comprehensive empirical study to explore the performance of LLMs on misinformation detection tasks. This study stands as the pioneering investigation into the understanding capabilities of multiple LLMs regarding both content and propagation across social media platforms. Our empirical studies on five misinformation detection datasets show that LLMs with diverse prompts achieve comparable performance in text-based misinformation detection but exhibit notably constrained capabilities in comprehending propagation structure compared to existing models in propagation-based misinformation detection. Besides, we further design four instruction-tuned strategies to enhance LLMs for both content and propagation-based misinformation detection. These strategies boost LLMs to actively learn effective features from multiple instances or hard instances, and eliminate irrelevant propagation structures, thereby achieving better detection performance. Extensive experiments further demonstrate LLMs would play a better capacity in content and propagation structure under these proposed strategies and achieve promising detection performance. These findings highlight the potential ability of LLMs to detect misinformation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengyang Chen",
      "Lingwei Wei",
      "Han Cao",
      "Wei Zhou",
      "Songlin Hu"
    ]
  },
  "http://arxiv.org/abs/2311.12689": {
    "title": "Fair Text Classification with Wasserstein Independence",
    "volume": "Nov",
    "abstract": "Group fairness is a central research topic in text classification, where reaching fair treatment between sensitive groups (e.g. women vs. men) remains an open challenge. This paper presents a novel method for mitigating biases in neural text classification, agnostic to the model architecture. Considering the difficulty to distinguish fair from unfair information in a text encoder, we take inspiration from adversarial training to induce Wasserstein independence between representations learned to predict our target label and the ones learned to predict some sensitive attribute. Our approach provides two significant advantages. Firstly, it does not require annotations of sensitive attributes in both testing and training data. This is more suitable for real-life scenarios compared to existing methods that require annotations of sensitive attributes at train time. Second, our approach exhibits a comparable or better fairness-accuracy trade-off compared to existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thibaud Leteno",
      "Antoine Gourru",
      "Charlotte Laclau",
      "Rémi Emonet",
      "Christophe Gravier"
    ]
  },
  "http://arxiv.org/abs/2311.12664": {
    "title": "The DURel Annotation Tool: Human and Computational Measurement of Semantic Proximity, Sense Clusters and Semantic Change",
    "volume": "Nov",
    "abstract": "We present the DURel tool that implements the annotation of semantic proximity between uses of words into an online, open source interface. The tool supports standardized human annotation as well as computational annotation, building on recent advances with Word-in-Context models. Annotator judgments are clustered with automatic graph clustering techniques and visualized for analysis. This allows to measure word senses with simple and intuitive micro-task judgments between use pairs, requiring minimal preparation efforts. The tool offers additional functionalities to compare the agreement between annotators to guarantee the inter-subjectivity of the obtained judgments and to calculate summary statistics giving insights into sense frequency distributions, semantic variation or changes of senses over time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Schlechtweg",
      "Shafqat Mumtaz Virk",
      "Pauline Sander",
      "Emma Sköldberg",
      "Lukas Theuer Linke",
      "Tuo Zhang",
      "Nina Tahmasebi",
      "Jonas Kuhn",
      "Sabine Schulte im Walde"
    ]
  },
  "http://arxiv.org/abs/2311.12649": {
    "title": "MathGloss: Building mathematical glossaries from text",
    "volume": "Nov",
    "abstract": "MathGloss is a project to create a knowledge graph (KG) for undergraduate mathematics from text, automatically, using modern natural language processing (NLP) tools and resources already available on the web. MathGloss is a linked database of undergraduate concepts in mathematics. So far, it combines five resources: (i) Wikidata, a collaboratively edited, multilingual knowledge graph hosted by the Wikimedia Foundation, (ii) terms covered in mathematics courses at the University of Chicago, (iii) the syllabus of the French undergraduate mathematics curriculum which includes hyperlinks to the automated theorem prover Lean 4, (iv) MuLiMa, a multilingual dictionary of mathematics curated by mathematicians, and (v) the nLab, a wiki for category theory also curated by mathematicians. MathGloss's goal is to bring together resources for learning mathematics and to allow every mathematician to tailor their learning to their own preferences. Moreover, by organizing different resources for learning undergraduate mathematics alongside those for learning formal mathematics, we hope to make it easier for mathematicians and formal tools (theorem provers, computer algebra systems, etc) experts to \"understand\" each other and break down some of the barriers to formal math",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucy Horowitz",
      "Valeria de Paiva"
    ]
  },
  "http://arxiv.org/abs/2311.12574": {
    "title": "IMGTB: A Framework for Machine-Generated Text Detection Benchmarking",
    "volume": "Nov",
    "abstract": "In the era of large language models generating high quality texts, it is a necessity to develop methods for detection of machine-generated text to avoid harmful use or simply due to annotation purposes. It is, however, also important to properly evaluate and compare such developed methods. Recently, a few benchmarks have been proposed for this purpose; however, integration of newest detection methods is rather challenging, since new methods appear each month and provide slightly different evaluation pipelines. In this paper, we present the IMGTB framework, which simplifies the benchmarking of machine-generated text detection methods by easy integration of custom (new) methods and evaluation datasets. Its configurability and flexibility makes research and development of new detection methods easier, especially their comparison to the existing state-of-the-art detectors. The default set of analyses, metrics and visualizations offered by the tool follows the established practices of machine-generated text detection benchmarking found in state-of-the-art literature",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michal Spiegel",
      "Dominik Macko"
    ]
  },
  "http://arxiv.org/abs/2311.12538": {
    "title": "In-Context Learning Functions with Varying Number of Minima",
    "volume": "Nov",
    "abstract": "Large Language Models (LLMs) have proven effective at In-Context Learning (ICL), an ability that allows them to create predictors from labeled examples. Few studies have explored the interplay between ICL and specific properties of functions it attempts to approximate. In our study, we use a formal framework to explore ICL and propose a new task of approximating functions with varying number of minima. We implement a method that allows for producing functions with given inputs as minima. We find that increasing the number of minima degrades ICL performance. At the same time, our evaluation shows that ICL outperforms 2-layer Neural Network (2NN) model. Furthermore, ICL learns faster than 2NN in all settings. We validate the findings through a set of few-shot experiments across various hyperparameter configurations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Oniani",
      "Yanshan Wang"
    ]
  },
  "http://arxiv.org/abs/2311.12537": {
    "title": "Oasis: Data Curation and Assessment System for Pretraining of Large Language Models",
    "volume": "Nov",
    "abstract": "Data is one of the most critical elements in building a large language model. However, existing systems either fail to customize a corpus curation pipeline or neglect to leverage comprehensive corpus assessment for iterative optimization of the curation. To this end, we present a pretraining corpus curation and assessment platform called Oasis -- a one-stop system for data quality improvement and quantification with user-friendly interactive interfaces. Specifically, the interactive modular rule filter module can devise customized rules according to explicit feedback. The debiased neural filter module builds the quality classification dataset in a negative-centric manner to remove the undesired bias. The adaptive document deduplication module could execute large-scale deduplication with limited memory resources. These three parts constitute the customized data curation module. And in the holistic data assessment module, a corpus can be assessed in local and global views, with three evaluation means including human, GPT-4, and heuristic metrics. We exhibit a complete process to use Oasis for the curation and assessment of pretraining data. In addition, an 800GB bilingual corpus curated by Oasis is publicly released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Zhou",
      "Yubo Chen",
      "Pengfei Cao",
      "Kang Liu",
      "Jun Zhao",
      "Shengping Liu"
    ]
  },
  "http://arxiv.org/abs/2311.12534": {
    "title": "Evaluation Metrics of Language Generation Models for Synthetic Traffic Generation Tasks",
    "volume": "Nov",
    "abstract": "Many Natural Language Generation (NLG) tasks aim to generate a single output text given an input prompt. Other settings require the generation of multiple texts, e.g., for Synthetic Traffic Generation (STG). This generation task is crucial for training and evaluating QA systems as well as conversational agents, where the goal is to generate multiple questions or utterances resembling the linguistic variability of real users. In this paper, we show that common NLG metrics, like BLEU, are not suitable for evaluating STG. We propose and evaluate several metrics designed to compare the generated traffic to the distribution of real user texts. We validate our metrics with an automatic procedure to verify whether they capture different types of quality issues of generated data; we also run human annotations to verify the correlation with human judgements. Experiments on three tasks, i.e., Shopping Utterance Generation, Product Question Generation and Query Auto Completion, demonstrate that our metrics are effective for evaluating STG tasks, and improve the agreement with human judgement up to 20% with respect to common NLG metrics. We believe these findings can pave the way towards better solutions for estimating the representativeness of synthetic text data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simone Filice",
      "Jason Ingyu Choi",
      "Giuseppe Castellucci",
      "Eugene Agichtein",
      "Oleg Rokhlenko"
    ]
  },
  "http://arxiv.org/abs/2311.12489": {
    "title": "Multilingual Word Embeddings for Low-Resource Languages using Anchors and a Chain of Related Languages",
    "volume": "Nov",
    "abstract": "Very low-resource languages, having only a few million tokens worth of data, are not well-supported by multilingual NLP approaches due to poor quality cross-lingual word representations. Recent work showed that good cross-lingual performance can be achieved if a source language is related to the low-resource target language. However, not all language pairs are related. In this paper, we propose to build multilingual word embeddings (MWEs) via a novel language chain-based approach, that incorporates intermediate related languages to bridge the gap between the distant source and target. We build MWEs one language at a time by starting from the resource rich source and sequentially adding each language in the chain till we reach the target. We extend a semi-joint bilingual approach to multiple languages in order to eliminate the main weakness of previous works, i.e., independently trained monolingual embeddings, by anchoring the target language around the multilingual space. We evaluate our method on bilingual lexicon induction for 4 language families, involving 4 very low-resource (<5M tokens) and 4 moderately low-resource (<50M) target languages, showing improved performance in both categories. Additionally, our analysis reveals the importance of good quality embeddings for intermediate languages as well as the importance of leveraging anchor points from all languages in the multilingual space",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Viktor Hangya",
      "Silvia Severini",
      "Radoslav Ralev",
      "Alexander Fraser",
      "Hinrich Schütze"
    ]
  },
  "http://arxiv.org/abs/2311.12480": {
    "title": "Speaker-Adapted End-to-End Visual Speech Recognition for Continuous Spanish",
    "volume": "Nov",
    "abstract": "Different studies have shown the importance of visual cues throughout the speech perception process. In fact, the development of audiovisual approaches has led to advances in the field of speech technologies. However, although noticeable results have recently been achieved, visual speech recognition remains an open research problem. It is a task in which, by dispensing with the auditory sense, challenges such as visual ambiguities and the complexity of modeling silence must be faced. Nonetheless, some of these challenges can be alleviated when the problem is approached from a speaker-dependent perspective. Thus, this paper studies, using the Spanish LIP-RTVE database, how the estimation of specialized end-to-end systems for a specific person could affect the quality of speech recognition. First, different adaptation strategies based on the fine-tuning technique were proposed. Then, a pre-trained CTC/Attention architecture was used as a baseline throughout our experiments. Our findings showed that a two-step fine-tuning process, where the VSR system is first adapted to the task domain, provided significant improvements when the speaker adaptation was addressed. Furthermore, results comparable to the current state of the art were reached even when only a limited amount of data was available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Gimeno-Gómez",
      "Carlos-D. Martínez-Hinarejos"
    ]
  },
  "http://arxiv.org/abs/2311.12475": {
    "title": "PhayaThaiBERT: Enhancing a Pretrained Thai Language Model with Unassimilated Loanwords",
    "volume": "Nov",
    "abstract": "While WangchanBERTa has become the de facto standard in transformer-based Thai language modeling, it still has shortcomings in regard to the understanding of foreign words, most notably English words, which are often borrowed without orthographic assimilation into Thai in many contexts. We identify the lack of foreign vocabulary in WangchanBERTa's tokenizer as the main source of these shortcomings. We then expand WangchanBERTa's vocabulary via vocabulary transfer from XLM-R's pretrained tokenizer and pretrain a new model using the expanded tokenizer, starting from WangchanBERTa's checkpoint, on a new dataset that is larger than the one used to train WangchanBERTa. Our results show that our new pretrained model, PhayaThaiBERT, outperforms WangchanBERTa in many downstream tasks and datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Panyut Sriwirote",
      "Jalinee Thapiang",
      "Vasan Timtong",
      "Attapol T. Rutherford"
    ]
  },
  "http://arxiv.org/abs/2311.12474": {
    "title": "CSMeD: Bridging the Dataset Gap in Automated Citation Screening for Systematic Literature Reviews",
    "volume": "Nov",
    "abstract": "Systematic literature reviews (SLRs) play an essential role in summarising, synthesising and validating scientific evidence. In recent years, there has been a growing interest in using machine learning techniques to automate the identification of relevant studies for SLRs. However, the lack of standardised evaluation datasets makes comparing the performance of such automated literature screening systems difficult. In this paper, we analyse the citation screening evaluation datasets, revealing that many of the available datasets are either too small, suffer from data leakage or have limited applicability to systems treating automated literature screening as a classification task, as opposed to, for example, a retrieval or question-answering task. To address these challenges, we introduce CSMeD, a meta-dataset consolidating nine publicly released collections, providing unified access to 325 SLRs from the fields of medicine and computer science. CSMeD serves as a comprehensive resource for training and evaluating the performance of automated citation screening models. Additionally, we introduce CSMeD-FT, a new dataset designed explicitly for evaluating the full text publication screening task. To demonstrate the utility of CSMeD, we conduct experiments and establish baselines on new datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wojciech Kusa",
      "Oscar E. Mendoza",
      "Matthias Samwald",
      "Petr Knoth",
      "Allan Hanbury"
    ]
  },
  "http://arxiv.org/abs/2311.12468": {
    "title": "Analysis of Visual Features for Continuous Lipreading in Spanish",
    "volume": "Nov",
    "abstract": "During a conversation, our brain is responsible for combining information obtained from multiple senses in order to improve our ability to understand the message we are perceiving. Different studies have shown the importance of presenting visual information in these situations. Nevertheless, lipreading is a complex task whose objective is to interpret speech when audio is not available. By dispensing with a sense as crucial as hearing, it will be necessary to be aware of the challenge that this lack presents. In this paper, we propose an analysis of different speech visual features with the intention of identifying which of them is the best approach to capture the nature of lip movements for natural Spanish and, in this way, dealing with the automatic visual speech recognition task. In order to estimate our system, we present an audiovisual corpus compiled from a subset of the RTVE database, which has been used in the Albayz\\'in evaluations. We employ a traditional system based on Hidden Markov Models with Gaussian Mixture Models. Results show that, although the task is difficult, in restricted conditions we obtain recognition results which determine that using eigenlips in combination with deep features is the best visual approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Gimeno-Gómez",
      "Carlos-D. Martínez-Hinarejos"
    ]
  },
  "http://arxiv.org/abs/2311.12457": {
    "title": "LIP-RTVE: An Audiovisual Database for Continuous Spanish in the Wild",
    "volume": "Nov",
    "abstract": "Speech is considered as a multi-modal process where hearing and vision are two fundamentals pillars. In fact, several studies have demonstrated that the robustness of Automatic Speech Recognition systems can be improved when audio and visual cues are combined to represent the nature of speech. In addition, Visual Speech Recognition, an open research problem whose purpose is to interpret speech by reading the lips of the speaker, has been a focus of interest in the last decades. Nevertheless, in order to estimate these systems in the currently Deep Learning era, large-scale databases are required. On the other hand, while most of these databases are dedicated to English, other languages lack sufficient resources. Thus, this paper presents a semi-automatically annotated audiovisual database to deal with unconstrained natural Spanish, providing 13 hours of data extracted from Spanish television. Furthermore, baseline results for both speaker-dependent and speaker-independent scenarios are reported using Hidden Markov Models, a traditional paradigm that has been widely used in the field of Speech Technologies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Gimeno-Gómez",
      "Carlos-D. Martínez-Hinarejos"
    ]
  },
  "http://arxiv.org/abs/2311.12420": {
    "title": "How Far Have We Gone in Vulnerability Detection Using Large Language Models",
    "volume": "Nov",
    "abstract": "As software becomes increasingly complex and prone to vulnerabilities, automated vulnerability detection is critically important, yet challenging. Given the significant successes of Large Language Models (LLMs) in various tasks, there is growing anticipation of their efficacy in vulnerability detection. However, a quantitative understanding of their potential in vulnerability detection is still missing. To bridge this gap, we introduce a comprehensive vulnerability benchmark VulBench. This benchmark aggregates high-quality data from a wide range of CTF (Capture-the-Flag) challenges and real-world applications, with annotations for each vulnerable function detailing the vulnerability type and its root cause. Through our experiments encompassing 16 LLMs and 6 state-of-the-art (SOTA) deep learning-based models and static analyzers, we find that several LLMs outperform traditional deep learning approaches in vulnerability detection, revealing an untapped potential in LLMs. This work contributes to the understanding and utilization of LLMs for enhanced software security",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Gao",
      "Hao Wang",
      "Yuchen Zhou",
      "Wenyu Zhu",
      "Chao Zhang"
    ]
  },
  "http://arxiv.org/abs/2311.12418": {
    "title": "Visual Analytics for Generative Transformer Models",
    "volume": "Nov",
    "abstract": "While transformer-based models have achieved state-of-the-art results in a variety of classification and generation tasks, their black-box nature makes them challenging for interpretability. In this work, we present a novel visual analytical framework to support the analysis of transformer-based generative networks. In contrast to previous work, which has mainly focused on encoder-based models, our framework is one of the first dedicated to supporting the analysis of transformer-based encoder-decoder models and decoder-only models for generative and classification tasks. Hence, we offer an intuitive overview that allows the user to explore different facets of the model through interactive visualization. To demonstrate the feasibility and usefulness of our framework, we present three detailed case studies based on real-world NLP research problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raymond Li",
      "Ruixin Yang",
      "Wen Xiao",
      "Ahmed AbuRaed",
      "Gabriel Murray",
      "Giuseppe Carenini"
    ]
  },
  "http://arxiv.org/abs/2311.12410": {
    "title": "nach0: Multimodal Natural and Chemical Languages Foundation Model",
    "volume": "Nov",
    "abstract": "Large Language Models (LLMs) have substantially driven scientific progress in various domains, and many papers have demonstrated their ability to tackle complex problems with creative solutions. Our paper introduces a new foundation model, nach0, capable of solving various chemical and biological tasks: biomedical question answering, named entity recognition, molecular generation, molecular synthesis, attributes prediction, and others. nach0 is a multi-domain and multi-task encoder-decoder LLM pre-trained on unlabeled text from scientific literature, patents, and molecule strings to incorporate a range of chemical and linguistic knowledge. We employed instruction tuning, where specific task-related instructions are utilized to fine-tune nach0 for the final set of tasks. To train nach0 effectively, we leverage the NeMo framework, enabling efficient parallel optimization of both base and large model versions. Extensive experiments demonstrate that our model outperforms state-of-the-art baselines on single-domain and cross-domain tasks. Furthermore, it can generate high-quality outputs in molecular and textual formats, showcasing its effectiveness in multi-domain setups",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Micha Livne",
      "Zulfat Miftahutdinov",
      "Elena Tutubalina",
      "Maksim Kuznetsov",
      "Daniil Polykovskiy",
      "Annika Brundyn",
      "Aastha Jhunjhunwala",
      "Anthony Costa",
      "Alex Aliper",
      "Alex Zhavoronkov"
    ]
  },
  "http://arxiv.org/abs/2311.12405": {
    "title": "IndoRobusta: Towards Robustness Against Diverse Code-Mixed Indonesian Local Languages",
    "volume": "Nov",
    "abstract": "Significant progress has been made on Indonesian NLP. Nevertheless, exploration of the code-mixing phenomenon in Indonesian is limited, despite many languages being frequently mixed with Indonesian in daily conversation. In this work, we explore code-mixing in Indonesian with four embedded languages, i.e., English, Sundanese, Javanese, and Malay; and introduce IndoRobusta, a framework to evaluate and improve the code-mixing robustness. Our analysis shows that the pre-training corpus bias affects the model's ability to better handle Indonesian-English code-mixing when compared to other local languages, despite having higher language diversity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Farid Adilazuarda",
      "Samuel Cahyawijaya",
      "Genta Indra Winata",
      "Pascale Fung",
      "Ayu Purwarianti"
    ]
  },
  "http://arxiv.org/abs/2311.12404": {
    "title": "InterPrompt: Interpretable Prompting for Interrelated Interpersonal Risk Factors in Reddit Posts",
    "volume": "Nov",
    "abstract": "Mental health professionals and clinicians have observed the upsurge of mental disorders due to Interpersonal Risk Factors (IRFs). To simulate the human-in-the-loop triaging scenario for early detection of mental health disorders, we recognized textual indications to ascertain these IRFs : Thwarted Belongingness (TBe) and Perceived Burdensomeness (PBu) within personal narratives. In light of this, we use N-shot learning with GPT-3 model on the IRF dataset, and underscored the importance of fine-tuning GPT-3 model to incorporate the context-specific sensitivity and the interconnectedness of textual cues that represent both IRFs. In this paper, we introduce an Interpretable Prompting (InterPrompt)} method to boost the attention mechanism by fine-tuning the GPT-3 model. This allows a more sophisticated level of language modification by adjusting the pre-trained weights. Our model learns to detect usual patterns and underlying connections across both the IRFs, which leads to better system-level explainability and trustworthiness. The results of our research demonstrate that all four variants of GPT-3 model, when fine-tuned with InterPrompt, perform considerably better as compared to the baseline methods, both in terms of classification and explanation generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MSVPJ Sathvik",
      "Surjodeep Sarkar",
      "Chandni Saxena",
      "Sunghwan Sohn",
      "Muskan Garg"
    ]
  },
  "http://arxiv.org/abs/2311.12399": {
    "title": "A Survey of Graph Meets Large Language Model: Progress and Future Directions",
    "volume": "Nov",
    "abstract": "Graph plays a significant role in representing and analyzing complex relationships in real-world applications such as citation networks, social networks, and biological data. Recently, Large Language Models (LLMs), which have achieved tremendous success in various domains, have also been leveraged in graph-related tasks to surpass traditional Graph Neural Networks (GNNs) based methods and yield state-of-the-art performance. In this survey, we first present a comprehensive review and analysis of existing methods that integrate LLMs with graphs. First of all, we propose a new taxonomy, which organizes existing methods into three categories based on the role (i.e., enhancer, predictor, and alignment component) played by LLMs in graph-related tasks. Then we systematically survey the representative methods along the three categories of the taxonomy. Finally, we discuss the remaining limitations of existing studies and highlight promising avenues for future research. The relevant papers are summarized and will be consistently updated at: https://github.com/yhLeeee/Awesome-LLMs-in-Graph-tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Li",
      "Zhixun Li",
      "Peisong Wang",
      "Jia Li",
      "Xiangguo Sun",
      "Hong Cheng",
      "Jeffrey Xu Yu"
    ]
  },
  "http://arxiv.org/abs/2311.12395": {
    "title": "Problems of Non-equivalent Words in Technical Translation",
    "volume": "Nov",
    "abstract": "Translating words which do not have equivalent in target language is not easy and finding proper equivalent of those words are very important to render correctly and understandably, the article defines some thoughts and ideas of scientists on the common problems of non-equivalent words from English to Russian language and includes English and Russian examples and ideas of certain scientist. The English language is worldwide spoken and there are 1.35 billion English speakers and over 258 million Russian speakers according to the 2021s statistics. Inevitably, these billions of speakers around the world have connection and they may have deal in different criteria. In order to understand one another they need to have a pure and fully-understood language. These pure languages understanding directly relates to translation knowledge where linguists and translators need to work and research to eradicate misunderstanding. Misunderstandings mostly appear in non-equivalent words because there are different local and internal words like food, garment, cultural and traditional words and others in every notion. Truly, most of these words do not have equivalent in the target language and these words need to be worked and find their equivalent in the target language to fully understand the both languages. However, some of these non-equivalent words are already professionally rendered to the target language but still there many other words to be rendered. Hence, this research paper includes different ways and rules of rendering non-equivalent words from source language to the target language",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Ibrahim Qani"
    ]
  },
  "http://arxiv.org/abs/2311.12375": {
    "title": "The Obscure Limitation of Modular Multilingual Language Models",
    "volume": "Nov",
    "abstract": "We expose the limitation of modular multilingual language models (MLMs) in multilingual inference scenarios with unknown languages. Existing evaluations of modular MLMs exclude the involvement of language identification (LID) modules, which obscures the performance of real-case multilingual scenarios of modular MLMs. In this work, we showcase the effect of adding LID on the multilingual evaluation of modular MLMs and provide discussions for closing the performance gap of caused by the pipelined approach of LID and modular MLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Farid Adilazuarda",
      "Samuel Cahyawijaya",
      "Ayu Purwarianti"
    ]
  },
  "http://arxiv.org/abs/2311.12373": {
    "title": "Beyond Turing: A Comparative Analysis of Approaches for Detecting Machine-Generated Text",
    "volume": "Nov",
    "abstract": "Significant progress has been made on text generation by pre-trained language models (PLMs), yet distinguishing between human and machine-generated text poses an escalating challenge. This paper offers an in-depth evaluation of three distinct methods used to address this task: traditional shallow learning, Language Model (LM) fine-tuning, and Multilingual Model fine-tuning. These approaches are rigorously tested on a wide range of machine-generated texts, providing a benchmark of their competence in distinguishing between human-authored and machine-authored linguistic constructs. The results reveal considerable differences in performance across methods, thus emphasizing the continued need for advancement in this crucial area of NLP. This study offers valuable insights and paves the way for future research aimed at creating robust and highly discriminative models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Farid Adilazuarda",
      "Nikolaos Nektarios Arkoulis",
      "Oleksii Chumakov"
    ]
  },
  "http://arxiv.org/abs/2311.12355": {
    "title": "Utilizing Language Models for Tour Itinerary Recommendation",
    "volume": "Nov",
    "abstract": "Tour itinerary recommendation involves planning a sequence of relevant Point-of-Interest (POIs), which combines challenges from the fields of both Operations Research (OR) and Recommendation Systems (RS). As an OR problem, there is the need to maximize a certain utility (e.g., popularity of POIs in the tour) while adhering to some constraints (e.g., maximum time for the tour). As a RS problem, it is heavily related to problem or filtering or ranking a subset of POIs that are relevant to a user and recommending it as part of an itinerary. In this paper, we explore the use of language models for the task of tour itinerary recommendation and planning. This task has the unique requirement of recommending personalized POIs relevant to users and planning these POIs as an itinerary that satisfies various constraints. We discuss some approaches in this area, such as using word embedding techniques like Word2Vec and GloVe for learning POI embeddings and transformer-based techniques like BERT for generating itineraries",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ngai Lam Ho",
      "Kwan Hui Lim"
    ]
  },
  "http://arxiv.org/abs/2311.12351": {
    "title": "Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey",
    "volume": "Nov",
    "abstract": "With the bomb ignited by ChatGPT, Transformer-based Large Language Models (LLMs) have paved a revolutionary path toward Artificial General Intelligence (AGI) and have been applied in diverse areas as knowledge bases, human interfaces, and dynamic agents. However, a prevailing limitation exists: many current LLMs, constrained by resources, are primarily pre-trained on shorter texts, rendering them less effective for longer-context prompts, commonly encountered in real-world settings. In this paper, we present a comprehensive survey focusing on the advancement of model architecture in Transformer-based LLMs to optimize long-context capabilities across all stages from pre-training to inference. We firstly delineate and analyze the problems of handling long-context input and output with the current Transformer-based models. Then, we mainly offer a holistic taxonomy to navigate the landscape of Transformer upgrades on architecture to solve these problems. Afterward, we provide the investigation on wildly used evaluation necessities tailored for long-context LLMs, including datasets, metrics, and baseline models, as well as some amazing optimization toolkits like libraries, systems, and compilers to augment LLMs' efficiency and efficacy across different stages. Finally, we further discuss the predominant challenges and potential avenues for future research in this domain. Additionally, we have established a repository where we curate relevant literature with real-time updates at https://github.com/Strivin0311/long-llms-learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunpeng Huang",
      "Jingwei Xu",
      "Zixu Jiang",
      "Junyu Lai",
      "Zenan Li",
      "Yuan Yao",
      "Taolue Chen",
      "Lijuan Yang",
      "Zhou Xin",
      "Xiaoxing Ma"
    ]
  },
  "http://arxiv.org/abs/2311.12337": {
    "title": "Do Smaller Language Models Answer Contextualised Questions Through Memorisation Or Generalisation?",
    "volume": "Nov",
    "abstract": "A distinction is often drawn between a model's ability to predict a label for an evaluation sample that is directly memorised from highly similar training samples versus an ability to predict the label via some method of generalisation. In the context of using Language Models for question-answering, discussion continues to occur as to the extent to which questions are answered through memorisation. We consider this issue for questions that would ideally be answered through reasoning over an associated context. We propose a method of identifying evaluation samples for which it is very unlikely our model would have memorised the answers. Our method is based on semantic similarity of input tokens and label tokens between training and evaluation samples. We show that our method offers advantages upon some prior approaches in that it is able to surface evaluation-train pairs that have overlap in either contiguous or discontiguous sequences of tokens. We use this method to identify unmemorisable subsets of our evaluation datasets. We train two Language Models in a multitask fashion whereby the second model differs from the first only in that it has two additional datasets added to the training regime that are designed to impart simple numerical reasoning strategies of a sort known to improve performance on some of our evaluation datasets but not on others. We then show that there is performance improvement between the two models on the unmemorisable subsets of the evaluation datasets that were expected to benefit from the additional training datasets. Specifically, performance on unmemorisable subsets of two of our evaluation datasets, DROP and ROPES significantly improves by 9.0%, and 25.7% respectively while other evaluation datasets have no significant change in performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Hartill",
      "Joshua Bensemann",
      "Michael Witbrock",
      "Patricia J. Riddle"
    ]
  },
  "http://arxiv.org/abs/2311.12323": {
    "title": "Modeling Political Orientation of Social Media Posts: An Extended Analysis",
    "volume": "Nov",
    "abstract": "Developing machine learning models to characterize political polarization on online social media presents significant challenges. These challenges mainly stem from various factors such as the lack of annotated data, presence of noise in social media datasets, and the sheer volume of data. The common research practice typically examines the biased structure of online user communities for a given topic or qualitatively measuring the impacts of polarized topics on social media. However, there is limited work focusing on analyzing polarization at the ground-level, specifically in the social media posts themselves. Such existing analysis heavily relies on annotated data, which often requires laborious human labeling, offers labels only to specific problems, and lacks the ability to determine the near-future bias state of a social media conversations. Understanding the degree of political orientation conveyed in social media posts is crucial for quantifying the bias of online user communities and investigating the spread of polarized content. In this work, we first introduce two heuristic methods that leverage on news media bias and post content to label social media posts. Next, we compare the efficacy and quality of heuristically labeled dataset with a randomly sampled human-annotated dataset. Additionally, we demonstrate that current machine learning models can exhibit improved performance in predicting political orientation of social media posts, employing both traditional supervised learning and few-shot learning setups. We conduct experiments using the proposed heuristic methods and machine learning approaches to predict the political orientation of posts collected from two social media forums with diverse political ideologies: Gab and Twitter",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sadia Kamal",
      "Brenner Little",
      "Jade Gullic",
      "Trevor Harms",
      "Kristin Olofsson",
      "Arunkumar Bagavathi"
    ]
  },
  "http://arxiv.org/abs/2311.12315": {
    "title": "AcademicGPT: Empowering Academic Research",
    "volume": "Nov",
    "abstract": "Large Language Models (LLMs) have demonstrated exceptional capabilities across various natural language processing tasks. Yet, many of these advanced LLMs are tailored for broad, general-purpose applications. In this technical report, we introduce AcademicGPT, designed specifically to empower academic research. AcademicGPT is a continual training model derived from LLaMA2-70B. Our training corpus mainly consists of academic papers, thesis, content from some academic domain, high-quality Chinese data and others. While it may not be extensive in data scale, AcademicGPT marks our initial venture into a domain-specific GPT tailored for research area. We evaluate AcademicGPT on several established public benchmarks such as MMLU and CEval, as well as on some specialized academic benchmarks like PubMedQA, SCIEval, and our newly-created ComputerScienceQA, to demonstrate its ability from general knowledge ability, to Chinese ability, and to academic ability. Building upon AcademicGPT's foundation model, we also developed several applications catered to the academic area, including General Academic Question Answering, AI-assisted Paper Reading, Paper Review, and AI-assisted Title and Abstract Generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shufa Wei",
      "Xiaolong Xu",
      "Xianbiao Qi",
      "Xi Yin",
      "Jun Xia",
      "Jingyi Ren",
      "Peijun Tang",
      "Yuxiang Zhong",
      "Yihao Chen",
      "Xiaoqin Ren",
      "Yuxin Liang",
      "Liankai Huang",
      "Kai Xie",
      "Weikang Gui",
      "Wei Tan",
      "Shuanglong Sun",
      "Yongquan Hu",
      "Qinxian Liu",
      "Nanjin Li",
      "Chihao Dai",
      "Lihua Wang",
      "Xiaohui Liu",
      "Lei Zhang",
      "Yutao Xie"
    ]
  },
  "http://arxiv.org/abs/2311.12298": {
    "title": "Noise in Relation Classification Dataset TACRED: Characterization and Reduction",
    "volume": "Nov",
    "abstract": "The overarching objective of this paper is two-fold. First, to explore model-based approaches to characterize the primary cause of the noise. in the RE dataset TACRED Second, to identify the potentially noisy instances. Towards the first objective, we analyze predictions and performance of state-of-the-art (SOTA) models to identify the root cause of noise in the dataset. Our analysis of TACRED shows that the majority of the noise in the dataset originates from the instances labeled as no-relation which are negative examples. For the second objective, we explore two nearest-neighbor-based strategies to automatically identify potentially noisy examples for elimination and reannotation. Our first strategy, referred to as Intrinsic Strategy (IS), is based on the assumption that positive examples are clean. Thus, we have used false-negative predictions to identify noisy negative examples. Whereas, our second approach, referred to as Extrinsic Strategy, is based on using a clean subset of the dataset to identify potentially noisy negative examples. Finally, we retrained the SOTA models on the eliminated and reannotated dataset. Our empirical results based on two SOTA models trained on TACRED-E following the IS show an average 4% F1-score improvement, whereas reannotation (TACRED-R) does not improve the original results. However, following ES, SOTA models show the average F1-score improvement of 3.8% and 4.4% when trained on respective eliminated (TACRED-EN) and reannotated (TACRED-RN) datasets respectively. We further extended the ES for cleaning positive examples as well, which resulted in an average performance improvement of 5.8% and 5.6% for the eliminated (TACRED-ENP) and reannotated (TACRED-RNP) datasets respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Parekh",
      "Ashish Anand",
      "Amit Awekar"
    ]
  },
  "http://arxiv.org/abs/2311.12289": {
    "title": "ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for Interdisciplinary Science",
    "volume": "Nov",
    "abstract": "Large language models record impressive performance on many natural language processing tasks. However, their knowledge capacity is limited to the pretraining corpus. Retrieval augmentation offers an effective solution by retrieving context from external knowledge sources to complement the language model. However, existing retrieval augmentation techniques ignore the structural relationships between these documents. Furthermore, retrieval models are not explored much in scientific tasks, especially in regard to the faithfulness of retrieved documents. In this paper, we propose a novel structure-aware retrieval augmented language model that accommodates document structure during retrieval augmentation. We create a heterogeneous document graph capturing multiple types of relationships (e.g., citation, co-authorship, etc.) that connect documents from more than 15 scientific disciplines (e.g., Physics, Medicine, Chemistry, etc.). We train a graph neural network on the curated document graph to act as a structural encoder for the corresponding passages retrieved during the model pretraining. Particularly, along with text embeddings of the retrieved passages, we obtain structural embeddings of the documents (passages) and fuse them together before feeding them to the language model. We evaluate our model extensively on various scientific benchmarks that include science question-answering and scientific document classification tasks. Experimental results demonstrate that structure-aware retrieval improves retrieving more coherent, faithful and contextually relevant passages, while showing a comparable performance in the overall accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sai Munikoti",
      "Anurag Acharya",
      "Sridevi Wagle",
      "Sameera Horawalavithana"
    ]
  },
  "http://arxiv.org/abs/2311.12275": {
    "title": "Enabling On-Device Large Language Model Personalization with Self-Supervised Data Selection and Synthesis",
    "volume": "Nov",
    "abstract": "After a large language model (LLM) is deployed on edge devices, it is desirable for these devices to learn from user-generated conversation data to generate user-specific and personalized responses in real-time. However, user-generated data usually contains sensitive and private information, and uploading such data to the cloud for annotation is not preferred if not prohibited. While it is possible to obtain annotation locally by directly asking users to provide preferred responses, such annotations have to be sparse to not affect user experience. In addition, the storage of edge devices is usually too limited to enable large-scale fine-tuning with full user-generated data. It remains an open question how to enable on-device LLM personalization, considering sparse annotation and limited on-device storage. In this paper, we propose a novel framework to select and store the most representative data online in a self-supervised way. Such data has a small memory footprint and allows infrequent requests of user annotations for further fine-tuning. To enhance fine-tuning quality, multiple semantically similar pairs of question texts and expected responses are generated using the LLM. Our experiments show that the proposed framework achieves the best user-specific content-generating capability (accuracy) and fine-tuning speed (performance) compared with vanilla baselines. To the best of our knowledge, this is the very first on-device LLM personalization framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiyang Qin",
      "Jun Xia",
      "Zhenge Jia",
      "Meng Jiang",
      "Ahmed Abbasi",
      "Peipei Zhou",
      "Jingtong Hu",
      "Yiyu Shi"
    ]
  },
  "http://arxiv.org/abs/2311.12233": {
    "title": "Unifying Corroborative and Contributive Attributions in Large Language Models",
    "volume": "Nov",
    "abstract": "As businesses, products, and services spring up around large language models, the trustworthiness of these models hinges on the verifiability of their outputs. However, methods for explaining language model outputs largely fall across two distinct fields of study which both use the term \"attribution\" to refer to entirely separate techniques: citation generation and training data attribution. In many modern applications, such as legal document generation and medical question answering, both types of attributions are important. In this work, we argue for and present a unified framework of large language model attributions. We show how existing methods of different types of attribution fall under the unified framework. We also use the framework to discuss real-world use cases where one or both types of attributions are required. We believe that this unified framework will guide the use case driven development of systems that leverage both types of attribution, as well as the standardization of their evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Theodora Worledge",
      "Judy Hanwen Shen",
      "Nicole Meister",
      "Caleb Winston",
      "Carlos Guestrin"
    ]
  },
  "http://arxiv.org/abs/2311.12179": {
    "title": "Leveraging Closed-Access Multilingual Embedding for Automatic Sentence Alignment in Low Resource Languages",
    "volume": "Nov",
    "abstract": "The importance of qualitative parallel data in machine translation has long been determined but it has always been very difficult to obtain such in sufficient quantity for the majority of world languages, mainly because of the associated cost and also the lack of accessibility to these languages. Despite the potential for obtaining parallel datasets from online articles using automatic approaches, forensic investigations have found a lot of quality-related issues such as misalignment, and wrong language codes. In this work, we present a simple but qualitative parallel sentence aligner that carefully leveraged the closed-access Cohere multilingual embedding, a solution that ranked second in the just concluded #CoHereAIHack 2023 Challenge (see https://ai6lagos.devpost.com). The proposed approach achieved $94.96$ and $54.83$ f1 scores on FLORES and MAFAND-MT, compared to $3.64$ and $0.64$ of LASER respectively. Our method also achieved an improvement of more than 5 BLEU scores over LASER, when the resulting datasets were used with MAFAND-MT dataset to train translation models. Our code and data are available for research purposes here (https://github.com/abumafrim/Cohere-Align)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Idris Abdulmumin",
      "Auwal Abubakar Khalid",
      "Shamsuddeen Hassan Muhammad",
      "Ibrahim Said Ahmad",
      "Lukman Jibril Aliyu",
      "Babangida Sani",
      "Bala Mairiga Abduljalil",
      "Sani Ahmad Hassan"
    ]
  },
  "http://arxiv.org/abs/2311.12131": {
    "title": "Human Learning by Model Feedback: The Dynamics of Iterative Prompting with Midjourney",
    "volume": "Nov",
    "abstract": "Generating images with a Text-to-Image model often requires multiple trials, where human users iteratively update their prompt based on feedback, namely the output image. Taking inspiration from cognitive work on reference games and dialogue alignment, this paper analyzes the dynamics of the user prompts along such iterations. We compile a dataset of iterative interactions of human users with Midjourney. Our analysis then reveals that prompts predictably converge toward specific traits along these iterations. We further study whether this convergence is due to human users, realizing they missed important details, or due to adaptation to the model's ``preferences'', producing better images for a specific language style. We show initial evidence that both possibilities are at play. The possibility that users adapt to the model's preference raises concerns about reusing user data for further training. The prompts may be biased towards the preferences of a specific model, rather than align with human intentions and natural manner of expression",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shachar Don-Yehiya",
      "Leshem Choshen",
      "Omri Abend"
    ]
  },
  "http://arxiv.org/abs/2311.12023": {
    "title": "LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning",
    "volume": "Nov",
    "abstract": "We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on adapting RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and moreover enables more aggressive quantization. For example, on the OpenAssistant benchmark LQ-LoRA is able to learn a 2.5-bit LLaMA-2 model that is competitive with a model finetuned with 4-bit QLoRA. When finetuned on a language modeling calibration dataset, LQ-LoRA can also be used for model compression; in this setting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average when including the low-rank components and requires 27GB of GPU memory) is competitive with the original model in full precision",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Guo",
      "Philip Greengard",
      "Eric P. Xing",
      "Yoon Kim"
    ]
  },
  "http://arxiv.org/abs/2311.12022": {
    "title": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark",
    "volume": "Nov",
    "abstract": "We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are \"Google-proof\"). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Rein",
      "Betty Li Hou",
      "Asa Cooper Stickland",
      "Jackson Petty",
      "Richard Yuanzhe Pang",
      "Julien Dirani",
      "Julian Michael",
      "Samuel R. Bowman"
    ]
  },
  "http://arxiv.org/abs/2311.12015": {
    "title": "GPT-4V(ision) for Robotics: Multimodal Task Planning from Human Demonstration",
    "volume": "Nov",
    "abstract": "We introduce a pipeline that enhances a general-purpose Vision Language Model, GPT-4V(ision), by integrating observations of human actions to facilitate robotic manipulation. This system analyzes videos of humans performing tasks and creates executable robot programs that incorporate affordance insights. The computation starts by analyzing the videos with GPT-4V to convert environmental and action details into text, followed by a GPT-4-empowered task planner. In the following analyses, vision systems reanalyze the video with the task plan. Object names are grounded using an open-vocabulary object detector, while focus on the hand-object relation helps to detect the moment of grasping and releasing. This spatiotemporal grounding allows the vision systems to further gather affordance data (e.g., grasp type, way points, and body postures). Experiments across various scenarios demonstrate this method's efficacy in achieving real robots' operations from human demonstrations in a zero-shot manner. The prompts of GPT-4V/GPT-4 are available at this project page: https://microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naoki Wake",
      "Atsushi Kanehira",
      "Kazuhiro Sasabuchi",
      "Jun Takamatsu",
      "Katsushi Ikeuchi"
    ]
  },
  "http://arxiv.org/abs/2311.11981": {
    "title": "H-COAL: Human Correction of AI-Generated Labels for Biomedical Named Entity Recognition",
    "volume": "Nov",
    "abstract": "With the rapid advancement of machine learning models for NLP tasks, collecting high-fidelity labels from AI models is a realistic possibility. Firms now make AI available to customers via predictions as a service (PaaS). This includes PaaS products for healthcare. It is unclear whether these labels can be used for training a local model without expensive annotation checking by in-house experts. In this work, we propose a new framework for Human Correction of AI-Generated Labels (H-COAL). By ranking AI-generated outputs, one can selectively correct labels and approach gold standard performance (100% human labeling) with significantly less human effort. We show that correcting 5% of labels can close the AI-human performance gap by up to 64% relative improvement, and correcting 20% of labels can close the performance gap by up to 86% relative improvement",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaojing Duan",
      "John P. Lalor"
    ]
  },
  "http://arxiv.org/abs/2311.11979": {
    "title": "On the Potential and Limitations of Few-Shot In-Context Learning to Generate Metamorphic Specifications for Tax Preparation Software",
    "volume": "Nov",
    "abstract": "Due to the ever-increasing complexity of income tax laws in the United States, the number of US taxpayers filing their taxes using tax preparation software (henceforth, tax software) continues to increase. According to the U.S. Internal Revenue Service (IRS), in FY22, nearly 50% of taxpayers filed their individual income taxes using tax software. Given the legal consequences of incorrectly filing taxes for the taxpayer, ensuring the correctness of tax software is of paramount importance. Metamorphic testing has emerged as a leading solution to test and debug legal-critical tax software due to the absence of correctness requirements and trustworthy datasets. The key idea behind metamorphic testing is to express the properties of a system in terms of the relationship between one input and its slightly metamorphosed twinned input. Extracting metamorphic properties from IRS tax publications is a tedious and time-consuming process. As a response, this paper formulates the task of generating metamorphic specifications as a translation task between properties extracted from tax documents - expressed in natural language - to a contrastive first-order logic form. We perform a systematic analysis on the potential and limitations of in-context learning with Large Language Models(LLMs) for this task, and outline a research agenda towards automating the generation of metamorphic specifications for tax preparation software",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dananjay Srinivas",
      "Rohan Das",
      "Saeid Tizpaz-Niari",
      "Ashutosh Trivedi",
      "Maria Leonor Pacheco"
    ]
  },
  "http://arxiv.org/abs/2311.11976": {
    "title": "Context-aware Neural Machine Translation for English-Japanese Business Scene Dialogues",
    "volume": "Nov",
    "abstract": "Despite the remarkable advancements in machine translation, the current sentence-level paradigm faces challenges when dealing with highly-contextual languages like Japanese. In this paper, we explore how context-awareness can improve the performance of the current Neural Machine Translation (NMT) models for English-Japanese business dialogues translation, and what kind of context provides meaningful information to improve translation. As business dialogue involves complex discourse phenomena but offers scarce training resources, we adapted a pretrained mBART model, finetuning on multi-sentence dialogue data, which allows us to experiment with different contexts. We investigate the impact of larger context sizes and propose novel context tokens encoding extra-sentential information, such as speaker turn and scene type. We make use of Conditional Cross-Mutual Information (CXMI) to explore how much of the context the model uses and generalise CXMI to study the impact of the extra-sentential context. Overall, we find that models leverage both preceding sentences and extra-sentential context (with CXMI increasing with context size) and we provide a more focused analysis on honorifics translation. Regarding translation quality, increased source-side context paired with scene and speaker information improves the model performance compared to previous work and our context-agnostic baselines, measured in BLEU and COMET metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sumire Honda",
      "Patrick Fernandes",
      "Chrysoula Zerva"
    ]
  },
  "http://arxiv.org/abs/2311.11973": {
    "title": "Adaptive Training Distributions with Scalable Online Bilevel Optimization",
    "volume": "Nov",
    "abstract": "Large neural networks pretrained on web-scale corpora are central to modern machine learning. In this paradigm, the distribution of the large, heterogeneous pretraining data rarely matches that of the application domain. This work considers modifying the pretraining distribution in the case where one has a small sample of data reflecting the targeted test conditions. We propose an algorithm motivated by a recent formulation of this setting as an online, bilevel optimization problem. With scalability in mind, our algorithm prioritizes computing gradients at training points which are likely to most improve the loss on the targeted distribution. Empirically, we show that in some cases this approach is beneficial over existing strategies from the domain adaptation literature but may not succeed in other cases. We propose a simple test to evaluate when our approach can be expected to work well and point towards further research to address current limitations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Grangier",
      "Pierre Ablin",
      "Awni Hannun"
    ]
  },
  "http://arxiv.org/abs/2311.11967": {
    "title": "Automatic Analysis of Substantiation in Scientific Peer Reviews",
    "volume": "Nov",
    "abstract": "With the increasing amount of problematic peer reviews in top AI conferences, the community is urgently in need of automatic quality control measures. In this paper, we restrict our attention to substantiation -- one popular quality aspect indicating whether the claims in a review are sufficiently supported by evidence -- and provide a solution automatizing this evaluation process. To achieve this goal, we first formulate the problem as claim-evidence pair extraction in scientific peer reviews, and collect SubstanReview, the first annotated dataset for this task. SubstanReview consists of 550 reviews from NLP conferences annotated by domain experts. On the basis of this dataset, we train an argument mining system to automatically analyze the level of substantiation in peer reviews. We also perform data analysis on the SubstanReview dataset to obtain meaningful insights on peer reviewing quality in NLP conferences over recent years",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanzhu Guo",
      "Guokan Shang",
      "Virgile Rennard",
      "Michalis Vazirgiannis",
      "Chloé Clavel"
    ]
  },
  "http://arxiv.org/abs/2311.11944": {
    "title": "FinanceBench: A New Benchmark for Financial Question Answering",
    "volume": "Nov",
    "abstract": "FinanceBench is a first-of-its-kind test suite for evaluating the performance of LLMs on open book financial question answering (QA). It comprises 10,231 questions about publicly traded companies, with corresponding answers and evidence strings. The questions in FinanceBench are ecologically valid and cover a diverse set of scenarios. They are intended to be clear-cut and straightforward to answer to serve as a minimum performance standard. We test 16 state of the art model configurations (including GPT-4-Turbo, Llama2 and Claude2, with vector stores and long context prompts) on a sample of 150 cases from FinanceBench, and manually review their answers (n=2,400). The cases are available open-source. We show that existing LLMs have clear limitations for financial QA. Notably, GPT-4-Turbo used with a retrieval system incorrectly answered or refused to answer 81% of questions. While augmentation techniques such as using longer context window to feed in relevant evidence improve performance, they are unrealistic for enterprise settings due to increased latency and cannot support larger financial documents. We find that all models examined exhibit weaknesses, such as hallucinations, that limit their suitability for use by enterprises",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranab Islam",
      "Anand Kannappan",
      "Douwe Kiela",
      "Rebecca Qian",
      "Nino Scherrer",
      "Bertie Vidgen"
    ]
  },
  "http://arxiv.org/abs/2311.11904": {
    "title": "LLMs as Visual Explainers: Advancing Image Classification with Evolving Visual Descriptions",
    "volume": "Nov",
    "abstract": "Vision-language models (VLMs) offer a promising paradigm for image classification by comparing the similarity between images and class embeddings. A critical challenge lies in crafting precise textual representations for class names. While previous studies have leveraged recent advancements in large language models (LLMs) to enhance these descriptors, their outputs often suffer from ambiguity and inaccuracy. We identify two primary causes: 1) The prevalent reliance on textual interactions with LLMs, leading to a mismatch between the generated text and the visual content in VLMs' latent space - a phenomenon we term the \"explain without seeing\" dilemma. 2) The oversight of the inter-class relationships, resulting in descriptors that fail to differentiate similar classes effectively. To address these issues, we propose a novel image classification framework combining VLMs with LLMs, named Iterative Optimization with Visual Feedback. In particular, our method develops an LLM-based agent, employing an evolutionary optimization strategy to refine class descriptors. Crucially, we incorporate visual feedback from VLM classification metrics, thereby guiding the optimization process with concrete visual data. Our method leads to improving accuracy on a wide range of image classification benchmarks, with 3.47\\% average gains over state-of-the-art methods. We also highlight the resulting descriptions serve as explainable and robust features that can consistently improve the performance across various backbone models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songhao Han",
      "Le Zhuo",
      "Yue Liao",
      "Si Liu"
    ]
  },
  "http://arxiv.org/abs/2311.11861": {
    "title": "Generating Valid and Natural Adversarial Examples with Large Language Models",
    "volume": "Nov",
    "abstract": "Deep learning-based natural language processing (NLP) models, particularly pre-trained language models (PLMs), have been revealed to be vulnerable to adversarial attacks. However, the adversarial examples generated by many mainstream word-level adversarial attack models are neither valid nor natural, leading to the loss of semantic maintenance, grammaticality, and human imperceptibility. Based on the exceptional capacity of language understanding and generation of large language models (LLMs), we propose LLM-Attack, which aims at generating both valid and natural adversarial examples with LLMs. The method consists of two stages: word importance ranking (which searches for the most vulnerable words) and word synonym replacement (which substitutes them with their synonyms obtained from LLMs). Experimental results on the Movie Review (MR), IMDB, and Yelp Review Polarity datasets against the baseline adversarial attack models illustrate the effectiveness of LLM-Attack, and it outperforms the baselines in human and GPT-4 evaluation by a significant margin. The model can generate adversarial examples that are typically valid and natural, with the preservation of semantic meaning, grammaticality, and human imperceptibility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zimu Wang",
      "Wei Wang",
      "Qi Chen",
      "Qiufeng Wang",
      "Anh Nguyen"
    ]
  },
  "http://arxiv.org/abs/2311.11855": {
    "title": "Evil Geniuses: Delving into the Safety of LLM-based Agents",
    "volume": "Nov",
    "abstract": "The rapid advancements in large language models (LLMs) have led to a resurgence in LLM-based agents, which demonstrate impressive human-like behaviors and cooperative capabilities in various interactions and strategy formulations. However, evaluating the safety of LLM-based agents remains a complex challenge. This paper elaborately conducts a series of manual jailbreak prompts along with a virtual chat-powered evil plan development team, dubbed Evil Geniuses, to thoroughly probe the safety aspects of these agents. Our investigation reveals three notable phenomena: 1) LLM-based agents exhibit reduced robustness against malicious attacks. 2) the attacked agents could provide more nuanced responses. 3) the detection of the produced improper responses is more challenging. These insights prompt us to question the effectiveness of LLM-based attacks on agents, highlighting vulnerabilities at various levels and within different role specializations within the system/agent of LLM-based agents. Extensive evaluation and discussion reveal that LLM-based agents face significant challenges in safety and yield insights for future research. Our code is available at https://github.com/T1aNS1R/Evil-Geniuses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Tian",
      "Xiao Yang",
      "Jingyuan Zhang",
      "Yinpeng Dong",
      "Hang Su"
    ]
  },
  "http://arxiv.org/abs/2311.11846": {
    "title": "Deepparse : An Extendable, and Fine-Tunable State-Of-The-Art Library for Parsing Multinational Street Addresses",
    "volume": "Nov",
    "abstract": "Segmenting an address into meaningful components, also known as address parsing, is an essential step in many applications from record linkage to geocoding and package delivery. Consequently, a lot of work has been dedicated to develop accurate address parsing techniques, with machine learning and neural network methods leading the state-of-the-art scoreboard. However, most of the work on address parsing has been confined to academic endeavours with little availability of free and easy-to-use open-source solutions. This paper presents Deepparse, a Python open-source, extendable, fine-tunable address parsing solution under LGPL-3.0 licence to parse multinational addresses using state-of-the-art deep learning algorithms and evaluated on over 60 countries. It can parse addresses written in any language and use any address standard. The pre-trained model achieves average $99~\\%$ parsing accuracies on the countries used for training with no pre-processing nor post-processing needed. Moreover, the library supports fine-tuning with new data to generate a custom address parser",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Beauchemin",
      "Marouane Yassine"
    ]
  },
  "http://arxiv.org/abs/2311.11844": {
    "title": "How to Use Large Language Models for Text Coding: The Case of Fatherhood Roles in Public Policy Documents",
    "volume": "Nov",
    "abstract": "Recent advances in large language models (LLMs) like GPT-3 and GPT-4 have opened up new opportunities for text analysis in political science. They promise automation with better results and less programming. In this study, we evaluate LLMs on three original coding tasks of non-English political science texts, and we provide a detailed description of a general workflow for using LLMs for text coding in political science research. Our use case offers a practical guide for researchers looking to incorporate LLMs into their research on text analysis. We find that, when provided with detailed label definitions and coding examples, an LLM can be as good as or even better than a human annotator while being much faster (up to hundreds of times), considerably cheaper (costing up to 60% less than human coding), and much easier to scale to large amounts of text. Overall, LLMs present a viable option for most text coding projects",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Lupo",
      "Oscar Magnusson",
      "Dirk Hovy",
      "Elin Naurin",
      "Lena Wängnerud"
    ]
  },
  "http://arxiv.org/abs/2311.11829": {
    "title": "System 2 Attention (is something you might need too)",
    "volume": "Nov",
    "abstract": "Soft attention in Transformer-based Large Language Models (LLMs) is susceptible to incorporating irrelevant information from the context into its latent representations, which adversely affects next token generations. To help rectify these issues, we introduce System 2 Attention (S2A), which leverages the ability of LLMs to reason in natural language and follow instructions in order to decide what to attend to. S2A regenerates the input context to only include the relevant portions, before attending to the regenerated context to elicit the final response. In experiments, S2A outperforms standard attention-based LLMs on three tasks containing opinion or irrelevant information, QA, math word problems and longform generation, where S2A increases factuality and objectivity, and decreases sycophancy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jason Weston",
      "Sainbayar Sukhbaatar"
    ]
  },
  "http://arxiv.org/abs/2311.11813": {
    "title": "Efficient Grammatical Error Correction Via Multi-Task Training and Optimized Training Schedule",
    "volume": "Nov",
    "abstract": "Progress in neural grammatical error correction (GEC) is hindered by the lack of annotated training data. Sufficient amounts of high-quality manually annotated data are not available, so recent research has relied on generating synthetic data, pretraining on it, and then fine-tuning on real datasets; performance gains have been achieved either by ensembling or by using huge pretrained models such as XXL-T5 as the backbone. In this work, we explore an orthogonal direction: how to use available data more efficiently. First, we propose auxiliary tasks that exploit the alignment between the original and corrected sentences, such as predicting a sequence of corrections. We formulate each task as a sequence-to-sequence problem and perform multi-task training. Second, we discover that the order of datasets used for training and even individual instances within a dataset may have important effects on the final performance, so we set out to find the best training schedule. Together, these two ideas lead to significant improvements, producing results that improve state of the art with much smaller models; in particular, we outperform the best models based on T5-XXL (11B parameters) with a BART-based model (400M parameters)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrey Bout",
      "Alexander Podolskiy",
      "Sergey Nikolenko",
      "Irina Piontkovskaya"
    ]
  },
  "http://arxiv.org/abs/2311.11797": {
    "title": "Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents",
    "volume": "Nov",
    "abstract": "Large language models (LLMs) have dramatically enhanced the field of language intelligence, as demonstrably evidenced by their formidable empirical performance across a spectrum of complex reasoning tasks. Additionally, theoretical proofs have illuminated their emergent reasoning capabilities, providing a compelling showcase of their advanced cognitive abilities in linguistic contexts. Critical to their remarkable efficacy in handling complex reasoning tasks, LLMs leverage the intriguing chain-of-thought (CoT) reasoning techniques, obliging them to formulate intermediate steps en route to deriving an answer. The CoT reasoning approach has not only exhibited proficiency in amplifying reasoning performance but also in enhancing interpretability, controllability, and flexibility. In light of these merits, recent research endeavors have extended CoT reasoning methodologies to nurture the development of autonomous language agents, which adeptly adhere to language instructions and execute actions within varied environments. This survey paper orchestrates a thorough discourse, penetrating vital research dimensions, encompassing: (i) the foundational mechanics of CoT techniques, with a focus on elucidating the circumstances and justification behind its efficacy; (ii) the paradigm shift in CoT; and (iii) the burgeoning of language agents fortified by CoT approaches. Prospective research avenues envelop explorations into generalization, efficiency, customization, scaling, and safety. This paper caters to a wide audience, including beginners seeking comprehensive knowledge of CoT reasoning and language agents, as well as experienced researchers interested in foundational mechanics and engaging in cutting-edge discussions on these topics. A repository for the related papers is available at https://github.com/Zoeyyao27/CoT-Igniting-Agent",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuosheng Zhang",
      "Yao Yao",
      "Aston Zhang",
      "Xiangru Tang",
      "Xinbei Ma",
      "Zhiwei He",
      "Yiming Wang",
      "Mark Gerstein",
      "Rui Wang",
      "Gongshen Liu",
      "Hai Zhao"
    ]
  },
  "http://arxiv.org/abs/2311.11796": {
    "title": "Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems",
    "volume": "Nov",
    "abstract": "Artificial Intelligence (AI) systems such as autonomous vehicles, facial recognition, and speech recognition systems are increasingly integrated into our daily lives. However, despite their utility, these AI systems are vulnerable to a wide range of attacks such as adversarial, backdoor, data poisoning, membership inference, model inversion, and model stealing attacks. In particular, numerous attacks are designed to target a particular model or system, yet their effects can spread to additional targets, referred to as transferable attacks. Although considerable efforts have been directed toward developing transferable attacks, a holistic understanding of the advancements in transferable attacks remains elusive. In this paper, we comprehensively explore learning-based attacks from the perspective of transferability, particularly within the context of cyber-physical security. We delve into different domains -- the image, text, graph, audio, and video domains -- to highlight the ubiquitous and pervasive nature of transferable attacks. This paper categorizes and reviews the architecture of existing attacks from various viewpoints: data, process, model, and system. We further examine the implications of transferable attacks in practical scenarios such as autonomous driving, speech recognition, and large language models (LLMs). Additionally, we outline the potential research directions to encourage efforts in exploring the landscape of transferable attacks. This survey offers a holistic understanding of the prevailing transferable attacks and their impacts across different domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangjing Wang",
      "Ce Zhou",
      "Yuanda Wang",
      "Bocheng Chen",
      "Hanqing Guo",
      "Qiben Yan"
    ]
  },
  "http://arxiv.org/abs/2311.11745": {
    "title": "Encoding Speaker-Specific Latent Speech Feature for Speech Synthesis",
    "volume": "Nov",
    "abstract": "In this work, we propose a novel method for modeling numerous speakers, which enables expressing the overall characteristics of speakers in detail like a trained multi-speaker model without additional training on the target speaker's dataset. Although various works with similar purposes have been actively studied, their performance has not yet reached that of trained multi-speaker models due to their fundamental limitations. To overcome previous limitations, we propose effective methods for feature learning and representing target speakers' speech characteristics by discretizing the features and conditioning them to a speech synthesis model. Our method obtained a significantly higher similarity mean opinion score (SMOS) in subjective similarity evaluation than seen speakers of a best-performing multi-speaker model, even with unseen speakers. The proposed method also outperforms a zero-shot method by significant margins. Furthermore, our method shows remarkable performance in generating new artificial speakers. In addition, we demonstrate that the encoded latent features are sufficiently informative to reconstruct an original speaker's speech completely. It implies that our method can be used as a general methodology to encode and reconstruct speakers' characteristics in various tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungil Kong",
      "Junmo Lee",
      "Jeongmin Kim",
      "Beomjeong Kim",
      "Jihoon Park",
      "Dohee Kong",
      "Changheon Lee",
      "Sangjin Kim"
    ]
  },
  "http://arxiv.org/abs/2311.11701": {
    "title": "Control in Hybrid Chatbots",
    "volume": "Nov",
    "abstract": "Customer data typically is held in database systems, which can be seen as rule-based knowledge base, whereas businesses increasingly want to benefit from the capabilities of large, pre-trained language models. In this technical report, we describe a case study of how a commercial rule engine and an integrated neural chatbot may be integrated, and what level of control that particular integration mode leads to. We also discuss alternative ways (including past ways realized in other systems) how researchers strive to maintain control and avoid what has recently been called model \"hallucination\"",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Rüdel",
      "Jochen L. Leidner"
    ]
  },
  "http://arxiv.org/abs/2311.11696": {
    "title": "Sparse Low-rank Adaptation of Pre-trained Language Models",
    "volume": "Nov",
    "abstract": "Fine-tuning pre-trained large language models in a parameter-efficient manner is widely studied for its effectiveness and efficiency. The popular method of low-rank adaptation (LoRA) offers a notable approach, hypothesizing that the adaptation process is intrinsically low-dimensional. Although LoRA has demonstrated commendable performance, it is implemented with a fixed and unalterable intrinsic rank that might not always be the ideal choice. Recognizing the need for more flexible adaptation, we extend the methodology of LoRA to an innovative approach we call sparse low-rank adaptation (SoRA) that enables dynamic adjustments to the intrinsic rank during the adaptation process. We achieve this through the incorporation of a gate unit optimized with proximal gradient method in the training stage, controlling the cardinality of rank under the sparsity of the gate. In the subsequent inference stage, we eliminate the parameter blocks corresponding to the zeroed-out ranks, to reduce each SoRA module back to a concise yet rank-optimal LoRA. Our approach strengthens the representation power of LoRA by initializing it with a higher rank, while efficiently taming a temporarily increased number of parameters via updating in a sparse way. We further introduce a sparsifying scheduler for SoRA, aiming to examine the impact of the number of non-zero parameters on the model's memorization and generalization. Our experimental results demonstrate that SoRA can outperform other baselines even with 70% retained parameters and 70% training time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ning Ding",
      "Xingtai Lv",
      "Qiaosen Wang",
      "Yulin Chen",
      "Bowen Zhou",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "http://arxiv.org/abs/2311.11690": {
    "title": "Refactoring Programs Using Large Language Models with Few-Shot Examples",
    "volume": "Nov",
    "abstract": "A less complex and more straightforward program is a crucial factor that enhances its maintainability and makes writing secure and bug-free programs easier. However, due to its heavy workload and the risks of breaking the working programs, programmers are reluctant to do code refactoring, and thus, it also causes the loss of potential learning experiences. To mitigate this, we demonstrate the application of using a large language model (LLM), GPT-3.5, to suggest less complex versions of the user-written Python program, aiming to encourage users to learn how to write better programs. We propose a method to leverage the prompting with few-shot examples of the LLM by selecting the best-suited code refactoring examples for each target programming problem based on the prior evaluation of prompting with the one-shot example. The quantitative evaluation shows that 95.68% of programs can be refactored by generating 10 candidates each, resulting in a 17.35% reduction in the average cyclomatic complexity and a 25.84% decrease in the average number of lines after filtering only generated programs that are semantically correct. Furthermore, the qualitative evaluation shows outstanding capability in code formatting, while unnecessary behaviors such as deleting or translating comments are also observed",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atsushi Shirafuji",
      "Yusuke Oda",
      "Jun Suzuki",
      "Makoto Morishita",
      "Yutaka Watanobe"
    ]
  },
  "http://arxiv.org/abs/2311.11608": {
    "title": "Taiyi: A Bilingual Fine-Tuned Large Language Model for Diverse Biomedical Tasks",
    "volume": "Nov",
    "abstract": "Recent advancements in large language models (LLMs) have shown promising results across a variety of natural language processing (NLP) tasks. The application of LLMs to specific domains, such as biomedicine, has achieved increased attention. However, most biomedical LLMs focus on enhancing performance in monolingual biomedical question answering and conversation tasks. To further investigate the effectiveness of the LLMs on diverse biomedical NLP tasks in different languages, we present Taiyi, a bilingual (English and Chinese) fine-tuned LLM for diverse biomedical tasks. In this work, we first curated a comprehensive collection of 140 existing biomedical text mining datasets across over 10 task types. Subsequently, a two-stage strategy is proposed for supervised fine-tuning to optimize the model performance across varied tasks. Experimental results on 13 test sets covering named entity recognition, relation extraction, text classification, question answering tasks demonstrate Taiyi achieves superior performance compared to general LLMs. The case study involving additional biomedical NLP tasks further shows Taiyi's considerable potential for bilingual biomedical multi-tasking. The source code, datasets, and model for Taiyi are freely available at https://github.com/DUTIR-BioNLP/Taiyi-LLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ling Luo",
      "Jinzhong Ning",
      "Yingwen Zhao",
      "Zhijun Wang",
      "Zeyuan Ding",
      "Peng Chen",
      "Weiru Fu",
      "Qinyu Han",
      "Guangtao Xu",
      "Yunzhi Qiu",
      "Dinghao Pan",
      "Jiru Li",
      "Hao Li",
      "Wenduo Feng",
      "Senbo Tu",
      "Yuqi Liu",
      "Zhihao Yang",
      "Jian Wang",
      "Yuanyuan Sun",
      "Hongfei Lin"
    ]
  },
  "http://arxiv.org/abs/2311.11601": {
    "title": "Addressing the Length Bias Problem in Document-Level Neural Machine Translation",
    "volume": "Nov",
    "abstract": "Document-level neural machine translation (DNMT) has shown promising results by incorporating more context information. However, this approach also introduces a length bias problem, whereby DNMT suffers from significant translation quality degradation when decoding documents that are much shorter or longer than the maximum sequence length during training. %i.e., the length bias problem. To solve the length bias problem, we propose to improve the DNMT model in training method, attention mechanism, and decoding strategy. Firstly, we propose to sample the training data dynamically to ensure a more uniform distribution across different sequence lengths. Then, we introduce a length-normalized attention mechanism to aid the model in focusing on target information, mitigating the issue of attention divergence when processing longer sequences. Lastly, we propose a sliding window strategy during decoding that integrates as much context information as possible without exceeding the maximum sequence length. The experimental results indicate that our method can bring significant improvements on several open datasets, and further analysis shows that our method can significantly alleviate the length bias problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuocheng Zhang",
      "Shuhao Gu",
      "Min Zhang",
      "Yang Feng"
    ]
  },
  "http://arxiv.org/abs/2311.11598": {
    "title": "Filling the Image Information Gap for VQA: Prompting Large Language Models to Proactively Ask Questions",
    "volume": "Nov",
    "abstract": "Large Language Models (LLMs) demonstrate impressive reasoning ability and the maintenance of world knowledge not only in natural language tasks, but also in some vision-language tasks such as open-domain knowledge-based visual question answering (OK-VQA). As images are invisible to LLMs, researchers convert images to text to engage LLMs into the visual question reasoning procedure. This leads to discrepancies between images and their textual representations presented to LLMs, which consequently impedes final reasoning performance. To fill the information gap and better leverage the reasoning capability, we design a framework that enables LLMs to proactively ask relevant questions to unveil more details in the image, along with filters for refining the generated information. We validate our idea on OK-VQA and A-OKVQA. Our method continuously boosts the performance of baselines methods by an average gain of 2.15% on OK-VQA, and achieves consistent improvements across different LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyue Wang",
      "Chi Chen",
      "Peng Li",
      "Yang Liu"
    ]
  },
  "http://arxiv.org/abs/2311.11583": {
    "title": "How well ChatGPT understand Malaysian English? An Evaluation on Named Entity Recognition and Relation Extraction",
    "volume": "Nov",
    "abstract": "Recently, ChatGPT has attracted a lot of interest from both researchers and the general public. While the performance of ChatGPT in named entity recognition and relation extraction from Standard English texts is satisfactory, it remains to be seen if it can perform similarly for Malaysian English. Malaysian English is unique as it exhibits morphosyntactic and semantical adaptation from local contexts. In this study, we assess ChatGPT's capability in extracting entities and relations from the Malaysian English News (MEN) dataset. We propose a three-step methodology referred to as \\textbf{\\textit{educate-predict-evaluate}}. The performance of ChatGPT is assessed using F1-Score across 18 unique prompt settings, which were carefully engineered for a comprehensive review. From our evaluation, we found that ChatGPT does not perform well in extracting entities from Malaysian English news articles, with the highest F1-Score of 0.497. Further analysis shows that the morphosyntactic adaptation in Malaysian English caused the limitation. However, interestingly, this morphosyntactic adaptation does not impact the performance of ChatGPT for relation extraction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohan Raj Chanthran",
      "Lay-Ki Soon",
      "Huey Fang Ong",
      "Bhawani Selvaretnam"
    ]
  },
  "http://arxiv.org/abs/2311.11564": {
    "title": "KBioXLM: A Knowledge-anchored Biomedical Multilingual Pretrained Language Model",
    "volume": "Nov",
    "abstract": "Most biomedical pretrained language models are monolingual and cannot handle the growing cross-lingual requirements. The scarcity of non-English domain corpora, not to mention parallel data, poses a significant hurdle in training multilingual biomedical models. Since knowledge forms the core of domain-specific corpora and can be translated into various languages accurately, we propose a model called KBioXLM, which transforms the multilingual pretrained model XLM-R into the biomedical domain using a knowledge-anchored approach. We achieve a biomedical multilingual corpus by incorporating three granularity knowledge alignments (entity, fact, and passage levels) into monolingual corpora. Then we design three corresponding training tasks (entity masking, relation masking, and passage relation prediction) and continue training on top of the XLM-R model to enhance its domain cross-lingual ability. To validate the effectiveness of our model, we translate the English benchmarks of multiple tasks into Chinese. Experimental results demonstrate that our model significantly outperforms monolingual and multilingual pretrained models in cross-lingual zero-shot and few-shot scenarios, achieving improvements of up to 10+ points. Our code is publicly available at https://github.com/ngwlh-gl/KBioXLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Geng",
      "Xu Yan",
      "Ziqiang Cao",
      "Juntao Li",
      "Wenjie Li",
      "Sujian Li",
      "Xinjie Zhou",
      "Yang Yang",
      "Jun Zhang"
    ]
  },
  "http://arxiv.org/abs/2311.11552": {
    "title": "Exploring Prompting Large Language Models as Explainable Metrics",
    "volume": "Nov",
    "abstract": "This paper describes the IUST NLP Lab submission to the Prompting Large Language Models as Explainable Metrics Shared Task at the Eval4NLP 2023 Workshop on Evaluation & Comparison of NLP Systems. We have proposed a zero-shot prompt-based strategy for explainable evaluation of the summarization task using Large Language Models (LLMs). The conducted experiments demonstrate the promising potential of LLMs as evaluation metrics in Natural Language Processing (NLP), particularly in the field of summarization. Both few-shot and zero-shot approaches are employed in these experiments. The performance of our best provided prompts achieved a Kendall correlation of 0.477 with human evaluations in the text summarization task on the test data. Code and results are publicly available on GitHub",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ghazaleh Mahmoudi"
    ]
  },
  "http://arxiv.org/abs/2311.11551": {
    "title": "Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning",
    "volume": "Nov",
    "abstract": "Large language models (LLMs) have showcased their capability with few-shot inference known as in-context learning. However, in-domain demonstrations are not always readily available in real scenarios, leading to cross-domain in-context learning. Besides, LLMs are still facing challenges in long-tail knowledge in unseen and unfamiliar domains. The above limitations demonstrate the necessity of Unsupervised Domain Adaptation (UDA). In this paper, we study the UDA problem under an in-context learning setting to adapt language models from the source domain to the target domain without any target labels. The core idea is to retrieve a subset of cross-domain elements that are the most similar to the query, and elicit language model to adapt in an in-context manner by learning both target domain distribution and the discriminative task signal simultaneously with the augmented cross-domain in-context examples. We devise different prompting and training strategies, accounting for different LM architectures to learn the target distribution via language modeling. With extensive experiments on Sentiment Analysis (SA) and Named Entity Recognition (NER) tasks, we thoroughly study the effectiveness of ICL for domain transfer and demonstrate significant improvements over baseline models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quanyu Long",
      "Wenya Wang",
      "Sinno Jialin Pan"
    ]
  },
  "http://arxiv.org/abs/2311.11518": {
    "title": "Multi-teacher Distillation for Multilingual Spelling Correction",
    "volume": "Nov",
    "abstract": "Accurate spelling correction is a critical step in modern search interfaces, especially in an era of mobile devices and speech-to-text interfaces. For services that are deployed around the world, this poses a significant challenge for multilingual NLP: spelling errors need to be caught and corrected in all languages, and even in queries that use multiple languages. In this paper, we tackle this challenge using multi-teacher distillation. On our approach, a monolingual teacher model is trained for each language/locale, and these individual models are distilled into a single multilingual student model intended to serve all languages/locales. In experiments using open-source data as well as user data from a worldwide search service, we show that this leads to highly effective spelling correction models that can meet the tight latency requirements of deployed services",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingfen Zhang",
      "Xuan Guo",
      "Sravan Bodapati",
      "Christopher Potts"
    ]
  },
  "http://arxiv.org/abs/2311.11516": {
    "title": "GPT in Data Science: A Practical Exploration of Model Selection",
    "volume": "Nov",
    "abstract": "There is an increasing interest in leveraging Large Language Models (LLMs) for managing structured data and enhancing data science processes. Despite the potential benefits, this integration poses significant questions regarding their reliability and decision-making methodologies. It highlights the importance of various factors in the model selection process, including the nature of the data, problem type, performance metrics, computational resources, interpretability vs accuracy, assumptions about data, and ethical considerations. Our objective is to elucidate and express the factors and assumptions guiding GPT-4's model selection recommendations. We employ a variability model to depict these factors and use toy datasets to evaluate both the model and the implementation of the identified heuristics. By contrasting these outcomes with heuristics from other platforms, our aim is to determine the effectiveness and distinctiveness of GPT-4's methodology. This research is committed to advancing our comprehension of AI decision-making processes, especially in the realm of model selection within data science. Our efforts are directed towards creating AI systems that are more transparent and comprehensible, contributing to a more responsible and efficient practice in data science",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathalia Nascimento",
      "Cristina Tavares",
      "Paulo Alencar",
      "Donald Cowan"
    ]
  },
  "http://arxiv.org/abs/2311.11509": {
    "title": "Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information",
    "volume": "Nov",
    "abstract": "In recent years, Large Language Models (LLM) have emerged as pivotal tools in various applications. However, these models are susceptible to adversarial prompt attacks, where attackers can carefully curate input strings that lead to undesirable outputs. The inherent vulnerability of LLMs stems from their input-output mechanisms, especially when presented with intensely out-of-distribution (OOD) inputs. This paper proposes a token-level detection method to identify adversarial prompts, leveraging the LLM's capability to predict the next token's probability. We measure the degree of the model's perplexity and incorporate neighboring token information to encourage the detection of contiguous adversarial prompt sequences. As a result, we propose two methods: one that identifies each token as either being part of an adversarial prompt or not, and another that estimates the probability of each token being part of an adversarial prompt",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengmian Hu",
      "Gang Wu",
      "Saayan Mitra",
      "Ruiyi Zhang",
      "Tong Sun",
      "Heng Huang",
      "Vishy Swaminathan"
    ]
  },
  "http://arxiv.org/abs/2311.11482": {
    "title": "Meta Prompting for AGI Systems",
    "volume": "Nov",
    "abstract": "This paper presents an in-depth exploration of Meta Prompting, a novel technique that revolutionizes the way large language models (LLMs), multi-modal foundation models, and AI systems approach problem-solving and data interpretation. Meta Prompting, rooted in type theory and category theory, prioritizes the structure and syntax of information, providing a unique framework that transcends traditional content-focused methods. We delve into the formal definitions of Meta Prompting, contrasting it with Few-Shot Prompting, and highlight its applicability and superiority in various AI applications. Key to this exploration is the expansion of Meta Prompting into the realm of complex reasoning. Here, we demonstrate how this technique adeptly breaks down intricate problems into manageable sub-problems, facilitating a step-by-step, detailed approach to problem-solving. This method proves especially advantageous in terms of token efficiency and offering a fair comparison in problem-solving scenarios, standing out against few-shot example approaches. Furthermore, the paper breaks new ground by extending Meta Prompting into multi-modal foundation model settings. This extension addresses the integration of diverse data types, such as images, audio, and video, within the structured framework of Meta Prompting, highlighting both the challenges and the vast potential of this approach in handling complex, multi-faceted data (The code is available at https://github.com/meta-prompting/meta-prompting)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Zhang"
    ]
  },
  "http://arxiv.org/abs/2311.11477": {
    "title": "What's left can't be right -- The remaining positional incompetence of contrastive vision-language models",
    "volume": "Nov",
    "abstract": "Contrastive vision-language models like CLIP have been found to lack spatial understanding capabilities. In this paper we discuss the possible causes of this phenomenon by analysing both datasets and embedding space. By focusing on simple left-right positional relations, we show that this behaviour is entirely predictable, even with large-scale datasets, demonstrate that these relations can be taught using synthetic data and show that this approach can generalise well to natural images - improving the performance on left-right relations on Visual Genome Relations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nils Hoehing",
      "Ellen Rushe",
      "Anthony Ventresque"
    ]
  },
  "http://arxiv.org/abs/2311.11462": {
    "title": "LLM aided semi-supervision for Extractive Dialog Summarization",
    "volume": "Nov",
    "abstract": "Generating high-quality summaries for chat dialogs often requires large labeled datasets. We propose a method to efficiently use unlabeled data for extractive summarization of customer-agent dialogs. In our method, we frame summarization as a question-answering problem and use state-of-the-art large language models (LLMs) to generate pseudo-labels for a dialog. We then use these pseudo-labels to fine-tune a chat summarization model, effectively transferring knowledge from the large LLM into a smaller specialized model. We demonstrate our method on the \\tweetsumm dataset, and show that using 10\\% of the original labelled data set we can achieve 65.9/57.0/61.0 ROUGE-1/-2/-L, whereas the current state-of-the-art trained on the entire training data set obtains 65.16/55.81/64.37 ROUGE-1/-2/-L. In other words, in the worst case (i.e., ROUGE-L) we still effectively retain 94.7% of the performance while using only 10% of the data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nishant Mishra",
      "Gaurav Sahu",
      "Iacer Calixto",
      "Ameen Abu-Hanna",
      "Issam H. Laradji"
    ]
  },
  "http://arxiv.org/abs/2311.11441": {
    "title": "Spot the Bot: Distinguishing Human-Written and Bot-Generated Texts Using Clustering and Information Theory Techniques",
    "volume": "Nov",
    "abstract": "With the development of generative models like GPT-3, it is increasingly more challenging to differentiate generated texts from human-written ones. There is a large number of studies that have demonstrated good results in bot identification. However, the majority of such works depend on supervised learning methods that require labelled data and/or prior knowledge about the bot-model architecture. In this work, we propose a bot identification algorithm that is based on unsupervised learning techniques and does not depend on a large amount of labelled data. By combining findings in semantic analysis by clustering (crisp and fuzzy) and information techniques, we construct a robust model that detects a generated text for different types of bot. We find that the generated texts tend to be more chaotic while literary works are more complex. We also demonstrate that the clustering of human texts results in fuzzier clusters in comparison to the more compact and well-separated clusters of bot-generated texts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vasilii Gromov",
      "Quynh Nhu Dang"
    ]
  },
  "http://arxiv.org/abs/2311.11435": {
    "title": "Unveiling Public Perceptions: Machine Learning-Based Sentiment Analysis of COVID-19 Vaccines in India",
    "volume": "Nov",
    "abstract": "In March 2020, the World Health Organisation declared COVID-19 a global pandemic as it spread to nearly every country. By mid-2021, India had introduced three vaccines: Covishield, Covaxin, and Sputnik. To ensure successful vaccination in a densely populated country like India, understanding public sentiment was crucial. Social media, particularly Reddit with over 430 million users, played a vital role in disseminating information. This study employs data mining techniques to analyze Reddit data and gauge Indian sentiments towards COVID-19 vaccines. Using Python's Text Blob library, comments are annotated to assess general sentiments. Results show that most Reddit users in India expressed neutrality about vaccination, posing a challenge for the Indian government's efforts to vaccinate a significant portion of the population",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Milind Gupta",
      "Abhishek Kaushik"
    ]
  },
  "http://arxiv.org/abs/2311.11415": {
    "title": "A Security Risk Taxonomy for Large Language Models",
    "volume": "Nov",
    "abstract": "As large language models (LLMs) permeate more and more applications, an assessment of their associated security risks becomes increasingly necessary. The potential for exploitation by malicious actors, ranging from disinformation to data breaches and reputation damage, is substantial. This paper addresses a gap in current research by focusing on the security risks posed by LLMs, which extends beyond the widely covered ethical and societal implications. Our work proposes a taxonomy of security risks along the user-model communication pipeline, explicitly focusing on prompt-based attacks on LLMs. We categorize the attacks by target and attack type within a prompt-based interaction scheme. The taxonomy is reinforced with specific attack examples to showcase the real-world impact of these risks. Through this taxonomy, we aim to inform the development of robust and secure LLM applications, enhancing their safety and trustworthiness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erik Derner",
      "Kristina Batistič",
      "Jan Zahálka",
      "Robert Babuška"
    ]
  },
  "http://arxiv.org/abs/2311.11375": {
    "title": "ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding",
    "volume": "Nov",
    "abstract": "Spoken language understanding (SLU) is a fundamental task in the task-oriented dialogue systems. However, the inevitable errors from automatic speech recognition (ASR) usually impair the understanding performance and lead to error propagation. Although there are some attempts to address this problem through contrastive learning, they (1) treat clean manual transcripts and ASR transcripts equally without discrimination in fine-tuning; (2) neglect the fact that the semantically similar pairs are still pushed away when applying contrastive learning; (3) suffer from the problem of Kullback-Leibler (KL) vanishing. In this paper, we propose Mutual Learning and Large-Margin Contrastive Learning (ML-LMCL), a novel framework for improving ASR robustness in SLU. Specifically, in fine-tuning, we apply mutual learning and train two SLU models on the manual transcripts and the ASR transcripts, respectively, aiming to iteratively share knowledge between these two models. We also introduce a distance polarization regularizer to avoid pushing away the intra-cluster pairs as much as possible. Moreover, we use a cyclical annealing schedule to mitigate KL vanishing issue. Experiments on three datasets show that ML-LMCL outperforms existing models and achieves new state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuxin Cheng",
      "Bowen Cao",
      "Qichen Ye",
      "Zhihong Zhu",
      "Hongxiang Li",
      "Yuexian Zou"
    ]
  },
  "http://arxiv.org/abs/2311.11331": {
    "title": "Portuguese FAQ for Financial Services",
    "volume": "Nov",
    "abstract": "Scarcity of domain-specific data in the Portuguese financial domain has disfavored the development of Natural Language Processing (NLP) applications. To address this limitation, the present study advocates for the utilization of synthetic data generated through data augmentation techniques. The investigation focuses on the augmentation of a dataset sourced from the Central Bank of Brazil FAQ, employing techniques that vary in semantic similarity. Supervised and unsupervised tasks are conducted to evaluate the impact of augmented data on both low and high semantic similarity scenarios. Additionally, the resultant dataset will be publicly disseminated on the Hugging Face Datasets platform, thereby enhancing accessibility and fostering broader engagement within the NLP research community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paulo Finardi",
      "Wanderley M. Melo",
      "Edgard D. Medeiros Neto",
      "Alex F. Mansano",
      "Pablo B. Costa",
      "Vinicius F. Caridá"
    ]
  },
  "http://arxiv.org/abs/2311.11301": {
    "title": "CHAMP: Efficient Annotation and Consolidation of Cluster Hierarchies",
    "volume": "Nov",
    "abstract": "Various NLP tasks require a complex hierarchical structure over nodes, where each node is a cluster of items. Examples include generating entailment graphs, hierarchical cross-document coreference resolution, annotating event and subevent relations, etc. To enable efficient annotation of such hierarchical structures, we release CHAMP, an open source tool allowing to incrementally construct both clusters and hierarchy simultaneously over any type of texts. This incremental approach significantly reduces annotation time compared to the common pairwise annotation approach and also guarantees maintaining transitivity at the cluster and hierarchy levels. Furthermore, CHAMP includes a consolidation mode, where an adjudicator can easily compare multiple cluster hierarchy annotations and resolve disagreements",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arie Cattan",
      "Tom Hope",
      "Doug Downey",
      "Roy Bar-Haim",
      "Lilach Eden",
      "Yoav Kantor",
      "Ido Dagan"
    ]
  },
  "http://arxiv.org/abs/2311.11271": {
    "title": "A Cross-Attention Augmented Model for Event-Triggered Context-Aware Story Generation",
    "volume": "Nov",
    "abstract": "Despite recent advancements, existing story generation systems continue to encounter difficulties in effectively incorporating contextual and event features, which greatly influence the quality of generated narratives. To tackle these challenges, we introduce a novel neural generation model, EtriCA, that enhances the relevance and coherence of generated stories by employing a cross-attention mechanism to map context features onto event sequences through residual mapping. This feature capturing mechanism enables our model to exploit logical relationships between events more effectively during the story generation process. To further enhance our proposed model, we employ a post-training framework for knowledge enhancement (KeEtriCA) on a large-scale book corpus. This allows EtriCA to adapt to a wider range of data samples. This results in approximately 5\\% improvement in automatic metrics and over 10\\% improvement in human evaluation. We conduct extensive experiments, including comparisons with state-of-the-art (SOTA) baseline models, to evaluate the performance of our framework on story generation. The experimental results, encompassing both automated metrics and human assessments, demonstrate the superiority of our model over existing state-of-the-art baselines. These results underscore the effectiveness of our model in leveraging context and event features to improve the quality of generated narratives",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Tang",
      "Tyler Loakman",
      "Chenghua Lin"
    ]
  },
  "http://arxiv.org/abs/2311.11268": {
    "title": "Towards Real-World Writing Assistance: A Chinese Character Checking Benchmark with Faked and Misspelled Characters",
    "volume": "Nov",
    "abstract": "Writing assistance is an application closely related to human life and is also a fundamental Natural Language Processing (NLP) research field. Its aim is to improve the correctness and quality of input texts, with character checking being crucial in detecting and correcting wrong characters. From the perspective of the real world where handwriting occupies the vast majority, characters that humans get wrong include faked characters (i.e., untrue characters created due to writing errors) and misspelled characters (i.e., true characters used incorrectly due to spelling errors). However, existing datasets and related studies only focus on misspelled characters mainly caused by phonological or visual confusion, thereby ignoring faked characters which are more common and difficult. To break through this dilemma, we present Visual-C$^3$, a human-annotated Visual Chinese Character Checking dataset with faked and misspelled Chinese characters. To the best of our knowledge, Visual-C$^3$ is the first real-world visual and the largest human-crafted dataset for the Chinese character checking scenario. Additionally, we also propose and evaluate novel baseline methods on Visual-C$^3$. Extensive empirical results and analyses show that Visual-C$^3$ is high-quality yet challenging. The Visual-C$^3$ dataset and the baseline methods will be publicly available to facilitate further research in the community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinghui Li",
      "Zishan Xu",
      "Shaoshen Chen",
      "Haojing Huang",
      "Yangning Li",
      "Yong Jiang",
      "Zhongli Li",
      "Qingyu Zhou",
      "Hai-Tao Zheng",
      "Ying Shen"
    ]
  },
  "http://arxiv.org/abs/2311.11267": {
    "title": "Rethinking Large Language Models in Mental Health Applications",
    "volume": "Nov",
    "abstract": "Large Language Models (LLMs) have become valuable assets in mental health, showing promise in both classification tasks and counseling applications. This paper offers a perspective on using LLMs in mental health applications. It discusses the instability of generative models for prediction and the potential for generating hallucinatory outputs, underscoring the need for ongoing audits and evaluations to maintain their reliability and dependability. The paper also distinguishes between the often interchangeable terms ``explainability'' and ``interpretability'', advocating for developing inherently interpretable methods instead of relying on potentially hallucinated self-explanations generated by LLMs. Despite the advancements in LLMs, human counselors' empathetic understanding, nuanced interpretation, and contextual awareness remain irreplaceable in the sensitive and complex realm of mental health counseling. The use of LLMs should be approached with a judicious and considerate mindset, viewing them as tools that complement human expertise rather than seeking to replace it",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaoxiong Ji",
      "Tianlin Zhang",
      "Kailai Yang",
      "Sophia Ananiadou",
      "Erik Cambria"
    ]
  },
  "http://arxiv.org/abs/2311.11229": {
    "title": "Causal ATE Mitigates Unintended Bias in Controlled Text Generation",
    "volume": "Nov",
    "abstract": "We study attribute control in language models through the method of Causal Average Treatment Effect (Causal ATE). Existing methods for the attribute control task in Language Models (LMs) check for the co-occurrence of words in a sentence with the attribute of interest, and control for them. However, spurious correlation of the words with the attribute in the training dataset, can cause models to hallucinate the presence of the attribute when presented with the spurious correlate during inference. We show that the simple perturbation-based method of Causal ATE removes this unintended effect. Additionally, we offer a theoretical foundation for investigating Causal ATE in the classification task, and prove that it reduces the number of false positives -- thereby mitigating the issue of unintended bias. Specifically, we ground it in the problem of toxicity mitigation, where a significant challenge lies in the inadvertent bias that often emerges towards protected groups post detoxification. We show that this unintended bias can be solved by the use of the Causal ATE metric",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rahul Madhavan",
      "Kahini Wadhawan"
    ]
  },
  "http://arxiv.org/abs/2311.11215": {
    "title": "SPLAIN: Augmenting CybersecurityWarnings with Reasons and Data",
    "volume": "Nov",
    "abstract": "Effective cyber threat recognition and prevention demand comprehensible forecasting systems, as prior approaches commonly offer limited and, ultimately, unconvincing information. We introduce Simplified Plaintext Language (SPLAIN), a natural language generator that converts warning data into user-friendly cyber threat explanations. SPLAIN is designed to generate clear, actionable outputs, incorporating hierarchically organized explanatory details about input data and system functionality. Given the inputs of individual sensor-induced forecasting signals and an overall warning from a fusion module, SPLAIN queries each signal for information on contributing sensors and data signals. This collected data is processed into a coherent English explanation, encompassing forecasting, sensing, and data elements for user review. SPLAIN's template-based approach ensures consistent warning structure and vocabulary. SPLAIN's hierarchical output structure allows each threat and its components to be expanded to reveal underlying explanations on demand. Our conclusions emphasize the need for designers to specify the \"how\" and \"why\" behind cyber warnings, advocate for simple structured templates in generating consistent explanations, and recognize that direct causal links in Machine Learning approaches may not always be identifiable, requiring some explanations to focus on general methodologies, such as model and training data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vera A. Kazakova",
      "Jena D. Hwang",
      "Bonnie J. Dorr",
      "Yorick Wilks",
      "J. Blake Gage",
      "Alex Memory",
      "Mark A. Clark"
    ]
  },
  "http://arxiv.org/abs/2311.11202": {
    "title": "Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models",
    "volume": "Nov",
    "abstract": "Language models have shown promise in various tasks but can be affected by undesired data during training, fine-tuning, or alignment. For example, if some unsafe conversations are wrongly annotated as safe ones, the model fine-tuned on these samples may be harmful. Therefore, the correctness of annotations, i.e., the credibility of the dataset, is important. This study focuses on the credibility of real-world datasets, including the popular benchmarks Jigsaw Civil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that can be used for training a harmless language model. Given the cost and difficulty of cleaning these datasets by humans, we introduce a systematic framework for evaluating the credibility of datasets, identifying label errors, and evaluating the influence of noisy labels in the curated language data, specifically focusing on unsafe comments and conversation classification. With the framework, we find and fix an average of 6.16% label errors in 11 datasets constructed from the above benchmarks. The data credibility and downstream learning performance can be remarkably improved by directly fixing label errors, indicating the significance of cleaning existing real-world datasets. Open-source: https://github.com/Docta-ai/docta",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaowei Zhu",
      "Jialu Wang",
      "Hao Cheng",
      "Yang Liu"
    ]
  }
}