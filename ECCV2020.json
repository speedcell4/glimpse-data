{
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/267_ECCV_2020_paper.php": {
    "title": "Quaternion Equivariant Capsule Networks for 3D Point Clouds",
    "abstract": "We present a 3D capsule module for processing point clouds that is equivariant to 3D rotations and translations, as well as invariant to permutations of the input points. The operator receives a sparse set of local reference frames, computed from an input point cloud and establishes end-to-end transformation equivariance through a novel dynamic routing procedure on quaternions. Further, we theoretically connect dynamic routing between capsules to the well-known Weiszfeld algorithm, a scheme for solving iterative re-weighted least squares (IRLS) problems with provable convergence properties. It is shown that such group dynamic routing can be interpreted as robust IRLS rotation averaging on capsule votes, where information is routed based on the final inlier scores. Based on our operator, we build a capsule network that disentangles geometry from pose, paving the way for more informative descriptors and a structured latent space. Our architecture allows joint object classification and orientation estimation without explicit supervision of rotations. We validate our algorithm empirically on common benchmark datasets",
    "volume": "main",
    "checked": true,
    "id": "4ea814b1c76995e445ccf0621b0e775ff53a1db6",
    "citation_count": 52
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/283_ECCV_2020_paper.php": {
    "title": "DeepFit: 3D Surface Fitting via Neural Network Weighted Least Squares",
    "abstract": "We propose a surface fitting method for unstructured 3D point clouds. This method, called DeepFit, incorporates a neural network to learn point-wise weights for weighted least squares polynomial surface fitting. The learned weights act as a soft selection for the neighborhood of surface points thus avoiding the scale selection required of previous methods. To train the network we propose a novel surface consistency loss that improves point weight estimation. The method enables extracting normal vectors and other geometrical properties, such as principal curvatures, the latter were not presented as ground truth during training. We achieve state-of-the-art results on a benchmark normal and curvature estimation dataset,demonstrate robustness to noise, outliers and density variations, and show its application on noise removal",
    "volume": "main",
    "checked": true,
    "id": "8f61868b44f9b396f9870be47aed5e52aa21a2f0",
    "citation_count": 22
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/343_ECCV_2020_paper.php": {
    "title": "NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search",
    "abstract": "In this paper, we propose an efficient NAS algorithm for generating task-specific models that are competitive under multiple competing objectives. It comprises of two surrogates, one at the architecture level to improve sample efficiency and one at the weights level, through a supernet, to improve gradient descent training efficiency. On standard benchmark datasets (C10, C100, ImageNet), the resulting models, dubbed NSGANetV2, either match or outperform models from existing approaches with the search being orders of magnitude more sample efficient. Furthermore, we demonstrate the effectiveness and versatility of the proposed method on six diverse non-standard datasets, e.g. STL-10, Flowers102, Oxford Pets, FGVC Aircrafts etc. In all cases, NSGANetV2s improve the state-of-the-art (under mobile setting), suggesting that NAS can be a viable alternative to conventional transfer learning approaches in handling diverse scenarios such as small-scale or fine-grained datasets. Code is available at https://github.com/mikelzc1990/nsganetv2",
    "volume": "main",
    "checked": true,
    "id": "8054513b37839878c5ee9d541d228b0148bce306",
    "citation_count": 62
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/384_ECCV_2020_paper.php": {
    "title": "Describing Textures using Natural Language",
    "abstract": "Textures in natural images can be characterized by color, shape, periodicity of elements within them, and other attributes that can be described using natural language. In this paper, we study the problem of describing visual attributes of texture on a novel dataset containing rich descriptions of textures, and conduct a systematic study of current generative and discriminative models for grounding language to images on this dataset. We find that while these models capture some properties of texture, they fail to capture several compositional properties, such as the colors of dots. We provide critical analysis of existing models by generating synthetic but realistic textures with different descriptions. Our dataset also allows us to train interpretable models and generate language-based explanations of what discriminative features are learned by deep networks for fine-grained categorization where texture plays a key role. We present visualizations of several fine-grained domains and show that texture attributes learned on our dataset offer improvements over expert-designed attributes on the Caltech-UCSD Birds dataset",
    "volume": "main",
    "checked": true,
    "id": "86f0fb3791761cdb5e9108721a536d752641d4bf",
    "citation_count": 7
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/410_ECCV_2020_paper.php": {
    "title": "Empowering Relational Network by Self-Attention Augmented Conditional Random Fields for Group Activity Recognition",
    "abstract": "This paper presents a novel relational network for group activity recognition. The core of our network is to augment the conditional random fields (CRF), amenable to learning inter-dependency of correlated observations, with the newly devised temporal and spatial self-attention to learn the temporal evolution and spatial relational contexts of every actor in videos. Such a combination utilizes the global receptive fields of self-attention to construct a spatio-temporal graph topology to address the temporal dependency and non-local relationships of the actors. The network first uses the temporal self-attention along with the spatial self-attention, which considers multiple cliques with different scales of locality to account for the diversity of the actors' relationships in group activities, to model the pairwise energy of CRF. Afterward, to accommodate the distinct characteristics of each video, a new mean-field inference algorithm with dynamic halting is also addressed. Finally, a bidirectional universal transformer encoder (UTE), which combines both of the forward and backward temporal context information, is used to aggregate the relational contexts and scene information for group activity recognition. Simulations show that the proposed approach surpasses the state-of-the-art methods on the widespread Volleyball and Collective Activity datasets",
    "volume": "main",
    "checked": true,
    "id": "33f62e6f851da560037f1ed008d2eb51bb80f062",
    "citation_count": 16
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/445_ECCV_2020_paper.php": {
    "title": "AiR: Attention with Reasoning Capability",
    "abstract": "While attention has been an increasingly popular component in deep neural networks to both interpret and boost performance of models, little work has examined how attention progresses to accomplish a task and whether it is reasonable. In this work, we propose an Attention with Reasoning capability (AiR) framework that uses attention to understand and improve the process leading to task outcomes. We first define an evaluation metric based on a sequence of atomic reasoning operations, enabling quantitative measurement of attention that considers the reasoning process. We then collect human eye-tracking and answer correctness data, and analyze various machine and human attentions on their reasoning capability and how they impact task performance. Furthermore, we propose a supervision method to jointly and progressively optimize attention, reasoning, and task performance so that models learn to look at regions of interests by following a reasoning process. We demonstrate the effectiveness of the proposed framework in analyzing and modeling attention with better reasoning capability and task performance. The code and data are available at https://github.com/szzexpoi/AiR",
    "volume": "main",
    "checked": true,
    "id": "96150d2fdbb72e11882ad06b346d6e0d2e819d51",
    "citation_count": 15
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/500_ECCV_2020_paper.php": {
    "title": "Self6D: Self-Supervised Monocular 6D Object Pose Estimation",
    "abstract": "6D object pose estimation is a fundamental problem in computer vision. Convolutional Neural Networks (CNNs) have recently proven to be capable of predicting reliable 6D pose estimates even from monocular images. Nonetheless, CNNs are identified as being extremely data-driven, and acquiring adequate annotations is oftentimes very time-consuming and labor intensive. To overcome this shortcoming, we propose the idea of monocular 6D pose estimation by means of self-supervised learning, removing the need for real annotations. After training our proposed network fully supervised with synthetic RGB data, we leverage recent advances in neural rendering to further self-supervise the model on unannotated real RGB-D data, seeking for a visually and geometrically optimal alignment. Extensive evaluations demonstrate that our proposed self-supervision is able to significantly enhance the model's original performance, outperforming all other methods relying on synthetic data or employing elaborate techniques from the domain adaptation realm",
    "volume": "main",
    "checked": true,
    "id": "1d947ceedc5a16c4e8bfae6031a4098d45a9f670",
    "citation_count": 39
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/529_ECCV_2020_paper.php": {
    "title": "Invertible Image Rescaling",
    "abstract": "High-resolution digital images are usually downscaled to fit various display screens or save the cost of storage and bandwidth, meanwhile the post-upscaling is adpoted to recover the original resolutions or the details in the zoom-in images. However, typical image downscaling is a non-injective mapping due to the loss of high-frequency information, which leads to the ill-posed problem of the inverse upscaling procedure and poses great challenges for recovering details from the downscaled low-resolution images. Simply upscaling with image super-resolution methods results in unsatisfactory recovering performance. In this work, we propose to solve this problem by modeling the downscaling and upscaling processes from a new perspective, i.e. an invertible bijective transformation, which can largely mitigate the ill-posed nature of image upscaling. We develop an Invertible Rescaling Net (IRN) with deliberately designed framework and objectives to produce visually-pleasing low-resolution images and meanwhile capture the distribution of the lost information using a latent variable following a specified distribution in the downscaling process. In this way, upscaling is made tractable by inversely passing a randomly-drawn latent variable with the low-resolution image through the network. Experimental results demonstrate the significant improvement of our model over existing methods in terms of both quantitative and qualitative evaluations of image upscaling reconstruction from downscaled images. Code is available at https://github.com/pkuxmq/Invertible-Image-Rescaling",
    "volume": "main",
    "checked": true,
    "id": "407f1d16ba4eb3cb4851429cae46c97d723a35a5",
    "citation_count": 82
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/612_ECCV_2020_paper.php": {
    "title": "Synthesize then Compare: Detecting Failures and Anomalies for Semantic Segmentation",
    "abstract": "The ability to detect failures and anomalies are fundamental requirements for building reliable systems for computer vision applications, especially safety-critical applications of semantic segmentation, such as autonomous driving and medical image analysis. In this paper, we systematically study failure and anomaly detection for semantic segmentation and propose a unified framework, consisting of two modules, to address these two related problems. The first module is an image synthesis module, which generates a synthesized image from a segmentation layout map, and the second is a comparison module, which computes the difference between the synthesized image and the input image. We validate our framework on three challenging datasets and improve the state-of-the-arts by large margins, i.e., 6% AUPR-Error on Cityscapes, 7% Pearson correlation on pancreatic tumor segmentation in MSD and 20% AUPR on StreetHazards anomaly segmentation",
    "volume": "main",
    "checked": true,
    "id": "c707517507873bc2cdc489b6fd9af74770468c48",
    "citation_count": 62
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/677_ECCV_2020_paper.php": {
    "title": "House-GAN: Relational Generative Adversarial Networks for Graph-constrained House Layout Generation",
    "abstract": "This paper proposes a novel graph-constrained generative adversarial network, whose generator and discriminator are built upon relational architecture. The main idea is to encode the constraint into the graph structure of its relational networks. We have demonstrated the proposed architecture for a new house layout generation problem, whose task is to take an architectural constraint as a graph (i.e., the number and types of rooms with their spatial adjacency) and produce a set of axis-aligned bounding boxes of rooms. We measure the quality of generated house layouts with the three metrics: the realism, the diversity, and the compatibility with the input graph constraint. Our qualitative and quantitative evaluations over 117,000 real floorplan images demonstrate. We will publicly share all our code and data",
    "volume": "main",
    "checked": true,
    "id": "e8a026d2dd10af43f8bc7fc9fd7a4f888c74efe0",
    "citation_count": 64
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/736_ECCV_2020_paper.php": {
    "title": "Crowdsampling the Plenoptic Function",
    "abstract": "Many popular tourist landmarks are captured in a multitude of online, public photos. These photos represent a sparse and unstructured sampling of the plenoptic function for a particular scene. In this paper,we present a new approach to novel view synthesis under time-varying illumination from such data. Our approach builds on the recent multi-plane image (MPI) format for representing local light fields under fixed viewing conditions. We introduce a new DeepMPI representation, motivated by observations on the sparsity structure of the plenoptic function, that allows for real-time synthesis of photorealistic views that are continuous in both space and across changes in lighting. Our method can synthesize the same compelling parallax and view-dependent effects as previous MPI methods, while simultaneously interpolating along changes in reflectance and illumination with time. We show how to learn a model of these effects in an unsupervised way from an unstructured collection of photos without temporal registration, demonstrating significant improvements over recent work in neural rendering. More information can be found crowdsampling.io",
    "volume": "main",
    "checked": true,
    "id": "37d29d649e47065e5b3f6e5a5151dd077fe85cd0",
    "citation_count": 43
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/738_ECCV_2020_paper.php": {
    "title": "VoxelPose: Towards Multi-Camera 3D Human Pose Estimation in Wild Environment",
    "abstract": "We present mph{VoxelPose} to estimate $3$D poses of multiple people from multiple camera views. In contrast to the previous efforts which require to establish cross-view correspondence based on noisy and incomplete $2$D pose estimates, mph{VoxelPose} directly operates in the $3$D space therefore avoids making incorrect decisions in each camera view. To achieve this goal, features in all camera views are aggregated in the $3$D voxel space and fed into mph{Cuboid Proposal Network} (CPN) to localize all people. Then we propose mph{Pose Regression Network} (PRN) to estimate a detailed $3$D pose for each proposal. The approach is robust to occlusion which occurs frequently in practice. Without bells and whistles, it outperforms the previous methods on several public datasets",
    "volume": "main",
    "checked": true,
    "id": "828c5100bd9c7d649c4f6aa9a1443838c5a6482a",
    "citation_count": 55
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/832_ECCV_2020_paper.php": {
    "title": "End-to-End Object Detection with Transformers",
    "abstract": "We present a new method that views object detection as a direct set prediction. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, we show that DETR can be easily generalized to produce a competitive panoptic segmentation prediction in a unified manner",
    "volume": "main",
    "checked": true,
    "id": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e",
    "citation_count": 3432
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/840_ECCV_2020_paper.php": {
    "title": "DeepSFM: Structure From Motion Via Deep Bundle Adjustment",
    "abstract": "Structure from motion (SfM) is an essential computer vision problem which has not been well handled by deep learning. One of the promising trends is to apply explicit structural constraint, e.g. 3D cost volume, into the network. However, existing methods usually assume accurate camera poses either from GT or other methods, which is unrealistic in practice. In this work, we design a physical driven architecture, namely DeepSFM, inspired by traditional Bundle Adjustment (BA), which consists of two cost volume based architectures for depth and pose estimation respectively, iteratively running to improve both. The explicit constraints on both depth (structure) and pose (motion), when combined with the learning components, bring the merit from both traditional BA and emerging deep learning technology. Extensive experiments on various datasets show that our model achieves the state-of-the-art performance on both depth and pose estimation with superior robustness against less number of inputs and the noise in initialization",
    "volume": "main",
    "checked": true,
    "id": "41e5fe4095500d483499d870a5415a9d8f0a854a",
    "citation_count": 42
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1044_ECCV_2020_paper.php": {
    "title": "Ladybird: Quasi-Monte Carlo Sampling for Deep Implicit Field Based 3D Reconstruction with Symmetry",
    "abstract": "Deep implicit field regression methods are effective for 3D reconstruction from single-view images. However, the impact of different sampling patterns on the reconstruction quality is not well-understood. In this work, we first study the effect of point set discrepancy on the network training. Based on Farthest Point Sampling algorithm, we propose a sampling scheme that theoretically encourages better generalization performance, and results in fast convergence for SGD-based optimization algorithms. Secondly, based on the reflective symmetry of an object, we propose a feature fusion method that alleviates issues due to self-occlusions which makes it difficult to utilize local image features. Our proposed system Ladybird is able to create high quality 3D object reconstructions from a single input image. We evaluate Ladybird on a large scale 3D dataset (ShapeNet) demonstrating highly competitive results in terms of Chamfer distance, Earth Mover's distance and Intersection Over Union (IoU)",
    "volume": "main",
    "checked": true,
    "id": "16158867c91f1819f5a646731c6e1da5505a8dd5",
    "citation_count": 26
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1059_ECCV_2020_paper.php": {
    "title": "Segment as Points for Efficient Online Multi-Object Tracking and Segmentation",
    "abstract": "Current multi-object tracking and segmentation (MOTS) methods follow the tracking-by-detection paradigm and adopt convolutions for feature extraction. However, as affected by the inherent receptive field, convolution based feature extraction inevitably mixes up the foreground features and the background features, resulting in ambiguities in the subsequent instance association. In this paper, we propose a highly effective method for learning instance embeddings based on segments by converting the compact image representation to un-ordered 2D point cloud representation. Our method generates a new tracking-by-points paradigm where discriminative instance embeddings are learned from randomly selected points rather than images. Furthermore, multiple informative data modalities are converted into point-wise representations to enrich point-wise features. The resulting online MOTS framework, named PointTrack, surpasses all the state-of-the-art methods including 3D tracking methods by large margins (5.4\\% higher MOTSA and 18 times faster over MOTSFusion) with the near real-time speed (22 FPS). Evaluations across three datasets demonstrate both the effectiveness and efficiency of our method. Moreover, based on the observation that current MOTS datasets lack crowded scenes, we build a more challenging MOTS dataset named APOLLO MOTS with higher instance density. Both APOLLO MOTS and our codes are publicly available at https://github.com/detectRecog/PointTrack",
    "volume": "main",
    "checked": true,
    "id": "000458c2651ab147d503429f6460aa80155e8e35",
    "citation_count": 46
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1105_ECCV_2020_paper.php": {
    "title": "Conditional Convolutions for Instance Segmentation",
    "abstract": "We propose a simple yet effective instance segmentation framework, termed CondInst (conditional convolutions for instance segmentation). Top-performing instance segmentation methods such as Mask R-CNN rely on ROI operations (typically ROIPool or ROIAlign) to obtain the final instance masks. In contrast, we propose to solve instance segmentation from a new perspective. Instead of using instance-wise ROIs as inputs to a network of fixed weights, we employ dynamic instance-aware networks, conditioned on instances. CondInst enjoys two advantages: 1) Instance segmentation is solved by a fully convolutional network, eliminating the need for ROI cropping and feature alignment. 2) Due to the much improved capacity of dynamically-generated conditional convolutions, the mask head can be very compact (e.g., 3 conv. layers, each having only 8 channels), leading to significantly faster inference. We demonstrate a simpler instance segmentation method that can achieve improved performance in both accuracy and inference speed. On the COCO dataset, we outperform a few recent methods including well-tuned Mask R-CNN baselines, without longer training schedules needed. Code is available: https://git.io/AdelaiDet",
    "volume": "main",
    "checked": true,
    "id": "862f2e2e5ba7b8b83f42226170c634ecb02834e4",
    "citation_count": 251
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1196_ECCV_2020_paper.php": {
    "title": "MutualNet: Adaptive ConvNet via Mutual Learning from Network Width and Resolution",
    "abstract": "We propose the width-resolution mutual learning method (MutualNet) to train a network that is executable at dynamic resource constraints to achieve adaptive accuracy-efficiency trade-offs at runtime. Our method trains a cohort of sub-networks with different widths using different input resolutions to mutually learn multi-scale representations for each sub-network. It achieves consistently better ImageNet top-1 accuracy over the state-of-the-art adaptive network US-Net under different computation constraints, and outperforms the best compound scaled MobileNet in EfficientNet by 1.5%. The superiority of our method is also validated on COCO object detection and instance segmentation as well as transfer learning. Surprisingly, the training strategy of MutualNet can also boost the performance of a single network, which substantially outperforms the powerful AutoAugmentation in both efficiency (GPU search hours: 15000 vs. 0) and accuracy (ImageNet: 77.6% vs. 78.6%). Code is provided in supplementary material",
    "volume": "main",
    "checked": true,
    "id": "0087d72c72fe3098393f7bbb92032c38a78c43ae",
    "citation_count": 49
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1203_ECCV_2020_paper.php": {
    "title": "Fashionpedia: Ontology, Segmentation, and an Attribute Localization Dataset",
    "abstract": "Segmentation, and an Attribute Localization Dataset\",\"In this work, we focus on the task of instance segmentation with attribute localization. This unifies instance segmentation (detect and segment each object instance) and visual categorization of fine-grained attributes (classify one or multiple attributes). The proposed task requires both localizing an object and describing its properties. To illustrate the various aspects of this task, we focus on the domain of fashion and introduce Fashionpedia as a step toward mapping out the visual aspects of the fashion world. Fashionpedia consists of two parts: (1) an ontology built by fashion experts containing 27 main apparel categories, 19 apparel parts, and 294 fine-grained attributes and their relationships and (2) a dataset consisting of everyday and celebrity event fashion images annotated with segmentation masks and their associated fine-grained attributes, built upon the backbone of the Fashionpedia ontology. In order to solve this challenging task, we propose a novel Attribute-Mask R-CNN model to jointly perform instance segmentation and localized attribute recognition, and provide a novel evaluation metric for the task. Fashionpedia is available at https://fashionpedia.github.io/home/",
    "volume": "main",
    "checked": true,
    "id": "7c368af1d2d4bd52c7f54a3fafcb84abc931eccd",
    "citation_count": 35
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1273_ECCV_2020_paper.php": {
    "title": "Privacy Preserving Structure-from-Motion",
    "abstract": "Over the last years, visual localization and mapping solutions have been adopted by an increasing number of mixed reality and robotics systems. The recent trend towards cloud-based localization and mapping systems has raised significant privacy concerns. These are mainly grounded by the fact that these services require users to upload visual data to their servers, which can reveal potentially confidential information, even if only derived image features are uploaded. Recent research addresses some of these concerns for the task of image-based localization by concealing the geometry of the query images and database maps. The core idea of the approach is to lift 2D/3D feature points to random lines, while still providing sufficient constraints for camera pose estimation. In this paper, we further build upon this idea and propose solutions to the different core algorithms of an incremental Structure-from-Motion pipeline based on random line features. With this work, we make another fundamental step towards enabling privacy preserving cloud-based mapping solutions. Various experiments on challenging real-world datasets demonstrate the practicality of our approach achieving comparable results to standard Structure-from-Motion systems",
    "volume": "main",
    "checked": true,
    "id": "de76caa665b14ed9696a4773b201a9faf0d4486d",
    "citation_count": 17
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1326_ECCV_2020_paper.php": {
    "title": "Rewriting a Deep Generative Model",
    "abstract": "A deep generative model such as a GAN learns to model a rich set of semantic and physical rules about the target distribution, but up to now, it has been obscure how such rules are encoded in the network, or how a rule could be changed. In this paper, we introduce a new problem setting: manipulation of specific rules encoded by a deep generative model. To address the problem, we propose a formulation in which the desired rule is changed by manipulating a layer of a deep network as a linear associative memory. We derive an algorithm for modifying one entry of the associative memory, and we demonstrate that several interesting structural rules can be located and modified within the layers of state-of-the-art generative models. We present a user interface to enable users to interactively change the rules of a generative model to achieve desired effects, and we show several proof-of-concept applications. Finally, results on multiple datasets demonstrate the advantage of our method against standard fine-tuning methods and edit transfer algorithms",
    "volume": "main",
    "checked": true,
    "id": "51bf7a3aee6b1f61b902625f6badffedf200d31a",
    "citation_count": 56
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1417_ECCV_2020_paper.php": {
    "title": "Compare and Reweight: Distinctive Image Captioning Using Similar Images Sets",
    "abstract": "A wide range of image captioning models has been developed, achieving significant improvement based on popular metrics, such as BLEU, CIDEr, and SPICE. However, although the generated captions can accurately describe the image, they are generic for similar images and lack distinctiveness, i.e., cannot properly describe the uniqueness of each image. In this paper, we aim to improve the distinctiveness of image captions through training with sets of similar images. First, we propose a distinctiveness metric --- between-set CIDEr (CIDErBtw) to evaluate the distinctiveness of a caption with respect to those of similar images. Our metric shows that the human annotations of each image are not equivalent based on distinctiveness. Thus we propose several new training strategies to encourage the distinctiveness of the generated caption for each image, which are based on using CIDErBtw in a weighted loss function or as a reinforcement learning reward. Finally, extensive experiments are conducted, showing that our proposed approach significantly improves both distinctiveness (as measured by CIDErBtw and retrieval metrics) and accuracy (e.g., as measured by CIDEr) for a wide variety of image captioning baselines. These results are further confirmed through a user study",
    "volume": "main",
    "checked": true,
    "id": "b6473852e19ebb31161b2f62d53912b431231fa5",
    "citation_count": 17
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1448_ECCV_2020_paper.php": {
    "title": "Long-term Human Motion Prediction with Scene Context",
    "abstract": "Human movement is goal-directed and influenced by the spatial layout of the objects in the scene. To plan future human motion, it is crucial to perceive the environment -- imagine how hard it is to navigate a new room with lights off. Existing works on predicting human motion do not pay attention to the scene context and thus struggle in long-term prediction. In this work, we propose a novel three-stage framework that exploits scene context to tackle this task. Given a single scene image and 2D pose histories, our method first samples multiple human motion goals, then plans 3D human paths towards each goal, and finally predicts 3D human pose sequences following each path. For stable training and rigorous evaluation, we contribute a diverse synthetic dataset with clean annotations. In both synthetic and real datasets, our method shows consistent quantitative and qualitative improvements over existing methods",
    "volume": "main",
    "checked": true,
    "id": "6e8b522ee29c232026ecf228ed1896bc65f019b4",
    "citation_count": 91
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1473_ECCV_2020_paper.php": {
    "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
    "abstract": "We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $( heta,\\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons. We will make our code and data available upon publication",
    "volume": "main",
    "checked": true,
    "id": "428b663772dba998f5dc6a24488fff1858a0899f",
    "citation_count": 1692
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1501_ECCV_2020_paper.php": {
    "title": "ReferIt3D: Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes",
    "abstract": "In this work we study the problem of using referential language to identify common objects in real-world 3D scenes. We focus on a challenging setup where the referred object belongs to a extit{fine-grained} object class and the underlying scene contains extit{multiple} object instances of that class. Due to the scarcity and unsuitability of existent 3D-oriented linguistic resources for this task, we first develop two large-scale and complementary visio-linguistic datasets: i) extbf{ extit{Sr3D}}, which contains 83.5K template-based utterances leveraging extit{spatial relations} with other fine-grained object classes to localize a referred object in a given scene, and ii) extbf{ extit{Nr3D}} which contains 41.5K extit{natural, free-form}, utterances collected by deploying a 2-player object reference game in 3D scenes. Using utterances of either datasets, human listeners can recognize the referred object with high ($>$86\\%, 92\\% resp.) accuracy. By tapping on this data, we develop novel neural listeners that can comprehend object-centric natural language and identify the referred object extit{directly} in a 3D scene. Our key technical contribution is designing an approach for combining linguistic and geometric information (in the form of 3D point-clouds) and creating multi-modal (3D) neural listeners. We also show that architectures which promote object-to-object communication via graph neural networks outperform less context-aware alternatives, and that language-assisted 3D object identification outperforms language-agnostic object classifiers",
    "volume": "main",
    "checked": true,
    "id": "53794499a3830c3ebb365ecc57f0e8c8a20a682d",
    "citation_count": 59
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1737_ECCV_2020_paper.php": {
    "title": "MatryODShka: Real-time 6DoF Video View Synthesis using Multi-Sphere Images",
    "abstract": "We introduce a method to convert stereo 360 (omnidirectional stereo) imagery into a layered, multi-sphere image representation for six degree-of-freedom (6DoF) rendering. Stereo 360 imagery can be captured from multi-camera systems for virtual reality (VR) rendering, but lacks motion parallax and correct-in-all-directions disparity cues. Together, these can quickly lead to VR sickness when viewing content. One solution is to try and generate a format suitable for 6DoF rendering, such as by estimating depth. However, this raises questions as to how to handle disoccluded regions in dynamic scenes. Our approach is to simultaneously learn depth and blending weights via a multi-sphere image representation, which can be rendered with correct 6DoF disparity and motion parallax in VR. This significantly improves comfort for the viewer, and can be inferred and rendered in real time on modern GPU hardware. Together, these move towards making VR video a more comfortable immersive medium",
    "volume": "main",
    "checked": true,
    "id": "2ca4088150cab021aae1fd8436faa5631919ae9f",
    "citation_count": 49
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1793_ECCV_2020_paper.php": {
    "title": "Learning and Aggregating Deep Local Descriptors for Instance-level Recognition",
    "abstract": "We propose an efficient method to learn deep local descriptors for instance-level recognition. The training only requires examples of positive and negative image pairs and is performed as metric learning of sum-pooled global image descriptors. At inference, the local descriptors are provided by the activations of internal components of the network. We demonstrate why such an approach learns local descriptors that work well for image similarity estimation with classical efficient match kernel methods. The experimental validation studies the trade-off between performance and memory requirements of the state-of-the-art image search approach based on match kernels. Compared to existing local descriptors, the proposed ones perform better in two instance-level recognition tasks and keep memory requirements lower. We experimentally show that global descriptors are not effective enough at large scale and that local descriptors are essential. We achieve state-of-the-art performance, in some cases even with a backbone network as small as ResNet18",
    "volume": "main",
    "checked": true,
    "id": "82ec5fab2e09792fdb740ac61d602b550e3b5046",
    "citation_count": 43
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1969_ECCV_2020_paper.php": {
    "title": "A Consistently Fast and Globally Optimal Solution to the Perspective-n-Point Problem",
    "abstract": "An approach for estimating the pose of a camera given a set of 3D points and their corresponding 2D image projections is presented. It formulates the problem as a non-linear quadratic program and identifies regions in the parameter space that contain unique minima with guarantees that at least one of them will be the global minimum. Each regional minimum is computed with a sequential quadratic programming scheme. These premises result in an algorithm that always determines the global minima of the perspective-n-point problem for any number of input correspondences, regardless of possible coplanar arrangements of the imaged 3D points. For its implementation, the algorithm merely requires ordinary operations available in any standard off-the-shelf linear algebra library. Comparative evaluation demonstrates that the algorithm achieves state-of-the-art results at a consistently low computational cost",
    "volume": "main",
    "checked": true,
    "id": "f8d81b6bb25d3586250e1ddef8912ed91743c6b5",
    "citation_count": 14
  }
}