{
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/267_ECCV_2020_paper.php": {
    "title": "Quaternion Equivariant Capsule Networks for 3D Point Clouds",
    "abstract": "We present a 3D capsule module for processing point clouds that is equivariant to 3D rotations and translations, as well as invariant to permutations of the input points. The operator receives a sparse set of local reference frames, computed from an input point cloud and establishes end-to-end transformation equivariance through a novel dynamic routing procedure on quaternions. Further, we theoretically connect dynamic routing between capsules to the well-known Weiszfeld algorithm, a scheme for solving iterative re-weighted least squares (IRLS) problems with provable convergence properties. It is shown that such group dynamic routing can be interpreted as robust IRLS rotation averaging on capsule votes, where information is routed based on the final inlier scores. Based on our operator, we build a capsule network that disentangles geometry from pose, paving the way for more informative descriptors and a structured latent space. Our architecture allows joint object classification and orientation estimation without explicit supervision of rotations. We validate our algorithm empirically on common benchmark datasets",
    "volume": "main",
    "checked": true,
    "id": "4ea814b1c76995e445ccf0621b0e775ff53a1db6",
    "citation_count": 52
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/283_ECCV_2020_paper.php": {
    "title": "DeepFit: 3D Surface Fitting via Neural Network Weighted Least Squares",
    "abstract": "We propose a surface fitting method for unstructured 3D point clouds. This method, called DeepFit, incorporates a neural network to learn point-wise weights for weighted least squares polynomial surface fitting. The learned weights act as a soft selection for the neighborhood of surface points thus avoiding the scale selection required of previous methods. To train the network we propose a novel surface consistency loss that improves point weight estimation. The method enables extracting normal vectors and other geometrical properties, such as principal curvatures, the latter were not presented as ground truth during training. We achieve state-of-the-art results on a benchmark normal and curvature estimation dataset,demonstrate robustness to noise, outliers and density variations, and show its application on noise removal",
    "volume": "main",
    "checked": true,
    "id": "8f61868b44f9b396f9870be47aed5e52aa21a2f0",
    "citation_count": 22
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/343_ECCV_2020_paper.php": {
    "title": "NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search",
    "abstract": "In this paper, we propose an efficient NAS algorithm for generating task-specific models that are competitive under multiple competing objectives. It comprises of two surrogates, one at the architecture level to improve sample efficiency and one at the weights level, through a supernet, to improve gradient descent training efficiency. On standard benchmark datasets (C10, C100, ImageNet), the resulting models, dubbed NSGANetV2, either match or outperform models from existing approaches with the search being orders of magnitude more sample efficient. Furthermore, we demonstrate the effectiveness and versatility of the proposed method on six diverse non-standard datasets, e.g. STL-10, Flowers102, Oxford Pets, FGVC Aircrafts etc. In all cases, NSGANetV2s improve the state-of-the-art (under mobile setting), suggesting that NAS can be a viable alternative to conventional transfer learning approaches in handling diverse scenarios such as small-scale or fine-grained datasets. Code is available at https://github.com/mikelzc1990/nsganetv2",
    "volume": "main",
    "checked": true,
    "id": "8054513b37839878c5ee9d541d228b0148bce306",
    "citation_count": 62
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/384_ECCV_2020_paper.php": {
    "title": "Describing Textures using Natural Language",
    "abstract": "Textures in natural images can be characterized by color, shape, periodicity of elements within them, and other attributes that can be described using natural language. In this paper, we study the problem of describing visual attributes of texture on a novel dataset containing rich descriptions of textures, and conduct a systematic study of current generative and discriminative models for grounding language to images on this dataset. We find that while these models capture some properties of texture, they fail to capture several compositional properties, such as the colors of dots. We provide critical analysis of existing models by generating synthetic but realistic textures with different descriptions. Our dataset also allows us to train interpretable models and generate language-based explanations of what discriminative features are learned by deep networks for fine-grained categorization where texture plays a key role. We present visualizations of several fine-grained domains and show that texture attributes learned on our dataset offer improvements over expert-designed attributes on the Caltech-UCSD Birds dataset",
    "volume": "main",
    "checked": true,
    "id": "86f0fb3791761cdb5e9108721a536d752641d4bf",
    "citation_count": 7
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/410_ECCV_2020_paper.php": {
    "title": "Empowering Relational Network by Self-Attention Augmented Conditional Random Fields for Group Activity Recognition",
    "abstract": "This paper presents a novel relational network for group activity recognition. The core of our network is to augment the conditional random fields (CRF), amenable to learning inter-dependency of correlated observations, with the newly devised temporal and spatial self-attention to learn the temporal evolution and spatial relational contexts of every actor in videos. Such a combination utilizes the global receptive fields of self-attention to construct a spatio-temporal graph topology to address the temporal dependency and non-local relationships of the actors. The network first uses the temporal self-attention along with the spatial self-attention, which considers multiple cliques with different scales of locality to account for the diversity of the actors' relationships in group activities, to model the pairwise energy of CRF. Afterward, to accommodate the distinct characteristics of each video, a new mean-field inference algorithm with dynamic halting is also addressed. Finally, a bidirectional universal transformer encoder (UTE), which combines both of the forward and backward temporal context information, is used to aggregate the relational contexts and scene information for group activity recognition. Simulations show that the proposed approach surpasses the state-of-the-art methods on the widespread Volleyball and Collective Activity datasets",
    "volume": "main",
    "checked": true,
    "id": "33f62e6f851da560037f1ed008d2eb51bb80f062",
    "citation_count": 16
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/445_ECCV_2020_paper.php": {
    "title": "AiR: Attention with Reasoning Capability",
    "abstract": "While attention has been an increasingly popular component in deep neural networks to both interpret and boost performance of models, little work has examined how attention progresses to accomplish a task and whether it is reasonable. In this work, we propose an Attention with Reasoning capability (AiR) framework that uses attention to understand and improve the process leading to task outcomes. We first define an evaluation metric based on a sequence of atomic reasoning operations, enabling quantitative measurement of attention that considers the reasoning process. We then collect human eye-tracking and answer correctness data, and analyze various machine and human attentions on their reasoning capability and how they impact task performance. Furthermore, we propose a supervision method to jointly and progressively optimize attention, reasoning, and task performance so that models learn to look at regions of interests by following a reasoning process. We demonstrate the effectiveness of the proposed framework in analyzing and modeling attention with better reasoning capability and task performance. The code and data are available at https://github.com/szzexpoi/AiR",
    "volume": "main",
    "checked": true,
    "id": "96150d2fdbb72e11882ad06b346d6e0d2e819d51",
    "citation_count": 15
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/500_ECCV_2020_paper.php": {
    "title": "Self6D: Self-Supervised Monocular 6D Object Pose Estimation",
    "abstract": "6D object pose estimation is a fundamental problem in computer vision. Convolutional Neural Networks (CNNs) have recently proven to be capable of predicting reliable 6D pose estimates even from monocular images. Nonetheless, CNNs are identified as being extremely data-driven, and acquiring adequate annotations is oftentimes very time-consuming and labor intensive. To overcome this shortcoming, we propose the idea of monocular 6D pose estimation by means of self-supervised learning, removing the need for real annotations. After training our proposed network fully supervised with synthetic RGB data, we leverage recent advances in neural rendering to further self-supervise the model on unannotated real RGB-D data, seeking for a visually and geometrically optimal alignment. Extensive evaluations demonstrate that our proposed self-supervision is able to significantly enhance the model's original performance, outperforming all other methods relying on synthetic data or employing elaborate techniques from the domain adaptation realm",
    "volume": "main",
    "checked": true,
    "id": "1d947ceedc5a16c4e8bfae6031a4098d45a9f670",
    "citation_count": 39
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/529_ECCV_2020_paper.php": {
    "title": "Invertible Image Rescaling",
    "abstract": "High-resolution digital images are usually downscaled to fit various display screens or save the cost of storage and bandwidth, meanwhile the post-upscaling is adpoted to recover the original resolutions or the details in the zoom-in images. However, typical image downscaling is a non-injective mapping due to the loss of high-frequency information, which leads to the ill-posed problem of the inverse upscaling procedure and poses great challenges for recovering details from the downscaled low-resolution images. Simply upscaling with image super-resolution methods results in unsatisfactory recovering performance. In this work, we propose to solve this problem by modeling the downscaling and upscaling processes from a new perspective, i.e. an invertible bijective transformation, which can largely mitigate the ill-posed nature of image upscaling. We develop an Invertible Rescaling Net (IRN) with deliberately designed framework and objectives to produce visually-pleasing low-resolution images and meanwhile capture the distribution of the lost information using a latent variable following a specified distribution in the downscaling process. In this way, upscaling is made tractable by inversely passing a randomly-drawn latent variable with the low-resolution image through the network. Experimental results demonstrate the significant improvement of our model over existing methods in terms of both quantitative and qualitative evaluations of image upscaling reconstruction from downscaled images. Code is available at https://github.com/pkuxmq/Invertible-Image-Rescaling",
    "volume": "main",
    "checked": true,
    "id": "407f1d16ba4eb3cb4851429cae46c97d723a35a5",
    "citation_count": 82
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/612_ECCV_2020_paper.php": {
    "title": "Synthesize then Compare: Detecting Failures and Anomalies for Semantic Segmentation",
    "abstract": "The ability to detect failures and anomalies are fundamental requirements for building reliable systems for computer vision applications, especially safety-critical applications of semantic segmentation, such as autonomous driving and medical image analysis. In this paper, we systematically study failure and anomaly detection for semantic segmentation and propose a unified framework, consisting of two modules, to address these two related problems. The first module is an image synthesis module, which generates a synthesized image from a segmentation layout map, and the second is a comparison module, which computes the difference between the synthesized image and the input image. We validate our framework on three challenging datasets and improve the state-of-the-arts by large margins, i.e., 6% AUPR-Error on Cityscapes, 7% Pearson correlation on pancreatic tumor segmentation in MSD and 20% AUPR on StreetHazards anomaly segmentation",
    "volume": "main",
    "checked": true,
    "id": "c707517507873bc2cdc489b6fd9af74770468c48",
    "citation_count": 62
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/677_ECCV_2020_paper.php": {
    "title": "House-GAN: Relational Generative Adversarial Networks for Graph-constrained House Layout Generation",
    "abstract": "This paper proposes a novel graph-constrained generative adversarial network, whose generator and discriminator are built upon relational architecture. The main idea is to encode the constraint into the graph structure of its relational networks. We have demonstrated the proposed architecture for a new house layout generation problem, whose task is to take an architectural constraint as a graph (i.e., the number and types of rooms with their spatial adjacency) and produce a set of axis-aligned bounding boxes of rooms. We measure the quality of generated house layouts with the three metrics: the realism, the diversity, and the compatibility with the input graph constraint. Our qualitative and quantitative evaluations over 117,000 real floorplan images demonstrate. We will publicly share all our code and data",
    "volume": "main",
    "checked": true,
    "id": "e8a026d2dd10af43f8bc7fc9fd7a4f888c74efe0",
    "citation_count": 64
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/736_ECCV_2020_paper.php": {
    "title": "Crowdsampling the Plenoptic Function",
    "abstract": "Many popular tourist landmarks are captured in a multitude of online, public photos. These photos represent a sparse and unstructured sampling of the plenoptic function for a particular scene. In this paper,we present a new approach to novel view synthesis under time-varying illumination from such data. Our approach builds on the recent multi-plane image (MPI) format for representing local light fields under fixed viewing conditions. We introduce a new DeepMPI representation, motivated by observations on the sparsity structure of the plenoptic function, that allows for real-time synthesis of photorealistic views that are continuous in both space and across changes in lighting. Our method can synthesize the same compelling parallax and view-dependent effects as previous MPI methods, while simultaneously interpolating along changes in reflectance and illumination with time. We show how to learn a model of these effects in an unsupervised way from an unstructured collection of photos without temporal registration, demonstrating significant improvements over recent work in neural rendering. More information can be found crowdsampling.io",
    "volume": "main",
    "checked": true,
    "id": "37d29d649e47065e5b3f6e5a5151dd077fe85cd0",
    "citation_count": 43
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/738_ECCV_2020_paper.php": {
    "title": "VoxelPose: Towards Multi-Camera 3D Human Pose Estimation in Wild Environment",
    "abstract": "We present mph{VoxelPose} to estimate $3$D poses of multiple people from multiple camera views. In contrast to the previous efforts which require to establish cross-view correspondence based on noisy and incomplete $2$D pose estimates, mph{VoxelPose} directly operates in the $3$D space therefore avoids making incorrect decisions in each camera view. To achieve this goal, features in all camera views are aggregated in the $3$D voxel space and fed into mph{Cuboid Proposal Network} (CPN) to localize all people. Then we propose mph{Pose Regression Network} (PRN) to estimate a detailed $3$D pose for each proposal. The approach is robust to occlusion which occurs frequently in practice. Without bells and whistles, it outperforms the previous methods on several public datasets",
    "volume": "main",
    "checked": true,
    "id": "828c5100bd9c7d649c4f6aa9a1443838c5a6482a",
    "citation_count": 55
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/832_ECCV_2020_paper.php": {
    "title": "End-to-End Object Detection with Transformers",
    "abstract": "We present a new method that views object detection as a direct set prediction. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, we show that DETR can be easily generalized to produce a competitive panoptic segmentation prediction in a unified manner",
    "volume": "main",
    "checked": true,
    "id": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e",
    "citation_count": 3432
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/840_ECCV_2020_paper.php": {
    "title": "DeepSFM: Structure From Motion Via Deep Bundle Adjustment",
    "abstract": "Structure from motion (SfM) is an essential computer vision problem which has not been well handled by deep learning. One of the promising trends is to apply explicit structural constraint, e.g. 3D cost volume, into the network. However, existing methods usually assume accurate camera poses either from GT or other methods, which is unrealistic in practice. In this work, we design a physical driven architecture, namely DeepSFM, inspired by traditional Bundle Adjustment (BA), which consists of two cost volume based architectures for depth and pose estimation respectively, iteratively running to improve both. The explicit constraints on both depth (structure) and pose (motion), when combined with the learning components, bring the merit from both traditional BA and emerging deep learning technology. Extensive experiments on various datasets show that our model achieves the state-of-the-art performance on both depth and pose estimation with superior robustness against less number of inputs and the noise in initialization",
    "volume": "main",
    "checked": true,
    "id": "41e5fe4095500d483499d870a5415a9d8f0a854a",
    "citation_count": 42
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1044_ECCV_2020_paper.php": {
    "title": "Ladybird: Quasi-Monte Carlo Sampling for Deep Implicit Field Based 3D Reconstruction with Symmetry",
    "abstract": "Deep implicit field regression methods are effective for 3D reconstruction from single-view images. However, the impact of different sampling patterns on the reconstruction quality is not well-understood. In this work, we first study the effect of point set discrepancy on the network training. Based on Farthest Point Sampling algorithm, we propose a sampling scheme that theoretically encourages better generalization performance, and results in fast convergence for SGD-based optimization algorithms. Secondly, based on the reflective symmetry of an object, we propose a feature fusion method that alleviates issues due to self-occlusions which makes it difficult to utilize local image features. Our proposed system Ladybird is able to create high quality 3D object reconstructions from a single input image. We evaluate Ladybird on a large scale 3D dataset (ShapeNet) demonstrating highly competitive results in terms of Chamfer distance, Earth Mover's distance and Intersection Over Union (IoU)",
    "volume": "main",
    "checked": true,
    "id": "16158867c91f1819f5a646731c6e1da5505a8dd5",
    "citation_count": 26
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1059_ECCV_2020_paper.php": {
    "title": "Segment as Points for Efficient Online Multi-Object Tracking and Segmentation",
    "abstract": "Current multi-object tracking and segmentation (MOTS) methods follow the tracking-by-detection paradigm and adopt convolutions for feature extraction. However, as affected by the inherent receptive field, convolution based feature extraction inevitably mixes up the foreground features and the background features, resulting in ambiguities in the subsequent instance association. In this paper, we propose a highly effective method for learning instance embeddings based on segments by converting the compact image representation to un-ordered 2D point cloud representation. Our method generates a new tracking-by-points paradigm where discriminative instance embeddings are learned from randomly selected points rather than images. Furthermore, multiple informative data modalities are converted into point-wise representations to enrich point-wise features. The resulting online MOTS framework, named PointTrack, surpasses all the state-of-the-art methods including 3D tracking methods by large margins (5.4\\% higher MOTSA and 18 times faster over MOTSFusion) with the near real-time speed (22 FPS). Evaluations across three datasets demonstrate both the effectiveness and efficiency of our method. Moreover, based on the observation that current MOTS datasets lack crowded scenes, we build a more challenging MOTS dataset named APOLLO MOTS with higher instance density. Both APOLLO MOTS and our codes are publicly available at https://github.com/detectRecog/PointTrack",
    "volume": "main",
    "checked": true,
    "id": "000458c2651ab147d503429f6460aa80155e8e35",
    "citation_count": 46
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1105_ECCV_2020_paper.php": {
    "title": "Conditional Convolutions for Instance Segmentation",
    "abstract": "We propose a simple yet effective instance segmentation framework, termed CondInst (conditional convolutions for instance segmentation). Top-performing instance segmentation methods such as Mask R-CNN rely on ROI operations (typically ROIPool or ROIAlign) to obtain the final instance masks. In contrast, we propose to solve instance segmentation from a new perspective. Instead of using instance-wise ROIs as inputs to a network of fixed weights, we employ dynamic instance-aware networks, conditioned on instances. CondInst enjoys two advantages: 1) Instance segmentation is solved by a fully convolutional network, eliminating the need for ROI cropping and feature alignment. 2) Due to the much improved capacity of dynamically-generated conditional convolutions, the mask head can be very compact (e.g., 3 conv. layers, each having only 8 channels), leading to significantly faster inference. We demonstrate a simpler instance segmentation method that can achieve improved performance in both accuracy and inference speed. On the COCO dataset, we outperform a few recent methods including well-tuned Mask R-CNN baselines, without longer training schedules needed. Code is available: https://git.io/AdelaiDet",
    "volume": "main",
    "checked": true,
    "id": "862f2e2e5ba7b8b83f42226170c634ecb02834e4",
    "citation_count": 251
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1196_ECCV_2020_paper.php": {
    "title": "MutualNet: Adaptive ConvNet via Mutual Learning from Network Width and Resolution",
    "abstract": "We propose the width-resolution mutual learning method (MutualNet) to train a network that is executable at dynamic resource constraints to achieve adaptive accuracy-efficiency trade-offs at runtime. Our method trains a cohort of sub-networks with different widths using different input resolutions to mutually learn multi-scale representations for each sub-network. It achieves consistently better ImageNet top-1 accuracy over the state-of-the-art adaptive network US-Net under different computation constraints, and outperforms the best compound scaled MobileNet in EfficientNet by 1.5%. The superiority of our method is also validated on COCO object detection and instance segmentation as well as transfer learning. Surprisingly, the training strategy of MutualNet can also boost the performance of a single network, which substantially outperforms the powerful AutoAugmentation in both efficiency (GPU search hours: 15000 vs. 0) and accuracy (ImageNet: 77.6% vs. 78.6%). Code is provided in supplementary material",
    "volume": "main",
    "checked": true,
    "id": "0087d72c72fe3098393f7bbb92032c38a78c43ae",
    "citation_count": 49
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1203_ECCV_2020_paper.php": {
    "title": "Fashionpedia: Ontology, Segmentation, and an Attribute Localization Dataset",
    "abstract": "Segmentation, and an Attribute Localization Dataset\",\"In this work, we focus on the task of instance segmentation with attribute localization. This unifies instance segmentation (detect and segment each object instance) and visual categorization of fine-grained attributes (classify one or multiple attributes). The proposed task requires both localizing an object and describing its properties. To illustrate the various aspects of this task, we focus on the domain of fashion and introduce Fashionpedia as a step toward mapping out the visual aspects of the fashion world. Fashionpedia consists of two parts: (1) an ontology built by fashion experts containing 27 main apparel categories, 19 apparel parts, and 294 fine-grained attributes and their relationships and (2) a dataset consisting of everyday and celebrity event fashion images annotated with segmentation masks and their associated fine-grained attributes, built upon the backbone of the Fashionpedia ontology. In order to solve this challenging task, we propose a novel Attribute-Mask R-CNN model to jointly perform instance segmentation and localized attribute recognition, and provide a novel evaluation metric for the task. Fashionpedia is available at https://fashionpedia.github.io/home/",
    "volume": "main",
    "checked": true,
    "id": "7c368af1d2d4bd52c7f54a3fafcb84abc931eccd",
    "citation_count": 35
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1273_ECCV_2020_paper.php": {
    "title": "Privacy Preserving Structure-from-Motion",
    "abstract": "Over the last years, visual localization and mapping solutions have been adopted by an increasing number of mixed reality and robotics systems. The recent trend towards cloud-based localization and mapping systems has raised significant privacy concerns. These are mainly grounded by the fact that these services require users to upload visual data to their servers, which can reveal potentially confidential information, even if only derived image features are uploaded. Recent research addresses some of these concerns for the task of image-based localization by concealing the geometry of the query images and database maps. The core idea of the approach is to lift 2D/3D feature points to random lines, while still providing sufficient constraints for camera pose estimation. In this paper, we further build upon this idea and propose solutions to the different core algorithms of an incremental Structure-from-Motion pipeline based on random line features. With this work, we make another fundamental step towards enabling privacy preserving cloud-based mapping solutions. Various experiments on challenging real-world datasets demonstrate the practicality of our approach achieving comparable results to standard Structure-from-Motion systems",
    "volume": "main",
    "checked": true,
    "id": "de76caa665b14ed9696a4773b201a9faf0d4486d",
    "citation_count": 17
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1326_ECCV_2020_paper.php": {
    "title": "Rewriting a Deep Generative Model",
    "abstract": "A deep generative model such as a GAN learns to model a rich set of semantic and physical rules about the target distribution, but up to now, it has been obscure how such rules are encoded in the network, or how a rule could be changed. In this paper, we introduce a new problem setting: manipulation of specific rules encoded by a deep generative model. To address the problem, we propose a formulation in which the desired rule is changed by manipulating a layer of a deep network as a linear associative memory. We derive an algorithm for modifying one entry of the associative memory, and we demonstrate that several interesting structural rules can be located and modified within the layers of state-of-the-art generative models. We present a user interface to enable users to interactively change the rules of a generative model to achieve desired effects, and we show several proof-of-concept applications. Finally, results on multiple datasets demonstrate the advantage of our method against standard fine-tuning methods and edit transfer algorithms",
    "volume": "main",
    "checked": true,
    "id": "51bf7a3aee6b1f61b902625f6badffedf200d31a",
    "citation_count": 56
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1417_ECCV_2020_paper.php": {
    "title": "Compare and Reweight: Distinctive Image Captioning Using Similar Images Sets",
    "abstract": "A wide range of image captioning models has been developed, achieving significant improvement based on popular metrics, such as BLEU, CIDEr, and SPICE. However, although the generated captions can accurately describe the image, they are generic for similar images and lack distinctiveness, i.e., cannot properly describe the uniqueness of each image. In this paper, we aim to improve the distinctiveness of image captions through training with sets of similar images. First, we propose a distinctiveness metric --- between-set CIDEr (CIDErBtw) to evaluate the distinctiveness of a caption with respect to those of similar images. Our metric shows that the human annotations of each image are not equivalent based on distinctiveness. Thus we propose several new training strategies to encourage the distinctiveness of the generated caption for each image, which are based on using CIDErBtw in a weighted loss function or as a reinforcement learning reward. Finally, extensive experiments are conducted, showing that our proposed approach significantly improves both distinctiveness (as measured by CIDErBtw and retrieval metrics) and accuracy (e.g., as measured by CIDEr) for a wide variety of image captioning baselines. These results are further confirmed through a user study",
    "volume": "main",
    "checked": true,
    "id": "b6473852e19ebb31161b2f62d53912b431231fa5",
    "citation_count": 17
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1448_ECCV_2020_paper.php": {
    "title": "Long-term Human Motion Prediction with Scene Context",
    "abstract": "Human movement is goal-directed and influenced by the spatial layout of the objects in the scene. To plan future human motion, it is crucial to perceive the environment -- imagine how hard it is to navigate a new room with lights off. Existing works on predicting human motion do not pay attention to the scene context and thus struggle in long-term prediction. In this work, we propose a novel three-stage framework that exploits scene context to tackle this task. Given a single scene image and 2D pose histories, our method first samples multiple human motion goals, then plans 3D human paths towards each goal, and finally predicts 3D human pose sequences following each path. For stable training and rigorous evaluation, we contribute a diverse synthetic dataset with clean annotations. In both synthetic and real datasets, our method shows consistent quantitative and qualitative improvements over existing methods",
    "volume": "main",
    "checked": true,
    "id": "6e8b522ee29c232026ecf228ed1896bc65f019b4",
    "citation_count": 91
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1473_ECCV_2020_paper.php": {
    "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
    "abstract": "We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $( heta,\\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons. We will make our code and data available upon publication",
    "volume": "main",
    "checked": true,
    "id": "428b663772dba998f5dc6a24488fff1858a0899f",
    "citation_count": 1692
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1501_ECCV_2020_paper.php": {
    "title": "ReferIt3D: Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes",
    "abstract": "In this work we study the problem of using referential language to identify common objects in real-world 3D scenes. We focus on a challenging setup where the referred object belongs to a extit{fine-grained} object class and the underlying scene contains extit{multiple} object instances of that class. Due to the scarcity and unsuitability of existent 3D-oriented linguistic resources for this task, we first develop two large-scale and complementary visio-linguistic datasets: i) extbf{ extit{Sr3D}}, which contains 83.5K template-based utterances leveraging extit{spatial relations} with other fine-grained object classes to localize a referred object in a given scene, and ii) extbf{ extit{Nr3D}} which contains 41.5K extit{natural, free-form}, utterances collected by deploying a 2-player object reference game in 3D scenes. Using utterances of either datasets, human listeners can recognize the referred object with high ($>$86\\%, 92\\% resp.) accuracy. By tapping on this data, we develop novel neural listeners that can comprehend object-centric natural language and identify the referred object extit{directly} in a 3D scene. Our key technical contribution is designing an approach for combining linguistic and geometric information (in the form of 3D point-clouds) and creating multi-modal (3D) neural listeners. We also show that architectures which promote object-to-object communication via graph neural networks outperform less context-aware alternatives, and that language-assisted 3D object identification outperforms language-agnostic object classifiers",
    "volume": "main",
    "checked": true,
    "id": "53794499a3830c3ebb365ecc57f0e8c8a20a682d",
    "citation_count": 59
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1737_ECCV_2020_paper.php": {
    "title": "MatryODShka: Real-time 6DoF Video View Synthesis using Multi-Sphere Images",
    "abstract": "We introduce a method to convert stereo 360 (omnidirectional stereo) imagery into a layered, multi-sphere image representation for six degree-of-freedom (6DoF) rendering. Stereo 360 imagery can be captured from multi-camera systems for virtual reality (VR) rendering, but lacks motion parallax and correct-in-all-directions disparity cues. Together, these can quickly lead to VR sickness when viewing content. One solution is to try and generate a format suitable for 6DoF rendering, such as by estimating depth. However, this raises questions as to how to handle disoccluded regions in dynamic scenes. Our approach is to simultaneously learn depth and blending weights via a multi-sphere image representation, which can be rendered with correct 6DoF disparity and motion parallax in VR. This significantly improves comfort for the viewer, and can be inferred and rendered in real time on modern GPU hardware. Together, these move towards making VR video a more comfortable immersive medium",
    "volume": "main",
    "checked": true,
    "id": "2ca4088150cab021aae1fd8436faa5631919ae9f",
    "citation_count": 49
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1793_ECCV_2020_paper.php": {
    "title": "Learning and Aggregating Deep Local Descriptors for Instance-level Recognition",
    "abstract": "We propose an efficient method to learn deep local descriptors for instance-level recognition. The training only requires examples of positive and negative image pairs and is performed as metric learning of sum-pooled global image descriptors. At inference, the local descriptors are provided by the activations of internal components of the network. We demonstrate why such an approach learns local descriptors that work well for image similarity estimation with classical efficient match kernel methods. The experimental validation studies the trade-off between performance and memory requirements of the state-of-the-art image search approach based on match kernels. Compared to existing local descriptors, the proposed ones perform better in two instance-level recognition tasks and keep memory requirements lower. We experimentally show that global descriptors are not effective enough at large scale and that local descriptors are essential. We achieve state-of-the-art performance, in some cases even with a backbone network as small as ResNet18",
    "volume": "main",
    "checked": true,
    "id": "82ec5fab2e09792fdb740ac61d602b550e3b5046",
    "citation_count": 43
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1969_ECCV_2020_paper.php": {
    "title": "A Consistently Fast and Globally Optimal Solution to the Perspective-n-Point Problem",
    "abstract": "An approach for estimating the pose of a camera given a set of 3D points and their corresponding 2D image projections is presented. It formulates the problem as a non-linear quadratic program and identifies regions in the parameter space that contain unique minima with guarantees that at least one of them will be the global minimum. Each regional minimum is computed with a sequential quadratic programming scheme. These premises result in an algorithm that always determines the global minima of the perspective-n-point problem for any number of input correspondences, regardless of possible coplanar arrangements of the imaged 3D points. For its implementation, the algorithm merely requires ordinary operations available in any standard off-the-shelf linear algebra library. Comparative evaluation demonstrates that the algorithm achieves state-of-the-art results at a consistently low computational cost",
    "volume": "main",
    "checked": true,
    "id": "f8d81b6bb25d3586250e1ddef8912ed91743c6b5",
    "citation_count": 14
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2096_ECCV_2020_paper.php": {
    "title": "Learn to Recover Visible Color for Video Surveillance in a Day",
    "abstract": "In silicon sensors, the interference between visible and near-infrared (NIR) signals is a crucial problem. For all-day video surveillance, commercial camera systems usually adopt auxiliary NIR cut filter and NIR LED illumination to selectively block or enhance NIR signal according to the surrounding light conditions. This switching between the daytime and the nighttime mode inevitably involves mechanical parts, and thus requires frequent maintenance. Furthermore, images captured at nighttime mode are in shortage of chrominance, which might hinder human interpretation and high-level computer vision algorithms in succession. In this paper, we present a deep learning based approach that directly generates human-friendly, visible color for video surveillance in a day. To enable training, we capture well-aligned video pairs through a customized optical device and contribute a large-scale dataset, video surveillance in a day (VSIAD). We propose a novel multi-task deep network with state synchronization modules to better utilize texture and chrominance information. Our trained model generates high-quality visible color images and achieves state-of-the-art performance on multiple metrics as well as subjective judgment",
    "volume": "main",
    "checked": true,
    "id": "fcf64554b303752bec56e1974163d6a5e02d17cb",
    "citation_count": 1
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2149_ECCV_2020_paper.php": {
    "title": "Deep Fashion3D: A Dataset and Benchmark for 3D Garment Reconstruction from Single Images",
    "abstract": "High-fidelity clothing reconstruction is the key to achieving photorealism in a wide range of applications including human digitization, virtual try-on, etc. Recent advances in learning-based approaches have accomplished unprecedented accuracy in recovering unclothed human shape and pose from single images, thanks to the availability of powerful statistical models, e.g. SMPL[38], learned from a large number of body scans. In contrast, modeling and recovering clothed human and 3D garments remains notoriously difficult, mostly due to the lack of large-scale clothing models available for the research community. We propose to fill this gap by introducing DeepFashion3D, the largest collection to date of 3D garment models, with the goal of establishing a novel benchmark and dataset for the evaluation of image-based garment reconstruction systems. Deep Fashion3D contains 2078 models reconstructed from real garments, which covers 10 different categories and 563 garment instances. It provides rich annotations including 3D feature lines, 3D body pose and the corresponded multi-view real images. In addition, each garment is randomly posed to enhance the variety of real clothing deformations. To demonstrate the advantage of \\datasetName{}, we propose a novel baseline approach for single-view garment reconstruction, which leverages the merits of both mesh and implicit representations. A novel adaptable template is proposed to enable the learning of all types of clothing in a single network. Extensive experiments have been conducted on the proposed dataset to verify its significance and usefulness. We will make Deep Fashion3D publicly available upon publication",
    "volume": "main",
    "checked": true,
    "id": "57942df22b253ebe052adeb82a9b92c764e61521",
    "citation_count": 46
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2193_ECCV_2020_paper.php": {
    "title": "Spatially Adaptive Inference with Stochastic Feature Sampling and Interpolation",
    "abstract": "In the feature maps of CNNs, there commonly exists considerable spatial redundancy that leads to much repetitive processing. Towards reducing this superfluous computation, we propose to compute features only at sparsely sampled locations, which are probabilistically chosen according to activation responses, and then densely reconstruct the feature map with an efficient interpolation procedure. With this sampling-interpolation scheme, our network avoids expending computation on spatial locations that can be effectively interpolated, while being robust to activation prediction errors through broadly distributed sampling. A technical challenge of this sampling-based approach is that the binary decision variables for representing discrete sampling locations are non-differentiable, making them incompatible with backpropagation. To circumvent this issue, we make use of a reparameterization trick based on the Gumbel-Softmax distribution, with which backpropagation can iterate these variables towards binary values. The presented network is experimentally shown to save substantial computation while maintaining accuracy over a variety of computer vision tasks",
    "volume": "main",
    "checked": true,
    "id": "11502aebebf633a0a10e9b4ea45c61eb01986c4f",
    "citation_count": 41
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2211_ECCV_2020_paper.php": {
    "title": "BorderDet: Border Feature for Dense Object Detection",
    "abstract": "Dense object detectors rely on the sliding-window paradigm that predicts the object over a regular grid of image. Meanwhile, the feature maps on the point of the grid are adopted to generate the bounding box predictions. The point feature is convenient to use but may lack the explicit border information for accurate localization. In this paper, We propose a simple and efficient operator called Border-Align to extract ``border features'' from the extreme point of the border to enhance the point feature. Based on the BorderAlign, we design a novel detection architecture called BorderDet, which explicitly exploits the border information for stronger classification and more accurate localization. With ResNet-50 backbone, our method improves single-stage detector FCOS by 2.8 AP gains (38.6 v.s. 41.4). With the ResNeXt-101-DCN backbone, our BorderDet obtains 50.3 AP, outperforming the existing state-of-the-art approaches",
    "volume": "main",
    "checked": true,
    "id": "5800afc4ca06a309d74baedd65a0b0666b3716d1",
    "citation_count": 67
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2258_ECCV_2020_paper.php": {
    "title": "Regularization with Latent Space Virtual Adversarial Training",
    "abstract": "Virtual Adversarial Training (VAT) has shown impressive results among recently developed regularization methods called consistency regularization. VAT utilizes adversarial samples, generated by injecting perturbation in the input space, for training and thereby enhances the generalization ability of a classifier. However, such adversarial samples can be generated only within a very small area around the input data point, which limits the adversarial effectiveness of such samples. To address this problem we propose LVAT (Latent space VAT), which injects perturbation in the latent space instead of the input space. LVAT can generate adversarial samples flexibly, resulting in more adverse effect and thus more effective regularization. The latent space is built by a generative model, and in this paper we examine two type of models: variational auto-encoder and normalizing flow, specifically Glow. We evaluated the performance of our method in both supervised and semi-supervised learning scenarios for an image classification task using SVHN and CIFAR-10 datasets. In our evaluation, we found that our method outperforms VAT and other state-of-the-art methods",
    "volume": "main",
    "checked": true,
    "id": "d405caa4758e6ef0431844e623072976f0654b1d",
    "citation_count": 4
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2263_ECCV_2020_paper.php": {
    "title": "Du²Net: Learning Depth Estimation from Dual-Cameras and Dual-Pixels",
    "abstract": "Computational stereo has reached a high level of accuracy, but degrades in the presence of occlusions, repeated textures, and correspondence errors along edges. We present a novel approach based on neural networks for depth estimation that combines stereo from dual cameras with stereo from a dual-pixel sensor, which is increasingly common on consumer cameras. Our network uses a novel architecture to fuse these two sources of information and can overcome the above-mentioned limitations of pure binocular stereo matching. Our method provides a dense depth map with sharp edges, which is crucial for computational photography applications like synthethic shallow-depth-of-field or 3D Photos. Additionally, we avoid the inherent ambiguity due to the aperture problem in stereo cameras by designing the stereo baseline to be orthogonal to the dual-pixel baseline. We present experiments and comparisons with state-of-the-art approaches that show that our method offers a substantial improvement over previous works",
    "volume": "main",
    "checked": false,
    "id": "1f3d16335f36a8211c496e1c3966d2768d687124",
    "citation_count": 20
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2307_ECCV_2020_paper.php": {
    "title": "Model-Agnostic Boundary-Adversarial Sampling for Test-Time Generalization in Few-Shot learning",
    "abstract": "Few-shot learning is an important research problem that tackles one of the greatest challenges of machine learning: learning a new task from a limited amount of labeled data. We propose a model-agnostic method that improves the test-time performance of any few-shot learning models with no additional training, and thus is free from the training-test domain gap. Based on only the few support samples in a meta-test task, our method generates the samples adversarial to the base few-shot classifier's boundaries and fine-tunes its embedding function in the direction that increases the classification margins of the adversarial samples. Consequently, the embedding space becomes denser around the labeled samples which makes the classifier robust to query samples. Experimenting on miniImageNet, CIFAR-FS, and FC100, we demonstrate that our method brings significant performance improvement to three different base methods with various properties, and achieves the state-of-the-art performance in a number of few-shot learning tasks",
    "volume": "main",
    "checked": true,
    "id": "c3868028316e6d5d3650103e0c74e4088e1ce260",
    "citation_count": 35
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2463_ECCV_2020_paper.php": {
    "title": "Targeted Attack for Deep Hashing based Retrieval",
    "abstract": "The deep hashing based retrieval method is widely adopted in large-scale image and video retrieval. However, there is little investigation on its security. In this paper, we propose a novel method, dubbed deep hashing targeted attack (DHTA), to study the targeted attack on such retrieval. Specifically, we first formulate the targeted attack as a point-to-set optimization, which minimizes the average distance between the hash code of an adversarial example and those of a set of objects with the target label. Then we design a novel component-voting scheme to obtain an anchor code as the representative of the set of hash codes of objects with the target label, whose optimality guarantee is also theoretically derived. To balance the performance and perceptibility, we propose to minimize the Hamming distance between the hash code of the adversarial example and the anchor code under the $ll^\\infty$ restriction on the perturbation. Extensive experiments verify that DHTA is effective in attacking both deep hashing based image retrieval and video retrieval",
    "volume": "main",
    "checked": true,
    "id": "001d825180947bc35bc6f06c5268237977b16d41",
    "citation_count": 38
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2471_ECCV_2020_paper.php": {
    "title": "Gradient Centralization: A New Optimization Technique for Deep Neural Networks",
    "abstract": "Optimization techniques are of great importance to eﬀectively and eﬃciently train a deep neural network (DNN). It has been shown that using the ﬁrst and second order statistics (e.g., mean and variance) to perform Z-score standardization on network activations or weight vectors, such as batch normalization (BN) and weight standardization (WS), can improve the training performance. Diﬀerent from those previous methods that mostly operate on activations or weights, we present a new optimization technique, namely gradient centralization (GC), which operates directly on gradients by centralizing the gradient vectors to have zero mean. GC can be viewed as a projected gradient descent method with a constrained loss function. We show that GC can regularize both the weight space and output feature space so that it can boost the generalization performance of DNNs. Moreover, GC improves the Lipschitzness of the loss function and its gradient so that the training process becomes more eﬃcient and stable. GC is very simple to implement and can be easily embedded into existing gradient based DNN optimizers with only one line of code. It can also be directly used to ﬁne-tune the pre-trained DNNs. Our experiments on various applications, including general image classiﬁcation, ﬁne-grained image classiﬁcation, detection and segmentation, demonstrate that GC can consistently improve the performance of DNN learning",
    "volume": "main",
    "checked": true,
    "id": "f7e43eaf7d7b3cf10d868ed46d5d9390adbb239c",
    "citation_count": 98
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2503_ECCV_2020_paper.php": {
    "title": "Content-Aware Unsupervised Deep Homography Estimation",
    "abstract": "Homography estimation is a basic image alignment method in many applications. It is usually done by extracting and matching sparse feature points, which are error-prone in low-light and low-texture images. On the other hand, previous deep homography approaches use either synthetic images for supervised learning or aerial images for unsupervised learning, both ignoring the importance of handling depth disparities and moving objects in real world applications. To overcome these problems, in this work we propose an unsupervised deep homography method with a new architecture design. In the spirit of the RANSAC procedure in traditional methods, we specifically learn an outlier mask to only select reliable regions for homography estimation. We calculate loss with respect to our learned deep features instead of directly comparing image content as did previously. To achieve the unsupervised training, we also formulate a novel triplet loss customized for our network. We verify our method by conducting comprehensive comparisons on a new dataset that covers a wide range of scenes with varying degrees of difficulties for the task. Experimental results reveal that our method outperforms the state-of-the-art including deep solutions and feature-based solutions",
    "volume": "main",
    "checked": true,
    "id": "a2ea64b6237bb53948b1660125c476f49b9bf684",
    "citation_count": 59
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2556_ECCV_2020_paper.php": {
    "title": "Multi-View Optimization of Local Feature Geometry",
    "abstract": "In this work, we address the problem of refining the geometry of local image features from multiple views without known scene or camera geometry. Current approaches to local feature detection are inherently limited in their keypoint localization accuracy because they only operate on a single view. This limitation has a negative impact on downstream tasks such as Structure-from-Motion, where inaccurate keypoints lead to large errors in triangulation and camera localization. Our proposed method naturally complements the traditional feature extraction and matching paradigm. We first estimate local geometric transformations between tentative matches and then optimize the keypoint locations over multiple views jointly according to a non-linear least squares formulation. Throughout a variety of experiments, we show that our method consistently improves the triangulation and camera localization performance for both hand-crafted and learned local features",
    "volume": "main",
    "checked": true,
    "id": "8430befc9b2f23cdc4a4ba694019eac1de8329b9",
    "citation_count": 12
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2597_ECCV_2020_paper.php": {
    "title": "The Phong Surface: Efficient 3D Model Fitting using Lifted Optimization",
    "abstract": "Realtime perceptual and interaction capabilities in mixed reality require a range of 3D tracking problems to be solved at low latency on resource-constrained hardware such as head-mounted devices. Indeed, for devices such as HoloLens 2 where the CPU and GPU are left available for applications, multiple tracking subsystems are required to run on a continuous, real-time basis while sharing a single Digital Signal Processor. To solve model-fitting problems for HoloLens 2 hand tracking, where the computational budget is approximately 100 times smaller than an iPhone 7, we introduce a new surface model: the `Phong surface'. Using ideas from computer graphics, the Phong surface describes the same 3D shape as a triangulated mesh model, but with continuous surface normals which enable the use of lifting-based optimization, providing significant efficiency gains over ICP-based methods. We show that Phong surfaces retain the convergence benefits of smoother surface models, while triangle meshes do not",
    "volume": "main",
    "checked": true,
    "id": "e1b77d9a8d4c5aed8cdc7e9ffa545052622829f5",
    "citation_count": 6
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2641_ECCV_2020_paper.php": {
    "title": "Forecasting Human-Object Interaction: Joint Prediction of Motor Attention and Actions in First Person Video",
    "abstract": "We address the challenging task of anticipating human-object interaction in first person videos. Most existing methods either ignore how the camera wearer interacts with objects, or simply considers body motion as a separate modality. In contrast, we observe that the intentional hand movement reveals critical information about the future activity. Motivated by this observation, we adopt intentional hand movement as a feature representation, and propose a novel deep network that jointly models and predicts the egocentric hand motion, interaction hotspots and future action. Specifically, we consider the future hand motion as the motor attention, and model this attention using probabilistic variables in our deep model. The predicted motor attention is further used to select the discriminative spatial-temporal visual features for predicting actions and interaction hotspots. We present extensive experiments demonstrating the benefit of the proposed joint model. Importantly, our model produces new state-of-the-art results for action anticipation on both EGTEA Gaze+ and the EPIC-Kitchens datasets. Our project page is available at https://aptx4869lm.github.io/ForecastingHOI/",
    "volume": "main",
    "checked": true,
    "id": "994481d46df92709b61614f5e756e40df4117622",
    "citation_count": 40
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2683_ECCV_2020_paper.php": {
    "title": "Learning Stereo from Single Images",
    "abstract": "Supervised deep networks are among the best methods for finding correspondences in stereo image pairs. Like all supervised approaches, these networks require ground truth data during training. However, collecting large quantities of accurate dense correspondence data is very challenging. We propose that it is unnecessary to have such a high reliance on ground truth depths or even corresponding stereo pairs. Inspired by recent progress in monocular depth estimation, we generate plausible disparity maps from single images. In turn, we use those flawed disparity maps in a carefully designed pipeline to generate stereo training pairs. Training in this manner makes it possible to convert any collection of single RGB images into stereo training data. This results in a significant reduction in human effort, with no need to collect real depths or to hand-design synthetic data. We can consequently train a stereo matching network from scratch on datasets like COCO, which were previously hard to exploit for stereo. Through extensive experiments we show that our approach outperforms stereo networks trained with standard synthetic datasets, when evaluated on KITTI, ETH3D, and Middlebury",
    "volume": "main",
    "checked": true,
    "id": "039154f8e22f55c5b53de1caf95825dd77708129",
    "citation_count": 28
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2748_ECCV_2020_paper.php": {
    "title": "Prototype Rectification for Few-Shot Learning",
    "abstract": "Few-shot learning requires to recognize novel classes with scarce labeled data. Prototypical network is useful in existing researches, however, training on narrow-size distribution of scarce data usually tends to get biased prototypes. In this paper, we figure out two key influencing factors of the process: the intra-class bias and the cross-class bias. We then propose a simple yet effective approach for prototype rectification in transductive setting. The approach utilizes label propagation to diminish the intra-class bias and feature shifting to diminish the cross-class bias. We also conduct theoretical analysis to derive its rationality as well as the lower bound of the performance. Effectiveness is shown on three few-shot benchmarks. Notably, our approach achieves state-of-the-art performance on both miniImageNet (70.31% on 1-shot and 81.89% on 5-shot) and tieredImageNet (78.74% on 1-shot and 86.92% on 5-shot)",
    "volume": "main",
    "checked": true,
    "id": "633c5630ca60cdab63b50d836d5f7939a5fdd8ad",
    "citation_count": 111
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2784_ECCV_2020_paper.php": {
    "title": "Learning Feature Descriptors using Camera Pose Supervision",
    "abstract": "Recent research on learned visual descriptors has shown promising improvements in correspondence estimation, a key component of many 3D vision tasks. However, existing descriptor learning frameworks typically require ground-truth correspondences between feature points for training, which are challenging to acquire at scale. In this paper we propose a novel weakly-supervised framework that can learn feature descriptors solely from relative camera poses between images. To do so, we devise both a new loss function that exploits the epipolar constraint given by camera poses, and a new model architecture that makes the whole pipeline differentiable and efficient. Because we no longer need pixel-level ground-truth correspondences, our framework opens up the possibility of training on much larger and more diverse datasets for better and unbiased descriptors. We call the resulting descriptors CAmera Pose Supervised, or CAPS, descriptors. Though trained with weak supervision, CAPS descriptors outperform even prior fully-supervised descriptors and achieve state-of-the-art performance on a variety of geometric tasks",
    "volume": "main",
    "checked": true,
    "id": "3a600dd4c9bdf7ec938dcd334f7e5ea13901c1b1",
    "citation_count": 52
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2785_ECCV_2020_paper.php": {
    "title": "Semantic Flow for Fast and Accurate Scene Parsing",
    "abstract": "In this paper, we focus on designing effective method for fast and accurate scene parsing. A common practice to improve the performance is to attain high resolution feature maps with strong semantic representation. Two strategies are widely used---atrous convolutions and feature pyramid fusion, are either computation intensive or ineffective. Inspired by the Optical Flow for motion alignment between adjacent video frames, we propose a Flow Alignment Module (FAM) to learn Semantic Flow between feature maps of adjacent levels, and broadcast high-level features to high resolution features effectively and efficiently. Furthermore, integrating our module to a common feature pyramid structure exhibits superior performance over other real-time methods even on light-weight backbone networks, such as ResNet-18. Extensive experiments are conducted on several challenging datasets, including Cityscapes, PASCAL Context, ADE20K and CamVid. Especially, our network is the first to achieve 80.4\\% mIoU on Cityscapes with a frame rate of 26 FPS. The code is available at \\url{https://github.com/donnyyou/torchcv}",
    "volume": "main",
    "checked": true,
    "id": "01831d9b657e011fb3d6d713f42001ccab9b769c",
    "citation_count": 104
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2788_ECCV_2020_paper.php": {
    "title": "Appearance Consensus Driven Self-Supervised Human Mesh Recovery",
    "abstract": "We present a self-supervised human mesh recovery framework to infer human pose and shape from monocular images in the absence of any paired supervision. Recent advances have shifted the interest towards directly regressing parameters of a parametric human model by supervising them on large-scale, images with 2D landmark annotations. This limits the generalizability of such approaches to operate on samples from unlabeled wild environments. Acknowledging this we propose a novel appearance consensus driven self-supervised objective. To effectively disentangle the foreground (FG) human we rely on image pairs depicting the same person (consistent FG) in varied pose and background (BG) which are obtained from unlabeled wild videos. The proposed FG appearance consistency objective makes use of a novel, differentiable extit{Color-recovery} module to obtain vertex colors without involving any trainable appearance extraction network; via efficient realization of color-picking and reflectional symmetry. We achieve state-of-the-art results on the standard model-based 3D pose estimation benchmarks at comparable supervision levels. Furthermore, the resulting colored mesh prediction opens up usage of our framework for a variety of appearance-related tasks beyond pose and shape estimation, thus establishing our superior generalizability",
    "volume": "main",
    "checked": true,
    "id": "b2e306dec48bfa4a7c229c009fa80d0458d09c6a",
    "citation_count": 20
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2825_ECCV_2020_paper.php": {
    "title": "Diffraction Line Imaging",
    "abstract": "We present a novel computational imaging principle that combines diffractive optics with line (1D) sensing. When light passes through a diffraction grating, it disperses as a function of wavelength. We exploit this principle to recover 2D and even 3D positions from only line images. We derive a detailed image formation model and a learning-based algorithm for 2D position estimation. We show several extensions of our system to improve the accuracy of the 2D positioning and expand the effective field of view. We demonstrate our approach in two applications: (a) fast passive imaging of sparse light sources like street lamps, headlights at night and LED-based motion capture, and (b) structured light 3D scanning with line illumination and line sensing. Line imaging has several advantages over 2D sensors: high frame rate, high dynamic range, high fill-factor with additional on-chip computation, low-cost beyond the visible spectrum, and high energy efficiency when used with line illumination. Thus, our system is able to achieve high-speed and high-accuracy 2D positioning of light sources and 3D scanning of scenes",
    "volume": "main",
    "checked": true,
    "id": "caad920c01270c9109e97b17d8c8f2e96ab6683c",
    "citation_count": 2
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2834_ECCV_2020_paper.php": {
    "title": "Aligning and Projecting Images to Class-conditional Generative Networks",
    "abstract": "We present a method for projecting an input image into the space of a class-conditional generative neural network. We propose a method that optimizes for transformation to counteract the model biases in generative neural networks. Specifically, we demonstrate that one can solve for image translation, scale, and global color transformation, during the projection optimization to address the object-center bias and color bias of a Generative Adversarial Network. This projection process poses a difficult optimization problem, and purely gradient-based optimizations fail to find good solutions. We describe a hybrid optimization strategy that finds good projections by estimating transformations and class parameters. We show the effectiveness of our method on real images and further demonstrate how the corresponding projections lead to better editability of these images",
    "volume": "main",
    "checked": false,
    "id": "5361f372294d0d5405763fcf983b23919817055d",
    "citation_count": 58
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2852_ECCV_2020_paper.php": {
    "title": "Suppress and Balance: A Simple Gated Network for Salient Object Detection",
    "abstract": "Most salient object detection approaches use U-Net or feature pyramid networks (FPN) as their basic structures. These methods ignore two key problems when the encoder exchanges information with the decoder: one is the lack of interference control between them, the other is without considering the disparity of the contributions of different encoder blocks. In this work, we propose a simple gated network (GateNet) to solve both issues at once. With the help of multilevel gate units, the valuable context information from the encoder can be optimally transmitted to the decoder. We design a novel gated dual branch structure to build the cooperation among different levels of features and improve the discriminability of the whole network. Through the dual branch design, more details of the saliency map can be further restored. In addition, we adopt the atrous spatial pyramid pooling based on the proposed “Fold” operation (Fold-ASPP) to accurately localize salient objects of various scales. Extensive experiments on five challenging datasets demonstrate that the proposed model performs favorably against most state-of-the-art methods under different evaluation metrics",
    "volume": "main",
    "checked": true,
    "id": "a0f9736c164afe7f57e232fe6c3e626ffee333f8",
    "citation_count": 162
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2904_ECCV_2020_paper.php": {
    "title": "Visual Memorability for Robotic Interestingness via Unsupervised Online Learning",
    "abstract": "In this paper, we explore the problem of interesting scene prediction for mobile robots. This area is currently underexplored but is crucial for many practical applications such as autonomous exploration and decision making. Inspired by industrial demands, we first propose a novel translation-invariant visual memory for recalling and identifying interesting scenes, then design a three-stage architecture of long-term, short-term, and online learning. This enables our system to learn human-like experience, environmental knowledge, and online adaption, respectively. Our approach achieves much higher accuracy than the state-of-the-art algorithms on challenging robotic interestingness datasets",
    "volume": "main",
    "checked": true,
    "id": "26bc8f0d2b61b5cded6b4850ec1aeab3e1ae0fab",
    "citation_count": 15
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2949_ECCV_2020_paper.php": {
    "title": "Post-Training Piecewise Linear Quantization for Deep Neural Networks",
    "abstract": "Quantization plays an important role in the energy-efficient deployment of Deep Neural Networks (DNNs) on resource-limited devices. Post-training quantization is highly desirable since it does not require retraining or access to the full training dataset. The well-established uniform scheme for post-training quantization achieves satisfactory results by converting DNNs from full-precision to 8-bit fixed-point integers. However, it suffers from significant performance degradation when quantizing to lower bit-widths. In this paper, we propose a PieceWise Linear Quantization (PWLQ) scheme to enable accurate approximation for tensor values that have bell-shaped distributions with long tails. Our approach breaks the entire quantization range into non-overlapping regions for each tensor, with each region being assigned an equal number of quantization levels. Optimal breakpoints that divide the entire range are found by minimizing the quantization error. Compared to state-of-the-art post-training quantization methods, experimental results show that our proposed method achieves superior performance on image classification, semantic segmentation, and object detection with minor overhead",
    "volume": "main",
    "checked": true,
    "id": "c655e5aff8291f0e71401a3cf3f3d60ae8adfc0d",
    "citation_count": 50
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2974_ECCV_2020_paper.php": {
    "title": "Joint Disentangling and Adaptation for Cross-Domain Person Re-Identification",
    "abstract": "Although a significant progress has been witnessed in supervised person re-identification (re-id), it remains challenging to generalize re-id models to new domains due to the huge domain gaps. Recently, there has been a growing interest in using unsupervised domain adaptation to address this scalability issue. Existing methods typically conduct adaptation on the representation space that contains both id-related and id-unrelated factors, thus inevitably undermining the adaptation efficacy of id-related features. In this paper, we seek to improve adaptation by purifying the representation space to be adapted. To this end, we propose a joint learning framework that disentangles id-related/unrelated features and enforces adaptation to work on the id-related feature space exclusively. Our model involves a disentangling module that encodes cross-domain images into a shared appearance space and two separate structure spaces, and an adaptation module that performs adversarial alignment and self-training on the shared appearance space. The two modules are co-designed to be mutually beneficial. Extensive experiments demonstrate that the proposed joint learning framework outperforms the state-of-the-art methods by clear margins",
    "volume": "main",
    "checked": true,
    "id": "b2be518280db9f7cd1c73f88bcc7c53401c8ce03",
    "citation_count": 100
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2978_ECCV_2020_paper.php": {
    "title": "In-Home Daily-Life Captioning Using Radio Signals",
    "abstract": "This paper aims to caption daily life --i.e., to create a textual description of people's activities and interactions with objects in their homes. Addressing this problem requires novel methods beyond traditional video captioning, as most people would have privacy concerns about deploying cameras throughout their homes. We introduce RF-Diary, a new model for captioning daily life by analyzing the privacy-preserving radio signal in the home with the home's floormap. RF-Diary can further observe and caption people's life through walls and occlusions and in dark settings. In designing RF-Diary, we exploit the ability of radio signals to capture people's 3D dynamics, and use the floormap to help the model learn people's interactions with objects. We also use a multi-modal feature alignment training scheme that leverages existing video-based captioning datasets to improve the performance of our radio-based captioning model. Extensive experimental results demonstrate that RF-Diary generates accurate captions under visible conditions. It also sustains its good performance in dark or occluded settings, where video-based captioning approaches fail to generate meaningful captions",
    "volume": "main",
    "checked": true,
    "id": "fadd6e5a8e877884dccb7ca5c8167f32f65ec5c4",
    "citation_count": 20
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3018_ECCV_2020_paper.php": {
    "title": "Self-Challenging Improves Cross-Domain Generalization",
    "abstract": "Convolutional Neural Networks (CNN) conduct image classification by activating dominant features that correlated with labels. When the training and testing data are under similar distributions, their dominant features are similar, leading to decent test performance. The performance is nonetheless unmet when tested with different distributions, leading to the challenges in cross-domain image classification. We introduce a simple training heuristic, Representation Self-Challenging (RSC), that significantly improves the generalization of CNN to the out-of-domain data. RSC iteratively challenges (discards) the dominant features activated on the training data, and forces the network to activate remaining features that correlate with labels. This process appears to activate feature representations applicable to out-of-domain data without prior knowledge of the new domain and without learning extra network parameters. We present the theoretical properties and conditions of RSC for improving cross-domain generalization. The experiments endorse the simple, effective, and architecture-agnostic nature of our RSC method",
    "volume": "main",
    "checked": true,
    "id": "09472ff0d3c3f975ef1fdc02cfb1605d3d4275fa",
    "citation_count": 214
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3029_ECCV_2020_paper.php": {
    "title": "A Competence-aware Curriculum for Visual Concepts Learning via Question Answering",
    "abstract": "Humans can progressively learn visual concepts from easy to hard questions. To mimic this efficient learning ability, we propose a competence-aware curriculum for visual concept learning in a question-answering manner. Specifically, we design a neural-symbolic concept learner for learning the visual concepts and a multi-dimensional Item Response Theory (mIRT) model for guiding the learning process with an adaptive curriculum. The mIRT effectively estimates the concept difficulty and the model competence at each learning step from accumulated model responses. The estimated concept difficulty and model competence are further utilized to select the most profitable training samples. Experimental results on CLEVR show that with a competence-aware curriculum, the proposed method achieves state-of-the-art performances with superior data efficiency and convergence speed. Specifically, the proposed model only uses 40% of training data and converges three times faster compared with other state-of-the-art methods",
    "volume": "main",
    "checked": true,
    "id": "c2d6fee1cc06354fdb3811aaa06910f4e34cd4a7",
    "citation_count": 16
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3047_ECCV_2020_paper.php": {
    "title": "Multitask Learning Strengthens Adversarial Robustness",
    "abstract": "Although deep networks achieve strong accuracy on a range of computer vision benchmarks, they remain vulnerable to adversarial attacks, where imperceptible input perturbations fool the network. We present both theoretical and empirical analyses that connect the adversarial robustness of a model to the number of tasks that it is trained on. Experiments on two datasets show that attack difficulty increases as the number of target tasks increase. Moreover, our results suggest that when models are trained on multiple tasks at once, they become more robust to adversarial attacks on individual tasks. While adversarial defense remains an open challenge, our results suggest that deep networks are vulnerable partly because they are trained on too few tasks",
    "volume": "main",
    "checked": true,
    "id": "5ba3c19d052edcf99a03b0734993d594c2f56e3c",
    "citation_count": 47
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3054_ECCV_2020_paper.php": {
    "title": "S2DNAS: Transforming Static CNN Model for Dynamic Inference via Neural Architecture Search",
    "abstract": "Recently, dynamic inference has emerged as a promising way to reduce the computational cost of deep convolutional neural networks (CNNs). In contrast to static methods (e.g., weight pruning), dynamic inference adaptively adjusts the inference process according to each input sample, which can considerably reduce the computational cost on \"\"easy\"\" samples while maintaining the overall model performance. In this paper, we introduce a general framework, S2DNAS, which can transform various static CNN models to support dynamic inference via neural architecture search. To this end, based on a given CNN model, we first generate a CNN architecture space in which each architecture is a multi-stage CNN generated from the given model using some predefined transformations. Then, we propose a reinforcement learning based approach to automatically search for the optimal CNN architecture in the generated space. At last, with the searched multi-stage network, we can perform dynamic inference by adaptively choosing a stage to evaluate for each sample. Unlike previous works that introduce irregular computations or complex controllers in the inference or re-design a CNN model from scratch, our method can generalize to most of the popular CNN architectures and the searched dynamic network can be directly deployed using existing deep learning frameworks in various hardware devices",
    "volume": "main",
    "checked": true,
    "id": "bb772fc6afa73560a4211516505bf5e17aac5319",
    "citation_count": 18
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3112_ECCV_2020_paper.php": {
    "title": "Improving Deep Video Compression by Resolution-adaptive Flow Coding",
    "abstract": "In the learning based video compression approaches, it is an essential issue to compress pixel-level optical flow maps by developing new motion vector (MV) encoders. In this work, we propose a new framework called Resolution-adaptive Flow Coding (RaFC) to effectively compress the flow maps globally and locally, in which we use multi-resolution representations instead of single-resolution representations for both the input flow maps and the output motion features of the MV encoder. To handle complex or simple motion patterns globally, our frame-level scheme RaFC-frame automatically decides the optimal flow map resolution for each video frame. To cope different types of motion patterns locally, our block-level scheme called RaFC-block can also select the optimal resolution for each local block of motion features. In addition, the rate-distortion criterion is applied to both RaFC-frame and RaFC-block and select the optimal motion coding mode for effective flow coding. Comprehensive experiments on four benchmark datasets HEVC, VTL, UVG and MCL-JCV clearly demonstrate the effectiveness of our overall RaFC framework after combing RaFC-frame and RaFC-block for video compression",
    "volume": "main",
    "checked": true,
    "id": "7950ab02b45afe1b44f71ff27666b484a340eb2d",
    "citation_count": 42
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3158_ECCV_2020_paper.php": {
    "title": "Motion Capture from Internet Videos",
    "abstract": "Recent advances in image-based human pose estimation make it possible to capture 3D human motion from a single RGB video. However, the inherent depth ambiguity and self-occlusion in a single view prohibit the recovery of as high-quality motion as multi-view reconstruction. While multi-view videos are not common, the videos of a celebrity performing a specific action are usually abundant on the Internet. Even if these videos were recorded at different time instances, they would encode the same motion characteristics of the person. Therefore, we propose to capture human motion by jointly analyzing these Internet videos instead of using single videos separately. However, this new task poses many new challenges that cannot be addressed by existing methods, as the videos are unsynchronized, the camera viewpoints are unknown, the background scenes are different, and the human motions are not exactly the same among videos. To address these challenges, we propose a novel optimization-based framework and experimentally demonstrate its ability to recover much more precise and detailed motion from multiple videos, compared against monocular motion capture methods",
    "volume": "main",
    "checked": true,
    "id": "807b4fe55ec95fc4a452d9b8cd3563db13859909",
    "citation_count": 29
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3183_ECCV_2020_paper.php": {
    "title": "Appearance-Preserving 3D Convolution for Video-based Person Re-identification",
    "abstract": "Due to the imperfect person detection results and posture changes, temporal appearance misalignment is unavoidable in video-based person re-identification (ReID). In this case, 3D convolution may destroy the appearance representation of person video clips, thus it is harmful to ReID. To address this problem, we propose Appearance-Preserving 3D Convolution (AP3D), which is composed of two components: an Appearance-Preserving Module (APM) and a 3D convolution kernel. With APM aligning the adjacent feature maps in pixel level, the following 3D convolution can model temporal information on the premise of maintaining the appearance representation quality. It is easy to combine AP3D with existing 3D ConvNets by simply replacing the original 3D convolution kernels with AP3Ds. Extensive experiments demonstrate the effectiveness of AP3D for video-based ReID and the results on three widely used datasets surpass the state-of-the-arts. Code is available at: https://github.com/guxinqian/AP3D",
    "volume": "main",
    "checked": true,
    "id": "468c12b1ff6a5c4f2630cdbaca214e6df0c935cc",
    "citation_count": 42
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3241_ECCV_2020_paper.php": {
    "title": "Solving the Blind Perspective-n-Point Problem End-To-End With Robust Differentiable Geometric Optimization",
    "abstract": "Blind Perspective-n-Point (PnP) is the problem of estimating the position and orientation of a camera relative to a scene, given 2D image points and 3D scene points, without prior knowledge of the 2D-3D correspondences. Solving for pose and correspondences simultaneously is extremely challenging since the search space is very large. Fortunately it is a coupled problem: the pose can be found easily given the correspondences and vice versa. Existing approaches assume that noisy correspondences are provided, that a good pose prior is available, or that the problem size is small. We instead propose the first fully end-to-end trainable network for solving the blind PnP problem efficiently and globally, that is, without the need for pose priors. We make use of recent results in differentiating optimization problems to incorporate geometric model fitting into an end-to-end learning framework, including Sinkhorn, RANSAC and PnP algorithms. Our proposed approach significantly outperforms other methods on synthetic and real data",
    "volume": "main",
    "checked": true,
    "id": "0c0c01827019b30b51479151b949bdac210093ad",
    "citation_count": 18
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3265_ECCV_2020_paper.php": {
    "title": "Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation",
    "abstract": "Learning a good image prior is a long-term goal for image restoration and manipulation. While existing methods like deep image prior (DIP) capture low-level image statistics, there are still gaps toward an image prior that captures rich image semantics including color, spatial coherence, textures, and high-level concepts. This work presents an effective way to exploit the image prior captured by a generative adversarial network (GAN) trained on large-scale natural images. As shown in Fig.1, the deep generative prior (DGP) provides compelling results to restore missing semantics, e.g., color, patch, resolution, of various degraded images. It also enables diverse image manipulation including random jittering, image morphing, and category transfer. Such highly flexible effects are made possible through relaxing the assumption of existing GAN-inversion methods, which tend to fix the generator. Notably, we allow the generator to be fine-tuned on-the-fly in a progressive manner regularized by feature distance obtained by the discriminator in GAN. We show that these easy-to-implement and practical changes help preserve the reconstruction to remain in the manifold of nature image, and thus lead to more precise and faithful reconstruction for real images. Code is at https://github.com/XingangPan/deep-generative-prior",
    "volume": "main",
    "checked": true,
    "id": "7101bc1c316740d99cd87185586829291a983a1d",
    "citation_count": 139
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3312_ECCV_2020_paper.php": {
    "title": "Deep Spatial-angular Regularization for Compressive Light Field Reconstruction over Coded Apertures",
    "abstract": "Coded aperture is a promising approach for capturing the 4-D light field (LF), in which the 4-D data are compressively modulated into 2-D coded measurements that are further decoded by reconstruction algorithms. The bottleneck lies in the reconstruction algorithms, resulting in rather limited reconstruction quality. To tackle this challenge, we propose a novel learning-based framework for the reconstruction of high-quality LFs from acquisitions via learned coded apertures. The proposed method incorporates the measurement observation into the deep learning framework elegantly to avoid relying entirely on data-driven priors for LF reconstruction. Specifically, we first formulate the compressive LF reconstruction as an inverse problem with an implicit regularization term. Then, we construct the regularization term with an efficient deep spatial-angular convolutional sub-network to comprehensively explore the signal distribution free from the limited representation ability and inefficiency of deterministic mathematical modeling. Experimental results show that the reconstructed LFs not only achieve much higher PSNR/SSIM but also preserve the LF parallax structure better, compared with state-of-the-art methods on both real and synthetic LF benchmarks. In addition, experiments show that our method is efficient and robust to noise, which is an essential advantage for a real camera system. The code is publicly available at https://github.com/angmt2008/LFCA",
    "volume": "main",
    "checked": true,
    "id": "cf406b94c187d465377c8103fb1d0513b4570e4d",
    "citation_count": 18
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3331_ECCV_2020_paper.php": {
    "title": "Video-based Remote Physiological Measurement via Cross-verified Feature Disentangling",
    "abstract": "Remote physiological measurements, e.g., remote photoplethysmography (rPPG) based heart rate (HR), heart rate variability (HRV) and respiration frequency (RF) measuring, are playing more and more important roles under the application scenarios where contact measurement is inconvenient or impossible. Since the amplitude of the physiological signals is usually very small, they can be easily affected by head movements, lighting conditions, and sensor diversities. To address these challenges, we propose a cross-verified feature disentangling strategy to disentangle the physiological features with non-physiological representations such as head movements and lighting conditions, and then use the distilled physiological features for robust multi-task physiological measurements. We first transform the input face videos into a multi-scale spatial-temporal map (MSTmap), which can suppress the irrelevant background and noise features while retaining most of the temporal characteristics of the periodic physiological signals. Then we take pairwise MSTmaps as inputs to an autoencoder architecture with two encoders (one for physiological signals and the other for non-physiological information) and use a cross-verified scheme to obtain physiological features disentangled with the non-physiological features. The disentangled features are finally used for the joint prediction of multiple physiological signals like average HR values and rPPG signals. Comprehensive experiments on different large-scale public datasets of multiple physiological measurement tasks as well as the cross-database testing demonstrate the robustness of our approach",
    "volume": "main",
    "checked": true,
    "id": "000f47c6bb00732dfe5302f85b64bc8896fc5457",
    "citation_count": 48
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3356_ECCV_2020_paper.php": {
    "title": "Combining Implicit Function Learning and Parametric Models for 3D Human Reconstruction",
    "abstract": "Implicit functions represented as deep learning approximations are powerful for reconstructing 3D surfaces. However, they can only produce static surfaces that are not controllable, which provides limited ability to modify the resulting model by editing its pose or shape parameters.Implicit functions represented as deep learning approximations are powerful for reconstructing 3D surfaces. However, they can only produce static surfaces that are not controllable, which provides limited ability to modify the resulting model by editing its pose or shape parameters. Nevertheless, such features are essential in building flexible models for both computer graphics and computer vision. In this work, we present methodology that combines detail-rich implicit functions and parametric representations in order to reconstruct 3D models of people that remain controllable and accurate even in the presence of clothing. Given sparse 3D point clouds sampled on the surface of a dressed person, we use an Implicit Part Network (IP-Net) to jointly predict the outer 3D surface of the dressed person, the inner body surface, and the semantic correspondences to a parametric body model. We subsequently use correspondences to fit the body model to our inner surface and then non-rigidly deform it (under a parametric body + displacement model) to the outer surface in order to capture garment, face and hair detail. In quantitative and qualitative experiments with both full body data and hand scans we show that the proposed methodology generalizes, and is effective even given incomplete point clouds collected from single-view depth images. We will release our models and code",
    "volume": "main",
    "checked": true,
    "id": "03e5d09cc7682df9f3bdac63fcec87eb298fcff8",
    "citation_count": 104
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3376_ECCV_2020_paper.php": {
    "title": "Orientation-aware Vehicle Re-identification with Semantics-guided Part Attention Network",
    "abstract": "Vehicle re-identification (re-ID) focuses on matching images of the same vehicle across different cameras. It is fundamentally challenging because differences between vehicles are sometimes subtle. While several studies incorporate spatial-attention mechanisms to help vehicle re-ID, they often require expensive keypoint labels or suffer from noisy attention if not trained with expensive labels. In this work, we propose a dedicated Semantics-guided Part Attention Network (SPAN) to robustly predict part attention masks for different views of vehicles given only image-level semantic labels during training. With the help of part attention masks, we can extract discriminative features in each part separately. Then we introduce Co-occurrence Part-attentive Distance Metric (CPDM) which places greater emphasis on co-occurrence vehicle parts when evaluating the feature distance of two images. Extensive experiments validate the effectiveness of the proposed method and show that our framework outperforms the state-of-the-art approaches",
    "volume": "main",
    "checked": true,
    "id": "3db110ac6a419cfa95662eb51bff6c52e6f671f2",
    "citation_count": 37
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3387_ECCV_2020_paper.php": {
    "title": "Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation",
    "abstract": "This paper studies the problem of learning semantic segmentation from image-level supervision only. Current popular solutions leverage object localization maps from classifiers as supervision signals, and struggle to make the localization maps capture more complete object content. Rather than previous efforts that primarily focus on intra-image information, we address the value of cross-image semantic relations for comprehensive object pattern mining. To achieve this, two neural co-attentions are incorporated into the classifier to complimentarily capture cross-image semantic similarities and differences. In particular, given a pair of training images, one co-attention enforces the classifier to recognize the common semantics from co-attentive objects, while the other one, called contrastive co-attention, drives the classifier to identify the unshared semantics from the rest, uncommon objects. This helps the classifier discover more object patterns and better ground semantics in image regions. In addition to boosting object pattern learning, the co-attention can leverage context from other related images to improve localization map inference, hence eventually benefiting semantic segmentation learning. More essentially, our algorithm provides a unified framework that handles well different WSSS settings, i.e., learning WSSS with (1) precise image-level supervision only, (2) extra simple single-label data, and (3) extra noisy web data. It sets new state-of-the-arts on all these settings, demonstrating well its efficacy and generalizability",
    "volume": "main",
    "checked": true,
    "id": "415a1db8bc9a92342c4b0f4ad150692e3766dab2",
    "citation_count": 142
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3439_ECCV_2020_paper.php": {
    "title": "CoReNet: Coherent 3D Scene Reconstruction from a Single RGB Image",
    "abstract": "Advances in deep learning techniques have allowed recent work to reconstruct the shape of a single object given only one RBG image as input. Building on common encoder-decoder architectures for this task, we propose three extensions: (1) ray-traced skip connections that propagate local 2D information to the output 3D volume in a physically correct manner; (2) a hybrid 3D volume representation that enables building translation equivariant models, while at the same time encoding fine object details without an excessive memory footprint; (3) a reconstruction loss tailored to capture overall object geometry. Furthermore, we adapt our model to address the harder task of reconstructing multiple objects from a single image. We reconstruct all objects jointly in one pass, producing a coherent reconstruction, where all objects live in a single consistent 3D coordinate frame relative to the camera and they do not intersect in 3D space. We also handle occlusions and resolve them by hallucinating the missing object parts in the 3D volume. We validate the impact of our contributions experimentally both on synthetic data from ShapeNet as well as real images from Pix3D. Our method improves over the state-of-the-art single-object methods on both datasets. Finally, we evaluate performance quantitatively on multiple object reconstruction with synthetic scenes assembled from ShapeNet objects",
    "volume": "main",
    "checked": true,
    "id": "44ae5310cc93d0b150611cda9b6d925c205e4486",
    "citation_count": 32
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3482_ECCV_2020_paper.php": {
    "title": "Layer-wise Conditioning Analysis in Exploring the Learning Dynamics of DNNs",
    "abstract": "Conditioning analysis uncovers the landscape of an optimization objective by exploring the spectrum of its curvature matrix. This has been well explored theoretically for linear models. We extend this analysis to deep neural networks (DNNs) in order to investigate their learning dynamics. To this end, we propose layer-wise conditioning analysis, which explores the optimization landscape with respect to each layer independently. Such an analysis is theoretically supported under mild assumptions that approximately hold in practice. Based on our analysis, we show that batch normalization (BN) can stabilize the training, but sometimes result in the false impression of a local minimum, which has detrimental effects on the learning. Besides, we experimentally observe that BN can improve the layer-wise conditioning of the optimization problem. Finally, we find that the last linear layer of a very deep residual network displays ill-conditioned behavior. We solve this problem by only adding one BN layer before the last linear layer, which achieves improved performance over the original and pre-activation residual networks",
    "volume": "main",
    "checked": true,
    "id": "4b819d954bc7c97882a55d4dded388e40a5924a1",
    "citation_count": 8
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3526_ECCV_2020_paper.php": {
    "title": "RAFT: Recurrent All-Pairs Field Transforms for Optical Flow",
    "abstract": "We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for estimating optical flow. RAFT extracts per-pixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a flow field through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves state-of-the-art performance on both KITTI and Sintel, with strong cross-dataset generalization and high efficiency in inference time, training speed, and parameter count",
    "volume": "main",
    "checked": true,
    "id": "3230e2d6b4671cc03974af2219c6d3270e6fac70",
    "citation_count": 626
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3528_ECCV_2020_paper.php": {
    "title": "Domain-invariant Stereo Matching Networks",
    "abstract": "State-of-the-art stereo matching networks have difficulties in generalizing to new unseen environments due to significant domain differences, such as color, illumination, contrast, and texture. In this paper, we aim at designing a domain-invariant stereo matching network (DSMNet) that generalizes well to unseen scenes. To achieve this goal, we propose i) a novel ``\"\"domain normalization\"\" approach that regularizes the distribution of learned representations to allow them to be invariant to domain differences, and ii) a trainable non-local graph-based filter for extracting robust structural and geometric representations that can further enhance domain-invariant generalizations. When trained on synthetic data and generalized to real test sets, our model performs significantly better than all state-of-the-art models. It even outperforms some deep learning models (e.g. DispNet, Content-CNN and MC-CNN) fine-tuned with test-domain data",
    "volume": "main",
    "checked": true,
    "id": "2a2b3ac844e98072015eab18cca4b54c95b2b039",
    "citation_count": 57
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3538_ECCV_2020_paper.php": {
    "title": "DeepHandMesh: A Weakly-supervised Deep Encoder-Decoder Framework for High-fidelity Hand Mesh Modeling",
    "abstract": "Human hands play a central role in interacting with other people and objects. For realistic replication of such hand motions, high-fidelity hand meshes have to be reconstructed. In this study, we firstly propose DeepHandMesh, a weakly-supervised deep encoder-decoder framework for high-fidelity hand mesh modeling. We design our system to be trained in an end-to-end and weakly-supervised manner; therefore, it does not require groundtruth meshes. Instead, it relies on weaker supervisions such as 3D joint coordinates and multi-view depth maps, which are easier to get than groundtruth meshes and do not dependent on the mesh topology. Although the proposed DeepHandMesh is trained in a weakly-supervised way, it provides significantly more realistic hand mesh than previous fully-supervised hand models. Our newly introduced penetration avoidance loss further improves results by replicating physical interaction between hand parts. Finally, we demonstrate that our system can also be applied successfully to the 3D hand mesh estimation from general images. Our hand model, dataset, and codes are publicly availableootnote{\\url{https://mks0601.github.io/DeepHandMesh/}}",
    "volume": "main",
    "checked": true,
    "id": "9aa8eab0446d9c8153a904f1dc632f2d7290d4a2",
    "citation_count": 32
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3544_ECCV_2020_paper.php": {
    "title": "Content Adaptive and Error Propagation Aware Deep Video Compression",
    "abstract": "Recently, learning based video compression methods attract increasing attention. However, previous works suffer from error propagation, which stems from the accumulation of reconstructed error in inter predictive coding. Meanwhile, previous learning based video codecs are also not adaptive to different video contents. To address these two problems, we propose a content adaptive and error propagation aware video compression system. Specifically, our method employs a joint training strategy by considering the compression performance of multiple consecutive frames instead of a single frame. Based on the learned long-term temporal information, our approach effectively alleviates error propagation in reconstructed frames. More importantly, instead of using the hand-crafted coding modes in the traditional compression systems, we design an online encoder updating scheme for the learned video compression. The proposed approach updates the parameters for encoder according to the rate-distortion criterion but keeps the decoder unchanged in the inference stage. Therefore, the encoder is adaptive to different video contents and achieves better compression efficiency by reducing the domain gap between the training and testing datasets. Our method is simple yet effective and outperforms the state-of-the-art learning based video codec on benchmark datasets without increasing the model size or decreasing the decoding speed",
    "volume": "main",
    "checked": true,
    "id": "08105564e809831f48bd1385254959e985c833d2",
    "citation_count": 45
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3553_ECCV_2020_paper.php": {
    "title": "Towards Streaming Perception",
    "abstract": "Embodied perception refers to the ability of an autonomous agent to perceive its environment so that it can (re)act. The responsiveness of the agent is largely governed by latency of its processing pipeline. While past work has studied the algorithmic trade-off between latency and accuracy, there has not been a clear metric to compare different methods along the Pareto optimal latency-accuracy curve. We point out a discrepancy between standard offline evaluation and real-time applications: by the time an algorithm finishes processing a particular image frame, the surrounding world has changed. To these ends, we present an approach that coherently integrates latency and accuracy into a single metric for real-time online perception, which we refer to as \"\"streaming accuracy\"\". The key insight behind this metric is to jointly evaluate the output of the entire perception stack at every time instant, forcing the stack to consider the amount of streaming data that should be ignored while computation is occurring. More broadly, building upon this metric, we introduce a meta-benchmark that systematically converts any image understanding task into a streaming perception task. We focus on the illustrative tasks of object detection and instance segmentation in urban video streams, and contribute a novel dataset with high-quality and temporally-dense annotations. Our proposed solutions and their empirical analysis demonstrate a number of surprising conclusions: (1) there exists an optimal \"\"sweet spot\"\" that maximizes streaming accuracy along the Pareto optimal latency-accuracy curve, (2) asynchronous tracking and future forecasting naturally emerge as internal representations that enable streaming image understanding, and (3) dynamic scheduling can be used to overcome temporal aliasing, yielding the paradoxical result that latency is sometimes minimized by sitting idle and \"\"doing nothing\"\"",
    "volume": "main",
    "checked": true,
    "id": "2c62be4b55661e8037117d697a2c0b296453ed11",
    "citation_count": 44
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3570_ECCV_2020_paper.php": {
    "title": "Towards Automated Testing and Robustification by Semantic Adversarial Data Generation",
    "abstract": "Widespread application of computer vision systems in real world tasks is currently hindered by their unexpected behavior on unseen examples. This occurs due to limitations of empirical testing on finite test sets and lack of systematic methods to identify the breaking points of a trained model. In this work we propose semantic adversarial editing,a method to synthesize plausible but difficult data points on which our target model breaks down. We achieve this with a differentiable object synthesizer which can change an object’s appearance while retaining its pose. Constrained adversarial optimization of object appearance through this synthesizer produces rare/difficult versions of an object which fool the target object detector. Experiments show that our approach effectively synthesizes difficult test data, dropping the performance of YoloV3 detector by more than 20 mAP points by changing the appearance of a single object and discovering failure modes of the model. The generated semantic adversarial data can also be used to robustify the detector through data augmentation, consistently improving its performance in both standard and out-of-dataset-distribution test sets, across three different datasets",
    "volume": "main",
    "checked": true,
    "id": "f176d108578af18d209defc8ff9633942f7b794b",
    "citation_count": 5
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3582_ECCV_2020_paper.php": {
    "title": "Adversarial Generative Grammars for Human Activity Prediction",
    "abstract": "In this paper we propose an adversarial generative grammar model for future prediction. The objective is to learn a model that explicitly captures temporal dependencies, providing a capability to forecast multiple, distinct future activities. Our adversarial grammar is designed so that it can learn stochastic production rules from the data distribution, jointly with its latent non-terminal representations. Being able to select multiple production rules during inference leads to different predicted outcomes, thus efficiently modeling many plausible futures. The adversarial generative grammar is evaluated on the Charades, MultiTHUMOS, Human3.6M, and 50 Salads datasets and on two activity prediction tasks: future 3D human pose prediction and future activity prediction. The proposed adversarial grammar outperforms the stateof-the-art approaches, being able to predict much more accurately and further in the future, than prior work. Code will be open sourced",
    "volume": "main",
    "checked": true,
    "id": "49dcefd14e92f3ffd5871abf78ee8ca5067fbb49",
    "citation_count": 18
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3587_ECCV_2020_paper.php": {
    "title": "GDumb: A Simple Approach that Questions Our Progress in Continual Learning",
    "abstract": "We discuss a general formulation for the Continual Learning (CL) problem for classification---a learning task where a stream provides samples to a learner and the goal of the learner, depending on the samples it receives, is to continually upgrade its knowledge about the old classes and learn new ones. Our formulation takes inspiration from the open-set recognition problem where test scenarios do not necessarily belong to the training distribution. We also discuss various quirks and assumptions encoded in recently proposed approaches for CL. We argue that some oversimplify the problem to an extent that leaves it with very little practical importance, and makes it extremely easy to perform well on. To validate this, we propose GDumb that (1) greedily stores samples in memory as they come and; (2) at test time, trains a model from scratch using samples only in the memory. We show that even though GDumb is not specifically designed for CL problems, it obtains state-of-the-art accuracies (often with large margins) in almost all the experiments when compared to a multitude of recently proposed algorithms. Surprisingly, it outperforms approaches in CL formulations for which they were specifically designed. This, we believe, raises concerns regarding our progress in CL for classification. Overall, we hope our formulation, characterizations and discussions will help in designing realistically useful CL algorithms, and GDumb will serve as a strong contender for the same",
    "volume": "main",
    "checked": true,
    "id": "08bb0b86b15d55bed854ba9f236defbf61d3d0db",
    "citation_count": 177
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3622_ECCV_2020_paper.php": {
    "title": "Learning Lane Graph Representations for Motion Forecasting",
    "abstract": "We propose a motion forecasting model that exploits a novel structured map representation as well as actor-map interactions. Instead of encoding vectorized maps as raster images, we construct a lane graph from raw map data to explicitly preserve the map structure. To capture the complex topology and long range dependencies of the lane graph, we propose LaneGCN which extends graph convolutions with multiple adjacency matrices and along-lane dilation. To capture the complex interactions between actors and maps, we exploit a fusion network consisting of four types of interactions, actor-to-lane, lane-to-lane, lane-to-actor and actor-to-actor. Powered by LaneGCN and actor-map interactions, our model is able to predict accurate and realistic multi-modal trajectories. Our approach significantly outperforms the state-of-the-art on the large scale Argoverse motion forecasting benchmark",
    "volume": "main",
    "checked": true,
    "id": "5c319a012777e6a4bc813645117ca1d260361bd6",
    "citation_count": 164
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3651_ECCV_2020_paper.php": {
    "title": "What Matters in Unsupervised Optical Flow",
    "abstract": "We systematically compare and analyze a set of key components in unsupervised optical flow to identify which photometric loss, occlusion handling, and smoothness regularization is most effective. Alongside this investigation we construct a number of novel improvements to unsupervised flow models, such as cost volume normalization, stopping the gradient at the occlusion mask, encouraging smoothness before upsampling the flow field, and continual self-supervision with image resizing. By combining the results of our investigation with our improved model components, we are able to present a new unsupervised flow technique that significantly outperforms the previous unsupervised state-of-the-art and performs on par with supervised FlowNet2 on the KITTI 2015 dataset, while also being significantly simpler than related approaches",
    "volume": "main",
    "checked": true,
    "id": "8d2d1396063f677e0ba01e06b64afe4ddf46a4f1",
    "citation_count": 79
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3678_ECCV_2020_paper.php": {
    "title": "Synthesis and Completion of Facades from Satellite Imagery",
    "abstract": "Automatic satellite-based reconstruction enables large and widespread creation of urban areas. However, satellite imagery is often noisy and incomplete, and is not suitable for reconstructing detailed building facades. We present a machine learning-based inverse procedural modeling method to automatically create synthetic facades from satellite imagery. Our key observation is that building facades exhibit regular, grid-like structures. Hence, we can overcome the low-resolution, noisy, and partial building data obtained from satellite imagery by synthesizing the underlying facade layout. Our method infers regular facade details from satellite-based image-fragments of a building, and applies them to occluded or under-sampled parts of the building, resulting in plausible, crisp facades. Using urban areas from six cities, we compare our approach to several state-of-the-art image completion/in-filling methods and our approach consistently creates better facade images",
    "volume": "main",
    "checked": true,
    "id": "d7a5aa2068b30bfc6761035a73eb9e089173f118",
    "citation_count": 4
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3772_ECCV_2020_paper.php": {
    "title": "Mapillary Planet-Scale Depth Dataset",
    "abstract": "Learning-based methods produce remarkable results on single image depth tasks when trained on well-established benchmarks, however, there is a large gap from these benchmarks to real-world performance that is usually obscured by the common practice of fine-tuning on the target dataset. We introduce a new depth dataset that is an order of magnitude larger than previous offerings, but more importantly, contains an unprecedented gamut of locations, camera models and scene types while offering metric depth (not just up-to-scale). Additionally, we investigate the problem of training single image depth networks using images captured with many different cameras, validating an existing approach and proposing a simpler alternative. With our contributions we achieve excellent results on challenging benchmarks before fine-tuning, and set the state of the art on the popular KITTI dataset after fine-tuning",
    "volume": "main",
    "checked": true,
    "id": "88db96702268ea0d75b6d1082ee5e176be22ff00",
    "citation_count": 6
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3838_ECCV_2020_paper.php": {
    "title": "V2VNet: Vehicle-to-Vehicle Communication for Joint Perception and Prediction",
    "abstract": "In this paper, we explore the use of vehicle-to-vehicle (V2V) communication to improve the perception and motion forecasting performance of self-driving vehicles. By intelligently aggregating the information received from multiple nearby vehicles, we can observe the same scene from different viewpoints. This allows us to see through occlusions and detect actors at long range, where the observations are very sparse or non-existent. We also show that our approach of sending compressed deep feature map activations achieves high accuracy while satisfying communication bandwidth requirements",
    "volume": "main",
    "checked": true,
    "id": "728f3e5932cc5c7bc617c72d2bcc7aad658dfab0",
    "citation_count": 59
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3891_ECCV_2020_paper.php": {
    "title": "Training Interpretable Convolutional Neural Networks by Differentiating Class-specific Filters",
    "abstract": "Convolutional neural networks (CNNs) have been successfully used in a range of tasks. However, CNNs are often viewed as \"\"black-box\"\" and lack of interpretability. One main reason is due to the filter-class entanglement -- an intricate many-to-many correspondence between filters and classes. Most existing works attempt post-hoc interpretation on a pre-trained model, while neglecting to reduce the entanglement underlying the model. In contrast, we focus on alleviating filter-class entanglement during training. Inspired by cellular differentiation, we propose a novel strategy to train interpretable CNNs by encouraging class-specific filters, among which each filter responds to only one (or few) class. Concretely, we design a learnable sparse Class-Specific Gate (CSG) structure to assign each filter with one (or few) class in a flexible way. The gate allows a filter's activation to pass only when the input samples come from the specific class. Extensive experiments demonstrate the fabulous performance of our method in generating a sparse and highly class-related representation of the input, which leads to stronger interpretability. Moreover, comparing with the standard training strategy, our model displays benefits in applications like object localization and adversarial sample detection. Code link: https://github.com/hyliang96/CSGCNN",
    "volume": "main",
    "checked": true,
    "id": "79381a40ff807441f0d5c7afda06fc95f94e0da8",
    "citation_count": 16
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3948_ECCV_2020_paper.php": {
    "title": "EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning",
    "abstract": "Finding out the computational redundant part of a trained Deep Neural Network (DNN) is the key question that pruning algorithms target on. Many algorithms try to predict model performance of the pruned sub-nets by introducing various evaluation methods. But they are either inaccurate or very complicated for general application. In this work, we present a pruning method called EagleEye, in which a simple yet efficient evaluation component based on adaptive batch normalization is applied to unveil a strong correlation between different pruned DNN structures and their final settled accuracy. This strong correlation allows us to fast spot the pruned candidates with highest potential accuracy without actually fine-tuning them. This module is also general to plug-in and improve some existing pruning algorithms. EagleEye achieves better pruning performance than all of the studied pruning algorithms in our experiments. Concretely, to prune MobileNet V1 and ResNet-50, EagleEye outperforms all compared methods by up to 3.8%. Even in the more challenging experiments of pruning the compact model of MobileNet V1, EagleEye achieves the highest accuracy of 70.9% with an overall 50% operations (FLOPs) pruned. All accuracy results are Top-1 ImageNet classification accuracy. Source code and models are accessible to open-source community.https://github.com/anonymous47823493/EagleEye",
    "volume": "main",
    "checked": true,
    "id": "38a6d0c2f5231b95d626a24435f18134b4254898",
    "citation_count": 68
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3975_ECCV_2020_paper.php": {
    "title": "Intrinsic Point Cloud Interpolation via Dual Latent Space Navigation",
    "abstract": "We present a learning-based method for interpolating and manipulating 3D shapes represented as point clouds, that is explicitly designed to preserve intrinsic shape properties. Our approach is based on constructing a dual encoding space that enables shape synthesis and, at the same time, provides links to the intrinsic shape metric, which is typically not available on point cloud data. Our method works in a single pass and avoids expensive optimization, employed by existing techniques. Furthermore, the strong regularization provided by our dual latent space approach also helps to improve shape recovery in challenging settings from noisy point clouds across datasets. Extensive experiments show that our method results in more realistic and smoother interpolations compared to baselines",
    "volume": "main",
    "checked": true,
    "id": "01831fef2819279de24dbbd91e548d59ce8d6e44",
    "citation_count": 8
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3976_ECCV_2020_paper.php": {
    "title": "Cross-Domain Cascaded Deep Translation",
    "abstract": "In recent years we have witnessed tremendous progress in unpaired image-to-image translation, propelled by the emergence of DNNs and adversarial training strategies. However, most existing methods focus on transfer of style and appearance, rather than on shape translation. The latter task is challenging, due to its intricate non-local nature, which calls for additional supervision. We mitigate this by descending the deep layers of a pre-trained network, where the deep features contain more semantics, and applying the translation between these deep features. Our translation is performed in a cascaded, deep-to-shallow, fashion, along the deep feature hierarchy: we first translate between the deepest layers that encode the higher-level semantic content of the image, proceeding to translate the shallower layers, conditioned on the deeper ones. We further demonstrate the effectiveness of using pre-trained deep features in the context of unconditioned image generation",
    "volume": "main",
    "checked": true,
    "id": "26647468012b6d727a9bbfa204a630a1b28cf279",
    "citation_count": 7
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4043_ECCV_2020_paper.php": {
    "title": "Look Ma, no landmarks!\" – Unsupervised, Model-based Dense Face Alignment",
    "abstract": "no landmarks!\"\" - Unsupervised, model-based dense face alignment\",\"In this paper, we show how to train an image-to-image network to predict dense correspondence between a face image and a 3D morphable model using only the model for supervision. We show that both geometric parameters (shape, pose and camera intrinsics) and photometric parameters (texture and lighting) can be inferred directly from the correspondence map using linear least squares and our novel inverse spherical harmonic lighting model. The least squares residuals provide an unsupervised training signal that allows us to avoid artefacts common in the literature such as shrinking and conservative underfitting. Our approach uses a network that is 10$ imes$ smaller than parameter regression networks, significantly reduces sensitivity to image alignment and allows known camera calibration or multi-image constraints to be incorporated during inference. We achieve results competitive with state-of-the-art but without any auxiliary supervision used by previous methods",
    "volume": "main",
    "checked": false,
    "id": "e30e96cff46897350a126f103942f2c116328e18",
    "citation_count": 12
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4158_ECCV_2020_paper.php": {
    "title": "Online Invariance Selection for Local Feature Descriptors",
    "abstract": "To be invariant, or not to be invariant: that is the question formulated in this work about local descriptors. A limitation of current feature descriptors is the trade-off between generalization and discriminative power: more invariance means less informative descriptors. We propose to overcome this limitation with a disentanglement of invariance in local descriptors and with an online selection of the most appropriate invariance given the context. Our framework consists in a joint learning of multiple local descriptors with different levels of invariance and of meta descriptors encoding the regional variations of an image. The similarity of these meta descriptors across images is used to select the right invariance when matching the local descriptors. Our approach, named Local Invariance Selection at Runtime for Descriptors (LISRD), enables descriptors to adapt to adverse changes in images, while remaining discriminative when invariance is not required. We demonstrate that our method can boost the performance of current descriptors and outperforms state-of-the-art descriptors in several matching tasks, when evaluated on challenging datasets with day-night illumination as well as viewpoint changes",
    "volume": "main",
    "checked": true,
    "id": "7fa0de6b93bd44bf48f779345e778c1004b028fe",
    "citation_count": 25
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4179_ECCV_2020_paper.php": {
    "title": "Rethinking Image Inpainting via a Mutual Encoder-Decoder with Feature Equalizations",
    "abstract": "Deep encoder-decoder based CNNs have advanced image inpainting methods for hole filling. While existing methods recover structures and textures step-by-step in the hole regions, they typically use two encoder-decoders for separate recovery. The CNN features of each encoder are learned to capture either missing structures or textures without considering them as a whole. The insufficient utilization of these encoder features hampers the performance of recovering both structures and textures. In this paper, we propose a mutual encoder-decoder CNN for joint recovery of both. We use CNN features from the deep and shallow layers of the encoder to represent structures and textures of an input image, respectively. The deep layer features are sent to a structure branch, while the shallow layer features are sent to a texture branch. In each branch, we fill holes in multiple scales of the CNN features. The filled CNN features from both branches are concatenated and then equalized. During feature equalization, we reweigh channel attentions first and propose a bilateral propagation activation function to enable spatial equalization. To this end, the filled CNN features of structure and texture mutually benefit each other to represent image content at all feature levels. We then use the equalized feature to supplement decoder features for output image generation through skip connections. Experiments on benchmark datasets show that the proposed method is effective to recover structures and textures and performs favorably against state-of-the-art approaches",
    "volume": "main",
    "checked": false,
    "id": "c3b629d418baaf5612492a0cf9734c0f8d1c7789",
    "citation_count": 122
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4358_ECCV_2020_paper.php": {
    "title": "TextCaps: a Dataset for Image Captioning with Reading Comprehension",
    "abstract": "Image descriptions can help visually impaired people to quickly understand the image content. While we made significant progress in automatically describing images and optical character recognition, current approaches are unable to include written text in their descriptions, although text is omnipresent in human environments and frequently critical to understand our surroundings. To study how to comprehend text in the context of an image we collect a novel dataset, TextCaps, with 145k captions for 28k images. Our dataset challenges a model to recognize text, relate it to its visual context, and decide what part of the text to copy or paraphrase, requiring spatial, semantic, and visual reasoning between multiple text tokens and visual entities, such as objects. We study baselines and adapt existing approaches to this new task, which we refer to as image captioning with reading comprehension. Our analysis with automatic and human studies shows that our new TextCaps dataset provides many new technical challenges over previous datasets",
    "volume": "main",
    "checked": true,
    "id": "33eadd4e666a894306a22ba0839c5e0cef77280e",
    "citation_count": 65
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4423_ECCV_2020_paper.php": {
    "title": "It is not the Journey but the Destination: Endpoint Conditioned Trajectory Prediction",
    "abstract": "Human trajectory forecasting with multiple socially interact-ing agents is of critical importance for autonomous navigation in human environments, e.g., for self-driving cars and social robots. In this work, we present Predicted Endpoint Conditioned Network (PECNet) for flexible human trajectory prediction. PECNet infers distant trajectory endpoints to assist in long-range multi-modal trajectory prediction. A novel non-local social pooling layer enables PECNet to infer diverse yet socially compliant trajectories. Additionally, we present a simple “truncation-trick” for improving few-shot multi-modal trajectory prediction performance. We show that PECNet improves state-of-the-art performance on the Stanford Drone trajectory prediction benchmark by ~20.9% and on the ETH/UCY benchmark by ∼40.8%. Code available at the home page: https://karttikeya.github.io/publication/htf/",
    "volume": "main",
    "checked": true,
    "id": "08809dada9c78ed7cb6eec79e41c72759364d22f",
    "citation_count": 158
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4440_ECCV_2020_paper.php": {
    "title": "Learning What to Learn for Video Object Segmentation",
    "abstract": "Video object segmentation (VOS) is a highly challenging problem, since the target object is only defined by a first-frame reference mask during inference. The problem of how to capture and utilize this limited information to accurately segment the target remains a fundamental research question. We address this by introducing an end-to-end trainable VOS architecture that integrates a differentiable few-shot learner. Our learner is designed to predict a powerful parametric model of the target by minimizing a segmentation error in the first frame. We further go beyond the standard few-shot learning paradigm by learning what our target model should learn in order to maximize segmentation accuracy. We perform extensive experiments on standard benchmarks. Our approach sets a new state-of-the-art on the large-scale YouTube-VOS 2018 dataset by achieving an overall score of 81.5, corresponding to a 2.6% relative improvement over the previous best result. The code and models are available at https://github.com/visionml/pytracking",
    "volume": "main",
    "checked": true,
    "id": "79304c6c8f8689ff2e8fd37383d0691c991f7181",
    "citation_count": 65
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4732_ECCV_2020_paper.php": {
    "title": "SIZER: A Dataset and Model for Parsing 3D Clothing and Learning Size Sensitive 3D Clothing",
    "abstract": "While models of 3D clothing learned from real data exist, no method can predict clothing deformation as a function of garment size. In this paper, we introduce SizerNet to predict 3D clothing conditioned on human body shape and garment size parameters, and ParserNet to infer garment meshes and shape under clothing with personal details in a single pass from an input mesh. SizerNet allows to estimate and visualize the dressing effect of a garment in various sizes, and ParserNet allows to edit clothing of an input mesh directly, removing the need for scan segmentation, which is a challenging problem in itself. To learn these models, we introduce the SIZER dataset of clothing size variation which includes 100 different subjects wearing casual clothing items in various sizes, totaling to approximately 2000 scans. This dataset includes the scans, registrations to the SMPL model, scans segmented in clothing parts, garment category and size labels. Our experiments show better parsing accuracy and size prediction than baseline methods trained on SIZER. The code, model and dataset will be released for research purposes at: https://virtualhumans.mpi-inf.mpg.de/sizer/",
    "volume": "main",
    "checked": true,
    "id": "a0693c024553f8ee2eacacc366732c3aecbff8db",
    "citation_count": 56
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4866_ECCV_2020_paper.php": {
    "title": "LIMP: Learning Latent Shape Representations with Metric Preservation Priors",
    "abstract": "In this paper, we advocate the adoption of metric preservation as a powerful prior for learning latent representations of deformable 3D shapes. Key to our construction is the introduction of a geometric distortion criterion, defined directly on the decoded shapes, translating the preservation of the metric on the decoding to the formation of linear paths in the underlying latent space. Our rationale lies in the observation that training samples alone are often insufficient to endow generative models with high fidelity, motivating the need for large training datasets. In contrast, metric preservation provides a rigorous way to control the amount of geometric distortion incurring in the construction of the latent space, leading in turn to synthetic samples of higher quality. We further demonstrate, for the first time, the adoption of differentiable intrinsic distances in the backpropagation of a geodesic loss. Our geometric priors are particularly relevant in the presence of scarce training data, where learning any meaningful latent structure can be especially challenging. The effectiveness and potential of our generative model is showcased in applications of style transfer and content generation using triangle meshes and point clouds",
    "volume": "main",
    "checked": true,
    "id": "14f3f4309391407d6a4f09ce8ea9ea4d1557f3b9",
    "citation_count": 41
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/5277_ECCV_2020_paper.php": {
    "title": "Unsupervised Sketch to Photo Synthesis",
    "abstract": "Humans can envision a realistic photo given a free-hand sketch that is not only spatially imprecise and geometrically distorted but also without colors and visual details. We study unsupervised sketch to photo synthesis for the first time, learning from unpaired sketch and photo data where the target photo for a sketch is unknown during training. Existing works only deal with either style difference or spatial deformation alone, synthesizing photos from edge-aligned line drawings or transforming shapes within the same modality, e.g., color images. Our insight is to decompose the unsupervised sketch to photo synthesis task into two stages of translation: First shape translation from sketches to grayscale photos and then content enrichment from grayscale to color photos. We also incorporate a self-supervised denoising objective and an attention module to handle abstraction and style variations that are specific to sketches. Our synthesis is sketch-faithful and photo-realistic, enabling sketch-based image retrieval and automatic sketch generation that captures human visual perception beyond the edge map of a photo",
    "volume": "main",
    "checked": true,
    "id": "1a4c823d26b65c9c7f61555d4ace57dab8b62255",
    "citation_count": 19
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/5360_ECCV_2020_paper.php": {
    "title": "A Simple Way to Make Neural Networks Robust Against Diverse Image Corruptions",
    "abstract": "The human visual system is remarkably robust against a wide range of naturally occurring variations and corruptions like rain or snow. In contrast, the performance of modern image recognition models strongly degrades when evaluated on previously unseen corruptions. Here, we demonstrate that a simple but properly tuned training with additive Gaussian and Speckle noise generalizes surprisingly well to unseen corruptions, easily reaching the state of the art on the corruption benchmark ImageNet-C (with ResNet50) and on MNIST-C. We build on top of these strong baseline results and show that an adversarial training of the recognition model against locally correlated worst-case noise distributions leads to an additional increase in performance. This regularization can be combined with previously proposed defense methods for further improvement",
    "volume": "main",
    "checked": true,
    "id": "78dabf7a32f9b76da8212a101482096197b437cd",
    "citation_count": 113
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/5457_ECCV_2020_paper.php": {
    "title": "SoftPoolNet: Shape Descriptor for Point Cloud Completion and Classification",
    "abstract": "Point clouds are often the default choice for many applications as they exhibit more flexibility and efficiency than volumetric data. Nevertheless, their unorganized nature - points are stored in an unordered way - makes them less suited to be processed by deep learning pipelines. In this paper, we propose a method for 3D object completion and classification based on point clouds. We introduce a new way of organizing the extracted features based on their activations, which we name soft pooling. For the decoder stage, we propose regional convolutions, a novel operator aimed at maximizing the global activation entropy. Furthermore, inspired by the local refining procedure in Point Completion Network (PCN), we also propose a patch-deforming operation to simulate deconvolutional operations for point clouds. This paper proves that our regional activation can be incorporated in many point cloud architectures like AtlasNet and PCN, leading to better performance for geometric completion. We evaluate our approach on different 3D tasks such as object completion and classification, achieving state-of-the-art accuracy",
    "volume": "main",
    "checked": true,
    "id": "b91b7b20f910a97ea4dbd1ea7e78e5e115bc5f31",
    "citation_count": 41
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/5800_ECCV_2020_paper.php": {
    "title": "Hierarchical Face Aging through Disentangled Latent Characteristics",
    "abstract": "Current age datasets lie in a long-tailed distribution, which brings difficulties to describe the aging mechanism for the imbalance ages. To alleviate it, we design a novel facial age prior to guide the aging mechanism modeling. To explore the age effects on facial images, we propose a Disentangled Adversarial Autoencoder (DAAE) to disentangle the facial images into three independent factors: age, identity and extraneous information. To avoid the \"\"wash away\"\" of age and identity information in face aging process, we propose a hierarchical conditional generator by passing the disentangled identity and age embeddings to the high-level and low-level layers with class-conditional BatchNorm. Finally, a disentangled adversarial learning mechanism is introduced to boost the image quality for face aging. In this way, when manipulating the age distribution, DAAE can achieve face aging with arbitrary ages. Further, given an input face image, the mean value of the learned age posterior distribution can be treated as an age estimator. These indicate that DAAE can efficiently and accurately estimate the age distribution by a disentangling manner. DAAE is the first attempt to achieve facial age analysis tasks, including face aging with arbitrary ages, exemplar-based face aging and age estimation, in an universal framework. The qualitative and quantitative experiments demonstrate the superiority of DAAE on five popular datasets, including CACD2000, Morph, UTKFace, FG-NET and AgeDB",
    "volume": "main",
    "checked": true,
    "id": "2b721f97ef3494d9266205fd6f616355e756448d",
    "citation_count": 10
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/5859_ECCV_2020_paper.php": {
    "title": "Hybrid Models for Open Set Recognition",
    "abstract": "Open set recognition requires a classifier to detect samples not belonging to any of the classes in its training set. Existing methods fit a probability distribution to the training samples on their embedding space and detect outliers according to this distribution. The embedding space is often obtained from a discriminative classifier. However, such discriminative representation focuses only on known classes, which may not be critical for distinguishing the unknown classes. We argue that the representation space should be jointly learned from the inlier classifier and the density estimator (served as an outlier detector). We propose the OpenHybrid framework, which is composed of an encoder to encode the input data into a joint embedding space, a classifier to classify samples to inlier classes, and a flow-based density estimator to detect whether a sample belongs to the unknown category. A typical problem of existing flow-based models is that they may assign a higher likelihood to outliers. However, we empirically observe that such an issue does not occur in our experiments when learning a joint representation for discriminative and generative components. Experiments on standard open set benchmarks also reveal that an end-to-end trained OpenHybrid model significantly outperforms state-of-the-art methods and flow-based baselines",
    "volume": "main",
    "checked": true,
    "id": "549e8e5eea04b301cbb805f5502afffef492d344",
    "citation_count": 79
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/5932_ECCV_2020_paper.php": {
    "title": "TopoGAN: A Topology-Aware Generative Adversarial Network",
    "abstract": "Existing generative adversarial networks (GANs) focus on generating realistic images based on CNN-derived image features, but fail to preserve the structural properties of real images. This can be fatal in applications where the underlying structure (e.g., neurons, vessels, membranes, and road networks) of the image carries crucial semantic meaning. In this paper, we propose a novel GAN model that learns the topology of real images, i.e., connectedness and loopy-ness. In particular, we introduce a new loss that bridges the gap between synthetic image distribution and real image distribution in the topological feature space. By optimizing this loss, the generator produces images with the same structural topology as real images. We also propose new GAN evaluation metrics that measure the topological realism of the synthetic images. We show in experiments that our method generates synthetic images with realistic topology. We also highlight the increased performance that our method brings to downstream tasks such as segmentation",
    "volume": "main",
    "checked": true,
    "id": "d43c08f559cde513ff244a5997b8b0bbce25bd66",
    "citation_count": 27
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/6101_ECCV_2020_paper.php": {
    "title": "Learning to Localize Actions from Moments",
    "abstract": "With the knowledge of action moments (i.e., trimmed video clips that each contains an action instance), humans could routinely localize an action temporally in an untrimmed video. Nevertheless, most practical methods still require all training videos to be labeled with temporal annotations (action category and temporal boundary) and develop the models in a fully-supervised manner, despite expensive labeling efforts and inapplicable to new categories. In this paper, we introduce a new design of transfer learning type to learn action localization for a large set of action categories, but only on action moments from the categories of interest and temporal annotations of untrimmed videos from a small set of action classes. Specifically, we present Action Herald Networks (AherNet) that integrate such design into an one-stage action localization framework. Technically, a weight transfer function is uniquely devised to build the transformation between classification of action moments or foreground video segments and action localization in synthetic contextual moments or untrimmed videos. The context of each moment is learnt through the adversarial mechanism to differentiate the generated features from those of background in untrimmed videos. Extensive experiments are conducted on the learning both across the splits of ActivityNet v1.3 and from THUMOS14 to ActivityNet v1.3. Our AherNet demonstrates the superiority even comparing to most fully-supervised action localization methods. More remarkably, we train AherNet to localize actions from 600 categories on the leverage of action moments in Kinetics-600 and temporal annotations from 200 classes in ActivityNet v1.3",
    "volume": "main",
    "checked": true,
    "id": "2ae9cea622f68e1e32b7e6ae5a3c92213e192bf8",
    "citation_count": 9
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/6147_ECCV_2020_paper.php": {
    "title": "ForkGAN: Seeing into the Rainy Night",
    "abstract": "We present a ForkGAN for task-agnostic image translation that can boost multiple vision tasks in adverse weather conditions. Three tasks of image localization/retrieval, semantic image segmentation, and object detection are evaluated. The key challenge is achieving high-quality image translation without any explicit supervision, or task awareness. Our innovation is a fork-shape generator with one encoder and two decoders that disentangles the domain-specific and domain-invariant information. We force the cyclic translation between the weather conditions to go through a common encoding space, and make sure the encoding features reveal no information about the domains. Experimental results show our algorithm produces state-of-the-art image synthesis results and boost three vision tasks' performances in adverse weathers",
    "volume": "main",
    "checked": true,
    "id": "1cd2d08fded9446f49191a185671f3f73bdc83a7",
    "citation_count": 37
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/6209_ECCV_2020_paper.php": {
    "title": "TCGM: An Information-Theoretic Framework for Semi-Supervised Multi-Modality Learning",
    "abstract": "Fusing data from multiple modalities provides more information to train machine learning systems. However, it is prohibitively expensive and time-consuming to label each modality with a large amount of data, which leads to a crucial problem of such semi-supervised multi-modal learning. Existing methods suffer from either ineffective fusion across modalities or lack of theoretical results under proper assumptions. In this paper, we propose a novel information-theoretic approach \\-- namely, extbf{T}otal extbf{C}orrelation extbf{G}ain extbf{M}aximization (TCGM) \\--- for semi-supervised multi-modal learning, which is endowed with promising properties: (i) it can utilize effectively the information across different modalities of unlabeled data points to facilitate training classifiers of each modality (ii) has theoretical guarantee to have theoretical guarantee to identify Bayesian classifiers, i.e., the ground truth posteriors of all modalities. Specifically, by maximizing TC-induced loss (namely TC gain) over classifiers of all modalities, these classifiers can cooperatively discover the equivalent class of ground-truth classifiers; and identify the unique ones by leveraging a limited percentage of labeled data. We apply our method and can achieve state-of-the-art results on various datasets, including the Newsgroup dataset, Emotion recognition (IEMOCAP and MOSI) and Medical Imaging (Alzheimer’s Disease Neuroimaging Initiative)",
    "volume": "main",
    "checked": true,
    "id": "3611f04c1861b6e956597d56afafdefc71c6af6a",
    "citation_count": 8
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/6502_ECCV_2020_paper.php": {
    "title": "ExchNet: A Unified Hashing Network for Large-Scale Fine-Grained Image Retrieval",
    "abstract": "Retrieving content relevant images from a large-scale fine-grained dataset could suffer from intolerably slow query speed and highly redundant storage cost, due to high-dimensional real-valued embeddings which aim to distinguish subtle visual differences of fine-grained objects. In this paper, we study the novel fine-grained hashing topic to generate compact binary codes for fine-grained images, leveraging the search and storage efficiency of hash learning to alleviate the aforementioned problems. Specifically, we propose a unified end-to-end trainable network, termed as ExchNet. Based on attention mechanisms and proposed attention constraints, it can firstly obtain both local and global features to represent object parts and whole fine-grained objects, respectively. Furthermore, to ensure the discriminative ability and semantic meaning's consistency of these part-level features across images, we design a local feature alignment approach by performing a feature exchanging operation. Later, an alternative learning algorithm is employed to optimize the whole ExchNet and then generate the final binary hash codes. Validated by extensive experiments, our ExchNet consistently outperforms state-of-the-art generic hashing methods on five fine-grained datasets, which shows our effectiveness. Moreover, compared with other approximate nearest neighbor methods, ExchNet achieves the best speed-up and storage reduction, revealing its efficiency and practicality",
    "volume": "main",
    "checked": true,
    "id": "bc70c7bb5c861b38c7368ff0df511cfe06a284a2",
    "citation_count": 17
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/22_ECCV_2020_paper.php": {
    "title": "TSIT: A Simple and Versatile Framework for Image-to-Image Translation",
    "abstract": "We introduce a simple and versatile framework for image-to-image translation. We unearth the importance of normalization layers, and provide a carefully designed two-stream generative model with newly proposed feature transformations in a coarse-to-fine fashion. This allows multi-scale semantic structure information and style representation to be effectively captured and fused by the network, permitting our method to scale to various tasks in both unsupervised and supervised settings. No additional constraints (e.g., cycle consistency) are needed, contributing to a very clean and simple method. Multi-modal image synthesis with arbitrary style control is made possible. A systematic study compares the proposed method with several state-of-the-art task-specific baselines, verifying its effectiveness in both perceptual quality and quantitative evaluations",
    "volume": "main",
    "checked": true,
    "id": "71e7cb307df918fd0fdc2e9d6a823536bfcb2d28",
    "citation_count": 51
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/43_ECCV_2020_paper.php": {
    "title": "ProxyBNN: Learning Binarized Neural Networks via Proxy Matrices",
    "abstract": "Training Binarized Neural Networks (BNNs) is challenging due to the discreteness. In order to efficiently optimize BNNs through backward propagations, real-valued auxiliary variables are commonly used to accumulate gradient updates. Those auxiliary variables are then directly quantized to binary weights in the forward pass, which brings about large quantization errors. In this paper, by introducing an appropriate proxy matrix, we reduce the weights quantization error while circumventing explicit binary regularizations on the full-precision auxiliary variables. Specifically, we regard pre-binarization weights as a linear combination of the basis vectors. The matrix composed of basis vectors is referred to as the proxy matrix, and auxiliary variables serve as the coefficients of this linear combination. We are the first to empirically identify and study the effectiveness of learning both basis and coefficients to construct the pre-binarization weights. This new proxy learning contributes to new leading performances on benchmark datasets",
    "volume": "main",
    "checked": true,
    "id": "8381e7ec8ca3b490f6514fbbf74fc3f15afbf28c",
    "citation_count": 13
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/148_ECCV_2020_paper.php": {
    "title": "HMOR: Hierarchical Multi-Person Ordinal Relations for Monocular Multi-Person 3D Pose Estimation",
    "abstract": "Remarkable progress has been made in 3D human pose estimation from a monocular RGB camera. However, only a few studies explored 3D multi-person cases. In this paper, we attempt to address the lack of a global perspective of the top-down approaches by introducing a novel form of supervision - Hierarchical Multi-person Ordinal Relations (HMOR). The HMOR encodes interaction information as the ordinal relations of depths and angles hierarchically, which captures the body-part and joint level semantic and maintains global consistency at the same time. In our approach, an integrated top-down model is designed to leverage these ordinal relations in the learning process. The integrated model estimates human bounding boxes, human depths, and root-relative 3D poses simultaneously, with a coarse-to-fine architecture to improve the accuracy of depth estimation. The proposed method significantly outperforms state-of-the-art methods on publicly available multi-person 3D pose datasets (9.2 mm improvement on 3DPW dataset, 12.3 PCK improvement on MuPoTS-3D dataset, and 20.5 mm improvement on CMU Panoptic dataset). In addition to superior performance, our method costs lower computation complexity and fewer model parameters. Our code will be made publicly available",
    "volume": "main",
    "checked": true,
    "id": "0fcd80a78878b3ecdb4400eff60915c0e275503a",
    "citation_count": 40
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/193_ECCV_2020_paper.php": {
    "title": "Mask2CAD: 3D Shape Prediction by Learning to Segment and Retrieve",
    "abstract": "Object recognition has seen significant progress in the image domain, with focus primarily on 2D perception. We propose to leverage existing large-scale datasets of 3D models to understand the underlying 3D structure of objects seen in an image by constructing a CAD-based representation of the objects and their poses. We present Mask2CAD, which jointly detects objects in real-world images and for each detected object, optimizes for the most similar CAD model and its pose.We construct a joint embedding space between the detected regions of an image corresponding to an object and 3D CAD models, enabling retrieval of CAD models for an input RGB image. This produces a clean, lightweight representation of the objects in an image; this CAD-based representation ensures a valid, efficient shape representation for applications such as content creation or interactive scenarios, and makes a step towards understanding the transformation of real-world imagery to a synthetic domain.Experiments on real-world images from Pix3D demonstrate the advantage of our approach in comparison to state of the art. To facilitate future research, we additionally propose a new image-to-3D baseline on ScanNet which features larger shape diversity, real-world occlusions, and challenging image views",
    "volume": "main",
    "checked": true,
    "id": "14a549e8b26b993e7d792e03fb6969591c0ef9ff",
    "citation_count": 29
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/223_ECCV_2020_paper.php": {
    "title": "A Unified Framework of Surrogate Loss by Refactoring and Interpolation",
    "abstract": "We introduce UniLoss, a unified framework to generate surrogate losses for training deep networks with gradient descent, reducing the amount of manual design of task-specific surrogate losses. Our key observation is that in many cases, evaluating a model with a performance metric on a batch of examples can be refactored into four steps: from input to real-valued scores, from scores to comparisons of pairs of scores, from comparisons to binary variables, and from binary variables to the final performance metric. Using this refactoring we generate differentiable approximations for each non-differentiable step through interpolation. Using UniLoss, we can optimize for different tasks and metrics using one unified framework, achieving comparable performance compared with task-specific losses. We validate the effectiveness of UniLoss on three tasks and four datasets. Code is available at https://github.com/princeton-vl/uniloss",
    "volume": "main",
    "checked": true,
    "id": "15c4bf562e1caa89456dda5fe032ae4f25bc05e2",
    "citation_count": 6
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/362_ECCV_2020_paper.php": {
    "title": "Deep Reflectance Volumes: Relightable Reconstructions from Multi-View Photometric Images",
    "abstract": "We present a deep learning approach to reconstruct scene appearance from unstructured images captured under collocated point lighting. At the heart of Deep Reflectance Volumes is a novel volumetric scene representation consisting of opacity, surface normal and reflectance voxel grids. We present a novel physically-based differentiable volume ray marching framework to render these scene volumes under arbitrary viewpoint and lighting. This allows us to optimize the scene volumes to minimize the error between their rendered images and the captured images. Our method is able to reconstruct real scenes with challenging non-Lambertian reflectance and complex geometry with occlusions and shadowing. Moreover, it accurately generalizes to novel viewpoints and lighting, including non-collocated lighting, rendering photorealistic images that are significantly better than state-of-the-art mesh-based methods. We also show that our learned reflectance volumes are editable, allowing for modifying the materials of the captured scenes",
    "volume": "main",
    "checked": true,
    "id": "0a512f38e69351029c79f38046bac911fcd8207f",
    "citation_count": 50
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/366_ECCV_2020_paper.php": {
    "title": "Memory-augmented Dense Predictive Coding for Video Representation Learning",
    "abstract": "The objective of this paper is self-supervised learning from video, in particular for representations for action recognition. We make the following contributions: (i) We propose a new architecture and learning framework Memory-augmented Dense Predictive Coding (MemDPC) for the task. It is trained with a predictive attention mechanism over the set of compressed memories, such that any future states can always be constructed by a convex combination of the condense representations, allowing to make multiple hypotheses efficiently.(ii) We investigate visual-only self-supervised video representation learning from RGB frames, or from unsupervised optical flow, or both. (iii) We thoroughly evaluate the quality of learnt representation on four different downstream tasks: action recognition, video retrieval, learning with scarce annotations, and unintentional action classification. In all cases, we demonstrate state-of-the-art or comparable performance over other approaches with orders of magnitude fewer training data",
    "volume": "main",
    "checked": true,
    "id": "202c79bbb45ab6524141feacc81caacc4ba00401",
    "citation_count": 149
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/378_ECCV_2020_paper.php": {
    "title": "PointMixup: Augmentation for Point Clouds",
    "abstract": "This paper introduces data augmentation for point clouds by interpolation between examples. Data augmentation by interpolation has shown to be a simple and effective approach in the image domain. Such a mixup is however not directly transferable to point clouds, as we do not have a one-to-one correspondence between the points of two different objects. In this paper, we define data augmentation between point clouds as a shortest path linear interpolation. To that end, we introduce PointMixup, an interpolation method that generates new examples through an optimal assignment of the path function between two point clouds. We prove that our PointMixup finds the shortest path between two point clouds and that the interpolation is assignment invariant and linear. With the definition of interpolation, PointMixup allows to introduce strong interpolation-based regularizers such as mixup and manifold mixup to the point cloud domain. Experimentally, we show the potential of PointMixup for point cloud classification, especially when examples are scarce, as well as increased robustness to noise and geometric transformations to points. The code for PointMixup and the experimental details are publicly available",
    "volume": "main",
    "checked": true,
    "id": "19924af14342f97e7e6feed405595ae1888bc72c",
    "citation_count": 55
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/415_ECCV_2020_paper.php": {
    "title": "Identity-Guided Human Semantic Parsing for Person Re-Identification",
    "abstract": "Existing alignment-based methods have to employ the pre-trained human parsing models to achieve the pixel-level alignment, and cannot identify the personal belongings (e.g., backpacks and reticule) which are crucial to person re-ID. In this paper, we propose the identity-guided human semantic parsing approach (ISP) to locate both the human body parts and personal belongings at pixel-level for aligned person re-ID only with person identity labels. We design the cascaded clustering on feature maps to generate the pseudo-labels of human parts. Specifically, for the pixels of all images of a person, we first group them to foreground or background and then group the foreground pixels to human parts. The cluster assignments are subsequently used as pseudo-labels of human parts to supervise the part estimation and ISP iteratively learns the feature maps and groups them. Finally, local features of both human body parts and personal belongings are obtained according to the self-learned part estimation, and only features of visible parts are utilized for the retrieval. Extensive experiments on three widely used datasets validate the superiority of ISP over lots of state-of-the-art methods",
    "volume": "main",
    "checked": true,
    "id": "267ebbabf19e7f67aae6359be42ee2e12479ba17",
    "citation_count": 89
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/462_ECCV_2020_paper.php": {
    "title": "Learning Gradient Fields for Shape Generation",
    "abstract": "In this work, we propose a novel technique to generate shapes from point cloud data. A point cloud can be viewed as samples from a distribution of 3D points whose density is concentrated near the surface of the shape. Point cloud generation thus amounts to moving randomly sampled points to high-density areas. We generate point clouds by performing stochastic gradient ascent on an unnormalized probability density, thereby moving sampled points toward the high-likelihood regions. Our model directly predicts the gradient of the log density field and can be trained with a simple objective adapted from score-based generative models. We show that our method can reach state-of-the-art performance for point cloud auto-encoding and generation, while also allowing for extraction of a high-quality implicit surface",
    "volume": "main",
    "checked": true,
    "id": "e9675f461418a9c591486e5d4cecd4b42addfd75",
    "citation_count": 100
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/467_ECCV_2020_paper.php": {
    "title": "COCO-FUNIT: Few-Shot Unsupervised Image Translation with a Content Conditioned Style Encoder",
    "abstract": "Unsupervised image-to-image translation intends to learn a mapping of an image in a given domain to an analogous image in a different domain, without explicit supervision of the mapping. Few-shot unsupervised image-to-image translation further attempts to generalize the model to an unseen domain by leveraging example images of the unseen domain provided at inference time. While remarkably successful, existing few-shot image-to-image translation models find it difficult to preserve the structure of the input image while emulating the appearance of the unseen domain, which we refer to as the extit{content loss} problem. This is particularly severe when the poses of the objects in the input and example images are very different. To address the issue, we propose a new few-shot image translation model, COCO-FUNIT, which computes the style embedding of the example images conditioned on the input image and a new module called the constant style bias. Through extensive experimental validations with comparison to the state-of-the-art, our model shows effectiveness in addressing the extit{content loss} problem. Code and pretrained models are available at \\url{https://nvlabs.github.io/COCO-FUNIT/}",
    "volume": "main",
    "checked": true,
    "id": "3dfeba459696a573c72f115f30218e5c348db6d8",
    "citation_count": 41
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/492_ECCV_2020_paper.php": {
    "title": "Corner Proposal Network for Anchor-free, Two-stage Object Detection",
    "abstract": "Two-stage Object Detection\",\"The goal of object detection is to determine the class and location of objects in an image. This paper proposes a novel anchor-free, two-stage framework which first extracts a number of object proposals by finding potential corner keypoint combinations and then assigns a class label to each proposal by a standalone classification stage. We demonstrate that these two stages are effective solutions for improving recall and precision, respectively, and they can be integrated into an end-to-end network. Our approach, dubbed Corner Proposal Network (CPN), enjoys the ability to detect objects of various scales and also avoids being confused by a large number of false-positive proposals. On the MS-COCO dataset, CPN achieves an AP of 49.2% which is competitive among state-of-the-art object detection methods. CPN also fits the scenario of computational efficiency, which achieves an AP of 41.6%/39.7% at 26.2/43.3 FPS, surpassing most competitors with the same inference speed. Code is available at https://github.com/Duankaiwen/CPNDet",
    "volume": "main",
    "checked": true,
    "id": "9641a861cea30c1b4b92f1fb01748811597db022",
    "citation_count": 50
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/495_ECCV_2020_paper.php": {
    "title": "PhraseClick: Toward Achieving Flexible Interactive Segmentation by Phrase and Click",
    "abstract": "Existing interactive object segmentation methods mainly take spatial interactions such as bounding boxes or clicks as input. However, these interactions do not contain information about explicit attributes of the target-of-interest and thus cannot quickly specify what the selected object exactly is, especially when there are diverse scales of candidate objects or the target-of-interest contains multiple objects. Therefore, excessive user interactions are often required to reach desirable results. On the other hand, in existing approaches attribute information of objects is often not well utilized in interactive segmentation. We propose to employ phrase expressions as another interaction input to infer the attributes of target object. In this way, we can 1) leverage spatial clicks to locate the target object and 2) utilize semantic phrases to qualify the attributes of the target object. Specifically, the phrase expressions focus on ``what\"\" the target object is and the spatial clicks are in charge of ``where\"\" the target object is, which together help to accurately segment the target-of-interest with smaller number of interactions. Moreover, the proposed approach is flexible in terms of interaction modes and can efficiently handle complex scenarios by leveraging the strengths of each type of input. Our multi-modal phrase+click approach achieves new state-of-the-art performance on interactive segmentation. To the best of our knowledge, this is the first work to leverage both clicks and phrases for interactive segmentation",
    "volume": "main",
    "checked": true,
    "id": "55f9831547577106695a57ee2d1cc0f0b563a948",
    "citation_count": 18
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/513_ECCV_2020_paper.php": {
    "title": "Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video Parsing",
    "abstract": "In this paper, we introduce a new problem, named audio-visual video parsing, which aims to parse a video into temporal event segments and label them as either audible, visible, or both. Such a problem is essential for a complete understanding of the scene depicted inside a video. To facilitate exploration, we collect a Look, Listen, and Parse (LLP) dataset to investigate audio-visual video parsing in a weakly-supervised manner. This task can be naturally formulated as a Multimodal Multiple Instance Learning (MMIL) problem. Concretely, we propose a novel hybrid attention network to explore unimodal and cross-modal temporal contexts simultaneously. We develop an attentive MMIL pooling method to adaptively explore useful audio and visual content from different temporal extent and modalities. Furthermore, we discover and mitigate modality bias and noisy label issues with an individual-guided learning mechanism and label smoothing technique, respectively. Experimental results show that the challenging audio-visual video parsing can be achieved even with only video-level weak labels. Our proposed framework can effectively leverage unimodal and cross-modal temporal contexts and alleviate modality bias and noisy labels problems",
    "volume": "main",
    "checked": true,
    "id": "3867340091c920dc5f8ba462197fa5bc924a98c4",
    "citation_count": 46
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/526_ECCV_2020_paper.php": {
    "title": "Learning Delicate Local Representations for Multi-Person Pose Estimation",
    "abstract": "In this paper, we propose a novel method called Residual Steps Network (RSN). RSN aggregates features with the same spatial size (Intra-level features) efficiently to obtain delicate local representations, which retain rich low-level spatial information and result in precise keypoint localization. Additionally, we observe the output features contribute differently to final performance. To tackle this problem, we propose an efficient attention mechanism - Pose Refine Machine (PRM) to make a trade-off between local and global representations in output features and further refine the keypoint locations. Our approach won the 1st place of COCO Keypoint Challenge 2019 and achieves state-of-the-art results on both COCO and MPII benchmarks, without using extra training data and pretrained model. Our single model achieves 78.6 on COCO test-dev, 93.0 on MPII test dataset. Ensembled models achieve 79.2 on COCO test-dev, 77.1 on COCO test-challenge dataset. The source code is publicly available for further research at https://github.com/caiyuanhao1998/RSN/",
    "volume": "main",
    "checked": true,
    "id": "3b7eca55735a22147e40bc0852bb329227df9a37",
    "citation_count": 83
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/544_ECCV_2020_paper.php": {
    "title": "Learning to Plan with Uncertain Topological Maps",
    "abstract": "We train an agent to navigate in 3D environments using a hierarchical strategy including a high-level graph based planner and a local policy. Our main contribution is a data driven learning based approach for planning under uncertainty in topological maps, requiring an estimate of shortest paths in valued graphs with a probabilistic structure. Whereas classical symbolic algorithms achieve optimal results on noise-less topologies, or optimal results in a probabilistic sense on graphs with probabilistic structure, we aim to show that machine learning can overcome missing information in the graph by taking into account rich high-dimensional node features, for instance visual information available at each location of the map. Compared to purely learned neural white box algorithms, we structure our neural model with an inductive bias for dynamic programming based shortest path algorithms, and we show that a particular parameterization of our neural model corresponds to the Bellman-Ford algorithm. By performing an empirical analysis of our method in simulated photo-realistic 3D environments, we demonstrate that the inclusion of visual features in the learned neural planner outperforms classical symbolic solutions for graph based planning",
    "volume": "main",
    "checked": true,
    "id": "1b709ce93e636962fcdc3b3df1fad0d0ad682857",
    "citation_count": 20
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/574_ECCV_2020_paper.php": {
    "title": "Neural Design Network: Graphic Layout Generation with Constraints",
    "abstract": "Graphic design is essential for visual communication with layouts being fundamental to composing attractive designs. Layout generation differs from pixel-level image synthesis and is unique in terms of the requirement of mutual relations among the desired components. We propose a method for design layout generation that can satisfy user-specified constraints. The proposed neural design network (NDN) consists of three modules. The first module predicts a graph with complete relations from a graph with user-specified relations. The second module generates a layout from the predicted graph. Finally, the third module fine-tunes the predicted layout. Quantitative and qualitative experiments demonstrate that the generated layouts are visually similar to real design layouts. We also construct real designs based on predicted layouts for a better understanding of the visual quality. Finally, we demonstrate a practical application on layout recommendation",
    "volume": "main",
    "checked": true,
    "id": "41c4e155460485f9ec8a2c9be3dd9803a93f0a0d",
    "citation_count": 46
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/591_ECCV_2020_paper.php": {
    "title": "Learning Open Set Network with Discriminative Reciprocal Points",
    "abstract": "Open set recognition is an emerging research area that aims to simultaneously classify samples from predefined classes and identify the rest as 'unknown'. In this process, one of the key challenges is to reduce the risk of generalizing the inherent characteristics of numerous unknown samples learned from a small amount of known data. In this paper, we propose a new concept, Reciprocal Point, which is the potential representation of the extra-class space corresponding to each known category. The sample can be classified to known or unknown by the otherness with reciprocal points. To tackle the open set problem, we offer a novel open space risk regularization term. Based on the bounded space constructed by reciprocal points, the risk of unknown is reduced through multi-category interaction. The novel learning framework called Reciprocal Point Learning (RPL), which can indirectly introduce the unknown information into the learner with only known classes, so as to learn more compact and discriminative representations. Moreover, we further construct a new large-scale challenging aircraft dataset for open set recognition: Aircraft 300 (Air-300). Extensive experiments on multiple benchmark datasets indicate that our framework is significantly superior to other existing approaches and achieves state-of-the-art performance on standard open set benchmarks",
    "volume": "main",
    "checked": true,
    "id": "b4637bbec842772826f04b8c75e6dab77d2d607d",
    "citation_count": 43
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/597_ECCV_2020_paper.php": {
    "title": "Convolutional Occupancy Networks",
    "abstract": "Recently, implicit neural representations have gained popularity for learning-based 3D reconstruction. While demonstrating promising results, most implicit approaches are limited to comparably simple geometry of single objects and do not scale to more complicated or large-scale scenes. The key limiting factor of implicit methods is their simple fully-connected network architecture which does not allow for integrating local information in the observations or incorporating inductive biases such as translational equivariance. In this paper, we propose Convolutional Occupancy Networks, a more flexible implicit representation for detailed reconstruction of objects and 3D scenes. By combining convolutional encoders with implicit occupancy decoders, our model incorporates inductive biases, enabling structured reasoning in 3D space. We investigate the effectiveness of the proposed representation by reconstructing complex geometry from noisy point clouds and low-resolution voxel representations. We empirically find that our method enables the fine-grained implicit 3D reconstruction of single objects, scales to large indoor scenes, and generalizes well from synthetic to real data",
    "volume": "main",
    "checked": true,
    "id": "5a6732513a1dc0bea059543f208a7556e3e31067",
    "citation_count": 353
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/672_ECCV_2020_paper.php": {
    "title": "Multi-person 3D Pose Estimation in Crowded Scenes Based on Multi-View Geometry",
    "abstract": "Epipolar constraints are at the core of feature matching and depth estimation in current multi-person multi-camera 3D human pose estimation methods. Despite the satisfactory performance of this formulation in sparser crowd scenes, its effectiveness is frequently challenged under denser crowd circumstances mainly due to two sources of ambiguity. The first is the mismatch of human joints resulting from the simple cues provided by the Euclidean distances between joints and epipolar lines. The second is the lack of robustness from the naive formulation of the problem as a least squares minimization. In this paper, we depart from the multi-person 3D pose estimation formulation, and instead reformulate it as crowd pose estimation. Our method consists of two key components: a graph model for fast cross-view matching, and a maximum a posteriori (MAP) estimator for the reconstruction of the 3D human poses. We demonstrate the effectiveness and superiority of our proposed method on four benchmark datasets",
    "volume": "main",
    "checked": true,
    "id": "9fe741bb270d8afb2298bd2ddf44407855848278",
    "citation_count": 19
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/849_ECCV_2020_paper.php": {
    "title": "TIDE: A General Toolbox for Identifying Object Detection Errors",
    "abstract": "We introduce TIDE, a framework and associated toolbox for analyzing the sources of error in object detection and instance segmentation algorithms. Importantly, our framework is applicable across datasets and can be applied directly to output prediction files without required knowledge of the underlying prediction system. Thus, our framework can be used as a drop-in replacement for the standard mAP computation while providing a comprehensive analysis of each model's strengths and weaknesses. We segment errors into six types and, crucially, are the first to introduce a technique for measuring the contribution of each error in a way that isolates its effect on overall performance. We show that such a representation is critical for drawing accurate, comprehensive conclusions through in-depth analysis across 4 datasets and 7 recognition models",
    "volume": "main",
    "checked": true,
    "id": "6ddf634ba4221f45958f1b06f551d5887ac29b18",
    "citation_count": 69
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/893_ECCV_2020_paper.php": {
    "title": "PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding",
    "abstract": "Arguably one of the top success stories of deep learning is transfer learning. The finding that pre-training a network on a rich source set (g, ImageNet) can help boost performance once fine-tuned on a usually much smaller target set, has been instrumental to many applications in language and vision. Yet, very little is known about its usefulness in 3D point cloud understanding. We see this as an opportunity considering the effort required for annotating data in 3D. In this work, we aim at facilitating research on 3D representation learning. Different from previous works, we focus on high-level scene understanding tasks. To this end, we select a suit of diverse datasets and tasks to measure the effect of unsupervised pre-training on a large source set of 3D scenes. Our findings are extremely encouraging: using a unified triplet of architecture, source dataset, and contrastive loss for pre-training, we achieve improvement over recent best results in segmentation and detection across 6 different benchmarks for indoor and outdoor, real and synthetic datasets -- demonstrating that the learned representation can generalize across domains. Furthermore, the improvement was similar to supervised pre-training, suggesting that future efforts should favor scaling data collection over more detailed annotation. We hope these findings will encourage more research on unsupervised pretext task design for 3D deep learning",
    "volume": "main",
    "checked": true,
    "id": "491188ac198663094537cb4c38ac1c8808ef7440",
    "citation_count": 187
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/922_ECCV_2020_paper.php": {
    "title": "DSA: More Efficient Budgeted Pruning via Differentiable Sparsity Allocation",
    "abstract": "Budgeted pruning is the problem of pruning under resource constraints. In budgeted pruning, how to distribute the resources across layers (i.e., sparsity allocation) is the key problem. Traditional methods solve it by discretely searching for the layer-wise pruning ratios, which lacks efficiency. In this paper, we propose Differentiable Sparsity Allocation (DSA), an efficient end-to-end budgeted pruning flow. Utilizing a novel differentiable pruning process, DSA finds the layer-wise pruning ratios with gradient-based optimization. It allocates sparsity in continuous space, which is more efficient than methods based on discrete evaluation and search. Furthermore, DSA could work in a pruning-from-scratch manner, whereas traditional budgeted pruning methods are applied to pre-trained models. Experimental results on CIFAR-10 and ImageNet show that DSA could achieve superior performance than current iterative budgeted pruning methods, and shorten the time cost of the overall pruning process by at least 1.5x in the meantime",
    "volume": "main",
    "checked": true,
    "id": "35bb6f9fce92e07dc3afb1a0b5883f40bafe1bac",
    "citation_count": 41
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/990_ECCV_2020_paper.php": {
    "title": "Circumventing Outliers of AutoAugment with Knowledge Distillation",
    "abstract": "AutoAugment has been a powerful algorithm that improves the accuracy of many vision tasks, yet it is sensitive to the operator space as well as hyper-parameters, and an improper setting may degenerate network optimization. This paper delves deep into the working mechanism, and reveals that AutoAugment may remove part of discriminative information from the training image and so insisting on the ground-truth label is no longer the best option. To relieve the inaccuracy of supervision, we make use of knowledge distillation that refers to the output of a teacher model to guide network training. Experiments are performed in standard image classification benchmarks, and demonstrate the effectiveness of our approach in suppressing noise of data augmentation and stabilizing training. Upon the cooperation of knowledge distillation and AutoAugment, we claim the new state-of-the-art on ImageNet classification with a top-1 accuracy of 85.7%",
    "volume": "main",
    "checked": true,
    "id": "b3e5bac38d098378ba24f826595b8ea877e342f6",
    "citation_count": 33
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/997_ECCV_2020_paper.php": {
    "title": "S2DNet: Learning Image Features for Accurate Sparse-to-Dense Matching",
    "abstract": "Establishing robust and accurate correspondences is a fundamental backbone to many computer vision algorithms. While recent learning-based feature matching methods have shown promising results in providing robust correspondences under challenging conditions, they are often limited in terms of precision. In this paper, we introduce S2DNet, a novel feature matching pipeline, designed and trained to efficiently establish both robust and accurate correspondences. By leveraging a sparse-to-dense matching paradigm, we cast the correspondence learning problem as a supervised classification task to learn to output highly peaked correspondence maps. We show that S2DNet achieves state-of-the-art results on the HPatches benchmark, as well as on several long-term visual localization datasets",
    "volume": "main",
    "checked": true,
    "id": "c5b766e5e40541b2a007201501560f74c5cd33fe",
    "citation_count": 13
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1054_ECCV_2020_paper.php": {
    "title": "RTM3D: Real-time Monocular 3D Detection from Object Keypoints for Autonomous Driving",
    "abstract": "In this work, we propose an efficient and accurate monocular 3D detection framework in single shot. Most successful 3D detectors take the projection constraint from the 3D bounding box to the 2D box as an important component. Four edges of a 2D box provide only four constraints and the performance deteriorates dramatically with the small error of the 2D detector. Different from these approaches, our method predicts the nine perspective keypoints of a 3D bounding box in image space, and then utilize the geometric relationship of 3D and 2D perspectives to recover the dimension, location, and orientation in 3D space. In this method, the properties of the object can be predicted stably even when the estimation of keypoints is very noisy, which enables us to obtain fast detection speed with a small architecture. Training our method only uses the 3D properties of the object without any extra annotations, category-specific 3D shape priors, or depth maps. Our method is the first real-time system (FPS>24) for monocular image 3D detection while achieves state-of-the-art performance on the KITTI benchmark",
    "volume": "main",
    "checked": true,
    "id": "3a71d497ca1b33cfac338dc7a818fe69ad2f528c",
    "citation_count": 135
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1062_ECCV_2020_paper.php": {
    "title": "Video Object Segmentation with Episodic Graph Memory Networks",
    "abstract": "How to make a segmentation model efficiently adapt to a specific video as well as online target appearance variations is a fun- damental issue in the field of video object segmentation. In this work, a graph memory network is developed to address the novel idea of “learning to update the segmentation model”. Specifically, we exploit an episodic memory network, organized as a fully connected graph, to store frames as nodes and capture cross-frame correlations by edges. Further, learnable controllers are embedded to ease memory reading and writing, as well as maintain a fixed memory scale. The structured, external memory design enables our model to comprehensively mine and quickly store new knowl- edge, even with limited visual information, and the differentiable memory controllers slowly learn an abstract method for storing useful represen- tations in the memory and how to later use these representations for prediction, via gradient descent. In addition, the proposed graph mem- ory network yields a neat yet principled framework, which can generalize well to both one-shot and zero-shot video object segmentation tasks. Ex- tensive experiments on four challenging benchmark datasets verify that our graph memory network is able to facilitate the adaptation of the segmentation network for case-by-case video object segmentation",
    "volume": "main",
    "checked": true,
    "id": "2d37411a4cf212fc8f9c2174b6fcfdf420d31ede",
    "citation_count": 117
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1101_ECCV_2020_paper.php": {
    "title": "Rethinking Bottleneck Structure for Efficient Mobile Network Design",
    "abstract": "The inverted residual block is dominating architecture design for mobile networks recently. It changes the classic residual bottleneck by introducing two design rules: learning inverted residuals and using linear bottlenecks. In this paper, we rethink the necessity of such design change and find it may bring risks of information loss and gradient confusion. We thus propose to flip the structure and present a novel bottleneck design, called sandglass block, that performs identity mapping and spatial transformation at higher dimensions and thus alleviates information loss and gradient confusion effectively. Extensively experiments demonstrate that, different from the common belief, such bottleneck structure is indeed more beneficial than the inverted ones for mobile networks. In ImageNet classification, by simply replacing the inverted residual block with sandglass block, without increasing parameters and computations, the classification accuracy can be improved by more than 1.7% over MobileNetV2. On Pascal VOC 2007 test set, we observe that there is also 0.9% mAP improvement in object detection. We further verify the effectiveness of sandglass block by adding it into the search space of neural architecture search method DARTS. With 25% parameter reduction, the classification accuracy is improved by 0.13% over previous DARTS models. Code will be made publicly available",
    "volume": "main",
    "checked": true,
    "id": "b91aa9c3c8d60d761b8375c9903c19113229a660",
    "citation_count": 62
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1104_ECCV_2020_paper.php": {
    "title": "Side-Tuning: A Baseline for Network Adaptation via Additive Side Networks",
    "abstract": "When training a neural network for a desired task, one may prefer to adapt a pre-trained network rather than starting from randomly initialized weights. Adaptation can be useful in cases when training data is scarce, when a single learner needs to perform multiple tasks, or when one wishes to encode priors in the network. The most commonly employed approaches for network adaptation are fine-tuning and using the pre-trained network as a fixed feature extractor, among others.  In this paper, we propose a straightforward alternative: side-tuning. Side-tuning adapts a pre-trained network by training a lightweight \"\"side\"\" network that is fused with the (unchanged) pre-trained network via summation. This simple method works as well as or better than existing solutions and it resolves some of the basic issues with fine-tuning, fixed features, and other common approaches. In particular, side-tuning is less prone to overfitting, is asymptotically consistent, and does not suffer from catastrophic forgetting in incremental learning. We demonstrate the performance of side-tuning under a diverse set of scenarios, including incremental learning (iCIFAR, iTaskonomy), reinforcement learning, imitation learning (visual navigation in Habitat), NLP question-answering (SQuAD v2), and single-task transfer learning (Taskonomy), with consistently promising results",
    "volume": "main",
    "checked": true,
    "id": "292475b9280d21ee5ad1a81ef6dac5244efd364e",
    "citation_count": 41
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1121_ECCV_2020_paper.php": {
    "title": "Towards Part-aware Monocular 3D Human Pose Estimation: An Architecture Search Approach",
    "abstract": "Even though most existing monocular 3D pose estimation approaches achieve very competitive results, they ignore the heterogeneity among human body parts by estimating them with the same network architecture. To accurately estimate 3D poses of different body parts, we attempt to build a part-aware 3D pose estimator by searching a set of network architectures. Consequently, our model automatically learns to select a suitable architecture to estimate each body part. Compared to models built on the commonly used ResNet-50 backbone, it reduces 62\\% parameters and achieves better performance. With roughly the same computational complexity as previous models, our approach achieves state-of-the-art results on both the single-person and multi-person 3D pose estimation benchmarks",
    "volume": "main",
    "checked": true,
    "id": "515c987a3162fadf271559a2f7f5e541a6bcfd00",
    "citation_count": 11
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1207_ECCV_2020_paper.php": {
    "title": "REVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets",
    "abstract": "Machine learning models are known to perpetuate and even amplify the biases present in the data. However, these data biases frequently do not become apparent until after the models are deployed. To tackle this issue and to enable the preemptive analysis of large-scale dataset, we present our tool. REVISE (REvealing VIsual biaSEs) is a tool that assists in the investigation of a visual dataset, surfacing potential biases currently along three dimensions: (1) object-based, (2) gender-based, and (3) geography-based. Object-based biases relate to size, context, or diversity of object representation. Gender-based metrics aim to reveal the stereotypical portrayal of people of different genders. Geography-based analyses consider the representation of different geographic locations. REVISE sheds light on the dataset along these dimensions; the responsibility then lies with the user to consider the cultural and historical context, and to determine which of the revealed biases may be problematic. The tool then further assists the user by suggesting actionable steps that may be taken to mitigate the revealed biases. Overall, the key aim of our work is to tackle the machine learning bias problem early in the pipeline. REVISE is available at https://github.com/princetonvisualai/revise-tool",
    "volume": "main",
    "checked": true,
    "id": "68205d26583b15d5687b41c3318de3b3702e80c6",
    "citation_count": 60
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1327_ECCV_2020_paper.php": {
    "title": "Contrastive Learning for Weakly Supervised Phrase Grounding",
    "abstract": "Phrase grounding, the problem of associating image regions to caption words, is a crucial component of vision-language tasks. We show that phrase grounding can be learned by optimizing word-region attention to maximize a lower bound on mutual information between images and caption words. Given pairs of images and captions, we maximize compatibility of the attention-weighted regions and the words in the corresponding caption, compared to non-corresponding pairs of images and captions. A key idea is to construct effective negative captions for learning through language model guided word substitutions. Training with our negatives yields a $\\sim10\\%$ absolute gain in accuracy over randomly-sampled negatives from the training data. Our weakly supervised phrase grounding model trained on COCO-Captions shows a healthy gain of $5.7\\%$ to achieve $76.7\\%$ accuracy on Flickr30K Entities benchmark. Our code and project material will be available at http://tanmaygupta.info/info-ground",
    "volume": "main",
    "checked": true,
    "id": "015639f092128b23af4f5db5c28d50ac0bcd742f",
    "citation_count": 63
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1362_ECCV_2020_paper.php": {
    "title": "Collaborative Learning of Gesture Recognition and 3D Hand Pose Estimation with Multi-Order Feature Analysis",
    "abstract": "Gesture recognition and 3D hand pose estimation are two highly correlated tasks, yet they are often handled separately. In this paper, we present a novel collaborative learning network for joint gesture recognition and 3D hand pose estimation. The proposed network exploits joint-aware features that are crucial for both tasks, with which gesture recognition and 3D hand pose estimation boost each other to learn highly discriminative features and models. In addition, a novel multi-order feature analysis method is introduced which learns posture and multi-order motion information from the intermediate feature maps of videos effectively and efficiently. Due to the exploitation of joint-aware features in common, the proposed technique is capable of learning gesture recognition and 3D hand pose estimation even when only gesture or pose labels are available, and this enables weakly supervised network learning with much reduced data labeling efforts. Extensive experiments show that our proposed method achieves superior gesture recognition and 3D hand pose estimation performance as compared with the state-of-the-art. Codes and models will be released upon the paper acceptance",
    "volume": "main",
    "checked": true,
    "id": "dcafbfa5030fe96fb7e39764d72742ccb58dc6a2",
    "citation_count": 19
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1425_ECCV_2020_paper.php": {
    "title": "Making an Invisibility Cloak: Real World Adversarial Attacks on Object Detectors",
    "abstract": "We present a systematic study of adversarial attacks on state-of-the-art object detection frameworks. Using standard detection datasets, we train patterns that suppress the objectness scores produced by a range of commonly used detectors, and ensembles of detectors. Through extensive experiments, we benchmark the effectiveness of adversarially trained patches under both white-box and black-box settings, and quantify transferability of attacks between datasets, object classes, and detector models. Finally, we present a detailed study of physical world attacks using printed posters and wearable clothes, and rigorously quantify the performance of such attacks with different metrics",
    "volume": "main",
    "checked": true,
    "id": "06fa914208bff3b5579ced913241868142daaeff",
    "citation_count": 122
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1449_ECCV_2020_paper.php": {
    "title": "TuiGAN: Learning Versatile Image-to-Image Translation with Two Unpaired Images",
    "abstract": "An unsupervised image-to-image translation (UI2I) task deals with learning a mapping between two domains without paired images. While existing UI2I methods usually require numerous unpaired images from different domains for training, there are many scenarios where training data is quite limited. In this paper, we argue that even if each domain contains a single image, UI2I can still be achieved. To this end, we propose TuiGAN, a generative model that is trained on only two unpaired images and amounts to one-shot unsupervised learning. With TuiGAN, an image is translated in a coarse-to-fine manner where the generated image is gradually refined from global structures to local details. We conduct extensive experiments to verify that our versatile method can outperform strong baselines on a wide variety of UI2I tasks. Moreover, TuiGAN is capable of achieving comparable performance with the state-of-the-art UI2I models trained with sufficient data",
    "volume": "main",
    "checked": true,
    "id": "bbdf98b9f16f6285f75154b689b22bbb2483c093",
    "citation_count": 29
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1479_ECCV_2020_paper.php": {
    "title": "Semi-Siamese Training for Shallow Face Learning",
    "abstract": "Most existing public face datasets, such as MS-Celeb-1M and VGGFace2, provide abundant information in both breadth (large number of IDs) and depth (sufficient number of samples) for training. However, in many real-world scenarios of face recognition, the training dataset is limited in depth, $ extit{i.e.}$ only two face images are available for each ID. $ extit{We define this situation as Shallow Face Learning, and find it problematic with existing training methods.}$ Unlike deep face data, the shallow face data lacks intra-class diversity. As such, it can lead to collapse of feature dimension and consequently the learned network can easily suffer from degeneration and over-fitting in the collapsed dimension. In this paper, we aim to address the problem by introducing a novel training method named Semi-Siamese Training (SST). A pair of Semi-Siamese networks constitute the forward propagation structure, and the training loss is computed with an updating gallery queue, conducting effective optimization on shallow training data. Our method is developed without extra-dependency, thus can be flexibly integrated with the existing loss functions and network architectures. Extensive experiments on various benchmarks of face recognition show the proposed method significantly improves the training, not only in shallow face learning, but also for conventional deep face data",
    "volume": "main",
    "checked": true,
    "id": "3e50340855891fcc7284581c0023e108bb9436ae",
    "citation_count": 17
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1488_ECCV_2020_paper.php": {
    "title": "GAN Slimming: All-in-One GAN Compression by A Unified Optimization Framework",
    "abstract": "Generative adversarial networks (GANs) have gained increasing popularity in various computer vision applications, and recently start to be deployed to resource-constrained mobile devices. Similar to other deep models, state-of-the-art GANs also suffer from high parameter complexities. That has recently motivated the exploration of compressing GANs (usually generators). Compared to the vast literature and prevailing success in compressing deep classifiers, the study of GAN compression remains in its infancy, so far leveraging individual compression techniques instead of more sophisticated combinations. We observe that due to the notorious instability of training GANs, heuristically stacking different compression techniques will result in unsatisfactory results. To this end, we propose the first end-to-end optimization framework combining multiple compression means for GAN compression, dubbed GAN Slimming (GS). GS seamlessly integrates three mainstream compression techniques: model distillation, channel pruning and quantization, together with the GAN minimax objective, into one unified optimization form, that can be efficiently optimized from end to end. Without bells and whistles, GS largely outperforms existing options in compressing image-to-image translation GANs. Specifically, we apply GS to compress CartoonGAN, a state-of-the-art style transfer network, by up to $47 imes$ times, with minimal visual quality degradation. Codes and pre-trained models can be found at https://github.com/TAMU-VITA/GAN-Slimming",
    "volume": "main",
    "checked": true,
    "id": "e60ec0a55ebe875a0026784303b525013c896065",
    "citation_count": 41
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1526_ECCV_2020_paper.php": {
    "title": "Human Interaction Learning on 3D Skeleton Point Clouds for Video Violence Recognition",
    "abstract": "This paper introduces a new method for recognizing violent behavior by learning contextual relationships between related people from human skeleton points. Unlike previous work, we first formulate 3D skeleton point clouds from human skeleton sequences extracted from videos and then perform interaction learning on these 3D skeleton point clouds. A novel extbf{S}keleton extbf{P}oints extbf{I}nteraction extbf{L}earning (SPIL) module, is proposed to model the interactions between skeleton points. Specifically, by constructing a specific weight distribution strategy between local regional points, SPIL aims to selectively focus on the most relevant parts of them based on their features and spatial-temporal position information. In order to capture diverse types of relation information, a multi-head mechanism is designed to aggregate different features from independent heads to jointly handle different types of relationships between points. Experimental results show that our model outperforms the existing networks and achieves new state-of-the-art performance on video violence datasets",
    "volume": "main",
    "checked": true,
    "id": "60498bfca85f39068f34d222484dc77b23f62035",
    "citation_count": 15
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1530_ECCV_2020_paper.php": {
    "title": "Binarized Neural Network for Single Image Super Resolution",
    "abstract": "Lighter model and faster inference are the focus of current single image super-resolution (SISR) research. However, existing methods are still hard to be applied in real-world applications due to the requirement of its heavy computation. Model quantization is an effective way to significantly reduce model size and computation time. We propose a simple but effective binary neural networks (BNN) based SISR model with a novel binarization scheme. Specially, we design a bit-accumulation mechanism to approximate the full-precision values, which could realize the approximation to the full precision number by accumulating the multi-layer's one-bit values.The proposed BNN-based SISR method could achieve superior performance with lower computational complexity and less model parameters. Extensive experiments show that the proposed model outperforms the state-of-the-art methods (binarization methods such as BNN, DoReFa-Net and ABC-Net) by large margins on 4 benchmark datasets, specially by average more than 0.8 dB in terms of Peak Signal-to-Noise Ratio (PSNR) on Set5 dataset",
    "volume": "main",
    "checked": true,
    "id": "4178d0f5e316d6d0122519f2b88943c8f4717f1f",
    "citation_count": 37
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1564_ECCV_2020_paper.php": {
    "title": "Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation",
    "abstract": "Convolution exploits locality for efficiency at a cost of missing long range context. Self-attention has been adopted to augment CNNs with non-local interactions. Recent works prove it possible to stack self-attention layers to obtain a fully attentional network by restricting the attention to a local region. In this paper, we attempt to remove this constraint by factorizing 2D self-attention into two 1D self-attentions. This reduces computation complexity and allows performing attention within a larger or even global region. In companion, we also propose a position-sensitive self-attention design. Combining both yields our position-sensitive axial-attention layer, a novel building block that one could stack to form axial-attention models for image classification and dense prediction. We demonstrate the effectiveness of our model on four large-scale datasets. In particular, our model outperforms all existing stand-alone self-attention models on ImageNet. Our Axial-DeepLab improves 2.8% PQ over bottom-up state-of-the-art on COCO test-dev. This previous state-of-the-art is attained by our small variant that is 3.8x parameter-efficient and 27x computation-efficient. Axial-DeepLab also achieves state-of-the-art results on Mapillary Vistas and Cityscapes",
    "volume": "main",
    "checked": true,
    "id": "8eba733040b016e9c7ec5c3dc87cc1b28a5c2000",
    "citation_count": 310
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1605_ECCV_2020_paper.php": {
    "title": "Adaptive Computationally Efficient Network for Monocular 3D Hand Pose Estimation",
    "abstract": "3D hand pose estimation is an important task for a wide range of real-world applications. Existing works in this domain mainly focus on designing advanced algorithms to achieve high pose estimation accuracy. However, besides accuracy, the computation efficiency that affects the computation speed and power consumption is also crucial for real-world applications. In this paper, we investigate the problem of reducing the overall computation cost yet maintaining the high accuracy for 3D hand pose estimation from video sequences. A novel model, called Adaptive Computationally Efficient (ACE) network, is proposed, which takes advantage of a Gaussian kernel based Gate Module to dynamically switch the computation between a light model and a heavy network for feature extraction. Our model employs the light model to compute efficient features for most of the frames and invokes the heavy model only when necessary. Combined with the temporal context, the proposed model accurately estimates the 3D hand pose. We evaluate our model on two publicly available datasets, and achieve state-of-the-art performance at only 22% of the computation cost compared to the ordinary temporal models",
    "volume": "main",
    "checked": true,
    "id": "21c3901de3093533adf6973a329808ca7750b98e",
    "citation_count": 14
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1624_ECCV_2020_paper.php": {
    "title": "Chained-Tracker: Chaining Paired Attentive Regression Results for End-to-End Joint Multiple-Object Detection and Tracking",
    "abstract": "Existing Multiple-Object Tracking (MOT) methods either follow the tracking-by-detection paradigm to conduct object detection, feature extraction and data association separately, or have two of the three subtasks integrated to form a partially end-to-end solution. Going beyond these sub-optimal frameworks, we propose a simple online model named Chained-Tracker (CTracker), which naturally integrates all the three subtasks into an end-to-end solution (the first as far as we know). It chains paired bounding boxes regression results estimated from overlapping nodes, of which each node covers two adjacent frames. The paired regression is made attentive by object-attention (brought by a detection module) and identity-attention (ensured by an ID verification module). The two major novelties: chained structure and paired attentive regression, make CTracker simple, fast and effective, setting new MOTA records on MOT16 and MOT17 challenge datasets (67.6 and 66.6, respectively), without relying on any extra training data. The source code of CTracker can be found at: github.com/pjl1995/CTracker",
    "volume": "main",
    "checked": true,
    "id": "bec1169363ac423033cc05e8058b7a621531b088",
    "citation_count": 135
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1631_ECCV_2020_paper.php": {
    "title": "Distribution-Balanced Loss for Multi-Label Classification in Long-Tailed Datasets",
    "abstract": "We present a new loss function called Distribution-Balanced Loss for the multi-label recognition problems that exhibit long-tailed class distributions. Compared to conventional single-label classification problem, multi-label recognition problems are often more challenging due to two significant issues, namely the co-occurrence of labels and the dominance of negative labels (when treated as multiple binary classification problems). The Distribution-Balanced Loss tackles these issues through two key modifications to the standard binary cross-entropy loss: 1) a new way to re-balance the weights that takes into account the impact caused by label co-occurrence, and 2) a negative tolerant regularization to mitigate the over-suppression of negative labels. Experiments on both Pascal VOC and COCO show that the models trained with this new loss function achieve significant performance gains over existing methods",
    "volume": "main",
    "checked": true,
    "id": "7eda53373b495c9b5d5a93c6371bcc6e3616171d",
    "citation_count": 88
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1676_ECCV_2020_paper.php": {
    "title": "Hamiltonian Dynamics for Real-World Shape Interpolation",
    "abstract": "We revisit the classical problem of 3D shape interpolation and propose a novel, physically plausible approach based on Hamiltonian dynamics. While most prior work focuses on synthetic input shapes, our formulation is designed to be applicable to real-world scans with imperfect input correspondences and various types of noise. To that end, we use recent progress on dynamic thin shell simulation and divergence-free shape deformation and combine them to address the inverse problem of finding a plausible intermediate sequence for two input shapes. In comparison to prior work that mainly focuses on small distortion of consecutive frames, we explicitly model volume preservation and momentum conservation, as well as an anisotropic local distortion model. We argue that, in order to get a robust interpolation for imperfect inputs, we need to model the input noise explicitly which results in an alignment based formulation. Finally, we show a qualitative and quantitative improvement over prior work on a broad range of synthetic and scanned data. Besides being more robust to noisy inputs, our method yields exactly volume preserving intermediate shapes, avoids self-intersections and is scalable to high resolution scans",
    "volume": "main",
    "checked": true,
    "id": "d12a07872e1ad20623d4440ec28da812ace99a79",
    "citation_count": 6
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1694_ECCV_2020_paper.php": {
    "title": "Learning to Scale Multilingual Representations for Vision-Language Tasks",
    "abstract": "Current multilingual vision-language models either require a large number of additional parameters for each supported language, or suffer performance degradation as languages are added. In this paper, we propose a Scalable Multilingual Aligned Language Representation (SMALR) that supports many languages with few model parameters without sacrificing downstream task performance. SMALR learns a fixed size language-agnostic representation for most words in a multilingual vocabulary, keeping language-specific features for just a few. We use a masked cross-language modeling loss to align features with context from other languages. Additionally, we propose a cross-lingual consistency module that ensures predictions made for a query and its machine translation are comparable. The effectiveness of SMALR is demonstrated with ten diverse languages, over twice the number supported in vision-language tasks to date. We evaluate on multilingual image-sentence retrieval and outperform prior work by 3-4% with less than 1/5th the training parameters compared to other word embedding methods",
    "volume": "main",
    "checked": true,
    "id": "a42ebe695d79e0cfbef309c54ec9443b73f38c4f",
    "citation_count": 26
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1710_ECCV_2020_paper.php": {
    "title": "Multi-modal Transformer for Video Retrieval",
    "abstract": "The task of retrieving video content relevant to natural language queries plays a critical role in effectively handling internet-scale datasets. Most of the existing methods for this caption-to-video retrieval problem do not fully exploit cross-modal cues present in video. Furthermore, they aggregate per-frame visual features with limited or no temporal information. In this paper, we present a multi-modal transformer to jointly encode the different modalities in video, which allows each of them to attend to the others. The transformer architecture is also leveraged to encode and model the temporal information. On the natural language side, we investigate the best practices to jointly optimize the language embedding together with the multi-modal transformer. This novel framework allows us to establish state-of-the-art results for video retrieval on three datasets. More details are available at http://thoth.inrialpes.fr/research/MMT",
    "volume": "main",
    "checked": true,
    "id": "6871f6c5437a747fae75a19962f418d234ce2dc1",
    "citation_count": 252
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1761_ECCV_2020_paper.php": {
    "title": "Feature Representation Matters: End-to-End Learning for Reference-based Image Super-resolution",
    "abstract": "In this paper, we are aiming for a general reference-based super-resolution setting: it does not require the low-resolution image and the high-resolution reference image to be well aligned or with a similar texture. Instead, we only intend to transfer the relevant textures from reference images to the output super-resolution image. To this end, we engaged neural texture transfer to swap texture features between the low-resolution image and the high-resolution reference image. We identify the importance of designing a super-resolution task-specific features rather than classification oriented features for neural texture transfer, making the feature extractor more compatible with the image synthesis task. We develop an end-to-end training framework for the reference-based super-resolution task, where the feature encoding network prior to matching and swapping is jointly trained with the image synthesis network. We also discover that learning the high-frequency residual is an effective way for the reference-based super-resolution task. Without bells and whistles, the proposed method E2ENT2 achieved better performance than state-of-the method (i.e., SRNTT with five loss functions) with only two basic loss functions. Extensive experimental results on several datasets demonstrate that the proposed method E2ENT2 can achieve superior performance to existing best models both quantitatively and qualitatively",
    "volume": "main",
    "checked": true,
    "id": "e513dbeba3302f5e86ee38b25adf5666bcf646f2",
    "citation_count": 15
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1802_ECCV_2020_paper.php": {
    "title": "RobustFusion: Human Volumetric Capture with Data-driven Visual Cues using a RGBD Camera",
    "abstract": "High-quality and complete 4D reconstruction of human activities is critical for immersive VR/AR experience, but it suffers from inherent self-scanning constraint and consequent fragile tracking under the monocular setting. In this paper, inspired by the huge potential of learning-based human modeling, we propose RobustFusion, a robust human performance capture system combined with various data-driven visual cues using a single RGBD camera. To break the orchestrated self-scanning constraint, we propose a data-driven model completion scheme to generate a complete and fine-detailed initial model using only the front-view input. To enable robust tracking, we embrace both the initial model and the various visual cues into a novel performance capture scheme with hybrid motion optimization and semantic volumetric fusion, which can successfully capture challenging human motions under the monocular setting without pre-scanned detailed template and owns the reinitialization ability to recover from tracking failures and the disappear-reoccur scenarios. Extensive experiments demonstrate the robustness of our approach to achieve high-quality 4D reconstruction for challenging human motions, liberating the cumbersome self-scanning constraint",
    "volume": "main",
    "checked": true,
    "id": "414de726a3e4f6cb41ab7cdf97284f584022c382",
    "citation_count": 47
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1886_ECCV_2020_paper.php": {
    "title": "Surface Normal Estimation of Tilted Images via Spatial Rectifier",
    "abstract": "In this paper, we present a spatial rectifier to estimate surface normals of tilted images. Tilted images are of particular interest as more visual data are captured by arbitrarily oriented sensors such as body-/robot-mounted cameras. Existing approaches exhibit bounded performance on predicting surface normals because they were trained using gravity-aligned images. Our two main hypotheses are: (1) visual scene layout is indicative of the gravity direction; and (2) not all surfaces are equally represented by a learned estimator due to the structured distribution of the training data, i.e., there exists a transformation for each tilted image that is more responsive to the learned estimator than others. We design a spatial rectifier that is learned to transform the surface normal distribution of a tilted image to the rectified one that matches the gravity-aligned training data distribution. Along with the spatial rectifier, we propose a novel truncated angular loss that offers a stronger gradient at small angular errors and robustness to outliers. The resulting estimator outperforms the state-of-the-art methods including data augmentation baselines not only on ScanNet and NYUv2 but also on a new dataset called Tilt-RGBD that includes considerable roll and pitch camera motion",
    "volume": "main",
    "checked": true,
    "id": "83286e225d68329cfb99f8dbd28f7b7b5dee8b58",
    "citation_count": 17
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1915_ECCV_2020_paper.php": {
    "title": "Multimodal Shape Completion via Conditional Generative Adversarial Networks",
    "abstract": "Several deep learning methods have been proposed for completing partial data from shape acquisition setups, i.e., filling the regions that were missing in the shape. These methods, however, only complete the partial shape with a single output, ignoring the ambiguity when reasoning the missing geometry. Hence, we pose a multi-modal shape completion problem, in which we seek to complete the partial shape with multiple outputs by learning a one-to-many mapping. We develop the first multimodal shape completion method that completes the partial shape via conditional generative modeling, without requiring paired training data. Our approach distills the ambiguity by conditioning the completion on a learned multimodal distribution of possible results. We extensively evaluate the approach on several datasets that contain varying forms of shape incompleteness, and compare among several baseline methods and variants of our methods qualitatively and quantitatively, demonstrating the merit of our method in completing partial shapes with both diversity and quality",
    "volume": "main",
    "checked": true,
    "id": "46e6021529a483750b1b4e3ed82f35e626f60914",
    "citation_count": 37
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1977_ECCV_2020_paper.php": {
    "title": "Generative Sparse Detection Networks for 3D Single-shot Object Detection",
    "abstract": "3D object detection has been widely studied due to its potential applicability to many promising areas such as robotics and augmented reality. Yet, the sparse nature of the 3D data poses unique challenges to this task. Most notably, the observable surface of the 3D point clouds is disjoint from the center of the instance to ground the bounding box prediction on. To this end, we propose Generative Sparse Detection Network (GSDN), a fully-convolutional single-shot sparse detection network that efficiently generates the support for object proposals. The key component of our model is a generative sparse tensor decoder, which uses a series of transposed convolutions and pruning layers to expand the support of sparse tensors while discarding unlikely object centers to maintain minimal runtime and memory footprint. GSDN can process unprecedentedly large-scale inputs with a single fully-convolutional feed-forward pass, thus does not require the heuristic post-processing stage that stitches results from sliding windows as other previous methods have. We validate our approach on three 3D indoor datasets including the large-scale 3D indoor reconstruction dataset where our method outperforms the state-of-the-art methods by a relative improvement of 7.14% while being 3.78 times faster than the best prior work",
    "volume": "main",
    "checked": true,
    "id": "fa2189b9ba9b4679be99cbfa4e1b8bc21d0f5c6c",
    "citation_count": 37
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1987_ECCV_2020_paper.php": {
    "title": "Grounded Situation Recognition",
    "abstract": "We introduce Grounded Situation Recognition (GSR), a task that requires producing structured semantic summaries of images describing: the primary activity, entities engaged in the activity with their roles (e.g. agent, tool), and bounding-box groundings of entities. GSR presents important technical challenges: identifying semantic saliency, categorizing and localizing a large and diverse set of entities, overcoming semantic sparsity, and disambiguating roles. Moreover, unlike in captioning, GSR is straightforward to evaluate. To study this new task we create the Situations With Groundings (SWiG) dataset which adds 278,336 bounding-box groundings to the 11,538 entity classes in the imsitu dataset. We propose a Joint Situation Localizer and find that jointly predicting situations and groundings with end-to-end training handily outperforms independent training with late fusion on the entire grounding metric suite with relative gains between 8% and 32%. Finally, we show initial findings on three exciting future directions enabled by our models: conditional querying, visual chaining, and grounded semantic aware image retrieval. Code and data available at https://prior.allenai.org/projects/gsr",
    "volume": "main",
    "checked": true,
    "id": "fc261c0efb5f9ce82581932d1440630b861fb85f",
    "citation_count": 36
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2019_ECCV_2020_paper.php": {
    "title": "Learning Modality Interaction for Temporal Sentence Localization and Event Captioning in Videos",
    "abstract": "Automatically generating sentences to describe events and temporally localizing sentences in a video are two important tasks that bridge language and videos. Recent techniques leverage the multimodal nature of videos by using off-the-shelf features to represent videos, but interactions between modalities are rarely explored. Inspired by the fact that there exist cross-modal interactions in the human brain, we propose a novel method for learning pairwise modality interactions in order to better exploit complementary information for each pair of modalities in videos and thus improve performances on both tasks. We model modality interaction in both the sequence and channel levels in a pairwise fashion, and the pairwise interaction also provides some explainability for the predictions of target tasks. We demonstrate the effectiveness of our method and validate specific design choices through extensive ablation studies. Our method turns out to achieve state-of-the-art performances on four standard benchmark datasets: MSVD and MSR-VTT (event captioning task), and Charades-STA and ActivityNet Captions (temporal sentence localization task)",
    "volume": "main",
    "checked": true,
    "id": "5fb52197928290d3020b2256ccab22d5bf93c366",
    "citation_count": 46
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2157_ECCV_2020_paper.php": {
    "title": "Unpaired Learning of Deep Image Denoising",
    "abstract": "We investigate the task of learning blind image denoising networks from an unpaired set of clean and noisy images. Such problem setting generally is practical and valuable considering that it is feasible to collect unpaired noisy and clean images in most real-world applications. And we further assume that the noise can be signal dependent but is spatially uncorrelated. In order to facilitate unpaired learning of denoising network, this paper presents a two-stage scheme by incorporating self-supervised learning and knowledge distillation. For self-supervised learning, we suggest a dilated blind-spot network (D-BSN) to learn denoising solely from real noisy images. Due to the spatial independence of noise, we adopt a network by stacking $1 imes1$ convolution layers to estimate the noise level map for each image. Both the D-BSN and image-specific noise model ($ ext{CNN}_{ ext{est}}$) can be jointly trained via maximizing the constrained log-likelihood. Given the output of D-BSN and estimated noise level map, improved denoising performance can be further obtained based on the Bayes' rule. As for knowledge distillation, we first apply the learned noise models to clean images to synthesize a paired set of training images, and use the real noisy images and the corresponding denoising results in the first stage to form another paired set. Then, the ultimate denoising model can be distilled by training an existing denoising network using these two paired sets. Experiments show that our unpaired learning method performs favorably on both synthetic noisy images and real-world noisy photographs in terms of quantitative and qualitative evaluation. Code is available at \\url{https://github.com/XHWXD/DBSN}",
    "volume": "main",
    "checked": true,
    "id": "d30920e7619a5016f9efe8c7d1d720920a0bd058",
    "citation_count": 41
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2191_ECCV_2020_paper.php": {
    "title": "Self-supervising Fine-grained Region Similarities for Large-scale Image Localization",
    "abstract": "The task of large-scale retrieval-based image localization is to estimate the geographical location of a query image by recognizing its nearest reference images from a city-scale dataset. However, the general public benchmarks only provide noisy GPS labels associated with the training images, which act as weak supervisions for learning image-to-image similarities. Such label noise prevents deep neural networks from learning discriminative features for accurate localization. To tackle this challenge, we propose to self-supervise image-to-region similarities in order to fully explore the potential of difficult positive images alongside their sub-regions. The estimated image-to-region similarities can serve as extra training supervision for improving the network in generations, which could in turn gradually refine the fine-grained similarities to achieve optimal performance. Our proposed self-enhanced image-to-region similarity labels effectively deal with the training bottleneck in the state-of-the-art pipelines without any additional parameters or manual annotations in both training and inference. Our method outperforms state-of-the-arts on the standard localization benchmarks by noticeable margins and shows excellent generalization capability on multiple image retrieval datasets",
    "volume": "main",
    "checked": true,
    "id": "512a0387df40fc0146da08cf45f5ca3cb173df11",
    "citation_count": 41
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2215_ECCV_2020_paper.php": {
    "title": "Rotationally-Temporally Consistent Novel View Synthesis of Human Performance Video",
    "abstract": "Novel view video synthesis aims to synthesize novel viewpoints videos given input captures of a human performance taken from multiple reference viewpoints and over consecutive time steps. Despite great advances in model-free novel view synthesis, existing methods present three limitations when applied to complex and time-varying human performance. First, these methods (and related datasets) mainly consider simple and symmetric objects. Second, they do not enforce explicit consistency across generated views. Third, they focus on static and non-moving objects. The fine-grained details of a human subject can therefore suffer from inconsistencies when synthesized across different viewpoints or time steps. To tackle these challenges, we introduce a human-specific framework that employs a learned 3D-aware representation. Specifically, we first introduce a novel siamese network that employs a gating layer for better reconstruction of the latent volumetric representation and, consequently, final visual results. Moreover, features from consecutive time steps are shared inside the network to improve temporal consistency. Second, we introduce a novel loss to explicitly enforce consistency across generated views both in space and in time. Third, we present the Multi-View Human Action (MVHA) dataset, consisting of near 1200 synthetic human performance captured from 54 viewpoints. Experiments on the MVHA, Pose-Varying Human Model and ShapeNet datasets show that our method outperforms the state-of-the-art baselines both in view generation quality and spatio-temporal consistency",
    "volume": "main",
    "checked": true,
    "id": "95be0cac6347b6df9b41c6a03196047c9dc401ce",
    "citation_count": 13
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2272_ECCV_2020_paper.php": {
    "title": "Side-Aware Boundary Localization for More Precise Object Detection",
    "abstract": "Current object detection frameworks mainly rely on bounding box regression to localize objects. Despite the remarkable progress in recent years, the precision of bounding box regression remains unsatisfactory, hence limiting performance in object detection. We observe that precise localization requires careful placement of each side of the bounding box. However, the mainstream approach, which focuses on predicting centers and sizes, is not the most effective way to accomplish this task, especially when there exists displacements with large variance between the anchors and the targets. In this paper, we propose an alternative approach, named as Side-Aware Boundary Localization (SABL), where each side of the bounding box is respectively localized with a dedicated network branch. To tackle the difficulty of precise localization in the presence of displacements with large variance, we further propose a two-step localization scheme, which first predicts a range of movement through bucket prediction and then pinpoints the precise position within the predicted bucket. We test the proposed method on both two-stage and single-stage detection frameworks. Replacing the standard bounding box regression branch with the proposed design leads to significant improvements on Faster R-CNN, RetinaNet, and Cascade R-CNN, by 3.0%, 1.7%, and 0.9%, respectively. Code is available at https://github.com/open-mmlab/mmdetection",
    "volume": "main",
    "checked": true,
    "id": "93b4acba14098a038f418031ea1b5424e7ac8947",
    "citation_count": 45
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2314_ECCV_2020_paper.php": {
    "title": "SF-Net: Single-Frame Supervision for Temporal Action Localization",
    "abstract": "In this paper, we study an intermediate form of supervision, i.e., single-frame supervision, for temporal action localization (TAL). To obtain the single-frame supervision, the annotators are asked to identify only a single frame within the temporal window of an action. This can significantly reduce the labor cost of obtaining full supervision which requires annotating the action boundary. Compared to the weak supervision that only annotates the video-level label, the single-frame supervision introduces extra temporal action signals while maintaining low annotation overhead. To make full use of such single-frame supervision, we propose a unified system called SF-Net. First, we propose to predict an actionness score for each video frame. Along with a typical category score, the actionness score can provide comprehensive information about the occurrence of a potential action and aid the temporal boundary refinement during inference. Second, we mine pseudo action and background frames based on the single-frame annotations. We identify pseudo action frames by adaptively expanding each annotated single frame to its nearby, contextual frames and we mine pseudo background frames from all the unannotated frames across multiple videos. Together with the ground-truth labeled frames, these pseudo-labeled frames are further used for training the classifier",
    "volume": "main",
    "checked": true,
    "id": "84d710727a9a5775ab4691a969f52bc3062325e2",
    "citation_count": 45
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2317_ECCV_2020_paper.php": {
    "title": "Negative Margin Matters: Understanding Margin in Few-shot Classification",
    "abstract": "In this paper, we unconventionally propose to adopt appropriate negative-margin to softmax loss for few-shot classification, which surprisingly works well for the open-set scenarios of few-shot classification. We then provide the intuitive explanation and the theoretical proof to understand why negative margin works well for few-shot classification. This claim is also demonstrated via sufficient experiments. With the negative-margin softmax loss, our approach achieves the state-of-the-art performance on all three standard benchmarks of few-shot classification. In the future, the negative margin may be applied in more general open-set scenarios that do not restrict the number of samples in novel classes",
    "volume": "main",
    "checked": true,
    "id": "2ad5f836e1d9876fa3fc53cb2c0a704b45988f0f",
    "citation_count": 156
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2323_ECCV_2020_paper.php": {
    "title": "Particularity beyond Commonality: Unpaired Identity Transfer with Multiple References",
    "abstract": "Unpaired image-to-image translation aims to translate images from the source class to target one by providing sufficient data for these classes. Current few-shot translation methods use multiple reference images to describe the target domain through extracting common features. In this paper, we focus on a more specific identity transfer problem and advocate that particular property in each individual image can also benefit generation. We accordingly propose a new multi-reference identity transfer framework by simultaneously making use of particularity and commonality of reference. It is achieved via a semantic pyramid alignment module to make proper use of geometric information for individual images, as well as an attention module to aggregate for the final transformation. Extensive experiments demonstrate the effectiveness of our framework given the promising results in a number of identity transfer applications",
    "volume": "main",
    "checked": true,
    "id": "46d1b42bafa6fe2696a329b381be0cafad2284b1",
    "citation_count": 1
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2342_ECCV_2020_paper.php": {
    "title": "Tracking Objects as Points",
    "abstract": "Tracking has traditionally been the art of following interest points through space and time. This changed with the rise of powerful deep networks. Nowadays, tracking is dominated by pipelines that perform object detection followed by temporal association, also known as tracking-by-detection. In this paper, we present a simultaneous detection and tracking algorithm that is simpler, faster, and more accurate than the state of the art. Our tracker, CenterTrack, applies a detection model to a pair of images and detections from the prior frame. Given this minimal input, CenterTrack localizes objects and predicts their associations with the previous frame. That's it. CenterTrack is simple, online (no peeking into the future), and real-time. It achieves $67.3\\%$ MOTA on the MOT17 challenge at 17 FPS and $89.4\\%$ MOTA on the KITTI tracking benchmark at 12 FPS, setting a new state of the art on both datasets. CenterTrack is easily extended to monocular 3D tracking by regressing additional 3D attributes. Using monocular video input, it achieves $28.3\\%$ AMOTA@0.2 on the newly released nuScenes 3D tracking benchmark, substantially outperforming the monocular baseline on this benchmark while running at 22 FPS",
    "volume": "main",
    "checked": true,
    "id": "6daaf6640ecd56779354c3937e5275c3eed0cfef",
    "citation_count": 406
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2390_ECCV_2020_paper.php": {
    "title": "CPGAN: Content-Parsing Generative Adversarial Networks for Text-to-Image Synthesis",
    "abstract": "Typical methods for text-to-image synthesis seek to design effective generative architecture to model the text-to-image mapping directly. It is fairly arduous due to the cross-modality translation. In this paper we circumvent this problem by focusing on parsing the content of both the input text and the synthesized image thoroughly to model the text-to-image consistency in the semantic level. Particularly, we design a memory structure to parse the textual content by exploring semantic correspondence between each word in the vocabulary to its various visual contexts across relevant images during text encoding. Meanwhile, the synthesized image is parsed to learn its semantics in an object-aware manner. Moreover, we customize a conditional discriminator to model the fine-grained correlations between words and image sub-regions to push for the text-image semantic alignment. Extensive experiments on COCO dataset manifest that our model advances the state-of-the-art performance significantly (from 35.69 to 52.73 in Inception Score)",
    "volume": "main",
    "checked": true,
    "id": "5b92d901f7242d40071f060841f3509d04e5b2eb",
    "citation_count": 23
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2402_ECCV_2020_paper.php": {
    "title": "Transporting Labels via Hierarchical Optimal Transport for Semi-Supervised Learning",
    "abstract": "Semi-Supervised Learning (SSL) based on Convolutional Neural Networks (CNNs) have recently been proven as powerful tools for standard tasks such as image classification when there is not a sufficient amount of labeled data available during the training. In this work, we consider the general setting of the SSL problem for image classification, where the labeled and unlabeled data come from the same underlying distribution. We propose a new SSL method that adopts a hierarchical Optimal Transport (OT) technique to find a mapping from empirical unlabeled measures to corresponding labeled measures by leveraging the minimum amount of transportation cost in the label space. Based on this mapping, pseudo-labels for the unlabeled data are inferred, which are then used along with the labeled data for training the CNN. We evaluated and compared our method with state-of-the-art SSL approaches on standard datasets to demonstrate the superiority of our SSL method",
    "volume": "main",
    "checked": true,
    "id": "719d729603d9c6fc8c104b94f09042a0884c0413",
    "citation_count": 7
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2449_ECCV_2020_paper.php": {
    "title": "MTI-Net: Multi-Scale Task Interaction Networks for Multi-Task Learning",
    "abstract": "In this paper, we argue about the importance of considering task interactions at multiple scales when distilling task information in a multi-task learning setup. In contrast to common belief, we show that tasks with high affinity at a certain scale are not guaranteed to retain this behaviour at other scales, and vice versa. We propose a novel architecture, namely MTI-Net, that builds upon this finding in three ways. First, it explicitly models task interactions at every scale via a multi-scale multi-modal distillation unit. Second, it propagates distilled task information from lower to higher scales via a feature propagation module. Third, it aggregates the refined task features from all scales via a feature aggregation unit to produce the final per-task predictions.Extensive experiments on two multi-task dense labeling datasets show that, unlike prior work, our multi-task model delivers on the full potential of multi-task learning, that is, smaller memory footprint, reduced number of calculations, and better performance w.r.t. single-task learning. The code is made publicly available",
    "volume": "main",
    "checked": true,
    "id": "54ccb7fe9a5befc845411f474c344ab6c8e731c2",
    "citation_count": 69
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2473_ECCV_2020_paper.php": {
    "title": "Learning to Factorize and Relight a City",
    "abstract": "We propose a learning-based framework for disentangling outdoor scenes into temporally-varying illumination and permanent scene factors. Inspired by the classic intrinsic image decomposition, our learning signal builds upon two insights: 1) combining the disentangled factors should reconstruct the original image, and 2) the permanent factors should stay constant across multiple temporal samples of the same scene. To facilitate training, we assemble a city-scale dataset of outdoor timelapse imagery from Google Street View, where the same locations are captured repeatedly through time. This data represents an unprecedented scale of spatio-temporal outdoor imagery. We show that our learned disentangled factors can be used to manipulate novel images in realistic ways, such as changing lighting effects and scene geometry. Please visit factorize-a-city.github.io for animated results",
    "volume": "main",
    "checked": true,
    "id": "f119f17e0b49b3e2e184bba5702700c22362d809",
    "citation_count": 24
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2495_ECCV_2020_paper.php": {
    "title": "Region Graph Embedding Network for Zero-Shot Learning",
    "abstract": "Most of the existing Zero-Shot Learning (ZSL) approaches learn direct embeddings from global features or image parts (regions) to the semantic space, which, however, fail to capture the appearance relationships between different local regions within a single image. In this paper, to model the relations among local image regions, we incorporate the region-based relation reasoning into ZSL. Our method, termed as Region Graph Embedding Network (RGEN), is trained end-to-end from raw image data. Specifically, RGEN consists of two branches: the Constrained Part Attention (CPA) branch and the Parts Relation Reasoning (PRR) branch. CPA branch is built upon attention and produces the image regions. To exploit the progressive interactions among these regions, we represent them as a region graph, on which the parts relation reasoning is performed with graph convolutions, thus leading to our PRR branch. To train our model, we introduce both a transfer loss and a balance loss to contrast class similarities and pursue the maximum response consistency among seen and unseen outputs, respectively. Extensive experiments on four datasets well validate the effectiveness of the proposed method under both ZSL and generalized ZSL settings",
    "volume": "main",
    "checked": true,
    "id": "95b5c738c527335234ea0145354028d6deb3a1cd",
    "citation_count": 75
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2534_ECCV_2020_paper.php": {
    "title": "GRAB: A Dataset of Whole-Body Human Grasping of Objects",
    "abstract": "Training computers to understand, model, and synthesize human grasping requires a rich dataset containing complex 3D object shapes, detailed contact information, hand pose and shape, and the 3D body motion over time. While \"\"grasping\"\" is commonly thought of as a single hand stably lifting an object, we capture the motion of the entire body and adopt the generalized notion of \"\"whole-body grasps\"\". Thus, we collect a new dataset, called GRAB (GRasping Actions with Bodies), of whole-body grasps, containing full 3D shape and pose sequences of 10 subjects interacting with 51 everyday objects of varying shape and size. Given MoCap markers, we fit the full 3D body shape and pose, including the articulated face and hands, as well as the 3D object pose. This gives detailed 3D meshes over time, from which we compute contact between the body and object. This is a unique dataset, that goes well beyond existing ones for modeling and understanding how humans grasp and manipulate objects, how their full body is involved, and how interaction varies with the task. We illustrate the practical value of GRAB with an example application; we train GrabNet, a conditional generative network, to predict 3D hand grasps for unseen 3D object shapes. The dataset and code are available for research purposes at https://grab.is.tue.mpg.de",
    "volume": "main",
    "checked": true,
    "id": "0808f356856bf8db3f5f24645dadc8e169fd14d2",
    "citation_count": 97
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2616_ECCV_2020_paper.php": {
    "title": "DEMEA: Deep Mesh Autoencoders for Non-Rigidly Deforming Objects",
    "abstract": "Mesh autoencoders are commonly used for dimensionality reduction, sampling and mesh modeling. We propose a general-purpose DEep MEsh Autoencoder \\hbox{(DEMEA)} which adds a novel embedded deformation layer to a graph-convolutional mesh autoencoder. The embedded deformation layer (EDL) is a differentiable deformable geometric proxy which explicitly models point displacements of non-rigid deformations in a lower dimensional space and serves as a local rigidity regularizer. DEMEA decouples the parameterization of the deformation from the final mesh resolution since the deformation is defined over a lower dimensional embedded deformation graph. We perform a large-scale study on four different datasets of deformable objects. Reasoning about the local rigidity of meshes using EDL allows us to achieve higher-quality results for highly deformable objects, compared to directly regressing vertex positions. We demonstrate multiple applications of DEMEA, including non-rigid 3D reconstruction from depth and shading cues, non-rigid surface tracking, as well as the transfer of deformations over different meshes",
    "volume": "main",
    "checked": true,
    "id": "c1455b004e374993df97661c154b5556891dd558",
    "citation_count": 24
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2623_ECCV_2020_paper.php": {
    "title": "RANSAC-Flow: Generic Two-stage Image Alignment",
    "abstract": "This paper considers the generic problem of dense alignment between two images, whether they be two frames of a video, two widely different views of a scene, two paintings depicting similar content, etc. Whereas each such task is typically addressed with a domain-specific solution, we show that a simple unsupervised approach performs surprisingly well across a range of tasks. Our main insight is that parametric and non-parametric alignment methods have complementary strengths. We propose a two-stage process: first, a feature-based parametric coarse alignment using one or more homographies, followed by non-parametric fine pixel-wise alignment. Coarse alignment is performed using RANSAC on off-the-shelf deep features. Fine alignment is learned in an unsupervised way by a deep network which optimizes a standard structural similarity metric (SSIM) between the two images, plus cycle-consistency. Despite its simplicity, our method shows competitive results on a range of tasks and datasets, including unsupervised optical flow on KITTI, dense correspondences on Hpatches, two-view geometry estimation on YFCC100M, localization on Aachen Day-Night, and, for the first time, fine alignment of artworks on the Brughel dataset. Our code and data are available at http://imagine.enpc.fr/~shenx/RANSAC-Flow",
    "volume": "main",
    "checked": true,
    "id": "3cda9a7fc83cd6ec2e02f23f46bfb6a7fa5f155b",
    "citation_count": 45
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2632_ECCV_2020_paper.php": {
    "title": "Semantic Object Prediction and Spatial Sound Super-Resolution with Binaural Sounds",
    "abstract": "Humans can robustly recognize and localize objects by integrating visual and auditory cues. While machines are able to do the same now with images, less work has been done with sounds. This work develops an approach for dense semantic labelling of sound-making objects, purely based on binaural sounds. We propose a novel sensor setup and record a new audio-visual dataset of street scenes with eight professional binaural microphones and a $360^{",
    "volume": "main",
    "checked": true,
    "id": "4d981cc84ee972104eb6d95a2421a42db8a0cf57",
    "citation_count": 27
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2636_ECCV_2020_paper.php": {
    "title": "Neural Object Learning for 6D Pose Estimation Using a Few Cluttered Images",
    "abstract": "Recent methods for 6D pose estimation of objects assume either textured 3D models or real images that cover the entire range of target poses. However, it is difficult to obtain textured 3D models and annotate the poses of objects in real scenarios. This paper proposes a method, Neural Object Learning (NOL), that creates synthetic images of objects in arbitrary poses by combining only a few observations from cluttered images. A novel refinement step is proposed to align inaccurate poses of objects in source images, which results in better quality images. Evaluations performed on two public datasets show that the rendered images created by NOL lead to state-of-the-art performance in comparison to methods that use 13 times the number of real images. Evaluations on our new dataset show multiple objects can be trained and recognized simultaneously using a sequence of a fixed scene",
    "volume": "main",
    "checked": true,
    "id": "4902b147c0ebefa592ef84f832a5c56d16468f6c",
    "citation_count": 9
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2666_ECCV_2020_paper.php": {
    "title": "Dense Hybrid Recurrent Multi-view Stereo Net with Dynamic Consistency Checking",
    "abstract": "In this paper, we propose an efficient and effective dense hybrid recurrent multi-view stereo net with dynamic consistency checking, namely $D^{2}$HC-RMVSNet, for accurate dense point cloud reconstruction. Our novel hybrid recurrent multi-view stereo net consists of two core modules: 1) a light DRENet (Dense Reception Expanded) module to extract dense feature maps of original size with multi-scale context information, 2) a hybrid HRU-LSTM (Hybrid Recurrent U-LSTM) to regularize 3D matching volume into predicted depth map, which efficiently aggregates different scale information by coupling LSTM and U-Net architecture. To further improve the accuracy and completeness of reconstructed point clouds, we leverage a dynamic consistency checking strategy instead of prefixed parameters and strategies widely adopted in existing methods for dense point cloud reconstruction. In doing so, we dynamically aggregate geometric consistency matching error among all the views. Our method ranks extbf{$1^{st}$} on the complex outdoor extsl{Tanks and Temples} benchmark over all the methods. Extensive experiments on the in-door extsl{DTU} dataset show our method exhibits competitive performance to the state-of-the-art method while dramatically reduces memory consumption, which costs only $19.4\\%$ of R-MVSNet memory consumption. The codebase is available at \\hyperlink{https://github.com/yhw-yhw/D2HC-RMVSNet}{https://github.com/yhw-yhw/D2HC-RMVSNet}",
    "volume": "main",
    "checked": true,
    "id": "2f150fa12c263d4eb325342146deec91515aac20",
    "citation_count": 48
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2707_ECCV_2020_paper.php": {
    "title": "Pixel-Pair Occlusion Relationship Map (P2ORM): Formulation, Inference & Application",
    "abstract": "Inference & Application\",\"We formalize concepts around geometric occlusion in 2D images (i.e., ignoring semantics), and propose a novel unified formulation of both occlusion boundaries and occlusion orientations via a pixel-pair occlusion relation. The former provides a way to generate large-scale accurate occlusion datasets while, based on the latter, we propose a novel method for task-independent pixel-level occlusion relationship estimation from single images. Experiments on a variety of datasets demonstrate that our method outperforms existing ones on this task. To further illustrate the value of our formulation, we also propose a new depth map refinement method that consistently improve the performance of state-of-the-art monocular depth estimation methods",
    "volume": "main",
    "checked": false,
    "id": "01a0467cd9b230ec82c97d7f7299a823bf7c0c58",
    "citation_count": 3
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2710_ECCV_2020_paper.php": {
    "title": "MovieNet: A Holistic Dataset for Movie Understanding",
    "abstract": "Recent years have seen remarkable advances in visual understanding. However, how to understand a story-based long video with artistic styles, e.g. movie, remains challenging. In this paper, we introduce MovieNet -- a holistic dataset for movie understanding. MovieNet contains 1,100 movies with a large amount of multi-modal data, e.g. trailers, photos, plot descriptions, etc. Besides, different aspects of manual annotations are provided in MovieNet, including 1.1M characters with bounding boxes and identities, 42K scene boundaries, 2.5K aligned description sentences, 65K tags of place and action, and 92K tags of cinematic style. To the best of our knowledge, MovieNet is the largest dataset with richest annotations for comprehensive movie understanding. Based on MovieNet, we set up several benchmarks for movie understanding from different angles. Extensive experiments are executed on these benchmarks to show the immeasurable value of MovieNet and the gap of current approaches towards comprehensive movie understanding. We believe that such a holistic dataset would promote the researches on story-based long video understanding and beyond. MovieNet will be published in compliance with regulations at https://movienet.github.io",
    "volume": "main",
    "checked": true,
    "id": "0732df185bdfcb9c908ec30bb441252593f58875",
    "citation_count": 72
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2723_ECCV_2020_paper.php": {
    "title": "Short-Term and Long-Term Context Aggregation Network for Video Inpainting",
    "abstract": "Video inpainting aims to restore missing regions of a video and has many applications such as video editing and object removal. However, existing methods either suffer from inaccurate short-term context aggregation or rarely explore long-term frame information. In this work, we present a novel context aggregation network to effectively exploit both short-term and long-term frame information for video inpainting. In the encoding stage, we propose boundary-aware short-term context aggregation, which aligns and aggregates, from neighbor frames, local regions that are closely related to the boundary context of missing regions into the target frame. Furthermore, we propose dynamic long-term context aggregation to globally refine the feature map generated in the encoding stage using long-term frame features, which are dynamically updated throughout the inpainting process. Experiments show that it outperforms state-of-the-art methods with better inpainting results and fast inpainting speed",
    "volume": "main",
    "checked": true,
    "id": "e198f21b3d052a809d3198ace4de78be32ee843c",
    "citation_count": 16
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2754_ECCV_2020_paper.php": {
    "title": "DH3D: Deep Hierarchical 3D Descriptors for Robust Large-Scale 6DoF Relocalization",
    "abstract": "For relocalization in large-scale point clouds, we propose the first approach that unifies global place recognition and local 6DoF pose refinement. To this end, we design a Siamese network that jointly learns 3D local feature detection and description directly from raw 3D points. It integrates FlexConv and Squeeze-and-Excitation (SE) to assure that the learned local descriptor captures multi-level geometric information and channel-wise relations. For detecting 3D keypoints we predict the discriminativeness of the local descriptors in an unsupervised manner. We generate the global descriptor by directly aggregating the learned local descriptors with an effective attention mechanism. In this way, local and global 3D descriptors are inferred in one single forward pass. Experiments on various benchmarks demonstrate that our method achieves competitive results for both global point cloud retrieval and local point cloud registration in comparison to state-of-the-art approaches. To validate the generalizability and robustness of our 3D keypoints, we demonstrate that our method also performs favorably without fine-tuning on the registration of point clouds that were generated by a visual SLAM system. Code and related materials are available at https://vision.in.tum.de/research/vslam/dh3d",
    "volume": "main",
    "checked": true,
    "id": "f3717cec44b3a4844facdc2525f3469655e22fc9",
    "citation_count": 41
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2755_ECCV_2020_paper.php": {
    "title": "Face Super-Resolution Guided by 3D Facial Priors",
    "abstract": "State-of-the-art face super-resolution methods employ deep convolutional neural networks to learn a mapping between low- and high-resolution facial patterns by exploring local appearance knowledge. However, most of these methods do not well exploit facial structures and identity information, and struggle to deal with facial images that exhibit large pose variations. In this paper, we propose a novel face super-resolution method that explicitly incorporates 3D facial priors which grasp the sharp facial structures. Our work is the first to explore 3D morphable knowledge based on the fusion of parametric descriptions of face attributes (e.g., identity, facial expression, texture, illumination, and face pose). Furthermore, the priors can easily be incorporated into any networks and are extremely efficient in improving the performance and accelerating the convergence speed. Firstly, a 3D face rendering branch is set up to obtain 3D priors of salient facial structures and identity knowledge. Secondly, the Spatial Attention Module is used to better exploit this hierarchical information (i.e., intensity similarity, 3D facial structure, and identity content) for the super-resolution problem. Extensive experiments demonstrate that the proposed 3D priors achieve superior face super-resolution results over the state-of-the-arts",
    "volume": "main",
    "checked": true,
    "id": "2a4ff4dc61deace4b71b19831c53eec46c15ee60",
    "citation_count": 18
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2763_ECCV_2020_paper.php": {
    "title": "Label Propagation with Augmented Anchors: A Simple Semi-Supervised Learning baseline for Unsupervised Domain Adaptation",
    "abstract": "Motivated by the problem relatedness between unsupervised domain adaptation (UDA) and semi-supervised learning (SSL), many state-of-the-art UDA methods adopt SSL principles (e.g., the cluster assumption) as their learning ingredients. However, they tend to overlook the very domain-shift nature of UDA. In this work, we take a step further to study the proper extensions of SSL techniques for UDA. Taking the algorithm of label propagation (LP) as an example, we analyze the challenges of adopting LP to UDA and theoretically analyze the conditions of affinity graph/matrix construction in order to achieve better propagation of true labels to unlabeled instances. Our analysis suggests a new algorithm of Label Propagation with Augmented Anchors (A$^2$LP), which could potentially improve LP via generation of unlabeled virtual instances (i.e., the augmented anchors) with high-confidence label predictions. To make the proposed A$^2$LP useful for UDA, we propose empirical schemes to generate such virtual instances. The proposed schemes also tackle the domain-shift challenge of UDA by alternating between pseudo labeling via A$^2$LP and domain-invariant feature learning. Experiments show that such a simple SSL extension improves over representative UDA methods of domain-invariant feature learning, and could empower two state-of-the-art methods on benchmark UDA datasets. Our results show the value of further investigation on SSL techniques for UDA problems",
    "volume": "main",
    "checked": true,
    "id": "5400bec0616019be65a3224df8c5220d7846d3b6",
    "citation_count": 44
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2767_ECCV_2020_paper.php": {
    "title": "Are Labels Necessary for Neural Architecture Search?",
    "abstract": "Existing neural network architectures in computer vision --- whether designed by humans or by machines --- were typically found using both images and their associated labels. In this paper, we ask the question: can we find high-quality neural architectures using only images, but no human-annotated labels? To answer this question, we first define a new setup called Unsupervised Neural Architecture Search (UnNAS). We then conduct two sets of experiments. In sample-based experiments, we train a large number (500) of diverse architectures with either supervised or unsupervised objectives, and found that the architecture rankings produced with and without labels are highly correlated. In search-based experiments, we run a well-established NAS algorithm (DARTS) using various unsupervised objectives, and report that the architectures searched without labels can be competitive to their counterparts searched with labels. Together, these results reveal the potentially surprising finding that labels are not necessary, and the image statistics alone may be sufficient to identify good neural architectures",
    "volume": "main",
    "checked": true,
    "id": "55edd0f014553e9faa2a9567f1323863f7e08410",
    "citation_count": 50
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2776_ECCV_2020_paper.php": {
    "title": "BLSM: A Bone-Level Skinned Model of the Human Mesh",
    "abstract": "We introduce BLSM, a bone-level skinned model of the human body mesh where bone scales are set prior to template synthesis, rather than the common, inverse practice. BLSM first sets bone lengths and joint angles to specify the skeleton, then specifies identity-specific surface variation, and finally bundles them together through linear blend skinning. We design these steps by constraining the joint angles to respect the kinematic constraints of the human body and by using accurate mesh convolution-based networks to capture identity-specific surface variation. We provide quantitative results on the problem of reconstructing a collection of 3D human scans, and show that we obtain improvements in reconstruction accuracy when comparing to a SMPL-type baseline. Our decoupled bone and shape representation also allows for out-of-box integration with standard graphics packages like Unity, facilitating full-body AR effects and image-driven character animation. Additional results and demos are available from the project webpage: http://arielai.com/blsm",
    "volume": "main",
    "checked": true,
    "id": "0de760ced3d0f95c5d0511b527f262b0afa99418",
    "citation_count": 5
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2826_ECCV_2020_paper.php": {
    "title": "Associative Alignment for Few-shot Image Classification",
    "abstract": "Few-shot image classification aims at training a model from only a few examples for each of the ``novel'' classes. This paper proposes the idea of associative alignment for leveraging part of the base data by aligning the novel training instances to the closely related ones in the base training set. This expands the size of the effective novel training set by adding extra ``related base'' instances to the few novel ones, thereby allowing a constructive fine-tuning. We propose two associative alignment strategies: 1) a metric-learning loss for minimizing the distance between related base samples and the centroid of novel instances in the feature space, and 2) a conditional adversarial alignment loss based on the Wasserstein distance. Experiments on four standard datasets and three backbones demonstrate that combining our centroid-based alignment loss results in absolute accuracy improvements of 4.4%, 1.2%, and 6.2% in 5-shot learning over the state of the art for object recognition, fine-grained classification, and cross-domain adaptation, respectively",
    "volume": "main",
    "checked": true,
    "id": "483b016c731d4917f3e077ffec1079ecd2241081",
    "citation_count": 78
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2873_ECCV_2020_paper.php": {
    "title": "Cyclic Functional Mapping: Self-supervised Correspondence between Non-isometric Deformable Shapes",
    "abstract": "We present the first utterly self-supervised network for dense correspondence mapping between non-isometric shapes. The task of alignment in non-Euclidean domains is one of the most fundamental and crucial problems in computer vision. As 3D scanners can generate highly complex and dense models, the mission of finding dense mappings between those models is vital. The novelty of our solution is based on a cyclic mapping between metric spaces, where the distance between a pair of points should remain invariant after the full cycle. As the same learnable rules that generate the point-wise descriptors apply in both directions, the network learns invariant structures without any labels while coping with non-isometric deformations. We show here state-of-the-art-results by a large margin for a variety of tasks compared to known self-supervised and supervised methods",
    "volume": "main",
    "checked": true,
    "id": "739874789dd8570434dcd21460a10eb925706999",
    "citation_count": 29
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2905_ECCV_2020_paper.php": {
    "title": "View-Invariant Probabilistic Embedding for Human Pose",
    "abstract": "Depictions of similar human body configurations can vary with changing viewpoints. Using only 2D information, we would like to enable vision algorithms to recognize similarity in human body poses across multiple views. This ability is useful for analyzing body movements and human behaviors in images and videos. In this paper, we propose an approach for learning a compact view-invariant embedding space from 2D joint keypoints alone, without explicitly predicting 3D poses. Since 2D poses are projected from 3D space, they have an inherent ambiguity, which is difficult to represent through a deterministic mapping. Hence, we use probabilistic embeddings to model this input uncertainty. Experimental results show that our embedding model achieves higher accuracy when retrieving similar poses across different camera views, in comparison with 2D-to-3D pose lifting models. We also demonstrate the effectiveness of applying our embeddings to view-invariant action recognition and video alignment. Our code will be released for research",
    "volume": "main",
    "checked": true,
    "id": "55cff809c878d91e1c23f33c7e448d11d60bc16a",
    "citation_count": 38
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2918_ECCV_2020_paper.php": {
    "title": "Contact and Human Dynamics from Monocular Video",
    "abstract": "Existing deep models predict 2D and 3D kinematic poses from video that are approximately accurate, but contain visible errors that violate physical constraints, such as feet penetrating the ground and bodies leaning at extreme angles. In this paper, we present a physics-based method for inferring 3D human motion from video sequences that takes initial 2D and 3D pose estimates as input. We first estimate ground contact timings with a novel prediction network which is trained without hand-labeled data. A physics-based trajectory optimization then solves for a physically-plausible motion, based on the inputs. We show this process produces motions that are significantly more realistic than those from purely kinematic methods, substantially improving quantitative measures of both kinematic and dynamic plausibility. We demonstrate our method on character animation and pose estimation tasks on dynamic motions of dancing and sports with complex contact patterns",
    "volume": "main",
    "checked": true,
    "id": "e702a1620a9dc75c9c3d262c07641a0ef1bfbac3",
    "citation_count": 41
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2950_ECCV_2020_paper.php": {
    "title": "PointPWC-Net: Cost Volume on Point Clouds for (Self-)Supervised Scene Flow Estimation",
    "abstract": "We propose a novel end-to-end deep scene flow model, called PointPWC-Net, that directly processes 3D point cloud scenes with large motions in a coarse-to-fine fashion. Flow computed at the coarse level is upsampled and warped to a finer level, enabling the algorithm to accommodate for large motion without a prohibitive search space. We introduce novel cost volume, upsampling, and warping layers to efficiently handle 3D point cloud data. Unlike traditional cost volumes that require exhaustively computing all the cost values on a high-dimensional grid, our point-based formulation discretizes the cost volume onto input 3D points, and a PointConv operation efficiently computes convolutions on the cost volume. Experiment results on FlyingThings3D and KITTI outperform the state-of-the-art by a large margin. We further explore novel self-supervised losses to train our model and achieve comparable results to state-of-the-art trained with supervised loss. Without any fine-tuning, our method also shows great generalization ability on the KITTI Scene Flow 2015 dataset, outperforming all previous methods",
    "volume": "main",
    "checked": true,
    "id": "a8b7201dbbf3515911f6459c0fc09c54f8f57eb6",
    "citation_count": 53
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2965_ECCV_2020_paper.php": {
    "title": "Points2Surf Learning Implicit Surfaces from Point Clouds",
    "abstract": "A key step in any scanning-based asset creation workflow is to convert unordered point clouds to a surface. Classical methods (e.g. Poisson reconstruction) start to degrade in the presence of noisy and partial scans. Hence, deep learning based methods have recently been proposed to produce complete surfaces, even from partial scans. However, such data-driven methods struggle to generalize to new shapes with large geometric and topological variations. We present Points2Surf, a novel patch-based learning framework that produces accurate surfaces directly from raw scans without normals. Learning a prior over a combination of detailed local patches and coarse global information improves generalization performance and reconstruction accuracy. Our extensive comparison on both synthetic and real data demonstrates a clear advantage of our method over state-of-the-art alternatives on previously unseen classes (on average, Points2Surf brings down reconstruction error by 30% over SPR and by 270%+ over deep learning based SotA methods) at the cost of longer computation times and a slight increase in small-scale topological noise in some cases. Our source code, pre-trained model, and dataset are available at: https://github.com/ErlerPhilipp/points2surf",
    "volume": "main",
    "checked": true,
    "id": "3dd1d31b64df6d7915b378fe0f3a7b12e3f4e284",
    "citation_count": 58
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2983_ECCV_2020_paper.php": {
    "title": "Few-Shot Scene-Adaptive Anomaly Detection",
    "abstract": "We address the problem of anomaly detection in videos. The goal is to identify unusual behaviours automatically by learning exclusively from normal videos. Most existing approaches are usually data-hungry and have limited generalization abilities. They usually need to be trained on a large number of videos from a target scene to achieve good results in that scene. In this paper, we propose a novel few-shot scene-adaptive anomaly detection problem to address the limitations of previous approaches. Our goal is to learn to detect anomalies in a previously unseen scene with only a few frames. A reliable solution for this new problem will have huge potential in real-world applications since it is expensive to collect a massive amount of data for each target scene. We propose a meta-learning based approach for solving this new problem; extensive experimental results demonstrate the effectiveness of our proposed method",
    "volume": "main",
    "checked": true,
    "id": "241cca17a6218728064eea0505a7b8e2e5db4c87",
    "citation_count": 40
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2986_ECCV_2020_paper.php": {
    "title": "Personalized Face Modeling for Improved Face Reconstruction and Motion Retargeting",
    "abstract": "Traditional methods for image-based 3D face reconstruction and facial motion retargeting fit a 3D morphable model (3DMM) to the face, which has limited modeling capacity and fail to generalize well to in-the-wild data. Use of deformation transfer or multilinear tensor as a personalized 3DMM for blendshape interpolation does not address the fact that facial expressions result in different local and global skin deformations in different persons. Moreover, existing methods learn a single albedo per user which is not enough to capture the expression-specific skin reflectance variations. We propose an end-to-end framework that jointly learns a personalized face model per user and per-frame facial motion parameters from a large corpus of in-the-wild videos of user expressions. Specifically, we learn user-specific expression blendshapes and dynamic (expression-specific) albedo maps by predicting personalized corrections on top of a 3DMM prior. We introduce novel training constraints to ensure that the corrected blendshapes retain their semantic meanings and the reconstructed geometry is disentangled from the albedo. Experimental results show that our personalization accurately captures fine-grained facial dynamics in a wide range of conditions and efficiently decouples the learned face model from facial motion, resulting in more accurate face reconstruction and facial motion retargeting compared to state-of-the-art methods",
    "volume": "main",
    "checked": true,
    "id": "5c9dbe60474a33884f2f8e481be34e8584f95c3e",
    "citation_count": 26
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2988_ECCV_2020_paper.php": {
    "title": "Entropy Minimisation Framework for Event-based Vision Model Estimation",
    "abstract": "We propose a novel EMin framework for event-based vision model estimation. The framework extends previous event-based motion compensation algorithms to handle models whose outputs have arbitrary dimensions. The main motivation comes from estimating motion from events directly in 3D space (e. g. events augmented with depth), without projecting them onto an image plane. This is achieved by modelling the event alignment according to candidate parameters and minimising the resultant dispersion. We provide a family of suitable entropy loss functions and an efficient approximation whose complexity is only linear with the number of events (e. g. the complexity does not depend on the number of image pixels). The framework is evaluated on several motion estimation problems, including optical flow and rotational motion. As proof of concept, we also test our framework on 6-DOF estimation by performing the optimisation directly in 3D space",
    "volume": "main",
    "checked": true,
    "id": "dc030a31551c52bcd2b9b2ff655d606aa80ace25",
    "citation_count": 9
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2992_ECCV_2020_paper.php": {
    "title": "Reconstructing NBA Players",
    "abstract": "Great progress has been made in 3D body pose and shape estimation from single photos. Yet, state-of-the-art results still suffer from errors due to challenging body poses, modeling clothing, and self occlusions. The domain of basketball games is particularly challenging, due to all of these factors. In this paper, we introduce a new approach for reconstruction of basketball players, that outperforms the state-of-the-art. Key to our approach is new approach for creating poseable, skinned models of NBA players, and a large database of meshes (derived from the NBA2K19 video game), that we are releasing to the research community. Based on these models, we introduce a new method that takes as input a single photo of a clothed player performing any basketball pose and outputs a high resolution mesh and pose of that player. We compare to state of the art methods for shape generation and show significant improvement in the results",
    "volume": "main",
    "checked": true,
    "id": "a627225a84757b3e36275584f445aa0daaaebf93",
    "citation_count": 23
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3087_ECCV_2020_paper.php": {
    "title": "PIoU Loss: Towards Accurate Oriented Object Detection in Complex Environments",
    "abstract": "Object detection using an oriented bounding box (OBB) can better target rotated objects by reducing the overlap with background areas. Existing OBB approaches are mostly built on horizontal bounding box detectors by introducing an additional angle dimension optimized by a distance loss. However, as the distance loss only minimizes the angle error of the OBB and that it loosely correlates to the IoU, it is insensitive to objects with high aspect ratios. Therefore, a novel loss, Pixels-IoU (PIoU) Loss, is formulated to exploit both the angle and IoU for accurate OBB regression. The PIoU loss is derived from IoU metric with a pixel-wise form, which is simple and suitable for both horizontal and oriented bounding box. To demonstrate its effectiveness, we evaluate the PIoU loss on both anchor-based and anchor-free frameworks. The experimental results show that PIoU loss can dramatically improve the performance of OBB detectors, particularly on objects with high aspect ratios and complex backgrounds. Besides, previous evaluation datasets did not include scenarios where the objects have high aspect ratios, hence a new dataset, Retail50K, is introduced to encourage the community to adapt OBB detectors for more complex environments",
    "volume": "main",
    "checked": true,
    "id": "c32adbd5522696f87ce9fff113d7f56a5090a8c4",
    "citation_count": 75
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3089_ECCV_2020_paper.php": {
    "title": "TENet: Triple Excitation Network for Video Salient Object Detection",
    "abstract": "In this paper, we propose a simple yet effective approach, named Triple Excitation Network, to reinforce the training of video salient object detection (VSOD) from three aspects, spatial, temporal, and online excitations. These excitation mechanisms are designed following the spirit of curriculum learning and aim to reduce learning ambiguities at the beginning of training by selectively exciting feature activations using ground truth. Then we gradually reduce the weight of ground truth excitations by a curriculum rate and replace it by a curriculum complementary map for better and faster convergence. In particular, the spatial excitation strengthens feature activations for clear object boundaries, while the temporal excitation imposes motions to emphasize spatio-temporal salient regions. Spatial and temporal excitations can combat the saliency shifting problem and conflict between spatial and temporal features of VSOD. Furthermore, our semi-curriculum learning design enables the first online refinement strategy for VSOD, which allows exciting and boosting saliency responses during testing without re-training. The proposed triple excitations can easily plug in different VSOD methods. Extensive experiments show the effectiveness of all three excitation methods and the proposed method outperforms state-of-the-art image and video salient object detection methods",
    "volume": "main",
    "checked": true,
    "id": "1caaf771cf0b8bd0105b01bf7f2b73a58904b5b0",
    "citation_count": 27
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3099_ECCV_2020_paper.php": {
    "title": "Deep Feedback Inverse Problem Solver",
    "abstract": "We present an efficient, effective, and generic approach towards solving inverse problems. The key idea is to leverage the feedback signal provided by the forward process and learn an iterative update model. Specifically, in each iteration, the neural network takes the feedback as input and outputs an update on current estimation. Our approach does not have any restrictions on the forward process; it does not require any prior knowledge either. Through the feedback information, our model not only can produce accurate estimations that are coherent to the input observation but also is capable of recovering from early incorrect predictions. We verify the performance of our model over a wide range of inverse problems, including 6-DOF pose estimation, illumination estimation, as well as inverse kinematics. Comparing to traditional optimization-based methods, we can achieve comparable or better performance while being two to three orders of magnitude faster. Compared to deep learning-based approaches, our model consistently improves the performance on all metrics",
    "volume": "main",
    "checked": true,
    "id": "49e1c57d7ac3d4e03f5f5c1d85d5b90ea5713e10",
    "citation_count": 7
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3119_ECCV_2020_paper.php": {
    "title": "Learning From Multiple Experts: Self-paced Knowledge Distillation for Long-tailed Classification",
    "abstract": "In real-world scenarios, data tends to exhibit a long-tailed distribution, which increases the difficulty of training deep networks. In this paper, we propose a novel self-paced knowledge distillation framework, termed Learning From Multiple Experts (LFME). Our method is inspired by the observation that networks trained on less imbalanced subsets of the distribution often yield better performances than their jointly-trained counterparts. We refer to these models as `Experts’, and the proposed LFME framework aggregates the knowledge from multiple `Experts' to learn a unified student model. Specifically, the proposed framework involves two levels of adaptive learning schedules: Self-paced Expert Selection and Curriculum Instance Selection, so that the knowledge is adaptively transferred to the `Student'. We conduct extensive experiments and demonstrate that our method is able to achieve superior performances compared to state-of-the-art methods. We also show that our method can be easily plugged into state-of-the-art long-tailed classification algorithms for further improvements",
    "volume": "main",
    "checked": true,
    "id": "029c31eee72ee0eb4d3058d48e276a2710f92325",
    "citation_count": 104
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3120_ECCV_2020_paper.php": {
    "title": "Hallucinating Visual Instances in Total Absentia",
    "abstract": "In this paper, we investigate a new visual restoration task, termed as hallucinating visual instances in total absentia (HVITA). Unlike conventional image inpainting task that works on images with only part of a visual instance missing, HVITA concerns scenarios where an object is completely absent from the scene. This seemingly minor difference in fact makes the HVITA a much challenging task, as the restoration algorithm would have to not only infer the category of the object in total absentia, but also hallucinate an object of which the appearance is consistent with the background. Towards solving HVITA, we propose an end-to-end deep approach that explicitly looks into the global semantics within the image. Specifically, we transform the input image to a semantic graph, wherein each node corresponds to a detected object in the scene. We then adopt a Graph Convolutional Network on top of the scene graph to estimate the category of the missing object in the masked region, and finally introduce a Generative Adversarial Module to carry out the hallucination. Experiments on COCO, Visual Genome and NYU Depth v2 datasets demonstrate that the proposed approach yields truly encouraging and visually plausible results",
    "volume": "main",
    "checked": true,
    "id": "aabb9961734b651ab1f4ed1164cd49caf076c7c8",
    "citation_count": 13
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3125_ECCV_2020_paper.php": {
    "title": "Weakly-supervised 3D Shape Completion in the Wild",
    "abstract": "3D shape completion for real data is important but challenging, since partial point clouds acquired by real-world sensors are usually sparse, noisy and unaligned. Different from previous methods, we address the problem of learning 3D complete shape from unaligned and real-world partial point clouds. To this end, we propose an unsupervised method to estimate both 3D canonical shape and 6-DoF pose for alignment, given multiple partial observations associated with the same instance. The network jointly optimizes canonical shapes and poses with multi-view geometry constraints during training, and can infer the complete shape given a single partial point cloud. Moreover, learned pose estimation can facilitate partial point cloud registration. Experiments on both synthetic and real data show that it is feasible and promising to learn 3D shape completion through large-scale data without shape and pose supervision",
    "volume": "main",
    "checked": true,
    "id": "28a607a204810e52ae77c810de9ab499c7cdeb9f",
    "citation_count": 26
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3335_ECCV_2020_paper.php": {
    "title": "DTVNet: Dynamic Time-lapse Video Generation via Single Still Image",
    "abstract": "This paper presents a novel end-to-end dynamic time-lapse video generation framework, named DTVNet, to generate diversified time-lapse videos from a single landscape image, which are conditioned on normalized motion vectors. The proposed DTVNet consists of two submodules: mph{Optical Flow Encoder} (OFE) and mph{Dynamic Video Generator} (DVG). The OFE maps a sequence of optical flow maps to a mph{normalized motion vector} that encodes the motion information inside the generated video. The DVG contains motion and content streams that learn from the motion vector and the single image respectively, as well as an encoder and a decoder to learn shared content features and construct video frames with corresponding motion respectively. Specifically, the mph{motion stream} introduces multiple mph{adaptive instance normalization} (AdaIN) layers to integrate multi-level motion information that are processed by linear layers. In the testing stage, videos with the same content but various motion information can be generated by different mph{normalized motion vectors} based on only one input image. We further conduct experiments on Sky Time-lapse dataset, and the results demonstrate the superiority of our approach over the state-of-the-art methods for generating high-quality and dynamic videos, as well as the variety for generating videos with various motion information",
    "volume": "main",
    "checked": true,
    "id": "c1847cbb4064ca05593f7b408783e9f183953488",
    "citation_count": 14
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3365_ECCV_2020_paper.php": {
    "title": "CLIFFNet for Monocular Depth Estimation with Hierarchical Embedding Loss",
    "abstract": "This paper proposes a hierarchical loss for monocular depth estimation, which measures the differences between the prediction and ground truth in hierarchical embedding spaces of depth maps. In order to find an appropriate embedding space, we design different architectures for hierarchical embedding generators (HEGs) and explore relevant tasks to train their parameters. Compared to conventional depth losses manually defined on a per-pixel basis, the proposed hierarchical loss can be learned in a data-driven manner. As verified by our experiments, the hierarchical loss even learned without additional labels can capture multi-scale context information, is more robust to local outliers, and thus delivers superior performance. To further improve depth accuracy, a cross level identity feature fusion network (CLIFFNet) is proposed, where low-level features with finer details are refined using more reliable high-level cues. Through end-to-end training, CLIFFNet can learn to select the optimal combinations of low-level and high-level features, leading to more effective cross level feature fusion. When trained using the proposed hierarchical loss, CLIFFNet sets a new state of the art on popular depth estimation benchmarks",
    "volume": "main",
    "checked": true,
    "id": "86960c70141aa65e9b2e7dcdec852a4b6010a531",
    "citation_count": 25
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3385_ECCV_2020_paper.php": {
    "title": "Collaborative Video Object Segmentation by Foreground-Background Integration",
    "abstract": "This paper investigates the principles of embedding learning to tackle the challenging semi-supervised video object segmentation. Different from previous practices that only explore the embedding learning using pixels from foreground object (s), we consider background should be equally treated and thus propose Collaborative video object segmentation by Foreground-Background Integration (CFBI) approach. Our CFBI implicitly imposes the feature embedding from the target foreground object and its corresponding background to be contrastive, promoting the segmentation results accordingly. With the feature embedding from both foreground and background, our CFBI performs the matching process between the reference and the predicted sequence from both pixel and instance levels, making the CFBI be robust to various object scales. We conduct extensive experiments on three popular benchmarks, ie, DAVIS 2016, DAVIS 2017, and YouTube-VOS. Our CFBI achieves the performance (J&F) of 89.4%, 81.9%, and 81.4%, respectively, outperforming all the other state-of-the-art methods. Code: \\url{https://github.com/z-x-yang/CFBI}",
    "volume": "main",
    "checked": true,
    "id": "29207e87e9a88caf8d796fe9e4e2ee54ffaa9f4b",
    "citation_count": 97
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3456_ECCV_2020_paper.php": {
    "title": "Adaptive Margin Diversity Regularizer for handling Data Imbalance in Zero-Shot SBIR",
    "abstract": "Data from new categories are continuously being discovered, which has sparked significant amount of research in developing approaches which generalizes to previously unseen categories, i.e. zero-shot setting. Zero-shot sketch-based image retrieval~(ZS-SBIR) is one such problem in the context of cross-domain retrieval, which has received lot of attention due to its various real-life applications. Since most real-world training data have a fair amount of imbalance; in this work, for the first time in literature, we extensively study the effect of training data imbalance on the generalization to unseen categories, with ZS-SBIR as the application area. We evaluate several state-of-the-art data imbalance mitigating techniques and analyze their results. Furthermore, we propose a novel framework AMDReg (Adaptive Margin Diversity Regularizer), which ensures that the embeddings of the sketch and images in the latent space are not only semantically meaningful, but they also are separated according to their class-representations in the training set. The proposed approach is model-independent, and it can be incorporated seamlessly with several state-of-the-art ZS-SBIR methods to improve their performance under imbalanced condition. Extensive experiments and analysis justifies the effectiveness of the proposed AMDReg for mitigating the effect of data imbalance for generalization to unseen classes in ZS-SBIR",
    "volume": "main",
    "checked": true,
    "id": "dfa94b6ed870ab861c99c63d6612b4da06feb9eb",
    "citation_count": 14
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3477_ECCV_2020_paper.php": {
    "title": "ETH-XGaze: A Large Scale Dataset for Gaze Estimation under Extreme Head Pose and Gaze Variation",
    "abstract": "Gaze estimation is a fundamental task in many applications of computer vision, human computer interaction and robotics. Many state-of-the-art methods are trained and tested on custom datasets, making comparison across methods challenging. Furthermore, existing gaze estimation datasets have limited head pose and gaze variations, and the evaluations are conducted using different protocols and metrics. In this paper, we propose a new gaze estimation dataset called ETH-XGaze, consisting of over one million high-resolution images of varying gaze under extreme head poses. We collect this dataset from 110 participants with a custom hardware setup including 18 digital SLR cameras and adjustable illumination conditions, and a calibrated system to record ground truth gaze targets. We show that our dataset can significantly improve the robustness of gaze estimation methods across different head poses and gaze angles. Additionally, we define a standardized experimental protocol and evaluation metric on ETH-XGaze, to better unify gaze estimation research going forward. The dataset and benchmark website are available at https://ait.ethz.ch/projects/2020/ETH-XGaze",
    "volume": "main",
    "checked": true,
    "id": "a4028f715f8c8a64a07949c48d97babb7e166dcd",
    "citation_count": 80
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3499_ECCV_2020_paper.php": {
    "title": "Calibration-free Structure-from-Motion with Calibrated Radial Trifocal Tensors",
    "abstract": "In this paper we consider the problem of Structure-from-Motion from images with unknown intrinsic calibration. Instead of estimating the internal camera parameters through some self-calibration procedure, we propose to use a subset of the reprojection constraints that is invariant to radial displacement. This allows us to recover metric 3D reconstructions without explicitly estimating the cameras' focal length or radial distortion parameters. The weaker projection model makes initializing the reconstruction especially difficult. To handle this additional challenge we propose two novel minimal solvers for radial trifocal tensor estimation. We evaluate our approach on real images and show that even for extreme optical systems, such as fisheye or catadioptric, we are able to get accurate reconstructions without performing any calibration",
    "volume": "main",
    "checked": true,
    "id": "5648f9819476ccaa83c8bd398add07e7c999eac2",
    "citation_count": 7
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3594_ECCV_2020_paper.php": {
    "title": "Occupancy Anticipation for Efficient Exploration and Navigation",
    "abstract": "State-of-the-art navigation methods leverage a spatial memory to generalize to new environments, but their occupancy maps are limited to capturing the geometric structures directly observed by the agent. We propose occupancy anticipation, where the agent uses its egocentric RGB-D observations to infer the occupancy state beyond the visible regions. In doing so, the agent builds its spatial awareness more rapidly, which facilitates efficient exploration and navigation in 3D environments. By exploiting context in both the egocentric views and top-down maps our model successfully anticipates a broader map of the environment, with performance significantly better than strong baselines. Furthermore, when deploying our model for the sequential decision-making tasks of exploration and navigation, we outperform state-of-the-art methods on the Gibson and Matterport3D datasets. Our approach is the winning entry in the 2020 Habitat PointNav Challenge. Project page: http://vision.cs.utexas.edu/projects/occupancy_anticipation",
    "volume": "main",
    "checked": true,
    "id": "7a9efbe9127eed39284e2d43af405d06c59db92f",
    "citation_count": 71
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3601_ECCV_2020_paper.php": {
    "title": "Unified Image and Video Saliency Modeling",
    "abstract": "Visual saliency modeling for images and videos is treated as two independent tasks in recent computer vision literature. While image saliency modeling is a well-studied problem and progress on benchmarks like SALICON and MIT300 is slowing, video saliency models have shown rapid gains on the recent DHF1K benchmark. Here, we take a step back and ask: Can image and video saliency modeling be approached via a unified model, with mutual benefit? We identify different sources of domain shift between image and video saliency data and between different video saliency datasets as a key challenge for effective joint modelling. To address this we propose four novel domain adaptation techniques - Domain-Adaptive Priors, Domain-Adaptive Fusion, Domain-Adaptive Smoothing and Bypass-RNN - in addition to an improved formulation of learned Gaussian priors. We integrate these techniques into a simple and lightweight encoder-RNN-decoder-style network, UNISAL, and train it jointly with image and video saliency data. We evaluate our method on the video saliency datasets DHF1K, Hollywood-2 and UCF-Sports, and the image saliency datasets SALICON and MIT300. With one set of parameters, UNISAL achieves state-of-the-art performance on all video saliency datasets and is on par with the state-of-the-art for image saliency datasets, despite faster runtime and a 5 to 20-fold smaller model size compared to all competing deep methods. We provide retrospective analyses and ablation studies which confirm the importance of the domain shift modeling. The code is available at https://github.com/rdroste/unisal",
    "volume": "main",
    "checked": true,
    "id": "b01cb5c42ba693c84967bfd2ec94e420586164cc",
    "citation_count": 57
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3604_ECCV_2020_paper.php": {
    "title": "TAO: A Large-Scale Benchmark for Tracking Any Object",
    "abstract": "For many years, multi-object tracking benchmarks have focused on a handful of categories. Motivated primarily by surveillance and self-driving applications, these datasets provide tracks for people, vehicles, and animals, ignoring the vast majority of objects in the world. By contrast, in the related field of object detection, the introduction of large-scale, diverse datasets (e.g., COCO) have fostered significant progress in developing highly robust solutions. To bridge this gap, we introduce a similarly diverse dataset for Tracking Any Object (TAO). It consists of 2,907 high resolution videos, captured in diverse environments, which are half a minute long on average. Importantly, we adopt a bottom-up approach for discovering a large vocabulary of 833 categories, an order of magnitude more than prior tracking benchmarks. To this end, we ask annotators to label objects that move at any point in the video, and give names to them post factum. Our vocabulary is both significantly larger and qualitatively different from existing tracking datasets. To ensure scalability of annotation, we employ a federated approach that focuses manual effort on labeling tracks for those relevant objects in a video (e.g., those that move). We perform an extensive evaluation of state-of-the-art trackers and make a number of important discoveries regarding large-vocabulary tracking in an open-world. In particular, we show that existing single- and multi-object trackers struggle when applied to this scenario in the wild, and that detection-based, multi-object trackers are in fact competitive with user-initialized ones. We hope that our dataset and analysis will boost further progress in the tracking community",
    "volume": "main",
    "checked": true,
    "id": "982cb4421cedce057ae2fc864efac8e43d9c0a5a",
    "citation_count": 65
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3657_ECCV_2020_paper.php": {
    "title": "A Generalization of Otsu's Method and Minimum Error Thresholding",
    "abstract": "We present Generalized Histogram Thresholding (GHT), a simple, fast, and effective technique for histogram-based image thresholding. GHT works by performing approximate maximum a posteriori estimation of a mixture of Gaussians with appropriate priors. We demonstrate that GHT subsumes three classic thresholding techniques as special cases: Otsu's method, Minimum Error Thresholding (MET), and weighted percentile thresholding. GHT thereby enables the continuous interpolation between those three algorithms, which allows thresholding accuracy to be improved significantly. GHT also provides a clarifying interpretation of the common practice of coarsening a histogram's bin width during thresholding. We show that GHT outperforms or matches the performance of all algorithms on a recent challenge for handwritten document image binarization (including deep neural networks trained to produce per-pixel binarizations), and can be implemented in a dozen lines of code or as a trivial modification to Otsu's method or MET",
    "volume": "main",
    "checked": true,
    "id": "33760968ea98d5c078b8dea67e0ed3b08aadeba4",
    "citation_count": 8
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3663_ECCV_2020_paper.php": {
    "title": "A Cordial Sync: Going Beyond Marginal Policies for Multi-Agent Embodied Tasks",
    "abstract": "Autonomous agents must learn to collaborate. It is not scalable to develop a new centralized agent every time a task’s difficulty outpaces a single agent’s abilities. While multi-agent collaboration research has flourished in gridworld-like environments, relatively little work has considered visually rich domains. Addressing this, we introduce the novel task FurnMove in which agents work together to move a piece of furniture through a living room to a goal. Unlike existing tasks, FurnMove requires agents to coordinate at every timestep. We identify two challenges when training agents to complete FurnMove: existing decentralized action sampling procedures do not permit expressive joint action policies and, in tasks requiring close coordination, the number of failed actions dominates successful actions. To confront these challenges we introduce SYNC-policies (synchronize your actions coherently) and CORDIAL (coordination loss). Using SYNC-policies and CORDIAL, our agents achieve a 58% completion rate on FurnMove, an impressive absolute gain of 25 percentage points over competitive decentralized baselines. Our dataset, code, and pretrained models are available at https://unnat.github.io/cordial-sync",
    "volume": "main",
    "checked": true,
    "id": "b143ee344fe3af4169bde8af8b682a2835dae4a4",
    "citation_count": 30
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3665_ECCV_2020_paper.php": {
    "title": "Big Transfer (BiT): General Visual Representation Learning",
    "abstract": "Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes — from 1 example per class to 1 M total examples. BiT achieves 87.5% top-1 accuracy on ILSVRC-2012, 99.4% on CIFAR-10, and 76.3% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8% on ILSVRC-2012 with 10 examples per class, and 97.0% on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance",
    "volume": "main",
    "checked": true,
    "id": "0495d9df8eb84dcdab4e5536179823cd26279949",
    "citation_count": 622
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3684_ECCV_2020_paper.php": {
    "title": "VisualCOMET: Reasoning about the Dynamic Context of a Still Image",
    "abstract": "Even from a single frame of a still image, people can reason about the dynamic story of the image before, after, and beyond the frame. For example, given an image of a man struggling to stay afloat in water, we can reason that the man fell into the water sometime in the past, the intent of that man at the moment is to stay alive, and he will need help in the near future or else he will get washed away. We propose Visual COMET, the novel framework of visual common-sense reasoning tasks to predict events that might have happened before, events that might happen next, and the intents of the people at present. To support research toward visual commonsense reasoning, we introduce the first large-scale repository of Visual Commonsense Graphs that consists of over 1.4 million textual descriptions of visual commonsense inferences carefully annotated over a diverse set of 59,000 images, each paired with short video summaries of before and after. In addition, we provide person-grounding (i.e., co-reference links) between people appearing in the image and people mentioned in the textual commonsense descriptions, allowing for tighter integration between images and text. We establish strong baseline performances on this task and demonstrate that integration between visual and textual commonsense reasoning is the key and wins over non-integrative alternatives",
    "volume": "main",
    "checked": true,
    "id": "4aacd623a46adc6a03c925fe3ac007c271c9c6ab",
    "citation_count": 41
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3831_ECCV_2020_paper.php": {
    "title": "Few-shot Action Recognition with Permutation-invariant Attention",
    "abstract": "Many few-shot learning models focus on recognising images. In contrast, we tackle a challenging task of few-shot action recognition from videos. We build on a C3D encoder for spatio-temporal video blocks to capture short-range action patterns. Such encoded blocks are aggregated by permutation-invariant pooling to make our approach robust to varying action lengths and long-range temporal dependencies whose patterns are unlikely to repeat even in clips of the same class. Subsequently, the pooled representations are combined into simple relation descriptors which encode so-called query and support clips. Finally, relation descriptors are fed to the comparator with the goal of similarity learning between query and support clips. Importantly, to re-weight block contributions during pooling, we exploit spatial and temporal attention modules and self-supervision. In naturalistic clips (of the same class) there exists a temporal distribution shift--the locations of discriminative temporal action hotspots vary. Thus, we permute blocks of a clip and align the resulting attention regions with similarly permuted attention regions of non-permuted clip to train the attention mechanism invariant to block (and thus long-term hotspot) permutations. Our method outperforms the state of the art on the HMDB51, UCF101, miniMIT datasets",
    "volume": "main",
    "checked": true,
    "id": "161ecde59203eaaa5347cdead5a1090f2a1669a2",
    "citation_count": 66
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3913_ECCV_2020_paper.php": {
    "title": "Character Grounding and Re-Identification in Story of Videos and Text Descriptions",
    "abstract": "We address character grounding and re-identification in multiple story-based videos like movies and associated text descriptions. In order to solve these related tasks in a mutually rewarding way, we propose a model named Character in Story Identification Network (CiSIN). Our method builds two semantically informative representations via joint training of multiple objectives for character grounding, video/text re-identification and gender prediction: Visual Track Embedding from videos and Textual Character Embedding from text context. These two representations are learned to retain rich semantic multimodal information that enables even simple MLPs to achieve the state-of-the-art performance on the target tasks. More specifically, our CiSIN model achieves the best performance in the Fill-in the Characters task of LSMDC 2019 challenges. Moreover, it outperforms previous state-of-the-art models in M-VAD Names dataset as a benchmark of multimodal character grounding and re-identification",
    "volume": "main",
    "checked": true,
    "id": "3afcf32596398c04bd734ee6469506522e224e52",
    "citation_count": 2
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3977_ECCV_2020_paper.php": {
    "title": "AABO: Adaptive Anchor Box Optimization for Object Detection via Bayesian Sub-sampling",
    "abstract": "Most state-of-the-art object detection systems follow an anchor-based diagram. Anchor boxes are densely proposed over the images and the network is trained to predict the boxes position offset as well as the classification confidence. Existing systems pre-define anchor box shapes and sizes and ad-hoc heuristic adjustments are used to define the anchor configurations. However, this might be sub-optimal or even wrong when a new dataset or a new model is adopted. In this paper, we study the problem of automatically optimizing anchor boxes for object detection. We first demonstrate that the number of anchors, anchor scales and ratios are crucial factors for a reliable object detection system. By carefully analyzing the existing bounding box patterns on the feature hierarchy, we design a flexible and tight hyper-parameter space for anchor configurations. Then we propose a novel hyper-parameter optimization method named AABO to determine more appropriate anchor boxes for a certain dataset, in which Bayesian Optimization and subsampling method are combined to achieve precise and efficient anchor configuration optimization. Experiments demonstrate the effectiveness of our proposed method on different detectors and datasets, e.g. achieving around 2.4% mAP improvement on COCO, 1.6% on ADE and 1.5% on VG, and the optimal anchors can bring 1.4% ∼ 2.4% mAP improvement on SOTA detectors by only optimizing anchor configurations, e.g. boosting Mask RCNN from 40.3% to 42.3%, and HTC detector from 46.8% to 48.2%",
    "volume": "main",
    "checked": true,
    "id": "56876a8737feea7bfe31e5aef5f4006dc5fd2f8f",
    "citation_count": 8
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3984_ECCV_2020_paper.php": {
    "title": "Learning Visual Context by Comparison",
    "abstract": "Finding diseases from an X-ray image is an important yet highly challenging task. Current methods for solving this task exploit various characteristics of the chest X-ray image, but one of the most important characteristics is still missing: the necessity of comparison between related regions in an image. In this paper, we present Attend-and-Compare Module (ACM) for capturing the difference between an object of interest and its corresponding context. We show that explicit difference modeling can be very helpful in tasks that require direct comparison between locations from afar. This module can be plugged into existing deep learning models. For evaluation, we apply our module to three chest X-ray recognition tasks and COCO object detection & segmentation tasks and observe consistent improvements across tasks. The code is available at https://github.com/mk-minchul/attend-and-compare",
    "volume": "main",
    "checked": true,
    "id": "04e234c7e4f300b4e8a16370728d875f9f484b39",
    "citation_count": 7
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3994_ECCV_2020_paper.php": {
    "title": "Large Scale Holistic Video Understanding",
    "abstract": "Video recognition has been advanced in recent years by benchmarks with rich annotations. However, research is still mainly limited to human action or sports recognition - focusing on a highly specific video understanding task and thus leaving a significant gap towards describing the overall content of a video. We fill this gap by presenting a large-scale ``Holistic Video Understanding Dataset\"\"~(HVU). HVU is organized hierarchically in a semantic taxonomy that focuses on multi-label and multi-task video understanding as a comprehensive problem that encompasses the recognition of multiple semantic aspects in the dynamic scene. HVU contains approx.~572k videos in total with 9 million annotations for training, validation, and test set spanning over 3142 labels. HVU encompasses semantic aspects defined on categories of scenes, objects, actions, events, attributes, and concepts which naturally capture the real-world scenarios.We demonstrate the generalisation capability of HVU on three challenging tasks: 1.) Video classification, 2.) Video captioning and 3.) Video clustering tasks. In particular for video classification, we introduce a new spatio-temporal deep neural network architecture called ``Holistic Appearance and Temporal Network\"\"~(HATNet) that builds on fusing 2D and 3D architectures into one by combining intermediate representations of appearance and temporal cues. HATNet focuses on the multi-label and multi-task learning problem and is trained in an end-to-end manner. %The experiments show that HATNet trained on HVU outperforms current state-of-the-art methods on challenging human action datasets: HMDB51, UCF101, and Kinetics. The dataset and codes will be made publicly available. Via our experiments, we validate the idea that holistic representation learning is complementary, and can play a key role in enabling many real-world applications",
    "volume": "main",
    "checked": true,
    "id": "1c2fe9e87e31e6fdb2b7bae5f9cc3c2d2612d13a",
    "citation_count": 50
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/3995_ECCV_2020_paper.php": {
    "title": "Indirect Local Attacks for Context-aware Semantic Segmentation Networks",
    "abstract": "Recently, deep networks have achieved impressive semantic segmentation performance, in particular thanks to their use of larger contextual information. In this paper, we show that the resulting networks are sensitive not only to global adversarial attacks, where perturbations affect the entire input image, but also to indirect local attacks, where the perturbations are confined to a small image region that does not overlap with the area that the attacker aims to fool. To this end, we introduce an indirect attack strategy, namely adaptive local attacks, aiming to find the best image location to perturb, while preserving the labels at this location and producing a realistic-looking segmentation map. Furthermore, we propose attack detection techniques both at the global image level and to obtain a pixel-wise localization of the fooled regions. Our results are unsettling: Because they exploit a larger context, more accurate semantic segmentation networks are more sensitive to indirect local attacks. We believe that our comprehensive analysis will motivate the community to design architectures with contextual dependencies that do not trade off robustness for accuracy",
    "volume": "main",
    "checked": true,
    "id": "6b994a8a325a9b025784cdece31d31f7b801268c",
    "citation_count": 15
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4294_ECCV_2020_paper.php": {
    "title": "Predicting Visual Overlap of Images Through Interpretable Non-Metric Box Embeddings",
    "abstract": "To what extent are two images picturing the same 3D surfaces? Even when this is a known scene, the answer typically requires an expensive search across scale space, with matching and geometric verification of large sets of local features. This expense is further multiplied when a query image is evaluated against a gallery, e.g. in visual relocalization. While we don't obviate the need for geometric verification, we propose an interpretable image-embedding that cuts the search in scale space to essentially a lookup.Our approach measures the asymmetric distance between two images. The model then learns a scene-specific measure of similarity, from training examples with known 3D visible-surface overlaps. The result is that we can quickly identify, for example, which test image is a close-up version of another, and by what scale factor. Subsequently, local features need only be detected at that scale. We validate our scene-specific model by showing how this embedding yields competitive image-matching results, while being simpler, faster, and also interpretable by humans",
    "volume": "main",
    "checked": true,
    "id": "b06099f6d099cee532aa18700ba35bc78b2b61f0",
    "citation_count": 10
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4296_ECCV_2020_paper.php": {
    "title": "Connecting Vision and Language with Localized Narratives",
    "abstract": "We propose Localized Narratives, a new form of multimodal image annotations connecting vision and language. We ask annotators to describe an image with their voice while simultaneously hovering their mouse over the region they are describing. Since the voice and the mouse pointer are synchronized, we can localize every single word in the description. This dense visual grounding takes the form of a mouse trace segment per word and is unique to our data. We annotated 849k images with Localized Narratives: the whole COCO, Flickr30k, and ADE20K datasets, and 671k images of Open Images, all of which we make publicly available. We provide an extensive analysis of these annotations showing they are diverse, accurate, and efficient to produce. We also demonstrate their utility on the application of controlled image captioning",
    "volume": "main",
    "checked": true,
    "id": "439369de9514e41e0f03fed552d8f6e5aebf51b2",
    "citation_count": 79
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4383_ECCV_2020_paper.php": {
    "title": "Adversarial T-shirt! Evading Person Detectors in A Physical World",
    "abstract": "It is known that deep neural networks (DNNs) are vulnerable to adversarial attacks. The so-called physical adversarial examples deceive DNN-based decision makers by attaching adversarial patches to real objects. However, most of the existing works on physical adversarial attacks focus on static objects such as glass frames, stop signs and images attached to cardboard. In this work, we proposed adversarial T-shirts, a robust physical adversarial example for evading person detectors even if it could undergo non-rigid deformation due to a moving person’s pose changes. To the best of our knowledge, this is the first work that models the effect of deformation for designing physical adversarial examples with respect to non-rigid objects such as T-shirts. We show that the proposed method achieves 74% and 57% attack success rates in the digital and physical worlds respectively against YOLOv2. In contrast, the state-of-the-art physical attack method to fool a person detector only achieves 18% attack success rate. Furthermore, by leveraging min-max optimization, we extend our method to the ensemble attack setting against two object detectors YOLO-v2 and Faster R-CNN simultaneously",
    "volume": "main",
    "checked": true,
    "id": "3d2cfca77ebe773532f9f178b726c20fcf6d4ee4",
    "citation_count": 143
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4404_ECCV_2020_paper.php": {
    "title": "Bounding-box Channels for Visual Relationship Detection",
    "abstract": "Recognizing the relationship between multiple objects in an image is essential for a deeper understanding of the meaning of the image. However, current visual recognition methods are still far from reaching human-level accuracy. Recent approaches have tackled this task by combining image features with semantic and spatial features, but the way they relate them to each other is weak, mostly because the spatial context in the image feature is lost. In this paper, we propose the bounding-box channels, a novel architecture capable of relating the semantic, spatial, and image features strongly. Our network learns bounding-box channels, which are initialized according to the position of the objects and the label of objects, and concatenated to the image features extracted from the objects. Then, they are input together to the relationship estimator. Our method can retain the spatial information in the image features, and strongly associate them with the semantic and spatial features. This way, our method is capable of effectively emphasizing the features in the object area for a better modeling of the relationships within objects. In addition, we experimentally show that our bounding-box channels have a high generalization ability. Our evaluation results show the efficacy of our architecture outperforming previous works in visual relationship detection",
    "volume": "main",
    "checked": true,
    "id": "ac840657d278d5b23e5ae1a4bd0d902d1157d247",
    "citation_count": 7
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4407_ECCV_2020_paper.php": {
    "title": "Minimal Rolling Shutter Absolute Pose with Unknown Focal Length and Radial Distortion",
    "abstract": "The internal geometry of most modern consumer cameras is not adequately described by the perspective projection. Almost all cameras exhibit some radial lens distortion and are equipped with electronic rolling shutter that induces distortions when the camera moves during the image capture. When focal length has not been calibrated offline, the parameters that describe the radial and rolling shutter distortions are usually unknown. While for global shutter cameras, minimal solvers for the absolute camera pose and unknown focal length and radial distortion are available, solvers for the rolling shutter were missing. We present the first minimal solutions for the absolute pose of a rolling shutter camera with unknown rolling shutter parameters, focal length, and radial distortion. Our new minimal solvers combine iterative schemes designed for calibrated rolling shutter cameras with fast generalized eigenvalue and Groebner basis solvers. In a series of experiments, with both synthetic and real data, we show that our new solvers provide accurate estimates of the camera pose, rolling shutter parameters, focal length, and radial distortion parameters",
    "volume": "main",
    "checked": true,
    "id": "7581d45f68df6ef0f238b867df2c2d7681aeeef8",
    "citation_count": 6
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4442_ECCV_2020_paper.php": {
    "title": "SRFlow: Learning the Super-Resolution Space with Normalizing Flow",
    "abstract": "Super-resolution is an ill-posed problem, since it allows for multiple predictions for a given low-resolution image. This fundamental fact is largely ignored by state-of-the-art deep learning based approaches. These methods instead train a deterministic mapping using combinations of reconstruction and adversarial losses. In this work, we therefore propose SRFlow: a normalizing flow based super-resolution method capable of learning the conditional distribution of the output given the low-resolution input. Our model is trained in a principled manner using a single loss, namely the negative log-likelihood. SRFlow therefore directly accounts for the ill-posed nature of the problem, and learns to predict diverse photo-realistic high-resolution images. Moreover, we utilize the strong image posterior learned by SRFlow to design flexible image manipulation techniques, capable of enhancing super-resolved images by, e.g., transferring content from other images. We perform extensive experiments on faces, as well as on super-resolution in general. SRFlow outperforms state-of-the-art GAN-based approaches in terms of both PSNR and perceptual quality metrics, while allowing for diversity through the exploration of the space of super-resolved solutions",
    "volume": "main",
    "checked": true,
    "id": "44af67de6810e871d1570daf610e7e1f6c8c2008",
    "citation_count": 140
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4452_ECCV_2020_paper.php": {
    "title": "DeepGMR: Learning Latent Gaussian Mixture Models for Registration",
    "abstract": "Point cloud registration is a fundamental problem in 3D computer vision, graphics and robotics. For the last few decades, existing registration algorithms have struggled in situations with large transformations, noise, and time constraints. In this paper, we introduce Deep Gaussian Mixture Registration (DeepGMR), the first learning-based registration method that explicitly leverages a probabilistic registration paradigm by formulating registration as the minimization of KL-divergence between two probability distributions modeled as mixtures of Gaussians. We design a neural network that extracts pose-invariant correspondences between raw point clouds and Gaussian Mixture Model (GMM) parameters and two differentiable compute blocks that recover the optimal transformation from matched GMM parameters. This construction allows the network learn an SE(3)-invariant feature space, producing a global registration method that is real-time, generalizable, and robust to noise. Across synthetic and real-world data, our proposed method shows favorable performance when compared with state-of-the-art geometry-based and learning-based registration methods",
    "volume": "main",
    "checked": true,
    "id": "4a05b2b8de6ebf305457b3bfc1ae8d13b4db37a6",
    "citation_count": 77
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4458_ECCV_2020_paper.php": {
    "title": "Active Perception using Light Curtains for Autonomous Driving",
    "abstract": "Most real-world 3D sensors such as LiDARs are passive, meaning that they sense the entire environment, while being decoupled from the recognition system that processes the sensor data. In this work, we propose a method for 3D object recognition using light curtains, a resource-efficient active sensor that measures depth at selected locations in the environment in a controllable manner. Crucially, we propose using prediction uncertainty of a deep learning based 3D point cloud detector to guide active sensing. Given a neural network's uncertainty, we develop a novel optimization algorithm to optimally place light curtains to maximize coverage of uncertain regions. Efficient optimization is achieved by encoding the physical constraints of the device into a constraint graph, which is optimized with dynamic programming. We show how a 3D detector can be trained to detect objects in a scene by sequentially placing uncertainty-guided light curtains to successively improve detection accuracy",
    "volume": "main",
    "checked": true,
    "id": "0dcba7532fef02948ea4f01446dcdd07f047f6c8",
    "citation_count": 7
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4521_ECCV_2020_paper.php": {
    "title": "Invertible Neural BRDF for Object Inverse Rendering",
    "abstract": "We introduce a novel neural network-based BRDF model and a Bayesian framework for object inverse rendering, i.e., joint estimation of reflectance and natural illumination from a single image of an object of known geometry. The BRDF is expressed with an invertible neural network, namely, normalizing flow, which provides the expressive power of a high-dimensional representation, computational simplicity of a compact analytical model, and physical plausibility of a real-world BRDF. We extract the latent space of real-world reflectance by conditioning this model, which directly results in a strong reflectance prior. We refer to this model as the invertible neural BRDF model (iBRDF). We also devise a deep illumination prior by leveraging the structural bias of deep neural networks. By integrating this novel BRDF model and reflectance and illumination priors in a MAP estimation formulation, we show that this joint estimation can be computed efficiently with stochastic gradient descent. We experimentally validate the accuracy of the invertible neural BRDF model on a large number of measured data and demonstrate its use in object inverse rendering on a number of synthetic and real images. The results show new ways in which deep neural networks can help solve challenging radiometric inverse problems",
    "volume": "main",
    "checked": true,
    "id": "247c59dfd5d80ff0db79f951355e3427cd899b71",
    "citation_count": 13
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4545_ECCV_2020_paper.php": {
    "title": "Semi-supervised Semantic Segmentation via Strong-weak Dual-branch Network",
    "abstract": "While existing works have explored a variety of techniques to push the envelop of weakly-supervised semantic segmentation, there is still a significant gap compared to the supervised methods. In real-world application, besides massive amount of weakly-supervised data there are usually a few available pixel-level annotations, based on which semi-supervised track becomes a promising way for semantic segmentation. Current methods simply bundle these two different sets of annotations together to train a segmentation network. However, we discover that such treatment is problematic and achieves even worse results than just using strong labels, which indicates the misuse of the weak ones. To fully explore the potential of the weak labels, we propose to impose separate treatments of strong and weak annotations via a strong-weak dual-branch network, which discriminates the massive inaccurate weak supervisions from those strong ones. We design a shared network component to exploit the joint discrimination of strong and weak annotations; meanwhile, the proposed dual branches separately handle full and weak supervised learning and effectively eliminate their mutual interference. This simple architecture requires only slight additional computational costs during training yet brings significant improvements over the previous methods. Experiments on two standard benchmark datasets show the effectiveness of the proposed method",
    "volume": "main",
    "checked": true,
    "id": "97ab6a2c2e5d414e6e21ea211531dcf51263e4cb",
    "citation_count": 31
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4571_ECCV_2020_paper.php": {
    "title": "Practical Deep Raw Image Denoising on Mobile Devices",
    "abstract": "Deep learning-based image denoising approaches have been extensively studied in recent years, prevailing in many public benchmark datasets. However, the stat-of-the-art networks are computationally too expensive to be directly applied on mobile devices. In this work, we propose a light-weight, efficient neural network-based raw image denoiser that runs smoothly on mainstream mobile devices, and produces high quality denoising results. Our key insights are twofold:(1) by measuring and estimating sensor noise level, a smaller network trained on synthetic sensor-specific data can out-perform larger ones trained on general data;(2) the large noise level variation under different ISO settings can be removed by a novel k-Sigma Transform, allowing a small network to efficiently handle a wide range of noise levels. We conduct extensive experiments to demonstrate the efficiency and accuracy of our approach. Our proposed mobile-friendly denoising model runs at ~70 milliseconds per megapixel on Qualcomm Snapdragon 855 chipset, and it is the basis of the night shot feature of several flagship smartphones released in 2019",
    "volume": "main",
    "checked": true,
    "id": "ec566c4fdae6720f7a975bda7ec0ab8e346084f3",
    "citation_count": 35
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4577_ECCV_2020_paper.php": {
    "title": "SoundSpaces: Audio-Visual Navigation in 3D Environments",
    "abstract": "Moving around in the world is naturally a multi-sensory experience, but today's embodied agents are deaf - restricted to solely their visual perception of the environment. We introduce audio-visual navigation for complex, acoustically and visually realistic 3D environments. By both seeing and hearing, the agent must learn to navigate to an audio-based target. We develop a multi-modal deep reinforcement learning pipeline to train navigation policies end-to-end from a stream of egocentric audio-visual observations, allowing the agent to (1) discover elements of the geometry of the physical space indicated by the reverberating audio and (2) detect and follow sound-emitting targets. We further introduce audio renderings based on geometrical acoustic simulations for a set of publicly available 3D assets and instrument AI-Habitat to support the new sensor, making it possible to insert arbitrary sound sources in an array of apartment, office, and hotel environments. Our results show that audio greatly benefits embodied visual navigation in 3D spaces",
    "volume": "main",
    "checked": true,
    "id": "21522b397f477a4a4e2253042d3c25cd03af9820",
    "citation_count": 103
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4602_ECCV_2020_paper.php": {
    "title": "Two-Stream Consensus Network for Weakly-Supervised Temporal Action Localization",
    "abstract": "Weakly-supervised Temporal Action Localization (W-TAL) aims to classify and localize all action instances in an untrimmed video under only video-level supervision. However, without frame-level annotations, it is challenging for W-TAL methods to identify false positive action proposals and generate action proposals with precise temporal boundaries. In this paper, we present a Two-Stream Consensus Network (TSCN) to simultaneously address these challenges. The proposed TSCN features an iterative refinement training method, where a frame-level pseudo ground truth is iteratively updated, and used to provide frame-level supervision for improved model training and false positive action proposal elimination. Furthermore, we propose a new attention normalization loss to encourage the predicted attention to act like a binary selection, and promote the precise localization of action instance boundaries. Experiments conducted on the THUMOS14 and ActivityNet datasets show that the proposed TSCN outperforms current state-of-the-art methods, and even achieves comparable results with some recent fully-supervised methods",
    "volume": "main",
    "checked": true,
    "id": "b41ec90b8e8972e6d09ae129ce4e004e37ad4015",
    "citation_count": 56
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4677_ECCV_2020_paper.php": {
    "title": "Erasing Appearance Preservation in Optimization-based Smoothing",
    "abstract": "Optimization-based image smoothing is routinely formulated as the game between a smoothing energy and an appearance preservation energy. Achieving adequate smoothing is a fundamental goal of these image smoothing algorithms. We show that partially \"\"erasing\"\" the appearance preservation facilitate adequate image smoothing. In this paper, we call this manipulation as Erasing Appearance Preservation (EAP). We conduct an user study, allowing users to indicate the \"\"erasing\"\" positions by drawing scribbles interactively, to verify the correctness and effectiveness of EAP. We observe the characteristics of human-indicated \"\"erasing\"\" positions, and then formulate a simple and effective 0-1 knapsack to automatically synthesize the \"\"erasing\"\" positions. We test our synthesized erasing positions in a majority of image smoothing methods. Experimental results and large-scale perceptual human judgments show that the EAP solution tends to encourage the pattern separation or elimination capabilities of image smoothing algorithms. We further study the performance of the EAP solution in many image decomposition problems to decompose textures, shadows, and the challenging specular reflections. We also present examinations of diversiform image manipulation applications like texture removal, retexturing, intrinsic decomposition, layer extraction, recoloring, material manipulation, etc. Due to the widespread applicability of image smoothing, the EAP is also likely to be used in more image editing applications",
    "volume": "main",
    "checked": true,
    "id": "ee57f611e671c5cf0bb9d2e2786d4e217b095c8c",
    "citation_count": 1
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4727_ECCV_2020_paper.php": {
    "title": "Counterfactual Vision-and-Language Navigation via Adversarial Path Sampler",
    "abstract": "Vision-and-Language Navigation (VLN) is a task where agents must decide how to move through a 3D environment to reach a goal by grounding natural language instructions to the visual surroundings. One of the problems of the VLN task is data scarcity since it is difficult to collect enough navigation paths with human-annotated instructions for interactive environments. In this paper, we explore the use of counterfactual thinking as a human-inspired data augmentation method that results in robust models. Counterfactual thinking is a concept that describes the human propensity to create possible alternatives to life events that have already occurred. We propose an adversarial-driven counterfactual reasoning model that can consider effective conditions instead of low-quality augmented data. In particular, we present a model-agnostic adversarial path sampler (APS) that learns to sample challenging paths that force the navigator to improve based on the navigation performance. APS also serves to do pre-exploration of unseen environments to strengthen the model's ability to generalize. We evaluate the influence of APS on the performance of different VLN baseline models using the room-to-room dataset (R2R). The results show that the adversarial training process with our proposed APS benefits VLN models under both seen and unseen environments. And the pre-exploration process can further gain additional improvements under unseen environments",
    "volume": "main",
    "checked": false,
    "id": "bf7cbd37e6be38f4c8871d70e639153479717a2d",
    "citation_count": 43
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4749_ECCV_2020_paper.php": {
    "title": "Guided Deep Decoder: Unsupervised Image Pair Fusion",
    "abstract": "The fusion of input and guidance images that have a tradeoff in their information (e.g., hyperspectral and RGB image fusion or pansharpening) can be interpreted as one general problem. However, previous studies applied a task-specific handcrafted prior and did not address the problems with a unified approach. To address this limitation, in this study, we propose a guided deep decoder network as a general prior. The proposed network is composed of an encoder-decoder network that exploits multi-scale features of a guidance image and a deep decoder network that generates an output image. The two networks are connected by feature refinement units to embed the multi-scale features of the guidance image into the deep decoder network. The proposed network allows the network parameters to be optimized in an unsupervised way without training data. Our results show that the proposed network can achieve state-of-the-art performance in various image fusion problems",
    "volume": "main",
    "checked": true,
    "id": "0cd35f9d03cfa35250b6bb54174dafc66875d16b",
    "citation_count": 29
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4809_ECCV_2020_paper.php": {
    "title": "Filter Style Transfer between Photos",
    "abstract": "Over the past few years, image-to-image style transfer has risen to the frontiers of neural image processing. While conventional methods were successful in various tasks such as color and texture transfer between images, none could effectively work with the custom filter effects that are applied by users through various platforms like Instagram. In this paper, we introduce a new concept of style transfer, Filter Style Transfer (FST). Unlike conventional style transfer, new technique FST can extract and transfer custom filter style from a filtered style image to a content image. FST first infers the original image from a filtered reference via image-to-image translation. Then it estimates filter parameters from the difference between them. To resolve the ill-posed nature of reconstructing the original image from the reference, we represent each pixel color of an image to class mean and deviation. Besides, to handle the intra-class color variation, we propose an uncertainty based weighted least square method for restoring an original image. To the best of our knowledge, FST is the first style transfer method that can transfer custom filter effects between FHD image under 2ms on a mobile device without any textual context loss",
    "volume": "main",
    "checked": true,
    "id": "b9802c0ed57fa2027e1365272bfbf1d3170f1b0e",
    "citation_count": 13
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4860_ECCV_2020_paper.php": {
    "title": "JGR-P2O: Joint Graph Reasoning based Pixel-to-Offset Prediction Network for 3D Hand Pose Estimation from a Single Depth Image",
    "abstract": "State-of-the-art single depth image-based 3D hand pose estimation methods are based on dense predictions, including voxel-to-voxel predictions, point-to-point regression, and pixel-wise estimations. Despite the good performance, those methods have a few issues in nature, such as the poor trade-off between accuracy and efficiency, and plain feature representation learning with local convolutions. In this paper, a novel pixel-wise prediction-based method is proposed to address the above issues. The key ideas are two-fold: a) explicitly modeling the dependencies among joints and the relations between the pixels and the joints for better local feature representation learning; b) unifying the dense pixel-wise offset predictions and direct joint regression for end-to-end training. Specifically, we first propose a graph convolutional network (GCN) based joint graph reasoning module to model the complex dependencies among joints and augment the representation capability of each pixel. Then we densely estimate all pixels' offsets to joints in both image plane and depth space and calculate the joints' positions by a weighted average over all pixels' predictions, totally discarding the complex post-processing operations. The proposed model is implemented with an efficient 2D fully convolutional network (FCN) backbone and has only about 1.4M parameters. Extensive experiments on multiple 3D hand pose estimation benchmarks demonstrate that the proposed method achieves new state-of-the-art accuracy while running very efficiently with around a speed of 110fps on a single NVIDIA 1080Ti GPU",
    "volume": "main",
    "checked": true,
    "id": "780d6a66db9d2006511039eb232598af8abdccad",
    "citation_count": 18
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4867_ECCV_2020_paper.php": {
    "title": "Dynamic Group Convolution for Accelerating Convolutional Neural Networks",
    "abstract": "Replacing normal convolutions with group convolutions can significantly increase the computational efficiency of modern deep convolutional networks, which has been widely adopted in compact network architecture designs. However, existing group convolutions undermine the original network structures by cutting off some connections permanently resulting in significant accuracy degradation. In this paper, we propose dynamic group convolution (DGC) that adaptively selects which part of input channels to be connected within each group for individual samples on the fly. Specifically, we equip each group with a small feature selector to automatically select the most important input channels conditioned on the input images. Multiple groups can adaptively capture abundant and complementary visual/semantic features for each input image. The DGC preserves the original network structure and has similar computational efficiency as the conventional group convolution simultaneously. Extensive experiments on multiple image classification benchmarks including CIFAR-10, CIFAR-100 and ImageNet demonstrate its superiority over the existing group convolution techniques and dynamic execution methods. The code is available at https://github.com/zhuogege1943/dgc",
    "volume": "main",
    "checked": true,
    "id": "4a819d20abbc171b8bca370fcf1b298b1166e839",
    "citation_count": 18
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4880_ECCV_2020_paper.php": {
    "title": "RD-GAN: Few/Zero-Shot Chinese Character Style Transfer via Radical Decomposition and Rendering",
    "abstract": "Style transfer has attracted much interest owing to its various applications. Compared with English character or general artistic style transfer, Chinese character style transfer remains a challenge owing to the large size of the vocabulary(70224 characters in GB18010-2005) and the complexity of the structure. Recently some GAN-based methods were proposed for style transfer; however, they treated Chinese characters as a whole, ignoring the structures and radicals that compose characters. In this paper, a novel radical decomposition-and-rendering-based GAN(RD-GAN) is proposed to utilize the radical-level compositions of Chinese characters and achieves few-shot/zero-shot Chinese character style transfer. The RD-GAN consists of three components: a radical extraction module (REM), radical rendering module (RRM), and multi-level discriminator (MLD). Experiments demonstrate that our method has a powerful few-shot/zero-shot generalization ability by using the radical-level compositions of Chinese characters",
    "volume": "main",
    "checked": true,
    "id": "dcd546b3a8475b9e7e4e67376fb196a23ed923a9",
    "citation_count": 18
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/5021_ECCV_2020_paper.php": {
    "title": "Object-Contextual Representations for Semantic Segmentation",
    "abstract": "In this paper, we address the semantic segmentation problem with a focus on the context aggregation strategy. Our motivation is that the label of a pixel is the category of the object that the pixel belongs to. We present a simple yet effective approach, object-contextual representations, characterizing a pixel by exploiting the representation of the corresponding object class. First, we learn object regions under the supervision of the ground-truth segmentation. Second, we compute the object region representation by aggregating the representations of the pixels lying in the object region. Last, % the representation similarity we compute the relation between each pixel and each object region, and augment the representation of each pixel with the object-contextual representation which is a weighted aggregation of all the object region representations according to their relations with the pixel. We empirically demonstrate that the proposed approach achieves competitive performance on various challenging semantic segmentation benchmarks: Cityscapes, ADE20K, LIP, PASCAL-Context, and COCO-Stuff. Cityscapes, ADE20K, LIP, PASCAL-Context and COCO-Stuff. Our submission \"\"HRNet + OCR + SegFix\"\" achieves the 1-st place on the Cityscapes leaderboard by the time of submission",
    "volume": "main",
    "checked": true,
    "id": "a88c914f5a738d38f02790bb5de41453bf17bde1",
    "citation_count": 597
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/5116_ECCV_2020_paper.php": {
    "title": "Efficient Spatio-Temporal Recurrent Neural Network for Video Deblurring",
    "abstract": "Real-time video deblurring still remains a challenging task due to the complexity of spatially and temporally varying blur itself and the requirement of low computational cost. To improve the network efficiency, we adopt residual dense blocks into RNN cells, so as to efficiently extract the spatial features of the current frame. Furthermore, a global spatio-temporal attention module is proposed to fuse the effective hierarchical features from past and future frames to help better deblur the current frame. For evaluation, we also collect a novel dataset with paired blurry/sharp video clips by using a co-axis beam splitter system. Through experiments on synthetic and realistic datasets, we show that our proposed method can achieve better deblurring performance both quantitatively and qualitatively with less computational cost against state-of-the-art video deblurring methods",
    "volume": "main",
    "checked": true,
    "id": "014a97c26474f99da1b9dcdaada12fc2e731d840",
    "citation_count": 58
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/5393_ECCV_2020_paper.php": {
    "title": "Joint Semantic Instance Segmentation on Graphs with the Semantic Mutex Watershed",
    "abstract": "Semantic instance segmentation is the task of simultaneously partitioning an image into distinct segments while associating each pixel with a class label. In commonly used pipelines, segmentation and label assignment are solved separately since joint optimization is computationally expensive. We propose a greedy algorithm for joint graph partitioning and labeling derived from the efficient Mutex Watershed partitioning algorithm. It optimizes an objective function closely related to the Symmetric Multiway Cut objective and empirically shows efficient scaling behavior. Due to the algorithm's efficiency, it can operate directly on pixels without prior over-segmentation of the image into superpixels. We evaluate the performance on the Cityscapes dataset (2D urban scenes) and on a 3D microscopy volume. In urban scenes, the proposed algorithm combined with current deep neural networks outperforms the strong baseline of `Panoptic Feature Pyramid Networks' by Kirillov et al. (2019). In the 3D electron microscopy images, we show explicitly that our joint formulation outperforms a separate optimization of the partitioning and labeling problems",
    "volume": "main",
    "checked": false,
    "id": "7f0cda214adcf307381164e8b4f37b4c48228523",
    "citation_count": 10
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/5471_ECCV_2020_paper.php": {
    "title": "Photon-Efficient 3D Imaging with A Non-Local Neural Network",
    "abstract": "Photon-efficient imaging has enabled a number of applications relying on single-photon sensors that can capture a 3D image with as few as one photon per pixel. In practice, however, measurements of low photon counts are often mixed with heavy background noise, which poses a great challenge for existing computational reconstruction algorithms. In this paper, we first analyze the long-range correlations in both spatial and temporal dimensions of the measurements. Then we propose a non-local neural network for depth reconstruction by exploiting the long-range correlations. The proposed network achieves decent reconstruction fidelity even under photon counts (and signal-to-background ratio, SBR) as low as 1 photon/pixel (and 0.01 SBR), which significantly surpasses the state-of-the-art. Moreover, our non-local network trained on simulated data can be well generalized to different real-world imaging systems, which could extend the application scope of photon-efficient imaging in challenging scenarios with a strict limit on optical flux. Code is available at https://github.com/JiayongO-O/PENonLocal",
    "volume": "main",
    "checked": true,
    "id": "71e4388d080d4e9ed91ede0e8bfa1366fbdb9fca",
    "citation_count": 12
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/5554_ECCV_2020_paper.php": {
    "title": "GeLaTO: Generative Latent Textured Objects",
    "abstract": "Accurate modeling of 3D objects exhibiting transparency, reflections and thin structures is an extremely challenging problem. Inspired by billboards and geometric proxies used in computer graphics, this paper proposes Generative Latent Textured Objects (GeLaTO), a compact representation that combines a set of coarse shape proxies defining low frequency geometry with learned neural textures, to encode both medium and fine scale geometry as well as view-dependent appearance. To generate the proxies' textures, we learn a joint latent space allowing category-level appearance and geometry interpolation. The proxies are independently rasterized with their corresponding neural texture and composited using a U-Net, which generates an output photorealistic image including an alpha map. We demonstrate the effectiveness of our approach by reconstructing complex objects from a sparse set of views. We show results on a dataset of real images of eyeglasses frames, which are particularly challenging to reconstruct using classical methods. We also demonstrate that these coarse proxies can be handcrafted when the underlying object geometry is easy to model, like eyeglasses, or generated using a neural network for more complex categories, such as cars",
    "volume": "main",
    "checked": true,
    "id": "c861eaf43aa30b35813e9a84f07a8e1651c9722e",
    "citation_count": 11
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/5672_ECCV_2020_paper.php": {
    "title": "Improving Vision-and-Language Navigation with Image-Text Pairs from the Web",
    "abstract": "Following a navigation instruction such as 'Walk down the stairs and stop at the brown sofa' requires embodied AI agents to ground referenced scene elements referenced (e.g. 'stairs') to visual content in the environment (pixels corresponding to 'stairs'). We ask the following question -- can we leverage abundant `disembodied' web-scraped vision-and-language corpora (e.g. Conceptual Captions) to learn the visual groundings that improve performance on a relatively data-starved embodied perception task (Vision-and-Language Navigation)? Specifically, we develop VLN-BERT, a visiolinguistic transformer-based model for scoring the compatibility between an instruction ('...stop at the brown sofa') and a trajectory of panoramic RGB images captured by the agent. We demonstrate that pretraining VLN-BERT on image-text pairs from the web before fine-tuning on embodied path-instruction data significantly improves performance on VLN -- outperforming prior state-of-the-art in the fully-observed setting by 4 absolute percentage points on success rate. Ablations of our pretraining curriculum show each stage to be impactful -- with their combination resulting in further gains",
    "volume": "main",
    "checked": true,
    "id": "d1ac487f21829ef56c8ffdcd37ea414bce68c809",
    "citation_count": 104
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/5685_ECCV_2020_paper.php": {
    "title": "Directional Temporal Modeling for Action Recognition",
    "abstract": "Many current activity recognition models use 3D convolutional neural networks (e.g. I3D, I3D-NL) to generate local spatial-temporal features. However, such features do not encode clip-level ordered temporal information. In this paper, we introduce a channel independent directional convolution (CIDC) operation, which learns to model the temporal evolution among local features. By applying multiple CIDC units we construct a light-weight network that models the clip-level temporal evolution across multiple spatial scales. Our CIDC network can be attached to any activity recognition backbone network. We evaluate our method on four popular activity recognition datasets and consistently improve upon state-of-the-art techniques. We further visualize the activation map of our CIDC network and show that it is able to focus on more meaningful, action related parts of the frame",
    "volume": "main",
    "checked": true,
    "id": "c40990d00633b63caf78082f8570a55e2ec5abbb",
    "citation_count": 21
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/5714_ECCV_2020_paper.php": {
    "title": "Shonan Rotation Averaging: Global Optimality by Surfing SO(p)(n)",
    "abstract": "Shonan Rotation Averaging is a fast, simple, and elegant rotation averaging algorithm that is guaranteed to recover globally optimal solutions under mild assumptions on the measurement noise. Our method employs semidefinite relaxation in order to recover provably globally optimal solutions of the rotation averaging problem. In contrast to prior work, we show how to solve large-scale instances of these relaxations using manifold minimization on (only slightly) higher-dimensional rotation manifolds, re-using existing high-performance (but local) structure-from-motion pipelines. Our method thus preserves the speed and scalability of those, while enabling the recovery of globally optimal solutions",
    "volume": "main",
    "checked": false,
    "id": "4c5b26e39b4581e94f12a623f8eeade722a74713",
    "citation_count": 28
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/5723_ECCV_2020_paper.php": {
    "title": "Semantic Curiosity for Active Visual Learning",
    "abstract": "In this paper, we study the task of embodied interactive learning for object detection. Given a set of environments (and some labeling budget), our goal is to learn an object detector by having an agent select what data to obtain labels for. How should an exploration policy decide which trajectory should be labeled? One possibility is to use a trained object detector's failure cases as an external reward. However, this will require labeling millions of frames required for training RL policies, which is infeasible. Instead, we explore a self-supervised approach for training our exploration policy by introducing a notion of semantic curiosity. Our semantic curiosity policy is based on a simple observation -- the detection outputs should be consistent. Therefore, our semantic curiosity rewards trajectories with inconsistent labeling behavior and encourages the exploration policy to explore such areas. The exploration policy trained via semantic curiosity generalizes to novel scenes and helps train an object detector that outperforms baselines trained with other possible alternatives such as random exploration, prediction-error curiosity, and coverage-maximizing exploration",
    "volume": "main",
    "checked": true,
    "id": "a37bc4d0f44d5e1bf7daf022aae936aa842801e4",
    "citation_count": 25
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/5821_ECCV_2020_paper.php": {
    "title": "Multi-Temporal Recurrent Neural Networks For Progressive Non-Uniform Single Image Deblurring With Incremental Temporal Training",
    "abstract": "Blind non-uniform image deblurring for severe blurs induced by large motions is still challenging. Multi-scale (MS) approach has been widely used for deblurring that sequentially recovers the downsampled original image in low spatial scale first and then further restores in high spatial scale using the result(s) from lower spatial scale(s). Here, we investigate a novel alternative approach to MS, called multi-temporal (MT), for non-uniform single image deblurring by exploiting time-resolved deblurring dataset from high-speed cameras. MT approach models severe blurs as a series of small blurs so that it deblurs small amount of blurs in the original spatial scale progressively instead of restoring the images in different spatial scales. To realize MT approach, we propose progressive deblurring over iterations and incremental temporal training with temporally augmented training data. Our MT approach, that can be seen as a form of curriculum learning in a wide sense, allows a number of state-of-the-art MS based deblurring methods to yield improved performances without using MS approach. We also proposed a MT recurrent neural network with recurrent feature maps that outperformed state-of-the-art deblurring methods with the smallest number of parameters",
    "volume": "main",
    "checked": true,
    "id": "20eb22846f41c99566046304dbb40e1515e3dd1a",
    "citation_count": 66
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/5975_ECCV_2020_paper.php": {
    "title": "ProgressFace: Scale-Aware Progressive Learning for Face Detection",
    "abstract": "Scale variation stands out as one of key challenges in face detection. Recent attempts have been made to cope with this issue by incorporating image / feature pyramids or adjusting anchor sampling / matching strategies. In this work, we propose a novel scale-aware progressive training mechanism to address large scale variations across faces. Inspired by curriculum learning, our method gradually learns large-to-small face instances. The preceding models learned with easier samples (i.e., large faces) can provide good initialization for succeeding learning with harder samples (i.e., small faces), ultimately deriving a better optimum of face detectors. Moreover, we propose an auxiliary anchor-free enhancement module to facilitate the learning of small faces by supplying positive anchors that may be not covered according to the criterion of IoU overlap. Such anchor-free module will be removed during inference and hence no extra computation cost is introduced. Extensive experimental results demonstrate the superiority of our method compared to the state-of-the-arts on the standard FDDB and WIDER FACE benchmarks. Especially, our ProgressFace-Light with MobileNet-0.25 backbone achieves 87.9% AP on the hard set of WIDER FACE, surpassing largely RetinaFace with the same backbone by 9.7%. Code and our trained face detection models are available at https://github.com/jiashu-zhu/ProgressFace",
    "volume": "main",
    "checked": true,
    "id": "992c2f63f8dc1ebf2c4fcbe70fa2f239dc975f1c",
    "citation_count": 11
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/6025_ECCV_2020_paper.php": {
    "title": "Learning Multi-layer Latent Variable Model via Variational Optimization of Short Run MCMC for Approximate Inference",
    "abstract": "This paper studies the fundamental problem of learning deep generative models that consist of multiple layers of latent variables organized in top-down architectures. Such models have high expressivity and allow for learning hierarchical representations. Learning such a generative model requires inferring the latent variables for each training example based on the posterior distribution of these latent variables. The inference typically requires Markov chain Monte Caro (MCMC) that can be time consuming. In this paper, we propose to use noise initialized non-persistent short run MCMC, such as finite step Langevin dynamics initialized from the prior distribution of the latent variables, as an approximate inference engine, where the step size of the Langevin dynamics is variationally optimized by minimizing the Kullback-Leibler divergence between the distribution produced by the short run MCMC and the posterior distribution. Our experiments show that the proposed method outperforms variational auto-encoder (VAE) in terms of reconstruction error and synthesis quality. The advantage of the proposed method is that it is simple and automatic without the need to design an inference model",
    "volume": "main",
    "checked": true,
    "id": "7bc416b082ad29a649eaad2bbe4a272a45da9b5d",
    "citation_count": 21
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/6053_ECCV_2020_paper.php": {
    "title": "CoTeRe-Net: Discovering Collaborative Ternary Relations in Videos",
    "abstract": "Modeling relations is crucial to understand videos for action and behavior recognition. Current relation models mainly reason about relations of invisibly implicit cues, while important relations of visually explicit cues are rarely considered, and the collaboration between them is usually ignored. In this paper, we propose a novel relation model that discovers relations of both implicit and explicit cues as well as their collaboration in videos. Our model concerns Collaborative Ternary Relations (CoTeRe), where the ternary relation involves channel (C, for implicit), temporal (T, for implicit), and spatial (S, for explicit) relation (R). We devise a flexible and effective CTSR module to collaborate ternary relations for 3D-CNNs, and then construct CoTeRe-Nets for action recognition. Extensive experiments on both ablation study and performance evaluation demonstrate that our CTSR module is significantly effective with approximate 3% gains and our CoTeRe-Nets outperform state-of-the-art approaches on three popular benchmarks. Boosts analysis and relations visualization also validate that relations of both implicit and explicit cues are discovered with efficacy by our method. Our code is available at https://github.com/zhenglab/cotere-net",
    "volume": "main",
    "checked": true,
    "id": "c826e169f505f6fda0872d0a3e1e156e15b5111e",
    "citation_count": 1
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/6100_ECCV_2020_paper.php": {
    "title": "Modeling the Effects of Windshield Refraction for Camera Calibration",
    "abstract": "In this paper, we study the effects of windshield refraction for autonomous driving applications. These distortion effects are surprisingly large and can not be explained by traditional camera models. Instead of using a generalized camera approach, we propose a novel approach to jointly optimize a traditional camera model, and a mathematical representation of the windshield’s surface. First, using the laws of geometric optics, the refraction is modeled using a local spherical approximation to the windshield’s geometry. Next, a spline-based model is proposed as a refinement to better adapt to deviations from the ideal shape and manufacturing variations. By jointly optimizing refraction and camera parameters, the projection error can be significantly reduced. The proposed models are validated on real windshield observations and custom setups to compare recordings with and without windshield, with accurate laser scan measurements as 3D ground truth",
    "volume": "main",
    "checked": true,
    "id": "000b5c61a292a41f7c79e9ef29d067c7b8e4441e",
    "citation_count": 6
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/6124_ECCV_2020_paper.php": {
    "title": "Unsupervised Domain Adaptation for Semantic Segmentation of NIR Images through Generative Latent Search",
    "abstract": "Segmentation of the pixels corresponding to human skin is an essential first step in multiple applications ranging from surveillance to heart-rate estimation from remote-photoplethysmography. However, the existing literature considers the problem only in the visible-range of the EM-spectrum which limits their utility in low or no light settings where the criticality of the application is higher. To alleviate this problem, we consider the problem of skin segmentation from the Near-infrared images. However, Deep learning based state-of-the-art segmentation techniques demands large amounts of labelled data that is unavailable for the current problem. Therefore we cast the skin segmentation problem as that of target-independent Unsupervised Domain Adaptation (UDA) where we use the data from the Red-channel of the visible-range to develop skin segmentation algorithm on NIR images. We propose a method for target-independent segmentation where the 'nearest-clone' of a target image in the source domain is searched and used as a proxy in the segmentation network trained only on the source domain. We prove the existence of 'nearest-clone' and propose a method to find it through an optimization algorithm over the latent space of a Deep generative model based on variational inference. We demonstrate the efficacy of the proposed method for NIR skin segmentation over the state-of-the-art UDA segmentation methods on the two newly created skin segmentation datasets in NIR domain despite not having access to the target NIR data. Additionally, we report state-of-the-art results for adaption from Synthia to Cityscapes which is a popular setting in Unsupervised Domain Adaptation for semantic segmentation. The code and datasets are available at https://github.com/ambekarsameer96/GLSS",
    "volume": "main",
    "checked": true,
    "id": "f6ac46b98140ad7da6c42124479234b9b1e44591",
    "citation_count": 6
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/6254_ECCV_2020_paper.php": {
    "title": "PROFIT: A Novel Training Method for sub-4-bit MobileNet Models",
    "abstract": "4-bit and lower precision mobile models are required due to the ever-increasing demand for better energy efficiency in mobile devices. In this work, we report that the activation instability induced by weight quantization (AIWQ) is the key obstacle to sub-4-bit quantization of mobile networks. To alleviate the AIWQ problem, we propose a novel training method called PROgressive-Freezing Iterative Training (PROFIT), which attempts to freeze layers whose weights are affected by the instability problem stronger than the other layers. We also propose a differentiable and unified quantization method (DuQ) and a negative padding idea to support asymmetric activation functions such as h-swish. We evaluate the proposed methods by quantizing MobileNet-v1, v2, and v3 on ImageNet and report that 4-bit quantization offers comparable (within 1.48 % top-1 accuracy) accuracy to full precision baseline. In the ablation study of the 3-bit quantization of MobileNet-v3, our proposed method outperforms the state-of-the-art method by a large margin, 12.86 % of top-1 accuracy. The quantized model and source code is available at https://github.com/EunhyeokPark/PROFIT",
    "volume": "main",
    "checked": true,
    "id": "010263f1009cb621b33b3f1f844d7e7476884a44",
    "citation_count": 39
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/6277_ECCV_2020_paper.php": {
    "title": "Visual Relation Grounding in Videos",
    "abstract": "In this paper, we explore a novel task named visual Relation Grounding in Videos (vRGV). The task aims at spatio-temporally localizing the given relations in the form of subject-predicate-object in the videos, so as to provide supportive visual facts for other high-level video-language tasks (e.g., video-language grounding and video question answering). The challenges in this task include but not limited to: (1) both the subject and object are required to be spatio-temporally localized to ground a query relation; (2) the temporal dynamic nature of visual relations in videos is difficult to capture; and (3) the grounding should be achieved without any direct supervision in space and time. To ground the relations, we tackle the challenges by collaboratively optimizing two sequences of regions over a constructed hierarchical spatio-temporal region graph through relation attending and reconstruction, in which we further propose a message passing mechanism by spatial attention shifting between visual entities. Experimental results demonstrate that our model can not only outperform baseline approaches significantly, but also produces visually meaningful facts to support visual grounding. (Code is available at https://github.com/doc-doc/vRGV)",
    "volume": "main",
    "checked": true,
    "id": "13ee363f71e07112210ac2ff27d46625f6f8edab",
    "citation_count": 22
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/6296_ECCV_2020_paper.php": {
    "title": "Weakly Supervised 3D Human Pose and Shape Reconstruction with Normalizing Flows",
    "abstract": "Monocular 3D human pose and shape estimation is challenging due to the many degrees of freedom of the human body and the difficulty to acquire training data for large-scale supervised learning in complex visual scenes where humans with diverse shape and appearance, appear against complex backgrounds,in a variety of poses, and are partially occluded, or involved in interactions. Essential to learning is leveraging effective 3D human priors, and the ability to work under weak supervision, at scale, by exploiting, to the largest extent, the detailed human body semantics in images. In this paper we present new priors as well as large-scale weakly supervised models for 3D human pose and shape estimation. Key to our formulation are new latent normalizing flow representations, as well as fully differentiable, structurally-sensitive, semantic body part alignment(re-projection) loss functions that ensure consistent estimates and sharp feedback signals for learning. In extensive experiments using both motion capture datasets like CMU, Human3.6M, 3DPW, or AMASS, and repositories like COCO, we show that our proposed methods outperform existing counterparts, supporting the construction of an increasingly more accurate family of models based on large-scale training with unlabeled image data",
    "volume": "main",
    "checked": true,
    "id": "81f3ab42f1d65d86f31ae45851e35749f56938ed",
    "citation_count": 66
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/6314_ECCV_2020_paper.php": {
    "title": "Controlling Style and Semantics in Weakly-Supervised Image Generation",
    "abstract": "We propose a weakly-supervised approach for conditional image generation of complex scenes where a user has fine control over objects appearing in the scene. We exploit sparse semantic maps to control object shapes and classes, as well as textual descriptions or attributes to control both local and global style. In order to condition our model on textual descriptions, we introduce a semantic attention module whose computational cost is independent of the image resolution. To further augment the controllability of the scene, we propose a two-step generation scheme that decomposes background and foreground. The label maps used to train our model are produced by a large-vocabulary object detector, which enables access to unlabeled data and provides structured instance information. In such a setting, we report better FID scores compared to fully-supervised settings where the model is trained on ground-truth semantic maps. We also showcase the ability of our model to manipulate a scene on complex datasets such as COCO and Visual Genome",
    "volume": "main",
    "checked": true,
    "id": "0578cc9ea5158ba57cb2fbaf966aae084ac1f36b",
    "citation_count": 17
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/6360_ECCV_2020_paper.php": {
    "title": "Jointly learning visual motion and confidence from local patches in event cameras",
    "abstract": "We propose the first network to jointly learn visual motion and confidence from events in spatially local patches. Event-based sensors deliver high temporal resolution motion information in a sparse, non-redundant format. This creates the potential for low computation, low latency motion recognition. Neural networks which extract global motion information, however, are generally computationally expensive. Here, we introduce a novel shallow and compact neural architecture and learning approach to capture reliable visual motion information along with the corresponding confidence of inference. Our network makes a prediction of the visual motion at each spatial location using only local events. Our confidence network then identifies which of these predictions will be accurate. In the task of recovering pan-tilt ego velocities from events, we show that each individual confident local prediction of our network can be expected to be as accurate as state of the art optimization approaches which utilize the full image. Furthermore, on a publicly available dataset, we find our local predictions generalize to scenes with camera motions and the presence of independently moving objects. This makes the output of our network well suited for motion based tasks, such as the segmentation of independently moving objects. We demonstrate on a publicly available motion segmentation dataset that restricting predictions to confident regions is sufficient to achieve results that exceed state of the art methods",
    "volume": "main",
    "checked": true,
    "id": "b606bb479c85a3d04bcdfca1c2ae17d91c30c623",
    "citation_count": 8
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/6406_ECCV_2020_paper.php": {
    "title": "SODA: Story Oriented Dense Video Captioning Evaluation Framework",
    "abstract": "Dense Video Captioning (DVC) is a challenging task that localizes all events in a short video and describes them with natural language sentences. The main goal of DVC is video story description, that is, to generate a concise video story that supports human video comprehension without watching it. In recent years, DVC has attracted increasing attention in the research community of Vision and Language, and has been employed as a task of the workshop, ActivityNet Challenge. In the current research community, the official scorer provided by AcivityNet Challenge is the de-facto standard evaluation framework for DVC systems. It computes averaged METEOR scores for matched pairs between generated and reference captions whose Interval of Union (IoU) exceeds a specific threshold value. However, the current framework does not take into account the story of the video, the ordering of captions. It also tends to give high scores to systems that generate redundant, several hundred captions, that humans cannot read. This paper proposes a new evaluation framework, Story Oriented Dense video cAptioning evaluation framework (SODA), for measuring the performance of video story description systems. SODA first tries to find temporally optimal matching between generated and reference captions to capture a story for a video. Then, it computes METEOR scores for the matching and derives F-measure scores from the METEOR scores to penalize redundant captions. To demonstrate that SODA gives low scores for inadequate captions in terms of video story description, we evaluate two state-of-the-art systems with it, varying the number of captions. The results show that SODA can give low scores against too many or too few captions and high scores against captions whose number equals to that of a reference, while the current framework gives good scores for all the cases. Furthermore, we show that SODA tends to give lower scores than the current evaluation framework in evaluating captions with incorrect order",
    "volume": "main",
    "checked": true,
    "id": "5a4c5fa5a25cff3c65e74f64504819683353ef1e",
    "citation_count": 9
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/6490_ECCV_2020_paper.php": {
    "title": "Sketch-Guided Object Localization in Natural Images",
    "abstract": "We introduce a novel problem of localizing all the instances of an object (seen or unseen during training) in a natural image via sketch query. We refer to this problem as sketch-guided object localization. This problem is distinctively different from the traditional sketch-based image retrieval task where the gallery set often contains images with only one object. The sketch-guided object localization proves to be more challenging when we consider the following: (i) the sketches used as queries are abstract representations with little information on the shape and salient attributes of the object, (ii) the sketches have significant variability as they are hand-drawn by a diverse set of untrained human subjects, and (iii) there exists a domain gap between sketch queries and target natural images as these are sampled from very different data distributions. To address the problem of sketch-guided object localization, we propose a novel mph{cross-modal attention} scheme that guides the region proposal network (RPN) to generate object proposals relevant to the sketch query. These object proposals are later scored against the query to obtain final localization. Our method is effective with as little as a single sketch query. Moreover, it also generalizes well to object categories not seen during training and is effective in localizing multiple object instances present in the image. Furthermore, we extend our framework to a multi-query setting using novel feature fusion and attention fusion strategies introduced in this paper. The localization performance is evaluated on publicly available object detection benchmarks, viz. MS-COCO and PASCAL-VOC, with sketch queries obtained from `Quick, Draw!'. The proposed method significantly outperforms related baselines on both single-query and multi-query localization tasks",
    "volume": "main",
    "checked": true,
    "id": "a18665005ac40082a12b77cfe148ff6e7ba05ccf",
    "citation_count": 7
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/6496_ECCV_2020_paper.php": {
    "title": "A unifying mutual information view of metric learning: cross-entropy vs. pairwise losses",
    "abstract": "Recently, substantial research efforts in Deep Metric Learning (DML) focused on designing complex pairwise-distance losses, which require convoluted schemes to ease optimization, such as sample mining or pair weighting. The standard cross-entropy loss for classification has been largely overlooked in DML. On the surface, the cross-entropy may seem unrelated and irrelevant to metric learning as it does not explicitly involve pairwise distances. However, we provide a theoretical analysis that links the cross-entropy to several well-known and recent pairwise losses. Our connections are drawn from two different perspectives: one based on an explicit optimization insight; the other on discriminative and generative views of the mutual information between the labels and the learned features. First, we explicitly demonstrate that the cross-entropy is an upper bound on a new pairwise loss, which has a structure similar to various pairwise losses: it minimizes intra-class distances while maximizing inter-class distances. As a result, minimizing the cross-entropy can be seen as an approximate bound-optimization (or Majorize-Minimize) algorithm for minimizing this pairwise loss. Second, we show that, more generally,minimizing the cross-entropy is actually equivalent to maximizing the mutual information, to which we connect several well-known pairwise losses. Furthermore, we show that various standard pairwise losses can be explicitly related to one another via bound relationships. Our findings indicate that the cross-entropy represents a proxy for maximizing the mutual information – as pairwise losses do – without the need for convoluted sample-mining heuristics. Our experiments†over four standard DML benchmarks strongly support our findings. We obtain state-of-the-art results, outperforming recent and complex DML methods",
    "volume": "main",
    "checked": true,
    "id": "60ace9569025c68fcda94f5d572b22f6188b72ba",
    "citation_count": 63
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/6959_ECCV_2020_paper.php": {
    "title": "Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models",
    "abstract": "Recent Transformer-based large-scale pre-trained models have revolutionized vision-and-language (V+L) research. Models such as ViLBERT, LXMERT and UNITER have significantly lifted state of the art across a wide range of V+L benchmarks. However, little is known about the inner mechanisms that destine their impressive success. To reveal the secrets behind the scene, we present VALUE (Vision-And-Language Understanding Evaluation), a set of meticulously designed probing tasks (e.g., Visual Coreference Resolution, Visual Relation Detection) generalizable to standard pre-trained V+L models, to decipher the inner workings of multimodal pre-training (e.g., implicit knowledge garnered in individual attention heads, inherent cross-modal alignment learned through contextualized multimodal embeddings). Through extensive analysis of each archetypal model architecture via these probing tasks, our key observations are: (i) Pre-trained models exhibit a propensity for attending over text rather than images during inference. (ii) There exists a subset of attention heads that are tailored for capturing cross-modal interactions. (iii) Learned attention matrix in pre-trained models demonstrates patterns coherent with the latent alignment between image regions and textual words. (iv) Plotted attention patterns reveal visually-interpretable relations among image regions. (v) Pure linguistic knowledge is also effectively encoded in the attention heads. These are valuable insights serving to guide future work towards designing better model architecture and objectives for multimodal pre-training",
    "volume": "main",
    "checked": true,
    "id": "26cfb57a9722599b361858d454ec816420723e36",
    "citation_count": 79
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/7231_ECCV_2020_paper.php": {
    "title": "The Hessian Penalty: A Weak Prior for Unsupervised Disentanglement",
    "abstract": "Existing popular methods for disentanglement rely on hand-picked priors and complex encoder-based architectures. In this paper, we propose the Hessian Penalty, a simple regularization function that encourages the input Hessian of a function to be diagonal. Our method is completely model-agnostic and can be applied to any deep generator with just a few lines of code. We show that our method automatically uncovers meaningful factors of variation in the standard basis when applied to ProgressiveGAN across several datasets. Additionally, we demonstrate that our regularization term can be used to identify interpretable directions in BigGAN's latent space in a fully unsupervised fashion. Finally, we provide provide empirical evidence that our regularization term encourages sparsity when applied to overparameterized latent spaces",
    "volume": "main",
    "checked": true,
    "id": "14d4b0f77c59c40e1fef3233a37af687d5ecb748",
    "citation_count": 66
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/5_ECCV_2020_paper.php": {
    "title": "STAR: Sparse Trained Articulated Human Body Regressor",
    "abstract": "The SMPL body model is widely used for the estimation, synthesis, and analysis of 3D human pose and shape. While popular, we show that SMPL has several limitations and introduce STAR, which is quantitatively and qualitatively superior to SMPL. First, SMPL has a huge number of parameters resulting from its use of global blend shapes. These dense pose-corrective offsets relate every vertex on the mesh to all the joints in the kinematic tree, capturing spurious long-range cor- relations. To address this, we define per-joint pose correctives and learn the subset of mesh vertices that are influenced by each joint movement. This sparse formulation results in more realistic deformations and significantly reduces the number of model parameters to 20% of SMPL. When trained on the same data as SMPL, STAR generalizes better despite having many fewer parameters. Second, SMPL factors pose-dependent deformations from body shape while, in reality, people with different shapes deform differently. Consequently, we learn shape-dependent pose- corrective blendshapes that depend on both body pose and BMI. Third, we show that the shape space of SMPL is not rich enough to capture the variation in the human population. We address this by training STAR with an additional 10,000 scans of male and female subjects, and show that this results in better model generalization. STAR is compact, generalizes better to new bodies and is a drop-in replacement for SMPL. STAR is publicly available for research purposes at http://star.is.tue.mpg.de",
    "volume": "main",
    "checked": true,
    "id": "531e91dee7a483c0a7d033a3606a594b6b23da13",
    "citation_count": 114
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/13_ECCV_2020_paper.php": {
    "title": "Optical Flow Distillation: Towards Efficient and Stable Video Style Transfer",
    "abstract": "Video style transfer techniques inspire many exciting applications on mobile devices. However, their efficiency and stability are still far from satisfactory. To boost the transfer stability across frames, optical flow is widely adopted, despite its high computational complexity, e.g., occupying over 97% inference time. This paper proposes to learn a lightweight video style transfer network via knowledge distillation paradigm. We adopt two teacher networks, one of which takes optical flow during inference while the other does not. The output difference between these two teacher networks highlights the improvements made by optical flow, which is then adopted to distill the target student network. Furthermore, a low-rank distillation loss is employed to stabilize the output of student network by mimicking the rank of input videos. Extensive experiments demonstrate that our student network without an optical flow module is still able to generate stable video and runs much faster than the teacher network",
    "volume": "main",
    "checked": true,
    "id": "3b1b2e2fc090ad383dc2923aaa16445f4c847ca9",
    "citation_count": 29
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/15_ECCV_2020_paper.php": {
    "title": "Collaboration by Competition: Self-coordinated Knowledge Amalgamation for Multi-talent Student Learning",
    "abstract": "A vast number of well-trained deep networks have been released by developers online for plug-and-play use. These networks specialize in different tasks and in many cases, the data and annotations used to train them are not publicly available. In this paper, we study how to reuse such heterogeneous pre-trained models as teachers, and build a versatile and compact student model, without accessing human annotations. To this end, we propose a self-coordinate knowledge amalgamation network (SOKA-Net) for learning the multi-talent student model. This is achieved via a dual-step adaptive competitive-cooperation training approach, where the knowledge of the heterogeneous teachers are in the first step amalgamated to guide the shared parameter learning of the student network, and followed by a gradient-based competition-balancing strategy to learn the multi-head prediction network as well as the loss weightings of the distinct tasks in the second step. The two steps, which we term as the collaboration and competition step respectively, are performed alternatively until the balance of the competition is reached for the ultimate collaboration. Experimental results demonstrate that, the learned student not only comes with a smaller size but all achieves performances on par with or even superior to those of the teachers",
    "volume": "main",
    "checked": true,
    "id": "74e3264172b1f293b37faff51db3795605c6d02d",
    "citation_count": 6
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/25_ECCV_2020_paper.php": {
    "title": "Do Not Disturb Me: Person Re-identification Under the Interference of Other Pedestrians",
    "abstract": "In the conventional person Re-ID setting, it is assumed that cropped images are the person images within the bounding box for each individual. However, in a crowded scene, off-shelf-detectors may generate bounding boxes involving multiple people, where the large proportion of background pedestrians or human occlusion exists. The representation extracted from such cropped images, which contain both the target and the interference pedestrians, might include distractive information. This will lead to wrong retrieval results. To address this problem, this paper presents a novel deep network termed Pedestrian-Interference Suppression Network (PISNet). PISNet leverages a Query-Guided Attention Block (QGAB) to enhance the feature of the target in the gallery, under the guidance of the query. Furthermore, the involving Guidance Reversed Attention Module and the Multi-Person Separation Loss promote QGAB to suppress the interference of other pedestrians. Our method is evaluated on two new pedestrian-interference datasets and the results show that the proposed method performs favorably against existing Re-ID methods",
    "volume": "main",
    "checked": true,
    "id": "dacf90db5fe8de3aec17b5edbb6c630f57cbe5bc",
    "citation_count": 24
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/31_ECCV_2020_paper.php": {
    "title": "Learning 3D Part Assembly from a Single Image",
    "abstract": "Autonomous assembly is a crucial capability for robots in many applications. For this task, several problems such as obstacle avoidance, motion planning, and actuator control have been extensively studied in robotics. However, when it comes to task specification, the space of possibilities remains under-explored. Towards this end, we introduce a novel problem,single-image-guided 3D part assembly, along with a learning-based solution. We study this problem in the setting of furniture assembly from a given complete set of parts and a single image depicting the entire assembled object. Multiple challenges exist in this setting, including handling ambiguity among parts (e.g., slats in a chair back and leg stretchers) and 3D pose prediction for parts and part subassemblies,whether visible or occluded. We address these issues by proposing a two-module pipeline that leverages strong 2D-3D correspondences and assembly-oriented graph message-passing to infer part relationships. In experiments with a PartNet-based synthetic benchmark, we demonstrate the effectiveness of our framework as compared with three baseline approaches",
    "volume": "main",
    "checked": true,
    "id": "63540ed70d842ed3fdbe16a1a4543c15982ed85e",
    "citation_count": 25
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/32_ECCV_2020_paper.php": {
    "title": "PT2PC: Learning to Generate 3D Point Cloud Shapes from Part Tree Conditions",
    "abstract": "Generative 3D shape modeling is a fundamental research area in computer vision and interactive computer graphics, with many real-world applications. This paper investigates the novel problem of generating a 3D point cloud geometry for a shape from a symbolic part tree representation. In order to learn such a conditional shape generation procedure in an end-to-end fashion, we propose a conditional GAN “part tree”-to-“point cloud” model (PT2PC) that disentangles the structural and geometric factors. The proposed model incorporates the part tree condition into the architecture design by passing messages top-down and bottom-up along the part tree hierarchy. Experimental results and user study demonstrate the strengths of our method in generating perceptually plausible and diverse 3D point clouds, given the part tree condition. We also propose a novel structural measure for evaluating if the generated shape point clouds satisfy the part tree conditions",
    "volume": "main",
    "checked": true,
    "id": "9992c3cf7d9f5713c8f64268d0b9d8892bd3293c",
    "citation_count": 28
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/50_ECCV_2020_paper.php": {
    "title": "Highly Efficient Salient Object Detection with 100K Parameters",
    "abstract": "Salient object detection models often demand a considerable amount of computation cost to make precise prediction for each pixel, making them hardly applicable on low-power devices. In this paper, we aim to relieve the contradiction between computation cost and model performance by improving the network efficiency to a higher degree. We propose a flexible convolutional module, namely generalized OctConv (gOctConv), to efficiently utilize both in-stage and cross-stages multi-scale features, while reducing the representation redundancy by a novel dynamic weight decay scheme. The effective dynamic weight decay scheme stably boosts the sparsity of parameters during training, supports learnable number of channels for each scale in gOctConv, allowing 80% of parameters reduce with negligible performance drop. Utilizing gOctConv, we build an extremely light-weighted model, namely CSNet, which achieves comparable performance with about 0.2% parameters (100k) of large models on popular salient object detection benchmarks. The source code is publicly available at https://mmcheng.net/sod100k",
    "volume": "main",
    "checked": true,
    "id": "33a6de4a5a10c5662e93da7b017f93bd582c96d4",
    "citation_count": 86
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/69_ECCV_2020_paper.php": {
    "title": "HardGAN: A Haze-Aware Representation Distillation GAN for Single Image Dehazing",
    "abstract": "In this paper, we present a Haze-Aware Representation Distillation Generative Adversarial Network named HardGAN for single-image dehazing. Unlike previous studies that intend to model the transmission map and global atmospheric light jointly to restore a clear image, we solve this regression problem by a multi-scale structure neural network embedded with our proposed Haze-Aware Representation Distillation (HARD) layer. Moreover, we re-introduce to utilize the normalization layer skillfully instead of stacking with the convolution layer directly as before to avoid the useful information wash away, as claimed in many image quality enhancement studies. Extensive experiment on several synthetic benchmark datasets as well as the NTIRE 2020 real-world images show our proposed multi-layer GAN-based network with HARD performs favorably against the state-of-the-art methods in terms of PSNR, SSIM, LPIPS, and human subjective evaluation",
    "volume": "main",
    "checked": true,
    "id": "06fa016072d257f3f09dd0ea9868baadbd856b58",
    "citation_count": 33
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/88_ECCV_2020_paper.php": {
    "title": "Lifespan Age Transformation Synthesis",
    "abstract": "We address the problem of single photo age progression and regression---the prediction of how a person might look in the future, or how they looked in the past. Most existing aging methods are limited to changing the texture, overlooking transformations in head shape that occur during the human aging and growth process. This limits the applicability of previous methods to aging of adults to slightly older adults, and application of those methods to photos of children does not produce quality results. We propose a new multi domain image-to-image generative adversarial network architecture, whose learned latent space accurately models the continuous aging process in both directions. The network is trained on the FFHQ dataset, which we labeled for ages, gender, and semantic segmentation, where fixed age classes are used as anchors to approximate the continuous age transformation. Our framework can predict a full head portrait in ages 0-70 from a single photo, modifying both texture and shape of the head. We demonstrate results on a wide variety of photos and datasets, and show significant improvement over the state of the art",
    "volume": "main",
    "checked": true,
    "id": "08f0c8eb6c9e354a38c59dae561241e4eb7dfc50",
    "citation_count": 44
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/90_ECCV_2020_paper.php": {
    "title": "Domain2Vec: Domain Embedding for Unsupervised Domain Adaptation",
    "abstract": "Conventional unsupervised domain adaptation (UDA) studies the knowledge transfer between a limited number of domains. This neglects the more practical scenario where data are distributed in numerous different domains in the real world. The domain similarity between those domains is critical for domain adaptation performance. To describe and learn relations between different domains, we propose a novel Domain2Vec model to provide vectorial representations of visual domains based on joint learning of feature disentanglement and Gram matrix. To evaluate the effectiveness of our Domain2Vec model, we create two large-scale cross-domain benchmarks. The first one is TinyDA, which contains 54 domains and about one million MNIST-style images. The second benchmark is DomainBank, which is collected from 56 existing vision datasets. We demonstrate that our embedding is capable of predicting domain similarities that match our intuition about visual relations between different domains. Extensive experiments are conducted to demonstrate the power of our new datasets in benchmarking state-of-the-art multi-source domain adaptation methods, as well as the advantage of our proposed model",
    "volume": "main",
    "checked": true,
    "id": "54d35a8e1209af521d0ea0d792558b64c07c9c1f",
    "citation_count": 15
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/106_ECCV_2020_paper.php": {
    "title": "Simulating Content Consistent Vehicle Datasets with Attribute Descent",
    "abstract": "This paper uses a graphic engine to simulate a large amount of training data with free annotations. Between synthetic and real data, there is a two-level domain gap, i.e., content level and appearance level. While the latter has been widely studied, we focus on reducing the content gap in attributes like illumination and viewpoint. To reduce the problem complexity, we choose a smaller and more controllable application, vehicle re-identification (re-ID). We introduce a large-scale synthetic dataset VehicleX. Created in Unity, it contains 1,362 vehicles of various 3D models with fully editable attributes. We propose an attribute descent approach to let VehicleX approximate the attributes in real-world datasets. Specifically, we manipulate each attribute in VehicleX, aiming to minimize the discrepancy between VehicleX and real data in terms of the Fr ́echet Inception Distance (FID). This attribute descent algorithm allows content domain adaptation (DA) orthogonal to existing appearance DA methods. We mix the optimized VehicleX data with real-world vehicle re-ID datasets, and observe consistent improvement. With the augmented datasets, we report competitive accuracy. We make the dataset, engine and our codes available at https://github.com/yorkeyao/VehicleX/",
    "volume": "main",
    "checked": true,
    "id": "4cd07662e23da2bc9592b595d1ba008e621a5173",
    "citation_count": 79
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/116_ECCV_2020_paper.php": {
    "title": "Multiview Detection with Feature Perspective Transformation",
    "abstract": "Incorporating multiple camera views for detection alleviates the impact of occlusions in crowded scenes. In a multiview detection system, we need to answer two important questions. First, how should we aggregate cues from multiple views? Second, how should we aggregate information from spatially neighboring locations? To address these questions, we introduce a novel multiview detector, MVDet. During multiview aggregation, for each location on the ground, existing methods use multiview anchor box features as representation, which potentially limits performance as pre-defined anchor boxes can be inaccurate. In contrast, via feature map perspective transformation, MVDet employs anchor-free representations with feature vectors directly sampled from corresponding pixels in multiple views. For spatial aggregation, different from previous methods that require design and operations outside of neural networks, MVDet takes a fully convolutional approach with large convolutional kernels on the multiview aggregated feature map. The proposed model is end-to-end learnable and achieves 88.2% MODA on Wildtrack dataset, outperforming the state-of-the-art by 14.1%. We also provide detailed analysis of MVDet on a newly introduced synthetic dataset, MultiviewX, which allows us to control the level of occlusion. Code and MultiviewX dataset are available at https://github.com/hou-yz/MVDet/",
    "volume": "main",
    "checked": true,
    "id": "0c9959d0b0cbe260a9998f986ef20090f68d4bda",
    "citation_count": 27
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/121_ECCV_2020_paper.php": {
    "title": "Learning Object Relation Graph and Tentative Policy for Visual Navigation",
    "abstract": "Target-driven visual navigation aims at navigating an agent towards a given target based on the observation of the agent. In this task, it is critical to learn informative visual representation and robust navigation policy. Aiming to improve these two components, this paper proposes three complementary techniques, object relation graph (ORG), trial-driven imitation learning (IL), and a memory-augmented tentative policy network (TPN). ORG improves visual representation learning by integrating object relationships, including category closeness and spatial correlations, mph{e.g.,} a TV usually co-occurs with a remote spatially. Both Trial-driven IL and TPN underlie robust navigation policy, instructing the agent to escape from deadlock states, such as looping or being stuck. Specifically, trial-driven IL is a type of supervision used in policy network training, while TPN, mimicking the IL supervision in unseen environment, is applied in testing. %instructing an agent to escape from deadlock states. Experiment in the artificial environment AI2-Thor validates that each of the techniques is effective. When combined, the techniques bring significantly improvement over baseline methods in navigation effectiveness and efficiency in unseen environments. We report 22.8\\% and 23.5\\% increase in success rate and Success weighted by Path Length (SPL), respectively. The code is available at \\url{https://github.com/xiaobaishu0097/ECCV-VN.git}",
    "volume": "main",
    "checked": true,
    "id": "d3dd97403d8fdb9aa9541ea84db029c1d6bbc9ed",
    "citation_count": 42
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/123_ECCV_2020_paper.php": {
    "title": "Adversarial Self-Supervised Learning for Semi-Supervised 3D Action Recognition",
    "abstract": "We consider the problem of semi-supervised 3D action recognition which has been rarely explored before. Its major challenge lies in how to effectively learn motion representations from unlabeled data. Self-supervised learning (SSL) has been proved very effective at learning representations from unlabeled data in the image domain. However, few effective self-supervised approaches exist for 3D action recognition, and directly applying SSL for semi-supervised learning suffers from misalignment of representations learned from SSL and supervised learning tasks. To address these issues, we present Adversarial Self-Supervised Learning (ASSL), a novel framework that tightly couples SSL and the semi-supervised scheme via neighbor relation exploration and adversarial learning. Specifically, we design an effective SSL scheme to improve the discrimination capability of learned representations for 3D action recognition, through exploring the data relations within a neighborhood. We further propose an adversarial regularization to align the feature distributions of labeled and unlabeled samples. To demonstrate effectiveness of the proposed ASSL in semi-supervised 3D action recognition, we conduct extensive experiments on NTU and N-UCLA datasets. The results confirm its advantageous performance over state-of-the-art semi-supervised methods in the few label regime for 3D action recognition",
    "volume": "main",
    "checked": true,
    "id": "8684daa869e50df9b03d7d0a356f2cbd5b47a7fe",
    "citation_count": 23
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/132_ECCV_2020_paper.php": {
    "title": "Across Scales & Across Dimensions: Temporal Super-Resolution using Deep Internal Learning",
    "abstract": "When a very fast dynamic event is recorded with a low-framerate camera, the resulting video suffers from severe motion blur (due to exposure time) and motion aliasing (due to low sampling rate in time). True Temporal Super-Resolution (TSR) is more than just Temporal-Interpolation (increasing framerate). It can also recover new high temporal frequencies beyond the temporal Nyquist limit of the input video, thus resolving both motion-blur and motion aliasing – effects that temporal frame interpolation (as sophisticated as it may be) cannot undo. In this paper we propose a “Deep Internal Learning” approach for true TSR. We train a video-specific FCN on examples extracted directly from the low-framerate input video. Our method exploits the strong recurrence of small space-time patches inside a single video sequence, both within and across different spatio-temporal scales of the video.We further observe (for the first time) that small space-time patches recur also across-dimensions of the video sequence – i.e., by swapping the spatial and temporal dimensions. In particular, the higher spatial resolution of video frames provides strong examples as to how to increase the temporal resolution of that video. Such internal video-specific examples give rise to strong self-supervision, requiring no data but the input video itself. This results in Zero-Shot Temporal-SR of complex videos, which removes both motion blur and motion aliasing, outperforming previous supervised methods trained on external video datasets",
    "volume": "main",
    "checked": false,
    "id": "02772404c8c6e1903a00798ce01492bc6820f665",
    "citation_count": 19
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/138_ECCV_2020_paper.php": {
    "title": "Inducing Optimal Attribute Representations for Conditional GANs",
    "abstract": "Conditional GANs (cGANs) are widely used in translating an image from one category to another. Meaningful conditions on GANsprovide greater flexibility and control over the nature of the target domain synthetic data. Existing conditional GANs commonly encode target domain label information as hard-coded categorical vectors in the form of0s and 1s. The major drawbacks of such representations are inability to encode the high-order semantic information of target categories and their relative dependencies. We propose a novel end-to-end learning framework based on Graph Convolutional Networks to learn the attribute representations to condition the generator. The GAN losses, the discriminator and attribute classification loss, are fed back to the graph resulting in the synthetic images that are more natural and clearer with respect to the attributes generation. Moreover, prior-arts are mostly given priorities to condition on the generator side, not on the discriminator side of GANs.We apply the conditions on the discriminator side as well via multi-task learning. We enhanced four state-of-the-art cGANs architectures: Stargan, Stargan-JNT, AttGAN and STGAN. Our extensive qualitative and quantitative evaluations on challenging face attributes manipulation data set,CelebA, LFWA, and RaFD, show that the cGANs enhanced by our meth-ods outperform by a large margin, compared to their counter-parts and other conditioning methods, in terms of both target attributes recognition rates and quality measures such as PSNR and SSIM",
    "volume": "main",
    "checked": true,
    "id": "5c681ce7cd84a21d68b92ed30aed3ca11641c175",
    "citation_count": 12
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/152_ECCV_2020_paper.php": {
    "title": "AR-Net: Adaptive Frame Resolution for Efficient Action Recognition",
    "abstract": "Action recognition is an open and challenging problem in computer vision. While current state-of-the-art models offer excellent recognition results, their computational expense limits their impact for many real-world applications. In this paper, we propose a novel approach, called AR-Net (Adaptive Resolution Network), that selects on-the-fly the optimal resolution for each frame conditioned on the input for efficient action recognition in long untrimmed videos. Specifically, given a video frame, a policy network is used to decide what input resolution should be used for processing by the action recognition model, with the goal of improving both accuracy and efficiency. We efficiently train the policy network jointly with the recognition model using standard back-propagation. Extensive experiments on several challenging action recognition benchmark datasets well demonstrate the efficacy of our proposed approach over state-of-the-art methods. The project page can be found at https://mengyuest.github.io/AR-Net",
    "volume": "main",
    "checked": true,
    "id": "11bf57d8a652de8e2ea436ff6a2707c95fa5197a",
    "citation_count": 76
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/156_ECCV_2020_paper.php": {
    "title": "Image-to-Voxel Model Translation for 3D Scene Reconstruction and Segmentation",
    "abstract": "Objects class, depth, and shape are instantly reconstructed by a human looking at a 2D image. While modern deep models solve each of these challenging tasks separately, they struggle to perform simultaneous scene 3D reconstruction and segmentation. We propose a single shot image-to-semantic voxel model translation framework. We train a generator adversarially against a discriminator that verifies the object's poses. Furthermore, trapezium-shaped voxels, volumetric residual blocks, and 2D-to-3D skip connections facilitate our model learning explicit reasoning about 3D scene structure. We collected a SemanticVoxels dataset with 116k images, ground-truth semantic voxel models, depth maps, and 6D object poses. Experiments on ShapeNet and our SemanticVoxels datasets demonstrate that our framework achieves and surpasses state-of-the-art in the reconstruction of scenes with multiple non-rigid objects of different classes. We made our model and dataset publicly available",
    "volume": "main",
    "checked": true,
    "id": "d4f3b8e374251f372af3e5386660a2522d879d1c",
    "citation_count": 11
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/157_ECCV_2020_paper.php": {
    "title": "Consistency Guided Scene Flow Estimation",
    "abstract": "Consistency Guided Scene Flow Estimation (CGSF) is a self-supervised framework for the joint reconstruction of 3D scene structure and motion from stereo video. The model takes two temporal stereo pairs as input, and predicts disparity and scene flow. The model self-adapts at test time by iteratively refining its predictions. The refinement process is guided by a consistency loss, which combines stereo and temporal photo-consistency with a geometric term that couples disparity and 3D motion. To handle inherent modeling error in the consistency loss (e.g. Lambertian assumptions) and for better generalization, we further introduce a learned, output refinement network, which takes the initial predictions, the loss, and the gradient as input, and efficiently predicts a correlated output update. In multiple experiments, including ablation studies, we show that the proposed model can reliably predict disparity and scene flow in challenging imagery, achieves better generalization than the state-of-the-art, and adapts quickly and robustly to unseen domains",
    "volume": "main",
    "checked": true,
    "id": "0c5d637b85ee9fa53352fc5bc9f9417ab138c590",
    "citation_count": 3
  },
  "https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/160_ECCV_2020_paper.php": {
    "title": "Autoregressive Unsupervised Image Segmentation",
    "abstract": "In this work, we propose a new unsupervised image segmentation approach based on mutual information maximization between different constructed views of the inputs. Taking inspiration from autoregressive generative models, that predict the current pixel from past pixels in a raster-scan ordering created with masked convolutions, we propose different autoregressive orderings over the inputs using various forms of masked convolutions to construct different views of the data. For a given input, the model produces a pair of predictions with two valid orderings, and is then trained to maximize the mutual information between the two outputs. These outputs can either be low-dimensional features for representation learning or output clusters corresponding to semantic labels for clustering. While masked convolutions are used during training, in inference, no masking is applied and we fall back to the standard convolution where the model has access to the full input. The proposed method outperforms the current state-of-the-art on unsupervised image segmentation. It is simple and easy to implement, and can be extended to other visual tasks and integrated seamlessly into existing unsupervised learning methods requiring different views of the data",
    "volume": "main",
    "checked": true,
    "id": "b9532b3593ce12155d1e751ce70ffb912001716a",
    "citation_count": 39
  }
}