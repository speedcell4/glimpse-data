{
  "https://openreview.net/forum?id=FPCMqjI0jXN": {
    "title": "Domino: Discovering Systematic Errors with Cross-Modal Embeddings",
    "volume": "oral",
    "abstract": "Machine learning models that achieve high overall accuracy often make systematic errors on important subsets (or slices) of data. Identifying underperforming slices is particularly challenging when working with high-dimensional inputs (e.g. images, audio), where important slices are often unlabeled. In order to address this issue, recent studies have proposed automated slice discovery methods (SDMs), which leverage learned model representations to mine input data for slices on which a model performs poorly. To be useful to a practitioner, these methods must identify slices that are both underperforming and coherent (i.e. united by a human-understandable concept). However, no quantitative evaluation framework currently exists for rigorously assessing SDMs with respect to these criteria. Additionally, prior qualitative evaluations have shown that SDMs often identify slices that are incoherent. In this work, we address these challenges by first designing a principled evaluation framework that enables a quantitative comparison of SDMs across 1,235 slice discovery settings in three input domains (natural images, medical images, and time-series data). Then, motivated by the recent development of powerful cross-modal representation learning approaches, we present Domino, an SDM that leverages cross-modal embeddings and a novel error-aware mixture model to discover and describe coherent slices. We find that Domino accurately identifies 36% of the 1,235 slices in our framework -- a 12 percentage point improvement over prior methods. Further, Domino is the first SDM that can provide natural language descriptions of identified slices, correctly generating the exact name of the slice in 35% of settings",
    "checked": true,
    "id": "0567131ec1f839240179927ceb61f51bdd173055",
    "semantic_title": "domino: discovering systematic errors with cross-modal embeddings",
    "citation_count": 157,
    "authors": []
  },
  "https://openreview.net/forum?id=NudBMY-tzDr": {
    "title": "Natural Language Descriptions of Deep Visual Features",
    "volume": "oral",
    "abstract": "Some neurons in deep networks specialize in recognizing highly specific perceptual, structural, or semantic features of inputs. In computer vision, techniques exist for identifying neurons that respond to individual concept categories like colors, textures, and object classes. But these techniques are limited in scope, labeling only a small subset of neurons and behaviors in any network. Is a richer characterization of neuron-level computation possible? We introduce a procedure (called MILAN, for mutual information-guided linguistic annotation of neurons) that automatically labels neurons with open-ended, compositional, natural language descriptions. Given a neuron, MILAN generates a description by searching for a natural language string that maximizes pointwise mutual information with the image regions in which the neuron is active. MILAN produces fine-grained descriptions that capture categorical, relational, and logical structure in learned features. These descriptions obtain high agreement with human-generated feature descriptions across a diverse set of model architectures and tasks, and can aid in understanding and controlling learned models. We highlight three applications of natural language neuron descriptions. First, we use MILAN for analysis, characterizing the distribution and importance of neurons selective for attribute, category, and relational information in vision models. Second, we use MILAN for auditing, surfacing neurons sensitive to human faces in datasets designed to obscure them. Finally, we use MILAN for editing, improving robustness in an image classifier by deleting neurons sensitive to text features spuriously correlated with class labels",
    "checked": true,
    "id": "2e7be2911be7c6c8b8016aa30953d479649f1ffe",
    "semantic_title": "natural language descriptions of deep visual features",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=tYRrOdSnVUy": {
    "title": "Non-Transferable Learning: A New Approach for Model Ownership Verification and Applicability Authorization",
    "volume": "oral",
    "abstract": "As Artificial Intelligence as a Service gains popularity, protecting well-trained models as intellectual property is becoming increasingly important. There are two common types of protection methods: ownership verification and usage authorization. In this paper, we propose Non-Transferable Learning (NTL), a novel approach that captures the exclusive data representation in the learned model and restricts the model generalization ability to certain domains. This approach provides effective solutions to both model verification and authorization. Specifically: 1) For ownership verification, watermarking techniques are commonly used but are often vulnerable to sophisticated watermark removal methods. By comparison, our NTL-based ownership verification provides robust resistance to state-of-the-art watermark removal methods, as shown in extensive experiments with 6 removal approaches over the digits, CIFAR10 & STL10, and VisDA datasets. 2) For usage authorization, prior solutions focus on authorizing specific users to access the model, but authorized users can still apply the model to any data without restriction. Our NTL-based authorization approach instead provides data-centric protection, which we call applicability authorization, by significantly degrading the performance of the model on unauthorized data. Its effectiveness is also shown through experiments on aforementioned datasets",
    "checked": true,
    "id": "a343ed52ae6075168272f0c70ee989ef81c46a83",
    "semantic_title": "non-transferable learning: a new approach for model ownership verification and applicability authorization",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=YWNAX0caEjI": {
    "title": "Neural Structured Prediction for Inductive Node Classification",
    "volume": "oral",
    "abstract": "This paper studies node classification in the inductive setting, i.e., aiming to learn a model on labeled training graphs and generalize it to infer node labels on unlabeled test graphs. This problem has been extensively studied with graph neural networks (GNNs) by learning effective node representations, as well as traditional structured prediction methods for modeling the structured output of node labels, e.g., conditional random fields (CRFs). In this paper, we present a new approach called the Structured Proxy Network (SPN), which combines the advantages of both worlds. SPN defines flexible potential functions of CRFs with GNNs. However, learning such a model is nontrivial as it involves optimizing a maximin game with high-cost inference. Inspired by the underlying connection between joint and marginal distributions defined by Markov networks, we propose to solve an approximate version of the optimization problem as a proxy, which yields a near-optimal solution, making learning more efficient. Extensive experiments on two settings show that our approach outperforms many competitive baselines",
    "checked": true,
    "id": "8ccf555540a8593ed6c0760586c91b7624346d02",
    "semantic_title": "neural structured prediction for inductive node classification",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=uxgg9o7bI_3": {
    "title": "A New Perspective on \"How Graph Neural Networks Go Beyond Weisfeiler-Lehman?",
    "volume": "oral",
    "abstract": "We propose a new perspective on designing powerful Graph Neural Networks (GNNs). In a nutshell, this enables a general solution to inject structural properties of graphs into a message-passing aggregation scheme of GNNs. As a theoretical basis, we develop a new hierarchy of local isomorphism on neighborhood subgraphs. Then, we theoretically characterize how message-passing GNNs can be designed to be more expressive than the Weisfeiler Lehman test. To elaborate this characterization, we propose a novel neural model, called GraphSNN, and prove that this model is strictly more expressive than the Weisfeiler Lehman test in distinguishing graph structures. We empirically verify the strength of our model on different graph learning tasks. It is shown that our model consistently improves the state-of-the-art methods on the benchmark tasks without sacrificing computational simplicity and efficiency",
    "checked": true,
    "id": "6a0cbf943183a6751ff438c16ad75434b4cf47da",
    "semantic_title": "a new perspective on \"how graph neural networks go beyond weisfeiler-lehman?",
    "citation_count": 101,
    "authors": []
  },
  "https://openreview.net/forum?id=LdlwbBP2mlq": {
    "title": "Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond",
    "volume": "oral",
    "abstract": "In distributed learning, local SGD (also known as federated averaging) and its simple baseline minibatch SGD are widely studied optimization methods. Most existing analyses of these methods assume independent and unbiased gradient estimates obtained via with-replacement sampling. In contrast, we study shuffling-based variants: minibatch and local Random Reshuffling, which draw stochastic gradients without replacement and are thus closer to practice. For smooth functions satisfying the Polyak-≈Åojasiewicz condition, we obtain convergence bounds (in the large epoch regime) which show that these shuffling-based variants converge faster than their with-replacement counterparts. Moreover, we prove matching lower bounds showing that our convergence analysis is tight. Finally, we propose an algorithmic modification called synchronized shuffling that leads to convergence rates faster than our lower bounds in near-homogeneous settings",
    "checked": true,
    "id": "e9e5e10d770012207903c2cfb5fe59320c2fcbd9",
    "semantic_title": "minibatch vs local sgd with shuffling: tight convergence bounds and beyond",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z7Lk2cQEG8a": {
    "title": "The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks: an Exact Characterization of Optimal Solutions",
    "volume": "oral",
    "abstract": "We prove that finding all globally optimal two-layer ReLU neural networks can be performed by solving a convex optimization program with cone constraints. Our analysis is novel, characterizes all optimal solutions, and does not leverage duality-based analysis which was recently used to lift neural network training into convex spaces. Given the set of solutions of our convex optimization program, we show how to construct exactly the entire set of optimal neural networks. We provide a detailed characterization of this optimal set and its invariant transformations. As additional consequences of our convex perspective, (i) we establish that Clarke stationary points found by stochastic gradient descent correspond to the global optimum of a subsampled convex problem (ii) we provide a polynomial-time algorithm for checking if a neural network is a global minimum of the training loss (iii) we provide an explicit construction of a continuous path between any neural network and the global minimum of its sublevel set and (iv) characterize the minimal size of the hidden layer so that the neural network optimization landscape has no spurious valleys. Overall, we provide a rich framework for studying the landscape of neural network training loss through convexity",
    "checked": true,
    "id": "aa607c6535e3e1953e5bd48f36efc432a0c167f8",
    "semantic_title": "the hidden convex optimization landscape of regularized two-layer relu networks: an exact characterization of optimal solutions",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=RQLLzMCefQu": {
    "title": "Provably Filtering Exogenous Distractors using Multistep Inverse Dynamics",
    "volume": "oral",
    "abstract": "Many real-world applications of reinforcement learning (RL) require the agent to deal with high-dimensional observations such as those generated from a megapixel camera. Prior work has addressed such problems with representation learning, through which the agent can provably extract endogenous, latent state information from raw observations and subsequently plan efficiently. However, such approaches can fail in the presence of temporally correlated noise in the observations, a phenomenon that is common in practice. We initiate the formal study of latent state discovery in the presence of such exogenous noise sources by proposing a new model, the Exogenous Block MDP (EX-BMDP), for rich observation RL. We start by establishing several negative results, by highlighting failure cases of prior representation learning based approaches. Then, we introduce the Predictive Path Elimination (PPE) algorithm, that learns a generalization of inverse dynamics and is provably sample and computationally efficient in EX-BMDPs when the endogenous state dynamics are near deterministic. The sample complexity of PPE depends polynomially on the size of the latent endogenous state space while not directly depending on the size of the observation space, nor the exogenous state space. We provide experiments on challenging exploration problems which show that our approach works empirically",
    "checked": true,
    "id": "b8879d5beb8242c7a08d8d86ce3e6216c99e5f88",
    "semantic_title": "provably filtering exogenous distractors using multistep inverse dynamics",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=b-ny3x071E5": {
    "title": "Bootstrapped Meta-Learning",
    "volume": "oral",
    "abstract": "Meta-learning empowers artificial intelligence to increase its efficiency by learning how to learn. Unlocking this potential involves overcoming a challenging meta-optimisation problem. We propose an algorithm that tackles this problem by letting the meta-learner teach itself. The algorithm first bootstraps a target from the meta-learner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. Focusing on meta-learning with gradients, we establish conditions that guarantee performance improvements and show that metric can be used to control meta-optimisation. Meanwhile, the bootstrapping mechanism can extend the effective meta-learning horizon without requiring backpropagation through all updates. We achieve a new state-of-the art for model-free agents on the Atari ALE benchmark and demonstrate that it yields both performance and efficiency gains in multi-task meta-learning. Finally, we explore how bootstrapping opens up new possibilities and find that it can meta-learn efficient exploration in an epsilon-greedy Q-learning agent - without backpropagating through the update rule",
    "checked": true,
    "id": "f8befa0bc3442979ff19e070f7c6b16d66a776c5",
    "semantic_title": "bootstrapped meta-learning",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=XzTtHjgPDsT": {
    "title": "Coordination Among Neural Modules Through a Shared Global Workspace",
    "volume": "oral",
    "abstract": "Deep learning has seen a movement away from representing examples with a monolithic hidden state towards a richly structured state. For example, Transformers segment by position, and object-centric architectures decompose images into entities. In all these architectures, interactions between different elements are modeled via pairwise interactions: Transformers make use of self-attention to incorporate information from other positions and object-centric architectures make use of graph neural networks to model interactions among entities. We consider how to improve on pairwise interactions in terms of global coordination and a coherent, integrated representation that can be used for downstream tasks. In cognitive science, a global workspace architecture has been proposed in which functionally specialized components share information through a common, bandwidth-limited communication channel. We explore the use of such a communication channel in the context of deep learning for modeling the structure of complex environments. The proposed method includes a shared workspace through which communication among different specialist modules takes place but due to limits on the communication bandwidth, specialist modules must compete for access. We show that capacity limitations have a rational basis in that (1) they encourage specialization and compositionality and (2) they facilitate the synchronization of otherwise independent specialists",
    "checked": true,
    "id": "78ea232dbabc67ca4d6d4a7c1bbf568e9b47cb8a",
    "semantic_title": "coordination among neural modules through a shared global workspace",
    "citation_count": 103,
    "authors": []
  },
  "https://openreview.net/forum?id=l4IHywGq6a": {
    "title": "Data-Efficient Graph Grammar Learning for Molecular Generation",
    "volume": "oral",
    "abstract": "The problem of molecular generation has received significant attention recently. Existing methods are typically based on deep neural networks and require training on large datasets with tens of thousands of samples. In practice, however, the size of class-specific chemical datasets is usually limited (e.g., dozens of samples) due to labor-intensive experimentation and data collection. Another major challenge is to generate only physically synthesizable molecules. This is a non-trivial task for neural network-based generative models since the relevant chemical knowledge can only be extracted and generalized from the limited training data. In this work, we propose a data-efficient generative model that can be learned from datasets with orders of magnitude smaller sizes than common benchmarks. At the heart of this method is a learnable graph grammar that generates molecules from a sequence of production rules. Without any human assistance, these production rules are automatically constructed from training data. Furthermore, additional chemical knowledge can be incorporated into the model by further grammar optimization. Our learned graph grammar yields state-of-the-art results on generating high-quality molecules for three monomer datasets that contain only ${\\sim}20$ samples each. Our approach also achieves remarkable performance in a challenging polymer generation task with $only$ $117$ training samples and is competitive against existing methods using $81$k data points",
    "checked": true,
    "id": "55703c37b77eac03a4dceb991cd03281f826d98e",
    "semantic_title": "data-efficient graph grammar learning for molecular generation",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=iC4UHbQ01Mp": {
    "title": "Poisoning and Backdooring Contrastive Learning",
    "volume": "oral",
    "abstract": "Multimodal contrastive learning methods like CLIP train on noisy and uncurated training datasets. This is cheaper than labeling datasets manually, and even improves out-of-distribution robustness. We show that this practice makes backdoor and poisoning attacks a significant threat. By poisoning just 0.01% of a dataset (e.g., just 300 images of the 3 million-example Conceptual Captions dataset), we can cause the model to misclassify test images by overlaying a small patch. Targeted poisoning attacks, whereby the model misclassifies a particular test input with an adversarially-desired label, are even easier requiring control of 0.0001% of the dataset (e.g., just three out of the 3 million images). Our attacks call into question whether training on noisy and uncurated Internet scrapes is desirable",
    "checked": true,
    "id": "6c50e8db8d44a3399a78adb8fab2d7f81a029c33",
    "semantic_title": "poisoning and backdooring contrastive learning",
    "citation_count": 179,
    "authors": []
  },
  "https://openreview.net/forum?id=w1UbdvWH_R3": {
    "title": "Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path",
    "volume": "oral",
    "abstract": "The recently discovered Neural Collapse (NC) phenomenon occurs pervasively in today's deep net training paradigm of driving cross-entropy (CE) loss towards zero. During NC, last-layer features collapse to their class-means, both classifiers and class-means collapse to the same Simplex Equiangular Tight Frame, and classifier behavior collapses to the nearest-class-mean decision rule. Recent works demonstrated that deep nets trained with mean squared error (MSE) loss perform comparably to those trained with CE. As a preliminary, we empirically establish that NC emerges in such MSE-trained deep nets as well through experiments on three canonical networks and five benchmark datasets. We provide, in a Google Colab notebook, PyTorch code for reproducing MSE-NC and CE-NC: https://colab.research.google.com/github/neuralcollapse/neuralcollapse/blob/main/neuralcollapse.ipynb. The analytically-tractable MSE loss offers more mathematical opportunities than the hard-to-analyze CE loss, inspiring us to leverage MSE loss towards the theoretical investigation of NC. We develop three main contributions: (I) We show a new decomposition of the MSE loss into (A) terms directly interpretable through the lens of NC and which assume the last-layer classifier is exactly the least-squares classifier; and (B) a term capturing the deviation from this least-squares classifier. (II) We exhibit experiments on canonical datasets and networks demonstrating that term-(B) is negligible during training. This motivates us to introduce a new theoretical construct: the central path, where the linear classifier stays MSE-optimal for feature activations throughout the dynamics. (III) By studying renormalized gradient flow along the central path, we derive exact dynamics that predict NC",
    "checked": true,
    "id": "79eff96fd96b688ff5368f7edb506598bc53d354",
    "semantic_title": "neural collapse under mse loss: proximity to and dynamics on the central path",
    "citation_count": 153,
    "authors": []
  },
  "https://openreview.net/forum?id=ltM1RMZntpu": {
    "title": "Weighted Training for Cross-Task Learning",
    "volume": "oral",
    "abstract": "In this paper, we introduce Target-Aware Weighted Training (TAWT), a weighted training algorithm for cross-task learning based on minimizing a representation-based task distance between the source and target tasks. We show that TAWT is easy to implement, is computationally efficient, requires little hyperparameter tuning, and enjoys non-asymptotic learning-theoretic guarantees. The effectiveness of TAWT is corroborated through extensive experiments with BERT on four sequence tagging tasks in natural language processing (NLP), including part-of-speech (PoS) tagging, chunking, predicate detection, and named entity recognition (NER). As a byproduct, the proposed representation-based task distance allows one to reason in a theoretically principled way about several critical aspects of cross-task learning, such as the choice of the source data and the impact of fine-tuning",
    "checked": true,
    "id": "0992638c434257f4b9691637dd5ada046eb14b17",
    "semantic_title": "weighted training for cross-task learning",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=wRODLDHaAiW": {
    "title": "iLQR-VAE : control-based learning of input-driven dynamics with applications to neural data",
    "volume": "oral",
    "abstract": "Understanding how neural dynamics give rise to behaviour is one of the most fundamental questions in systems neuroscience. To achieve this, a common approach is to record neural populations in behaving animals, and model these data as emanating from a latent dynamical system whose state trajectories can then be related back to behavioural observations via some form of decoding. As recordings are typically performed in localized circuits that form only a part of the wider implicated network, it is important to simultaneously learn the local dynamics and infer any unobserved external input that might drive them. Here, we introduce iLQR-VAE, a novel control-based approach to variational inference in nonlinear dynamical systems, capable of learning both latent dynamics, initial conditions, and ongoing external inputs. As in recent deep learning approaches, our method is based on an input-driven sequential variational autoencoder (VAE). The main novelty lies in the use of the powerful iterative linear quadratic regulator algorithm (iLQR) in the recognition model. Optimization of the standard evidence lower-bound requires differentiating through iLQR solutions, which is made possible by recent advances in differentiable control. Importantly, having the recognition model be implicitly defined by the generative model greatly reduces the number of free parameters and allows for flexible, high-quality inference. This makes it possible for instance to evaluate the model on a single long trial after training on smaller chunks. We demonstrate the effectiveness of iLQR-VAE on a range of synthetic systems, with autonomous as well as input-driven dynamics. We further apply it to neural and behavioural recordings in non-human primates performing two different reaching tasks, and show that iLQR-VAE yields high-quality kinematic reconstructions from the neural data",
    "checked": true,
    "id": "6aac809cc3c9c39b3c156538494fb5742e0549cf",
    "semantic_title": "ilqr-vae : control-based learning of input-driven dynamics with applications to neural data",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=z7p2V6KROOV": {
    "title": "Extending the WILDS Benchmark for Unsupervised Adaptation",
    "volume": "oral",
    "abstract": "Machine learning systems deployed in the wild are often trained on a source distribution but deployed on a different target distribution. Unlabeled data can be a powerful point of leverage for mitigating these distribution shifts, as it is frequently much more available than labeled data and can often be obtained from distributions beyond the source distribution as well. However, existing distribution shift benchmarks with unlabeled data do not reflect the breadth of scenarios that arise in real-world applications. In this work, we present the WILDS 2.0 update, which extends 8 of the 10 datasets in the WILDS benchmark of distribution shifts to include curated unlabeled data that would be realistically obtainable in deployment. These datasets span a wide range of applications (from histology to wildlife conservation), tasks (classification, regression, and detection), and modalities (photos, satellite images, microscope slides, text, molecular graphs). The update maintains consistency with the original WILDS benchmark by using identical labeled training, validation, and test sets, as well as identical evaluation metrics. We systematically benchmark state-of-the-art methods that use unlabeled data, including domain-invariant, self-training, and self-supervised methods, and show that their success on WILDS is limited. To facilitate method development, we provide an open-source package that automates data loading and contains the model architectures and methods used in this paper. Code and leaderboards are available at https://wilds.stanford.edu",
    "checked": true,
    "id": "ab2a8ca21309859ed027928dc38e6915be0e6776",
    "semantic_title": "extending the wilds benchmark for unsupervised adaptation",
    "citation_count": 108,
    "authors": []
  },
  "https://openreview.net/forum?id=avgclFZ221l": {
    "title": "Asymmetry Learning for Counterfactually-invariant Classification in OOD Tasks",
    "volume": "oral",
    "abstract": "Generalizing from observed to new related environments (out-of-distribution) is central to the reliability of classifiers. However, most classifiers fail to predict label $Y$ from input $X$ when the change in environment is due a (stochastic) input transformation $T^\\text{te} \\circ X'$ not observed in training, as in training we observe $T^\\text{tr} \\circ X'$, where $X'$ is a hidden variable. This work argues that when the transformations in train $T^\\text{tr}$ and test $T^\\text{te}$ are (arbitrary) symmetry transformations induced by a collection of known $m$ equivalence relations, the task of finding a robust OOD classifier can be defined as finding the simplest causal model that defines a causal connection between the target labels and the symmetry transformations that are associated with label changes. We then propose a new learning paradigm, asymmetry learning, that identifies which symmetries the classifier must break in order to correctly predict $Y$ in both train and test. Asymmetry learning performs a causal model search that, under certain identifiability conditions, finds classifiers that perform equally well in-distribution and out-of-distribution. Finally, we show how to learn counterfactually-invariant representations with asymmetry learning in two physics tasks",
    "checked": true,
    "id": "61d0b903c290047ae67f30092e1dd95b39210a13",
    "semantic_title": "asymmetry learning for counterfactually-invariant classification in ood tasks",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=KB5onONJIAU": {
    "title": "Comparing Distributions by Measuring Differences that Affect Decision Making",
    "volume": "oral",
    "abstract": "Measuring the discrepancy between two probability distributions is a fundamental problem in machine learning and statistics. We propose a new class of discrepancies based on the optimal loss for a decision task -- two distributions are different if the optimal decision loss is higher on their mixture than on each individual distribution. By suitably choosing the decision task, this generalizes the Jensen-Shannon divergence and the maximum mean discrepancy family. We apply our approach to two-sample tests, and on various benchmarks, we achieve superior test power compared to competing methods. In addition, a modeler can directly specify their preferences when comparing distributions through the decision loss. We apply this property to understanding the effects of climate change on different social and economic activities, evaluating sample quality, and selecting features targeting different decision tasks",
    "checked": true,
    "id": "1e07dd0c5958dc36ecfb2a1314cdffa0b06c8b3d",
    "semantic_title": "comparing distributions by measuring differences that affect decision making",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=UseMOjWENv": {
    "title": "MIDI-DDSP: Detailed Control of Musical Performance via Hierarchical Modeling",
    "volume": "oral",
    "abstract": "Musical expression requires control of both what notes that are played, and how they are performed. Conventional audio synthesizers provide detailed expressive controls, but at the cost of realism. Black-box neural audio synthesis and concatenative samplers can produce realistic audio, but have few mechanisms for control. In this work, we introduce MIDI-DDSP a hierarchical model of musical instruments that enables both realistic neural audio synthesis and detailed user control. Starting from interpretable Differentiable Digital Signal Processing (DDSP) synthesis parameters, we infer musical notes and high-level properties of their expressive performance (such as timbre, vibrato, dynamics, and articulation). This creates a 3-level hierarchy (notes, performance, synthesis) that affords individuals the option to intervene at each level, or utilize trained priors (performance given notes, synthesis given performance) for creative assistance. Through quantitative experiments and listening tests, we demonstrate that this hierarchy can reconstruct high-fidelity audio, accurately predict performance attributes for a note sequence, independently manipulate the attributes of a given performance, and as a complete system, generate realistic audio from a novel note sequence. By utilizing an interpretable hierarchy, with multiple levels of granularity, MIDI-DDSP opens the door to assistive tools to empower individuals across a diverse range of musical experience",
    "checked": true,
    "id": "95a35473fd1936927dd4a53fe0a5d2d6762d99b3",
    "semantic_title": "midi-ddsp: detailed control of musical performance via hierarchical modeling",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=N0n_QyQ5lBF": {
    "title": "Unsupervised Vision-Language Grammar Induction with Shared Structure Modeling",
    "volume": "oral",
    "abstract": "We introduce a new task, unsupervised vision-language (VL) grammar induction. Given an image-caption pair, the goal is to extract a shared hierarchical structure for both image and language simultaneously. We argue that such structured output, grounded in both modalities, is a clear step towards the high-level understanding of multimodal information. Besides challenges existing in conventional visually grounded grammar induction tasks, VL grammar induction requires a model to capture contextual semantics and perform a fine-grained alignment. To address these challenges, we propose a novel method, CLIORA, which constructs a shared vision-language constituency tree structure with context-dependent semantics for all possible phrases in different levels of the tree. It computes a matching score between each constituent and image region, trained via contrastive learning. It integrates two levels of fusion, namely at feature-level and at score-level, so as to allow fine-grained alignment. We introduce a new evaluation metric for VL grammar induction, CCRA, and show a 3.3% improvement over a strong baseline on Flickr30k Entities. We also evaluate our model via two derived tasks, i.e., language grammar induction and phrase grounding, and improve over the state-of-the-art for both",
    "checked": true,
    "id": "89a5b94a0b5c8dbb5633ae21ff9f956372f7a53b",
    "semantic_title": "unsupervised vision-language grammar induction with shared structure modeling",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=EhYjZy6e1gJ": {
    "title": "PiCO: Contrastive Label Disambiguation for Partial Label Learning",
    "volume": "oral",
    "abstract": "Partial label learning (PLL) is an important problem that allows each training example to be labeled with a coarse candidate set, which well suits many real-world data annotation scenarios with label ambiguity. Despite the promise, the performance of PLL often lags behind the supervised counterpart. In this work, we bridge the gap by addressing two key research challenges in PLL---representation learning and label disambiguation---in one coherent framework. Specifically, our proposed framework PiCO consists of a contrastive learning module along with a novel class prototype-based label disambiguation algorithm. PiCO produces closely aligned representations for examples from the same classes and facilitates label disambiguation. Theoretically, we show that these two components are mutually beneficial, and can be rigorously justified from an expectation-maximization (EM) algorithm perspective. Extensive experiments demonstrate that PiCO significantly outperforms the current state-of-the-art approaches in PLL and even achieves comparable results to fully supervised learning. Code and data available: https://github.com/hbzju/PiCO",
    "checked": true,
    "id": "ed8790e896260a40285172ec93190fb7ec67cae2",
    "semantic_title": "pico: contrastive label disambiguation for partial label learning",
    "citation_count": 150,
    "authors": []
  },
  "https://openreview.net/forum?id=0EXmFzUn5I": {
    "title": "Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting",
    "volume": "oral",
    "abstract": "Accurate prediction of the future given the past based on time series data is of paramount importance, since it opens the door for decision making and risk management ahead of time. In practice, the challenge is to build a flexible but parsimonious model that can capture a wide range of temporal dependencies. In this paper, we propose Pyraformer by exploring the multiresolution representation of the time series. Specifically, we introduce the pyramidal attention module (PAM) in which the inter-scale tree structure summarizes features at different resolutions and the intra-scale neighboring connections model the temporal dependencies of different ranges. Under mild conditions, the maximum length of the signal traversing path in Pyraformer is a constant (i.e., $\\mathcal O(1)$) with regard to the sequence length $L$, while its time and space complexity scale linearly with $L$. Extensive numerical results show that Pyraformer typically achieves the highest prediction accuracy in both single-step and long-range forecasting tasks with the least amount of time and memory consumption, especially when the sequence is long",
    "checked": true,
    "id": "30dcc0e191a376fea0e7a46f94c53872c029efc9",
    "semantic_title": "pyraformer: low-complexity pyramidal attention for long-range time series modeling and forecasting",
    "citation_count": 671,
    "authors": []
  },
  "https://openreview.net/forum?id=wIzUeM3TAU": {
    "title": "Expressiveness and Approximation Properties of Graph Neural Networks",
    "volume": "oral",
    "abstract": "Characterizing the separation power of graph neural networks (GNNs) provides an understanding of their limitations for graph learning tasks. Results regarding separation power are, however, usually geared at specific GNNs architectures, and tools for understanding arbitrary GNN architectures are generally lacking. We provide an elegant way to easily obtain bounds on the separation power of GNNs in terms of the Weisfeiler-Leman (WL) tests, which have become the yardstick to measure the separation power of GNNs. The crux is to view GNNs as expressions in a procedural tensor language describing the computations in the layers of the GNNs. Then, by a simple analysis of the obtained expressions, in terms of the number of indexes used and the nesting depth of summations, bounds on the separation power in terms of the WL-tests readily follow. We use tensor language to define Higher-Order Message-Passing Neural Networks (or k-MPNNs), a natural extension of MPNNs. Furthermore, the tensor language point of view allows for the derivation of universality results for classes of GNNs in a natural way. Our approach provides a toolbox with which GNN architecture designers can analyze the separation power of their GNNs, without needing to know the intricacies of the WL-tests. We also provide insights in what is needed to boost the separation power of GNNs",
    "checked": true,
    "id": "307c18712c6fb9617a450cfb0c2d540d81a9bea9",
    "semantic_title": "expressiveness and approximation properties of graph neural networks",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=1L0C5ROtFp": {
    "title": "Filtered-CoPhy: Unsupervised Learning of Counterfactual Physics in Pixel Space",
    "volume": "oral",
    "abstract": "Learning causal relationships in high-dimensional data (images, videos) is a hard task, as they are often defined on low dimensional manifolds and must be extracted from complex signals dominated by appearance, lighting, textures and also spurious correlations in the data. We present a method for learning counterfactual reasoning of physical processes in pixel space, which requires the prediction of the impact of interventions on initial conditions. Going beyond the identification of structural relationships, we deal with the challenging problem of forecasting raw video over long horizons. Our method does not require the knowledge or supervision of any ground truth positions or other object or scene properties. Our model learns and acts on a suitable hybrid latent representation based on a combination of dense features, sets of 2D keypoints and an additional latent vector per keypoint. We show that this better captures the dynamics of physical processes than purely dense or sparse representations. We introduce a new challenging and carefully designed counterfactual benchmark for predictions in pixel space and outperform strong baselines in physics-inspired ML and video prediction",
    "checked": true,
    "id": "b0b51bf9267baf50c5b415bf9e206779228759db",
    "semantic_title": "filtered-cophy: unsupervised learning of counterfactual physics in pixel space",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=p-BhZSz59o4": {
    "title": "BEiT: BERT Pre-Training of Image Transformers",
    "volume": "oral",
    "abstract": "We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e., image patches (such as 16 x 16 pixels), and visual tokens (i.e., discrete tokens). We first ``tokenize'' the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods",
    "checked": true,
    "id": "722ad6ac92286507437b31486f47987d6ece05c9",
    "semantic_title": "beit: bert pre-training of image transformers",
    "citation_count": 3076,
    "authors": []
  },
  "https://openreview.net/forum?id=UYneFzXSJWh": {
    "title": "Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution",
    "volume": "oral",
    "abstract": "When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer---the \"head\"). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (BREEDS-Living17, BREEDS-Entity30, DomainNet, CIFAR $\\to$ STL, CIFAR-10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2% higher accuracy ID but 7% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head---this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1% better ID, 10% better OOD than full fine-tuning)",
    "checked": true,
    "id": "29b77089a0a40f46372ce2dca9c3bb2dd5d46b1d",
    "semantic_title": "fine-tuning can distort pretrained features and underperform out-of-distribution",
    "citation_count": 749,
    "authors": []
  },
  "https://openreview.net/forum?id=Qg2vi4ZbHM9": {
    "title": "StyleAlign: Analysis and Applications of Aligned StyleGAN Models",
    "volume": "oral",
    "abstract": "In this paper, we perform an in-depth study of the properties and applications of aligned generative models. We refer to two models as aligned if they share the same architecture, and one of them (the child) is obtained from the other (the parent) via fine-tuning to another domain, a common practice in transfer learning. Several works already utilize some basic properties of aligned StyleGAN models to perform image-to-image translation. Here, we perform the first detailed exploration of model alignment, also focusing on StyleGAN. First, we empirically analyze aligned models and provide answers to important questions regarding their nature. In particular, we find that the child model's latent spaces are semantically aligned with those of the parent, inheriting incredibly rich semantics, even for distant data domains such as human faces and churches. Second, equipped with this better understanding, we leverage aligned models to solve a diverse set of tasks. In addition to image translation, we demonstrate fully automatic cross-domain image morphing. We further show that zero-shot vision tasks may be performed in the child domain, while relying exclusively on supervision in the parent domain. We demonstrate qualitatively and quantitatively that our approach yields state-of-the-art results, while requiring only simple fine-tuning and inversion",
    "checked": true,
    "id": "83bb34556b953a172bf5687131b24c17e015f476",
    "semantic_title": "stylealign: analysis and applications of aligned stylegan models",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=qnQN4yr6FJz": {
    "title": "Variational Inference for Discriminative Learning with Generative Modeling of Feature Incompletion",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "82df2081fadcc832d24d01dfcec8e96912702bfc",
    "semantic_title": "variational inference for discriminative learning with generative modeling of feature incompletion",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=uYLFoz1vlAC": {
    "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
    "semantic_title": "efficiently modeling long sequences with structured state spaces",
    "citation_count": 2275,
    "authors": []
  },
  "https://openreview.net/forum?id=bVuP3ltATMz": {
    "title": "Large Language Models Can Be Strong Differentially Private Learners",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "6f674172fd1a14d4f6a566d7ba1f75303c8d3ff7",
    "semantic_title": "large language models can be strong differentially private learners",
    "citation_count": 433,
    "authors": []
  },
  "https://openreview.net/forum?id=PzcvxEMzvQC": {
    "title": "GeoDiff: A Geometric Diffusion Model for Molecular Conformation Generation",
    "volume": "oral",
    "abstract": "",
    "checked": true,
    "id": "c871d2dc802d276608a6734637f8bc9e6da0d837",
    "semantic_title": "geodiff: a geometric diffusion model for molecular conformation generation",
    "citation_count": 581,
    "authors": []
  },
  "https://openreview.net/forum?id=zIUyj55nXR": {
    "title": "Frame Averaging for Invariant and Equivariant Network Design",
    "volume": "oral",
    "abstract": "Many machine learning tasks involve learning functions that are known to be invariant or equivariant to certain symmetries of the input data. However, it is often challenging to design neural network architectures that respect these symmetries while being expressive and computationally efficient. For example, Euclidean motion invariant/equivariant graph or point cloud neural networks. We introduce Frame Averaging (FA), a highly general purpose and systematic framework for adapting known (backbone) architectures to become invariant or equivariant to new symmetry types. Our framework builds on the well known group averaging operator that guarantees invariance or equivariance but is intractable. In contrast, we observe that for many important classes of symmetries, this operator can be replaced with an averaging operator over a small subset of the group elements, called a frame. We show that averaging over a frame guarantees exact invariance or equivariance while often being much simpler to compute than averaging over the entire group. Furthermore, we prove that FA-based models have maximal expressive power in a broad setting and in general preserve the expressive power of their backbone architectures. Using frame averaging, we propose a new class of universal Graph Neural Networks (GNNs), universal Euclidean motion invariant point cloud networks, and Euclidean motion invariant Message Passing (MP) GNNs. We demonstrate the practical effectiveness of FA on several applications including point cloud normal estimation, beyond $2$-WL graph separation, and $n$-body dynamics prediction, achieving state-of-the-art results in all of these benchmarks",
    "checked": true,
    "id": "c5f3ce9c1a9b64ecd093c40b42cffe1819d6092f",
    "semantic_title": "frame averaging for invariant and equivariant network design",
    "citation_count": 153,
    "authors": []
  },
  "https://openreview.net/forum?id=oapKSVM2bcj": {
    "title": "Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation",
    "volume": "oral",
    "abstract": "Tensor computations underlie modern scientific computing and deep learning. A number of tensor frameworks emerged varying in execution model, hardware support, memory management, model definition, etc. However, tensor operations in all frameworks follow the same paradigm. Recent neural network architectures demonstrate demand for higher expressiveness of tensor operations. The current paradigm is not suited to write readable, reliable, or easy-to-modify code for multidimensional tensor manipulations. Moreover, some commonly used operations do not provide sufficient checks and can break a tensor structure. These mistakes are elusive as no tools or tests can detect them. Independently, API discrepancies complicate code transfer between frameworks. We propose einops notation: a uniform and generic way to manipulate tensor structure, that significantly improves code readability and flexibility by focusing on the structure of input and output tensors. We implement einops notation in a Python package that efficiently supports multiple widely used frameworks and provides framework-independent minimalist API for tensor manipulations",
    "checked": true,
    "id": "8a85ef6a7ebcd8735b868bf9c4a77e6a3c195caa",
    "semantic_title": "einops: clear and reliable tensor manipulations with einstein-like notation",
    "citation_count": 110,
    "authors": []
  },
  "https://openreview.net/forum?id=Dl4LetuLdyK": {
    "title": "A Fine-Grained Analysis on Distribution Shift",
    "volume": "oral",
    "abstract": "Robustness to distribution shifts is critical for deploying machine learning models in the real world. Despite this necessity, there has been little work in defining the underlying mechanisms that cause these shifts and evaluating the robustness of algorithms across multiple, different distribution shifts. To this end, we introduce a framework that enables fine-grained analysis of various distribution shifts. We provide a holistic analysis of current state-of-the-art methods by evaluating 19 distinct methods grouped into five categories across both synthetic and real-world datasets. Overall, we train more than 85K models. Our experimental framework can be easily extended to include new methods, shifts, and datasets. We find, unlike previous work (Gulrajani & Lopez-Paz, 2021), that progress has been made over a standard ERM baseline; in particular, pretraining and augmentations (learned or heuristic) offer large gains in many cases. However, the best methods are not consistent over different datasets and shifts. We will open source our experimental framework, allowing future work to evaluate new methods over multiple shifts to obtain a more complete picture of a method's effectiveness. Code is available at github.com/deepmind/distribution_shift_framework",
    "checked": true,
    "id": "0e845ef0a3ae71bd32a6954fafe0702d0f0f033f",
    "semantic_title": "a fine-grained analysis on distribution shift",
    "citation_count": 227,
    "authors": []
  },
  "https://openreview.net/forum?id=5hLP5JY9S2d": {
    "title": "Open-Set Recognition: A Good Closed-Set Classifier is All You Need",
    "volume": "oral",
    "abstract": "The ability to identify whether or not a test sample belongs to one of the semantic classes in a classifier's training set is critical to practical deployment of the model. This task is termed open-set recognition (OSR) and has received significant attention in recent years. In this paper, we first demonstrate that the ability of a classifier to make the 'none-of-above' decision is highly correlated with its accuracy on the closed-set classes. We find that this relationship holds across loss objectives and architectures, and further demonstrate the trend both on the standard OSR benchmarks as well as on a large-scale ImageNet evaluation. Second, we use this correlation to boost the performance of the maximum softmax probability OSR 'baseline' by improving its closed-set accuracy, and with this strong baseline achieve state-of-the-art on a number of OSR benchmarks. Similarly, we boost the performance of the existing state-of-the-art method by improving its closed-set accuracy, but the resulting discrepancy with the strong baseline is marginal. Our third contribution is to present the 'Semantic Shift Benchmark' (SSB), which better respects the task of detecting semantic novelty, as opposed to low-level distributional shifts as tackled by neighbouring machine learning fields. On this new evaluation, we again demonstrate that there is negligible difference between the strong baseline and the existing state-of-the-art. Code available at: https://github.com/sgvaze/osr_closed_set_all_you_need",
    "checked": true,
    "id": "ba4dc8bdf6f1b9756def3a9b6a52fd797d13f4d4",
    "semantic_title": "open-set recognition: a good closed-set classifier is all you need",
    "citation_count": 488,
    "authors": []
  },
  "https://openreview.net/forum?id=M752z9FKJP": {
    "title": "Learning Strides in Convolutional Neural Networks",
    "volume": "oral",
    "abstract": "Convolutional neural networks typically contain several downsampling operators, such as strided convolutions or pooling layers, that progressively reduce the resolution of intermediate representations. This provides some shift-invariance while reducing the computational complexity of the whole architecture. A critical hyperparameter of such layers is their stride: the integer factor of downsampling. As strides are not differentiable, finding the best configuration either requires cross-validation or discrete optimization (e.g. architecture search), which rapidly become prohibitive as the search space grows exponentially with the number of downsampling layers. Hence, exploring this search space by gradient descent would allow finding better configurations at a lower computational cost. This work introduces DiffStride, the first downsampling layer with learnable strides. Our layer learns the size of a cropping mask in the Fourier domain, that effectively performs resizing in a differentiable way. Experiments on audio and image classification show the generality and effectiveness of our solution: we use DiffStride as a drop-in replacement to standard downsampling layers and outperform them. In particular, we show that introducing our layer into a ResNet-18 architecture allows keeping consistent high performance on CIFAR10, CIFAR100 and ImageNet even when training starts from poor random stride configurations. Moreover, formulating strides as learnable variables allows us to introduce a regularization term that controls the computational complexity of the architecture. We show how this regularization allows trading off accuracy for efficiency on ImageNet",
    "checked": true,
    "id": "6de8a22a3c3a6ad3727aaa02c2e7816c7f716207",
    "semantic_title": "learning strides in convolutional neural networks",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=7UmjRGzp-A": {
    "title": "Understanding over-squashing and bottlenecks on graphs via curvature",
    "volume": "oral",
    "abstract": "Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from distant nodes as a factor limiting the efficiency of message passing for tasks relying on long-distance interactions. This phenomenon, referred to as 'over-squashing', has been heuristically attributed to graph bottlenecks where the number of $k$-hop neighbors grows rapidly with $k$. We provide a precise description of the over-squashing phenomenon in GNNs and analyze how it arises from bottlenecks in the graph. For this purpose, we introduce a new edge-based combinatorial curvature and prove that negatively curved edges are responsible for the over-squashing issue. We also propose and experimentally test a curvature-based graph rewiring method to alleviate the over-squashing",
    "checked": true,
    "id": "04ac11f8db83406b03d92aa4571fc3e6c176c1e3",
    "semantic_title": "understanding over-squashing and bottlenecks on graphs via curvature",
    "citation_count": 514,
    "authors": []
  },
  "https://openreview.net/forum?id=8c50f-DoWAu": {
    "title": "Diffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling Scheme",
    "volume": "oral",
    "abstract": "Voice conversion is a common speech synthesis task which can be solved in different ways depending on a particular real-world scenario. The most challenging one often referred to as one-shot many-to-many voice conversion consists in copying target voice from only one reference utterance in the most general case when both source and target speakers do not belong to the training dataset. We present a scalable high-quality solution based on diffusion probabilistic modeling and demonstrate its superior quality compared to state-of-the-art one-shot voice conversion approaches. Moreover, focusing on real-time applications, we investigate general principles which can make diffusion models faster while keeping synthesis quality at a high level. As a result, we develop a novel Stochastic Differential Equations solver suitable for various diffusion model types and generative tasks as shown through empirical studies and justify it by theoretical analysis",
    "checked": true,
    "id": "d49a230b7718bd82fd7816d9d78e3ebd49118d2a",
    "semantic_title": "diffusion-based voice conversion with fast maximum likelihood sampling scheme",
    "citation_count": 153,
    "authors": []
  },
  "https://openreview.net/forum?id=EskfH0bwNVn": {
    "title": "Resolving Training Biases via Influence-based Data Relabeling",
    "volume": "oral",
    "abstract": "The performance of supervised learning methods easily suffers from the training bias issue caused by train-test distribution mismatch or label noise. Influence function is a technique that estimates the impacts of a training sample on the model's predictions. Recent studies on \\emph{data resampling} have employed influence functions to identify \\emph{harmful} training samples that will degrade model's test performance. They have shown that discarding or downweighting the identified harmful training samples is an effective way to resolve training biases. In this work, we move one step forward and propose an influence-based relabeling framework named RDIA for reusing harmful training samples toward better model performance. To achieve this, we use influence functions to estimate how relabeling a training sample would affect model's test performance and further develop a novel relabeling function R. We theoretically prove that applying R to relabel harmful training samples allows the model to achieve lower test loss than simply discarding them for any classification tasks using cross-entropy loss. Extensive experiments on ten real-world datasets demonstrate RDIA outperforms the state-of-the-art data resampling methods and improves model's robustness against label noise",
    "checked": true,
    "id": "d9a1efee3c3c71f7586c67ee80367330f9a0e11f",
    "semantic_title": "resolving training biases via influence-based data relabeling",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=9Hrka5PA7LW": {
    "title": "Representational Continuity for Unsupervised Continual Learning",
    "volume": "oral",
    "abstract": "Continual learning (CL) aims to learn a sequence of tasks without forgetting the previously acquired knowledge. However, recent CL advances are restricted to supervised continual learning (SCL) scenarios. Consequently, they are not scalable to real-world applications where the data distribution is often biased and unannotated. In this work, we focus on unsupervised continual learning (UCL), where we learn the feature representations on an unlabelled sequence of tasks and show that reliance on annotated data is not necessary for continual learning. We conduct a systematic study analyzing the learned feature representations and show that unsupervised visual representations are surprisingly more robust to catastrophic forgetting, consistently achieve better performance, and generalize better to out-of-distribution tasks than SCL. Furthermore, we find that UCL achieves a smoother loss landscape through qualitative analysis of the learned representations and learns meaningful feature representations. Additionally, we propose Lifelong Unsupervised Mixup (LUMP), a simple yet effective technique that interpolates between the current task and previous tasks' instances to alleviate catastrophic forgetting for unsupervised representations",
    "checked": true,
    "id": "771e0af5535a0122004c265d8d17931c710677b6",
    "semantic_title": "representational continuity for unsupervised continual learning",
    "citation_count": 126,
    "authors": []
  },
  "https://openreview.net/forum?id=RJkAHKp7kNZ": {
    "title": "Vision-Based Manipulators Need to Also See from Their Hands",
    "volume": "oral",
    "abstract": "We study how the choice of visual perspective affects learning and generalization in the context of physical manipulation from raw sensor observations. Compared with the more commonly used global third-person perspective, a hand-centric (eye-in-hand) perspective affords reduced observability, but we find that it consistently improves training efficiency and out-of-distribution generalization. These benefits hold across a variety of learning algorithms, experimental settings, and distribution shifts, and for both simulated and real robot apparatuses. However, this is only the case when hand-centric observability is sufficient; otherwise, including a third-person perspective is necessary for learning, but also harms out-of-distribution generalization. To mitigate this, we propose to regularize the third-person information stream via a variational information bottleneck. On six representative manipulation tasks with varying hand-centric observability adapted from the Meta-World benchmark, this results in a state-of-the-art reinforcement learning agent operating from both perspectives improving its out-of-distribution generalization on every task. While some practitioners have long put cameras in the hands of robots, our work systematically analyzes the benefits of doing so and provides simple and broadly applicable insights for improving end-to-end learned vision-based robotic manipulation",
    "checked": true,
    "id": "bc912d8c1a7165d89483d96d64b2ef20703ed2cb",
    "semantic_title": "vision-based manipulators need to also see from their hands",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=ajXWF7bVR8d": {
    "title": "Meta-Learning with Fewer Tasks through Task Interpolation",
    "volume": "oral",
    "abstract": "Meta-learning enables algorithms to quickly learn a newly encountered task with just a few labeled examples by transferring previously learned knowledge. However, the bottleneck of current meta-learning algorithms is the requirement of a large number of meta-training tasks, which may not be accessible in real-world scenarios. To address the challenge that available tasks may not densely sample the space of tasks, we propose to augment the task set through interpolation. By meta-learning with task interpolation (MLTI), our approach effectively generates additional tasks by randomly sampling a pair of tasks and interpolating the corresponding features and labels. Under both gradient-based and metric-based meta-learning settings, our theoretical analysis shows MLTI corresponds to a data-adaptive meta-regularization and further improves the generalization. Empirically, in our experiments on eight datasets from diverse domains including image recognition, pose prediction, molecule property prediction, and medical image classification, we find that the proposed general MLTI framework is compatible with representative meta-learning algorithms and consistently outperforms other state-of-the-art strategies",
    "checked": true,
    "id": "781fe7b9957a8b3a316f5ab63cef79e8d78fb88d",
    "semantic_title": "meta-learning with fewer tasks through task interpolation",
    "citation_count": 57,
    "authors": []
  },
  "https://openreview.net/forum?id=iRCUlgmdfHJ": {
    "title": "DISCOVERING AND EXPLAINING THE REPRESENTATION BOTTLENECK OF DNNS",
    "volume": "oral",
    "abstract": "This paper explores the bottleneck of feature representations of deep neural networks (DNNs), from the perspective of the complexity of interactions between input variables encoded in DNNs. To this end, we focus on the multi-order interaction between input variables, where the order represents the complexity of interactions. We discover that a DNN is more likely to encode both too simple and too complex interactions, but usually fails to learn interactions of intermediate complexity. Such a phenomenon is widely shared by different DNNs for different tasks. This phenomenon indicates a cognition gap between DNNs and humans, and we call it a representation bottleneck. We theoretically prove the underlying reason for the representation bottleneck. Furthermore, we propose losses to encourage/penalize the learning of interactions of specific complexities, and analyze the representation capacities of interactions of different complexities. The code is available at https://github.com/Nebularaid2000/bottleneck",
    "checked": true,
    "id": "9f97b49e2fe4e06a998d19537eb292552018db7d",
    "semantic_title": "discovering and explaining the representation bottleneck of dnns",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=WAid50QschI": {
    "title": "Sparse Communication via Mixed Distributions",
    "volume": "oral",
    "abstract": "Neural networks and other machine learning models compute continuous representations, while humans communicate mostly through discrete symbols. Reconciling these two forms of communication is desirable for generating human-readable interpretations or learning discrete latent variable models, while maintaining end-to-end differentiability. Some existing approaches (such as the Gumbel-Softmax transformation) build continuous relaxations that are discrete approximations in the zero-temperature limit, while others (such as sparsemax transformations and the Hard Concrete distribution) produce discrete/continuous hybrids. In this paper, we build rigorous theoretical foundations for these hybrids, which we call \"mixed random variables.'' Our starting point is a new \"direct sum'' base measure defined on the face lattice of the probability simplex. From this measure, we introduce new entropy and Kullback-Leibler divergence functions that subsume the discrete and differential cases and have interpretations in terms of code optimality. Our framework suggests two strategies for representing and sampling mixed random variables, an extrinsic (\"sample-and-project'') and an intrinsic one (based on face stratification). We experiment with both approaches on an emergent communication benchmark and on modeling MNIST and Fashion-MNIST data with variational auto-encoders with mixed latent variables",
    "checked": true,
    "id": "3bbb736d74fc7c96ebbe7ebf5bf82060840bb7f4",
    "semantic_title": "sparse communication via mixed distributions",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=gEZrGCozdqR": {
    "title": "Finetuned Language Models are Zero-Shot Learners",
    "volume": "oral",
    "abstract": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning‚Äîfinetuning language models on a collection of datasets described via instructions‚Äîsubstantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning",
    "checked": true,
    "id": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd",
    "semantic_title": "finetuned language models are zero-shot learners",
    "citation_count": 4168,
    "authors": []
  },
  "https://openreview.net/forum?id=_CfpJazzXT2": {
    "title": "F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization",
    "volume": "oral",
    "abstract": "Neural network quantization is a promising compression technique to reduce memory footprint and save energy consumption, potentially leading to real-time inference. However, there is a performance gap between quantized and full-precision models. To reduce it, existing quantization approaches require high-precision INT32 or full-precision multiplication during inference for scaling or dequantization. This introduces a noticeable cost in terms of memory, speed, and required energy. To tackle these issues, we present F8Net, a novel quantization framework consisting in only Ô¨Åxed-point 8-bit multiplication. To derive our method, we Ô¨Årst discuss the advantages of Ô¨Åxed-point multiplication with different formats of Ô¨Åxed-point numbers and study the statistical behavior of the associated Ô¨Åxed-point numbers. Second, based on the statistical and algorithmic analysis, we apply different Ô¨Åxed-point formats for weights and activations of different layers. We introduce a novel algorithm to automatically determine the right format for each layer during training. Third, we analyze a previous quantization algorithm‚Äîparameterized clipping activation (PACT)‚Äîand reformulate it using Ô¨Åxed-point arithmetic. Finally, we unify the recently proposed method for quantization Ô¨Åne-tuning and our Ô¨Åxed-point approach to show the potential of our method. We verify F8Net on ImageNet for MobileNet V1/V2 and ResNet18/50. Our approach achieves comparable and better performance, when compared not only to existing quantization techniques with INT32 multiplication or Ô¨Çoating point arithmetic, but also to the full-precision counterparts, achieving state-of-the-art performance",
    "checked": true,
    "id": "d31a2a1b1d2378030aed23f6888bce02897e20e7",
    "semantic_title": "f8net: fixed-point 8-bit only multiplication for network quantization",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=UcDUxjPYWSr": {
    "title": "Transform2Act: Learning a Transform-and-Control Policy for Efficient Agent Design",
    "volume": "oral",
    "abstract": "An agent's functionality is largely determined by its design, i.e., skeletal structure and joint attributes (e.g., length, size, strength). However, finding the optimal agent design for a given function is extremely challenging since the problem is inherently combinatorial and the design space is prohibitively large. Additionally, it can be costly to evaluate each candidate design which requires solving for its optimal controller. To tackle these problems, our key idea is to incorporate the design procedure of an agent into its decision-making process. Specifically, we learn a conditional policy that, in an episode, first applies a sequence of transform actions to modify an agent's skeletal structure and joint attributes, and then applies control actions under the new design. To handle a variable number of joints across designs, we use a graph-based policy where each graph node represents a joint and uses message passing with its neighbors to output joint-specific actions. Using policy gradient methods, our approach enables joint optimization of agent design and control as well as experience sharing across different designs, which improves sample efficiency substantially. Experiments show that our approach, Transform2Act, outperforms prior methods significantly in terms of convergence speed and final performance. Notably, Transform2Act can automatically discover plausible designs similar to giraffes, squids, and spiders. Code and videos are available at https://sites.google.com/view/transform2act",
    "checked": true,
    "id": "4a26969478c80e15a68dda8712da34c8a6a441ef",
    "semantic_title": "transform2act: learning a transform-and-control policy for efficient agent design",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=s03AQxehtd_": {
    "title": "ProtoRes: Proto-Residual Network for Pose Authoring via Learned Inverse Kinematics",
    "volume": "oral",
    "abstract": "Our work focuses on the development of a learnable neural representation of human pose for advanced AI assisted animation tooling. Specifically, we tackle the problem of constructing a full static human pose based on sparse and variable user inputs (e.g. locations and/or orientations of a subset of body joints). To solve this problem, we propose a novel neural architecture that combines residual connections with prototype encoding of a partially specified pose to create a new complete pose from the learned latent space. We show that our architecture outperforms a baseline based on Transformer, both in terms of accuracy and computational efficiency. Additionally, we develop a user interface to integrate our neural model in Unity, a real-time 3D development platform. Furthermore, we introduce two new datasets representing the static human pose modeling problem, based on high-quality human motion capture data, which will be released publicly along with model code",
    "checked": true,
    "id": "3642b974f794e4dc6cc9327a09cc8c28b6ae6bd3",
    "semantic_title": "protores: proto-residual network for pose authoring via learned inverse kinematics",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=-70L8lpp9DF": {
    "title": "Hyperparameter Tuning with Renyi Differential Privacy",
    "volume": "oral",
    "abstract": "For many differentially private algorithms, such as the prominent noisy stochastic gradient descent (DP-SGD), the analysis needed to bound the privacy leakage of a single training run is well understood. However, few studies have reasoned about the privacy leakage resulting from the multiple training runs needed to fine tune the value of the training algorithm's hyperparameters. In this work, we first illustrate how simply setting hyperparameters based on non-private training runs can leak private information. Motivated by this observation, we then provide privacy guarantees for hyperparameter search procedures within the framework of Renyi Differential Privacy. Our results improve and extend the work of Liu and Talwar (STOC 2019). Our analysis supports our previous observation that tuning hyperparameters does indeed leak private information, but we prove that, under certain assumptions, this leakage is modest, as long as each candidate training run needed to select hyperparameters is itself differentially private",
    "checked": true,
    "id": "12d8a96760e1752bb7fd78f6507ec91ec7581f79",
    "semantic_title": "hyperparameter tuning with renyi differential privacy",
    "citation_count": 141,
    "authors": []
  },
  "https://openreview.net/forum?id=qj1IZ-6TInc": {
    "title": "Real-Time Neural Voice Camouflage",
    "volume": "oral",
    "abstract": "Automatic speech recognition systems have created exciting possibilities for applications, however they also enable opportunities for systematic eavesdropping.We propose a method to camouflage a person's voice from these systems without inconveniencing the conversation between people in the room. Standard adversarial attacks are not effective in real-time streaming situations because the characteristics of the signal will have changed by the time the attack is executed. We introduce predictive adversarial attacks, which achieves real-time performance by forecasting the attack vector that will be the most effective in the future. Under real-time constraints, our method jams the established speech recognition system DeepSpeech 3.9x more than online projected gradient descent as measured through word error rate, and 6.6x more as measured through character error rate. We furthermore demonstrate our approach is practically effective in realistic environments with complex scene geometries",
    "checked": true,
    "id": "4e2dfe2b54bcd5d5c8178aca868959568298f0c8",
    "semantic_title": "real-time neural voice camouflage",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=NMEceG4v69Y": {
    "title": "CycleMLP: A MLP-like Architecture for Dense Prediction",
    "volume": "oral",
    "abstract": "This paper presents a simple MLP-like architecture, CycleMLP, which is a versatile backbone for visual recognition and dense predictions. As compared to modern MLP architectures, e.g. , MLP-Mixer, ResMLP, and gMLP, whose architectures are correlated to image size and thus are infeasible in object detection and segmentation, CycleMLP has two advantages compared to modern approaches. (1) It can cope with various image sizes. (2) It achieves linear computational complexity to image size by using local windows. In contrast, previous MLPs have $O(N^2)$ computations due to fully spatial connections. We build a family of models which surpass existing MLPs and even state-of-the-art Transformer-based models, e.g. Swin Transformer, while using fewer parameters and FLOPs. We expand the MLP-like models' applicability, making them a versatile backbone for dense prediction tasks. CycleMLP achieves competitive results on object detection, instance segmentation, and semantic segmentation. In particular, CycleMLP-Tiny outperforms Swin-Tiny by 1.3% mIoU on ADE20K dataset with fewer FLOPs. Moreover, CycleMLP also shows excellent zero-shot robustness on ImageNet-C dataset",
    "checked": true,
    "id": "f75cddf2d42ed01b34686704eb3504becef67442",
    "semantic_title": "cyclemlp: a mlp-like architecture for dense prediction",
    "citation_count": 242,
    "authors": []
  },
  "https://openreview.net/forum?id=0xiJLKH-ufZ": {
    "title": "Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models",
    "volume": "oral",
    "abstract": "Diffusion probabilistic models (DPMs) represent a class of powerful generative models. Despite their success, the inference of DPMs is expensive since it generally needs to iterate over thousands of timesteps. A key problem in the inference is to estimate the variance in each timestep of the reverse process. In this work, we present a surprising result that both the optimal reverse variance and the corresponding optimal KL divergence of a DPM have analytic forms w.r.t. its score function. Building upon it, we propose \\textit{Analytic-DPM}, a training-free inference framework that estimates the analytic forms of the variance and KL divergence using the Monte Carlo method and a pretrained score-based model. Further, to correct the potential bias caused by the score-based model, we derive both lower and upper bounds of the optimal variance and clip the estimate for a better result. Empirically, our analytic-DPM improves the log-likelihood of various DPMs, produces high-quality samples, and meanwhile enjoys a $20\\times$ to $80\\times$ speed up",
    "checked": true,
    "id": "9b7b218b0f4e14f97260b6192add37da5e9ae2c5",
    "semantic_title": "analytic-dpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models",
    "citation_count": 366,
    "authors": []
  },
  "https://openreview.net/forum?id=uSE03demja": {
    "title": "RISP: Rendering-Invariant State Predictor with Differentiable Simulation and Rendering for Cross-Domain Parameter Estimation",
    "volume": "oral",
    "abstract": "This work considers identifying parameters characterizing a physical system's dynamic motion directly from a video whose rendering configurations are inaccessible. Existing solutions require massive training data or lack generalizability to unknown rendering configurations. We propose a novel approach that marries domain randomization and differentiable rendering gradients to address this problem. Our core idea is to train a rendering-invariant state-prediction (RISP) network that transforms image differences into state differences independent of rendering configurations, e.g., lighting, shadows, or material reflectance. To train this predictor, we formulate a new loss on rendering variances using gradients from differentiable rendering. Moreover, we present an efficient, second-order method to compute the gradients of this loss, allowing it to be integrated seamlessly into modern deep learning frameworks. We evaluate our method in rigid-body and deformable-body simulation environments using four tasks: state estimation, system identification, imitation learning, and visuomotor control. We further demonstrate the efficacy of our approach on a real-world example: inferring the state and action sequences of a quadrotor from a video of its motion sequences. Compared with existing methods, our approach achieves significantly lower reconstruction errors and has better generalizability among unknown rendering configurations",
    "checked": true,
    "id": "da98d61a5ed63abe34d52bc9d500f702f9239e87",
    "semantic_title": "risp: rendering-invariant state predictor with differentiable simulation and rendering for cross-domain parameter estimation",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=3wU2UX0voE": {
    "title": "The Information Geometry of Unsupervised Reinforcement Learning",
    "volume": "oral",
    "abstract": "How can a reinforcement learning (RL) agent prepare to solve downstream tasks if those tasks are not known a priori? One approach is unsupervised skill discovery, a class of algorithms that learn a set of policies without access to a reward function. Such algorithms bear a close resemblance to representation learning algorithms (e.g., contrastive learning) in supervised learning, in that both are pretraining algorithms that maximize some approximation to a mutual information objective. While prior work has shown that the set of skills learned by such methods can accelerate downstream RL tasks, prior work offers little analysis into whether these skill learning algorithms are optimal, or even what notion of optimality would be appropriate to apply to them. In this work, we show that unsupervised skill discovery algorithms based on mutual information maximization do not learn skills that are optimal for every possible reward function. However, we show that the distribution over skills provides an optimal initialization minimizing regret against adversarially-chosen reward functions, assuming a certain type of adaptation procedure. Our analysis also provides a geometric perspective on these skill learning methods",
    "checked": true,
    "id": "40888b859c5b40868943162e3c4769dae1aed716",
    "semantic_title": "the information geometry of unsupervised reinforcement learning",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=pMQwKL1yctf": {
    "title": "Language modeling via stochastic processes",
    "volume": "oral",
    "abstract": "Modern language models can generate high-quality short texts. However, they often meander or are incoherent when generating longer texts. These issues arise from the next-token-only language modeling objective. To address these issues, we introduce Time Control (TC), a language model that implicitly plans via a latent stochastic process. TC does this by learning a representation which maps the dynamics of how text changes in a document to the dynamics of a stochastic process of interest. Using this representation, the language model can generate text by first implicitly generating a document plan via a stochastic process, and then generating text that is consistent with this latent plan. Compared to domain-specific methods and fine-tuning GPT2 across a variety of text domains, TC improves performance on text infilling and discourse coherence. On long text generation settings, TC preserves the text structure both in terms of ordering (up to +40% better) and text length consistency (up to +17% better). Human evaluators also prefer TC's output 28.6% more than the baselines",
    "checked": true,
    "id": "6960918666a8c9d50343bbe9e94baf6415edd5fb",
    "semantic_title": "language modeling via stochastic processes",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=g1SzIRLQXMM": {
    "title": "Wiring Up Vision: Minimizing Supervised Synaptic Updates Needed to Produce a Primate Ventral Stream",
    "volume": "spotlight",
    "abstract": "After training on large datasets, certain deep neural networks are surprisingly good models of the neural mechanisms of adult primate visual object recognition. Nevertheless, these models are considered poor models of the development of the visual system because they posit millions of sequential, precisely coordinated synaptic updates, each based on a labeled image. While ongoing research is pursuing the use of unsupervised proxies for labels, we here explore a complementary strategy of reducing the required number of supervised synaptic updates to produce an adult-like ventral visual stream (as judged by the match to V1, V2, V4, IT, and behavior). Such models might require less precise machinery and energy expenditure to coordinate these updates and would thus move us closer to viable neuroscientific hypotheses about how the visual system wires itself up. Relative to standard model training on labeled images in ImageNet, we here demonstrate that the total number of supervised weight updates can be substantially reduced using three complementary strategies: First, we find that only 2% of supervised updates (epochs and images) are needed to achieve 80% of the match to adult ventral stream. Specifically, training benefits predictions of higher visual cortex the most whereas early visual cortex predictions only improve marginally over the course of training. Second, by improving the random distribution of synaptic connectivity, we find that 54% of the brain match can already be achieved \"at birth\" (i.e. no training at all). Third, we find that, by training only 5% of model synapses, we can still achieve nearly 80% of the match to the ventral stream. This approach further improves on ImageNet performance over previous attempts in computer vision of minimizing trained components without substantially increasing the relative number of trained parameters. These results reflect first steps in modeling not just primate adult visual processing during inference, but also how the ventral visual stream might be \"wired up\" by evolution (a model's \"birth\" state) and by developmental learning (a model's updates based on visual experience)",
    "checked": true,
    "id": "19d68db5346c837bb428160619356e36045b351c",
    "semantic_title": "wiring up vision: minimizing supervised synaptic updates needed to produce a primate ventral stream",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=CALFyKVs87": {
    "title": "Dynamics-Aware Comparison of Learned Reward Functions",
    "volume": "spotlight",
    "abstract": "The ability to learn reward functions plays an important role in enabling the deployment of intelligent agents in the real world. However, $\\textit{comparing}$ reward functions, for example as a means of evaluating reward learning methods, presents a challenge. Reward functions are typically compared by considering the behavior of optimized policies, but this approach conflates deficiencies in the reward function with those of the policy search algorithm used to optimize it. To address this challenge, Gleave et al. (2020) propose the Equivalent-Policy Invariant Comparison (EPIC) distance. EPIC avoids policy optimization, but in doing so requires computing reward values at transitions that may be impossible under the system dynamics. This is problematic for learned reward functions because it entails evaluating them outside of their training distribution, resulting in inaccurate reward values that we show can render EPIC ineffective at comparing rewards. To address this problem, we propose the Dynamics-Aware Reward Distance (DARD), a new reward pseudometric. DARD uses an approximate transition model of the environment to transform reward functions into a form that allows for comparisons that are invariant to reward shaping while only evaluating reward functions on transitions close to their training distribution. Experiments in simulated physical domains demonstrate that DARD enables reliable reward comparisons without policy optimization and is significantly more predictive than baseline methods of downstream policy performance when dealing with learned reward functions",
    "checked": true,
    "id": "bd45112d6f1745a6dccf10f4c423ad9331102cde",
    "semantic_title": "dynamics-aware comparison of learned reward functions",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=5LXw_QplBiF": {
    "title": "Learning Hierarchical Structures with Differentiable Nondeterministic Stacks",
    "volume": "spotlight",
    "abstract": "Learning hierarchical structures in sequential data -- from simple algorithmic patterns to natural language -- in a reliable, generalizable way remains a challenging problem for neural language models. Past work has shown that recurrent neural networks (RNNs) struggle to generalize on held-out algorithmic or syntactic patterns without supervision or some inductive bias. To remedy this, many papers have explored augmenting RNNs with various differentiable stacks, by analogy with finite automata and pushdown automata (PDAs). In this paper, we improve the performance of our recently proposed Nondeterministic Stack RNN (NS-RNN), which uses a differentiable data structure that simulates a nondeterministic PDA, with two important changes. First, the model now assigns unnormalized positive weights instead of probabilities to stack actions, and we provide an analysis of why this improves training. Second, the model can directly observe the state of the underlying PDA. Our model achieves lower cross-entropy than all previous stack RNNs on five context-free language modeling tasks (within 0.05 nats of the information-theoretic lower bound), including a task on which the NS-RNN previously failed to outperform a deterministic stack RNN baseline. Finally, we propose a restricted version of the NS-RNN that incrementally processes infinitely long sequences, and we present language modeling results on the Penn Treebank",
    "checked": true,
    "id": "1a22e6406e67c1737ec073cc646f60cb78631a4c",
    "semantic_title": "learning hierarchical structures with differentiable nondeterministic stacks",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=eMudnJsb1T5": {
    "title": "Sampling with Mirrored Stein Operators",
    "volume": "spotlight",
    "abstract": "We introduce a new family of particle evolution samplers suitable for constrained domains and non-Euclidean geometries. Stein Variational Mirror Descent and Mirrored Stein Variational Gradient Descent minimize the Kullback-Leibler (KL) divergence to constrained target distributions by evolving particles in a dual space defined by a mirror map. Stein Variational Natural Gradient exploits non-Euclidean geometry to more efficiently minimize the KL divergence to unconstrained targets. We derive these samplers from a new class of mirrored Stein operators and adaptive kernels developed in this work. We demonstrate that these new samplers yield accurate approximations to distributions on the simplex, deliver valid confidence intervals in post-selection inference, and converge more rapidly than prior methods in large-scale unconstrained posterior inference. Finally, we establish the convergence of our new procedures under verifiable conditions on the target distribution",
    "checked": true,
    "id": "535536f05afaebaffa460d8e2c6ff3ad5a973431",
    "semantic_title": "sampling with mirrored stein operators",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=X6D9bAHhBQ1": {
    "title": "Planning in Stochastic Environments with a Learned Model",
    "volume": "spotlight",
    "abstract": "Model-based reinforcement learning has proven highly successful. However, learning a model in isolation from its use during planning is problematic in complex environments. To date, the most effective techniques have instead combined value-equivalent model learning with powerful tree-search methods. This approach is exemplified by MuZero, which has achieved state-of-the-art performance in a wide range of domains, from board games to visually rich environments, with discrete and continuous action spaces, in online and offline settings. However, previous instantiations of this approach were limited to the use of deterministic models. This limits their performance in environments that are inherently stochastic, partially observed, or so large and complex that they appear stochastic to a finite agent. In this paper we extend this approach to learn and plan with stochastic models. Specifically, we introduce a new algorithm, Stochastic MuZero, that learns a stochastic model incorporating afterstates, and uses this model to perform a stochastic tree search. Stochastic MuZero matched or exceeded the state of the art in a set of canonical single and multi-agent environments, including 2048 and backgammon, while maintaining the same performance as standard MuZero in the game of Go",
    "checked": true,
    "id": "899b22558b417e5afbdf013963a393d3daf2dabc",
    "semantic_title": "planning in stochastic environments with a learned model",
    "citation_count": 66,
    "authors": []
  },
  "https://openreview.net/forum?id=T8wHz4rnuGL": {
    "title": "RotoGrad: Gradient Homogenization in Multitask Learning",
    "volume": "spotlight",
    "abstract": "Multitask learning is being increasingly adopted in applications domains like computer vision and reinforcement learning. However, optimally exploiting its advantages remains a major challenge due to the effect of negative transfer. Previous works have tracked down this issue to the disparities in gradient magnitudes and directions across tasks, when optimizing the shared network parameters. While recent work has acknowledged that negative transfer is a two-fold problem, existing approaches fall short as they only focus on either homogenizing the gradient magnitude across tasks; or greedily change the gradient directions, overlooking future conflicts. In this work, we introduce RotoGrad, an algorithm that tackles negative transfer as a whole: it jointly homogenizes gradient magnitudes and directions, while ensuring training convergence. We show that RotoGrad outperforms competing methods in complex problems, including multi-label classification in CelebA and computer vision tasks in the NYUv2 dataset. A Pytorch implementation can be found in https://github.com/adrianjav/rotograd",
    "checked": true,
    "id": "a8f97a65f1cb6106b83d3c21a1dad5fa005aee43",
    "semantic_title": "rotograd: gradient homogenization in multitask learning",
    "citation_count": 99,
    "authors": []
  },
  "https://openreview.net/forum?id=D6nH3719vZy": {
    "title": "On Improving Adversarial Transferability of Vision Transformers",
    "volume": "spotlight",
    "abstract": "Vision transformers (ViTs) process input images as sequences of patches via self-attention; a radically different architecture than convolutional neural networks (CNNs). This makes it interesting to study the adversarial feature space of ViT models and their transferability. In particular, we observe that adversarial patterns found via conventional adversarial attacks show very \\emph{low} black-box transferability even for large ViT models. We show that this phenomenon is only due to the sub-optimal attack procedures that do not leverage the true representation potential of ViTs. A deep ViT is composed of multiple blocks, with a consistent architecture comprising of self-attention and feed-forward layers, where each block is capable of independently producing a class token. Formulating an attack using only the last class token (conventional approach) does not directly leverage the discriminative information stored in the earlier tokens, leading to poor adversarial transferability of ViTs.Using the compositional nature of ViT models, we enhance transferability of existing attacks by introducing two novel strategies specific to the architecture of ViT models. \\emph{(i) Self-Ensemble:} We propose a method to find multiple discriminative pathways by dissecting a single ViT model into an ensemble of networks. This allows explicitly utilizing class-specific information at each ViT block. \\emph{(ii) Token Refinement:} We then propose to refine the tokens to further enhance the discriminative capacity at each block of ViT.Our token refinement systematically combines the class tokens with structural information preserved within the patch tokens. An adversarial attack when applied to such refined tokens within the ensemble of classifiers found in a single vision transformer has significantly higher transferability and thereby brings out the true generalization potential of the ViT's adversarial space. Code: https://t.ly/hBbW",
    "checked": true,
    "id": "0918125daacb6c2b3a2d3f155ad095d5ae8fb9b9",
    "semantic_title": "on improving adversarial transferability of vision transformers",
    "citation_count": 102,
    "authors": []
  },
  "https://openreview.net/forum?id=eW5R4Cek6y6": {
    "title": "On Predicting Generalization using GANs",
    "volume": "spotlight",
    "abstract": "Research on generalization bounds for deep networks seeks to give ways to predict test error using just the training dataset and the network parameters. While generalization bounds can give many insights about architecture design, training algorithms etc., what they do not currently do is yield good predictions for actual test error. A recently introduced Predicting Generalization in Deep Learning competition aims to encourage discovery of methods to better predict test error. The current paper investigates a simple idea: can test error be predicted using {\\em synthetic data,} produced using a Generative Adversarial Network (GAN) that was trained on the same training dataset? Upon investigating several GAN models and architectures, we find that this turns out to be the case. In fact, using GANs pre-trained on standard datasets, the test error can be predicted without requiring any additional hyper-parameter tuning. This result is surprising because GANs have well-known limitations (e.g. mode collapse) and are known to not learn the data distribution accurately. Yet the generated samples are good enough to substitute for test data. Several additional experiments are presented to explore reasons why GANs do well at this task. In addition to a new approach for predicting generalization, the counter-intuitive phenomena presented in our work may also call for a better understanding of GANs' strengths and limitations",
    "checked": true,
    "id": "0951c005254b3947be88bacadaa4f9c5c1806ead",
    "semantic_title": "on predicting generalization using gans",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=L3_SsSNMmy": {
    "title": "On the Connection between Local Attention and Dynamic Depth-wise Convolution",
    "volume": "spotlight",
    "abstract": "Vision Transformer (ViT) attains state-of-the-art performance in visual recognition, and the variant, Local Vision Transformer, makes further improvements. The major component in Local Vision Transformer, local attention, performs the attention separately over small local windows. We rephrase local attention as a channel-wise locally-connected layer and analyze it from two network regularization manners, sparse connectivity and weight sharing, as well as dynamic weight computation. We point out that local attention resembles depth-wise convolution and its dynamic variants in sparse connectivity: there is no connection across channels, and each position is connected to the positions within a small local window. The main differences lie in (i) weight sharing - depth-wise convolution shares connection weights (kernel weights) across spatial positions and attention shares the connection weights across channels, and (ii) dynamic weight computation manners - local attention is based on dot-products between pairwise positions in the local window, and dynamic convolution is based on linear projections conducted on the center representation or the globally pooled representation. The connection between local attention and dynamic depth-wise convolution is empirically verified by the ablation study about weight sharing and dynamic weight computation in Local Vision Transformer and (dynamic) depth-wise convolution. We empirically observe that the models based on depth-wise convolution and the dynamic variants with lower computation complexity perform on-par with or slightly better than Swin Transformer, an instance of Local Vision Transformer, for ImageNet classification, COCO object detection and ADE semantic segmentation. Code is available at https://github.com/Atten4Vis/DemystifyLocalViT",
    "checked": true,
    "id": "6b6ffb94626e672caffafc77097491d9ee7a8682",
    "semantic_title": "on the connection between local attention and dynamic depth-wise convolution",
    "citation_count": 117,
    "authors": []
  },
  "https://openreview.net/forum?id=uorVGbWV5sw": {
    "title": "Strength of Minibatch Noise in SGD",
    "volume": "spotlight",
    "abstract": "The noise in stochastic gradient descent (SGD), caused by minibatch sampling, is poorly understood despite its practical importance in deep learning. This work presents the first systematic study of the SGD noise and fluctuations close to a local minimum. We first analyze the SGD noise in linear regression in detail and then derive a general formula for approximating SGD noise in different types of minima. For application, our results (1) provide insight into the stability of training a neural network, (2) suggest that a large learning rate can help generalization by introducing an implicit regularization, (3) explain why the linear learning rate-batchsize scaling law fails at a large learning rate or at a small batchsize and (4) can provide an understanding of how discrete-time nature of SGD affects the recently discovered power-law phenomenon of SGD",
    "checked": true,
    "id": "6cd064b3fb736cef8404ab24227add67c7c23779",
    "semantic_title": "strength of minibatch noise in sgd",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=cU8rknuhxc": {
    "title": "Learning more skills through optimistic exploration",
    "volume": "spotlight",
    "abstract": "Unsupervised skill learning objectives (Eysenbach et al., 2019; Gregor et al., 2016) allow agents to learn rich repertoires of behavior in the absence of extrinsic rewards. They work by simultaneously training a policy to produce distinguishable latent-conditioned trajectories, and a discriminator to evaluate distinguishability by trying to infer latents from trajectories. The hope is for the agent to explore and master the environment by encouraging each skill (latent) to reliably reach different states. However, an inherent exploration problem lingers: when a novel state is actually encountered, the discriminator will necessarily not have seen enough training data to produce accurate and confident skill classifications, leading to low intrinsic reward for the agent and effective penalization of the sort of exploration needed to actually maximize the objective. To combat this inherent pessimism towards exploration, we derive an information gain auxiliary objective that involves training an ensemble of discriminators and rewarding the policy for their disagreement. Our objective directly estimates the epistemic uncertainty that comes from the discriminator not having seen enough training examples, thus providing an intrinsic reward more tailored to the true objective compared to pseudocount-based methods (Burda et al., 2019). We call this exploration bonus discriminator disagreement intrinsic reward, or DISDAIN. We demonstrate empirically that DISDAIN improves skill learning both in a tabular grid world (Four Rooms) and the 57 games of the Atari Suite (from pixels). Thus, we encourage researchers to treat pessimism with DISDAIN",
    "checked": true,
    "id": "3c91a4534d51d21ae67e4a9f9287bb2a14dc5e3b",
    "semantic_title": "learning more skills through optimistic exploration",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=PLDOnFoVm4": {
    "title": "Reinforcement Learning under a Multi-agent Predictive State Representation Model: Method and Theory",
    "volume": "spotlight",
    "abstract": "We study reinforcement learning for partially observable multi-agent systems where each agent only has access to its own observation and reward and aims to maximize its cumulative rewards. To handle partial observations, we propose graph-assisted predictive state representations (GAPSR), a scalable multi-agent representation learning framework that leverages the agent connectivity graphs to aggregate local representations computed by each agent. In addition, our representations are readily able to incorporate dynamic interaction graphs and kernel space embeddings of the predictive states, and thus have strong flexibility and representation power. Based on GAPSR, we propose an end-to-end MARL algorithm that simultaneously infers the predictive representations and uses the representations as the input of a policy optimization algorithm. Empirically, we demonstrate the efficacy of the proposed algorithm provided on both a MAMuJoCo robotic learning experiment and a multi-agent particle learning environment",
    "checked": true,
    "id": "5a24ed22fac891d2e463248aebbfa81e165f9d99",
    "semantic_title": "reinforcement learning under a multi-agent predictive state representation model: method and theory",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=26gKg6x-ie": {
    "title": "Adversarial Support Alignment",
    "volume": "spotlight",
    "abstract": "We study the problem of aligning the supports of distributions. Compared to the existing work on distribution alignment, support alignment does not require the densities to be matched. We propose symmetric support difference as a divergence measure to quantify the mismatch between supports. We show that select discriminators (e.g. discriminator trained for Jensen-Shannon divergence) are able to map support differences as support differences in their one-dimensional output space. Following this result, our method aligns supports by minimizing a symmetrized relaxed optimal transport cost in the discriminator 1D space via an adversarial process. Furthermore, we show that our approach can be viewed as a limit of existing notions of alignment by increasing transportation assignment tolerance. We quantitatively evaluate the method across domain adaptation tasks with shifts in label distributions. Our experiments show that the proposed method is more robust against these shifts than other alignment-based baselines",
    "checked": true,
    "id": "cce3a7980d6afee9ce7156df8aa76aabd0d9b918",
    "semantic_title": "adversarial support alignment",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=41e9o6cQPj": {
    "title": "GreaseLM: Graph REASoning Enhanced Language Models",
    "volume": "spotlight",
    "abstract": "Answering complex questions about textual narratives requires reasoning over both stated context and the world knowledge that underlies it. However, pretrained language models (LM), the foundation of most modern QA systems, do not robustly represent latent relationships between concepts, which is necessary for reasoning. While knowledge graphs (KG) are often used to augment LMs with structured representations of world knowledge, it remains an open question how to effectively fuse and reason over the KG representations and the language context, which provides situational constraints and nuances. In this work, we propose GreaseLM, a new model that fuses encoded representations from pretrained LMs and graph neural networks over multiple layers of modality interaction operations. Information from both modalities propagates to the other, allowing language context representations to be grounded by structured world knowledge, and allowing linguistic nuances (e.g., negation, hedging) in the context to inform the graph representations of knowledge. Our results on three benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA) and medical question answering (i.e., MedQA-USMLE) domains demonstrate that GreaseLM can more reliably answer questions that require reasoning over both situational constraints and structured knowledge, even outperforming models 8x larger",
    "checked": true,
    "id": "4ab41d9780f1d1ac34d39fa7e527e73652507fcc",
    "semantic_title": "greaselm: graph reasoning enhanced language models for question answering",
    "citation_count": 238,
    "authors": []
  },
  "https://openreview.net/forum?id=DTkEfj0Ygb8": {
    "title": "Learning meta-features for AutoML",
    "volume": "spotlight",
    "abstract": "This paper tackles the AutoML problem, aimed to automatically select an ML algorithm and its hyper-parameter configuration most appropriate to the dataset at hand. The proposed approach, MetaBu, learns new meta-features via an Optimal Transport procedure, aligning the manually designed \\mf s with the space of distributions on the hyper-parameter configurations. MetaBu meta-features, learned once and for all, induce a topology on the set of datasets that is exploited to define a distribution of promising hyper-parameter configurations amenable to AutoML. Experiments on the OpenML CC-18 benchmark demonstrate that using MetaBu meta-features boosts the performance of state of the art AutoML systems, AutoSklearn (Feurer et al. 2015) and Probabilistic Matrix Factorization (Fusi et al. 2018). Furthermore, the inspection of MetaBu meta-features gives some hints into when an ML algorithm does well. Finally, the topology based on MetaBu meta-features enables to estimate the intrinsic dimensionality of the OpenML benchmark w.r.t. a given ML algorithm or pipeline. The source code is available at https://github.com/luxusg1/metabu",
    "checked": true,
    "id": "e168798c3be0133ca66ae30a4e200d4bd6740b8b",
    "semantic_title": "learning meta-features for automl",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=Dup_dDqkZC5": {
    "title": "Latent Variable Sequential Set Transformers for Joint Multi-Agent Motion Prediction",
    "volume": "spotlight",
    "abstract": "Robust multi-agent trajectory prediction is essential for the safe control of robotic systems. A major challenge is to efficiently learn a representation that approximates the true joint distribution of contextual, social, and temporal information to enable planning. We propose Latent Variable Sequential Set Transformers which are encoder-decoder architectures that generate scene-consistent multi-agent trajectories. We refer to these architectures as \"AutoBots\". The encoder is a stack of interleaved temporal and social multi-head self-attention (MHSA) modules which alternately perform equivariant processing across the temporal and social dimensions. The decoder employs learnable seed parameters in combination with temporal and social MHSA modules allowing it to perform inference over the entire future scene in a single forward pass efficiently. AutoBots can produce either the trajectory of one ego-agent or a distribution over the future trajectories for all agents in the scene. For the single-agent prediction case, our model achieves top results on the global nuScenes vehicle motion prediction leaderboard, and produces strong results on the Argoverse vehicle prediction challenge. In the multi-agent setting, we evaluate on the synthetic partition of TrajNet++ dataset to showcase the model's socially-consistent predictions. We also demonstrate our model on general sequences of sets and provide illustrative experiments modelling the sequential structure of the multiple strokes that make up symbols in the Omniglot data. A distinguishing feature of AutoBots is that all models are trainable on a single desktop GPU (1080 Ti) in under 48h",
    "checked": true,
    "id": "6c1bb8b017c469208bd7e3a80639bdb5f1726e2c",
    "semantic_title": "latent variable sequential set transformers for joint multi-agent motion prediction",
    "citation_count": 160,
    "authors": []
  },
  "https://openreview.net/forum?id=5FUq05QRc5b": {
    "title": "Understanding Latent Correlation-Based Multiview Learning and Self-Supervision: An Identifiability Perspective",
    "volume": "spotlight",
    "abstract": "Multiple views of data, both naturally acquired (e.g., image and audio) and artificially produced (e.g., via adding different noise to data samples), have proven useful in enhancing representation learning. Natural views are often handled by multiview analysis tools, e.g., (deep) canonical correlation analysis [(D)CCA], while the artificial ones are frequently used in self-supervised learning (SSL) paradigms, e.g., BYOL and Barlow Twins. Both types of approaches often involve learning neural feature extractors such that the embeddings of data exhibit high cross-view correlations. Although intuitive, the effectiveness of correlation-based neural embedding is mostly empirically validated. This work aims to understand latent correlation maximization-based deep multiview learning from a latent component identification viewpoint. An intuitive generative model of multiview data is adopted, where the views are different nonlinear mixtures of shared and private components. Since the shared components are view/distortion-invariant, representing the data using such components is believed to reveal the identity of the samples effectively and robustly. Under this model, latent correlation maximization is shown to guarantee the extraction of the shared components across views (up to certain ambiguities). In addition, it is further shown that the private information in each view can be provably disentangled from the shared using proper regularization design. A finite sample analysis, which has been rare in nonlinear mixture identifiability study, is also presented. The theoretical results and newly designed regularization are tested on a series of tasks",
    "checked": true,
    "id": "72311c24962cd27b0f7820ece1895f4d416d26ac",
    "semantic_title": "understanding latent correlation-based multiview learning and self-supervision: an identifiability perspective",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=EDeVYpT42oS": {
    "title": "Deconstructing the Inductive Biases of Hamiltonian Neural Networks",
    "volume": "spotlight",
    "abstract": "Physics-inspired neural networks (NNs), such as Hamiltonian or Lagrangian NNs, dramatically outperform other learned dynamics models by leveraging strong inductive biases. These models, however, are challenging to apply to many real world systems, such as those that don't conserve energy or contain contacts, a common setting for robotics and reinforcement learning. In this paper, we examine the inductive biases that make physics-inspired models successful in practice. We show that, contrary to conventional wisdom, the improved generalization of HNNs is the result of modeling acceleration directly and avoiding artificial complexity from the coordinate system, rather than symplectic structure or energy conservation. We show that by relaxing the inductive biases of these models, we can match or exceed performance on energy-conserving systems while dramatically improving performance on practical, non-conservative systems. We extend this approach to constructing transition models for common Mujoco environments, showing that our model can appropriately balance inductive biases with the flexibility required for model-based control",
    "checked": true,
    "id": "3f8dae850dfc1163990f9b513164b42908515a08",
    "semantic_title": "deconstructing the inductive biases of hamiltonian neural networks",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=TrjbxzRcnf-": {
    "title": "Memorizing Transformers",
    "volume": "spotlight",
    "abstract": "Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights. We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate $k$NN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. On benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time",
    "checked": true,
    "id": "0e802c0739771acf70e60d59c2df51cd7e8c50c0",
    "semantic_title": "memorizing transformers",
    "citation_count": 194,
    "authors": []
  },
  "https://openreview.net/forum?id=X8cLTHexYyY": {
    "title": "Learning-Augmented k -means Clustering",
    "volume": "spotlight",
    "abstract": "$k$-means clustering is a well-studied problem due to its wide applicability. Unfortunately, there exist strong theoretical limits on the performance of any algorithm for the $k$-means problem on worst-case inputs. To overcome this barrier, we consider a scenario where ``advice'' is provided to help perform clustering. Specifically, we consider the $k$-means problem augmented with a predictor that, given any point, returns its cluster label in an approximately optimal clustering up to some, possibly adversarial, error. We present an algorithm whose performance improves along with the accuracy of the predictor, even though na\\\"{i}vely following the accurate predictor can still lead to a high clustering cost. Thus if the predictor is sufficiently accurate, we can retrieve a close to optimal clustering with nearly optimal runtime, breaking known computational barriers for algorithms that do not have access to such advice. We evaluate our algorithms on real datasets and show significant improvements in the quality of clustering",
    "checked": true,
    "id": "0d352c530dd041a4c7a525e8b3f0e94f60c8fb34",
    "semantic_title": "learning-augmented k-means clustering",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=SsPCtEY6yCl": {
    "title": "On the Uncomputability of Partition Functions in Energy-Based Sequence Models",
    "volume": "spotlight",
    "abstract": "In this paper, we argue that energy-based sequence models backed by expressive parametric families can result in uncomputable and inapproximable partition functions. Among other things, this makes model selection--and therefore learning model parameters--not only difficult, but generally _undecidable_. The reason is that there are no good deterministic or randomized estimates of partition functions. Specifically, we exhibit a pathological example where under common assumptions, _no_ useful importance sampling estimates of the partition function can guarantee to have variance bounded below a rational number. As alternatives, we consider sequence model families whose partition functions are computable (if they exist), but at the cost of reduced expressiveness. Our theoretical results suggest that statistical procedures with asymptotic guarantees and sheer (but finite) amounts of compute are not the only things that make sequence modeling work; computability concerns must not be neglected as we consider more expressive model parametrizations",
    "checked": true,
    "id": "dc04b92fc0ceec383dd4d262ff759a3cbd4cd3f6",
    "semantic_title": "on the uncomputability of partition functions in energy-based sequence models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=fILj7WpI-g": {
    "title": "Perceiver IO: A General Architecture for Structured Inputs & Outputs",
    "volume": "spotlight",
    "abstract": "A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain & task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence",
    "checked": true,
    "id": "9933a5af7895354087baf6c96b64dc8a8973eaed",
    "semantic_title": "perceiver io: a general architecture for structured inputs & outputs",
    "citation_count": 651,
    "authors": []
  },
  "https://openreview.net/forum?id=POvMvLi91f": {
    "title": "DR3: Value-Based Deep Reinforcement Learning Requires Explicit Regularization",
    "volume": "spotlight",
    "abstract": "Despite overparameterization, deep networks trained via supervised learning are surprisingly easy to optimize and exhibit excellent generalization. One hypothesis to explain this is that overparameterized deep networks enjoy the benefits of implicit regularization induced by stochastic gradient descent, which favors parsimonious solutions that generalize well on test inputs. It is reasonable to surmise that deep reinforcement learning (RL) methods could also benefit from this effect. In this paper, we discuss how the implicit regularization effect of SGD seen in supervised learning could in fact be harmful in the offline deep RL setting, leading to poor generalization and degenerate feature representations. Our theoretical analysis shows that when existing models of implicit regularization are applied to temporal difference learning, the resulting derived regularizer favors degenerate solutions with excessive aliasing, in stark contrast to the supervised learning case. We back up these findings empirically, showing that feature representations learned by a deep network value function trained via bootstrapping can indeed become degenerate, aliasing the representations for state-action pairs that appear on either side of the Bellman backup. To address this issue, we derive the form of this implicit regularizer and, inspired by this derivation, propose a simple and effective explicit regularizer, called DR3, that counteracts the undesirable effects of this implicit regularizer. When combined with existing offline RL methods, DR3 substantially improves performance and stability, alleviating unlearning in Atari 2600 games, D4RL domains and robotic manipulation from images",
    "checked": true,
    "id": "c271b4d25bc184bc94622cef6c9aba80e8e2cce3",
    "semantic_title": "dr3: value-based deep reinforcement learning requires explicit regularization",
    "citation_count": 74,
    "authors": []
  },
  "https://openreview.net/forum?id=iMSjopcOn0p": {
    "title": "MT3: Multi-Task Multitrack Music Transcription",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "8f4bc7e92526faeb65fabd60e5d8c86392fce414",
    "semantic_title": "mt3: multi-task multitrack music transcription",
    "citation_count": 105,
    "authors": []
  },
  "https://openreview.net/forum?id=nHpzE7DqAnG": {
    "title": "Does your graph need a confidence boost? Convergent boosted smoothing on graphs with tabular node features",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "80e9b6a7e9b58be3429cacabf0db0be8fe7fd379",
    "semantic_title": "does your graph need a confidence boost? convergent boosted smoothing on graphs with tabular node features",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=_xwr8gOBeV1": {
    "title": "Geometric and Physical Quantities improve E(3) Equivariant Message Passing",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "d420253d38e08881969ed1e3afbe6e1ef3fe1368",
    "semantic_title": "geometric and physical quantities improve e(3) equivariant message passing",
    "citation_count": 265,
    "authors": []
  },
  "https://openreview.net/forum?id=l3SDgUh7qZO": {
    "title": "SphereFace2: Binary Classification is All You Need for Deep Face Recognition",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "fe67ba856f8610af3dce291c6bd5f65295caa99b",
    "semantic_title": "sphereface2: binary classification is all you need for deep face recognition",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=mHu2vIds_-b": {
    "title": "Boosting Randomized Smoothing with Variance Reduced Classifiers",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "589ee45d3125d49a9d4db72cf97db664dc3dad3a",
    "semantic_title": "boosting randomized smoothing with variance reduced classifiers",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=t5EmXZ3ZLR": {
    "title": "SOSP: Efficiently Capturing Global Correlations by Second-Order Structured Pruning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "7ba86f4a666cb0ba04a4c5c11e0c751cc1fbc204",
    "semantic_title": "sosp: efficiently capturing global correlations by second-order structured pruning",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=8Py-W8lSUgy": {
    "title": "Relational Multi-Task Learning: Modeling Relations between Data and Tasks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "70e192974552e2ace78d8378ef8b7443ab6fd091",
    "semantic_title": "relational multi-task learning: modeling relations between data and tasks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sRZ3GhmegS": {
    "title": "CoBERL: Contrastive BERT for Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "f908112125aaf7505237737b2f08e40dbdd9a110",
    "semantic_title": "coberl: contrastive bert for reinforcement learning",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=qwBK94cP1y": {
    "title": "Optimal Transport for Causal Discovery",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "ee2ee79708fdc3f37924a68993c0f29d4d247fbb",
    "semantic_title": "optimal transport for causal discovery",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=I1hQbx10Kxn": {
    "title": "On Bridging Generic and Personalized Federated Learning for Image Classification",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "36a7bb5f85d79acee1b859588017564367a0ecc6",
    "semantic_title": "on bridging generic and personalized federated learning for image classification",
    "citation_count": 256,
    "authors": []
  },
  "https://openreview.net/forum?id=4-D6CZkRXxI": {
    "title": "Value Gradient weighted Model-Based Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "f0e91a012f447a3ba6692632abbc272cee41d1a0",
    "semantic_title": "value gradient weighted model-based reinforcement learning",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=-llS6TiOew": {
    "title": "Fairness in Representation for Multilingual NLP: Insights from Controlled Experiments on Conditional Language Modeling",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "7a670e9c4cf6655cfbcacf169565d4d645c0d475",
    "semantic_title": "fairness in representation for multilingual nlp: insights from controlled experiments on conditional language modeling",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=YJ1WzgMVsMt": {
    "title": "Reinforcement Learning with Sparse Rewards using Guidance from Offline Demonstration",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "00438218d81c2d50fc96592e16c07ae720440bb6",
    "semantic_title": "reinforcement learning with sparse rewards using guidance from offline demonstration",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=49A1Y6tRhaq": {
    "title": "Linking Emergent and Natural Languages via Corpus Transfer",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "8176556a3fefa7c1b16262ba75499a4643682262",
    "semantic_title": "linking emergent and natural languages via corpus transfer",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=wv6g8fWLX2q": {
    "title": "TAMP-S2GCNets: Coupling Time-Aware Multipersistence Knowledge Representation with Spatio-Supra Graph Convolutional Networks for Time-Series Forecasting",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "ee8d0023ec8e4fc6df19935a94e0d186432d0239",
    "semantic_title": "tamp-s2gcnets: coupling time-aware multipersistence knowledge representation with spatio-supra graph convolutional networks for time-series forecasting",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=K0E_F0gFDgA": {
    "title": "The MultiBERTs: BERT Reproductions for Robustness Analysis",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "5b540745f4b51f95bf90fb3420e51edb037fc51a",
    "semantic_title": "the multiberts: bert reproductions for robustness analysis",
    "citation_count": 97,
    "authors": []
  },
  "https://openreview.net/forum?id=vSix3HPYKSU": {
    "title": "Message Passing Neural PDE Solvers",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "be8d39424a9010bfc0805385cc91edee383c2e24",
    "semantic_title": "message passing neural pde solvers",
    "citation_count": 316,
    "authors": []
  },
  "https://openreview.net/forum?id=Ek7PSN7Y77z": {
    "title": "Multi-Stage Episodic Control for Strategic Exploration in Text Games",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "3b61bc41dff751edbead03aab5e4a1da1aafcc06",
    "semantic_title": "multi-stage episodic control for strategic exploration in text games",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=V3C8p78sDa": {
    "title": "Exploring the Limits of Large Scale Pre-training",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "c206a6e7f51f5e1b6bfc479a174b66ad88ada2db",
    "semantic_title": "exploring the limits of large scale pre-training",
    "citation_count": 129,
    "authors": []
  },
  "https://openreview.net/forum?id=JGO8CvG5S9": {
    "title": "Universal Approximation Under Constraints is Possible with Transformers",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "e1bccf9e3fa257b550e1e1bc477c63ab82d18457",
    "semantic_title": "universal approximation under constraints is possible with transformers",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=hR_SMu8cxCV": {
    "title": "Scaling Laws for Neural Machine Translation",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "de1fdaf92488f2f33ddc0272628c8543778d0da9",
    "semantic_title": "scaling laws for neural machine translation",
    "citation_count": 120,
    "authors": []
  },
  "https://openreview.net/forum?id=8H5bpVwvt5": {
    "title": "AdaRL: What, Where, and How to Adapt in Transfer Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "924656f720c47d7bc215b522c6837a708f54fc45",
    "semantic_title": "adarl: what, where, and how to adapt in transfer reinforcement learning",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=GQjaI9mLet": {
    "title": "Independent SE(3)-Equivariant Models for End-to-End Rigid Protein Docking",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "b190df7856f71856cbd98e1394c6513df279d53e",
    "semantic_title": "independent se(3)-equivariant models for end-to-end rigid protein docking",
    "citation_count": 160,
    "authors": []
  },
  "https://openreview.net/forum?id=0RDcd5Axok": {
    "title": "Towards a Unified View of Parameter-Efficient Transfer Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "43a87867fe6bf4eb920f97fc753be4b727308923",
    "semantic_title": "towards a unified view of parameter-efficient transfer learning",
    "citation_count": 1024,
    "authors": []
  },
  "https://openreview.net/forum?id=BS49l-B5Bql": {
    "title": "GNN-LM: Language Modeling based on Global Contexts via GNN",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "7d1e859fefee1eaac430c38d01cd35003604288b",
    "semantic_title": "gnn-lm: language modeling based on global contexts via gnn",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=metRpM4Zrcb": {
    "title": "Continual Learning with Filter Atom Swapping",
    "volume": "spotlight",
    "abstract": "Continual learning has been widely studied in recent years to resolve the catastrophic forgetting of deep neural networks. In this paper, we first enforce a low-rank filter subspace by decomposing convolutional filters within each network layer over a small set of filter atoms. Then, we perform continual learning with filter atom swapping. In other words, we learn for each task a new filter subspace for each convolutional layer, i.e., hundreds of parameters as filter atoms, but keep subspace coefficients shared across tasks. By maintaining a small footprint memory of filter atoms, we can easily archive models for past tasks to avoid forgetting. The effectiveness of this simple scheme for continual learning is illustrated both empirically and theoretically. The proposed atom swapping framework further enables flexible and efficient model ensemble with members selected within a task or across tasks to improve the performance in different continual learning settings. Being validated on multiple benchmark datasets with different convolutional network structures, the proposed method outperforms the state-of-the-art methods in both accuracy and scalability",
    "checked": true,
    "id": "453118d8a9284a104a111666c6a1f7c331432092",
    "semantic_title": "continual learning with filter atom swapping",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=7YDLgf9_zgm": {
    "title": "Continual Learning with Recursive Gradient Optimization",
    "volume": "spotlight",
    "abstract": "Learning multiple tasks sequentially without forgetting previous knowledge, called Continual Learning(CL), remains a long-standing challenge for neural networks. Most existing methods rely on additional network capacity or data replay. In contrast, we introduce a novel approach which we refer to as Recursive Gradient Optimization(RGO). RGO is composed of an iteratively updated optimizer that modifies the gradient to minimize forgetting without data replay and a virtual Feature Encoding Layer(FEL) that represents different long-term structures with only task descriptors. Experiments demonstrate that RGO has significantly better performance on popular continual classification benchmarks when compared to the baselines and achieves new state-of-the-art performance on 20-split-CIFAR100(82.22%) and 20-split-miniImageNet(72.63%). With higher average accuracy than Single-Task Learning(STL), this method is flexible and reliable to provide continual learning capabilities for learning models that rely on gradient descent",
    "checked": true,
    "id": "088be49337c7a91eafe0eae85592e0f8a80f7e43",
    "semantic_title": "continual learning with recursive gradient optimization",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=g8NJR6fCCl8": {
    "title": "NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning",
    "volume": "spotlight",
    "abstract": "Deployment of machine learning models in real high-risk settings (e.g. healthcare) often depends not only on the model's accuracy but also on its fairness, robustness, and interpretability. Generalized Additive Models (GAMs) are a class of interpretable models with a long history of use in these high-risk domains, but they lack desirable features of deep learning such as differentiability and scalability. In this work, we propose a neural GAM (NODE-GAM) and neural GA$^2$M (NODE-GA$^2$M) that scale well and perform better than other GAMs on large datasets, while remaining interpretable compared to other ensemble and deep learning models. We demonstrate that our models find interesting patterns in the data. Lastly, we show that we are able to improve model accuracy via self-supervised pre-training, an improvement that is not possible for non-differentiable GAMs",
    "checked": true,
    "id": "531a8c8455ffee00fa9ae68fd4bfe3f680f824f2",
    "semantic_title": "node-gam: neural generalized additive model for interpretable deep learning",
    "citation_count": 84,
    "authors": []
  },
  "https://openreview.net/forum?id=dgxFTxuJ50e": {
    "title": "Learnability of convolutional neural networks for infinite dimensional input via mixed and anisotropic smoothness",
    "volume": "spotlight",
    "abstract": "Among a wide range of success of deep learning, convolutional neural networks have been extensively utilized in several tasks such as speech recognition, image processing, and natural language processing, which require inputs with large dimensions. Several studies have investigated function estimation capability of deep learning, but most of them have assumed that the dimensionality of the input is much smaller than the sample size. However, for typical data in applications such as those handled by the convolutional neural networks described above, the dimensionality of inputs is relatively high or even infinite. In this paper, we investigate the approximation and estimation errors of the (dilated) convolutional neural networks when the input is infinite dimensional. Although the approximation and estimation errors of neural networks are affected by the curse of dimensionality in the existing analyses for typical function spaces such as the \\Holder and Besov spaces, we show that, by considering anisotropic smoothness, they can alleviate exponential dependency on the dimensionality but they only depend on the smoothness of the target functions. Our theoretical analysis supports the great practical success of convolutional networks. Furthermore, we show that the dilated convolution is advantageous when the smoothness of the target function has a sparse structure",
    "checked": true,
    "id": "34b6284beb50b0b60afe20632dea33db23ada5e0",
    "semantic_title": "learnability of convolutional neural networks for infinite dimensional input via mixed and anisotropic smoothness",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=tD7eCtaSkR": {
    "title": "Improved deterministic l2 robustness on CIFAR-10 and CIFAR-100",
    "volume": "spotlight",
    "abstract": "Training convolutional neural networks (CNNs) with a strict Lipschitz constraint under the $l_{2}$ norm is useful for provable adversarial robustness, interpretable gradients and stable training. While $1$-Lipschitz CNNs can be designed by enforcing a $1$-Lipschitz constraint on each layer, training such networks requires each layer to have an orthogonal Jacobian matrix (for all inputs) to prevent the gradients from vanishing during backpropagation. A layer with this property is said to be Gradient Norm Preserving (GNP). In this work, we introduce a procedure to certify the robustness of $1$-Lipschitz CNNs by relaxing the orthogonalization of the last linear layer of the network that significantly advances the state of the art for both standard and provable robust accuracies on CIFAR-100 (gains of $4.80\\%$ and $4.71\\%$, respectively). We further boost their robustness by introducing (i) a novel Gradient Norm preserving activation function called the Householder activation function (that includes every $\\mathrm{GroupSort}$ activation) and (ii) a certificate regularization. On CIFAR-10, we achieve significant improvements over prior works in provable robust accuracy ($5.81\\%$) with only a minor drop in standard accuracy ($-0.29\\%$). Code for reproducing all experiments in the paper is available at \\url{https://github.com/singlasahil14/SOC}",
    "checked": true,
    "id": "a3c052386f0fae0c84c6743271ddb7a938fd755c",
    "semantic_title": "improved deterministic l2 robustness on cifar-10 and cifar-100",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=CyKHoKyvgnp": {
    "title": "Transition to Linearity of Wide Neural Networks is an Emerging Property of Assembling Weak Models",
    "volume": "spotlight",
    "abstract": "Wide neural networks with linear output layer have been shown to be near-linear, and to have near-constant neural tangent kernel (NTK), in a region containing the optimization path of gradient descent. These findings seem counter-intuitive since in general neural networks are highly complex models. Why does a linear structure emerge when the neural networks become wide? In this work, we provide a new perspective on this \"transition to linearity\" by considering a neural network as an assembly model recursively built from a set of sub-models corresponding to individual neurons. In this view, we show that the linearity of wide neural networks is, in fact, an emerging property of assembling a large number of diverse ``weak'' sub-models, none of which dominate the assembly",
    "checked": true,
    "id": "e4e153dcd5d261efcf1647264e367c32e446c658",
    "semantic_title": "transition to linearity of wide neural networks is an emerging property of assembling weak models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=RxplU3vmBx": {
    "title": "Looking Back on Learned Experiences For Class/task Incremental Learning",
    "volume": "spotlight",
    "abstract": "Classical deep neural networks are limited in their ability to learn from emerging streams of training data. When trained sequentially on new or evolving tasks, their performance degrades sharply, making them inappropriate in real-world use cases. Existing methods tackle it by either storing old data samples or only updating a parameter set of deep neural networks, which, however, demands a large memory budget or spoils the flexibility of models to learn the incremented task distribution. In this paper, we shed light on an on-call transfer set to provide past experiences whenever a new task arises in the data stream. In particular, we propose a Cost-Free Incremental Learning (CF-IL) not only to replay past experiences the model has learned but also to perform this in a cost free manner. Towards this end, we introduced a memory recovery paradigm in which we query the network to synthesize past exemplars whenever a new task emerges. Thus, our method needs no extra memory for data buffering or network growing, besides calls the proposed memory recovery paradigm to provide past exemplars, named a transfer set in order to mitigate catastrophically forgetting the former tasks in the Incremental Learning (IL) setup. Moreover, in contrast with recently proposed methods, the suggested paradigm does not desire a parallel architecture since it only relies on the learner network. Compared to the state-of-the-art data techniques without buffering past data samples, CF-IL demonstrates significantly better performance on the well-known datasets whether a task oracle is available in test time (Task-IL) or not (Class-IL)",
    "checked": true,
    "id": "95794ba6bc47f35077ab5b1bf3df0c86fd8a7e11",
    "semantic_title": "looking back on learned experiences for class/task incremental learning",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=US2rTP5nm_": {
    "title": "EntQA: Entity Linking as Question Answering",
    "volume": "spotlight",
    "abstract": "A conventional approach to entity linking is to first find mentions in a given document and then infer their underlying entities in the knowledge base. A well-known limitation of this approach is that it requires finding mentions without knowing their entities, which is unnatural and difficult. We present a new model that does not suffer from this limitation called $\\textbf{EntQA}$, which stands for $\\mbox{\\textbf{Ent}ity}$ linking as $\\mbox{\\textbf{Q}uestion}$ $\\mbox{\\textbf{A}nswering}$. EntQA first proposes candidate entities with a fast retrieval module, and then scrutinizes the document to find mentions of each candidate with a powerful reader module. Our approach combines progress in entity linking with that in open-domain question answering and capitalizes on pretrained models for dense entity retrieval and reading comprehension. Unlike in previous works, we do not rely on a mention-candidates dictionary or large-scale weak supervision. EntQA achieves strong results on the GERBIL benchmarking platform",
    "checked": true,
    "id": "f18a9dc44d66346a8d75005f7b5ab9e7e4899de5",
    "semantic_title": "entqa: entity linking as question answering",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=2_vhkAMARk": {
    "title": "Escaping limit cycles: Global convergence for constrained nonconvex-nonconcave minimax problems",
    "volume": "spotlight",
    "abstract": "This paper introduces a new extragradient-type algorithm for a class of nonconvex-nonconcave minimax problems. It is well-known that finding a local solution for general minimax problems is computationally intractable. This observation has recently motivated the study of structures sufficient for convergence of first order methods in the more general setting of variational inequalities when the so-called weak Minty variational inequality (MVI) holds. This problem class captures non-trivial structures as we demonstrate with examples, for which a large family of existing algorithms provably converge to limit cycles. Our results require a less restrictive parameter range in the weak MVI compared to what is previously known, thus extending the applicability of our scheme. The proposed algorithm is applicable to constrained and regularized problems, and involves an adaptive stepsize allowing for potentially larger stepsizes. Our scheme also converges globally even in settings where the underlying operator exhibits limit cycles",
    "checked": false,
    "id": "bfc3c3299eb11f08e68d9f4357f3fbfc981faf0e",
    "semantic_title": "the landscape of the proximal point method for nonconvex‚Äìnonconcave minimax optimization",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=IwJPj2MBcIa": {
    "title": "Compositional Attention: Disentangling Search and Retrieval",
    "volume": "spotlight",
    "abstract": "Multi-head, key-value attention is the backbone of transformer-like model architectures which have proven to be widely successful in recent years. This attention mechanism uses multiple parallel key-value attention blocks (called heads), each performing two fundamental computations: (1) search - selection of a relevant entity from a set via query-key interaction, and (2) retrieval - extraction of relevant features from the selected entity via a value matrix. Standard attention heads learn a rigid mapping between search and retrieval. In this work, we first highlight how this static nature of the pairing can potentially: (a) lead to learning of redundant parameters in certain tasks, and (b) hinder generalization. To alleviate this problem, we propose a novel attention mechanism, called Compositional Attention, that replaces the standard head structure. The proposed mechanism disentangles search and retrieval and composes them in a dynamic, flexible and context-dependent manner. Through a series of numerical experiments, we show that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings. Through our qualitative analysis, we demonstrate that Compositional Attention leads to dynamic specialization based on the type of retrieval needed. Our proposed mechanism generalizes multi-head attention, allows independent scaling of search and retrieval and is easy to implement in a variety of established network architectures",
    "checked": true,
    "id": "b8b813111c411ae61881ab9cd25707d9de6444ec",
    "semantic_title": "compositional attention: disentangling search and retrieval",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=XWODe7ZLn8f": {
    "title": "Contrastive Fine-grained Class Clustering via Generative Adversarial Networks",
    "volume": "spotlight",
    "abstract": "Unsupervised fine-grained class clustering is a practical yet challenging task due to the difficulty of feature representations learning of subtle object details. We introduce C3-GAN, a method that leverages the categorical inference power of InfoGAN with contrastive learning. We aim to learn feature representations that encourage a dataset to form distinct cluster boundaries in the embedding space, while also maximizing the mutual information between the latent code and its image observation. Our approach is to train a discriminator, which is also used for inferring clusters, to optimize the contrastive loss, where image-latent pairs that maximize the mutual information are considered as positive pairs and the rest as negative pairs. Specifically, we map the input of a generator, which was sampled from the categorical distribution, to the embedding space of the discriminator and let them act as a cluster centroid. In this way, C3-GAN succeeded in learning a clustering-friendly embedding space where each cluster is distinctively separable. Experimental results show that C3-GAN achieved the state-of-the-art clustering performance on four fine-grained image datasets, while also alleviating the mode collapse phenomenon. Code is available at https://github.com/naver-ai/c3-gan",
    "checked": true,
    "id": "3d79e7bc943c1e0bed999fbb249e4529a2fb1de9",
    "semantic_title": "contrastive fine-grained class clustering via generative adversarial networks",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=1xXvPrAshao": {
    "title": "Learning Multimodal VAEs through Mutual Supervision",
    "volume": "spotlight",
    "abstract": "Multimodal VAEs seek to model the joint distribution over heterogeneous data (e.g.\\ vision, language), whilst also capturing a shared representation across such modalities. Prior work has typically combined information from the modalities by reconciling idiosyncratic representations directly in the recognition model through explicit products, mixtures, or other such factorisations. Here we introduce a novel alternative, the MEME, that avoids such explicit combinations by repurposing semi-supervised VAEs to combine information between modalities implicitly through mutual supervision. This formulation naturally allows learning from partially-observed data where some modalities can be entirely missing---something that most existing approaches either cannot handle, or do so to a limited extent. We demonstrate that MEME outperforms baselines on standard metrics across both partial and complete observation schemes on the MNIST-SVHN (image--image) and CUB (image--text) datasets. We also contrast the quality of the representations learnt by mutual supervision against standard approaches and observe interesting trends in its ability to capture relatedness between data",
    "checked": true,
    "id": "1362d9c784f210aa2002f7fad16cb5c2efa8b774",
    "semantic_title": "learning multimodal vaes through mutual supervision",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=dEwfxt14bca": {
    "title": "When should agents explore?",
    "volume": "spotlight",
    "abstract": "Exploration remains a central challenge for reinforcement learning (RL). Virtually all existing methods share the feature of a *monolithic* behaviour policy that changes only gradually (at best). In contrast, the exploratory behaviours of animals and humans exhibit a rich diversity, namely including forms of *switching* between modes. This paper presents an initial study of mode-switching, non-monolithic exploration for RL. We investigate different modes to switch between, at what timescales it makes sense to switch, and what signals make for good switching triggers. We also propose practical algorithmic components that make the switching mechanism adaptive and robust, which enables flexibility without an accompanying hyper-parameter-tuning burden. Finally, we report a promising initial study on Atari, using two-mode exploration and switching at sub-episodic time-scales",
    "checked": true,
    "id": "cfabb20df17109036a82934cc12b5c8e92223f6c",
    "semantic_title": "when should agents explore?",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=zz9hXVhf40": {
    "title": "Revisiting Design Choices in Offline Model Based Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "Offline reinforcement learning enables agents to leverage large pre-collected datasets of environment transitions to learn control policies, circumventing the need for potentially expensive or unsafe online data collection. Significant progress has been made recently in offline model-based reinforcement learning, approaches which leverage a learned dynamics model. This typically involves constructing a probabilistic model, and using the model uncertainty to penalize rewards where there is insufficient data, solving for a pessimistic MDP that lower bounds the true MDP. Existing methods, however, exhibit a breakdown between theory and practice, whereby pessimistic return ought to be bounded by the total variation distance of the model from the true dynamics, but is instead implemented through a penalty based on estimated model uncertainty. This has spawned a variety of uncertainty heuristics, with little to no comparison between differing approaches. In this paper, we compare these heuristics, and design novel protocols to investigate their interaction with other hyperparameters, such as the number of models, or imaginary rollout horizon. Using these insights, we show that selecting these key hyperparameters using Bayesian Optimization produces superior configurations that are vastly different to those currently used in existing hand-tuned state-of-the-art methods, and result in drastically stronger performance",
    "checked": true,
    "id": "877e8140cb7f26042f6c5f1eefcf68a2748721f0",
    "semantic_title": "revisiting design choices in offline model based reinforcement learning",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=KhLK0sHMgXK": {
    "title": "NASPY: Automated Extraction of Automated Machine Learning Models",
    "volume": "spotlight",
    "abstract": "We present NASPY, an end-to-end adversarial framework to extract the networkarchitecture of deep learning models from Neural Architecture Search (NAS). Existing works about model extraction attacks mainly focus on conventional DNN models with very simple operations, or require heavy manual analysis with lots of domain knowledge. In contrast, NASPY introduces seq2seq models to automatically identify novel and complicated operations (e.g., separable convolution,dilated convolution) from hardware side-channel sequences. We design two models (RNN-CTC and transformer), which can achieve only 3.2% and 11.3% error rates for operation prediction. We further present methods to recover the model hyper-parameters and topology from the operation sequence . With these techniques, NASPY is able to extract the complete NAS model architecture with high fidelity and automation, which are rarely analyzed before",
    "checked": true,
    "id": "76b2638a986099d9f5092f81e45974319f3daeea",
    "semantic_title": "naspy: automated extraction of automated machine learning models",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=FLA55mBee6Q": {
    "title": "COptiDICE: Offline Constrained Reinforcement Learning via Stationary Distribution Correction Estimation",
    "volume": "spotlight",
    "abstract": "We consider the offline constrained reinforcement learning (RL) problem, in which the agent aims to compute a policy that maximizes expected return while satisfying given cost constraints, learning only from a pre-collected dataset. This problem setting is appealing in many real-world scenarios, where direct interaction with the environment is costly or risky, and where the resulting policy should comply with safety constraints. However, it is challenging to compute a policy that guarantees satisfying the cost constraints in the offline RL setting, since the off-policy evaluation inherently has an estimation error. In this paper, we present an offline constrained RL algorithm that optimizes the policy in the space of the stationary distribution. Our algorithm, COptiDICE, directly estimates the stationary distribution corrections of the optimal policy with respect to returns, while constraining the cost upper bound, with the goal of yielding a cost-conservative policy for actual constraint satisfaction. Experimental results show that COptiDICE attains better policies in terms of constraint satisfaction and return-maximization, outperforming baseline algorithms",
    "checked": true,
    "id": "d1a596b5dce1ff183a763cdd1610a95f7f05e170",
    "semantic_title": "coptidice: offline constrained reinforcement learning via stationary distribution correction estimation",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=nhnJ3oo6AB": {
    "title": "Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers",
    "volume": "spotlight",
    "abstract": "We propose to address quadrupedal locomotion tasks using Reinforcement Learning (RL) with a Transformer-based model that learns to combine proprioceptive information and high-dimensional depth sensor inputs. While learning-based locomotion has made great advances using RL, most methods still rely on domain randomization for training blind agents that generalize to challenging terrains. Our key insight is that proprioceptive states only offer contact measurements for immediate reaction, whereas an agent equipped with visual sensory observations can learn to proactively maneuver environments with obstacles and uneven terrain by anticipating changes in the environment many steps ahead. In this paper, we introduce LocoTransformer, an end-to-end RL method that leverages both proprioceptive states and visual observations for locomotion control. We evaluate our method in challenging simulated environments with different obstacles and uneven terrain. We transfer our learned policy from simulation to a real robot by running it indoor and in-the-wild with unseen obstacles and terrain. Our method not only significantly improves over baselines, but also achieves far better generalization performance, especially when transferred to the real robot. Our project page with videos is at https://rchalyang.github.io/LocoTransformer/",
    "checked": true,
    "id": "065dc8e953d5e5da138c33a8c3f4f7181b19cd54",
    "semantic_title": "learning vision-guided quadrupedal locomotion end-to-end with cross-modal transformers",
    "citation_count": 123,
    "authors": []
  },
  "https://openreview.net/forum?id=dwg5rXg1WS_": {
    "title": "ViTGAN: Training GANs with Vision Transformers",
    "volume": "spotlight",
    "abstract": "Recently, Vision Transformers (ViTs) have shown competitive performance on image recognition while requiring less vision-specific inductive biases. In this paper, we investigate if such performance can be extended to image generation. To this end, we integrate the ViT architecture into generative adversarial networks (GANs). For ViT discriminators, we observe that existing regularization methods for GANs interact poorly with self-attention, causing serious instability during training. To resolve this issue, we introduce several novel regularization techniques for training GANs with ViTs. For ViT generators, we examine architectural choices for latent and pixel mapping layers to faciliate convergence. Empirically, our approach, named ViTGAN, achieves comparable performance to the leading CNN- based GAN models on three datasets: CIFAR-10, CelebA, and LSUN bedroom",
    "checked": true,
    "id": "bd163f27b409a4d903632009d38df77cfd70a437",
    "semantic_title": "vitgan: training gans with vision transformers",
    "citation_count": 199,
    "authors": []
  },
  "https://openreview.net/forum?id=AJsI-ymaKn_": {
    "title": "POETREE: Interpretable Policy Learning with Adaptive Decision Trees",
    "volume": "spotlight",
    "abstract": "Building models of human decision-making from observed behaviour is critical to better understand, diagnose and support real-world policies such as clinical care. As established policy learning approaches remain focused on imitation performance, they fall short of explaining the demonstrated decision-making process. Policy Extraction through decision Trees (POETREE) is a novel framework for interpretable policy learning, compatible with fully-offline and partially-observable clinical decision environments -- and builds probabilistic tree policies determining physician actions based on patients' observations and medical history. Fully-differentiable tree architectures are grown incrementally during optimization to adapt their complexity to the modelling task, and learn a representation of patient history through recurrence, resulting in decision tree policies that adapt over time with patient information. This policy learning method outperforms the state-of-the-art on real and synthetic medical datasets, both in terms of understanding, quantifying and evaluating observed behaviour as well as in accurately replicating it -- with potential to improve future decision support systems",
    "checked": true,
    "id": "3986a3070cde3354e9d80faff70d72c328434032",
    "semantic_title": "poetree: interpretable policy learning with adaptive decision trees",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=iEvAf8i6JjO": {
    "title": "TRGP: Trust Region Gradient Projection for Continual Learning",
    "volume": "spotlight",
    "abstract": "Catastrophic forgetting is one of the major challenges in continual learning. To address this issue, some existing methods put restrictive constraints on the optimization space of the new task for minimizing the interference to old tasks. However, this may lead to unsatisfactory performance for the new task, especially when the new task is strongly correlated with old tasks. To tackle this challenge, we propose Trust Region Gradient Projection (TRGP) for continual learning to facilitate the forward knowledge transfer based on an efficient characterization of task correlation. Particularly, we introduce a notion of 'trust region' to select the most related old tasks for the new task in a layer-wise and single-shot manner, using the norm of gradient projection onto the subspace spanned by task inputs. Then, a scaled weight projection is proposed to cleverly reuse the frozen weights of the selected old tasks in the trust region through a layer-wise scaling matrix. By jointly optimizing the scaling matrices and the model, where the model is updated along the directions orthogonal to the subspaces of old tasks, TRGP can effectively prompt knowledge transfer without forgetting. Extensive experiments show that our approach achieves significant improvement over related state-of-the-art methods",
    "checked": true,
    "id": "b895a5c79406c8db2807e78b00fdeadc80e5a774",
    "semantic_title": "trgp: trust region gradient projection for continual learning",
    "citation_count": 93,
    "authors": []
  },
  "https://openreview.net/forum?id=g5ynW-jMq4M": {
    "title": "Properties from mechanisms: an equivariance perspective on identifiable representation learning",
    "volume": "spotlight",
    "abstract": "A key goal of unsupervised representation learning is ``inverting'' a data generating process to recover its latent properties. Existing work that provably achieves this goal relies on strong assumptions on relationships between the latent variables (e.g., independence conditional on auxiliary information). In this paper, we take a very different perspective on the problem and ask, ``Can we instead identify latent properties by leveraging knowledge of the mechanisms that govern their evolution?'' We provide a complete characterization of the sources of non-identifiability as we vary knowledge about a set of possible mechanisms. In particular, we prove that if we know the exact mechanisms under which the latent properties evolve, then identification can be achieved up to any equivariances that are shared by the underlying mechanisms. We generalize this characterization to settings where we only know some hypothesis class over possible mechanisms, as well as settings where the mechanisms are stochastic. We demonstrate the power of this mechanism-based perspective by showing that we can leverage our results to generalize existing identifiable representation learning results. These results suggest that by exploiting inductive biases on mechanisms, it is possible to design a range of new identifiable representation learning approaches",
    "checked": true,
    "id": "bb12ccf1860719560eb41a5e69ce420de6438c57",
    "semantic_title": "properties from mechanisms: an equivariance perspective on identifiable representation learning",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=dUV91uaXm3": {
    "title": "Revisiting Over-smoothing in BERT from the Perspective of Graph",
    "volume": "spotlight",
    "abstract": "Recently over-smoothing phenomenon of Transformer-based models is observed in both vision and language fields. However, no existing work has delved deeper to further investigate the main cause of this phenomenon. In this work, we make the attempt to analyze the over-smoothing problem from the perspective of graph, where such problem was first discovered and explored. Intuitively, the self-attention matrix can be seen as a normalized adjacent matrix of a corresponding graph. Based on the above connection, we provide some theoretical analysis and find that layer normalization plays a key role in the over-smoothing issue of Transformer-based models. Specifically, if the standard deviation of layer normalization is sufficiently large, the output of Transformer stacks will converge to a specific low-rank subspace and result in over-smoothing. To alleviate the over-smoothing problem, we consider hierarchical fusion strategies, which combine the representations from different layers adaptively to make the output more diverse. Extensive experiment results on various data sets illustrate the effect of our fusion method",
    "checked": true,
    "id": "383116b2685d0c7ae9669ac929b628b9af0af5f3",
    "semantic_title": "revisiting over-smoothing in bert from the perspective of graph",
    "citation_count": 82,
    "authors": []
  },
  "https://openreview.net/forum?id=XEW8CQgArno": {
    "title": "Training invariances and the low-rank phenomenon: beyond linear networks",
    "volume": "spotlight",
    "abstract": "The implicit bias induced by the training of neural networks has become a topic of rigorous study. In the limit of gradient flow and gradient descent with appropriate step size, it has been shown that when one trains a deep linear network with logistic or exponential loss on linearly separable data, the weights converge to rank-$1$ matrices. In this paper, we extend this theoretical result to the last few linear layers of the much wider class of nonlinear ReLU-activated feedforward networks containing fully-connected layers and skip connections. Similar to the linear case, the proof relies on specific local training invariances, sometimes referred to as alignment, which we show to hold for submatrices where neurons are stably-activated in all training examples, and it reflects empirical results in the literature. We also show this is not true in general for the full matrix of ReLU fully-connected layers. Our proof relies on a specific decomposition of the network into a multilinear function and another ReLU network whose weights are constant under a certain parameter directional convergence",
    "checked": true,
    "id": "ee14966a8aea312b1af6569108f09440922fb704",
    "semantic_title": "training invariances and the low-rank phenomenon: beyond linear networks",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=lpkGn3k2YdD": {
    "title": "Learning Long-Term Reward Redistribution via Randomized Return Decomposition",
    "volume": "spotlight",
    "abstract": "Many practical applications of reinforcement learning require agents to learn from sparse and delayed rewards. It challenges the ability of agents to attribute their actions to future outcomes. In this paper, we consider the problem formulation of episodic reinforcement learning with trajectory feedback. It refers to an extreme delay of reward signals, in which the agent can only obtain one reward signal at the end of each trajectory. A popular paradigm for this problem setting is learning with a designed auxiliary dense reward function, namely proxy reward, instead of sparse environmental signals. Based on this framework, this paper proposes a novel reward redistribution algorithm, randomized return decomposition (RRD), to learn a proxy reward function for episodic reinforcement learning. We establish a surrogate problem by Monte-Carlo sampling that scales up least-squares-based reward redistribution to long-horizon problems. We analyze our surrogate loss function by connection with existing methods in the literature, which illustrates the algorithmic properties of our approach. In experiments, we extensively evaluate our proposed method on a variety of benchmark tasks with episodic rewards and demonstrate substantial improvement over baseline algorithms",
    "checked": true,
    "id": "80110db20124242316a13795e964d09cd15a3802",
    "semantic_title": "learning long-term reward redistribution via randomized return decomposition",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=siCt4xZn5Ve": {
    "title": "What Happens after SGD Reaches Zero Loss? --A Mathematical Framework",
    "volume": "spotlight",
    "abstract": "Understanding the implicit bias of Stochastic Gradient Descent (SGD) is one of the key challenges in deep learning, especially for overparametrized models, where the local minimizers of the loss function $L$ can form a manifold. Intuitively, with a sufficiently small learning rate $\\eta$, SGD tracks Gradient Descent (GD) until it gets close to such manifold, where the gradient noise prevents further convergence. In such regime, Blanc et al. (2020) proved that SGD with label noise locally decreases a regularizer-like term, the sharpness of loss, $\\text{tr}[\\nabla^2 L]$. The current paper gives a general framework for such analysis by adapting ideas from Katzenberger (1991). It allows in principle a complete characterization for the regularization effect of SGD around such manifold---i.e., the \"implicit bias\"---using a stochastic differential equation (SDE) describing the limiting dynamics of the parameters, which is determined jointly by the loss function and the noise covariance. This yields some new results: (1) a *global* analysis of the implicit bias valid for $\\eta^{-2}$ steps, in contrast to the local analysis of Blanc et al. (2020) that is only valid for $\\eta^{-1.6}$ steps and (2) allowing *arbitrary* noise covariance. As an application, we show with arbitrary large initialization, label noise SGD can always escape the kernel regime and only requires $O(\\kappa\\ln d)$ samples for learning an $\\kappa$-sparse overparametrized linear model in $\\mathbb{R}^d$ (Woodworth et al., 2020), while GD initialized in the kernel regime requires $\\Omega(d)$ samples. This upper bound is minimax optimal and improves the previous $\\widetilde{O}(\\kappa^2)$ upper bound (HaoChen et al., 2020)",
    "checked": true,
    "id": "40c115c43adee6bc7a00ffd444ee9c045360d97d",
    "semantic_title": "what happens after sgd reaches zero loss? -a mathematical framework",
    "citation_count": 108,
    "authors": []
  },
  "https://openreview.net/forum?id=45L_dgP48Vd": {
    "title": "Graph-Augmented Normalizing Flows for Anomaly Detection of Multiple Time Series",
    "volume": "spotlight",
    "abstract": "Anomaly detection is a widely studied task for a broad variety of data types; among them, multiple time series appear frequently in applications, including for example, power grids and traffic networks. Detecting anomalies for multiple time series, however, is a challenging subject, owing to the intricate interdependencies among the constituent series. We hypothesize that anomalies occur in low density regions of a distribution and explore the use of normalizing flows for unsupervised anomaly detection, because of their superior quality in density estimation. Moreover, we propose a novel flow model by imposing a Bayesian network among constituent series. A Bayesian network is a directed acyclic graph (DAG) that models causal relationships; it factorizes the joint probability of the series into the product of easy-to-evaluate conditional probabilities. We call such a graph-augmented normalizing flow approach GANF and propose joint estimation of the DAG with flow parameters. We conduct extensive experiments on real-world datasets and demonstrate the effectiveness of GANF for density estimation, anomaly detection, and identification of time series distribution drift",
    "checked": true,
    "id": "c25975cc81949931f79ffb135354c474681d1ccc",
    "semantic_title": "graph-augmented normalizing flows for anomaly detection of multiple time series",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=z1-I6rOKv1S": {
    "title": "Autoregressive Quantile Flows for Predictive Uncertainty Estimation",
    "volume": "spotlight",
    "abstract": "Numerous applications of machine learning involve representing probability distributions over high-dimensional data. We propose autoregressive quantile flows, a flexible class of normalizing flow models trained using a novel objective based on proper scoring rules. Our objective does not require calculating computationally expensive determinants of Jacobians during training and supports new types of neural architectures, such as neural autoregressive flows from which it is easy to sample. We leverage these models in quantile flow regression, an approach that parameterizes predictive conditional distributions with flows, resulting in improved probabilistic predictions on tasks such as time series forecasting and object detection. Our novel objective functions and neural flow parameterizations also yield improvements on popular generation and density estimation tasks, and represent a step beyond maximum likelihood learning of flows",
    "checked": true,
    "id": "f877161dc4c0ac262aceda2f7f0c46589b2df7bd",
    "semantic_title": "autoregressive quantile flows for predictive uncertainty estimation",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=DNRADop4ksB": {
    "title": "On the Importance of Firth Bias Reduction in Few-Shot Classification",
    "volume": "spotlight",
    "abstract": "Learning accurate classifiers for novel categories from very few examples, known as few-shot image classification, is a challenging task in statistical machine learning and computer vision. The performance in few-shot classification suffers from the bias in the estimation of classifier parameters; however, an effective underlying bias reduction technique that could alleviate this issue in training few-shot classifiers has been overlooked. In this work, we demonstrate the effectiveness of Firth bias reduction in few-shot classification. Theoretically, Firth bias reduction removes the $O(N^{-1})$ first order term from the small-sample bias of the Maximum Likelihood Estimator. Here we show that the general Firth bias reduction technique simplifies to encouraging uniform class assignment probabilities for multinomial logistic classification, and almost has the same effect in cosine classifiers. We derive an easy-to-implement optimization objective for Firth penalized multinomial logistic and cosine classifiers, which is equivalent to penalizing the cross-entropy loss with a KL-divergence between the predictions and the uniform label distribution. Then, we empirically evaluate that it is consistently effective across the board for few-shot image classification, regardless of (1) the feature representations from different backbones, (2) the number of samples per class, and (3) the number of classes. Furthermore, we demonstrate the effectiveness of Firth bias reduction on cross-domain and imbalanced data settings. Our implementation is available at https://github.com/ehsansaleh/firth_bias_reduction",
    "checked": true,
    "id": "1729f308d7874f7a51a19571d5c96c3cc1a53d4d",
    "semantic_title": "on the importance of firth bias reduction in few-shot classification",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=04pGUg0-pdZ": {
    "title": "Finite-Time Convergence and Sample Complexity of Multi-Agent Actor-Critic Reinforcement Learning with Average Reward",
    "volume": "spotlight",
    "abstract": "In this paper, we establish the first finite-time convergence result of the actor-critic algorithm for fully decentralized multi-agent reinforcement learning (MARL) problems with average reward. In this problem, a set of $N$ agents work cooperatively to maximize the global average reward through interacting with their neighbors over a communication network. We consider a practical MARL setting, where the rewards and actions of each agent are only known to itself, and the knowledge of joint actions of the agents is not assumed. Toward this end, we propose a mini-batch Markovian sampled fully decentralized actor-critic algorithm and analyze its finite-time convergence and sample complexity. We show that the sample complexity of this algorithm is $\\mathcal{O}(N^{2}/\\epsilon^{2}\\log(N/\\epsilon))$. Interestingly, this sample complexity bound matches that of the state-of-the-art single-agent actor-critic algorithms for reinforcement learning",
    "checked": true,
    "id": "43b3b2148b8593c407fb3d5dd9578efc5212203f",
    "semantic_title": "finite-time convergence and sample complexity of multi-agent actor-critic reinforcement learning with average reward",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=ieNJYujcGDO": {
    "title": "Towards Understanding the Data Dependency of Mixup-style Training",
    "volume": "spotlight",
    "abstract": "In the Mixup training paradigm, a model is trained using convex combinations of data points and their associated labels. Despite seeing very few true data points during training, models trained using Mixup seem to still minimize the original empirical risk and exhibit better generalization and robustness on various tasks when compared to standard training. In this paper, we investigate how these benefits of Mixup training rely on properties of the data in the context of classification. For minimizing the original empirical risk, we compute a closed form for the Mixup-optimal classification, which allows us to construct a simple dataset on which minimizing the Mixup loss leads to learning a classifier that does not minimize the empirical loss on the data. On the other hand, we also give sufficient conditions for Mixup training to also minimize the original empirical risk. For generalization, we characterize the margin of a Mixup classifier, and use this to understand why the decision boundary of a Mixup classifier can adapt better to the full structure of the training data when compared to standard training. In contrast, we also show that, for a large class of linear models and linearly separable datasets, Mixup training leads to learning the same classifier as standard training",
    "checked": true,
    "id": "e19e9cf2aa468267f1f1f1075840d6b77f594c14",
    "semantic_title": "towards understanding the data dependency of mixup-style training",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=oDFvtxzPOx": {
    "title": "Self-Supervision Enhanced Feature Selection with Correlated Gates",
    "volume": "spotlight",
    "abstract": "Discovering relevant input features for predicting a target variable is a key scientific question. However, in many domains, such as medicine and biology, feature selection is confounded by a scarcity of labeled samples coupled with significant correlations among features. In this paper, we propose a novel deep learning approach to feature selection that addresses both challenges simultaneously. First, we pre-train the network using unlabeled samples within a self-supervised learning framework by solving pretext tasks that require the network to learn informative representations from partial feature sets. Then, we fine-tune the pre-trained network to discover relevant features using labeled samples. During both training phases, we explicitly account for the correlation structure of the input features by generating correlated gate vectors from a multivariate Bernoulli distribution. Experiments on multiple real-world datasets including clinical and omics demonstrate that our model discovers relevant features that provide superior prediction performance compared to the state-of-the-art benchmarks in practical scenarios where there is often limited labeled data and high correlations among features",
    "checked": true,
    "id": "3f5692b1f79fdebc1a92e8868504d0d08c9ee056",
    "semantic_title": "self-supervision enhanced feature selection with correlated gates",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=CzceR82CYc": {
    "title": "Score-Based Generative Modeling with Critically-Damped Langevin Diffusion",
    "volume": "spotlight",
    "abstract": "Score-based generative models (SGMs) have demonstrated remarkable synthesis quality. SGMs rely on a diffusion process that gradually perturbs the data towards a tractable distribution, while the generative model learns to denoise. The complexity of this denoising task is, apart from the data distribution itself, uniquely determined by the diffusion process. We argue that current SGMs employ overly simplistic diffusions, leading to unnecessarily complex denoising processes, which limit generative modeling performance. Based on connections to statistical mechanics, we propose a novel critically-damped Langevin diffusion (CLD) and show that CLD-based SGMs achieve superior performance. CLD can be interpreted as running a joint diffusion in an extended space, where the auxiliary variables can be considered \"velocities\" that are coupled to the data variables as in Hamiltonian dynamics. We derive a novel score matching objective for CLD and show that the model only needs to learn the score function of the conditional distribution of the velocity given data, an easier task than learning scores of the data directly. We also derive a new sampling scheme for efficient synthesis from CLD-based diffusion models. We find that CLD outperforms previous SGMs in synthesis quality for similar network architectures and sampling compute budgets. We show that our novel sampler for CLD significantly outperforms solvers such as Euler‚ÄìMaruyama. Our framework provides new insights into score-based denoising diffusion models and can be readily used for high-resolution image synthesis. Project page and code: https://nv-tlabs.github.io/CLD-SGM",
    "checked": true,
    "id": "a28cdccba07dbf977795e15ff2c9b7ec80dac050",
    "semantic_title": "score-based generative modeling with critically-damped langevin diffusion",
    "citation_count": 259,
    "authors": []
  },
  "https://openreview.net/forum?id=DIjCrlsu6Z": {
    "title": "Controlling Directions Orthogonal to a Classifier",
    "volume": "spotlight",
    "abstract": "We propose to identify directions invariant to a given classifier so that these directions can be controlled in tasks such as style transfer. While orthogonal decomposition is directly identifiable when the given classifier is linear, we formally define a notion of orthogonality in the non-linear case. We also provide a surprisingly simple method for constructing the orthogonal classifier (a classifier utilizing directions other than those of the given classifier). Empirically, we present three use cases where controlling orthogonal variation is important: style transfer, domain adaptation, and fairness. The orthogonal classifier enables desired style transfer when domains vary in multiple aspects, improves domain adaptation with label shifts and mitigates the unfairness as a predictor. The code is available at https://github.com/Newbeeer/orthogonal_classifier",
    "checked": true,
    "id": "c2c26d7e6b3679cd0a48ca8eace3ffed24f273a3",
    "semantic_title": "controlling directions orthogonal to a classifier",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=2eXhNpHeW6E": {
    "title": "R5: Rule Discovery with Reinforced and Recurrent Relational Reasoning",
    "volume": "spotlight",
    "abstract": "Systematicity, i.e., the ability to recombine known parts and rules to form new sequences while reasoning over relational data, is critical to machine intelligence. A model with strong systematicity is able to train on small-scale tasks and generalize to large-scale tasks. In this paper, we propose R5, a relational reasoning framework based on reinforcement learning that reasons over relational graph data and explicitly mines underlying compositional logical rules from observations. R5 has strong systematicity and being robust to noisy data. It consists of a policy value network equipped with Monte Carlo Tree Search to perform recurrent relational prediction and a backtrack rewriting mechanism for rule mining. By alternately applying the two components, R5 progressively learns a set of explicit rules from data and performs explainable and generalizable relation prediction. We conduct extensive evaluations on multiple datasets. Experimental results show that R5 outperforms various embedding-based and rule induction baselines on relation prediction tasks while achieving a high recall rate in discovering ground truth rules",
    "checked": true,
    "id": "477e4f98e03a88628b86605e10c9d2f9fbc73a0e",
    "semantic_title": "r5: rule discovery with reinforced and recurrent relational reasoning",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=J4iSIR9fhY0": {
    "title": "Representation Learning for Online and Offline RL in Low-rank MDPs",
    "volume": "spotlight",
    "abstract": "This work studies the question of Representation Learning in RL: how can we learn a compact low-dimensional representation such that on top of the representation we can perform RL procedures such as exploration and exploitation, in a sample efficient manner. We focus on the low-rank Markov Decision Processes (MDPs) where the transition dynamics correspond to a low-rank transition matrix. Unlike prior works that assume the representation is known (e.g., linear MDPs), here we need to learn the representation for the low-rank MDP. We study both the online RL and offline RL settings. For the online setting, operating with the same computational oracles used in FLAMBE (Agarwal et.al), the state-of-art algorithm for learning representations in low-rank MDPs, we propose an algorithm REP-UCB Upper Confidence Bound driven Representation learning for RL), which significantly improves the sample complexity from $\\widetilde{O}( A^9 d^7 / (\\epsilon^{10} (1-\\gamma)^{22}))$ for FLAMBE to $\\widetilde{O}( A^4 d^4 / (\\epsilon^2 (1-\\gamma)^{2}) )$ with $d$ being the rank of the transition matrix (or dimension of the ground truth representation), $A$ being the number of actions, and $\\gamma$ being the discounted factor. Notably, REP-UCB is simpler than FLAMBE, as it directly balances the interplay between representation learning, exploration, and exploitation, while FLAMBE is an explore-then-commit style approach and has to perform reward-free exploration step-by-step forward in time. For the offline RL setting, we develop an algorithm that leverages pessimism to learn under a partial coverage condition: our algorithm is able to compete against any policy as long as it is covered by the offline distribution",
    "checked": true,
    "id": "8d16ffb11c7b62181146db43296852424426a3cd",
    "semantic_title": "representation learning for online and offline rl in low-rank mdps",
    "citation_count": 132,
    "authors": []
  },
  "https://openreview.net/forum?id=X_hByk2-5je": {
    "title": "Lossless Compression with Probabilistic Circuits",
    "volume": "spotlight",
    "abstract": "Despite extensive progress on image generation, common deep generative model architectures are not easily applied to lossless compression. For example, VAEs suffer from a compression cost overhead due to their latent variables. This overhead can only be partially eliminated with elaborate schemes such as bits-back coding, often resulting in poor single-sample compression rates. To overcome such problems, we establish a new class of tractable lossless compression models that permit efficient encoding and decoding: Probabilistic Circuits (PCs). These are a class of neural networks involving $|p|$ computational units that support efficient marginalization over arbitrary subsets of the $D$ feature dimensions, enabling efficient arithmetic coding. We derive efficient encoding and decoding schemes that both have time complexity $\\mathcal{O} (\\log(D) \\cdot |p|)$, where a naive scheme would have linear costs in $D$ and $|p|$, making the approach highly scalable. Empirically, our PC-based (de)compression algorithm runs 5-40 times faster than neural compression algorithms that achieve similar bitrates. By scaling up the traditional PC structure learning pipeline, we achieve state-of-the-art results on image datasets such as MNIST. Furthermore, PCs can be naturally integrated with existing neural compression algorithms to improve the performance of these base models on natural image datasets. Our results highlight the potential impact that non-standard learning architectures may have on neural data compression",
    "checked": true,
    "id": "43157e4f2e4c7f85cdfa9e4a92eff7b3bcecb2c7",
    "semantic_title": "lossless compression with probabilistic circuits",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=T8vZHIRTrY": {
    "title": "Understanding Domain Randomization for Sim-to-real Transfer",
    "volume": "spotlight",
    "abstract": "Reinforcement learning encounters many challenges when applied directly in the real world. Sim-to-real transfer is widely used to transfer the knowledge learned from simulation to the real world. Domain randomization---one of the most popular algorithms for sim-to-real transfer---has been demonstrated to be effective in various tasks in robotics and autonomous driving. Despite its empirical successes, theoretical understanding on why this simple algorithm works is largely missing. In this paper, we propose a theoretical framework for sim-to-real transfers, in which the simulator is modeled as a set of MDPs with tunable parameters (corresponding to unknown physical parameters such as friction). We provide sharp bounds on the sim-to-real gap---the difference between the value of policy returned by domain randomization and the value of an optimal policy for the real world. We prove that sim-to-real transfer can succeed under mild conditions without any real-world training samples. Our theory also highlights the importance of using memory (i.e., history-dependent policies) in domain randomization. Our proof is based on novel techniques that reduce the problem of bounding the sim-to-real gap to the problem of designing efficient learning algorithms for infinite-horizon MDPs, which we believe are of independent interest",
    "checked": true,
    "id": "2340e5b64224b66c10e603898c0cc24c8e2793c3",
    "semantic_title": "understanding domain randomization for sim-to-real transfer",
    "citation_count": 133,
    "authors": []
  },
  "https://openreview.net/forum?id=7F9cOhdvfk_": {
    "title": "$\\mathrm{SO}(2)$-Equivariant Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "Equivariant neural networks enforce symmetry within the structure of their convolutional layers, resulting in a substantial improvement in sample efficiency when learning an equivariant or invariant function. Such models are applicable to robotic manipulation learning which can often be formulated as a rotationally symmetric problem. This paper studies equivariant model architectures in the context of $Q$-learning and actor-critic reinforcement learning. We identify equivariant and invariant characteristics of the optimal $Q$-function and the optimal policy and propose equivariant DQN and SAC algorithms that leverage this structure. We present experiments that demonstrate that our equivariant versions of DQN and SAC can be significantly more sample efficient than competing algorithms on an important class of robotic manipulation problems",
    "checked": true,
    "id": "347131d36cda1c921158bb6928a488ab13bd26e5",
    "semantic_title": "so(2)-equivariant reinforcement learning",
    "citation_count": 88,
    "authors": []
  },
  "https://openreview.net/forum?id=CuV_qYkmKb3": {
    "title": "Scarf: Self-Supervised Contrastive Learning using Random Feature Corruption",
    "volume": "spotlight",
    "abstract": "Self-supervised contrastive representation learning has proved incredibly successful in the vision and natural language domains, enabling state-of-the-art performance with orders of magnitude less labeled data. However, such methods are domain-specific and little has been done to leverage this technique on real-world \\emph{tabular} datasets. We propose \\textsc{Scarf}, a simple, widely-applicable technique for contrastive learning, where views are formed by corrupting a random subset of features. When applied to pre-train deep neural networks on the 69 real-world, tabular classification datasets from the OpenML-CC18 benchmark, \\textsc{Scarf} not only improves classification accuracy in the fully-supervised setting but does so also in the presence of label noise and in the semi-supervised setting where only a fraction of the available training data is labeled. We show that \\textsc{Scarf} complements existing strategies and outperforms alternatives like autoencoders. We conduct comprehensive ablations, detailing the importance of a range of factors",
    "checked": true,
    "id": "08bd0ebcf5e0cd08a9748683692678c36dee9c07",
    "semantic_title": "scarf: self-supervised contrastive learning using random feature corruption",
    "citation_count": 194,
    "authors": []
  },
  "https://openreview.net/forum?id=sOK-zS6WHB": {
    "title": "Responsible Disclosure of Generative Models Using Scalable Fingerprinting",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "5a64123f1564401dabf5db45d6f1c27d7b0c6c6c",
    "semantic_title": "responsible disclosure of generative models using scalable fingerprinting",
    "citation_count": 100,
    "authors": []
  },
  "https://openreview.net/forum?id=JSR-YDImK95": {
    "title": "Path Auxiliary Proposal for MCMC in Discrete Space",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "dfd10d51c4e7dd93e93f1cb2459e839e0ac1eb51",
    "semantic_title": "path auxiliary proposal for mcmc in discrete space",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=7b4zxUnrO2N": {
    "title": "Possibility Before Utility: Learning And Using Hierarchical Affordances",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "6343543986bd0b2800f0ef468604b659f0a24ec5",
    "semantic_title": "possibility before utility: learning and using hierarchical affordances",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=DfMqlB0PXjM": {
    "title": "Interpretable Unsupervised Diversity Denoising and Artefact Removal",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "b28841064bd35ae71b7bf4fc117e87f0bd734aaa",
    "semantic_title": "interpretable unsupervised diversity denoising and artefact removal",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=HTx7vrlLBEj": {
    "title": "Half-Inverse Gradients for Physical Deep Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "49228c2db9ba4d622f9351c83d67a3a86b22f3ec",
    "semantic_title": "half-inverse gradients for physical deep learning",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=X_ch3VrNSRg": {
    "title": "EE-Net: Exploitation-Exploration Neural Networks in Contextual Bandits",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "64839c2bbfcf40d4ece312da523906d177d56c3c",
    "semantic_title": "ee-net: exploitation-exploration neural networks in contextual bandits",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=iMH1e5k7n3L": {
    "title": "Spike-inspired rank coding for fast and accurate recurrent neural networks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "2c26ad7eea22b40b849f0fc7a6e29030cfbf578a",
    "semantic_title": "spike-inspired rank coding for fast and accurate recurrent neural networks",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=W9G_ImpHlQd": {
    "title": "How to Robustify Black-Box ML Models? A Zeroth-Order Optimization Perspective",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "7d56ea205f9a8f83225569e3acf4c6c8c5b0977e",
    "semantic_title": "how to robustify black-box ml models? a zeroth-order optimization perspective",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=FEDfGWVZYIn": {
    "title": "RelaxLoss: Defending Membership Inference Attacks without Losing Utility",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "8dd7861ffb563d7768c7b77a5988bd4354be73a3",
    "semantic_title": "relaxloss: defending membership inference attacks without losing utility",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=eBS-3YiaIL-": {
    "title": "Analyzing and Improving the Optimization Landscape of Noise-Contrastive Estimation",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "08bfc2eb24636087f2be571bcb39e99ba3930e57",
    "semantic_title": "analyzing and improving the optimization landscape of noise-contrastive estimation",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=mmUA7_O9mjY": {
    "title": "Contact Points Discovery for Soft-Body Manipulations with Differentiable Physics",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "e3dd531e8aaec2966cfdc9f7e00703157658d1c1",
    "semantic_title": "contact points discovery for soft-body manipulations with differentiable physics",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=cmt-6KtR4c4": {
    "title": "Leveraging Automated Unit Tests for Unsupervised Code Translation",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "1aed58bd07026492194672adec494dc37c894a28",
    "semantic_title": "leveraging automated unit tests for unsupervised code translation",
    "citation_count": 128,
    "authors": []
  },
  "https://openreview.net/forum?id=BB4e8Atc1eR": {
    "title": "Scalable Sampling for Nonsymmetric Determinantal Point Processes",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "126808451f9dfc57abaab3a9f6a467824113c51f",
    "semantic_title": "scalable sampling for nonsymmetric determinantal point processes",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=FRxhHdnxt1": {
    "title": "Amortized Tree Generation for Bottom-up Synthesis Planning and Synthesizable Molecular Design",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "59186f8ca5ffc029c365bfe4fe6386f753d21694",
    "semantic_title": "amortized tree generation for bottom-up synthesis planning and synthesizable molecular design",
    "citation_count": 66,
    "authors": []
  },
  "https://openreview.net/forum?id=LI2bhrE_2A": {
    "title": "Iterative Refinement Graph Neural Network for Antibody Sequence-Structure Co-design",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "01bacd0a631f11fd090b5cc68596fccc7d58b4e0",
    "semantic_title": "iterative refinement graph neural network for antibody sequence-structure co-design",
    "citation_count": 152,
    "authors": []
  },
  "https://openreview.net/forum?id=HbtFCX2PLq0": {
    "title": "Churn Reduction via Distillation",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "a695e233173b89d9e400d8de0934a068af8c1f4b",
    "semantic_title": "churn reduction via distillation",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=7twQI5VnC8": {
    "title": "Learning Causal Models from Conditional Moment Restrictions by Importance Weighting",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "face5fecc4c3ccd9874c41e5704ed27bf8539f7e",
    "semantic_title": "learning causal models from conditional moment restrictions by importance weighting",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=hfU7Ka5cfrC": {
    "title": "Scalable One-Pass Optimisation of High-Dimensional Weight-Update Hyperparameters by Implicit Differentiation",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "6d3b5211d6d0ebd17e34ce3eee25321f492d0ee0",
    "semantic_title": "scalable one-pass optimisation of high-dimensional weight-update hyperparameters by implicit differentiation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vrW3tvDfOJQ": {
    "title": "Sample Efficient Deep Reinforcement Learning via Uncertainty Estimation",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "6f987340adf4f0b90cf900d3e219341cf46248d5",
    "semantic_title": "sample efficient deep reinforcement learning via uncertainty estimation",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=O1DEtITim__": {
    "title": "Learning Pruning-Friendly Networks via Frank-Wolfe: One-Shot, Any-Sparsity, And No Retraining",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "55c235224f2c15f076184c3c65337d36aafa460f",
    "semantic_title": "learning pruning-friendly networks via frank-wolfe: one-shot, any-sparsity, and no retraining",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=qTHBE7E9iej": {
    "title": "Learning transferable motor skills with hierarchical latent mixture policies",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "c85662dcd17eed4452019b640a30a323970472ef",
    "semantic_title": "learning transferable motor skills with hierarchical latent mixture policies",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=gPvB4pdu_Z": {
    "title": "Compositional Training for End-to-End Deep AUC Maximization",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "d140fecf5c1582b588717c88b55001106b25bd31",
    "semantic_title": "compositional training for end-to-end deep auc maximization",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=45Mr7LeKR9": {
    "title": "Explanations of Black-Box Models based on Directional Feature Interactions",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "a4ab2885279de2a34e7554aff01ce4c2a8512cf3",
    "semantic_title": "explanations of black-box models based on directional feature interactions",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=Fl3Mg_MZR-": {
    "title": "On Lottery Tickets and Minimal Task Representations in Deep Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "054756e00b52512b432c1aae9b847f84aa6b64c9",
    "semantic_title": "on lottery tickets and minimal task representations in deep reinforcement learning",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=4AZz9osqrar": {
    "title": "Self-supervised Learning is More Robust to Dataset Imbalance",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "a01ac66f5f66a2b23152f631b920972e4407275c",
    "semantic_title": "self-supervised learning is more robust to dataset imbalance",
    "citation_count": 175,
    "authors": []
  },
  "https://openreview.net/forum?id=apv504XsysP": {
    "title": "Ab-Initio Potential Energy Surfaces by Pairing GNNs with Neural Wave Functions",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "5db6ef158cb0ca101b4d482f4f01ef94b7e7628f",
    "semantic_title": "ab-initio potential energy surfaces by pairing gnns with neural wave functions",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=SidzxAb9k30": {
    "title": "Near-Optimal Reward-Free Exploration for Linear Mixture MDPs with Plug-in Solver",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "5b8392ca789edfad543c3f27ca6221b6cb434713",
    "semantic_title": "near-optimal reward-free exploration for linear mixture mdps with plug-in solver",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=MEpKGLsY8f": {
    "title": "Meta Discovery: Learning to Discover Novel Classes given Very Limited Data",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "2967375a78cbf845c2119b50dceb957d50d205e9",
    "semantic_title": "meta discovery: learning to discover novel classes given very limited data",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=PRZoSmCinhf": {
    "title": "Constrained Policy Optimization via Bayesian World Models",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "f1e48bfb4464fedb94ced2d85b74991efcfe2856",
    "semantic_title": "constrained policy optimization via bayesian world models",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=OIs3SxU5Ynl": {
    "title": "VAE Approximation Error: ELBO and Exponential Families",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "8bca798d8eb54a2e6d692eb253166697703d9c9c",
    "semantic_title": "vae approximation error: elbo and exponential families",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=CAjxVodl_v": {
    "title": "Generalized Decision Transformer for Offline Hindsight Information Matching",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "387a17823d7c47c0bd3390a124708933032989e0",
    "semantic_title": "generalized decision transformer for offline hindsight information matching",
    "citation_count": 108,
    "authors": []
  },
  "https://openreview.net/forum?id=1HxTO6CTkz": {
    "title": "Unifying Likelihood-free Inference with Black-box Optimization and Beyond",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "375e3c06366be62c96e0339ab6cf889eb0bec068",
    "semantic_title": "unifying likelihood-free inference with black-box optimization and beyond",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=AJAR-JgNw__": {
    "title": "DEPTS: Deep Expansion Learning for Periodic Time Series Forecasting",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "d62fdf39a501f79df7c514a023e02a4dd0bcc967",
    "semantic_title": "depts: deep expansion learning for periodic time series forecasting",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=tBtoZYKd9n": {
    "title": "Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "95b7bd36ba9fbfa3f9e0a1cd4b3f1660d6780297",
    "semantic_title": "evaluation metrics for graph generative models: problems, pitfalls, and practical solutions",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=tV3N0DWMxCg": {
    "title": "Natural Posterior Network: Deep Bayesian Predictive Uncertainty for Exponential Family Distributions",
    "volume": "spotlight",
    "abstract": "Uncertainty awareness is crucial to develop reliable machine learning models. In this work, we propose the Natural Posterior Network (NatPN) for fast and high-quality uncertainty estimation for any task where the target distribution belongs to the exponential family. Thus, NatPN finds application for both classification and general regression settings. Unlike many previous approaches, NatPN does not require out-of-distribution (OOD) data at training time. Instead, it leverages Normalizing Flows to fit a single density on a learned low-dimensional and task-dependent latent space. For any input sample, NatPN uses the predicted likelihood to perform a Bayesian update over the target distribution. Theoretically, NatPN assigns high uncertainty far away from training data. Empirically, our extensive experiments on calibration and OOD detection show that NatPN delivers highly competitive performance for classification, regression and count prediction tasks",
    "checked": true,
    "id": "b5ad8e8d9ab0ea57f448fe58847d5bce3ce5cb0c",
    "semantic_title": "natural posterior network: deep bayesian predictive uncertainty for exponential family distributions",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=KxbhdyiPHE": {
    "title": "Learning Altruistic Behaviours in Reinforcement Learning without External Rewards",
    "volume": "spotlight",
    "abstract": "Can artificial agents learn to assist others in achieving their goals without knowing what those goals are? Generic reinforcement learning agents could be trained to behave altruistically towards others by rewarding them for altruistic behaviour, i.e., rewarding them for benefiting other agents in a given situation. Such an approach assumes that other agents' goals are known so that the altruistic agent can cooperate in achieving those goals. However, explicit knowledge of other agents' goals is often difficult to acquire. In the case of human agents, their goals and preferences may be difficult to express fully; they might be ambiguous or even contradictory. Thus, it is beneficial to develop agents that do not depend on external supervision and learn altruistic behaviour in a task-agnostic manner. We propose to act altruistically towards other agents by giving them more choice and allowing them to achieve their goals better. Some concrete examples include opening a door for others or safeguarding them to pursue their objectives without interference. We formalize this concept and propose an altruistic agent that learns to increase the choices another agent has by preferring to maximize the number of states that the other agent can reach in its future. We evaluate our approach in three different multi-agent environments where another agent's success depends on altruistic behaviour. Finally, we show that our unsupervised agents can perform comparably to agents explicitly trained to work cooperatively, in some cases even outperforming them",
    "checked": true,
    "id": "4c5b2f64b661957a90945e58e8f35a8ca829d887",
    "semantic_title": "learning altruistic behaviours in reinforcement learning without external rewards",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=wQfgfb8VKTn": {
    "title": "Context-Aware Sparse Deep Coordination Graphs",
    "volume": "spotlight",
    "abstract": "Learning sparse coordination graphs adaptive to the coordination dynamics among agents is a long-standing problem in cooperative multi-agent learning. This paper studies this problem and proposes a novel method using the variance of payoff functions to construct context-aware sparse coordination topologies. We theoretically consolidate our method by proving that the smaller the variance of payoff functions is, the less likely action selection will change after removing the corresponding edge. Moreover, we propose to learn action representations to effectively reduce the influence of payoff functions' estimation errors on graph construction. To empirically evaluate our method, we present the Multi-Agent COordination (MACO) benchmark by collecting classic coordination problems in the literature, increasing their difficulty, and classifying them into different types. We carry out a case study and experiments on the MACO and StarCraft II micromanagement benchmark to demonstrate the dynamics of sparse graph learning, the influence of graph sparseness, and the learning performance of our method",
    "checked": true,
    "id": "3e277a778533737fe1fb7f4dbfc6c0c15b02e92d",
    "semantic_title": "context-aware sparse deep coordination graphs",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xDIvIqQ3DXD": {
    "title": "On the approximation properties of recurrent encoder-decoder architectures",
    "volume": "spotlight",
    "abstract": "Encoder-decoder architectures have recently gained popularity in sequence to sequence modelling, featuring in state-of-the-art models such as transformers. However, a mathematical understanding of their working principles still remains limited. In this paper, we study the approximation properties of recurrent encoder-decoder architectures. Prior work established theoretical results for RNNs in the linear setting, where approximation capabilities can be related to smoothness and memory of target temporal relationships. Here, we uncover that the encoder and decoder together form a particular \"temporal product structure\" which determines the approximation efficiency. Moreover, the encoder-decoder architecture generalises RNNs with the capability to learn time-inhomogeneous relationships. Our results provide the theoretical understanding of approximation properties of the recurrent encoder-decoder architecture, which precisely characterises, in the considered setting, the types of temporal relationships that can be efficiently learned",
    "checked": true,
    "id": "765a88e682672dc1478737a4df8503c9230d2b1d",
    "semantic_title": "on the approximation properties of recurrent encoder-decoder architectures",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=Nfl-iXa-y7R": {
    "title": "Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models",
    "volume": "spotlight",
    "abstract": "Overparameterized neural networks generalize well but are expensive to train. Ideally one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty in sparsifying all model components. The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is difficult and expensive. To address this, our main insight is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. As butterfly matrices are not hardware efficient, we propose simple variants of butterfly (block and flat) to take advantage of modern hardware. Our method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP). We empirically validate that Pixelated Butterfly is $3\\times$ faster than Butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs. On the ImageNet classification and WikiText-103 language modeling tasks, our sparse models train up to 2.3$\\times$ faster than the dense MLP-Mixer, Vision Transformer, and GPT-2 small with no drop in accuracy",
    "checked": true,
    "id": "90b21dbad8969b74d704eed15a3d98722a88e464",
    "semantic_title": "pixelated butterfly: simple and efficient sparse training for neural network models",
    "citation_count": 83,
    "authors": []
  },
  "https://openreview.net/forum?id=shpkpVXzo3h": {
    "title": "8-bit Optimizers via Block-wise Quantization",
    "volume": "spotlight",
    "abstract": "Stateful optimizers maintain gradient statistics over time, e.g., the exponentially smoothed sum (SGD with momentum) or squared sum (Adam) of past gradient values. This state can be used to accelerate optimization significantly, compared to plain stochastic gradient descent, but uses memory that might otherwise be allocated to model parameters, thereby limiting the maximum size of models trained in practice. In this paper, we develop the first optimizers that use 8-bit statistics while maintaining the performance levels of using 32-bit optimizer states. To overcome the resulting computational, quantization, and stability challenges, we develop block-wise dynamic quantization. Block-wise quantization divides input tensors into smaller blocks that are independently quantized. Each block is processed in parallel across cores, yielding faster optimization and high precision quantization. To maintain stability and performance, we combine block-wise quantization with two additional changes: (1) dynamic quantization, a form of non-linear optimization that is precise for both large and small magnitude values, and (2) a stable embedding layer to reduce gradient variance that comes from the highly non-uniform distribution of input tokens in language models. As a result, our 8-bit optimizers maintain 32-bit performance with a small fraction of the memory footprint on a range of tasks, including 1.5B parameter language modeling, GLUE finetuning, ImageNet classification, WMT'14 machine translation, MoCo v2 contrastive ImageNet pretraining+finetuning, and RoBERTa pretraining, without changes to the original optimizer hyperparameters. We open-source our 8-bit optimizers as a drop-in replacement that only requires a two-line code change",
    "checked": true,
    "id": "11fe37ab6faf6bf85ad2f5746c154dec5412bd04",
    "semantic_title": "8-bit optimizers via block-wise quantization",
    "citation_count": 335,
    "authors": []
  },
  "https://openreview.net/forum?id=yeP_zx9vqNm": {
    "title": "Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks",
    "volume": "spotlight",
    "abstract": "Recent work suggests that feature constraints in the training datasets of deep neural networks (DNNs) drive robustness to adversarial noise (Ilyas et al., 2019). The representations learned by such adversarially robust networks have also been shown to be more human perceptually-aligned than non-robust networks via image manipulations (Santurkar et al., 2019, Engstrom et al., 2019). Despite appearing closer to human visual perception, it is unclear if the constraints in robust DNN representations match biological constraints found in human vision. Human vision seems to rely on texture-based/summary statistic representations in the periphery, which have been shown to explain phenomena such as crowding (Balas et al., 2009) and performance on visual search tasks (Rosenholtz et al., 2012). To understand how adversarially robust optimizations/representations compare to human vision, we performed a psychophysics experiment using a metamer task similar to Freeman \\& Simoncelli, 2011, Wallis et al., 2016 and Deza et al., 2019 where we evaluated how well human observers could distinguish between images synthesized to match adversarially robust representations compared to non-robust representations and a texture synthesis model of peripheral vision (Texforms a la Long et al., 2018). We found that the discriminability of robust representation and texture model images decreased to near chance performance as stimuli were presented farther in the periphery. Moreover, performance on robust and texture-model images showed similar trends within participants, while performance on non-robust representations changed minimally across the visual field. These results together suggest that (1) adversarially robust representations capture peripheral computation better than non-robust representations and (2) robust representations capture peripheral computation similar to current state-of-the-art texture peripheral vision models. More broadly, our findings support the idea that localized texture summary statistic representations may drive human invariance to adversarial perturbations and that the incorporation of such representations in DNNs could give rise to useful properties like adversarial robustness",
    "checked": true,
    "id": "c875d546411c598df075ae555fbb3108fea02910",
    "semantic_title": "finding biological plausibility for adversarially robust features via metameric tasks",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=DmpCfq6Mg39": {
    "title": "Omni-Dimensional Dynamic Convolution",
    "volume": "spotlight",
    "abstract": "Learning a single static convolutional kernel in each convolutional layer is the common training paradigm of modern Convolutional Neural Networks (CNNs). Instead, recent research in dynamic convolution shows that learning a linear combination of n convolutional kernels weighted with their input-dependent attentions can significantly improve the accuracy of light-weight CNNs, while maintaining efficient inference. However, we observe that existing works endow convolutional kernels with the dynamic property through one dimension (regarding the convolutional kernel number) of the kernel space, but the other three dimensions (regarding the spatial size, the input channel number and the output channel number for each convolutional kernel) are overlooked. Inspired by this, we present Omni-dimensional Dynamic Convolution (ODConv), a more generalized yet elegant dynamic convolution design, to advance this line of research. ODConv leverages a novel multi-dimensional attention mechanism with a parallel strategy to learn complementary attentions for convolutional kernels along all four dimensions of the kernel space at any convolutional layer. As a drop-in replacement of regular convolutions, ODConv can be plugged into many CNN architectures. Extensive experiments on the ImageNet and MS-COCO datasets show that ODConv brings solid accuracy boosts for various prevailing CNN backbones including both light-weight and large ones, e.g., 3.77%~5.71%|1.86%~3.72% absolute top-1 improvements to MobivleNetV2|ResNet family on the ImageNet dataset. Intriguingly, thanks to its improved feature learning ability, ODConv with even one single kernel can compete with or outperform existing dynamic convolution counterparts with multiple kernels, substantially reducing extra parameters. Furthermore, ODConv is also superior to other attention modules for modulating the output features or the convolutional weights. Code and models will be available at https://github.com/OSVAI/ODConv",
    "checked": true,
    "id": "77e8c2d78638056cd7a347d7c6e36406ca176dbd",
    "semantic_title": "omni-dimensional dynamic convolution",
    "citation_count": 280,
    "authors": []
  },
  "https://openreview.net/forum?id=BjyvwnXXVn_": {
    "title": "EViT: Expediting Vision Transformers via Token Reorganizations",
    "volume": "spotlight",
    "abstract": "Vision Transformers (ViTs) take all the image patches as tokens and construct multi-head self-attention (MHSA) among them. Complete leverage of these image tokens brings redundant computations since not all the tokens are attentive in MHSA. Examples include that tokens containing semantically meaningless or distractive image backgrounds do not positively contribute to the ViT predictions. In this work, we propose to reorganize image tokens during the feed-forward process of ViT models, which is integrated into ViT during training. For each forward inference, we identify the attentive image tokens between MHSA and FFN (i.e., feed-forward network) modules, which is guided by the corresponding class token attention. Then, we reorganize image tokens by preserving attentive image tokens and fusing inattentive ones to expedite subsequent MHSA and FFN computations. To this end, our method EViT improves ViTs from two perspectives. First, under the same amount of input image tokens, our method reduces MHSA and FFN computation for efficient inference. For instance, the inference speed of DeiT-S is increased by 50% while its recognition accuracy is decreased by only 0.3% for ImageNet classification. Second, by maintaining the same computational cost, our method empowers ViTs to take more image tokens as input for recognition accuracy improvement, where the image tokens are from higher resolution images. An example is that we improve the recognition accuracy of DeiT-S by 1% for ImageNet classification at the same computational cost of a vanilla DeiT-S. Meanwhile, our method does not introduce more parameters to ViTs. Experiments on the standard benchmarks show the effectiveness of our method. The code is available at https://github.com/youweiliang/evit",
    "checked": true,
    "id": "293535c2b0ef674e1ed9a7ba227e37cca35e5e4b",
    "semantic_title": "evit: expediting vision transformers via token reorganizations",
    "citation_count": 105,
    "authors": []
  },
  "https://openreview.net/forum?id=wENMvIsxNN": {
    "title": "D-CODE: Discovering Closed-form ODEs from Observed Trajectories",
    "volume": "spotlight",
    "abstract": "For centuries, scientists have manually designed closed-form ordinary differential equations (ODEs) to model dynamical systems. An automated tool to distill closed-form ODEs from observed trajectories would accelerate the modeling process. Traditionally, symbolic regression is used to uncover a closed-form prediction function $a=f(b)$ with label-feature pairs $(a_i, b_i)$ as training examples. However, an ODE models the time derivative $\\dot{x}(t)$ of a dynamical system, e.g. $\\dot{x}(t) = f(x(t),t)$, and the \"label\" $\\dot{x}(t)$ is usually *not* observed. The existing ways to bridge this gap only perform well for a narrow range of settings with low measurement noise, frequent sampling, and non-chaotic dynamics. In this work, we propose the Discovery of Closed-form ODE framework (D-CODE), which advances symbolic regression beyond the paradigm of supervised learning. D-CODE leverages a novel objective function based on the variational formulation of ODEs to bypass the unobserved time derivative. For formal justification, we prove that this objective is a valid proxy for the estimation error of the true (but unknown) ODE. In the experiments, D-CODE successfully discovered the governing equations of a diverse range of dynamical systems under challenging measurement settings with high noise and infrequent sampling",
    "checked": true,
    "id": "fd05d3d6dbed753b0f4b081a986ba5d63d958833",
    "semantic_title": "d-code: discovering closed-form odes from observed trajectories",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=w60btE_8T2m": {
    "title": "Spanning Tree-based Graph Generation for Molecules",
    "volume": "spotlight",
    "abstract": "In this paper, we explore the problem of generating molecules using deep neural networks, which has recently gained much interest in chemistry. To this end, we propose a spanning tree-based graph generation (STGG) framework based on formulating molecular graph generation as a construction of a spanning tree and the residual edges. Such a formulation exploits the sparsity of molecular graphs and allows using compact tree-constructive operations to define the molecular graph connectivity. Based on the intermediate graph structure of the construction process, our framework can constrain its generation to molecular graphs that satisfy the chemical valence rules. We also newly design a Transformer architecture with tree-based relative positional encodings for realizing the tree construction procedure. Experiments on QM9, ZINC250k, and MOSES benchmarks verify the effectiveness of the proposed framework in metrics such as validity, Frechet ChemNet distance, and fragment similarity. We also demonstrate the usefulness of STGG in maximizing penalized LogP value of molecules",
    "checked": true,
    "id": "2761e45bb1901c3723cd7af9fa9f6deb65ff22fc",
    "semantic_title": "spanning tree-based graph generation for molecules",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=bERaNdoegnO": {
    "title": "Policy improvement by planning with Gumbel",
    "volume": "spotlight",
    "abstract": "AlphaZero is a powerful reinforcement learning algorithm based on approximate policy iteration and tree search. However, AlphaZero can fail to improve its policy network, if not visiting all actions at the root of a search tree. To address this issue, we propose a policy improvement algorithm based on sampling actions without replacement. Furthermore, we use the idea of policy improvement to replace the more heuristic mechanisms by which AlphaZero selects and uses actions, both at root nodes and at non-root nodes. Our new algorithms, Gumbel AlphaZero and Gumbel MuZero, respectively without and with model-learning, match the state of the art on Go, chess, and Atari, and significantly improve prior performance when planning with few simulations",
    "checked": true,
    "id": "93e598ac56f29ed2638f393af9c93eec0ea07c1a",
    "semantic_title": "policy improvement by planning with gumbel",
    "citation_count": 67,
    "authors": []
  },
  "https://openreview.net/forum?id=t8O-4LKFVx": {
    "title": "Learning Optimal Conformal Classifiers",
    "volume": "spotlight",
    "abstract": "Modern deep learning based classifiers show very high accuracy on test data but this does not provide sufficient guarantees for safe deployment, especially in high-stake AI applications such as medical diagnosis. Usually, predictions are obtained without a reliable uncertainty estimate or a formal guarantee. Conformal prediction (CP) addresses these issues by using the classifier's predictions, e.g., its probability estimates, to predict confidence sets containing the true class with a user-specified probability. However, using CP as a separate processing step after training prevents the underlying model from adapting to the prediction of confidence sets. Thus, this paper explores strategies to differentiate through CP during training with the goal of training model with the conformal wrapper end-to-end. In our approach, conformal training (ConfTr), we specifically \"simulate\" conformalization on mini-batches during training. Compared to standard training, ConfTr reduces the average confidence set size (inefficiency) of state-of-the-art CP methods applied after training. Moreover, it allows to \"shape\" the confidence sets predicted at test time, which is difficult for standard CP. On experiments with several datasets, we show ConfTr can influence how inefficiency is distributed across classes, or guide the composition of confidence sets in terms of the included classes, while retaining the guarantees offered by CP",
    "checked": true,
    "id": "7157c8eb81d972094230d28d0817004e3acf4148",
    "semantic_title": "learning optimal conformal classifiers",
    "citation_count": 102,
    "authors": []
  },
  "https://openreview.net/forum?id=9Vrb9D0WI4": {
    "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
    "volume": "spotlight",
    "abstract": "Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely unseen tasks specified in natural language. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several datasets, often outperforming models 16√ó its size. Further, our model attains strong performance on a subset of tasks from the BIG-Bench benchmark, outperforming models 6√ó its size. All trained models are available at https://github.com/bigscience-workshop/t-zero, and all prompts are available at https://github.com/bigscience-workshop/promptsource",
    "checked": true,
    "id": "fe0825f9ddb1cccb545f4249da55b6b55e577bbd",
    "semantic_title": "multitask prompted training enables zero-shot task generalization",
    "citation_count": 152,
    "authors": []
  },
  "https://openreview.net/forum?id=57PipS27Km": {
    "title": "Continuous-Time Meta-Learning with Forward Mode Differentiation",
    "volume": "spotlight",
    "abstract": "Drawing inspiration from gradient-based meta-learning methods with infinitely small gradient steps, we introduce Continuous-Time Meta-Learning (COMLN), a meta-learning algorithm where adaptation follows the dynamics of a gradient vector field. Specifically, representations of the inputs are meta-learned such that a task-specific linear classifier is obtained as a solution of an ordinary differential equation (ODE). Treating the learning process as an ODE offers the notable advantage that the length of the trajectory is now continuous, as opposed to a fixed and discrete number of gradient steps. As a consequence, we can optimize the amount of adaptation necessary to solve a new task using stochastic gradient descent, in addition to learning the initial conditions as is standard practice in gradient-based meta-learning. Importantly, in order to compute the exact meta-gradients required for the outer-loop updates, we devise an efficient algorithm based on forward mode differentiation, whose memory requirements do not scale with the length of the learning trajectory, thus allowing longer adaptation in constant memory. We provide analytical guarantees for the stability of COMLN, we show empirically its efficiency in terms of runtime and memory usage, and we illustrate its effectiveness on a range of few-shot image classification problems",
    "checked": true,
    "id": "2947873e49f3185cf39be950c4f8603a1fa901cd",
    "semantic_title": "continuous-time meta-learning with forward mode differentiation",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=zXM0b4hi5_B": {
    "title": "On the relation between statistical learning and perceptual distances",
    "volume": "spotlight",
    "abstract": "It has been demonstrated many times that the behavior of the human visual system is connected to the statistics of natural images. Since machine learning relies on the statistics of training data as well, the above connection has interesting implications when using perceptual distances (which mimic the behavior of the human visual system) as a loss function. In this paper, we aim to unravel the non-trivial relationships between the probability distribution of the data, perceptual distances, and unsupervised machine learning. To this end, we show that perceptual sensitivity is correlated with the probability of an image in its close neighborhood. We also explore the relation between distances induced by autoencoders and the probability distribution of the training data, as well as how these induced distances are correlated with human perception. Finally, we find perceptual distances do not always lead to noticeable gains in performance over Euclidean distance in common image processing tasks, except when data is scarce and the perceptual distance provides regularization. We propose this may be due to a double-counting effect of the image statistics, once in the perceptual distance and once in the training procedure",
    "checked": true,
    "id": "600b3057346e0fa1e1afb59a179acbcb4a679bb6",
    "semantic_title": "on the relation between statistical learning and perceptual distances",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=vA7doMdgi75": {
    "title": "Implicit Bias of Projected Subgradient Method Gives Provable Robust Recovery of Subspaces of Unknown Codimension",
    "volume": "spotlight",
    "abstract": "Robust subspace recovery (RSR) is the problem of learning a subspace from sample data points corrupted by outliers. Dual Principal Component Pursuit (DPCP) is a robust subspace recovery method that aims to find a basis for the orthogonal complement of the subspace by minimizing the sum of the distances of the points to the subspaces subject to orthogonality constraints on the basis. Prior work has shown that DPCP can provably recover the correct subspace in the presence of outliers as long as the true dimension of the subspace is known. In this paper, we show that if the orthogonality constraints --adopted in previous DPCP formulations-- are relaxed and random initialization is used instead of spectral one, DPCP can provably recover a subspace of \\emph{unknown dimension}. Specifically, we propose a very simple algorithm based on running multiple instances of a projected sub-gradient descent method (PSGM), with each problem instance seeking to find one vector in the null space of the subspace. We theoretically prove that under mild conditions this approach succeeds with high probability. In particular, we show that 1) all of the problem instances will converge to a vector in the nullspace of the subspace and 2) the ensemble of problem instance solutions will be sufficiently diverse to fully span the nullspace of the subspace thus also revealing its true unknown codimension. We provide empirical results that corroborate our theoretical results and showcase the remarkable implicit rank regularization behavior of the PSGM algorithm that allows us to perform RSR without knowing the subspace dimension",
    "checked": true,
    "id": "f1057950e69b736c35a4fe9c013083d6118811ed",
    "semantic_title": "implicit bias of projected subgradient method gives provable robust recovery of subspaces of unknown codimension",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=MkTPtnjeYTV": {
    "title": "On the Optimal Memorization Power of ReLU Neural Networks",
    "volume": "spotlight",
    "abstract": "We study the memorization power of feedforward ReLU neural networks. We show that such networks can memorize any $N$ points that satisfy a mild separability assumption using $\\tilde{O}\\left(\\sqrt{N}\\right)$ parameters. Known VC-dimension upper bounds imply that memorizing $N$ samples requires $\\Omega(\\sqrt{N})$ parameters, and hence our construction is optimal up to logarithmic factors. We also give a generalized construction for networks with depth bounded by $1 \\leq L \\leq \\sqrt{N}$, for memorizing $N$ samples using $\\tilde{O}(N/L)$ parameters. This bound is also optimal up to logarithmic factors. Our construction uses weights with large bit complexity. We prove that having such a large bit complexity is both necessary and sufficient for memorization with a sub-linear number of parameters",
    "checked": true,
    "id": "ded2a06120007faacd9bdd4ff39fec65b5756b44",
    "semantic_title": "on the optimal memorization power of relu neural networks",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=6Tk2noBdvxt": {
    "title": "Programmatic Reinforcement Learning without Oracles",
    "volume": "spotlight",
    "abstract": "Deep reinforcement learning (RL) has led to encouraging successes in many challenging control tasks. However, a deep RL model lacks interpretability due to the difficulty of identifying how the model's control logic relates to its network structure. Programmatic policies structured in more interpretable representations emerge as a promising solution. Yet two shortcomings remain: First, synthesizing programmatic policies requires optimizing over the discrete and non-differentiable search space of program architectures. Previous works are suboptimal because they only enumerate program architectures greedily guided by a pretrained RL oracle. Second, these works do not exploit compositionality, an important programming concept, to reuse and compose primitive functions to form a complex function for new tasks. Our first contribution is a programmatically interpretable RL framework that conducts program architecture search on top of a continuous relaxation of the architecture space defined by programming language grammar rules. Our algorithm allows policy architectures to be learned with policy parameters via bilevel optimization using efficient policy-gradient methods, and thus does not require a pretrained oracle. Our second contribution is improving programmatic policies to support compositionality by integrating primitive functions learned to grasp task-agnostic skills as a composite program to solve novel RL problems. Experiment results demonstrate that our algorithm excels in discovering optimal programmatic policies that are highly interpretable. The code of this work is available at https://github.com/RU-Automated-Reasoning-Group/pi-PRL",
    "checked": true,
    "id": "77b2b707bc416293053d4244d57a035b78444e80",
    "semantic_title": "programmatic reinforcement learning without oracles",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=LtKcMgGOeLt": {
    "title": "When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations",
    "volume": "spotlight",
    "abstract": "Vision Transformers (ViTs) and MLPs signal further efforts on replacing hand-wired features or inductive biases with general-purpose neural architectures. Existing works empower the models by massive data, such as large-scale pre-training and/or repeated strong data augmentations, and still report optimization-related problems (e.g., sensitivity to initialization and learning rates). Hence, this paper investigates ViTs and MLP-Mixers from the lens of loss geometry, intending to improve the models' data efficiency at training and generalization at inference. Visualization and Hessian reveal extremely sharp local minima of converged models. By promoting smoothness with a recently proposed sharpness-aware optimizer, we substantially improve the accuracy and robustness of ViTs and MLP-Mixers on various tasks spanning supervised, adversarial, contrastive, and transfer learning (e.g., +5.3\\% and +11.0\\% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively, with the simple Inception-style preprocessing). We show that the improved smoothness attributes to sparser active neurons in the first few layers. The resultant ViTs outperform ResNets of similar size and throughput when trained from scratch on ImageNet without large-scale pre-training or strong data augmentations. Model checkpoints are available at \\url{https://github.com/google-research/vision_transformer}",
    "checked": true,
    "id": "42a7015e48a1e00b70ebb442a82afb4b10017c0b",
    "semantic_title": "when vision transformers outperform resnets without pretraining or strong data augmentations",
    "citation_count": 349,
    "authors": []
  },
  "https://openreview.net/forum?id=01AMRlen9wJ": {
    "title": "Online Hyperparameter Meta-Learning with Hypergradient Distillation",
    "volume": "spotlight",
    "abstract": "Many gradient-based meta-learning methods assume a set of parameters that do not participate in inner-optimization, which can be considered as hyperparameters. Although such hyperparameters can be optimized using the existing gradient-based hyperparameter optimization (HO) methods, they suffer from the following issues. Unrolled differentiation methods do not scale well to high-dimensional hyperparameters or horizon length, Implicit Function Theorem (IFT) based methods are restrictive for online optimization, and short horizon approximations suffer from short horizon bias. In this work, we propose a novel HO method that can overcome these limitations, by approximating the second-order term with knowledge distillation. Specifically, we parameterize a single Jacobian-vector product (JVP) for each HO step and minimize the distance from the true second-order term. Our method allows online optimization and also is scalable to the hyperparameter dimension and the horizon length. We demonstrate the effectiveness of our method on three different meta-learning methods and two benchmark datasets",
    "checked": true,
    "id": "dcd3b04dd8f5d7849e010251d1bc9835b5e66f3f",
    "semantic_title": "online hyperparameter meta-learning with hypergradient distillation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LBvk4QWIUpm": {
    "title": "Tighter Sparse Approximation Bounds for ReLU Neural Networks",
    "volume": "spotlight",
    "abstract": "A well-known line of work (Barron, 1993; Breiman, 1993; Klusowski & Barron, 2018) provides bounds on the width $n$ of a ReLU two-layer neural network needed to approximate a function $f$ over the ball $\\mathcal{B}_R(\\mathbb{R}^d)$ up to error $\\epsilon$, when the Fourier based quantity $C_f = \\int_{\\mathbb{R}^d} \\|\\xi\\|^2 |\\hat{f}(\\xi)| \\ d\\xi$ is finite. More recently Ongie et al. (2019) used the Radon transform as a tool for analysis of infinite-width ReLU two-layer networks. In particular, they introduce the concept of Radon-based $\\mathcal{R}$-norms and show that a function defined on $\\mathbb{R}^d$ can be represented as an infinite-width two-layer neural network if and only if its $\\mathcal{R}$-norm is finite. In this work, we extend the framework of Ongie et al. (2019) and define similar Radon-based semi-norms ($\\mathcal{R}, \\mathcal{U}$-norms) such that a function admits an infinite-width neural network representation on a bounded open set $\\mathcal{U} \\subseteq \\mathbb{R}^d$ when its $\\mathcal{R}, \\mathcal{U}$-norm is finite. Building on this, we derive sparse (finite-width) neural network approximation bounds that refine those of Breiman (1993); Klusowski & Barron (2018). Finally, we show that infinite-width neural network representations on bounded open sets are not unique and study their structure, providing a functional view of mode connectivity",
    "checked": true,
    "id": "79650143c8f77b0f960db1b3b32bec5c00124338",
    "semantic_title": "tighter sparse approximation bounds for relu neural networks",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=vwj6aUeocyf": {
    "title": "Long Expressive Memory for Sequence Modeling",
    "volume": "spotlight",
    "abstract": "We propose a novel method called Long Expressive Memory (LEM) for learning long-term sequential dependencies. LEM is gradient-based, it can efficiently process sequential tasks with very long-term dependencies, and it is sufficiently expressive to be able to learn complicated input-output maps. To derive LEM, we consider a system of multiscale ordinary differential equations, as well as a suitable time-discretization of this system. For LEM, we derive rigorous bounds to show the mitigation of the exploding and vanishing gradients problem, a well-known challenge for gradient-based recurrent sequential learning methods. We also prove that LEM can approximate a large class of dynamical systems to high accuracy. Our empirical results, ranging from image and time-series classification through dynamical systems prediction to speech recognition and language modeling, demonstrate that LEM outperforms state-of-the-art recurrent neural networks, gated recurrent units, and long short-term memory models",
    "checked": true,
    "id": "67f754478d0057b9d4d5d96260c2523314b10eda",
    "semantic_title": "long expressive memory for sequence modeling",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=LzQQ89U1qm_": {
    "title": "Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy",
    "volume": "spotlight",
    "abstract": "Unsupervised detection of anomaly points in time series is a challenging problem, which requires the model to derive a distinguishable criterion. Previous methods tackle the problem mainly through learning pointwise representation or pairwise association, however, neither is sufficient to reason about the intricate dynamics. Recently, Transformers have shown great power in unified modeling of pointwise representation and pairwise association, and we find that the self-attention weight distribution of each time point can embody rich association with the whole series. Our key observation is that due to the rarity of anomalies, it is extremely difficult to build nontrivial associations from abnormal points to the whole series, thereby, the anomalies' associations shall mainly concentrate on their adjacent time points. This adjacent-concentration bias implies an association-based criterion inherently distinguishable between normal and abnormal points, which we highlight through the Association Discrepancy. Technically, we propose the Anomaly Transformer with a new Anomaly-Attention mechanism to compute the association discrepancy. A minimax strategy is devised to amplify the normal-abnormal distinguishability of the association discrepancy. The Anomaly Transformer achieves state-of-the-art results on six unsupervised time series anomaly detection benchmarks of three applications: service monitoring, space & earth exploration, and water treatment",
    "checked": true,
    "id": "a46b06a4b8b4deecf96a4e42cd19b4696f999e66",
    "semantic_title": "anomaly transformer: time series anomaly detection with association discrepancy",
    "citation_count": 618,
    "authors": []
  },
  "https://openreview.net/forum?id=TIdIXIpzhoI": {
    "title": "Progressive Distillation for Fast Sampling of Diffusion Models",
    "volume": "spotlight",
    "abstract": "Diffusion models have recently shown great promise for generative modeling, outperforming GANs on perceptual quality and autoregressive models at density estimation. A remaining downside is their slow sampling time: generating high quality samples takes many hundreds or thousands of model evaluations. Here we make two contributions to help eliminate this downside: First, we present new parameterizations of diffusion models that provide increased stability when using few sampling steps, compared to models in the literature. Second, we present a method to distill a trained deterministic diffusion sampler, using many steps, into a new diffusion model that takes half as many sampling steps. We then keep progressively applying this distillation procedure to our model, halving the number of required sampling steps each time. On standard image generation benchmarks like CIFAR-10, ImageNet, and LSUN, we start out with (near) state-of-the-art samplers taking 1024 or 8192 steps, and are able to distill down to models taking as little as 4 steps without losing much perceptual quality; achieving, for example, a FID of 3.0 on CIFAR-10 in 4 steps. Finally, we show that the full progressive distillation procedure does not take more time than it takes to train the original model, thus representing an efficient solution for generative modeling using diffusion at both train and test time",
    "checked": true,
    "id": "625d57bd52c60cd79aa4add6c4420dc2ad3b808a",
    "semantic_title": "on distillation of guided diffusion models",
    "citation_count": 620,
    "authors": []
  },
  "https://openreview.net/forum?id=7gWSJrP3opB": {
    "title": "A General Analysis of Example-Selection for Stochastic Gradient Descent",
    "volume": "spotlight",
    "abstract": "Training example order in SGD has long been known to affect convergence rate. Recent results show that accelerated rates are possible in a variety of cases for permutation-based sample orders, in which each example from the training set is used once before any example is reused. In this paper, we develop a broad condition on the sequence of examples used by SGD that is sufficient to prove tight convergence rates in both strongly convex and non-convex settings. We show that our approach suffices to recover, and in some cases improve upon, previous state-of-the-art analyses for four known example-selection schemes: (1) shuffle once, (2) random reshuffling, (3) random reshuffling with data echoing, and (4) Markov Chain Gradient Descent. Motivated by our theory, we propose two new example-selection approaches. First, using quasi-Monte-Carlo methods, we achieve unprecedented accelerated convergence rates for learning with data augmentation. Second, we greedily choose a fixed scan-order to minimize the metric used in our condition and show that we can obtain more accurate solutions from the same number of epochs of SGD. We conclude by empirically demonstrating the utility of our approach for both convex linear-model and deep learning tasks. Our code is available at: https://github.com/EugeneLYC/qmc-ordering",
    "checked": true,
    "id": "c0b07ce929c821ab6f7d84ce424d3850a516555e",
    "semantic_title": "a general analysis of example-selection for stochastic gradient descent",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=WvOGCEAQhxl": {
    "title": "Assessing Generalization of SGD via Disagreement",
    "volume": "spotlight",
    "abstract": "We empirically show that the test error of deep networks can be estimated by training the same architecture on the same training set but with two different runs of Stochastic Gradient Descent (SGD), and then measuring the disagreement rate between the two networks on unlabeled test data. This builds on -- and is a stronger version of -- the observation in Nakkiran&Bansal 20, which requires the runs to be on separate training sets. We further theoretically show that this peculiar phenomenon arises from the well-calibrated nature of ensembles of SGD-trained models. This finding not only provides a simple empirical measure to directly predict the test error using unlabeled test data, but also establishes a new conceptual connection between generalization and calibration",
    "checked": true,
    "id": "7f8b0cd4ef0ee1fb9970d83c71b765fc67f35887",
    "semantic_title": "assessing generalization of sgd via disagreement",
    "citation_count": 123,
    "authors": []
  },
  "https://openreview.net/forum?id=YZHES8wIdE": {
    "title": "Generative Planning for Temporally Coordinated Exploration in Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "Standard model-free reinforcement learning algorithms optimize a policy that generates the action to be taken in the current time step in order to maximize expected future return. While flexible, it faces difficulties arising from the inefficient exploration due to its single step nature. In this work, we present Generative Planning method (GPM), which can generate actions not only for the current step, but also for a number of future steps (thus termed as generative planning). This brings several benefits to GPM. Firstly, since GPM is trained by maximizing value, the plans generated from it can be regarded as intentional action sequences for reaching high value regions. GPM can therefore leverage its generated multi-step plans for temporally coordinated exploration towards high value regions, which is potentially more effective than a sequence of actions generated by perturbing each action at single step level, whose consistent movement decays exponentially with the number of exploration steps. Secondly, starting from a crude initial plan generator, GPM can refine it to be adaptive to the task, which, in return, benefits future explorations. This is potentially more effective than commonly used action-repeat strategy, which is non-adaptive in its form of plans. Additionally, since the multi-step plan can be interpreted as the intent of the agent from now to a span of time period into the future, it offers a more informative and intuitive signal for interpretation. Experiments are conducted on several benchmark environments and the results demonstrated its effectiveness compared with several baseline methods",
    "checked": true,
    "id": "33e3f13087abd5241d55523140720f5e684b7bee",
    "semantic_title": "generative planning for temporally coordinated exploration in reinforcement learning",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=Y4cs1Z3HnqL": {
    "title": "Pessimistic Bootstrapping for Uncertainty-Driven Offline Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "Offline Reinforcement Learning (RL) aims to learn policies from previously collected datasets without exploring the environment. Directly applying off-policy algorithms to offline RL usually fails due to the extrapolation error caused by the out-of-distribution (OOD) actions. Previous methods tackle such problem by penalizing the Q-values of OOD actions or constraining the trained policy to be close to the behavior policy. Nevertheless, such methods typically prevent the generalization of value functions beyond the offline data and also lack precise characterization of OOD data. In this paper, we propose Pessimistic Bootstrapping for offline RL (PBRL), a purely uncertainty-driven offline algorithm without explicit policy constraints. Specifically, PBRL conducts uncertainty quantification via the disagreement of bootstrapped Q-functions, and performs pessimistic updates by penalizing the value function based on the estimated uncertainty. To tackle the extrapolating error, we further propose a novel OOD sampling method. We show that such OOD sampling and pessimistic bootstrapping yields provable uncertainty quantifier in linear MDPs, thus providing the theoretical underpinning for PBRL. Extensive experiments on D4RL benchmark show that PBRL has better performance compared to the state-of-the-art algorithms",
    "checked": true,
    "id": "23abe79046d7f7b430d5d21b6a93598d0aa1b9c2",
    "semantic_title": "pessimistic bootstrapping for uncertainty-driven offline reinforcement learning",
    "citation_count": 144,
    "authors": []
  },
  "https://openreview.net/forum?id=dFbKQaRk15w": {
    "title": "Equivariant Subgraph Aggregation Networks",
    "volume": "spotlight",
    "abstract": "Message-passing neural networks (MPNNs) are the leading architecture for deep learning on graph-structured data, in large part due to their simplicity and scalability. Unfortunately, it was shown that these architectures are limited in their expressive power. This paper proposes a novel framework called Equivariant Subgraph Aggregation Networks (ESAN) to address this issue. Our main observation is that while two graphs may not be distinguishable by an MPNN, they often contain distinguishable subgraphs. Thus, we propose to represent each graph as a set of subgraphs derived by some predefined policy, and to process it using a suitable equivariant architecture. We develop novel variants of the 1-dimensional Weisfeiler-Leman (1-WL) test for graph isomorphism, and prove lower bounds on the expressiveness of ESAN in terms of these new WL variants. We further prove that our approach increases the expressive power of both MPNNs and more expressive architectures. Moreover, we provide theoretical results that describe how design choices such as the subgraph selection policy and equivariant neural architecture affect our architecture's expressive power. To deal with the increased computational cost, we propose a subgraph sampling scheme, which can be viewed as a stochastic version of our framework. A comprehensive set of experiments on real and synthetic datasets demonstrates that our framework improves the expressive power and overall performance of popular GNN architectures",
    "checked": true,
    "id": "6667a57c8ffa3f3c0d724b1e8e986758995df2b8",
    "semantic_title": "equivariant subgraph aggregation networks",
    "citation_count": 194,
    "authors": []
  },
  "https://openreview.net/forum?id=D78Go4hVcxO": {
    "title": "How Do Vision Transformers Work?",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "430bab3890e1e52c4c1f74900b0e408e47a1cb8f",
    "semantic_title": "how do vision transformers work?",
    "citation_count": 531,
    "authors": []
  },
  "https://openreview.net/forum?id=kZ0UYdhqkNY": {
    "title": "Variational methods for simulation-based inference",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "401782ba0dacfe3cc1c84d0d6054fa1ab416248c",
    "semantic_title": "variational methods for simulation-based inference",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=JprM0p-q0Co": {
    "title": "Tackling the Generative Learning Trilemma with Denoising Diffusion GANs",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "4f73a26f7c2c942d7f1b8267e1307d6f5207613b",
    "semantic_title": "tackling the generative learning trilemma with denoising diffusion gans",
    "citation_count": 612,
    "authors": []
  },
  "https://openreview.net/forum?id=yKIAXjkJc2F": {
    "title": "Imbedding Deep Neural Networks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "e547ef1c681f17a79520ead66a5c265217695ebd",
    "semantic_title": "imbedding deep neural networks",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=ZkC8wKoLbQ7": {
    "title": "Understanding and Preventing Capacity Loss in Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "0a818d426c813b7a4758c959d38ece5f2cc11476",
    "semantic_title": "understanding and preventing capacity loss in reinforcement learning",
    "citation_count": 130,
    "authors": []
  },
  "https://openreview.net/forum?id=1JDiK_TbV4S": {
    "title": "Source-Free Adaptation to Measurement Shift via Bottom-Up Feature Restoration",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "6c48095f7883d726dd2679394c79c1c955b2aaa1",
    "semantic_title": "source-free adaptation to measurement shift via bottom-up feature restoration",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=lnEaqbTJIRz": {
    "title": "The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "a2412fdebd53bd25476f834ae2b8aa8cb44cb1e1",
    "semantic_title": "the inductive bias of in-context learning: rethinking pretraining example design",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=AUGBfDIV9rL": {
    "title": "Emergent Communication at Scale",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "051c85b10124bd2de6fcc29519a187f479d451bf",
    "semantic_title": "emergent communication at scale",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=vds4SNooOe": {
    "title": "Superclass-Conditional Gaussian Mixture Model For Learning Fine-Grained Embeddings",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "1ae600e7a24da0e6914efaaa75ff12e9d6e21208",
    "semantic_title": "superclass-conditional gaussian mixture model for learning fine-grained embeddings",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=-ApAkox5mp": {
    "title": "SHINE: SHaring the INverse Estimate from the forward pass for bi-level optimization and implicit models",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "3735f440f59a759a09ff0ccc8d0dfba24cc07665",
    "semantic_title": "shine: sharing the inverse estimate from the forward pass for bi-level optimization and implicit models",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=ccWaPGl9Hq": {
    "title": "Towards Deployment-Efficient Reinforcement Learning: Lower Bound and Optimality",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "ff5abca5cb5c193a19c2cebad8f99e3d9cb928d1",
    "semantic_title": "towards deployment-efficient reinforcement learning: lower bound and optimality",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=A3HHaEdqAJL": {
    "title": "Task Relatedness-Based Generalization Bounds for Meta Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "2b70bb9d21d09199362bd98fced966481caf4646",
    "semantic_title": "task relatedness-based generalization bounds for meta learning",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=pFyXqxChZc": {
    "title": "IntSGD: Adaptive Floatless Compression of Stochastic Gradients",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "1836c0c53bc8c77aa56d45834c3a3ff910089bc2",
    "semantic_title": "intsgd: adaptive floatless compression of stochastic gradients",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=iLHOIDsPv1P": {
    "title": "PAC-Bayes Information Bottleneck",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "39000df05781f49365cb1b223d228dd9b859efc4",
    "semantic_title": "pac-bayes information bottleneck",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=jXKKDEi5vJt": {
    "title": "Byzantine-Robust Learning on Heterogeneous Datasets via Bucketing",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "a9c13ada98fa493dd3552c794fdb3af9e9fbd523",
    "semantic_title": "byzantine-robust learning on heterogeneous datasets via bucketing",
    "citation_count": 155,
    "authors": []
  },
  "https://openreview.net/forum?id=AvcfxqRy4Y": {
    "title": "Understanding the Role of Self Attention for Efficient Speech Recognition",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "9da271bd20d17ec9d22b4ac5f18e0f8fb6bd0b92",
    "semantic_title": "understanding the role of self attention for efficient speech recognition",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=8WawVDdKqlL": {
    "title": "Label Encoding for Regression Networks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "aba33952946e1669fa7ed4e1ca112fc97962737e",
    "semantic_title": "label encoding for regression networks",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=zNHzqZ9wrRB": {
    "title": "Equivariant Transformers for Neural Network based Molecular Potentials",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "68ab47dbf07dcff861548ee37c906b6793dfd75c",
    "semantic_title": "torchmd-net: equivariant transformers for neural network based molecular potentials",
    "citation_count": 215,
    "authors": []
  },
  "https://openreview.net/forum?id=9XhPLAjjRB": {
    "title": "SGD Can Converge to Local Maxima",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "5ec766770e4cd2bdb927a384ef1e0dbc63b31b24",
    "semantic_title": "sgd can converge to local maxima",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=H0oaWl6THa": {
    "title": "Hybrid Local SGD for Federated Learning with Heterogeneous Communications",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "0aa7cb075978ed8c24f3e2a8ddc3ccb14df3e9a5",
    "semantic_title": "hybrid local sgd for federated learning with heterogeneous communications",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=EAy7C1cgE1L": {
    "title": "Increasing the Cost of Model Extraction with Calibrated Proof of Work",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "2ee4127a2a6aab51a03305d8a564693181bc6424",
    "semantic_title": "increasing the cost of model extraction with calibrated proof of work",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=HFmAukZ-k-2": {
    "title": "Learning the Dynamics of Physical Systems from Sparse Observations with Finite Element Networks",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "162b00580f87c9b0a2ac1c17658b316f4b038b05",
    "semantic_title": "learning the dynamics of physical systems from sparse observations with finite element networks",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=BnQhMqDfcKG": {
    "title": "Probabilistic Implicit Scene Completion",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "49d707930d942a04e06aa2c13545ead4aa00336e",
    "semantic_title": "probabilistic implicit scene completion",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=7l1IjZVddDW": {
    "title": "Improving Federated Learning Face Recognition via Privacy-Agnostic Clusters",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "eaccb8a31f7d7b7b442747bf2ffe1d0b4c6570b3",
    "semantic_title": "improving federated learning face recognition via privacy-agnostic clusters",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=HndgQudNb91": {
    "title": "Learning to Downsample for Segmentation of Ultra-High Resolution Images",
    "volume": "poster",
    "abstract": "Many computer vision systems require low-cost segmentation algorithms based on deep learning, either because of the enormous size of input images or limited computational budget. Common solutions uniformly downsample the input images to meet memory constraints, assuming all pixels are equally informative. In this work, we demonstrate that this assumption can harm the segmentation performance because the segmentation difficulty varies spatially (see Figure 1 \"Uniform\"). We combat this problem by introducing a learnable downsampling module, which can be optimised together with the given segmentation model in an end-to-end fashion. We formulate the problem of training such downsampling module as optimisation of sampling density distributions over the input images given their low-resolution views. To defend against degenerate solutions (e.g. over-sampling trivial regions like the backgrounds), we propose a regularisation term that encourages the sampling locations to concentrate around the object boundaries. We find the downsampling module learns to sample more densely at difficult locations, thereby improving the segmentation performance (see Figure 1 \"Ours\"). Our experiments on benchmarks of high-resolution street view, aerial and medical images demonstrate substantial improvements in terms of efficiency-and-accuracy trade-off compared to both uniform downsampling and two recent advanced downsampling techniques",
    "checked": true,
    "id": "c31e7b6b56d0c8584ca4d1b6cc01f511a705273b",
    "semantic_title": "learning to downsample for segmentation of ultra-high resolution images",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=7fFO4cMBx_9": {
    "title": "Variational Neural Cellular Automata",
    "volume": "poster",
    "abstract": "In nature, the process of cellular growth and differentiation has lead to an amazing diversity of organisms --- algae, starfish, giant sequoia, tardigrades, and orcas are all created by the same generative process. Inspired by the incredible diversity of this biological generative process, we propose a generative model, the Variational Neural Cellular Automata (VNCA), which is loosely inspired by the biological processes of cellular growth and differentiation. Unlike previous related works, the VNCA is a proper probabilistic generative model, and we evaluate it according to best practices. We find that the VNCA learns to reconstruct samples well and that despite its relatively few parameters and simple local-only communication, the VNCA can learn to generate a large variety of output from information encoded in a common vector format. While there is a significant gap to the current state-of-the-art in terms of generative modeling performance, we show that the VNCA can learn a purely self-organizing generative process of data. Additionally, the self-organizing nature bestows the VNCA with some inherent robustness against perturbations in the early stages of growth",
    "checked": true,
    "id": "9e2fcbf361e3d89b10f1ae2e3d9c49876db42976",
    "semantic_title": "variational neural cellular automata",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=FKp8-pIRo3y": {
    "title": "Wish you were here: Hindsight Goal Selection for long-horizon dexterous manipulation",
    "volume": "poster",
    "abstract": "Complex sequential tasks in continuous-control settings often require agents to successfully traverse a set of ``narrow passages'' in their state space. Solving such tasks with a sparse reward in a sample-efficient manner poses a challenge to modern reinforcement learning (RL) due to the associated long-horizon nature of the problem and the lack of sufficient positive signal during learning. Various tools have been applied to address this challenge. When available, large sets of demonstrations can guide agent exploration. Hindsight relabelling on the other hand does not require additional sources of information. However, existing strategies explore based on task-agnostic goal distributions, which can render the solution of long-horizon tasks impractical. In this work, we extend hindsight relabelling mechanisms to guide exploration along task-specific distributions implied by a small set of successful demonstrations. We evaluate the approach on four complex, single and dual arm, robotics manipulation tasks against strong suitable baselines. The method requires far fewer demonstrations to solve all tasks and achieves a significantly higher overall performance as task complexity increases. Finally, we investigate the robustness of the proposed solution with respect to the quality of input representations and the number of demonstrations",
    "checked": true,
    "id": "fdc60fed400833caf0b936321f91b41b154fd09f",
    "semantic_title": "wish you were here: hindsight goal selection for long-horizon dexterous manipulation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KntaNRo6R48": {
    "title": "L0-Sparse Canonical Correlation Analysis",
    "volume": "poster",
    "abstract": "Canonical Correlation Analysis (CCA) models are powerful for studying the associations between two sets of variables. The canonically correlated representations, termed \\textit{canonical variates} are widely used in unsupervised learning to analyze unlabeled multi-modal registered datasets. Despite their success, CCA models may break (or overfit) if the number of variables in either of the modalities exceeds the number of samples. Moreover, often a significant fraction of the variables measures modality-specific information, and thus removing them is beneficial for identifying the \\textit{canonically correlated variates}. Here, we propose $\\ell_0$-CCA, a method for learning correlated representations based on sparse subsets of variables from two observed modalities. Sparsity is obtained by multiplying the input variables by stochastic gates, whose parameters are learned together with the CCA weights via an $\\ell_0$-regularized correlation loss. We further propose $\\ell_0$-Deep CCA for solving the problem of non-linear sparse CCA by modeling the correlated representations using deep nets. We demonstrate the efficacy of the method using several synthetic and real examples. Most notably, by gating nuisance input variables, our approach improves the extracted representations compared to other linear, non-linear and sparse CCA-based models",
    "checked": true,
    "id": "ee8e867420d7e3603dc5e8aab50555a386e23a13",
    "semantic_title": "l0-sparse canonical correlation analysis",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=B7ZbqNLDn-_": {
    "title": "Recycling Model Updates in Federated Learning: Are Gradient Subspaces Low-Rank?",
    "volume": "poster",
    "abstract": "In this paper, we question the rationale behind propagating large numbers of parameters through a distributed system during federated learning. We start by examining the rank characteristics of the subspace spanned by gradients (i.e., the gradient-space) in centralized model training, and observe that the gradient-space often consists of a few leading principal components accounting for an overwhelming majority (95-99%) of the explained variance. Motivated by this, we propose the \"Look-back Gradient Multiplier\" (LBGM) algorithm, which utilizes this low-rank property of the gradient-space in federated learning. Operationally, LBGM recycles the gradients between model update rounds to significantly reduce the number of parameters to be propagated through the system. We analytically characterize the convergence behavior of LBGM, revealing the nature of the trade-off between communication savings and model performance. Our subsequent experimental results demonstrate the improvement LBGM obtains on communication overhead compared to federated learning baselines. Additionally, we show that LBGM is a general plug-and-play algorithm that can be used standalone or stacked on top of existing sparsification techniques for distributed model training",
    "checked": true,
    "id": "4c99c73c14176159a6cd19e2366e98e87a9a2aa0",
    "semantic_title": "recycling model updates in federated learning: are gradient subspaces low-rank?",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=ucASPPD9GKN": {
    "title": "Is Homophily a Necessity for Graph Neural Networks?",
    "volume": "poster",
    "abstract": "Graph neural networks (GNNs) have shown great prowess in learning representations suitable for numerous graph-based machine learning tasks. When applied to semi-supervised node classification, GNNs are widely believed to work well due to the homophily assumption (``like attracts like''), and fail to generalize to heterophilous graphs where dissimilar nodes connect. Recent works design new architectures to overcome such heterophily-related limitations, citing poor baseline performance and new architecture improvements on a few heterophilous graph benchmark datasets as evidence for this notion. In our experiments, we empirically find that standard graph convolutional networks (GCNs) can actually achieve better performance than such carefully designed methods on some commonly used heterophilous graphs. This motivates us to reconsider whether homophily is truly necessary for good GNN performance. We find that this claim is not quite true, and in fact, GCNs can achieve strong performance on heterophilous graphs under certain conditions. Our work carefully characterizes these conditions and provides supporting theoretical understanding and empirical observations. Finally, we examine existing heterophilous graphs benchmarks and reconcile how the GCN (under)performs on them based on this understanding",
    "checked": true,
    "id": "8a1e3d41ea3d730e562d8c19b2bdb50a23208842",
    "semantic_title": "is homophily a necessity for graph neural networks?",
    "citation_count": 248,
    "authors": []
  },
  "https://openreview.net/forum?id=Ve0Wth3ptT_": {
    "title": "DEGREE: Decomposition Based Explanation for Graph Neural Networks",
    "volume": "poster",
    "abstract": "Graph Neural Networks (GNNs) are gaining extensive attention for their application in graph data. However, the black-box nature of GNNs prevents users from understanding and trusting the models, thus hampering their applicability. Whereas explaining GNNs remains a challenge, most existing methods fall into approximation based and perturbation based approaches with suffer from faithfulness problems and unnatural artifacts respectively. To tackle these problems, we propose DEGREE (Decomposition based Explanation for GRaph nEural nEtworks) to provide a faithful explanation for GNN predictions. By decomposing the information generation and aggregation mechanism of GNNs, DEGREE allows tracking the contributions of specific components of the input graph to the final prediction. Based on this, we further design a subgraph level interpretation algorithm to reveal complex interactions between graph nodes that are overlooked by previous methods. The efficiency of our algorithm can be further improved by utilizing GNN characteristics. Finally, we conduct quantitative and qualitative experiments on synthetic and real-world datasets to demonstrate the effectiveness of DEGREE on node classification and graph classification tasks",
    "checked": false,
    "id": "a594403e8ad4357d005281f4982c2928332f48c6",
    "semantic_title": "interactive explainable ai platform for graph neural networks",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=T0B9AoM_bFg": {
    "title": "Improving Mutual Information Estimation with Annealed and Energy-Based Bounds",
    "volume": "poster",
    "abstract": "Mutual information (MI) is a fundamental quantity in information theory and machine learning. However, direct estimation of MI is intractable, even if the true joint probability density for the variables of interest is known, as it involves estimating a potentially high-dimensional log partition function. In this work, we present a unifying view of existing MI bounds from the perspective of importance sampling, and propose three novel bounds based on this approach. Since a tight MI bound without density information requires a sample size exponential in the true MI, we assume either a single marginal or the full joint density information is known. In settings where the full joint density is available, we propose Multi-Sample Annealed Importance Sampling (AIS) bounds on MI, which we demonstrate can tightly estimate large values of MI in our experiments. In settings where only a single marginal distribution is known, we propose Generalized IWAE (GIWAE) and MINE-AIS bounds. Our GIWAE bound unifies variational and contrastive bounds in a single framework that generalizes InfoNCE, IWAE, and Barber-Agakov bounds. Our MINE-AIS method improves upon existing energy-based methods such as MINE-DV and MINE-F by directly optimizing a tighter lower bound on MI. MINE-AIS uses MCMC sampling to estimate gradients for training and Multi-Sample AIS for evaluating the bound. Our methods are particularly suitable for evaluating MI in deep generative models, since explicit forms of the marginal or joint densities are often available. We evaluate our bounds on estimating the MI of VAEs and GANs trained on the MNIST and CIFAR datasets, and showcase significant gains over existing bounds in these challenging settings with high ground truth MI",
    "checked": true,
    "id": "af7a0aa310a2277999e3144365b422afcc7e99dd",
    "semantic_title": "improving mutual information estimation with annealed and energy-based bounds",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bp-LJ4y_XC": {
    "title": "Sequence Approximation using Feedforward Spiking Neural Network for Spatiotemporal Learning: Theory and Optimization Methods",
    "volume": "poster",
    "abstract": "A dynamical system of spiking neurons with only feedforward connections can classify spatiotemporal patterns without recurrent connections. However, the theoretical construct of a feedforward spiking neural network (SNN) for approximating a temporal sequence remains unclear, making it challenging to optimize SNN architectures for learning complex spatiotemporal patterns. In this work, we establish a theoretical framework to understand and improve sequence approximation using a feedforward SNN. Our framework shows that a feedforward SNN with one neuron per layer and skip-layer connections can approximate the mapping function between any arbitrary pairs of input and output spike train on a compact domain. Moreover, we prove that heterogeneous neurons with varying dynamics and skip-layer connections improve sequence approximation using feedforward SNN. Consequently, we propose SNN architectures incorporating the preceding constructs that are trained using supervised backpropagation-through-time (BPTT) and unsupervised spiking-timing-dependent plasticity (STDP) algorithms for classification of spatiotemporal data. A dual-search-space Bayesian optimization method is developed to optimize architecture and parameters of the proposed SNN with heterogeneous neuron dynamics and skip-layer connections",
    "checked": true,
    "id": "d4bc30ab26b2c8bfc519c9d81f9aa167ab460965",
    "semantic_title": "sequence approximation using feedforward spiking neural network for spatiotemporal learning: theory and optimization methods",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=nwKXyFvaUm": {
    "title": "Diverse Client Selection for Federated Learning via Submodular Maximization",
    "volume": "poster",
    "abstract": "In every communication round of federated learning, a random subset of clients communicate their model updates back to the server which then aggregates them all. The optimal size of this subset is not known and several studies have shown that typically random selection does not perform very well in terms of convergence, learning efficiency and fairness. We, in this paper, propose to select a small diverse subset of clients, namely those carrying representative gradient information, and we transmit only these updates to the server. Our aim is for updating via only a subset to approximate updating via aggregating all client information. We achieve this by choosing a subset that maximizes a submodular facility location function defined over gradient space. We introduce \"federated averaging with diverse client selection (DivFL)\". We provide a thorough analysis of its convergence in the heterogeneous setting and apply it both to synthetic and to real datasets. Empirical results show several benefits to our approach including improved learning efficiency, faster convergence and also more uniform (i.e., fair) performance across clients. We further show a communication-efficient version of DivFL that can still outperform baselines on the above metrics",
    "checked": true,
    "id": "91a5751912175f3a9cdf754f774839a0489f076e",
    "semantic_title": "diverse client selection for federated learning via submodular maximization",
    "citation_count": 114,
    "authors": []
  },
  "https://openreview.net/forum?id=jT1EwXu-4hj": {
    "title": "From Intervention to Domain Transportation: A Novel Perspective to Optimize Recommendation",
    "volume": "poster",
    "abstract": "The interventional nature of recommendation has attracted increasing attention in recent years. It particularly motivates researchers to formulate learning and evaluating recommendation as causal inference and data missing-not-at-random problems. However, few take seriously the consequence of violating the critical assumption of overlapping, which we prove can significantly threaten the validity and interpretation of the outcome. We find a critical piece missing in the current understanding of information retrieval (IR) systems: as interventions, recommendation not only affects the already observed data, but it also interferes with the target domain (distribution) of interest. We then rephrase optimizing recommendation as finding an intervention that best transports the patterns it learns from the observed domain to its intervention domain. Towards this end, we use domain transportation to characterize the learning-intervention mechanism of recommendation. We design a principled transportation-constraint risk minimization objective and convert it to a two-player minimax game. We prove the consistency, generalization, and excessive risk bounds for the proposed objective, and elaborate how they compare to the current results. Finally, we carry out extensive real-data and semi-synthetic experiments to demonstrate the advantage of our approach, and launch online testing with a real-world IR system",
    "checked": true,
    "id": "588b41d7a6d97fbf52fc7d4086752e6330951fc9",
    "semantic_title": "from intervention to domain transportation: a novel perspective to optimize recommendation",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=JxFgJbZ-wft": {
    "title": "Variational Predictive Routing with Nested Subjective Timescales",
    "volume": "poster",
    "abstract": "Discovery and learning of an underlying spatiotemporal hierarchy in sequential data is an important topic for machine learning. Despite this, little work has been done to explore hierarchical generative models that can flexibly adapt their layerwise representations in response to datasets with different temporal dynamics. Here, we present Variational Predictive Routing (VPR) ‚Äì a neural probabilistic inference system that organizes latent representations of video features in a temporal hierarchy, based on their rates of change, thus modeling continuous data as a hierarchical renewal process. By employing an event detection mechanism that relies solely on the system's latent representations (without the need of a separate model), VPR is able to dynamically adjust its internal state following changes in the observed features, promoting an optimal organisation of representations across the levels of the model's latent hierarchy. Using several video datasets, we show that VPR is able to detect event boundaries, disentangle spatiotemporal features across its hierarchy, adapt to the dynamics of the data, and produce accurate time-agnostic rollouts of the future. Our approach integrates insights from neuroscience and introduces a framework with high potential for applications in model-based reinforcement learning, where flexible and informative state-space rollouts are of particular interest",
    "checked": true,
    "id": "5ba6bda6979b155f194f68d933656253cc90c9f8",
    "semantic_title": "variational predictive routing with nested subjective timescales",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=RhB1AdoFfGE": {
    "title": "Sample and Computation Redistribution for Efficient Face Detection",
    "volume": "poster",
    "abstract": "Although tremendous strides have been made in uncontrolled face detection, accurate face detection with a low computation cost remains an open challenge. In this paper, we point out that computation distribution and scale augmentation are the keys to detecting small faces from low-resolution images. Motivated by these observations, we introduce two simple but effective methods: (1) Computation Redistribution (CR), which reallocates the computation between the backbone, neck and head of the model; and (2) Sample Redistribution (SR), which augments training samples for the most needed stages. The proposed Sample and Computation Redistribution for Face Detection (SCRFD) is implemented by a random search in a meticulously designed search space. Extensive experiments conducted on WIDER FACE demonstrate the state-of-the-art accuracy-efficiency trade-off for the proposed SCRFD family across a wide range of compute regimes. In particular, SCRFD-34GF outperforms the best competitor, TinaFace, by $4.78\\%$ (AP at hard set) while being more than 3$\\times$ faster on GPUs with VGA-resolution images. Code is available at: https://github.com/deepinsight/insightface/tree/master/detection/scrfd",
    "checked": true,
    "id": "99d171aa12175c4d5d97d91c4032757719e22ae0",
    "semantic_title": "sample and computation redistribution for efficient face detection",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=NkZq4OEYN-": {
    "title": "Sound Adversarial Audio-Visual Navigation",
    "volume": "poster",
    "abstract": "Audio-visual navigation task requires an agent to find a sound source in a realistic, unmapped 3D environment by utilizing egocentric audio-visual observations. Existing audio-visual navigation works assume a clean environment that solely contains the target sound, which, however, would not be suitable in most real-world applications due to the unexpected sound noise or intentional interference. In this work, we design an acoustically complex environment in which, besides the target sound, there exists a sound attacker playing a zero-sum game with the agent. More specifically, the attacker can move and change the volume and category of the sound to make the agent suffer from finding the sounding object while the agent tries to dodge the attack and navigate to the goal under the intervention. Under certain constraints to the attacker, we can improve the robustness of the agent towards unexpected sound attacks in audio-visual navigation. For better convergence, we develop a joint training mechanism by employing the property of a centralized critic with decentralized actors. Experiments on two real-world 3D scan datasets, Replica, and Matterport3D, verify the effectiveness and the robustness of the agent trained under our designed environment when transferred to the clean environment or the one containing sound attackers with random policy. Project: https://yyf17.github.io/SAAVN",
    "checked": true,
    "id": "282e7b9777f9c3a48a3b1bbd9b11bf7821fb2fea",
    "semantic_title": "sound adversarial audio-visual navigation",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=12RoR2o32T": {
    "title": "Out-of-distribution Generalization in the Presence of Nuisance-Induced Spurious Correlations",
    "volume": "poster",
    "abstract": "In many prediction problems, spurious correlations are induced by a changing relationship between the label and a nuisance variable that is also correlated with the covariates. For example, in classifying animals in natural images, the background, which is a nuisance, can predict the type of animal. This nuisance-label relationship does not always hold, and the performance of a model trained under one such relationship may be poor on data with a different nuisance-label relationship. To build predictive models that perform well regardless of the nuisance-label relationship, we develop Nuisance-Randomized Distillation (NURD). We introduce the nuisance-randomized distribution, a distribution where the nuisance and the label are independent. Under this distribution, we define the set of representations such that conditioning on any member, the nuisance and the label remain independent. We prove that the representations in this set always perform better than chance, while representations outside of this set may not. NURD finds a representation from this set that is most informative of the label under the nuisance-randomized distribution, and we prove that this representation achieves the highest performance regardless of the nuisance-label relationship. We evaluate NURD on several tasks including chest X-ray classification where, using non-lung patches as the nuisance, NURD produces models that predict pneumonia under strong spurious correlations",
    "checked": true,
    "id": "dcb24c4f11d3fb39b2c38f964dd65d3def587d1d",
    "semantic_title": "out-of-distribution generalization in the presence of nuisance-induced spurious correlations",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=OM_lYiHXiCL": {
    "title": "AEVA: Black-box Backdoor Detection Using Adversarial Extreme Value Analysis",
    "volume": "poster",
    "abstract": "Deep neural networks (DNNs) are proved to be vulnerable against backdoor attacks. A backdoor could be embedded in the target DNNs through injecting a backdoor trigger into the training examples, which can cause the target DNNs misclassify an input attached with the backdoor trigger. Recent backdoor detection methods often require the access to the original poisoned training data, the parameters of the target DNNs, or the predictive confidence for each given input, which are impractical in many real-world applications, e.g., on-device de-ployed DNNs. We address the black-box hard-label backdoor detection problem where the DNN is a fully black-box and only its final output label is accessible. We approach this problem from the optimization perspective and show that the objective of backdoor detection is bounded by an adversarial objective. Further theoretical and empirical studies reveal that this adversarial objective leads to a solution with highly skewed distribution; a singularity is often observed in the adversarial map of a backdoor-infected example, which we call the adversarial singularity phenomenon. Based on this observation, we propose the adversarial extreme value analysis(AEVA) algorithm to detect backdoors in black-box neural networks. The AEVA algorithm is based on an extreme value analysis on the adversarial map, computed from the monte-carlo gradient estimation due to the black-box hard-label constraint. Evidenced by extensive experiments across three popular tasks and backdoor attacks, our approach is shown effective in detecting backdoor attacks under the black-box hard-label scenarios",
    "checked": true,
    "id": "192b598c581ba0eabb4bbaa178eb8be6314c3943",
    "semantic_title": "aeva: black-box backdoor detection using adversarial extreme value analysis",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=5ECQL05ub0J": {
    "title": "Resonance in Weight Space: Covariate Shift Can Drive Divergence of SGD with Momentum",
    "volume": "poster",
    "abstract": "Most convergence guarantees for stochastic gradient descent with momentum (SGDm) rely on iid sampling. Yet, SGDm is often used outside this regime, in settings with temporally correlated input samples such as continual learning and reinforcement learning. Existing work has shown that SGDm with a decaying step-size can converge under Markovian temporal correlation. In this work, we show that SGDm under covariate shift with a fixed step-size can be unstable and diverge. In particular, we show SGDm under covariate shift is a parametric oscillator, and so can suffer from a phenomenon known as resonance. We approximate the learning system as a time varying system of ordinary differential equations, and leverage existing theory to characterize the system's divergence/convergence as resonant/nonresonant modes. The theoretical result is limited to the linear setting with periodic covariate shift, so we empirically supplement this result to show that resonance phenomena persist even under non-periodic covariate shift, nonlinear dynamics with neural networks, and optimizers other than SGDm",
    "checked": true,
    "id": "a8b150013942d949a03ecf20545a3b5d66ca369a",
    "semantic_title": "resonance in weight space: covariate shift can drive divergence of sgd with momentum",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WqoBaaPHS-": {
    "title": "Top-label calibration and multiclass-to-binary reductions",
    "volume": "poster",
    "abstract": "We propose a new notion of multiclass calibration called top-label calibration. A classifier is said to be top-label calibrated if the reported probability for the predicted class label---the top-label---is calibrated, conditioned on the top-label. This conditioning is essential for practical utility of the calibration property, since the top-label is always reported and we must condition on what is reported. However, the popular notion of confidence calibration erroneously skips this conditioning. Furthermore, we outline a multiclass-to-binary (M2B) reduction framework that unifies confidence, top-label, and class-wise calibration, among others. As its name suggests, M2B works by reducing multiclass calibration to different binary calibration problems; various types of multiclass calibration can then be achieved using simple binary calibration routines. We instantiate the M2B framework with the well-studied histogram binning (HB) binary calibrator, and prove that the overall procedure is multiclass calibrated without making any assumptions on the underlying data distribution. In an empirical evaluation with four deep net architectures on CIFAR-10 and CIFAR-100, we find that the M2B + HB procedure achieves lower top-label and class-wise calibration error than other approaches such as temperature scaling. Code for this work is available at https://github.com/aigen/df-posthoc-calibration",
    "checked": true,
    "id": "2e6dbff05337265390ee33f8d684c8d44a228073",
    "semantic_title": "top-label calibration and multiclass-to-binary reductions",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=JfaWawZ8BmX": {
    "title": "Anisotropic Random Feature Regression in High Dimensions",
    "volume": "poster",
    "abstract": "In contrast to standard statistical wisdom, modern learning algorithms typically find their best performance in the overparameterized regime in which the model has many more parameters than needed to fit the training data. A growing number of recent works have shown that random feature models can offer a detailed theoretical explanation for this unexpected behavior, but typically these analyses have utilized isotropic distributional assumptions on the underlying data generation process, thereby failing to provide a realistic characterization of real-world models that are designed to identify and harness the structure in natural data. In this work, we examine the high-dimensional asymptotics of random feature regression in the presence of structured data, allowing for arbitrary input correlations and arbitrary alignment between the data and the weights of the target function. We define a partial order on the space of weight-data alignments and prove that generalization performance improves in response to stronger alignment. We also clarify several previous observations in the literature by distinguishing the behavior of the sample-wise and parameter-wise learning curves, finding that sample-wise multiple descent can occur at scales dictated by the eigenstructure of the data covariance, but that parameter-wise multiple descent is limited to double descent, although strong anisotropy can induce additional signatures such as wide plateaus and steep cliffs. Finally, these signatures are related to phase transitions in the spectrum of the feature kernel matrix, and unlike the double descent peak, persist even under optimal regularization",
    "checked": true,
    "id": "36bebabf39a981cabf3c38de3c3c3fc6b345bb8f",
    "semantic_title": "anisotropic random feature regression in high dimensions",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=L01Nn_VJ9i": {
    "title": "Back2Future: Leveraging Backfill Dynamics for Improving Real-time Predictions in Future",
    "volume": "poster",
    "abstract": "For real-time forecasting in domains like public health and macroeconomics, data collection is a non-trivial and demanding task. Often after being initially released, it undergoes several revisions later (maybe due to human or technical constraints) - as a result, it may take weeks until the data reaches a stable value. This so-called ‚Äòbackfill' phenomenon and its effect on model performance have been barely addressed in the prior literature. In this paper, we introduce the multi-variate backfill problem using COVID-19 as the motivating example. We construct a detailed dataset composed of relevant signals over the past year of the pandemic. We then systematically characterize several patterns in backfill dynamics and leverage our observations for formulating a novel problem and neural framework, Back2Future, that aims to refines a given model's predictions in real-time. Our extensive experiments demonstrate that our method refines the performance of the diverse set of top models for COVID-19 forecasting and GDP growth forecasting. Specifically, we show that Back2Future refined top COVID-19 models by 6.65% to 11.24% and yield an 18% improvement over non-trivial baselines. In addition, we show that our model improves model evaluation too; hence policy-makers can better understand the true accuracy of forecasting models in real-time",
    "checked": true,
    "id": "035c271232ab6951016ef24eb2046f9d2c458145",
    "semantic_title": "back2future: leveraging backfill dynamics for improving real-time predictions in future",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=lrocYB-0ST2": {
    "title": "Approximation and Learning with Deep Convolutional Models: a Kernel Perspective",
    "volume": "poster",
    "abstract": "The empirical success of deep convolutional networks on tasks involving high-dimensional data such as images or audio suggests that they can efficiently approximate certain functions that are well-suited for such tasks. In this paper, we study this through the lens of kernel methods, by considering simple hierarchical kernels with two or three convolution and pooling layers, inspired by convolutional kernel networks. These achieve good empirical performance on standard vision datasets, while providing a precise description of their functional space that yields new insights on their inductive bias. We show that the RKHS consists of additive models of interaction terms between patches, and that its norm encourages spatial similarities between these terms through pooling layers. We then provide generalization bounds which illustrate how pooling and patches yield improved sample complexity guarantees when the target function presents such regularities",
    "checked": true,
    "id": "88965ac9e0666e94174a54b0490679eee4dfc7be",
    "semantic_title": "approximation and learning with deep convolutional models: a kernel perspective",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=vgqS1vkkCbE": {
    "title": "Value Function Spaces: Skill-Centric State Abstractions for Long-Horizon Reasoning",
    "volume": "poster",
    "abstract": "Reinforcement learning can train policies that effectively perform complex tasks. However for long-horizon tasks, the performance of these methods degrades with horizon, often necessitating reasoning over and chaining lower-level skills. Hierarchical reinforcement learning aims to enable this by providing a bank of low-level skills as action abstractions. Hierarchies can further improve on this by abstracting the space states as well. We posit that a suitable state abstraction should depend on the capabilities of the available lower-level policies. We propose Value Function Spaces: a simple approach that produces such a representation by using the value functions corresponding to each lower-level skill. These value functions capture the affordances of the scene, thus forming a representation that compactly abstracts task relevant information and robustly ignores distractors. Empirical evaluations for maze-solving and robotic manipulation tasks demonstrate that our approach improves long-horizon performance and enables better zero-shot generalization than alternative model-free and model-based methods",
    "checked": true,
    "id": "0213fa01c7b8aa7668b11fd9edf283fe10d5719e",
    "semantic_title": "value function spaces: skill-centric state abstractions for long-horizon reasoning",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=gNp54NxHUPJ": {
    "title": "Fast Regression for Structured Inputs",
    "volume": "poster",
    "abstract": "We study the $\\ell_p$ regression problem, which requires finding $\\mathbf{x}\\in\\mathbb R^{d}$ that minimizes $\\|\\mathbf{A}\\mathbf{x}-\\mathbf{b}\\|_p$ for a matrix $\\mathbf{A}\\in\\mathbb R^{n \\times d}$ and response vector $\\mathbf{b}\\in\\mathbb R^{n}$. There has been recent interest in developing subsampling methods for this problem that can outperform standard techniques when $n$ is very large. However, all known subsampling approaches have run time that depends exponentially on $p$, typically, $d^{\\mathcal{O}(p)}$, which can be prohibitively expensive. We improve on this work by showing that for a large class of common \\emph{structured matrices}, such as combinations of low-rank matrices, sparse matrices, and Vandermonde matrices, there are subsampling based methods for $\\ell_p$ regression that depend polynomially on $p$. For example, we give an algorithm for $\\ell_p$ regression on Vandermonde matrices that runs in time $\\mathcal{O}(n\\log^3 n+(dp^2)^{0.5+\\omega}\\cdot\\text{polylog}\\,n)$, where $\\omega$ is the exponent of matrix multiplication. The polynomial dependence on $p$ crucially allows our algorithms to extend naturally to efficient algorithms for $\\ell_\\infty$ regression, via approximation of $\\ell_\\infty$ by $\\ell_{\\mathcal{O}(\\log n)}$. Of practical interest, we also develop a new subsampling algorithm for $\\ell_p$ regression for arbitrary matrices, which is simpler than previous approaches for $p \\ge 4$",
    "checked": true,
    "id": "4baaa1e4f87c337914576e1147a975332cb337a6",
    "semantic_title": "fast regression for structured inputs",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=qhC8mr2LEKq": {
    "title": "CrossBeam: Learning to Search in Bottom-Up Program Synthesis",
    "volume": "poster",
    "abstract": "Many approaches to program synthesis perform a search within an enormous space of programs to find one that satisfies a given specification. Prior works have used neural models to guide combinatorial search algorithms, but such approaches still explore a huge portion of the search space and quickly become intractable as the size of the desired program increases. To tame the search space blowup, we propose training a neural model to learn a hands-on search policy for bottom-up synthesis, instead of relying on a combinatorial search algorithm. Our approach, called CrossBeam, uses the neural model to choose how to combine previously-explored programs into new programs, taking into account the search history and partial program executions. Motivated by work in structured prediction on learning to search, CrossBeam is trained on-policy using data extracted from its own bottom-up searches on training tasks. We evaluate CrossBeam in two very different domains, string manipulation and logic programming. We observe that CrossBeam learns to search efficiently, exploring much smaller portions of the program space compared to the state-of-the-art",
    "checked": true,
    "id": "1a2738a3a7f0e9290e6ba4862316891aa7838123",
    "semantic_title": "crossbeam: learning to search in bottom-up program synthesis",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=M6M8BEmd6dq": {
    "title": "PEARL: Data Synthesis via Private Embeddings and Adversarial Reconstruction Learning",
    "volume": "poster",
    "abstract": "We propose a new framework of synthesizing data using deep generative models in a differentially private manner. Within our framework, sensitive data are sanitized with rigorous privacy guarantees in a one-shot fashion, such that training deep generative models is possible without re-using the original data. Hence, no extra privacy costs or model constraints are incurred, in contrast to popular gradient sanitization approaches, which, among other issues, cause degradation in privacy guarantees as the training iteration increases. We demonstrate a realization of our framework by making use of the characteristic function and an adversarial re-weighting objective, which are of independent interest as well. Our proposal has theoretical guarantees of performance, and empirical evaluations on multiple datasets show that our approach outperforms other methods at reasonable levels of privacy",
    "checked": true,
    "id": "ab118ecc89641bb30beefa2a7b916aec63671694",
    "semantic_title": "pearl: data synthesis via private embeddings and adversarial reconstruction learning",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=aOX3a9q3RVV": {
    "title": "Divisive Feature Normalization Improves Image Recognition Performance in AlexNet",
    "volume": "poster",
    "abstract": "Local divisive normalization provides a phenomenological description of many nonlinear response properties of neurons across visual cortical areas. To gain insight into the utility of this operation, we studied the effects on AlexNet of a local divisive normalization between features, with learned parameters. Developing features were arranged in a line topology, with the influence between features determined by an exponential function of the distance between them. We compared an AlexNet model with no normalization or with canonical normalizations (Batch, Group, Layer) to the same models with divisive normalization added. Divisive normalization always improved performance for models with batch or group or no normalization, generally by 1-2 percentage points, on both the CIFAR-100 and ImageNet databases. To gain insight into mechanisms underlying the improved performance, we examined several aspects of network representations. In the early layers both canonical and divisive normalizations reduced manifold capacities and increased average dimension of the individual categorical manifolds. In later layers the capacity was higher and manifold dimension lower for models roughly in order of their performance improvement. Examining the sparsity of activations across a given layer, divisive normalization layers increased sparsity, while the canonical normalization layers decreased it. Nonetheless, in the final layer, the sparseness of activity increased in the order of no normalization, divisive, com- bined, and canonical. We also investigated how the receptive fields (RFs) in the first convolutional layer (where RFs are most interpretable) change with normalization. Divisive normalization enhanced RF Fourier power at low wavelengths, while divisive+canonical enhanced power at mid (batch, group) or low (layer) wavelengths, compared to canonical alone or no normalization. In conclusion, divisive normalization enhances image recognition performance, most strongly when combined with canonical normalization, and in doing so it reduces manifold capacity and sparsity in early layers while increasing them in final layers, and increases low- or mid-wavelength power in the first-layer receptive fields",
    "checked": true,
    "id": "04e96e36267501a8a4ec1038d129445f8116241a",
    "semantic_title": "divisive feature normalization improves image recognition performance in alexnet",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=bTteFbU99ye": {
    "title": "Evaluating Distributional Distortion in Neural Language Modeling",
    "volume": "poster",
    "abstract": "A fundamental characteristic of natural language is the high rate at which speakers produce novel expressions. Because of this novelty, a heavy-tail of rare events accounts for a significant amount of the total probability mass of distributions in language (Baayen, 2001). Standard language modeling metrics such as perplexity quantify the performance of language models (LM) in aggregate. As a result, we have relatively little understanding of whether neural LMs accurately estimate the probability of sequences in this heavy-tail of rare events. To address this gap, we develop a controlled evaluation scheme which uses generative models trained on natural data as artificial languages from which we can exactly compute sequence probabilities. Training LMs on generations from these artificial languages, we compare the sequence-level probability estimates given by LMs to the true probabilities in the target language. Our experiments reveal that LSTM and Transformer language models (i) systematically underestimate the probability of sequences drawn from the target language, and (ii) do so more severely for less-probable sequences. Investigating where this probability mass went, (iii) we find that LMs tend to overestimate the probability of ill formed (perturbed) sequences. In addition, we find that this underestimation behaviour (iv) is weakened, but not eliminated by greater amounts of training data, and (v) is exacerbated for target distributions with lower entropy",
    "checked": true,
    "id": "d4550863c9b4102472a2326ab994aafdb13de7b9",
    "semantic_title": "evaluating distributional distortion in neural language modeling",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=r5qumLiYwf9": {
    "title": "MaGNET: Uniform Sampling from Deep Generative Network Manifolds Without Retraining",
    "volume": "poster",
    "abstract": "Deep Generative Networks (DGNs) are extensively employed in Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and their variants to approximate the data manifold, and data distribution on that manifold. However, training samples are often obtained based on preferences, costs, or convenience producing artifacts in the empirical data distribution e.g. the large fraction of smiling faces in the CelebA dataset or the large fraction of dark-haired individuals in FFHQ). {\\em These inconsistencies will be reproduced when sampling from the trained DGN, which has far-reaching potential implications for fairness, data augmentation, anomaly detection, domain adaptation, and beyond.} In response, we develop a differential geometry based sampler -coined MaGNET- that, given any trained DGN, produces samples that are uniformly distributed on the learned manifold. We prove theoretically and empirically that our technique produces a uniform distribution on the manifold regardless of the training set distribution. We perform a range of experiments on various datasets and DGNs. One of them considers the state-of-the-art StyleGAN2 trained on FFHQ dataset, where uniform sampling via MaGNET increases distribution precision \\& recall by 4.12\\% \\& 3.01\\% and decreases gender bias by 41.2\\%, without requiring labels or retraining",
    "checked": true,
    "id": "62f80fffbdc8b58870a2743cc24f931346406b0c",
    "semantic_title": "magnet: uniform sampling from deep generative network manifolds without retraining",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=xnYACQquaGV": {
    "title": "Neural Contextual Bandits with Deep Representation and Shallow Exploration",
    "volume": "poster",
    "abstract": "We study neural contextual bandits, a general class of contextual bandits, where each context-action pair is associated with a raw feature vector, but the specific reward generating function is unknown. We propose a novel learning algorithm that transforms the raw feature vector using the last hidden layer of a deep ReLU neural network (deep representation learning), and uses an upper confidence bound (UCB) approach to explore in the last linear layer (shallow exploration). We prove that under standard assumptions, our proposed algorithm achieves $\\tilde{O}(\\sqrt{T})$ finite-time regret, where $T$ is the learning time horizon. Compared with existing neural contextual bandit algorithms, our approach is computationally much more efficient since it only needs to explore in the last layer of the deep neural network",
    "checked": true,
    "id": "1355d22b6f85a28d46ce5ec31b1a4ba38e38147e",
    "semantic_title": "neural contextual bandits with deep representation and shallow exploration",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=NoB8YgRuoFU": {
    "title": "PI3NN: Out-of-distribution-aware Prediction Intervals from Three Neural Networks",
    "volume": "poster",
    "abstract": "We propose a novel prediction interval (PI) method for uncertainty quantification, which addresses three major issues with the state-of-the-art PI methods. First, existing PI methods require retraining of neural networks (NNs) for every given confidence level and suffer from the crossing issue in calculating multiple PIs. Second, they usually rely on customized loss functions with extra sensitive hyperparameters for which fine tuning is required to achieve a well-calibrated PI. Third, they usually underestimate uncertainties of out-of-distribution (OOD) samples leading to over-confident PIs. Our PI3NN method calculates PIs from linear combinations of three NNs, each of which is independently trained using the standard mean squared error loss. The coefficients of the linear combinations are computed using root-finding algorithms to ensure tight PIs for a given confidence level. We theoretically prove that PI3NN can calculate PIs for a series of confidence levels without retraining NNs and it completely avoids the crossing issue. Additionally, PI3NN does not introduce any unusual hyperparameters resulting in a stable performance. Furthermore, we address OOD identification challenge by introducing an initialization scheme which provides reasonably larger PIs of the OOD samples than those of the in-distribution samples. Benchmark and real-world experiments show that our method outperforms several state-of-the-art approaches with respect to predictive uncertainty quality, robustness, and OOD samples identification",
    "checked": true,
    "id": "f258d0b23a1d51e901fed6dd1e475e45993f5454",
    "semantic_title": "pi3nn: out-of-distribution-aware prediction intervals from three neural networks",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=kj0_45Y4r9i": {
    "title": "Discriminative Similarity for Data Clustering",
    "volume": "poster",
    "abstract": "Similarity-based clustering methods separate data into clusters according to the pairwise similarity between the data, and the pairwise similarity is crucial for their performance. In this paper, we propose {\\em Clustering by Discriminative Similarity (CDS)}, a novel method which learns discriminative similarity for data clustering. CDS learns an unsupervised similarity-based classifier from each data partition, and searches for the optimal partition of the data by minimizing the generalization error of the learnt classifiers associated with the data partitions. By generalization analysis via Rademacher complexity, the generalization error bound for the unsupervised similarity-based classifier is expressed as the sum of discriminative similarity between the data from different classes. It is proved that the derived discriminative similarity can also be induced by the integrated squared error bound for kernel density classification. In order to evaluate the performance of the proposed discriminative similarity, we propose a new clustering method using a kernel as the similarity function, CDS via unsupervised kernel classification (CDSK), with its effectiveness demonstrated by experimental results",
    "checked": true,
    "id": "c91abdc3d7400ef8e28c88cfcd3e016d6890e5f8",
    "semantic_title": "discriminative similarity for data clustering",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=q4tZR1Y-UIs": {
    "title": "It Takes Four to Tango: Multiagent Self Play for Automatic Curriculum Generation",
    "volume": "poster",
    "abstract": "We are interested in training general-purpose reinforcement learning agents that can solve a wide variety of goals. Training such agents efficiently requires automatic generation of a goal curriculum. This is challenging as it requires (a) exploring goals of increasing difficulty, while ensuring that the agent (b) is exposed to a diverse set of goals in a sample efficient manner and (c) does not catastrophically forget previously solved goals. We propose Curriculum Self Play (CuSP), an automated goal generation framework that seeks to satisfy these desiderata by virtue of a multi-player game with 4 agents. We extend the asymmetric curricula learning in PAIRED (Dennis et al., 2020) to a symmetrized game that carefully balances cooperation and competition between two off-policy student learners and two regret-maximizing teachers. CuSP additionally introduces entropic goal coverage and accounts for the non-stationary nature of the students, allowing us to automatically induce a curriculum that balances progressive exploration with anti-catastrophic exploitation. We demonstrate that our method succeeds at generating an effective curricula of goals for a range of control tasks, outperforming other methods at zero-shot test-time generalization to novel out-of-distribution goals",
    "checked": true,
    "id": "9fc6b6d3fead719cdc95263e62209548223576d1",
    "semantic_title": "it takes four to tango: multiagent selfplay for automatic curriculum generation",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=HOjLHrlZhmx": {
    "title": "CROP: Certifying Robust Policies for Reinforcement Learning through Functional Smoothing",
    "volume": "poster",
    "abstract": "As reinforcement learning (RL) has achieved great success and been even adopted in safety-critical domains such as autonomous vehicles, a range of empirical studies have been conducted to improve its robustness against adversarial attacks. However, how to certify its robustness with theoretical guarantees still remains challenging. In this paper, we present the Ô¨Årst uniÔ¨Åed framework CROP (Certifying Robust Policies for RL) to provide robustness certiÔ¨Åcation on both action and reward levels. In particular, we propose two robustness certiÔ¨Åcation criteria: robustness of per-state actions and lower bound of cumulative rewards. We then develop a local smoothing algorithm for policies derived from Q-functions to guarantee the robustness of actions taken along the trajectory; we also develop a global smoothing algorithm for certifying the lower bound of a Ô¨Ånite-horizon cumulative reward, as well as a novel local smoothing algorithm to perform adaptive search in order to obtain tighter reward certiÔ¨Åcation. Empirically, we apply CROP to evaluate several existing empirically robust RL algorithms, including adversarial training and different robust regularization, in four environments (two representative Atari games, Highway, and CartPole). Furthermore, by evaluating these algorithms against adversarial attacks, we demonstrate that our certiÔ¨Åcations are often tight. All experiment results are available at website https://crop-leaderboard.github.io",
    "checked": true,
    "id": "5374af7bb076f9eaea7aea3045edd9b6a76d0a3b",
    "semantic_title": "crop: certifying robust policies for reinforcement learning through functional smoothing",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=CCu6RcUMwK0": {
    "title": "Neural Link Prediction with Walk Pooling",
    "volume": "poster",
    "abstract": "Graph neural networks achieve high accuracy in link prediction by jointly leveraging graph topology and node attributes. Topology, however, is represented indirectly; state-of-the-art methods based on subgraph classification label nodes with distance to the target link, so that, although topological information is present, it is tempered by pooling. This makes it challenging to leverage features like loops and motifs associated with network formation mechanisms. We propose a link prediction algorithm based on a new pooling scheme called WalkPool. WalkPool combines the expressivity of topological heuristics with the feature-learning ability of neural networks. It summarizes a putative link by random walk probabilities of adjacent paths. Instead of extracting transition probabilities from the original graph, it computes the transition matrix of a ``predictive'' latent graph by applying attention to learned features; this may be interpreted as feature-sensitive topology fingerprinting. WalkPool can leverage unsupervised node features or be combined with GNNs and trained end-to-end. It outperforms state-of-the-art methods on all common link prediction benchmarks, both homophilic and heterophilic, with and without node attributes. Applying WalkPool to a set of unsupervised GNNs significantly improves prediction accuracy, suggesting that it may be used as a general-purpose graph pooling scheme",
    "checked": true,
    "id": "610621a8ac941ae6c6781250a3cdb705616c983c",
    "semantic_title": "neural link prediction with walk pooling",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=YeShU5mLfLt": {
    "title": "On the Convergence of Certified Robust Training with Interval Bound Propagation",
    "volume": "poster",
    "abstract": "Interval Bound Propagation (IBP) is so far the base of state-of-the-art methods for training neural networks with certifiable robustness guarantees when potential adversarial perturbations present, while the convergence of IBP training remains unknown in existing literature. In this paper, we present a theoretical analysis on the convergence of IBP training. With an overparameterized assumption, we analyze the convergence of IBP robust training. We show that when using IBP training to train a randomly initialized two-layer ReLU neural network with logistic loss, gradient descent can linearly converge to zero robust training error with a high probability if we have sufficiently small perturbation radius and large network width",
    "checked": true,
    "id": "889dc6112e4cfa6ac2557c9d3a4b47bc40854ba3",
    "semantic_title": "on the convergence of certified robust training with interval bound propagation",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=sX3XaHwotOg": {
    "title": "Pretraining Text Encoders with Adversarial Mixture of Training Signal Generators",
    "volume": "poster",
    "abstract": "We present a new framework AMOS that pretrains text encoders with an Adversarial learning curriculum via a Mixture Of Signals from multiple auxiliary generators. Following ELECTRA-style pretraining, the main encoder is trained as a discriminator to detect replaced tokens generated by auxiliary masked language models (MLMs). Different from ELECTRA which trains one MLM as the generator, we jointly train multiple MLMs of different sizes to provide training signals at various levels of difficulty. To push the discriminator to learn better with challenging replaced tokens, we learn mixture weights over the auxiliary MLMs' outputs to maximize the discriminator loss by backpropagating the gradient from the discriminator via Gumbel-Softmax. For better pretraining efficiency, we propose a way to assemble multiple MLMs into one unified auxiliary model. AMOS outperforms ELECTRA and recent state-of-the-art pretrained models by about 1 point on the GLUE benchmark for BERT base-sized models",
    "checked": true,
    "id": "02f36289e227595dd2786b32c3975ce7e61dff48",
    "semantic_title": "pretraining text encoders with adversarial mixture of training signal generators",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=0jP2n0YFmKG": {
    "title": "Towards Training Billion Parameter Graph Neural Networks for Atomic Simulations",
    "volume": "poster",
    "abstract": "Recent progress in Graph Neural Networks (GNNs) for modeling atomic simulations has the potential to revolutionize catalyst discovery, which is a key step in making progress towards the energy breakthroughs needed to combat climate change. However, the GNNs that have proven most effective for this task are memory intensive as they model higher-order interactions in the graphs such as those between triplets or quadruplets of atoms, making it challenging to scale these models. In this paper, we introduce Graph Parallelism, a method to distribute input graphs across multiple GPUs, enabling us to train very large GNNs with hundreds of millions or billions of parameters. We empirically evaluate our method by scaling up the recently proposed DimeNet++ and GemNet models by over an order of magnitude in the number of parameters. On the large-scale Open Catalyst 2020 (OC20) dataset, these graph-parallelized models lead to relative improvements of 1) 15% on the force MAE metric on the S2EF task and 2) 21% on the AFbT metric on the IS2RS task, establishing new state-of-the-art results",
    "checked": true,
    "id": "6a62ca8c617656caef26a31b13e1cadab6e996e3",
    "semantic_title": "towards training billion parameter graph neural networks for atomic simulations",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=shbAgEsk3qM": {
    "title": "Understanding and Leveraging Overparameterization in Recursive Value Estimation",
    "volume": "poster",
    "abstract": "The theory of function approximation in reinforcement learning (RL) typically considers low capacity representations that incur a tradeoff between approximation error, stability and generalization. Current deep architectures, however, operate in an overparameterized regime where approximation error is not necessarily a bottleneck. To better understand the utility of deep models in RL we present an analysis of recursive value estimation using \\emph{overparameterized} linear representations that provides useful, transferable findings. First, we show that classical updates such as temporal difference (TD) learning or fitted-value-iteration (FVI) converge to \\emph{different} fixed points than residual minimization (RM) in the overparameterized linear case. We then develop a unified interpretation of overparameterized linear value estimation as minimizing the Euclidean norm of the weights subject to alternative constraints. A practical consequence is that RM can be modified by a simple alteration of the backup targets to obtain the same fixed points as FVI and TD (when they converge), while universally ensuring stability. Further, we provide an analysis of the generalization error of these methods, demonstrating per iterate bounds on the value prediction error of FVI, and fixed point bounds for TD and RM. Given this understanding, we then develop new algorithmic tools for improving recursive value estimation with deep models. In particular, we extract two regularizers that penalize out-of-span top-layer weights and co-linearity in top-layer features respectively. Empirically we find that these regularizers dramatically improve the stability of TD and FVI, while allowing RM to match and even sometimes surpass their generalization performance with assured stability",
    "checked": true,
    "id": "46f100e7f8a8e73b38371d4ddef110fece012031",
    "semantic_title": "understanding and leveraging overparameterization in recursive value estimation",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=dPyRNUlttBv": {
    "title": "Optimization and Adaptive Generalization of Three layer Neural Networks",
    "volume": "poster",
    "abstract": "While there has been substantial recent work studying generalization of neural networks, the ability of deep nets in automating the process of feature extraction still evades a thorough mathematical understanding. As a step toward this goal, we analyze learning and generalization of a three-layer neural network with ReLU activations in a regime that goes beyond the linear approximation of the network, and is hence not captured by the common Neural Tangent Kernel. We show that despite nonconvexity of the empirical loss, a variant of SGD converges in polynomially many iterations to a good solution that generalizes. In particular, our generalization bounds are adaptive: they automatically optimize over a family of kernels that includes the Neural Tangent Kernel, to provide the tightest bound",
    "checked": true,
    "id": "bee817efbcd3e1e67995ea033462540bad5e66d4",
    "semantic_title": "optimization and adaptive generalization of three layer neural networks",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=-TSe5o7STVR": {
    "title": "Non-Parallel Text Style Transfer with Self-Parallel Supervision",
    "volume": "poster",
    "abstract": "The performance of existing text style transfer models is severely limited by the non-parallel datasets on which the models are trained. In non-parallel datasets, no direct mapping exists between sentences of the source and target style; the style transfer models thus only receive weak supervision of the target sentences during training, which often leads the model to discard too much style-independent information, or utterly fail to transfer the style. In this work, we propose LaMer, a novel text style transfer framework based on large-scale language models. LaMer first mines the roughly parallel expressions in the non-parallel datasets with scene graphs, and then employs MLE training, followed by imitation learning refinement, to leverage the intrinsic parallelism within the data. On two benchmark tasks (sentiment & formality transfer) and a newly proposed challenging task (political stance transfer), our model achieves qualitative advances in transfer accuracy, content preservation, and fluency. Further empirical and human evaluations demonstrate that our model not only makes training more efficient, but also generates more readable and diverse expressions than previous models",
    "checked": true,
    "id": "85e5ed73c769141c5eb2f51410c8979818169cde",
    "semantic_title": "non-parallel text style transfer with self-parallel supervision",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=qhkFX-HLuHV": {
    "title": "Can an Image Classifier Suffice For Action Recognition?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bd189ee2c5860461d0165a2971f5ff32b66b8c41",
    "semantic_title": "can an image classifier suffice for action recognition?",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=IK9ap6nxXr2": {
    "title": "Interacting Contour Stochastic Gradient Langevin Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4210a4e8e3396e8099ef457ca3312d6d653c9d2f",
    "semantic_title": "interacting contour stochastic gradient langevin dynamics",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=MIX3fJkl_1": {
    "title": "NeuPL: Neural Population Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "080072f28bd52a35c01ee4caecf94887eb3d54c9",
    "semantic_title": "neupl: neural population learning",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=hniLRD_XCA": {
    "title": "DeSKO: Stability-Assured Robust Control with a Deep Stochastic Koopman Operator",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6d3b65097096bb0ec52896e9d8a8ad194f18d290",
    "semantic_title": "desko: stability-assured robust control with a deep stochastic koopman operator",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=oiZJwC_fyS": {
    "title": "Neural Network Approximation based on Hausdorff distance of Tropical Zonotopes",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d42ebfcac832ccf54fd40ddc8a84050a6a0e609f",
    "semantic_title": "neural network approximation based on hausdorff distance of tropical zonotopes",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=hqkhcFHOeKD": {
    "title": "Learning Towards The Largest Margins",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b1207872d58030bc85b798cd8c1ef8570bb5ee3b",
    "semantic_title": "learning towards the largest margins",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=28ib9tf6zhr": {
    "title": "Patch-Fool: Are Vision Transformers Always Robust Against Adversarial Perturbations?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c24f4fcec3f1fba7a741f6348aafe5899d06ef77",
    "semantic_title": "patch-fool: are vision transformers always robust against adversarial perturbations?",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=Q5uh1Nvv5dm": {
    "title": "AdaMatch: A Unified Approach to Semi-Supervised Learning and Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f847384ad644ce5123311231bd30313d90a82b5a",
    "semantic_title": "adamatch: a unified approach to semi-supervised learning and domain adaptation",
    "citation_count": 159,
    "authors": []
  },
  "https://openreview.net/forum?id=l_amHf1oaK": {
    "title": "Complete Verification via Multi-Neuron Relaxation Guided Branch-and-Bound",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d7aff3e7c84e8440b49a3cd36f686c6486094956",
    "semantic_title": "complete verification via multi-neuron relaxation guided branch-and-bound",
    "citation_count": 93,
    "authors": []
  },
  "https://openreview.net/forum?id=VFBjuF8HEp": {
    "title": "Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7e839c2667479d91e21e84583c27257dc7dc1a36",
    "semantic_title": "learning fast samplers for diffusion models by differentiating through sample quality",
    "citation_count": 199,
    "authors": []
  },
  "https://openreview.net/forum?id=lzupY5zjaU9": {
    "title": "Distribution Compression in Near-Linear Time",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9ede06642621f187412d1ded62e2d672ac8c1149",
    "semantic_title": "distribution compression in near-linear time",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=nnU3IUMJmN": {
    "title": "Capturing Structural Locality in Non-parametric Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c6bb04f3d8000b7e800f6359082de39548c7da79",
    "semantic_title": "capturing structural locality in non-parametric language models",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=9Nk6AJkVYB": {
    "title": "Audio Lottery: Speech Recognition Made Ultra-Lightweight, Noise-Robust, and Transferable",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2cecb623ba2e2ee4872bb07b794552fd73a87976",
    "semantic_title": "audio lottery: speech recognition made ultra-lightweight, noise-robust, and transferable",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=swrMQttr6wN": {
    "title": "Learning to Map for Active Semantic Goal Navigation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cdfe59dd102d8f8e780c893f3438eaf7723aecea",
    "semantic_title": "learning to map for active semantic goal navigation",
    "citation_count": 87,
    "authors": []
  },
  "https://openreview.net/forum?id=1W0z96MFEoH": {
    "title": "Benchmarking the Spectrum of Agent Capabilities",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8e128a1b2efb0ddf688902ade4405d22d5b61eec",
    "semantic_title": "benchmarking the spectrum of agent capabilities",
    "citation_count": 162,
    "authors": []
  },
  "https://openreview.net/forum?id=vqGi8Kp0wM": {
    "title": "Mind the Gap: Domain Gap Control for Single Shot Domain Adaptation for Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c17cfe3add196c1d66a7981f7246363bfe4f31e4",
    "semantic_title": "mind the gap: domain gap control for single shot domain adaptation for generative adversarial networks",
    "citation_count": 87,
    "authors": []
  },
  "https://openreview.net/forum?id=EnwCZixjSh": {
    "title": "On Evaluation Metrics for Graph Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "410f0a0a2311c50e6dd2338f2708286ea8c87f23",
    "semantic_title": "on evaluation metrics for graph generative models",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=HfUyCRBeQc": {
    "title": "Selective Ensembles for Consistent Predictions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9cf305d6c9e8ffb571b28aea718ff42d1a9efdc7",
    "semantic_title": "selective ensembles for consistent predictions",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=WLEx3Jo4QaB": {
    "title": "Graph Condensation for Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5f3173e24d17b92a96e82d0499b365f341edfcd2",
    "semantic_title": "graph condensation for graph neural networks",
    "citation_count": 171,
    "authors": []
  },
  "https://openreview.net/forum?id=bVvMOtLMiw": {
    "title": "DIVA: Dataset Derivative of a Learning Task",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "892e110278714f9a40c942f17ec2bd851e199895",
    "semantic_title": "diva: dataset derivative of a learning task",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=sA4qIu3zv6v": {
    "title": "Towards General Function Approximation in Zero-Sum Markov Games",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "eb46163d767f7de1d3de725909b73d04a1eefd68",
    "semantic_title": "towards general function approximation in zero-sum markov games",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=6PvWo1kEvlT": {
    "title": "Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis--Hastings",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f71d0c4cae964a71c8a2f2c0919075d3bb18ca63",
    "semantic_title": "exposing the implicit energy networks behind masked language models via metropolis-hastings",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=EZNOb_uNpJk": {
    "title": "ClimateGAN: Raising Climate Change Awareness by Generating Images of Floods",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "05e4b8d6db3dac820734482ccf67ca2a0bd29263",
    "semantic_title": "climategan: raising climate change awareness by generating images of floods",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=nhN-fqxmNGx": {
    "title": "A Comparison of Hamming Errors of Representative Variable Selection Methods",
    "volume": "poster",
    "abstract": "Lasso is a celebrated method for variable selection in linear models, but it faces challenges when the covariates are moderately or strongly correlated. This motivates alternative approaches such as using a non-convex penalty, adding a ridge regularization, or conducting a post-Lasso thresholding. In this paper, we compare Lasso with 5 other methods: Elastic net, SCAD, forward selection, thresholded Lasso, and forward backward selection. We measure their performances theoretically by the expected Hamming error, assuming that the regression coefficients are ${\\it iid}$ drawn from a two-point mixture and that the Gram matrix is block-wise diagonal. By deriving the rates of convergence of Hamming errors and the phase diagrams, we obtain useful conclusions about the pros and cons of different methods",
    "checked": true,
    "id": "42386ac380737c8879f7d9cb9e413114672e47e7",
    "semantic_title": "a comparison of hamming errors of representative variable selection methods",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WE4qe9xlnQw": {
    "title": "A Program to Build E(N)-Equivariant Steerable CNNs",
    "volume": "poster",
    "abstract": "Equivariance is becoming an increasingly popular design choice to build data efficient neural networks by exploiting prior knowledge about the symmetries of the problem at hand. Euclidean steerable CNNs are one of the most common classes of equivariant networks. While the constraints these architectures need to satisfy are understood, existing approaches are tailored to specific (classes of) groups. No generally applicable method that is practical for implementation has been described so far. In this work, we generalize the Wigner-Eckart theorem proposed in Lang & Weiler (2020), which characterizes general $G$-steerable kernel spaces for compact groups $G$ over their homogeneous spaces, to arbitrary $G$-spaces. This enables us to directly parameterize filters in terms of a band-limited basis on the whole space rather than on $G$'s orbits, but also to easily implement steerable CNNs equivariant to a large number of groups. To demonstrate its generality, we instantiate our method on a variety of isometry groups acting on the Euclidean space $\\mathbb{R}^3$. Our framework allows us to build $E(3)$ and $SE(3)$-steerable CNNs like previous works, but also CNNs with arbitrary $G\\leq O(3)$-steerable kernels. For example, we build 3D CNNs equivariant to the symmetries of platonic solids or choose $G=SO(2)$ when working with 3D data having only azimuthal symmetries. We compare these models on 3D shapes and molecular datasets, observing improved performance by matching the model's symmetries to the ones of the data",
    "checked": true,
    "id": "24a0cc377b41f361b2ef6500f28446c986486717",
    "semantic_title": "a program to build e(n)-equivariant steerable cnns",
    "citation_count": 113,
    "authors": []
  },
  "https://openreview.net/forum?id=UdxJ2fJx7N0": {
    "title": "Minimax Optimization with Smooth Algorithmic Adversaries",
    "volume": "poster",
    "abstract": "This paper considers minimax optimization $\\min_x \\max_y f(x, y)$ in the challenging setting where $f$ can be both nonconvex in $x$ and nonconcave in $y$. Though such optimization problems arise in many machine learning paradigms including training generative adversarial networks (GANs) and adversarially robust models, from a theoretical point of view, two fundamental issues remain: (i) the absence of simple and efficiently computable optimality notions, and (ii) cyclic or diverging behavior of existing algorithms. This paper proposes a new theoretical framework for nonconvex-nonconcave minimax optimization that addresses both of the above issues. The starting point of this paper is the observation that, under a computational budget, the max-player can not fully maximize $f(x,\\cdot)$ since nonconcave maximization is NP-hard in general. So, we propose a new framework, and a corresponding algorithm, for the min-player to play against \\emph{smooth algorithms} deployed by the adversary (i.e., the max-player) instead of against full maximization. Our algorithm is guaranteed to make monotonic progress (thus having no limit cycles or diverging behavior), and to find an appropriate ``stationary point'' in a polynomial number of iterations. Our framework covers practically relevant settings where the smooth algorithms deployed by the adversary are multi-step stochastic gradient ascent, and its accelerated version. We further present experimental results that confirm our theoretical findings and demonstrate the effectiveness of the proposed approach in practice on simple, conceptual settings",
    "checked": true,
    "id": "dbdefb498b619912a726fec7c85533594a1c6a1b",
    "semantic_title": "minimax optimization with smooth algorithmic adversaries",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=CI-xXX9dg9l": {
    "title": "On Distributed Adaptive Optimization with Gradient Compression",
    "volume": "poster",
    "abstract": "We study COMP-AMS, a distributed optimization framework based on gradient averaging and adaptive AMSGrad algorithm. Gradient compression with error feedback is applied to reduce the communication cost in the gradient transmission process. Our convergence analysis of COMP-AMS shows that such compressed gradient averaging strategy yields same convergence rate as standard AMSGrad, and also exhibits the linear speedup effect w.r.t. the number of local workers. Compared with recently proposed protocols on distributed adaptive methods, COMP-AMS is simple and convenient. Numerical experiments are conducted to justify the theoretical findings, and demonstrate that the proposed method can achieve same test accuracy as the full-gradient AMSGrad with substantial communication savings. With its simplicity and efficiency, COMP-AMS can serve as a useful distributed training framework for adaptive methods",
    "checked": true,
    "id": "bf64a012b7d8fc831da367fc56419e5bf9c968c7",
    "semantic_title": "on distributed adaptive optimization with gradient compression",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=o_HsiMPYh_x": {
    "title": "Leveraging unlabeled data to predict out-of-distribution performance",
    "volume": "poster",
    "abstract": "Real-world machine learning deployments are characterized by mismatches between the source (training) and target (test) distributions that may cause performance drops. In this work, we investigate methods for predicting the target domain accuracy using only labeled source data and unlabeled target data. We propose Average Thresholded Confidence (ATC), a practical method that learns a \\emph{threshold} on the model's confidence, predicting accuracy as the fraction of unlabeled examples for which model confidence exceeds that threshold. ATC outperforms previous methods across several model architectures, types of distribution shifts (e.g., due to synthetic corruptions, dataset reproduction, or novel subpopulations), and datasets (\\textsc{Wilds}-FMoW, ImageNet, \\breeds, CIFAR, and MNIST). In our experiments, ATC estimates target performance $2\\text{--}4\\times$ more accurately than prior methods. We also explore the theoretical foundations of the problem, proving that, in general, identifying the accuracy is just as hard as identifying the optimal predictor and thus, the efficacy of any method rests upon (perhaps unstated) assumptions on the nature of the shift. Finally, analyzing our method on some toy distributions, we provide insights concerning when it works",
    "checked": true,
    "id": "042fe4e996a1133e2c4c11f70aa2654e88f31687",
    "semantic_title": "leveraging unlabeled data to predict out-of-distribution performance",
    "citation_count": 146,
    "authors": []
  },
  "https://openreview.net/forum?id=7udZAsEzd60": {
    "title": "VC dimension of partially quantized neural networks in the overparametrized regime",
    "volume": "poster",
    "abstract": "Vapnik-Chervonenkis (VC) theory has so far been unable to explain the small generalization error of overparametrized neural networks. Indeed, existing applications of VC theory to large networks obtain upper bounds on VC dimension that are proportional to the number of weights, and for a large class of networks, these upper bound are known to be tight. In this work, we focus on a class of partially quantized networks that we refer to as hyperplane arrangement neural networks (HANNs). Using a sample compression analysis, we show that HANNs can have VC dimension significantly smaller than the number of weights, while being highly expressive. In particular, empirical risk minimization over HANNs in the overparametrized regime achieves the minimax rate for classification with Lipschitz posterior class probability. We further demonstrate the expressivity of HANNs empirically. On a panel of 121 UCI datasets, overparametrized HANNs are able to match the performance of state-of-the-art full-precision models",
    "checked": true,
    "id": "7e52442932a8c316a87028964bd43a1e7442eba3",
    "semantic_title": "vc dimension of partially quantized neural networks in the overparametrized regime",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Rf58LPCwJj0": {
    "title": "Optimal Representations for Covariate Shift",
    "volume": "poster",
    "abstract": "Machine learning systems often experience a distribution shift between training and testing. In this paper, we introduce a simple variational objective whose optima are exactly the set of all representations on which risk minimizers are guaranteed to be robust to any distribution shift that preserves the Bayes predictor, e.g., covariate shifts. Our objective has two components. First, a representation must remain discriminative for the task, i.e., some predictor must be able to simultaneously minimize the source and target risk. Second, the representation's marginal support needs to be the same across source and target. We make this practical by designing self-supervised objectives that only use unlabelled data and augmentations to train robust representations. Our objectives give insights into the robustness of CLIP, and further improve CLIP's representations to achieve SOTA results on DomainBed",
    "checked": true,
    "id": "5382d9bc17aabfd47b7c7d9873d2b64fdde48305",
    "semantic_title": "optimal representations for covariate shift",
    "citation_count": 74,
    "authors": []
  },
  "https://openreview.net/forum?id=ei3SY1_zYsE": {
    "title": "Fortuitous Forgetting in Connectionist Networks",
    "volume": "poster",
    "abstract": "Forgetting is often seen as an unwanted characteristic in both human and machine learning. However, we propose that forgetting can in fact be favorable to learning. We introduce forget-and-relearn as a powerful paradigm for shaping the learning trajectories of artificial neural networks. In this process, the forgetting step selectively removes undesirable information from the model, and the relearning step reinforces features that are consistently useful under different conditions. The forget-and-relearn framework unifies many existing iterative training algorithms in the image classification and language emergence literature, and allows us to understand the success of these algorithms in terms of the disproportionate forgetting of undesirable information. We leverage this understanding to improve upon existing algorithms by designing more targeted forgetting operations. Insights from our analysis provide a coherent view on the dynamics of iterative training in neural networks and offer a clear path towards performance improvements",
    "checked": true,
    "id": "48ec22d24a83d3dfc12e4a6bac8bf77af1f41c3e",
    "semantic_title": "fortuitous forgetting in connectionist networks",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=So6YAqnqgMj": {
    "title": "EigenGame Unloaded: When playing games is better than optimizing",
    "volume": "poster",
    "abstract": "We build on the recently proposed EigenGame that views eigendecomposition as a competitive game. EigenGame's updates are biased if computed using minibatches of data, which hinders convergence and more sophisticated parallelism in the stochastic setting. In this work, we propose an unbiased stochastic update that is asymptotically equivalent to EigenGame, enjoys greater parallelism allowing computation on datasets of larger sample sizes, and outperforms EigenGame in experiments. We present applications to finding the principal components of massive datasets and performing spectral clustering of graphs. We analyze and discuss our proposed update in the context of EigenGame and the shift in perspective from optimization to games",
    "checked": true,
    "id": "f9464bda6ca2e1ae20a335163b0172a3fcf599f8",
    "semantic_title": "eigengame unloaded: when playing games is better than optimizing",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=Oh1r2wApbPv": {
    "title": "Contextualized Scene Imagination for Generative Commonsense Reasoning",
    "volume": "poster",
    "abstract": "Humans use natural language to compose common concepts from their environment into plausible, day-to-day scene descriptions. However, such generative commonsense reasoning (GCSR) skills are lacking in state-of-the-art text generation methods. Descriptive sentences about arbitrary concepts generated by neural text generation models (e.g., pre-trained text-to-text Transformers) are often grammatically fluent but may not correspond to human common sense, largely due to their lack of mechanisms to capture concept relations, to identify implicit concepts, and to perform generalizable reasoning about unseen concept compositions. In this paper, we propose an Imagine-and-Verbalize (I\\&V) method, which learns to imagine a relational scene knowledge graph (SKG) with relations between the input concepts, and leverage the SKG as a constraint when generating a plausible scene description. We collect and harmonize a set of knowledge resources from different domains and modalities, providing a rich auxiliary supervision signal for I\\&V. The experiments demonstrate the effectiveness of I\\&V in improving language models on both concept-to-sentence and concept-to-story generation tasks, while enabling the model to learn well from fewer task examples and generate SKGs that make common sense to human annotators",
    "checked": true,
    "id": "cc37f5821b2ea4c4528b5ba3b96f302db7ec3870",
    "semantic_title": "contextualized scene imagination for generative commonsense reasoning",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=Wm3EA5OlHsG": {
    "title": "Scene Transformer: A unified architecture for predicting future trajectories of multiple agents",
    "volume": "poster",
    "abstract": "Predicting the motion of multiple agents is necessary for planning in dynamic environments. This task is challenging for autonomous driving since agents (e.g., vehicles and pedestrians) and their associated behaviors may be diverse and influence one another. Most prior work have focused on predicting independent futures for each agent based on all past motion, and planning against these independent predictions. However, planning against independent predictions can make it challenging to represent the future interaction possibilities between different agents, leading to sub-optimal planning. In this work, we formulate a model for predicting the behavior of all agents jointly, producing consistent futures that account for interactions between agents. Inspired by recent language modeling approaches, we use a masking strategy as the query to our model, enabling one to invoke a single model to predict agent behavior in many ways, such as potentially conditioned on the goal or full future trajectory of the autonomous vehicle or the behavior of other agents in the environment. Our model architecture employs attention to combine features across road elements, agent interactions, and time steps. We evaluate our approach on autonomous driving datasets for both marginal and joint motion prediction, and achieve state of the art performance across two popular datasets. Through combining a scene-centric approach, agent permutation equivariant model, and a sequence masking strategy, we show that our model can unify a variety of motion prediction tasks from joint motion predictions to conditioned prediction",
    "checked": true,
    "id": "a4bc6aee56e91c8881c421fa3472e2d211e0bdb2",
    "semantic_title": "scene transformer: a unified architecture for predicting future trajectories of multiple agents",
    "citation_count": 155,
    "authors": []
  },
  "https://openreview.net/forum?id=qY79G8jGsep": {
    "title": "DISSECT: Disentangled Simultaneous Explanations via Concept Traversals",
    "volume": "poster",
    "abstract": "Explaining deep learning model inferences is a promising venue for scientific understanding, improving safety, uncovering hidden biases, evaluating fairness, and beyond, as argued by many scholars. One of the principal benefits of counterfactual explanations is allowing users to explore \"what-if\" scenarios through what does not and cannot exist in the data, a quality that many other forms of explanation such as heatmaps and influence functions are inherently incapable of doing. However, most previous work on generative explainability cannot disentangle important concepts effectively, produces unrealistic examples, or fails to retain relevant information. We propose a novel approach, DISSECT, that jointly trains a generator, a discriminator, and a concept disentangler to overcome such challenges using little supervision. DISSECT generates Concept Traversals (CTs), defined as a sequence of generated examples with increasing degrees of concepts that influence a classifier's decision. By training a generative model from a classifier's signal, DISSECT offers a way to discover a classifier's inherent \"notion\" of distinct concepts automatically rather than rely on user-predefined concepts. We show that DISSECT produces CTs that (1) disentangle several concepts, (2) are influential to a classifier's decision and are coupled to its reasoning due to joint training (3), are realistic, (4) preserve relevant information, and (5) are stable across similar inputs. We validate DISSECT on several challenging synthetic and realistic datasets where previous methods fall short of satisfying desirable criteria for interpretability and show that it performs consistently well. Finally, we present experiments showing applications of DISSECT for detecting potential biases of a classifier and identifying spurious artifacts that impact predictions",
    "checked": true,
    "id": "28fc865105bf91c70d13e7e19effc53da9d247b1",
    "semantic_title": "dissect: disentangled simultaneous explanations via concept traversals",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=Az7opqbQE-3": {
    "title": "Heteroscedastic Temporal Variational Autoencoder For Irregularly Sampled Time Series",
    "volume": "poster",
    "abstract": "Irregularly sampled time series commonly occur in several domains where they present a significant challenge to standard deep learning models. In this paper, we propose a new deep learning framework for probabilistic interpolation of irregularly sampled time series that we call the Heteroscedastic Temporal Variational Autoencoder (HeTVAE). HeTVAE includes a novel input layer to encode information about input observation sparsity, a temporal VAE architecture to propagate uncertainty due to input sparsity, and a heteroscedastic output layer to enable variable uncertainty in the output interpolations. Our results show that the proposed architecture is better able to reflect variable uncertainty through time due to sparse and irregular sampling than a range of baseline and traditional models, as well as recently proposed deep latent variable models that use homoscedastic output layers",
    "checked": true,
    "id": "02ddd2ac73c072d25ed8676b6a425b22f4963bb3",
    "semantic_title": "heteroscedastic temporal variational autoencoder for irregularly sampled time series",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=vUH85MOXO7h": {
    "title": "A Neural Tangent Kernel Perspective of Infinite Tree Ensembles",
    "volume": "poster",
    "abstract": "In practical situations, the tree ensemble is one of the most popular models along with neural networks. A soft tree is a variant of a decision tree. Instead of using a greedy method for searching splitting rules, the soft tree is trained using a gradient method in which the entire splitting operation is formulated in a differentiable form. Although ensembles of such soft trees have been used increasingly in recent years, little theoretical work has been done to understand their behavior. By considering an ensemble of infinite soft trees, this paper introduces and studies the Tree Neural Tangent Kernel (TNTK), which provides new insights into the behavior of the infinite ensemble of soft trees. Using the TNTK, we theoretically identify several non-trivial properties, such as global convergence of the training, the equivalence of the oblivious tree structure, and the degeneracy of the TNTK induced by the deepening of the trees",
    "checked": true,
    "id": "05f9306ad29607699d033a6fe97e4c9a1565151f",
    "semantic_title": "a neural tangent kernel perspective of infinite tree ensembles",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=nKWjE4QF1hB": {
    "title": "AlphaZero-based Proof Cost Network to Aid Game Solving",
    "volume": "poster",
    "abstract": "The AlphaZero algorithm learns and plays games without hand-crafted expert knowledge. However, since its objective is to play well, we hypothesize that a better objective can be defined for the related but separate task of solving games. This paper proposes a novel approach to solving problems by modifying the training target of the AlphaZero algorithm, such that it prioritizes solving the game quickly, rather than winning. We train a Proof Cost Network (PCN), where proof cost is a heuristic that estimates the amount of work required to solve problems. This matches the general concept of the so-called proof number from proof number search, which has been shown to be well-suited for game solving. We propose two specific training targets. The first finds the shortest path to a solution, while the second estimates the proof cost. We conduct experiments on solving 15x15 Gomoku and 9x9 Killall-Go problems with both MCTS-based and FDFPN solvers. Comparisons between using AlphaZero networks and PCN as heuristics show that PCN can solve more problems",
    "checked": true,
    "id": "6305bb7712d21a090f65832df5194c62260b330b",
    "semantic_title": "alphazero-based proof cost network to aid game solving",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=f2lrIbGx3x7": {
    "title": "Bayesian Framework for Gradient Leakage",
    "volume": "poster",
    "abstract": "Federated learning is an established method for training machine learning models without sharing training data. However, recent work has shown that it cannot guarantee data privacy as shared gradients can still leak sensitive information. To formalize the problem of gradient leakage, we propose a theoretical framework that enables, for the first time, analysis of the Bayes optimal adversary phrased as an optimization problem. We demonstrate that existing leakage attacks can be seen as approximations of this optimal adversary with different assumptions on the probability distributions of the input data and gradients. Our experiments confirm the effectiveness of the Bayes optimal adversary when it has knowledge of the underlying distribution. Further, our experimental evaluation shows that several existing heuristic defenses are not effective against stronger attacks, especially early in the training process. Thus, our findings indicate that the construction of more effective defenses and their evaluation remains an open problem",
    "checked": true,
    "id": "2e9064208fd23c998f67f79531346504c9cfc7f3",
    "semantic_title": "bayesian framework for gradient leakage",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=YpPiNigTzMT": {
    "title": "Universalizing Weak Supervision",
    "volume": "poster",
    "abstract": "Weak supervision (WS) frameworks are a popular way to bypass hand-labeling large datasets for training data-hungry models. These approaches synthesize multiple noisy but cheaply-acquired estimates of labels into a set of high-quality pseudo-labels for downstream training. However, the synthesis technique is specific to a particular kind of label, such as binary labels or sequences, and each new label type requires manually designing a new synthesis algorithm. Instead, we propose a universal technique that enables weak supervision over any label type while still offering desirable properties, including practical flexibility, computational efficiency, and theoretical guarantees. We apply this technique to important problems previously not tackled by WS frameworks including learning to rank, regression, and learning in hyperbolic space. Theoretically, our synthesis approach produces a consistent estimators for learning some challenging but important generalizations of the exponential family model. Experimentally, we validate our framework and show improvement over baselines in diverse settings including real-world learning-to-rank and regression problems along with learning on hyperbolic manifolds",
    "checked": true,
    "id": "f1a1cbdac308f9122d415e35827126e345e22b73",
    "semantic_title": "universalizing weak supervision",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=ULfq0qR25dY": {
    "title": "Maximum n-times Coverage for Vaccine Design",
    "volume": "poster",
    "abstract": "We introduce the maximum $n$-times coverage problem that selects $k$ overlays to maximize the summed coverage of weighted elements, where each element must be covered at least $n$ times. We also define the min-cost $n$-times coverage problem where the objective is to select the minimum set of overlays such that the sum of the weights of elements that are covered at least $n$ times is at least $\\tau$. Maximum $n$-times coverage is a generalization of the multi-set multi-cover problem, is NP-complete, and is not submodular. We introduce two new practical solutions for $n$-times coverage based on integer linear programming and sequential greedy optimization. We show that maximum $n$-times coverage is a natural way to frame peptide vaccine design, and find that it produces a pan-strain COVID-19 vaccine design that is superior to 29 other published designs in predicted population coverage and the expected number of peptides displayed by each individual's HLA molecules",
    "checked": true,
    "id": "32b2818ecf36bd8ad9a5b0e864c7f1326717920d",
    "semantic_title": "maximum n-times coverage for vaccine design",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=0JzqUlIVVDd": {
    "title": "KL Guided Domain Adaptation",
    "volume": "poster",
    "abstract": "Domain adaptation is an important problem and often needed for real-world applications. In this problem, instead of i.i.d. training and testing datapoints, we assume that the source (training) data and the target (testing) data have different distributions. With that setting, the empirical risk minimization training procedure often does not perform well, since it does not account for the change in the distribution. A common approach in the domain adaptation literature is to learn a representation of the input that has the same (marginal) distribution over the source and the target domain. However, these approaches often require additional networks and/or optimizing an adversarial (minimax) objective, which can be very expensive or unstable in practice. To improve upon these marginal alignment techniques, in this paper, we first derive a generalization bound for the target loss based on the training loss and the reverse Kullback-Leibler (KL) divergence between the source and the target representation distributions. Based on this bound, we derive an algorithm that minimizes the KL term to obtain a better generalization to the target domain. We show that with a probabilistic representation network, the KL term can be estimated efficiently via minibatch samples without any additional network or a minimax objective. This leads to a theoretically sound alignment method which is also very efficient and stable in practice. Experimental results also suggest that our method outperforms other representation-alignment approaches",
    "checked": true,
    "id": "aab9863570372c6847fe40581332579388178665",
    "semantic_title": "kl guided domain adaptation",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=Mspk_WYKoEH": {
    "title": "From Stars to Subgraphs: Uplifting Any GNN with Local Structure Awareness",
    "volume": "poster",
    "abstract": "Message Passing Neural Networks (MPNNs) are a common type of Graph Neural Network (GNN), in which each node's representation is computed recursively by aggregating representations (\"messages\") from its immediate neighbors akin to a star-shaped pattern. MPNNs are appealing for being efficient and scalable, however their expressiveness is upper-bounded by the 1st-order Weisfeiler-Lehman isomorphism test (1-WL). In response, prior works propose highly expressive models at the cost of scalability and sometimes generalization performance. Our work stands between these two regimes: we introduce a general framework to uplift any MPNN to be more expressive, with limited scalability overhead and greatly improved practical performance. We achieve this by extending local aggregation in MPNNs from star patterns to general subgraph patterns (e.g., k-egonets): in our framework, each node representation is computed as the encoding of a surrounding induced subgraph rather than encoding of immediate neighbors only (i.e. a star). We choose the subgraph encoder to be a GNN (mainly MPNNs, considering scalability) to design a general framework that serves as a wrapper to uplift any GNN. We call our proposed method GNN-AK (GNN As Kernel), as the framework resembles a convolutional neural network by replacing the kernel with GNNs. Theoretically, we show that our framework is strictly more powerful than 1&2-WL, and is not less powerful than 3-WL. We also design subgraph sampling strategies which greatly reduce memory footprint and improve speed while maintaining performance. Our method sets new state-of-the-art performance by large margins for several well-known graph ML tasks; specifically, 0.08 MAE on ZINC, 74.79% and 86.887% accuracy on CIFAR10 and PATTERN respectively",
    "checked": true,
    "id": "7918ff88695fe7894a972758533e23f09113ea68",
    "semantic_title": "from stars to subgraphs: uplifting any gnn with local structure awareness",
    "citation_count": 185,
    "authors": []
  },
  "https://openreview.net/forum?id=-8sBpe7rDiV": {
    "title": "NETWORK INSENSITIVITY TO PARAMETER NOISE VIA PARAMETER ATTACK DURING TRAINING",
    "volume": "poster",
    "abstract": "Neuromorphic neural network processors, in the form of compute-in-memory crossbar arrays of memristors, or in the form of subthreshold analog and mixed-signal ASICs, promise enormous advantages in compute density and energy efficiency for NN-based ML tasks. However, these technologies are prone to computational non-idealities, due to process variation and intrinsic device physics. This degrades the task performance of networks deployed to the processor, by introducing parameter noise into the deployed model. While it is possible to calibrate each device, or train networks individually for each processor, these approaches are expensive and impractical for commercial deployment. Alternative methods are therefore needed to train networks that are inherently robust against parameter variation, as a consequence of network architecture and parameters. We present a new network training algorithm that attacks network parameters during training, and promotes robust performance during inference in the face of random parameter variation. Our approach introduces a loss regularization term that penalizes the susceptibility of a network to weight perturbation. We compare against previous approaches for producing parameter insensitivity such as dropout, weight smoothing and introducing parameter noise during training. We show that our approach produces models that are more robust to random mismatch-induced parameter variation as well as to targeted parameter variation. Our approach finds minima in flatter locations in the weight-loss landscape compared with other approaches, highlighting that the networks found by our technique are less sensitive to parameter perturbation. Our work provides an approach to deploy neural network architectures to inference devices that suffer from computational non-idealities, with minimal loss of performance. This method will enable deployment at scale to novel energy-efficient computational substrates, promoting cheaper and more prevalent edge inference",
    "checked": true,
    "id": "dbf898f4935340449883fef8ba505059525bd98a",
    "semantic_title": "network insensitivity to parameter noise via parameter attack during training",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=fXHl76nO2AZ": {
    "title": "Gradient Importance Learning for Incomplete Observations",
    "volume": "poster",
    "abstract": "Though recent works have developed methods that can generate estimates (or imputations) of the missing entries in a dataset to facilitate downstream analysis, most depend on assumptions that may not align with real-world applications and could suffer from poor performance in subsequent tasks such as classification. This is particularly true if the data have large missingness rates or a small sample size. More importantly, the imputation error could be propagated into the prediction step that follows, which may constrain the capabilities of the prediction model. In this work, we introduce the gradient importance learning (GIL) method to train multilayer perceptrons (MLPs) and long short-term memories (LSTMs) to directly perform inference from inputs containing missing values without imputation. Specifically, we employ reinforcement learning (RL) to adjust the gradients used to train these models via back-propagation. This allows the model to exploit the underlying information behind missingness patterns. We test the approach on real-world time-series (i.e., MIMIC-III), tabular data obtained from an eye clinic, and a standard dataset (i.e., MNIST), where our imputation-free predictions outperform the traditional two-step imputation-based predictions using state-of-the-art imputation methods",
    "checked": true,
    "id": "747feca620cdedc3456fe39bf8f7e621f3c8f784",
    "semantic_title": "gradient importance learning for incomplete observations",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=v6s3HVjPerv": {
    "title": "Do Users Benefit From Interpretable Vision? A User Study, Baseline, And Dataset",
    "volume": "poster",
    "abstract": "A variety of methods exist to explain image classification models. However, whether they provide any benefit to users over simply comparing various inputs and the model's respective predictions remains unclear. We conducted a user study (N=240) to test how such a baseline explanation technique performs against concept-based and counterfactual explanations. To this end, we contribute a synthetic dataset generator capable of biasing individual attributes and quantifying their relevance to the model. In a study, we assess if participants can identify the relevant set of attributes compared to the ground-truth. Our results show that the baseline outperformed concept-based explanations. Counterfactual explanations from an invertible neural network performed similarly as the baseline. Still, they allowed users to identify some attributes more accurately. Our results highlight the importance of measuring how well users can reason about biases of a model, rather than solely relying on technical evaluations or proxy tasks. We open-source our study and dataset so it can serve as a blue-print for future studies",
    "checked": true,
    "id": "be1f40bebd38b3e66638da680aad9b3ce21ac5ec",
    "semantic_title": "do users benefit from interpretable vision? a user study, baseline, and dataset",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=Qycd9j5Qp9J": {
    "title": "Understanding the Variance Collapse of SVGD in High Dimensions",
    "volume": "poster",
    "abstract": "Stein variational gradient descent (SVGD) is a deterministic inference algorithm that evolves a set of particles to fit a target distribution. Despite its computational efficiency, SVGD often underestimates the variance of the target distribution in high dimensions. In this work we attempt to explain the variance collapse in SVGD. On the qualitative side, we compare the SVGD update with gradient descent on the maximum mean discrepancy (MMD) objective; we observe that the variance collapse phenomenon relates to the bias from deterministic updates present in the \"driving force\" of SVGD, and empirically verify that removal of such bias leads to more accurate variance estimation. On the quantitative side, we demonstrate that the variance collapse of SVGD can be accurately predicted in the proportional asymptotic limit, i.e., when the number of particles $n$ and dimensions $d$ diverge at the same rate. In particular, for learning high-dimensional isotropic Gaussians, we derive the exact equilibrium variance for both SVGD and MMD-descent under certain near-orthogonality assumption on the converged particles, and confirm that SVGD suffers from the \"curse of dimensionality\"",
    "checked": true,
    "id": "9de2faee84881212450715bfabf8b91763e0adde",
    "semantic_title": "understanding the variance collapse of svgd in high dimensions",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=ZOcX-eybqoL": {
    "title": "Generalisation in Lifelong Reinforcement Learning through Logical Composition",
    "volume": "poster",
    "abstract": "We leverage logical composition in reinforcement learning to create a framework that enables an agent to autonomously determine whether a new task can be immediately solved using its existing abilities, or whether a task-specific skill should be learned. In the latter case, the proposed algorithm also enables the agent to learn the new task faster by generating an estimate of the optimal policy. Importantly, we provide two main theoretical results: we bound the performance of the transferred policy on a new task, and we give bounds on the necessary and sufficient number of tasks that need to be learned throughout an agent's lifetime to generalise over a distribution. We verify our approach in a series of experiments, where we perform transfer learning both after learning a set of base tasks, and after learning an arbitrary set of tasks. We also demonstrate that, as a side effect of our transfer learning approach, an agent can produce an interpretable Boolean expression of its understanding of the current task. Finally, we demonstrate our approach in the full lifelong setting where an agent receives tasks from an unknown distribution. Starting from scratch, an agent is able to quickly generalise over the task distribution after learning only a few tasks, which are sub-logarithmic in the size of the task space",
    "checked": true,
    "id": "ef3a4da272c35fe6dbd71e27385f0a253eacc038",
    "semantic_title": "generalisation in lifelong reinforcement learning through logical composition",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=gSdSJoenupI": {
    "title": "PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions",
    "volume": "poster",
    "abstract": "Cross-entropy loss and focal loss are the most common choices when training deep neural networks for classification problems. Generally speaking, however, a good loss function can take on much more flexible forms, and should be tailored for different tasks and datasets. Motivated by how functions can be approximated via Taylor expansion, we propose a simple framework, named PolyLoss, to view and design loss functions as a linear combination of polynomial functions. Our PolyLoss allows the importance of different polynomial bases to be easily adjusted depending on the targeting tasks and datasets, while naturally subsuming the aforementioned cross-entropy loss and focal loss as special cases. Extensive experimental results show that the optimal choice within the PolyLoss is indeed dependent on the task and dataset. Simply by introducing one extra hyperparameter and adding one line of code, our Poly-1 formulation outperforms the cross-entropy loss and focal loss on 2D image classification, instance segmentation, object detection, and 3D object detection tasks, sometimes by a large margin",
    "checked": true,
    "id": "30c6126b7bb567d06c5c62ad811175fd400a38f8",
    "semantic_title": "polyloss: a polynomial expansion perspective of classification loss functions",
    "citation_count": 150,
    "authors": []
  },
  "https://openreview.net/forum?id=I2Hw58KHp8O": {
    "title": "Improving Non-Autoregressive Translation Models Without Distillation",
    "volume": "poster",
    "abstract": "Transformer-based autoregressive (AR) machine translation models have achieved significant performance improvements, nearing human-level accuracy on some languages. The AR framework translates one token at a time which can be time consuming, especially for long sequences. To accelerate inference, recent work has been exploring non-autoregressive (NAR) approaches that translate blocks of tokens in parallel. Despite significant progress, leading NAR models still lag behind their AR counterparts, and only become competitive when trained with distillation. In this paper we investigate possible reasons behind this performance gap, namely, the indistinguishability of tokens, and mismatch between training and inference. We then propose the Conditional Masked Language Model with Correction (CMLMC) that addresses these problems. Empirically, we show that CMLMC achieves state-of-the-art NAR performance when trained on raw data without distillation and approaches AR performance on multiple datasets. Full code for this work will be released at the time of publication",
    "checked": true,
    "id": "1967b9127084aa696f32e20f42f0f8d7be583be5",
    "semantic_title": "improving non-autoregressive translation models without distillation",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=zzk231Ms1Ih": {
    "title": "A Theory of Tournament Representations",
    "volume": "poster",
    "abstract": "Real-world tournaments are almost always intransitive. Recent works have noted that parametric models which assume $d$ dimensional node representations can effectively model intransitive tournaments. However, nothing is known about the structure of the class of tournaments that arise out of any fixed $d$ dimensional representations. In this work, we develop a novel theory for understanding parametric tournament representations. Our first contribution is to structurally characterize the class of tournaments that arise out of $d$ dimensional representations. We do this by showing that these tournament classes have forbidden configurations that must necessarily be a union of flip classes, a novel way to partition the set of all tournaments. We further characterize rank $2$ tournaments completely by showing that the associated forbidden flip class contains just $2$ tournaments. Specifically, we show that the rank $2$ tournaments are equivalent to locally transitive tournaments. This insight allows us to show that the minimum feedback arc set problem on this tournament class can be solved using the standard Quicksort procedure. We also exhibit specific forbidden configurations for rank $4$ tournaments. For a general rank $d$ tournament class, we show that the flip class associated with a coned-doubly regular tournament of size $\\mathcal{O}(\\sqrt{d})$ must be a forbidden configuration. To answer a dual question, using a celebrated result of Froster, we show a lower bound of $\\Theta(\\sqrt{n})$ on the minimum dimension needed to represent all tournaments on $n$ nodes. For any given tournament, we show a novel upper bound on the smallest representation dimension that depends on the least size of the number of unique nodes in any feedback arc set of the flip class associated with a tournament. We show how our results also shed light on the upper bound of sign-rank of matrices",
    "checked": true,
    "id": "c8008a051073285a8ee8463c1eaf69c5f2d801a3",
    "semantic_title": "a theory of tournament representations",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=OJm3HZuj4r7": {
    "title": "Convergent and Efficient Deep Q Learning Algorithm",
    "volume": "poster",
    "abstract": "Despite the empirical success of the deep Q network (DQN) reinforcement learning algorithm and its variants, DQN is still not well understood and it does not guarantee convergence. In this work, we show that DQN can indeed diverge and cease to operate in realistic settings. Although there exist gradient-based convergent methods, we show that they actually have inherent problems in learning dynamics which cause them to fail even for simple tasks. To overcome these problems, we propose a convergent DQN algorithm (C-DQN) that is guaranteed to converge and can work with large discount factors (0.9998). It learns robustly in difficult settings and can learn several difficult games in the Atari 2600 benchmark that DQN fails to solve",
    "checked": true,
    "id": "9a69433192e15ac673339632c8e3d05d37b90254",
    "semantic_title": "convergent and efficient deep q learning algorithm",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=TXsjU8BaibT": {
    "title": "Trigger Hunting with a Topological Prior for Trojan Detection",
    "volume": "poster",
    "abstract": "Despite their success and popularity, deep neural networks (DNNs) are vulnerable when facing backdoor attacks. This impedes their wider adoption, especially in mission critical applications. This paper tackles the problem of Trojan detection, namely, identifying Trojaned models ‚Äì models trained with poisoned data. One popular approach is reverse engineering, i.e., recovering the triggers on a clean image by manipulating the model's prediction. One major challenge of reverse engineering approach is the enormous search space of triggers. To this end, we propose innovative priors such as diversity and topological simplicity to not only increase the chances of finding the appropriate triggers but also improve the quality of the found triggers. Moreover, by encouraging a diverse set of trigger candidates, our method can perform effectively in cases with unknown target labels. We demonstrate that these priors can significantly improve the quality of the recovered triggers, resulting in substantially improved Trojan detection accuracy as validated on both synthetic and publicly available TrojAI benchmarks",
    "checked": true,
    "id": "400ecda0ce56b6ef77c51247167680103666fc41",
    "semantic_title": "trigger hunting with a topological prior for trojan detection",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=JM2kFbJvvI": {
    "title": "Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep RL",
    "volume": "poster",
    "abstract": "Evaluating the worst-case performance of a reinforcement learning (RL) agent under the strongest/optimal adversarial perturbations on state observations (within some constraints) is crucial for understanding the robustness of RL agents. However, finding the optimal adversary is challenging, in terms of both whether we can find the optimal attack and how efficiently we can find it. Existing works on adversarial RL either use heuristics-based methods that may not find the strongest adversary, or directly train an RL-based adversary by treating the agent as a part of the environment, which can find the optimal adversary but may become intractable in a large state space. This paper introduces a novel attacking method to find the optimal attacks through collaboration between a designed function named \"actor\" and an RL-based learner named \"director'\". The actor crafts state perturbations for a given policy perturbation direction, and the director learns to propose the best policy perturbation directions. Our proposed algorithm, PA-AD, is theoretically optimal and significantly more efficient than prior RL-based works in environments with large state spaces. Empirical results show that our proposed PA-AD universally outperforms state-of-the-art attacking methods in various Atari and MuJoCo environments. By applying PA-AD to adversarial training, we achieve state-of-the-art empirical robustness in multiple tasks under strong adversaries",
    "checked": true,
    "id": "de5f1958728467c36c80302957b69f9fbe7e1df5",
    "semantic_title": "who is the strongest enemy? towards optimal and efficient evasion attacks in deep rl",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=v3aeIsY_vVX": {
    "title": "Chunked Autoregressive GAN for Conditional Waveform Synthesis",
    "volume": "poster",
    "abstract": "Conditional waveform synthesis models learn a distribution of audio waveforms given conditioning such as text, mel-spectrograms, or MIDI. These systems employ deep generative models that model the waveform via either sequential (autoregressive) or parallel (non-autoregressive) sampling. Generative adversarial networks (GANs) have become a common choice for non-autoregressive waveform synthesis. However, state-of-the-art GAN-based models produce artifacts when performing mel-spectrogram inversion. In this paper, we demonstrate that these artifacts correspond with an inability for the generator to learn accurate pitch and periodicity. We show that simple pitch and periodicity conditioning is insufficient for reducing this error relative to using autoregression. We discuss the inductive bias that autoregression provides for learning the relationship between instantaneous frequency and phase, and show that this inductive bias holds even when autoregressively sampling large chunks of the waveform during each forward pass. Relative to prior state-of-the-art GAN-based models, our proposed model, Chunked Autoregressive GAN (CARGAN) reduces pitch error by 40-60%, reduces training time by 58%, maintains a fast inference speed suitable for real-time or interactive applications, and maintains or improves subjective quality",
    "checked": true,
    "id": "01fbbc3a5c317478aed073bd90ef7bd3693e7bbb",
    "semantic_title": "chunked autoregressive gan for conditional waveform synthesis",
    "citation_count": 78,
    "authors": []
  },
  "https://openreview.net/forum?id=psh0oeMSBiF": {
    "title": "COPA: Certifying Robust Policies for Offline Reinforcement Learning against Poisoning Attacks",
    "volume": "poster",
    "abstract": "As reinforcement learning (RL) has achieved near human-level performance in a variety of tasks, its robustness has raised great attention. While a vast body of research has explored test-time (evasion) attacks in RL and corresponding defenses, its robustness against training-time (poisoning) attacks remains largely unanswered. In this work, we focus on certifying the robustness of ofÔ¨Çine RL in the presence of poisoning attacks, where a subset of training trajectories could be arbitrarily manipulated. We propose the Ô¨Årst certiÔ¨Åcation framework, COPA, to certify the number of poisoning trajectories that can be tolerated regarding different certiÔ¨Åcation criteria. Given the complex structure of RL, we propose two certiÔ¨Åcation criteria: per-state action stability and cumulative reward bound. To further improve the certiÔ¨Åcation, we propose new partition and aggregation protocols to train robust policies. We further prove that some of the proposed certiÔ¨Åcation methods are theoretically tight and some are NP-Complete problems. We leverage COPA to certify three RL environments trained with different algorithms and conclude: (1) The proposed robust aggregation protocols such as temporal aggregation can signiÔ¨Åcantly improve the certiÔ¨Åcations; (2) Our certiÔ¨Åcations for both per-state action stability and cumulative reward bound are efÔ¨Åcient and tight; (3) The certiÔ¨Åcation for different training algorithms and environments are different, implying their intrinsic robustness properties. All experimental results are available at https://copa-leaderboard.github.io",
    "checked": true,
    "id": "9700b4d0a9e6a64452192ae1c98e1aba34bc8c28",
    "semantic_title": "copa: certifying robust policies for offline reinforcement learning against poisoning attacks",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=Vzh1BFUCiIX": {
    "title": "ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning",
    "volume": "poster",
    "abstract": "Despite the recent success of multi-task learning and transfer learning for natural language processing (NLP), few works have systematically studied the effect of scaling up the number of tasks during pre-training. Towards this goal, this paper introduces ExMix (Extreme Mixture): a massive collection of 107 supervised NLP tasks across diverse domains and task-families. Using ExMix, we study the effect of multi-task pre-training at the largest scale to date, and analyze co-training transfer amongst common families of tasks. Through this analysis, we show that manually curating an ideal set of tasks for multi-task pre-training is not straightforward, and that multi-task scaling can vastly improve models on its own. Finally, we propose ExT5: a model pre-trained using a multi-task objective of self-supervised span denoising and supervised ExMix. Via extensive experiments, we show that ExT5 outperforms strong T5 baselines on SuperGLUE, GEM, Rainbow, Closed-Book QA tasks, and several tasks outside of ExMix. ExT5 also significantly improves sample efficiency while pre-training",
    "checked": true,
    "id": "36dc86be6d1c2604a3a81707cfb70d80cf4a27ff",
    "semantic_title": "ext5: towards extreme multi-task scaling for transfer learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=gRCCdgpVZf": {
    "title": "Provable Adaptation across Multiway Domains via Representation Learning",
    "volume": "poster",
    "abstract": "This paper studies zero-shot domain adaptation where each domain is indexed on a multi-dimensional array, and we only have data from a small subset of domains. Our goal is to produce predictors that perform well on \\emph{unseen} domains. We propose a model which consists of a domain-invariant latent representation layer and a domain-specific linear prediction layer with a low-rank tensor structure. Theoretically, we present explicit sample complexity bounds to characterize the prediction error on unseen domains in terms of the number of domains with training data and the number of data per domain. To our knowledge, this is the first finite-sample guarantee for zero-shot domain adaptation. In addition, we provide experiments on two-way MNIST and four-way fiber sensing datasets to demonstrate the effectiveness of our proposed model",
    "checked": true,
    "id": "11aacbe1c8534d2fcfa8eb1967c277144da82682",
    "semantic_title": "provable adaptation across multiway domains via representation learning",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=EXHG-A3jlM": {
    "title": "Efficient Token Mixing for Transformers via Adaptive Fourier Neural Operators",
    "volume": "poster",
    "abstract": "Vision transformers have delivered tremendous success in representation learning. This is primarily due to effective token mixing through self attention. However, this scales quadratically with the number of pixels, which becomes infeasible for high-resolution inputs. To cope with this challenge, we propose Adaptive Fourier Neural Operator (AFNO) as an efficient token mixer that learns to mix in the Fourier domain. AFNO is based on a principled foundation of operator learning which allows us to frame token mixing as a continuous global convolution without any dependence on the input resolution. This principle was previously used to design FNO, which solves global convolution efficiently in the Fourier domain and has shown promise in learning challenging PDEs. To handle challenges in visual representation learning such as discontinuities in images and high resolution inputs, we propose principled architectural modifications to FNO which results in memory and computational efficiency. This includes imposing a block-diagonal structure on the channel mixing weights, adaptively sharing weights across tokens, and sparsifying the frequency modes via soft-thresholding and shrinkage. The resulting model is highly parallel with a quasi-linear complexity and has linear memory in the sequence size. AFNO outperforms self-attention mechanisms for few-shot segmentation in terms of both efficiency and accuracy. For Cityscapes segmentation with the Segformer-B3 backbone, AFNO can handle a sequence size of 65k and outperforms other efficient self-attention mechanisms",
    "checked": true,
    "id": "32e0057c9a06d23182ff41553c9df1c9a8c4b757",
    "semantic_title": "efficient token mixing for transformers via adaptive fourier neural operators",
    "citation_count": 87,
    "authors": []
  },
  "https://openreview.net/forum?id=xENf4QUL4LW": {
    "title": "Sample Selection with Uncertainty of Losses for Learning with Noisy Labels",
    "volume": "poster",
    "abstract": "In learning with noisy labels, the sample selection approach is very popular, which regards small-loss data as correctly labeled data during training. However, losses are generated on-the-Ô¨Çy based on the model being trained with noisy labels, and thus large-loss data are likely but not certain to be incorrect. There are actually two possibilities of a large-loss data point: (a) it is mislabeled, and then its loss decreases slower than other data, since deep neural networks learn patterns Ô¨Årst; (b) it belongs to an underrepresented group of data and has not been selected yet. In this paper, we incorporate the uncertainty of losses by adopting interval estimation instead of point estimation of losses, where lower bounds of the conÔ¨Ådence intervals of losses derived from distribution-free concentration inequalities, but not losses themselves, are used for sample selection. In this way, we also give large-loss but less selected data a try; then, we can better distinguish between the cases (a) and (b) by seeing if the losses effectively decrease with the uncertainty after the try. As a result, we can better explore underrepresented data that are correctly labeled but seem to be mislabeled at Ô¨Årst glance. Experiments demonstrate that the proposed method is superior to baselines and robust to a broad range of label noise types",
    "checked": true,
    "id": "b9ea1c5fd417c5af2e3aa44a2a3c66f77bac190e",
    "semantic_title": "sample selection with uncertainty of losses for learning with noisy labels",
    "citation_count": 123,
    "authors": []
  },
  "https://openreview.net/forum?id=GsH-K1VIyy": {
    "title": "Data-Driven Offline Optimization for Architecting Hardware Accelerators",
    "volume": "poster",
    "abstract": "To attain higher efficiency, the industry has gradually reformed towards application-specific hardware accelerators. While such a paradigm shift is already starting to show promising results, designers need to spend considerable manual effort and perform large number of time-consuming simulations to find accelerators that can accelerate multiple target applications while obeying design constraints. Moreover, such a simulation-driven approach must be re-run from scratch every time the set of target applications or design constraints change. An alternative paradigm is to use a data-driven, offline approach that utilizes logged simulation data, to architect hardware accelerators, without needing any form of simulations. Such an approach not only alleviates the need to run time-consuming simulation, but also enables data reuse and applies even when set of target applications changes. In this paper, we develop such a data-driven offline optimization method for designing hardware accelerators, dubbed PRIME, that enjoys all of these properties. Our approach learns a conservative, robust estimate of the desired cost function, utilizes infeasible points and optimizes the design against this estimate without any additional simulator queries during optimization. PRIME architects accelerators---tailored towards both single- and multi-applications---improving performance upon stat-of-the-art simulation-driven methods by about 1.54x and 1.20x, while considerably reducing the required total simulation time by 93% and 99%, respectively. In addition, PRIME also architects effective accelerators for unseen applications in a zero-shot setting, outperforming simulation-based methods by 1.26x",
    "checked": true,
    "id": "d352df83b48a221867b2bcd42e72f6491b6445d9",
    "semantic_title": "data-driven offline optimization for architecting hardware accelerators",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=H7HDG--DJF0": {
    "title": "Multi-Agent MDP Homomorphic Networks",
    "volume": "poster",
    "abstract": "This paper introduces Multi-Agent MDP Homomorphic Networks, a class of networks that allows distributed execution using only local information, yet is able to share experience between global symmetries in the joint state-action space of cooperative multi-agent systems. In cooperative multi-agent systems, complex symmetries arise between different configurations of the agents and their local observations. For example, consider a group of agents navigating: rotating the state globally results in a permutation of the optimal joint policy. Existing work on symmetries in single agent reinforcement learning can only be generalized to the fully centralized setting, because such approaches rely on the global symmetry in the full state-action spaces, and these can result in correspondences across agents. To encode such symmetries while still allowing distributed execution we propose a factorization that decomposes global symmetries into local transformations. Our proposed factorization allows for distributing the computation that enforces global symmetries over local agents and local interactions. We introduce a multi-agent equivariant policy network based on this factorization. We show empirically on symmetric multi-agent problems that globally symmetric distributable policies improve data efficiency compared to non-equivariant baselines",
    "checked": true,
    "id": "9a03d9abfd504bc94686acb920103f802b0878da",
    "semantic_title": "multi-agent mdp homomorphic networks",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=yhCp5RcZD7": {
    "title": "Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields",
    "volume": "poster",
    "abstract": "We present implicit displacement fields, a novel representation for detailed 3D geometry. Inspired by a classic surface deformation technique, displacement mapping, our method represents a complex surface as a smooth base surface plus a displacement along the base's normal directions, resulting in a frequency-based shape decomposition, where the high-frequency signal is constrained geometrically by the low-frequency signal. Importantly, this disentanglement is unsupervised thanks to a tailored architectural design that has an innate frequency hierarchy by construction. We explore implicit displacement field surface reconstruction and detail transfer and demonstrate superior representational power, training stability, and generalizability",
    "checked": true,
    "id": "719af7de83ef30bb46815de20e5cee604ed7b9b8",
    "semantic_title": "geometry-consistent neural shape representation with implicit displacement fields",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=tyTH9kOxcvh": {
    "title": "Modeling Label Space Interactions in Multi-label Classification using Box Embeddings",
    "volume": "poster",
    "abstract": "Multi-label classification is a challenging structured prediction task in which a set of output class labels are predicted for each input. Real-world datasets often have natural or latent taxonomic relationships between labels, making it desirable for models to employ label representations capable of capturing such taxonomies. Most existing multi-label classification methods do not do so, resulting in label predictions that are inconsistent with the taxonomic constraints, thus failing to accurately represent the fundamentals of problem setting. In this work, we introduce the multi-label box model (MBM), a multi-label classification method that combines the encoding power of neural networks with the inductive bias and probabilistic semantics of box embeddings (Vilnis, et al 2018). Box embeddings can be understood as trainable Venn-diagrams based on hyper-rectangles. Representing labels by boxes rather than vectors, MBM is able to capture taxonomic relations among labels. Furthermore, since box embeddings allow these relations to be learned by stochastic gradient descent from data, and to be read as calibrated conditional probabilities, our model is endowed with a high degree of interpretability. This interpretability also facilitates the injection of partial information about label-label relationships into model training, to further improve its consistency. We provide theoretical grounding for our method and show experimentally the model's ability to learn the true latent taxonomic structure from data. Through extensive empirical evaluations on both small and large-scale multi-label classification datasets, we show that BBM can significantly improve taxonomic consistency while preserving or surpassing the state-of-the-art predictive performance",
    "checked": true,
    "id": "325d64c3583a52cef76a33f174028493a850f960",
    "semantic_title": "modeling label space interactions in multi-label classification using box embeddings",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=ZKy2X3dgPA": {
    "title": "It Takes Two to Tango: Mixup for Deep Metric Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a7721bf626f4996117dbb88b385be2e12462e7e6",
    "semantic_title": "it takes two to tango: mixup for deep metric learning",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=G89-1yZLFHk": {
    "title": "Data Efficient Language-Supervised Zero-Shot Recognition with Optimal Transport Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "38cc96e5a780716dc7e19e037d96d952b3a37940",
    "semantic_title": "data efficient language-supervised zero-shot recognition with optimal transport distillation",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=Oy9WeuZD51": {
    "title": "A Statistical Framework for Efficient Out of Distribution Detection in Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9e52bc970e53b350d393c7c3880ddab074867367",
    "semantic_title": "a statistical framework for efficient out of distribution detection in deep neural networks",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=HuaYQfggn5u": {
    "title": "FedBABU: Toward Enhanced Representation for Federated Image Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "3cd5b67f7bc25a14a9505d27a5b18cfb5592769b",
    "semantic_title": "fedbabu: towards enhanced representation for federated image classification",
    "citation_count": 231,
    "authors": []
  },
  "https://openreview.net/forum?id=AP1MKT37rJ": {
    "title": "Should I Run Offline Reinforcement Learning or Behavioral Cloning?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a8c409791404828a3276ffb0e4a71d3263b0526e",
    "semantic_title": "should i run offline reinforcement learning or behavioral cloning?",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=CLpxpXqqBV": {
    "title": "Learning State Representations via Retracing in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "51bd95feec91c5e0d3b9829fa95f3f8fb3d67501",
    "semantic_title": "learning state representations via retracing in reinforcement learning",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=O-r8LOR-CCA": {
    "title": "Open-World Semi-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0016122bc5dfe0684baaa672c53014d48b79a65f",
    "semantic_title": "open-world semi-supervised learning",
    "citation_count": 203,
    "authors": []
  },
  "https://openreview.net/forum?id=af1eUDdUVz": {
    "title": "Evading Adversarial Example Detection Defenses with Orthogonal Projected Gradient Descent",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "13a5aedf89c0e6c10b18350e4b228708f22a6605",
    "semantic_title": "evading adversarial example detection defenses with orthogonal projected gradient descent",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=Azh9QBQ4tR7": {
    "title": "Reducing Excessive Margin to Achieve a Better Accuracy vs. Robustness Trade-off",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8bb8f9ee86a5fde354497c0148ccde07c01ad811",
    "semantic_title": "reducing excessive margin to achieve a better accuracy vs. robustness trade-off",
    "citation_count": 145,
    "authors": []
  },
  "https://openreview.net/forum?id=WxuE_JWxjkW": {
    "title": "Expressivity of Emergent Languages is a Trade-off between Contextual Complexity and Unpredictability",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2635b0d4b61ad5f2867aa2cfaac63c6aba185d43",
    "semantic_title": "expressivity of emergent language is a trade-off between contextual complexity and unpredictability",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=hcoswsDHNAW": {
    "title": "Fast AdvProp",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f2eaaf8afc89a86035fd7127305f2bb9d2169495",
    "semantic_title": "fast advprop",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=8in_5gN9I0": {
    "title": "Triangle and Four Cycle Counting with Predictions in Graph Streams",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2abbbc44ac51362540cdac1e34a99c03091424da",
    "semantic_title": "triangle and four cycle counting with predictions in graph streams",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=js62_xuLDDv": {
    "title": "Is Fairness Only Metric Deep? Evaluating and Addressing Subgroup Gaps in Deep Metric Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "528fb2ab93bd2a63354513a191b83907ccf4ccb5",
    "semantic_title": "is fairness only metric deep? evaluating and addressing subgroup gaps in deep metric learning",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=xMJWUKJnFSw": {
    "title": "NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f6c4d57c710b461ece658a0b8e7427e862fef116",
    "semantic_title": "nodepiece: compositional and parameter-efficient representations of large knowledge graphs",
    "citation_count": 96,
    "authors": []
  },
  "https://openreview.net/forum?id=e42KbIw6Wb": {
    "title": "Pix2seq: A Language Modeling Framework for Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "19b3b074d38b250d024920732ae51a8ffa0996dd",
    "semantic_title": "pix2seq: a language modeling framework for object detection",
    "citation_count": 376,
    "authors": []
  },
  "https://openreview.net/forum?id=PQQp7AJwz3": {
    "title": "Particle Stochastic Dual Coordinate Ascent: Exponential convergent algorithm for mean field neural network optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8815d9fd43661ffdc6f9240dbca0a020c4f80d98",
    "semantic_title": "particle stochastic dual coordinate ascent: exponential convergent algorithm for mean field neural network optimization",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=7_JR7WpwKV1": {
    "title": "The Effects of Invertibility on the Representational Complexity of Encoders in Variational Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7bb65e9167e5d21f04ebaacdd7bc59f7c4972bb7",
    "semantic_title": "the effects of invertibility on the representational complexity of encoders in variational autoencoders",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ro_zAjZppv": {
    "title": "Tracking the risk of a deployed model and detecting harmful distribution shifts",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2e51e475ca448e9234c9a4e49c5d9a6605591084",
    "semantic_title": "tracking the risk of a deployed model and detecting harmful distribution shifts",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=BmJV7kyAmg": {
    "title": "Towards Understanding the Robustness Against Evasion Attack on Categorical Data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f01b92624046cadffe6d8a26805e03502ed3fd81",
    "semantic_title": "towards understanding the robustness against evasion attack on categorical data",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=WPI2vbkAl3Q": {
    "title": "Learning Curves for SGD on Structured Features",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b6ce7f9ccc5399f8dabd4f48bc9e8c84a7b432f2",
    "semantic_title": "learning curves for sgd on structured features",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=Qaw16njk6L": {
    "title": "NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b8a919f4a2aaa97bef19aa43e01f8bc347693b73",
    "semantic_title": "nasvit: neural architecture search for efficient vision transformers with gradient conflict aware supernet training",
    "citation_count": 99,
    "authors": []
  },
  "https://openreview.net/forum?id=sTNHCrIKDQc": {
    "title": "Graphon based Clustering and Testing of Networks: Algorithms and Theory",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1435272aeb0dfb8692b6d74fcb473b5e529a89c6",
    "semantic_title": "graphon based clustering and testing of networks: algorithms and theory",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=TYw3-OlrRm-": {
    "title": "Network Augmentation for Tiny Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "af5f082555ea12991a603a717cb98d79cace550b",
    "semantic_title": "network augmentation for tiny deep learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o-1v9hdSult": {
    "title": "Bridging the Gap: Providing Post-Hoc Symbolic Explanations for Sequential Decision-Making Problems with Inscrutable Representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9a2bbb65a072a69a28c0b3cb2143f0e62c505f50",
    "semantic_title": "bridging the gap: providing post-hoc symbolic explanations for sequential decision-making problems with inscrutable representations",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=C8Ltz08PtBp": {
    "title": "Distributional Reinforcement Learning with Monotonic Splines",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "95ab0295c77a7064ea8aa50e98d9ca617c70d254",
    "semantic_title": "distributional reinforcement learning with monotonic splines",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=R79ZGjHhv6p": {
    "title": "Toward Faithful Case-based Reasoning through Learning Prototypes in a Nearest Neighbor-friendly Space",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "329edaadc5c83ed3c4b5cb4dbedce0976d2d7dbd",
    "semantic_title": "toward faithful case-based reasoning through learning prototypes in a nearest neighbor-friendly space",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=iMqTLyfwnOO": {
    "title": "Augmented Sliced Wasserstein Distances",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5c2e520e88203b1f4cb82c0f9a9682d9cf2e969a",
    "semantic_title": "augmented sliced wasserstein distances",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=Az-7gJc6lpr": {
    "title": "Relational Learning with Variational Bayes",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "71ad3fdcb38268ea092e1ace8344864b8c532688",
    "semantic_title": "relational learning with variational bayes",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=UMfhoMtIaP5": {
    "title": "Provably Robust Adversarial Examples",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7cb6a6369c6b01de8f88539687cb4acc121edb94",
    "semantic_title": "provably robust adversarial examples",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=vcUmUvQCloe": {
    "title": "Joint Shapley values: a measure of joint feature importance",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ec73937dddc1d5573072d39cc5131178c796dc03",
    "semantic_title": "joint shapley values: a measure of joint feature importance",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=v8OlxjGn23S": {
    "title": "Low-Budget Active Learning via Wasserstein Distance: An Integer Programming Approach",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1ec19d5a07f7baa20c0d68ee53d290bf88b9dc23",
    "semantic_title": "low budget active learning via wasserstein distance: an integer programming approach",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=fVu3o-YUGQK": {
    "title": "Efficient Self-supervised Vision Transformers for Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b70bb1855e217edffb5dfa0632e8216860821870",
    "semantic_title": "efficient self-supervised vision transformers for representation learning",
    "citation_count": 215,
    "authors": []
  },
  "https://openreview.net/forum?id=9RUHPlladgh": {
    "title": "Visual Representation Learning Does Not Generalize Strongly Within the Same Domain",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "61b1171efcf0242eb011816de1aa415f4262c55a",
    "semantic_title": "visual representation learning does not generalize strongly within the same domain",
    "citation_count": 78,
    "authors": []
  },
  "https://openreview.net/forum?id=e2Lle5cij9D": {
    "title": "Hidden Convexity of Wasserstein GANs: Interpretable Generative Models with Closed-Form Solutions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d6206e3cc9fdefcaa901cd9634a892216b6a98d4",
    "semantic_title": "hidden convexity of wasserstein gans: interpretable generative models with closed-form solutions",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=NRX9QZ6yqt": {
    "title": "Memory Augmented Optimizers for Deep Learning",
    "volume": "poster",
    "abstract": "Popular approaches for minimizing loss in data-driven learning often involve an abstraction or an explicit retention of the history of gradients for efficient parameter updates. The aggregated history of gradients nudges the parameter updates in the right direction even when the gradients at any given step are not informative. Although the history of gradients summarized in meta-parameters or explicitly stored in memory has been shown effective in theory and practice, the question of whether $all$ or only a subset of the gradients in the history are sufficient in deciding the parameter updates remains unanswered. In this paper, we propose a framework of memory-augmented gradient descent optimizers that retain a limited view of their gradient history in their internal memory. Such optimizers scale well to large real-life datasets, and our experiments show that the memory augmented extensions of standard optimizers enjoy accelerated convergence and improved performance on a majority of computer vision and language tasks that we considered. Additionally, we prove that the proposed class of optimizers with fixed-size memory converge under assumptions of strong convexity, regardless of which gradients are selected or how they are linearly combined to form the update step",
    "checked": true,
    "id": "c4025c12d802491a56d1801b7ca32a58ea1b14bb",
    "semantic_title": "memory augmented optimizers for deep learning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=c87d0TS4yX": {
    "title": "Orchestrated Value Mapping for Reinforcement Learning",
    "volume": "poster",
    "abstract": "We present a general convergent class of reinforcement learning algorithms that is founded on two distinct principles: (1) mapping value estimates to a different space using arbitrary functions from a broad class, and (2) linearly decomposing the reward signal into multiple channels. The first principle enables incorporating specific properties into the value estimator that can enhance learning. The second principle, on the other hand, allows for the value function to be represented as a composition of multiple utility functions. This can be leveraged for various purposes, e.g. dealing with highly varying reward scales, incorporating a priori knowledge about the sources of reward, and ensemble learning. Combining the two principles yields a general blueprint for instantiating convergent algorithms by orchestrating diverse mapping functions over multiple reward channels. This blueprint generalizes and subsumes algorithms such as Q-Learning, Log Q-Learning, and Q-Decomposition. In addition, our convergence proof for this general class relaxes certain required assumptions in some of these algorithms. Based on our theory, we discuss several interesting configurations as special cases. Finally, to illustrate the potential of the design space that our theory opens up, we instantiate a particular algorithm and evaluate its performance on the Atari suite",
    "checked": true,
    "id": "32455a01db6a0054bb606358eab53596a99a64df",
    "semantic_title": "orchestrated value mapping for reinforcement learning",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=CIaQKbTBwtU": {
    "title": "Learning to Generalize across Domains on Single Test Samples",
    "volume": "poster",
    "abstract": "We strive to learn a model from a set of source domains that generalizes well to unseen target domains. The main challenge in such a domain generalization scenario is the unavailability of any target domain data during training, resulting in the learned model not being explicitly adapted to the unseen target domains. We propose learning to generalize across domains on single test samples. We leverage a meta-learning paradigm to learn our model to acquire the ability of adaptation with single samples at training time so as to further adapt itself to each single test sample at test time. We formulate the adaptation to the single test sample as a variational Bayesian inference problem, which incorporates the test sample as a conditional into the generation of model parameters. The adaptation to each test sample requires only one feed-forward computation at test time without any fine-tuning or self-supervised training on additional data from the unseen domains. Extensive ablation studies demonstrate that our model learns the ability to adapt models to each single sample by mimicking domain shifts during training. Further, our model achieves at least comparable -- and often better -- performance than state-of-the-art methods on multiple benchmarks for domain generalization",
    "checked": true,
    "id": "e6a619cdb4bdfa973078809f4b4d215c98174e2f",
    "semantic_title": "learning to generalize across domains on single test samples",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=lY0-7bj0Vfz": {
    "title": "Prototype memory and attention mechanisms for few shot image generation",
    "volume": "poster",
    "abstract": "Recent discoveries indicate that the neural codes in the primary visual cortex (V1) of macaque monkeys are complex, diverse and sparse. This leads us to ponder the computational advantages and functional role of these \"grandmother cells.\" Here, we propose that such cells can serve as prototype memory priors that bias and shape the distributed feature processing within the image generation process in the brain. These memory prototypes are learned by momentum online clustering and are utilized via a memory-based attention operation, which we define as Memory Concept Attention (MoCA). To test our proposal, we show in a few-shot image generation task, that having a prototype memory during attention can improve image synthesis quality, learn interpretable visual concept clusters, as well as improve the robustness of the model. Interestingly, we also find that our attentional memory mechanism can implicitly modify the horizontal connections by updating the transformation into the prototype embedding space for self-attention. Insofar as GANs can be seen as plausible models for reasoning about the top-down synthesis in the analysis-by-synthesis loop of the hierarchical visual cortex, our findings demonstrate a plausible computational role for these \"prototype concept\" neurons in visual processing in the brain",
    "checked": true,
    "id": "4df285d03d9945fbd3720337e4b1f2b3f0d68fe0",
    "semantic_title": "prototype memory and attention mechanisms for few shot image generation",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=FEBFJ98FKx": {
    "title": "TPU-GAN: Learning temporal coherence from dynamic point cloud sequences",
    "volume": "poster",
    "abstract": "Point cloud sequence is an important data representation that provides flexible shape and motion information. Prior work demonstrates that incorporating scene flow information into loss can make model learn temporally coherent feature spaces. However, it is prohibitively expensive to acquire point correspondence information across frames in real-world environments. In this work, we propose a super-resolution generative adversarial network (GAN) for upsampling dynamic point cloud sequences, which does not require point correspondence annotation. Our model, Temporal Point cloud Upsampling GAN (TPU-GAN), can implicitly learn the underlying temporal coherence from point cloud sequence, which in turn guides the generator to produce temporally coherent output. In addition, we propose a learnable masking module to adapt upsampling ratio according to the point distribution. We conduct extensive experiments on point cloud sequences from two different domains: particles in the fluid dynamical system and human action scanned data. The quantitative and qualitative evaluation demonstrates the effectiveness of our method on upsampling tasks as well as learning temporal coherence from irregular point cloud sequences",
    "checked": true,
    "id": "186a83af4f769833bf37138408e56c9be8a85a86",
    "semantic_title": "tpu-gan: learning temporal coherence from dynamic point cloud sequences",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=JBAZe2yN6Ub": {
    "title": "A First-Occupancy Representation for Reinforcement Learning",
    "volume": "poster",
    "abstract": "Both animals and artificial agents benefit from state representations that support rapid transfer of learning across tasks and which enable them to efficiently traverse their environments to reach rewarding states. The successor representation (SR), which measures the expected cumulative, discounted state occupancy under a fixed policy, enables efficient transfer to different reward structures in an otherwise constant Markovian environment and has been hypothesized to underlie aspects of biological behavior and neural activity. However, in the real world, rewards may only be available for consumption once, may shift location, or agents may simply aim to reach goal states as rapidly as possible without the constraint of artificially imposed task horizons. In such cases, the most behaviorally-relevant representation would carry information about when the agent was likely to first reach states of interest, rather than how often it should expect to visit them over a potentially infinite time span. To reflect such demands, we introduce the first-occupancy representation (FR), which measures the expected temporal discount to the first time a state is accessed. We demonstrate that the FR facilitates exploration, the selection of efficient paths to desired states, allows the agent, under certain conditions, to plan provably optimal trajectories defined by a sequence of subgoals, and induces similar behavior to animals avoiding threatening stimuli",
    "checked": true,
    "id": "5d0c5b55db27d44e80406a825fb86ae3e1325a8b",
    "semantic_title": "a first-occupancy representation for reinforcement learning",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=ci7LBzDn2Q": {
    "title": "Deep ReLU Networks Preserve Expected Length",
    "volume": "poster",
    "abstract": "Assessing the complexity of functions computed by a neural network helps us understand how the network will learn and generalize. One natural measure of complexity is how the network distorts length - if the network takes a unit-length curve as input, what is the length of the resulting curve of outputs? It has been widely believed that this length grows exponentially in network depth. We prove that in fact this is not the case: the expected length distortion does not grow with depth, and indeed shrinks slightly, for ReLU networks with standard random initialization. We also generalize this result by proving upper bounds both for higher moments of the length distortion and for the distortion of higher-dimensional volumes. These theoretical results are corroborated by our experiments",
    "checked": true,
    "id": "454343bad831481f154a2e4506e44e3159cc0b38",
    "semantic_title": "deep relu networks preserve expected length",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=lTqGXfn9Tv": {
    "title": "Phenomenology of Double Descent in Finite-Width Neural Networks",
    "volume": "poster",
    "abstract": "`Double descent' delineates the generalization behaviour of models depending on the regime they belong to: under- or over-parameterized. The current theoretical understanding behind the occurrence of this phenomenon is primarily based on linear and kernel regression models --- with informal parallels to neural networks via the Neural Tangent Kernel. Therefore such analyses do not adequately capture the mechanisms behind double descent in finite-width neural networks, as well as, disregard crucial components --- such as the choice of the loss function. We address these shortcomings by leveraging influence functions in order to derive suitable expressions of the population loss and its lower bound, while imposing minimal assumptions on the form of the parametric model. Our derived bounds bear an intimate connection with the spectrum of the Hessian at the optimum, and importantly, exhibit a double descent behaviour at the interpolation threshold. Building on our analysis, we further investigate how the loss function affects double descent --- and thus uncover interesting properties of neural networks and their Hessian spectra near the interpolation threshold",
    "checked": true,
    "id": "4026d3e4d928fc0e7413b0090969d9a7fd260982",
    "semantic_title": "phenomenology of double descent in finite-width neural networks",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=F72ximsx7C1": {
    "title": "How Attentive are Graph Attention Networks?",
    "volume": "poster",
    "abstract": "Graph Attention Networks (GATs) are one of the most popular GNN architectures and are considered as the state-of-the-art architecture for representation learning with graphs. In GAT, every node attends to its neighbors given its own representation as the query. However, in this paper we show that GAT computes a very limited kind of attention: the ranking of the attention scores is unconditioned on the query node. We formally define this restricted kind of attention as static attention and distinguish it from a strictly more expressive dynamic attention. Because GATs use a static attention mechanism, there are simple graph problems that GAT cannot express: in a controlled problem, we show that static attention hinders GAT from even fitting the training data. To remove this limitation, we introduce a simple fix by modifying the order of operations and propose GATv2: a dynamic graph attention variant that is strictly more expressive than GAT. We perform an extensive evaluation and show that GATv2 outperforms GAT across 12 OGB and other benchmarks while we match their parametric costs. Our code is available at https://github.com/tech-srl/how_attentive_are_gats . GATv2 is available as part of the PyTorch Geometric library, the Deep Graph Library, and the TensorFlow GNN library",
    "checked": true,
    "id": "ab30672c8c5e4787f6a5985f26a8f281f0db2fb8",
    "semantic_title": "how attentive are graph attention networks?",
    "citation_count": 1240,
    "authors": []
  },
  "https://openreview.net/forum?id=92tYQiil17": {
    "title": "Learning Transferable Reward for Query Object Localization with Policy Adaptation",
    "volume": "poster",
    "abstract": "We propose a reinforcement learning based approach to query object localization, for which an agent is trained to localize objects of interest specified by a small exemplary set. We learn a transferable reward signal formulated using the exemplary set by ordinal metric learning. Our proposed method enables test-time policy adaptation to new environments where the reward signals are not readily available, and outperforms fine-tuning approaches that are limited to annotated images. In addition, the transferable reward allows repurposing the trained agent from one specific class to another class. Experiments on corrupted MNIST, CU-Birds, and COCO datasets demonstrate the effectiveness of our approach",
    "checked": true,
    "id": "2017b16ada7c0744c215bc00b1732e758ca2b3cd",
    "semantic_title": "learning transferable reward for query object localization with policy adaptation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=8FhxBtXSl0": {
    "title": "CKConv: Continuous Kernel Convolution For Sequential Data",
    "volume": "poster",
    "abstract": "Conventional neural architectures for sequential data present important limitations. Recurrent neural networks suffer from exploding and vanishing gradients, small effective memory horizons, and must be trained sequentially. Convolutional neural networks cannot handle sequences of unknown size and their memory horizon must be defined a priori. In this work, we show that these problems can be solved by formulating the convolutional kernels of CNNs as continuous functions. The resulting Continuous Kernel Convolution (CKConv) handles arbitrarily long sequences in a parallel manner, within a single operation, and without relying on any form of recurrence. We show that Continuous Kernel Convolutional Networks (CKCNNs) obtain state-of-the-art results in multiple datasets, e.g., permuted MNIST, and, thanks to their continuous nature, are able to handle non-uniformly sampled datasets and irregularly-sampled data natively. CKCNNs match or perform better than neural ODEs designed for these purposes in a faster and simpler manner",
    "checked": true,
    "id": "1d5c8c6e5a774d2fef8d92bd28670a6345a97f7a",
    "semantic_title": "ckconv: continuous kernel convolution for sequential data",
    "citation_count": 132,
    "authors": []
  },
  "https://openreview.net/forum?id=H4PmOqSZDY": {
    "title": "Towards Empirical Sandwich Bounds on the Rate-Distortion Function",
    "volume": "poster",
    "abstract": "Rate-distortion (R-D) function, a key quantity in information theory, characterizes the fundamental limit of how much a data source can be compressed subject to a fidelity criterion, by any compression algorithm. As researchers push for ever-improving compression performance, establishing the R-D function of a given data source is not only of scientific interest, but also reveals the possible room for improvement in existing compression algorithms. Previous work on this problem relied on distributional assumptions on the data source (Gibson, 2017) or only applied to discrete data (Blahut, 1972; Arimoto, 1972). By contrast, this paper makes the first attempt at an algorithm for sandwiching the R-D function of a general (not necessarily discrete) source requiring only i.i.d. data samples. We estimate R-D sandwich bounds for a variety of artificial and real-world data sources, in settings far beyond the feasibility of any known method, and shed light on the optimality of neural data compression (Ball√© et al., 2021; Yang et al., 2022). Our R-D upper bound on natural images indicates theoretical room for improving state-of-the-art image compression methods by at least one dB in PSNR at various bitrates. Our data and code can be found at https://github.com/mandt-lab/RD-sandwich",
    "checked": true,
    "id": "8f77b6563918f729aea987bee0547fff87116815",
    "semantic_title": "towards empirical sandwich bounds on the rate-distortion function",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=wfZGut6e09": {
    "title": "Pareto Policy Adaptation",
    "volume": "poster",
    "abstract": "We present a policy gradient method for Multi-Objective Reinforcement Learning under unknown, linear preferences. By enforcing Pareto stationarity, a first-order condition for Pareto optimality, we are able to design a simple policy gradient algorithm that approximates the Pareto front and infers the unknown preferences. Our method relies on a projected gradient descent solver that identifies common ascent directions for all objectives. Leveraging the solution of that solver, we introduce Pareto Policy Adaptation (PPA), a loss function that adapts the policy to be optimal with respect to any distribution over preferences. PPA uses implicit differentiation to back-propagate the loss gradient bypassing the operations of the projected gradient descent solver. Our approach is straightforward, easy to implement and can be used with all existing policy gradient and actor-critic methods. We evaluate our method in a series of reinforcement learning tasks",
    "checked": true,
    "id": "f883ad9d52b3925274ed7690ec5d60effece2272",
    "semantic_title": "pareto policy adaptation",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=BrFIKuxrZE": {
    "title": "Fair Normalizing Flows",
    "volume": "poster",
    "abstract": "Fair representation learning is an attractive approach that promises fairness of downstream predictors by encoding sensitive data. Unfortunately, recent work has shown that strong adversarial predictors can still exhibit unfairness by recovering sensitive attributes from these representations. In this work, we present Fair Normalizing Flows (FNF), a new approach offering more rigorous fairness guarantees for learned representations. Specifically, we consider a practical setting where we can estimate the probability density for sensitive groups. The key idea is to model the encoder as a normalizing flow trained to minimize the statistical distance between the latent representations of different groups. The main advantage of FNF is that its exact likelihood computation allows us to obtain guarantees on the maximum unfairness of any potentially adversarial downstream predictor. We experimentally demonstrate the effectiveness of FNF in enforcing various group fairness notions, as well as other attractive properties such as interpretability and transfer learning, on a variety of challenging real-world datasets",
    "checked": true,
    "id": "947cf3e0d055bcafe5848afe10fca4e2484da2f8",
    "semantic_title": "fair normalizing flows",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=5QhUE1qiVC6": {
    "title": "The Convex Geometry of Backpropagation: Neural Network Gradient Flows Converge to Extreme Points of the Dual Convex Program",
    "volume": "poster",
    "abstract": "We study non-convex subgradient flows for training two-layer ReLU neural networks from a convex geometry and duality perspective. We characterize the implicit bias of unregularized non-convex gradient flow as convex regularization of an equivalent convex model. We then show that the limit points of non-convex subgradient flows can be identified via primal-dual correspondence in this convex optimization problem. Moreover, we derive a sufficient condition on the dual variables which ensures that the stationary points of the non-convex objective are the KKT points of the convex objective, thus proving convergence of non-convex gradient flows to the global optimum. For a class of regular training data distributions such as orthogonal separable data, we show that this sufficient condition holds. Therefore, non-convex gradient flows in fact converge to optimal solutions of a convex optimization problem. We present numerical results verifying the predictions of our theory for non-convex subgradient descent",
    "checked": true,
    "id": "de54fe9d50f0ecccd2aa1468ca3b353f1865eed7",
    "semantic_title": "the convex geometry of backpropagation: neural network gradient flows converge to extreme points of the dual convex program",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=5MLb3cLCJY": {
    "title": "Adaptive Wavelet Transformer Network for 3D Shape Representation Learning",
    "volume": "poster",
    "abstract": "We present a novel method for 3D shape representation learning using multi-scale wavelet decomposition. Previous works often decompose 3D shapes into complementary components in spatial domain at a single scale. In this work, we study to decompose 3D shapes into sub-bands components in frequency domain at multiple scales, resulting in a hierarchical decomposition tree in a principled manner rooted in multi-resolution wavelet analysis. Specifically, we propose Adaptive Wavelet Transformer Network (AWT-Net) that firstly generates approximation or detail wavelet coefficients per point, classifying each point into high or low sub-bands components, using lifting scheme at multiple scales recursively and hierarchically. Then, AWT-Net exploits Transformer to enhance the original shape features by querying and fusing features from different but integrated sub-bands. The wavelet coefficients can be learned without direct supervision on coefficients, and AWT-Net is fully differentiable and can be learned in an end-to-end fashion. Extensive experiments demonstrate that AWT-Net achieves competitive performance on 3D shape classification and segmentation benchmarks",
    "checked": true,
    "id": "0dd29e0907722e9957681cd5f46ceb4fcd578536",
    "semantic_title": "adaptive wavelet transformer network for 3d shape representation learning",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=g5tANwND04i": {
    "title": "On the Convergence of mSGD and AdaGrad for Stochastic Optimization",
    "volume": "poster",
    "abstract": "As one of the most fundamental stochastic optimization algorithms, stochastic gradient descent (SGD) has been intensively developed and extensively applied in machine learning in the past decade. There have been some modified SGD-type algorithms, which outperform the SGD in many competitions and applications in terms of convergence rate and accuracy, such as momentum-based SGD (mSGD) and adaptive gradient algorithm (AdaGrad). Despite these empirical successes, the theoretical properties of these algorithms have not been well established due to technical difficulties. With this motivation, we focus on convergence analysis of mSGD and AdaGrad for any smooth (possibly non-convex) loss functions in stochastic optimization. First, we prove that the iterates of mSGD are asymptotically convergent to a connected set of stationary points with probability one, which is more general than existing works on subsequence convergence or convergence of time averages. Moreover, we prove that the loss function of mSGD decays at a certain rate faster than that of SGD. In addition, we prove the iterates of AdaGrad are asymptotically convergent to a connected set of stationary points with probability one. Also, this result extends the results from the literature on subsequence convergence and the convergence of time averages. Despite the generality of the above convergence results, we have relaxed some assumptions of gradient noises, convexity of loss functions, as well as boundedness of iterates",
    "checked": true,
    "id": "b8a0ded7907faf2e513116e7a0af44577e0c9dab",
    "semantic_title": "on the convergence of msgd and adagrad for stochastic optimization",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=nioAdKCEdXB": {
    "title": "Likelihood Training of Schr√∂dinger Bridge using Forward-Backward SDEs Theory",
    "volume": "poster",
    "abstract": "Schr√∂dinger Bridge (SB) is an entropy-regularized optimal transport problem that has received increasing attention in deep generative modeling for its mathematical flexibility compared to the Scored-based Generative Model (SGM). However, it remains unclear whether the optimization principle of SB relates to the modern training of deep generative models, which often rely on constructing log-likelihood objectives.This raises questions on the suitability of SB models as a principled alternative for generative applications. In this work, we present a novel computational framework for likelihood training of SB models grounded on Forward-Backward Stochastic Differential Equations Theory ‚Äì a mathematical methodology appeared in stochastic optimal control that transforms the optimality condition of SB into a set of SDEs. Crucially, these SDEs can be used to construct the likelihood objectives for SB that, surprisingly, generalizes the ones for SGM as special cases. This leads to a new optimization principle that inherits the same SB optimality yet without losing applications of modern generative training techniques, and we show that the resulting training algorithm achieves comparable results on generating realistic images on MNIST, CelebA, and CIFAR10. Our code is available at https://github.com/ghliu/SB-FBSDE",
    "checked": true,
    "id": "509e166d5e66df10675a0e15063daad518dcc5ad",
    "semantic_title": "likelihood training of schr√∂dinger bridge using forward-backward sdes theory",
    "citation_count": 200,
    "authors": []
  },
  "https://openreview.net/forum?id=twv2QlJhXzo": {
    "title": "Imitation Learning from Observations under Transition Model Disparity",
    "volume": "poster",
    "abstract": "Learning to perform tasks by leveraging a dataset of expert observations, also known as imitation learning from observations (ILO), is an important paradigm for learning skills without access to the expert reward function or the expert actions. We consider ILO in the setting where the expert and the learner agents operate in different environments, with the source of the discrepancy being the transition dynamics model. Recent methods for scalable ILO utilize adversarial learning to match the state-transition distributions of the expert and the learner, an approach that becomes challenging when the dynamics are dissimilar. In this work, we propose an algorithm that trains an intermediary policy in the learner environment and uses it as a surrogate expert for the learner. The intermediary policy is learned such that the state transitions generated by it are close to the state transitions in the expert dataset. To derive a practical and scalable algorithm, we employ concepts from prior work on estimating the support of a probability distribution. Experiments using MuJoCo locomotion tasks highlight that our method compares favorably to the baselines for ILO with transition dynamics mismatch",
    "checked": true,
    "id": "4c0f15750769e7f96e3f5ff3446e2829f4c0ed2d",
    "semantic_title": "imitation learning from observations under transition model disparity",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=4C93Qvn-tz": {
    "title": "MCMC Should Mix: Learning Energy-Based Model with Neural Transport Latent Space MCMC",
    "volume": "poster",
    "abstract": "Learning energy-based model (EBM) requires MCMC sampling of the learned model as an inner loop of the learning algorithm. However, MCMC sampling of EBMs in high-dimensional data space is generally not mixing, because the energy function, which is usually parametrized by deep network, is highly multi-modal in the data space. This is a serious handicap for both theory and practice of EBMs. In this paper, we propose to learn EBM with a flow-based model (or in general latent variable model) serving as a backbone, so that the EBM is a correction or an exponential tilting of the flow-based model. We show that the model has a particularly simple form in the space of the latent variables of the generative model, and MCMC sampling of the EBM in the latent space mixes well and traverses modes in the data space. This enables proper sampling and learning of EBMs",
    "checked": true,
    "id": "11319c881ab2c1033c7c840cb1abc425e1377f40",
    "semantic_title": "mcmc should mix: learning energy-based model with neural transport latent space mcmc",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=rrWeE9ZDw_": {
    "title": "Autonomous Learning of Object-Centric Abstractions for High-Level Planning",
    "volume": "poster",
    "abstract": "We propose a method for autonomously learning an object-centric representation of a continuous and high-dimensional environment that is suitable for planning. Such representations can immediately be transferred between tasks that share the same types of objects, resulting in agents that require fewer samples to learn a model of a new task. We first demonstrate our approach on a 2D crafting domain consisting of numerous objects where the agent learns a compact, lifted representation that generalises across objects. We then apply it to a series of Minecraft tasks to learn object-centric representations and object types - directly from pixel data - that can be leveraged to solve new tasks quickly. The resulting learned representations enable the use of a task-level planner, resulting in an agent capable of transferring learned representations to form complex, long-term plans",
    "checked": true,
    "id": "c5c4c1b71132f8ac34bb9a6dad1dfe979e69902d",
    "semantic_title": "autonomous learning of object-centric abstractions for high-level planning",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=fCSq8yrDkc": {
    "title": "A fast and accurate splitting method for optimal transport: analysis and implementation",
    "volume": "poster",
    "abstract": "We develop a fast and reliable method for solving large-scale optimal transport (OT) problems at an unprecedented combination of speed and accuracy. Built on the celebrated Douglas-Rachford splitting technique, our method tackles the original OT problem directly instead of solving an approximate regularized problem, as many state-of-the-art techniques do. This allows us to provide sparse transport plans and avoid numerical issues of methods that use entropic regularization. The algorithm has the same cost per iteration as the popular Sinkhorn method, and each iteration can be executed efficiently, in parallel. The proposed method enjoys an iteration complexity $O(1/\\epsilon)$ compared to the best-known $O(1/\\epsilon^2)$ of the Sinkhorn method. In addition, we establish a linear convergence rate for our formulation of the OT problem. We detail an efficient GPU implementation of the proposed method that maintains a primal-dual stopping criterion at no extra cost. Substantial experiments demonstrate the effectiveness of our method, both in terms of computation times and robustness",
    "checked": true,
    "id": "2bacba88a59ac9f307fc450d388bf03921e0466d",
    "semantic_title": "a fast and accurate splitting method for optimal transport: analysis and implementation",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=VLgmhQDVBV": {
    "title": "Implicit Bias of MSE Gradient Optimization in Underparameterized Neural Networks",
    "volume": "poster",
    "abstract": "We study the dynamics of a neural network in function space when optimizing the mean squared error via gradient flow. We show that in the underparameterized regime the network learns eigenfunctions of an integral operator $T_K$ determined by the Neural Tangent Kernel at rates corresponding to their eigenvalues. For example, for uniformly distributed data on the sphere $S^{d - 1}$ and rotation invariant weight distributions, the eigenfunctions of $T_K$ are the spherical harmonics. Our results can be understood as describing a spectral bias in the underparameterized regime. The proofs use the concept of ``Damped Deviations'' where deviations of the NTK matter less for eigendirections with large eigenvalues. Aside from the underparameterized regime, the damped deviations point-of-view allows us to extend certain results in the literature in the overparameterized setting",
    "checked": true,
    "id": "0301a9186a35d9085f8db7f1847fcbcfcfa26232",
    "semantic_title": "implicit bias of mse gradient optimization in underparameterized neural networks",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=POTMtpYI1xH": {
    "title": "Discovering Latent Concepts Learned in BERT",
    "volume": "poster",
    "abstract": "A large number of studies that analyze deep neural network models and their ability to encode various linguistic and non-linguistic concepts provide an interpretation of the inner mechanics of these models. The scope of the analyses is limited to pre-defined concepts that reinforce the traditional linguistic knowledge and do not reflect on how novel concepts are learned by the model. We address this limitation by discovering and analyzing latent concepts learned in neural network models in an unsupervised fashion and provide interpretations from the model's perspective. In this work, we study: i) what latent concepts exist in the pre-trained BERT model, ii) how the discovered latent concepts align or diverge from classical linguistic hierarchy and iii) how the latent concepts evolve across layers. Our findings show: i) a model learns novel concepts (e.g. animal categories and demographic groups), which do not strictly adhere to any pre-defined categorization (e.g. POS, semantic tags), ii) several latent concepts are based on multiple properties which may include semantics, syntax, and morphology, iii) the lower layers in the model dominate in learning shallow lexical concepts while the higher layers learn semantic relations and iv) the discovered latent concepts highlight potential biases learned in the model. We also release a novel BERT ConceptNet dataset consisting of 174 concept labels and 1M annotated instances",
    "checked": true,
    "id": "e33b7282f1e547054a660377383b8ab8464f676e",
    "semantic_title": "discovering latent concepts learned in bert",
    "citation_count": 66,
    "authors": []
  },
  "https://openreview.net/forum?id=dNigytemkL": {
    "title": "The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks",
    "volume": "poster",
    "abstract": "In this paper, we conjecture that if the permutation invariance of neural networks is taken into account, SGD solutions will likely have no barrier in the linear interpolation between them. Although it is a bold conjecture, we show how extensive empirical attempts fall short of refuting it. We further provide a preliminary theoretical result to support our conjecture. Our conjecture has implications for the lottery ticket hypothesis, distributed training, and ensemble methods. The source code is available at \\url{https://github.com/rahimentezari/PermutationInvariance}",
    "checked": true,
    "id": "4dc48bd4e1c0e5986b36eca8339bd45e944d8a82",
    "semantic_title": "the role of permutation invariance in linear mode connectivity of neural networks",
    "citation_count": 251,
    "authors": []
  },
  "https://openreview.net/forum?id=B5XahNLmna": {
    "title": "Data Poisoning Won't Save You From Facial Recognition",
    "volume": "poster",
    "abstract": "Data poisoning has been proposed as a compelling defense against facial recognition models trained on Web-scraped pictures. Users can perturb images they post online, so that models will misclassify future (unperturbed) pictures. We demonstrate that this strategy provides a false sense of security, as it ignores an inherent asymmetry between the parties: users' pictures are perturbed once and for all before being published (at which point they are scraped) and must thereafter fool all future models---including models trained adaptively against the users' past attacks, or models that use new technologies discovered after the attack. We evaluate two systems for poisoning attacks against large-scale facial recognition, Fawkes (500,000+ downloads) and LowKey. We demonstrate how an \"oblivious\" model trainer can simply wait for future developments in computer vision to nullify the protection of pictures collected in the past. We further show that an adversary with black-box access to the attack can (i) train a robust model that resists the perturbations of collected pictures and (ii) detect poisoned pictures uploaded online. We caution that facial recognition poisoning will not admit an \"arms race\" between attackers and defenders. Once perturbed pictures are scraped, the attack cannot be changed so any future successful defense irrevocably undermines users' privacy",
    "checked": true,
    "id": "9d8a948634204fedef929f1e0a24eb0cfc3685eb",
    "semantic_title": "data poisoning won't save you from facial recognition",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=Opmqtk_GvYL": {
    "title": "MetaMorph: Learning Universal Controllers with Transformers",
    "volume": "poster",
    "abstract": "Multiple domains like vision, natural language, and audio are witnessing tremendous progress by leveraging Transformers for large scale pre-training followed by task specific fine tuning. In contrast, in robotics we primarily train a single robot for a single task. However, modular robot systems now allow for the flexible combination of general-purpose building blocks into task optimized morphologies. However, given the exponentially large number of possible robot morphologies, training a controller for each new design is impractical. In this work, we propose MetaMorph, a Transformer based approach to learn a universal controller over a modular robot design space. MetaMorph is based on the insight that robot morphology is just another modality on which we can condition the output of a Transformer. Through extensive experiments we demonstrate that large scale pre-training on a variety of robot morphologies results in policies with combinatorial generalization capabilities, including zero shot generalization to unseen robot morphologies. We further demonstrate that our pre-trained policy can be used for sample-efficient transfer to completely new robot morphologies and tasks",
    "checked": true,
    "id": "e97bcb1695b586fdd7b76893bcd57d493f339ca6",
    "semantic_title": "metamorph: learning universal controllers with transformers",
    "citation_count": 108,
    "authors": []
  },
  "https://openreview.net/forum?id=P-pPW1nxf1r": {
    "title": "HTLM: Hyper-Text Pre-Training and Prompting of Language Models",
    "volume": "poster",
    "abstract": "We introduce HTLM, a hyper-text language model trained on a large-scale web crawl. Modeling hyper-text has a number of advantages: (1) it is easily gathered at scale, (2) it provides rich document-level and end-task-adjacent supervision (e.g. 'class' and 'id' attributes often encode document category information), and (3) it allows for new structured prompting that follows the established semantics of HTML (e.g. to do zero-shot summarization by infilling '<title>' tags for a webpage that contains the input text). We show that pretraining with a BART-style denoising loss directly on simplified HTML provides highly effective transfer for a wide range of end tasks and supervision levels. HTLM matches or exceeds the performance of comparably sized text-only LMs for zero-shot prompting and fine-tuning for classification benchmarks, while also setting new state-of-the-art performance levels for zero-shot summarization. We also find that hyper-text prompts provide more value to HTLM, in terms of data efficiency, than plain text prompts do for existing LMs, and that HTLM is highly effective at auto-prompting itself, by simply generating the most likely hyper-text formatting for any available training data. We will release all code and models to support future HTLM research",
    "checked": true,
    "id": "e596b8adbffa546dbc163e817fb3de72744ec4f6",
    "semantic_title": "htlm: hyper-text pre-training and prompting of language models",
    "citation_count": 78,
    "authors": []
  },
  "https://openreview.net/forum?id=h0OYV0We3oh": {
    "title": "Illiterate DALL-E Learns to Compose",
    "volume": "poster",
    "abstract": "Although DALL-E has shown an impressive ability of composition-based systematic generalization in image generation, it requires the dataset of text-image pairs and the compositionality is provided by the text. In contrast, object-centric representation models like the Slot Attention model learn composable representations without the text prompt. However, unlike DALL-E, its ability to systematically generalize for zero-shot generation is significantly limited. In this paper, we propose a simple but novel slot-based autoencoding architecture, called SLATE, for combining the best of both worlds: learning object-centric representations that allow systematic generalization in zero-shot image generation without text. As such, this model can also be seen as an illiterate DALL-E model. Unlike the pixel-mixture decoders of existing object-centric representation models, we propose to use the Image GPT decoder conditioned on the slots for capturing complex interactions among the slots and pixels. In experiments, we show that this simple and easy-to-implement architecture not requiring a text prompt achieves significant improvement in in-distribution and out-of-distribution (zero-shot) image generation and qualitatively comparable or better slot-attention structure than the models based on mixture decoders",
    "checked": true,
    "id": "daa5ca0a39ecec8ab5c534196eca526bafe41051",
    "semantic_title": "illiterate dall-e learns to compose",
    "citation_count": 150,
    "authors": []
  },
  "https://openreview.net/forum?id=JYtwGwIL7ye": {
    "title": "The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models",
    "volume": "poster",
    "abstract": "Reward hacking---where RL agents exploit gaps in misspecified proxy rewards---has been widely observed, but not yet systematically studied. To understand reward hacking, we construct four RL environments with different misspecified rewards. We investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, and observation space noise. Typically, more capable agents are able to better exploit reward misspecifications, causing them to attain higher proxy reward and lower true reward. Moreover, we find instances of \\emph{phase transitions}: capability thresholds at which the agent's behavior qualitatively shifts, leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To encourage further research on reward misspecification, address this, we propose an anomaly detection task for aberrant policies and offer several baseline detectors",
    "checked": true,
    "id": "10b9ca173c665e3f2c322c2d5ce9b9d433fe4629",
    "semantic_title": "the effects of reward misspecification: mapping and mitigating misaligned models",
    "citation_count": 215,
    "authors": []
  },
  "https://openreview.net/forum?id=J_2xNmVcY4": {
    "title": "Optimizing Neural Networks with Gradient Lexicase Selection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "c15dc309beb930058bc80e995fd5c507348ebacd",
    "semantic_title": "optimizing deep neural networks through neuroevolution with stochastic gradient descent",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=68n2s9ZJWF8": {
    "title": "Offline Reinforcement Learning with Implicit Q-Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "348a855fe01f3f4273bf0ecf851ca688686dbfcc",
    "semantic_title": "offline reinforcement learning with implicit q-learning",
    "citation_count": 1050,
    "authors": []
  },
  "https://openreview.net/forum?id=To-R742x7se": {
    "title": "Learning Distributionally Robust Models at Scale via Composite Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3b35d09d5153edb814bfd82d63666dfb178902d0",
    "semantic_title": "learning distributionally robust models at scale via composite optimization",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=noaG7SrPVK0": {
    "title": "Counterfactual Plans under Distributional Ambiguity",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "262e060ddfa7db0563b083d31552b2855e2daa1e",
    "semantic_title": "counterfactual plans under distributional ambiguity",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=srtIXtySfT4": {
    "title": "Neural Parameter Allocation Search",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "235de6164e754400f82b77fb33be84fc43c79078",
    "semantic_title": "neural parameter allocation search",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=d2TT6gK9qZn": {
    "title": "Non-Linear Operator Approximations for Initial Value Problems",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6d7f1e6f1313e91eacda627b968dbc959343b54e",
    "semantic_title": "non-linear operator approximations for initial value problems",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=7IWGzQ6gZ1D": {
    "title": "Constructing a Good Behavior Basis for Transfer using Generalized Policy Updates",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "46168a5ff72e9f9e0c98e3d63f5758f475af5183",
    "semantic_title": "constructing a good behavior basis for transfer using generalized policy updates",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=7TZeCsNOUB_": {
    "title": "Collapse by Conditioning: Training Class-conditional GANs with Limited Data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "80d5305deec523715dd2b047fde7b929cac6499c",
    "semantic_title": "collapse by conditioning: training class-conditional gans with limited data",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=dSw0QtRMJkO": {
    "title": "High Probability Bounds for a Class of Nonconvex Algorithms with AdaGrad Stepsize",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f1d2a203a8f4e3f596bee84bae87d5f415eff81e",
    "semantic_title": "high probability bounds for a class of nonconvex algorithms with adagrad stepsize",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=1NUsBU-7HAL": {
    "title": "Map Induction: Compositional spatial submap learning for efficient exploration in novel environments",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "75e55822158a651c8121992918ad2d319532b9a6",
    "semantic_title": "map induction: compositional spatial submap learning for efficient exploration in novel environments",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=gFDFKC4gHL4": {
    "title": "How Did the Model Change? Efficiently Assessing Machine Learning API Shifts",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a5f989cf32e7b1379bafffd82e371a2a0bdb2098",
    "semantic_title": "how did the model change? efficiently assessing machine learning api shifts",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=31d5RLCUuXC": {
    "title": "A Tale of Two Flows: Cooperative Learning of Langevin Flow and Normalizing Flow Toward Energy-Based Model",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e598e7a5ca5f37cf2e5fe6e3805f541bd65c5439",
    "semantic_title": "a tale of two flows: cooperative learning of langevin flow and normalizing flow toward energy-based model",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=WVX0NNVBBkV": {
    "title": "Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8bcb5534227214b83255f5b9dedbc0d46a44794a",
    "semantic_title": "robust learning meets generative models: can proxy distributions improve adversarial robustness?",
    "citation_count": 135,
    "authors": []
  },
  "https://openreview.net/forum?id=ECvgmYVyeUz": {
    "title": "Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9d2ecdc3792904186203efdabc0b40621558dd03",
    "semantic_title": "chaos is a ladder: a new theoretical understanding of contrastive learning via augmentation overlap",
    "citation_count": 109,
    "authors": []
  },
  "https://openreview.net/forum?id=xNO7OEIcJc6": {
    "title": "Language-biased image classification: evaluation based on semantic representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9f984b48d57a7e0704c4cb46264e9f697cf60e42",
    "semantic_title": "language-biased image classification: evaluation based on semantic representations",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=fwzUgo0FM9v": {
    "title": "Robbing the Fed: Directly Obtaining Private Data in Federated Learning with Modified Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8effb06f8f719da740f5cbd5e388354776bf94d9",
    "semantic_title": "robbing the fed: directly obtaining private data in federated learning with modified models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3pugbNqOh5m": {
    "title": "Practical Conditional Neural Process Via Tractable Dependent Predictions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8a95c7a1d0527f64921fbf5634c35c137d115782",
    "semantic_title": "practical conditional neural processes via tractable dependent predictions",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=OWZVD-l-ZrC": {
    "title": "Reward Uncertainty for Exploration in Preference-based Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cc9f2fd320a279741403c4bfbeb91179803c428c",
    "semantic_title": "reward uncertainty for exploration in preference-based reinforcement learning",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=oj2yn1Q4Ett": {
    "title": "Decentralized Learning for Overparameterized Problems: A Multi-Agent Kernel Approximation Approach",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "61531184e5d9e5602ef8f8c87d17c19c6300cf49",
    "semantic_title": "decentralized learning for overparameterized problems: a multi-agent kernel approximation approach",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=YiBa9HKTyXE": {
    "title": "Permutation-Based SGD: Is Random Optimal?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1b7af5e0ef5ce854c5d56e9953f026bbdc371327",
    "semantic_title": "permutation-based sgd: is random optimal?",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=4p6_5HBWPCw": {
    "title": "Graph-less Neural Networks: Teaching Old MLPs New Tricks Via Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8994fd8dc3a4e18e93013b8aa8a79c732bf81572",
    "semantic_title": "graph-less neural networks: teaching old mlps new tricks via distillation",
    "citation_count": 200,
    "authors": []
  },
  "https://openreview.net/forum?id=B8DVo9B1YE0": {
    "title": "Relating transformers to models and neural representations of the hippocampal formation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "06e40b7a703079c280f8f0886ac2bd984cd318ce",
    "semantic_title": "relating transformers to models and neural representations of the hippocampal formation",
    "citation_count": 91,
    "authors": []
  },
  "https://openreview.net/forum?id=ChMLTGRjFcU": {
    "title": "How many degrees of freedom do we need to train deep networks: a loss landscape perspective",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "93cdcc6f246422d5ac08930a8ee80ac2213ee65a",
    "semantic_title": "how many degrees of freedom do we need to train deep networks: a loss landscape perspective",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=uqBOne3LUKy": {
    "title": "Is Importance Weighting Incompatible with Interpolating Classifiers?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a4e3a467804781a3c5a427f4032bf180ea8bf585",
    "semantic_title": "is importance weighting incompatible with interpolating classifiers?",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=ibrUkC-pbis": {
    "title": "Neural Models for Output-Space Invariance in Combinatorial Problems",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6294b2966523a264e99cf90dffaf31e40874c6cb",
    "semantic_title": "neural models for output-space invariance in combinatorial problems",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=iUuzzTMUw9K": {
    "title": "StyleNeRF: A Style-based 3D Aware Generator for High-resolution Image Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3fdda879abf2462b09139fb1fd1c2c147c9a0ef0",
    "semantic_title": "stylenerf: a style-based 3d-aware generator for high-resolution image synthesis",
    "citation_count": 581,
    "authors": []
  },
  "https://openreview.net/forum?id=8eb12UQYxrG": {
    "title": "The Role of Pretrained Representations for the OOD Generalization of RL Agents",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e48ae8395d28ce3cc7990e4acd83362a65656af2",
    "semantic_title": "the role of pretrained representations for the ood generalization of rl agents",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=rhOiUS8KQM9": {
    "title": "Enabling Arbitrary Translation Objectives with Adaptive Tree Search",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4bd990e865aea7de457e917d3b0f8412567bdb5d",
    "semantic_title": "enabling arbitrary translation objectives with adaptive tree search",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rpxJc9j04U": {
    "title": "Proof Artifact Co-Training for Theorem Proving with Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9231927bc0a9ed10de64cad05640587893eba4b1",
    "semantic_title": "proof artifact co-training for theorem proving with language models",
    "citation_count": 135,
    "authors": []
  },
  "https://openreview.net/forum?id=aBO5SvgSt1": {
    "title": "Mirror Descent Policy Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "08eb40da621640e14ffac36c5e2595d7c0250541",
    "semantic_title": "mirror descent policy optimization",
    "citation_count": 94,
    "authors": []
  },
  "https://openreview.net/forum?id=OcKMT-36vUs": {
    "title": "A Loss Curvature Perspective on Training Instabilities of Deep Learning Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f6e05a4269a60d5807d99872296be34e4e25ea17",
    "semantic_title": "a loss curvature perspective on training instabilities of deep learning models",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=xP3cPq2hQC": {
    "title": "Cross-Domain Imitation Learning via Optimal Transport",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "09f9af75d631ff9e6b71726c9344b9bedb921b42",
    "semantic_title": "cross-domain imitation learning via optimal transport",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=0UXT6PpRpW": {
    "title": "Large-Scale Representation Learning on Graphs via Bootstrapping",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b38fcf2aba8b173ba11ada830b7038f496643893",
    "semantic_title": "large-scale representation learning on graphs via bootstrapping",
    "citation_count": 253,
    "authors": []
  },
  "https://openreview.net/forum?id=xZ6H7wydGl": {
    "title": "Robust and Scalable SDE Learning: A Functional Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "95088cb4b573cde65e748d9764dbbced0157309c",
    "semantic_title": "robust and scalable sde learning: a functional perspective",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=JPkQwEdYn8": {
    "title": "Neural Processes with Stochastic Attention: Paying more attention to the context dataset",
    "volume": "poster",
    "abstract": "Neural processes (NPs) aim to stochastically complete unseen data points based on a given context dataset. NPs essentially leverage a given dataset as a context representation to derive a suitable identifier for a novel task. To improve the prediction accuracy, many variants of NPs have investigated context embedding approaches that generally design novel network architectures and aggregation functions satisfying permutation invariant. In this work, we propose a stochastic attention mechanism for NPs to capture appropriate context information. From the perspective of information theory, we demonstrate that the proposed method encourages context embedding to be differentiated from a target dataset, allowing NPs to consider features in a target dataset and context embedding independently. We observe that the proposed method can appropriately capture context embedding even under noisy data sets and restricted task distributions, where typical NPs suffer from a lack of context embeddings. We empirically show that our approach substantially outperforms conventional NPs in various domains through 1D regression, predator-prey model, and image completion. Moreover, the proposed method is also validated by MovieLens-10k dataset, a real-world problem",
    "checked": true,
    "id": "d46f0fca0ab3920a8956664da0153308018d88e4",
    "semantic_title": "neural processes with stochastic attention: paying more attention to the context dataset",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=SLz5sZjacp": {
    "title": "Evaluating Disentanglement of Structured Representations",
    "volume": "poster",
    "abstract": "We introduce the first metric for evaluating disentanglement at individual hierarchy levels of a structured latent representation. Applied to object-centric generative models, this offers a systematic, unified approach to evaluating (i) object separation between latent slots (ii) disentanglement of object properties inside individual slots (iii) disentanglement of intrinsic and extrinsic object properties. We theoretically show that our framework gives stronger guarantees of selecting a good model than previous disentanglement metrics. Experimentally, we demonstrate that viewing object compositionality as a disentanglement problem addresses several issues with prior visual metrics of object separation. As a core technical component, we present the first representation probing algorithm handling slot permutation invariance",
    "checked": true,
    "id": "be88e1259f82d3bb760e0f096398b6dadd28efd6",
    "semantic_title": "evaluating disentanglement of structured representations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CS4463zx6Hi": {
    "title": "Geometric Transformers for Protein Interface Contact Prediction",
    "volume": "poster",
    "abstract": "Computational methods for predicting the interface contacts between proteins come highly sought after for drug discovery as they can significantly advance the accuracy of alternative approaches, such as protein-protein docking, protein function analysis tools, and other computational methods for protein bioinformatics. In this work, we present the Geometric Transformer, a novel geometry-evolving graph transformer for rotation and translation-invariant protein interface contact prediction, packaged within DeepInteract, an end-to-end prediction pipeline. DeepInteract predicts partner-specific protein interface contacts (i.e., inter-protein residue-residue contacts) given the 3D tertiary structures of two proteins as input. In rigorous benchmarks, DeepInteract, on challenging protein complex targets from the 13th and 14th CASP-CAPRI experiments as well as Docking Benchmark 5, achieves 14% and 1.1% top L/5 precision (L: length of a protein unit in a complex), respectively. In doing so, DeepInteract, with the Geometric Transformer as its graph-based backbone, outperforms existing methods for interface contact prediction in addition to other graph-based neural network backbones compatible with DeepInteract, thereby validating the effectiveness of the Geometric Transformer for learning rich relational-geometric features for downstream tasks on 3D protein structures",
    "checked": true,
    "id": "8fcc10377b8bd04dc69e248027d3ebe12306d4b0",
    "semantic_title": "geometric transformers for protein interface contact prediction",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=E4EE_ohFGz": {
    "title": "Diurnal or Nocturnal? Federated Learning of Multi-branch Networks from Periodically Shifting Distributions",
    "volume": "poster",
    "abstract": "Federated learning has been deployed to train machine learning models from decentralized client data on mobile devices in practice. The clients available for training are observed to have periodically shifting distributions changing with the time of day, which can cause instability in training and degrade the model performance. In this paper, instead of modeling the distribution shift with a block-cyclic pattern as previous works, we model it with a mixture of distributions that gradually shifts between daytime and nighttime modes, and find this intuitive model to better match the observations in practical federated learning systems. Furthermore, we propose to jointly train a clustering model and a multi-branch network to allocate lightweight specialized branches to clients from different modes. A temporal prior is used to significantly boost the training performance. Experiments for image classification on EMNIST and CIFAR datasets, and next word prediction on the Stack Overflow dataset show that the proposed algorithm can counter the effects of the distribution shift and significantly improve the final model performance",
    "checked": true,
    "id": "4f3bbc2f389e6650f27b14aef05c928156883a24",
    "semantic_title": "diurnal or nocturnal? federated learning of multi-branch networks from periodically shifting distributions",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=5kq11Tl1z4": {
    "title": "IGLU: Efficient GCN Training via Lazy Updates",
    "volume": "poster",
    "abstract": "Training multi-layer Graph Convolution Networks (GCN) using standard SGD techniques scales poorly as each descent step ends up updating node embeddings for a large portion of the graph. Recent attempts to remedy this sub-sample the graph that reduces compute but introduce additional variance and may offer suboptimal performance. This paper develops the IGLU method that caches intermediate computations at various GCN layers thus enabling lazy updates that significantly reduce the compute cost of descent. IGLU introduces bounded bias into the gradients but nevertheless converges to a first-order saddle point under standard assumptions such as objective smoothness. Benchmark experiments show that IGLU offers up to 1.2% better accuracy despite requiring up to 88% less compute",
    "checked": true,
    "id": "348a65fd16a8502dc75d9960b27c8261f9bc3c27",
    "semantic_title": "iglu: efficient gcn training via lazy updates",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=FmBegXJToY": {
    "title": "Procedural generalization by planning with self-supervised world models",
    "volume": "poster",
    "abstract": "One of the key promises of model-based reinforcement learning is the ability to generalize using an internal model of the world to make predictions in novel environments and tasks. However, the generalization ability of model-based agents is not well understood because existing work has focused on model-free agents when benchmarking generalization. Here, we explicitly measure the generalization ability of model-based agents in comparison to their model-free counterparts. We focus our analysis on MuZero (Schrittwieser et al., 2020), a powerful model-based agent, and evaluate its performance on both procedural and task generalization. We identify three factors of procedural generalization---planning, self-supervised representation learning, and procedural data diversity---and show that by combining these techniques, we achieve state-of-the art generalization performance and data efficiency on Procgen (Cobbe et al., 2019). However, we find that these factors do not always provide the same benefits for the task generalization benchmarks in Meta-World (Yu et al., 2019), indicating that transfer remains a challenge and may require different approaches than procedural generalization. Overall, we suggest that building generalizable agents requires moving beyond the single-task, model-free paradigm and towards self-supervised model-based agents that are trained in rich, procedural, multi-task environments",
    "checked": true,
    "id": "44164c068499fbe387a1765104d69a8cbc5f0327",
    "semantic_title": "procedural generalization by planning with self-supervised world models",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=-Gk_IPJWvk": {
    "title": "Top-N: Equivariant Set and Graph Generation without Exchangeability",
    "volume": "poster",
    "abstract": "This work addresses one-shot set and graph generation, and, more specifically, the parametrization of probabilistic decoders that map a vector-shaped prior to a distribution over sets or graphs. Sets and graphs are most commonly generated by first sampling points i.i.d. from a normal distribution, and then processing these points along with the prior vector using Transformer layers or Graph Neural Networks. This architecture is designed to generate exchangeable distributions, i.e., all permutations of the generated outputs are equally likely. We however show that it only optimizes a proxy to the evidence lower bound, which makes it hard to train. We then study equivariance in generative settings and show that non-exchangeable methods can still achieve permutation equivariance. Using this result, we introduce Top-n creation, a differentiable generation mechanism that uses the latent vector to select the most relevant points from a trainable reference set. Top-n can replace i.i.d. generation in any Variational Autoencoder or Generative Adversarial Network. Experimentally, our method outperforms i.i.d. generation by 15% at SetMNIST reconstruction, by 33% at object detection on CLEVR, generates sets that are 74% closer to the true distribution on a synthetic molecule-like dataset, and generates more valid molecules on QM9",
    "checked": true,
    "id": "07e503916bf4881c2b65336ca1f8f5a81d7f3b9c",
    "semantic_title": "top-n: equivariant set and graph generation without exchangeability",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=P7FLfMLTSEX": {
    "title": "The Spectral Bias of Polynomial Neural Networks",
    "volume": "poster",
    "abstract": "Polynomial neural networks (PNNs) have been recently shown to be particularly effective at image generation and face recognition, where high-frequency information is critical. Previous studies have revealed that neural networks demonstrate a $\\text{\\it{spectral bias}}$ towards low-frequency functions, which yields faster learning of low-frequency components during training. Inspired by such studies, we conduct a spectral analysis of the Neural Tangent Kernel (NTK) of PNNs. We find that the $\\Pi$-Net family, i.e., a recently proposed parametrization of PNNs, speeds up the learning of the higher frequencies. We verify the theoretical bias through extensive experiments. We expect our analysis to provide novel insights into designing architectures and learning frameworks by incorporating multiplicative interactions via polynomials",
    "checked": true,
    "id": "4b510291d020f9bb7bb3acea9a0f78af78f824b9",
    "semantic_title": "the spectral bias of polynomial neural networks",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=-e4EXDWXnSn": {
    "title": "Invariant Causal Representation Learning for Out-of-Distribution Generalization",
    "volume": "poster",
    "abstract": "Due to spurious correlations, machine learning systems often fail to generalize to environments whose distributions differ from the ones used at training time. Prior work addressing this, either explicitly or implicitly, attempted to find a data representation that has an invariant relationship with the target. This is done by leveraging a diverse set of training environments to reduce the effect of spurious features and build an invariant predictor. However, these methods have generalization guarantees only when both data representation and classifiers come from a linear model class. We propose invariant Causal Representation Learning (iCaRL), an approach that enables out-of-distribution (OOD) generalization in the nonlinear setting (i.e., nonlinear representations and nonlinear classifiers). It builds upon a practical and general assumption: the prior over the data representation (i.e., a set of latent variables encoding the data) given the target and the environment belongs to general exponential family distributions, i.e., a more flexible conditionally non-factorized prior that can actually capture complicated dependences between the latent variables. Based on this, we show that it is possible to identify the data representation up to simple transformations. We also show that all direct causes of the target can be fully discovered, which further enables us to obtain generalization guarantees in the nonlinear setting. Experiments on both synthetic and real-world datasets demonstrate that our approach outperforms a variety of baseline methods",
    "checked": true,
    "id": "164b72b345a4cb03f62547abf62a033dcbd784ae",
    "semantic_title": "invariant causal representation learning for out-of-distribution generalization",
    "citation_count": 94,
    "authors": []
  },
  "https://openreview.net/forum?id=HCRVf71PMF": {
    "title": "LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5",
    "volume": "poster",
    "abstract": "Existing approaches to lifelong language learning rely on plenty of labeled data for learning a new task, which is hard to obtain in most real scenarios. Considering that humans can continually learn new tasks from a handful of examples, we expect the models also to be able to generalize well on new few-shot tasks without forgetting the previous ones. In this work, we define this more challenging yet practical problem as Lifelong Few-shot Language Learning (LFLL) and propose a unified framework for it based on prompt tuning of T5. Our framework called LFPT5 takes full advantage of PT's strong few-shot learning ability, and simultaneously trains the model as a task solver and a data generator. Before learning a new domain of the same task type, LFPT5 generates pseudo (labeled) samples of previously learned domains, and later gets trained on those samples to alleviate forgetting of previous knowledge as it learns the new domain. In addition, a KL divergence loss is minimized to achieve label consistency between the previous and the current model. While adapting to a new task type, LFPT5 includes and tunes additional prompt embeddings for the new task. With extensive experiments, we demonstrate that LFPT5 can be applied to various different types of tasks and significantly outperform previous methods in different LFLL settings",
    "checked": true,
    "id": "fa133b4200729a57db96ae50aff8c4a5ff819f43",
    "semantic_title": "lfpt5: a unified framework for lifelong few-shot language learning based on prompt tuning of t5",
    "citation_count": 113,
    "authors": []
  },
  "https://openreview.net/forum?id=6yVvwR9H9Oj": {
    "title": "On Non-Random Missing Labels in Semi-Supervised Learning",
    "volume": "poster",
    "abstract": "Semi-Supervised Learning (SSL) is fundamentally a missing label problem, in which the label Missing Not At Random (MNAR) problem is more realistic and challenging, compared to the widely-adopted yet naive Missing Completely At Random assumption where both labeled and unlabeled data share the same class distribution. Different from existing SSL solutions that overlook the role of ''class'' in causing the non-randomness, e.g., users are more likely to label popular classes, we explicitly incorporate ''class'' into SSL. Our method is three-fold: 1) We propose Class-Aware Propensity (CAP) that exploits the unlabeled data to train an improved classifier using the biased labeled data. 2) To encourage rare class training, whose model is low-recall but high-precision that discards too many pseudo-labeled data, we propose Class-Aware Imputation (CAI) that dynamically decreases (or increases) the pseudo-label assignment threshold for rare (or frequent) classes. 3) Overall, we integrate CAP and CAI into a Class-Aware Doubly Robust (CADR) estimator for training an unbiased SSL model. Under various MNAR settings and ablations, our method not only significantly outperforms existing baselines, but also surpasses other label bias removal SSL methods",
    "checked": true,
    "id": "410b8ab687f6f333e254512b49b8703908021fb2",
    "semantic_title": "on non-random missing labels in semi-supervised learning",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=sPfB2PI87BZ": {
    "title": "Mapping conditional distributions for domain adaptation under generalized target shift",
    "volume": "poster",
    "abstract": "We consider the problem of unsupervised domain adaptation (UDA) between a source and a target domain under conditional and label shift a.k.a Generalized Target Shift (GeTarS). Unlike simpler UDA settings, few works have addressed this challenging problem. Recent approaches learn domain-invariant representations, yet they have practical limitations and rely on strong assumptions that may not hold in practice. In this paper, we explore a novel and general approach to align pretrained representations, which circumvents existing drawbacks. Instead of constraining representation invariance, it learns an optimal transport map, implemented as a NN, which maps source representations onto target ones. Our approach is flexible and scalable, it preserves the problem's structure and it has strong theoretical guarantees under mild assumptions. In particular, our solution is unique, matches conditional distributions across domains, recovers target proportions and explicitly controls the target generalization risk. Through an exhaustive comparison on several datasets, we challenge the state-of-the-art in GeTarS",
    "checked": true,
    "id": "7b9af84120079445410210626e2909a03b828211",
    "semantic_title": "mapping conditional distributions for domain adaptation under generalized target shift",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=oWZsQ8o5EA": {
    "title": "On the Generalization of Models Trained with SGD: Information-Theoretic Bounds and Implications",
    "volume": "poster",
    "abstract": "This paper follows up on a recent work of Neu et al. (2021) and presents some new information-theoretic upper bounds for the generalization error of machine learning models, such as neural networks, trained with SGD. We apply these bounds to analyzing the generalization behaviour of linear and two-layer ReLU networks. Experimental study of these bounds provide some insights on the SGD training of neural networks. They also point to a new and simple regularization scheme which we show performs comparably to the current state of the art",
    "checked": true,
    "id": "55f77fd6a6ec1869c08c74cdb45640f275e65788",
    "semantic_title": "on the generalization of models trained with sgd: information-theoretic bounds and implications",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=3PN4iyXBeF": {
    "title": "Amortized Implicit Differentiation for Stochastic Bilevel Optimization",
    "volume": "poster",
    "abstract": "We study a class of algorithms for solving bilevel optimization problems in both stochastic and deterministic settings when the inner-level objective is strongly convex. Specifically, we consider algorithms based on inexact implicit differentiation and we exploit a warm-start strategy to amortize the estimation of the exact gradient. We then introduce a unified theoretical framework inspired by the study of singularly perturbed systems to analyze such amortized algorithms. By using this framework, our analysis shows these algorithms to match the computational complexity of oracle methods that have access to an unbiased estimate of the gradient, thus outperforming many existing results for bilevel optimization. We illustrate these findings on synthetic experiments and demonstrate the efficiency of these algorithms on hyper-parameter optimization experiments involving several thousands of variables",
    "checked": true,
    "id": "dab2cd1db9f2182d77df4d6fa689f37e93b506a7",
    "semantic_title": "amortized implicit differentiation for stochastic bilevel optimization",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=FlwzVjfMryn": {
    "title": "Multi-objective Optimization by Learning Space Partition",
    "volume": "poster",
    "abstract": "In contrast to single-objective optimization (SOO), multi-objective optimization (MOO) requires an optimizer to find the Pareto frontier, a subset of feasible solutions that are not dominated by other feasible solutions. In this paper, we propose LaMOO, a novel multi-objective optimizer that learns a model from observed samples to partition the search space and then focus on promising regions that are likely to contain a subset of the Pareto frontier. The partitioning is based on the dominance number, which measures \"how close'' a data point is to the Pareto frontier among existing samples. To account for possible partition errors due to limited samples and model mismatch, we leverage Monte Carlo Tree Search (MCTS) to exploit promising regions while exploring suboptimal regions that may turn out to contain good solutions later. Theoretically, we prove the efficacy of learning space partitioning via LaMOO under certain assumptions. Empirically, on the HyperVolume (HV) benchmark, a popular MOO metric, LaMOO substantially outperforms strong baselines on multiple real-world MOO tasks, by up to 225% in sample efficiency for neural architecture search on Nasbench201, and up to 10% for molecular design",
    "checked": true,
    "id": "4b253e537326a845991fbe1ac182394fbb7f83e4",
    "semantic_title": "multi-objective optimization by learning space partitions",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=gJcEM8sxHK": {
    "title": "Mapping Language Models to Grounded Conceptual Spaces",
    "volume": "poster",
    "abstract": "A fundamental criticism of text-only language models (LMs) is their lack of grounding---that is, the ability to tie a word for which they have learned a representation, to its actual use in the world. However, despite this limitation, large pre-trained LMs have been shown to have a remarkable grasp of the conceptual structure of language, as demonstrated by their ability to answer questions, generate fluent text, or make inferences about entities, objects, and properties that they have never physically observed. In this work we investigate the extent to which the rich conceptual structure that LMs learn indeed reflects the conceptual structure of the non-linguistic world---which is something that LMs have never observed. We do this by testing whether the LMs can learn to map an entire conceptual domain (e.g., direction or colour) onto a grounded world representation given only a small number of examples. For example, we show a model what the word ``left\" means using a textual depiction of a grid world, and assess how well it can generalise to related concepts, for example, the word ``right\", in a similar grid world. We investigate a range of generative language models of varying sizes (including GPT-2 and GPT-3), and see that although the smaller models struggle to perform this mapping, the largest model can not only learn to ground the concepts that it is explicitly taught, but appears to generalise to several instances of unseen concepts as well. Our results suggest an alternative means of building grounded language models: rather than learning grounded representations ``from scratch'', it is possible that large text-only models learn a sufficiently rich conceptual structure that could allow them to be grounded in a data-efficient way",
    "checked": true,
    "id": "57db9833549247241decf442fcc30f6eb414981b",
    "semantic_title": "mapping language models to grounded conceptual spaces",
    "citation_count": 172,
    "authors": []
  },
  "https://openreview.net/forum?id=iulEMLYh1uR": {
    "title": "The Efficiency Misnomer",
    "volume": "poster",
    "abstract": "Model efficiency is a critical aspect of developing and deploying machine learning models. Inference time and latency directly affect the user experience, and some applications have hard requirements. In addition to inference costs, model training also have direct financial and environmental impacts. Although there are numerous well-established metrics (cost indicators) for measuring model efficiency, researchers and practitioners often assume that these metrics are correlated with each other and report only a few of them. In this paper, we thoroughly discuss common cost indicators, their advantages and disadvantages, and how they can contradict each other. We demonstrate how incomplete reporting of cost indicators can lead to partial conclusions and a blurred or incomplete picture of the practical considerations of different models. We further present suggestions to improve reporting of efficiency metrics",
    "checked": true,
    "id": "66d735987a31d666a6459566ae026c40ab9a1c3a",
    "semantic_title": "the efficiency misnomer",
    "citation_count": 109,
    "authors": []
  },
  "https://openreview.net/forum?id=auOPcdAcoy": {
    "title": "Hybrid Memoised Wake-Sleep: Approximate Inference at the Discrete-Continuous Interface",
    "volume": "poster",
    "abstract": "Modeling complex phenomena typically involves the use of both discrete and continuous variables. Such a setting applies across a wide range of problems, from identifying trends in time-series data to performing effective compositional scene understanding in images. Here, we propose Hybrid Memoised Wake-Sleep (HMWS), an algorithm for effective inference in such hybrid discrete-continuous models. Prior approaches to learning suffer as they need to perform repeated expensive inner-loop discrete inference. We build on a recent approach, Memoised Wake-Sleep (MWS), which alleviates part of the problem by memoising discrete variables, and extend it to allow for a principled and effective way to handle continuous variables by learning a separate recognition model used for importance-sampling based approximate inference and marginalization. We evaluate HMWS in the GP-kernel learning and 3D scene understanding domains, and show that it outperforms current state-of-the-art inference methods",
    "checked": true,
    "id": "5a533bce8c685b67d5279ed72e25835520177727",
    "semantic_title": "hybrid memoised wake-sleep: approximate inference at the discrete-continuous interface",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=MR7XubKUFB": {
    "title": "Adversarial Retriever-Ranker for Dense Text Retrieval",
    "volume": "poster",
    "abstract": "Current dense text retrieval models face two typical challenges. First, it adopts a siamese dual-encoder architecture to encode query and document independently for fast indexing and searching, whereas neglecting the finer-grained term-wise interactions. This results in a sub-optimal recall performance. Second, it highly relies on a negative sampling technique to build up the negative documents in its contrastive loss. To address these challenges, we present Adversarial Retriever-Ranker (AR2), which consists of a dual-encoder retriever plus a cross-encoder ranker. The two models are jointly optimized according to a minimax adversarial objective: the retriever learns to retrieve negative documents to cheat the ranker, while the ranker learns to rank a collection of candidates including both the ground-truth and the retrieved ones, as well as providing progressive direct feedback to the dual-encoder retriever. Through this adversarial game, the retriever gradually produces harder negative documents to train a better ranker, whereas the cross-encoder ranker provides progressive feedback to improve retriever. We evaluate AR2 on three benchmarks. Experimental results show that AR2 consistently and significantly outperforms existing dense retriever methods and achieves new state-of-the-art results on all of them. This includes the improvements on Natural Questions R@5 to 77.9%(+2.1%), TriviaQA R@5 to 78.2%(+1.4), and MS-MARCO MRR@10 to 39.5%(+1.3%). We will make our code, models, and data publicly available",
    "checked": true,
    "id": "606957402ee0741ed130c6a03ec7f20d30b7083f",
    "semantic_title": "adversarial retriever-ranker for dense text retrieval",
    "citation_count": 122,
    "authors": []
  },
  "https://openreview.net/forum?id=t5s-hd1bqLk": {
    "title": "Conditioning Sequence-to-sequence Networks with Learned Activations",
    "volume": "poster",
    "abstract": "Conditional neural networks play an important role in a number of sequence-to-sequence modeling tasks, including personalized sound enhancement (PSE), speaker dependent automatic speech recognition (ASR), and generative modeling such as text-to-speech synthesis. In conditional neural networks, the output of a model is often influenced by a conditioning vector, in addition to the input. Common approaches of conditioning include input concatenation or modulation with the conditioning vector, which comes at a cost of increased model size. In this work, we introduce a novel approach of neural network conditioning by learning intermediate layer activations based on the conditioning vector. We systematically explore and show that learned activation functions can produce conditional models with comparable or better quality, while decreasing model sizes, thus making them ideal candidates for resource-efficient on-device deployment. As exemplary target use-cases we consider (i) the task of PSE as a pre-processing technique for improving telephony or pre-trained ASR performance under noise, and (ii) personalized ASR in single speaker scenarios. We find that conditioning via activation function learning is an effective modeling strategy, suggesting a broad applicability of the proposed technique across a number of application domains",
    "checked": true,
    "id": "36c2b7f37554bc508b7d0a0bebb07b240c56169b",
    "semantic_title": "conditioning sequence-to-sequence networks with learned activations",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=BRFWxcZfAdC": {
    "title": "LOSSY COMPRESSION WITH DISTRIBUTION SHIFT AS ENTROPY CONSTRAINED OPTIMAL TRANSPORT",
    "volume": "poster",
    "abstract": "We study an extension of lossy compression where the reconstruction distribution is different from the source distribution in order to account for distributional shift due to processing. We formulate this as a generalization of optimal transport with an entropy bottleneck to account for the rate constraint due to compression. We provide expressions for the tradeoff between compression rate and the achievable distortion with and without shared common randomness between the encoder and decoder. We study the examples of binary, uniform and Gaussian sources (in an asymptotic setting) in detail and demonstrate that shared randomness can strictly improve the tradeoff. For the case without common randomness and squared-Euclidean distortion, we show that the optimal solution partially decouples into the problem of optimal compression and transport and also characterize the penalty associated with fully decoupling them. We provide experimental results by training deep learning end-to-end compression systems for performing denoising on SVHN and super-resolution on MNIST suggesting consistency with our theoretical results",
    "checked": true,
    "id": "226425817d69aa51f6df1b7b8eca65e29a4e84fb",
    "semantic_title": "lossy compression with distribution shift as entropy constrained optimal transport",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=gKLAAfiytI": {
    "title": "Equivariant Self-Supervised Learning: Encouraging Equivariance in Representations",
    "volume": "poster",
    "abstract": "In state-of-the-art self-supervised learning (SSL) pre-training produces semantically good representations by encouraging them to be invariant under meaningful transformations prescribed from human knowledge. In fact, the property of invariance is a trivial instance of a broader class called equivariance, which can be intuitively understood as the property that representations transform according to the way the inputs transform. Here, we show that rather than using only invariance, pre-training that encourages non-trivial equivariance to some transformations, while maintaining invariance to other transformations, can be used to improve the semantic quality of representations. Specifically, we extend popular SSL methods to a more general framework which we name Equivariant Self-Supervised Learning (E-SSL). In E-SSL, a simple additional pre-training objective encourages equivariance by predicting the transformations applied to the input. We demonstrate E-SSL's effectiveness empirically on several popular computer vision benchmarks, e.g. improving SimCLR to 72.5% linear probe accuracy on ImageNet. Furthermore, we demonstrate usefulness of E-SSL for applications beyond computer vision; in particular, we show its utility on regression problems in photonics science. Our code, datasets and pre-trained models are available at https://github.com/rdangovs/essl to aid further research in E-SSL",
    "checked": true,
    "id": "b81c1a10fb333b59d34fdf022690285b8ae7dbdc",
    "semantic_title": "equivariant self-supervised learning: encouraging equivariance in representations",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=25kzAhUB1lz": {
    "title": "Direct then Diffuse: Incremental Unsupervised Skill Discovery for State Covering and Goal Reaching",
    "volume": "poster",
    "abstract": "Learning meaningful behaviors in the absence of reward is a difficult problem in reinforcement learning. A desirable and challenging unsupervised objective is to learn a set of diverse skills that provide a thorough coverage of the state space while being directed, i.e., reliably reaching distinct regions of the environment. In this paper, we build on the mutual information framework for skill discovery and introduce UPSIDE, which addresses the coverage-directedness trade-off in the following ways: 1) We design policies with a decoupled structure of a directed skill, trained to reach a specific region, followed by a diffusing part that induces a local coverage. 2) We optimize policies by maximizing their number under the constraint that each of them reaches distinct regions of the environment (i.e., they are sufficiently discriminable) and prove that this serves as a lower bound to the original mutual information objective. 3) Finally, we compose the learned directed skills into a growing tree that adaptively covers the environment. We illustrate in several navigation and control environments how the skills learned by UPSIDE solve sparse-reward downstream tasks better than existing baselines",
    "checked": true,
    "id": "a01fae01cd9a3067fa6b8a777e70efe86bdc4699",
    "semantic_title": "direct then diffuse: incremental unsupervised skill discovery for state covering and goal reaching",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=Nh7CtbyoqV5": {
    "title": "Normalization of Language Embeddings for Cross-Lingual Alignment",
    "volume": "poster",
    "abstract": "Learning a good transfer function to map the word vectors from two languages into a shared cross-lingual word vector space plays a crucial role in cross-lingual NLP. It is useful in translation tasks and important in allowing complex models built on a high-resource language like English to be directly applied on an aligned low resource language. While Procrustes and other techniques can align language models with some success, it has recently been identified that structural differences (for instance, due to differing word frequency) create different profiles for various monolingual embedding. When these profiles differ across languages, it correlates with how well languages can align and their performance on cross-lingual downstream tasks. In this work, we develop a very general language embedding normalization procedure, building and subsuming various previous approaches, which removes these structural profiles across languages without destroying their intrinsic meaning. We demonstrate that meaning is retained and alignment is improved on similarity, translation, and cross-language classification tasks. Our proposed normalization clearly outperforms all prior approaches like centering and vector normalization on each task and with each alignment approach",
    "checked": true,
    "id": "1b9b1ea97a421cad079c87540d1b2bf1a24ee7af",
    "semantic_title": "normalization of language embeddings for cross-lingual alignment",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=Q76Y7wkiji": {
    "title": "Boosting the Certified Robustness of L-infinity Distance Nets",
    "volume": "poster",
    "abstract": "Recently, Zhang et al. (2021) developed a new neural network architecture based on $\\ell_\\infty$-distance functions, which naturally possesses certified $\\ell_\\infty$ robustness by its construction. Despite the novel design and theoretical foundation, so far the model only achieved comparable performance to conventional networks. In this paper, we make the following two contributions: $\\mathrm{(i)}$ We demonstrate that $\\ell_\\infty$-distance nets enjoy a fundamental advantage in certified robustness over conventional networks (under typical certification approaches); $\\mathrm{(ii)}$ With an improved training process we are able to significantly boost the certified accuracy of $\\ell_\\infty$-distance nets. Our training approach largely alleviates the optimization problem that arose in the previous training scheme, in particular, the unexpected large Lipschitz constant due to the use of a crucial trick called \\textit{$\\ell_p$-relaxation}. The core of our training approach is a novel objective function that combines scaled cross-entropy loss and clipped hinge loss with a decaying mixing coefficient. Experiments show that using the proposed training strategy, the certified accuracy of $\\ell_\\infty$-distance net can be dramatically improved from 33.30% to 40.06% on CIFAR-10 ($\\epsilon=8/255$), meanwhile outperforming other approaches in this area by a large margin. Our results clearly demonstrate the effectiveness and potential of $\\ell_\\infty$-distance net for certified robustness. Codes are available at https://github.com/zbh2047/L_inf-dist-net-v2",
    "checked": true,
    "id": "bef771d2430af7be7525201bd677e82cec38510f",
    "semantic_title": "boosting the certified robustness of l-infinity distance nets",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=ZBESeIUB5k": {
    "title": "Stochastic Training is Not Necessary for Generalization",
    "volume": "poster",
    "abstract": "It is widely believed that the implicit regularization of SGD is fundamental to the impressive generalization behavior we observe in neural networks. In this work, we demonstrate that non-stochastic full-batch training can achieve comparably strong performance to SGD on CIFAR-10 using modern architectures. To this end, we show that the implicit regularization of SGD can be completely replaced with explicit regularization. Our observations indicate that the perceived difficulty of full-batch training may be the result of its optimization properties and the disproportionate time and effort spent by the ML community tuning optimizers and hyperparameters for small-batch training",
    "checked": true,
    "id": "60f9fa6d4f966c316353ee2753021c4587fa0273",
    "semantic_title": "stochastic training is not necessary for generalization",
    "citation_count": 78,
    "authors": []
  },
  "https://openreview.net/forum?id=7KdAoOsI81C": {
    "title": "Transfer RL across Observation Feature Spaces via Model-Based Regularization",
    "volume": "poster",
    "abstract": "In many reinforcement learning (RL) applications, the observation space is specified by human developers and restricted by physical realizations, and may thus be subject to dramatic changes over time (e.g. increased number of observable features). However, when the observation space changes, the previous policy will likely fail due to the mismatch of input features, and another policy must be trained from scratch, which is inefficient in terms of computation and sample complexity. Following theoretical insights, we propose a novel algorithm which extracts the latent-space dynamics in the source task, and transfers the dynamics model to the target task to use as a model-based regularizer. Our algorithm works for drastic changes of observation space (e.g. from vector-based observation to image-based observation), without any inter-task mapping or any prior knowledge of the target task. Empirical results show that our algorithm significantly improves the efficiency and stability of learning in the target task",
    "checked": true,
    "id": "38f0c0e8b9218a505ebb5435d1b859b28b9687f0",
    "semantic_title": "transfer rl across observation feature spaces via model-based regularization",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=kR1hC6j48Tp": {
    "title": "GATSBI: Generative Adversarial Training for Simulation-Based Inference",
    "volume": "poster",
    "abstract": "Simulation-based inference (SBI) refers to statistical inference on stochastic models for which we can generate samples, but not compute likelihoods. Like SBI algorithms, generative adversarial networks (GANs) do not require explicit likelihoods. We study the relationship between SBI and GANs, and introduce GATSBI, an adversarial approach to SBI. GATSBI reformulates the variational objective in an adversarial setting to learn implicit posterior distributions. Inference with GATSBI is amortised across observations, works in high-dimensional posterior spaces and supports implicit priors. We evaluate GATSBI on two common SBI benchmark problems and on two high-dimensional simulators. On a model for wave propagation on the surface of a shallow water body, we show that GATSBI can return well-calibrated posterior estimates even in high dimensions. On a model of camera optics, it infers a high-dimensional posterior given an implicit prior, and performs better than a state-of-the-art SBI approach. We also show how GATSBI can be extended to perform sequential posterior estimation to focus on individual observations. Overall, GATSBI opens up opportunities for leveraging advances in GANs to perform Bayesian inference on high-dimensional simulation-based models",
    "checked": true,
    "id": "9557af7426fde227e1cbd623dd4d221122265863",
    "semantic_title": "gatsbi: generative adversarial training for simulation-based inference",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=AwgtcUAhBq": {
    "title": "Domain Adversarial Training: A Game Perspective",
    "volume": "poster",
    "abstract": "The dominant line of work in domain adaptation has focused on learning invariant representations using domain-adversarial training. In this paper, we interpret this approach from a game theoretical perspective. Defining optimal solutions in domain-adversarial training as a local Nash equilibrium, we show that gradient descent in domain-adversarial training can violate the asymptotic convergence guarantees of the optimizer, oftentimes hindering the transfer performance. Our analysis leads us to replace gradient descent with high-order ODE solvers (i.e., Runge‚ÄìKutta), for which we derive asymptotic convergence guarantees. This family of optimizers is significantly more stable and allows more aggressive learning rates, leading to high performance gains when used as a drop-in replacement over standard optimizers. Our experiments show that in conjunction with state-of-the-art domain-adversarial methods, we achieve up to 3.5% improvement with less than of half training iterations. Our optimizers are easy to implement, free of additional parameters, and can be plugged into any domain-adversarial framework",
    "checked": true,
    "id": "ffeaa70ae5e1280bafb29cc2d9fe80aae340eb70",
    "semantic_title": "domain adversarial training: a game perspective",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=MXdFBmHT4C": {
    "title": "Differentiable Expectation-Maximization for Set Representation Learning",
    "volume": "poster",
    "abstract": "We tackle the set2vec problem, the task of extracting a vector representation from an input set comprised of a variable number of feature vectors. Although recent approaches based on self attention such as (Set)Transformers were very successful due to the capability of capturing complex interaction between set elements, the computational overhead is the well-known downside. The inducing-point attention and the latest optimal transport kernel embedding (OTKE) are promising remedies that attain comparable or better performance with reduced computational cost, by incorporating a fixed number of learnable queries in attention. In this paper we approach the set2vec problem from a completely different perspective. The elements of an input set are considered as i.i.d.~samples from a mixture distribution, and we define our set embedding feed-forward network as the maximum-a-posterior (MAP) estimate of the mixture which is approximately attained by a few Expectation-Maximization (EM) steps. The whole MAP-EM steps are differentiable operations with a fixed number of mixture parameters, allowing efficient auto-diff back-propagation for any given downstream task. Furthermore, the proposed mixture set data fitting framework allows unsupervised set representation learning naturally via marginal likelihood maximization aka the empirical Bayes. Interestingly, we also find that OTKE can be seen as a special case of our framework, specifically a single-step EM with extra balanced assignment constraints on the E-step. Compared to OTKE, our approach provides more flexible set embedding as well as prior-induced model regularization. We evaluate our approach on various tasks demonstrating improved performance over the state-of-the-arts",
    "checked": true,
    "id": "854141a1ba72fec0f531dc45c21bed64e7ef1f7a",
    "semantic_title": "differentiable expectation-maximization for set representation learning",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=vIC-xLFuM6": {
    "title": "Overcoming The Spectral Bias of Neural Value Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f57ad283f8d55c3f49d36dcc4673243ec352f6c7",
    "semantic_title": "overcoming the spectral bias of neural value approximation",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=AIgn9uwfcD1": {
    "title": "Prospect Pruning: Finding Trainable Weights at Initialization using Meta-Gradients",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "be1210aa1ddbe7d6a654045a5aabbdc2a4827e6f",
    "semantic_title": "prospect pruning: finding trainable weights at initialization using meta-gradients",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=PVJ6j87gOHz": {
    "title": "CoMPS: Continual Meta Policy Search",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b64b3880198289fca95e54a001da3dd336502d7a",
    "semantic_title": "comps: continual meta policy search",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=ziRLU3Y2PN_": {
    "title": "Generalized rectifier wavelet covariance models for texture synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0c13593e5024f2d84d089c0f0bec03f6f5d35d20",
    "semantic_title": "generalized rectifier wavelet covariance models for texture synthesis",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=_5js_8uTrx1": {
    "title": "Towards Evaluating the Robustness of Neural Networks Learned by Transduction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "90fe9785e1ff58c1d7aa22319009e0d0e3077d29",
    "semantic_title": "towards evaluating the robustness of neural networks learned by transduction",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=oJGDYQFKL3i": {
    "title": "OBJECT DYNAMICS DISTILLATION FOR SCENE DECOMPOSITION AND REPRESENTATION",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "88022cc82844753ad95841ed23d18f0995ad198c",
    "semantic_title": "object dynamics distillation for scene decomposition and representation",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=NlObxR0rosG": {
    "title": "Practical Integration via Separable Bijective Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9ae51d683e131995ed684cfb9122dafbbce36b50",
    "semantic_title": "practical integration via separable bijective networks",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=zuqcmNVK4c2": {
    "title": "Self-Joint Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0b8313c5be24f248a42433b7a49db7d94fc98508",
    "semantic_title": "self-joint supervised learning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Jjcv9MTqhcq": {
    "title": "Rethinking Supervised Pre-Training for Better Downstream Transferring",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "05700beec6388a11ff72cbe996c9f7634a8f8f2d",
    "semantic_title": "rethinking supervised pre-training for better downstream transferring",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=OUz_9TiTv9j": {
    "title": "A Zest of LIME: Towards Architecture-Independent Model Distances",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "74843350d925d60f68aaaa7ac109dccec2fd9ae7",
    "semantic_title": "a zest of lime: towards architecture-independent model distances",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=KTPuIsx4pmo": {
    "title": "Meta-Imitation Learning by Watching Video Demonstrations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e258ff722e473ee9c6800369d89ef9528e7db839",
    "semantic_title": "meta-imitation learning by watching video demonstrations",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=6ET9SzlgNX": {
    "title": "Understanding Intrinsic Robustness Using Label Uncertainty",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1d05745fddf59153931976d07468e710b6ef3939",
    "semantic_title": "understanding intrinsic robustness using label uncertainty",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=_QLmakITKg": {
    "title": "Efficient Split-Mix Federated Learning for On-Demand and In-Situ Customization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c91d39810492e585809658bb9eec065979b9214b",
    "semantic_title": "efficient split-mix federated learning for on-demand and in-situ customization",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=RXQ-FPbQYVn": {
    "title": "Anti-Concentrated Confidence Bonuses For Scalable Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c4d91bca6066282da421671ab06ac1e156f19851",
    "semantic_title": "anti-concentrated confidence bonuses for scalable exploration",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=5-2mX9_U5i": {
    "title": "Sqrt(d) Dimension Dependence of Langevin Monte Carlo",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f71004edb4c3c26874837b55108e82e1b1a92818",
    "semantic_title": "sqrt(d) dimension dependence of langevin monte carlo",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=dZPgfwaTaXv": {
    "title": "Relational Surrogate Loss Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "96f79556c4a6f11f88ede749f2e379cc97d237c2",
    "semantic_title": "relational surrogate loss learning",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=fy_XRVHqly": {
    "title": "Structure-Aware Transformer Policy for Inhomogeneous Multi-Task Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0cabfee41564ec6eefdfcccc938502d4a7813e53",
    "semantic_title": "structure-aware transformer policy for inhomogeneous multi-task reinforcement learning",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=3HJOA-1hb0e": {
    "title": "Toward Efficient Low-Precision Training: Data Format Optimization and Hysteresis Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "83132bd8b07bf383e15654e693115d41eaa21c6c",
    "semantic_title": "toward efficient low-precision training: data format optimization and hysteresis quantization",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=upnDJ7itech": {
    "title": "Knowledge Infused Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "706c6b3781374b0b11f98f204a4ddd05b26ed009",
    "semantic_title": "knowledge infused decoding",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=N1WI0vJLER": {
    "title": "Parallel Training of GRU Networks with a Multi-Grid Solver for Long Sequences",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0c7fa4d8935a05afadebe0a2daf21ef1d91daf49",
    "semantic_title": "parallel training of gru networks with a multi-grid solver for long sequences",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=73MEhZ0anV": {
    "title": "QUERY EFFICIENT DECISION BASED SPARSE ATTACKS AGAINST BLACK-BOX DEEP LEARNING MODELS",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7a752d2aa7e93cc8c3cafebbd9eae5c0f7b5210e",
    "semantic_title": "query efficient decision based sparse attacks against black-box deep learning models",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=gJLEXy3ySpu": {
    "title": "Almost Tight L0-norm Certified Robustness of Top-k Predictions against Adversarial Perturbations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0808bdbb02e60fd3ac3c13f454346ea47067b987",
    "semantic_title": "almost tight l0-norm certified robustness of top-k predictions against adversarial perturbations",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=Vjki79-619-": {
    "title": "Proving the Lottery Ticket Hypothesis for Convolutional Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "af55b591775c43b987f23cd541cb059ea89e7f5c",
    "semantic_title": "proving the lottery ticket hypothesis for convolutional neural networks",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=Vog_3GXsgmb": {
    "title": "Discovering Nonlinear PDEs from Scarce Data with Physics-encoded Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2deebc5b3134bea437aaef6ce329fef127c5a4f2",
    "semantic_title": "discovering nonlinear pdes from scarce data with physics-encoded learning",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=KJztlfGPdwW": {
    "title": "Rethinking Goal-Conditioned Supervised Learning and Its Connection to Offline RL",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7f712d58084e32ddc1b0cd60932f8bc0a0916330",
    "semantic_title": "rethinking goal-conditioned supervised learning and its connection to offline rl",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=P1QUVhOtEFP": {
    "title": "Topologically Regularized Data Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7cf25d2b572acd3734c0439eb4e12e1502075b41",
    "semantic_title": "topologically regularized data embeddings",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=oh4TirnfSem": {
    "title": "PF-GNN: Differentiable particle filtering based approximation of universal graph representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "696fc28d71cf4d681568d89646cce12f4d1eea50",
    "semantic_title": "pf-gnn: differentiable particle filtering based approximation of universal graph representations",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=AMpki9kp8Cn": {
    "title": "Nonlinear ICA Using Volume-Preserving Transformations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "adbfd52a9d4a091036b37725d077b38301bd43d8",
    "semantic_title": "nonlinear ica using volume-preserving transformations",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=18Ys0-PzyPI": {
    "title": "Online Ad Hoc Teamwork under Partial Observability",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "967d246f296166eaad859520eb3bf918fbc3a275",
    "semantic_title": "online ad hoc teamwork under partial observability",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=vwLLQ-HwqhZ": {
    "title": "Continual Normalization: Rethinking Batch Normalization for Online Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f0373087349d3393e42cd15c6aa01e6f320d4341",
    "semantic_title": "continual normalization: rethinking batch normalization for online continual learning",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=SHbhHHfePhP": {
    "title": "Equivariant Graph Mechanics Networks with Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1407ea6d9ada333e9fc8de9416b203d58e9d9282",
    "semantic_title": "equivariant graph mechanics networks with constraints",
    "citation_count": 89,
    "authors": []
  },
  "https://openreview.net/forum?id=vfsRB5MImo9": {
    "title": "Towards Continual Knowledge Learning of Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ce828f9986b196308a3e40b1de58af1e8e68d728",
    "semantic_title": "towards continual knowledge learning of language models",
    "citation_count": 167,
    "authors": []
  },
  "https://openreview.net/forum?id=nf3A0WZsXS5": {
    "title": "Surreal-GAN:Semi-Supervised Representation Learning via GAN for uncovering heterogeneous disease-related imaging patterns",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "30a0618406ef01b53fc53fa755eb2e3d4b800bbf",
    "semantic_title": "surreal-gan: semi-supervised representation learning via gan for uncovering heterogeneous disease-related imaging patterns",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=TfhfZLQ2EJO": {
    "title": "SURF: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f2a2401a35b6b892d43642b31700e83e88b2ebb8",
    "semantic_title": "surf: semi-supervised reward learning with data augmentation for feedback-efficient preference-based reinforcement learning",
    "citation_count": 98,
    "authors": []
  },
  "https://openreview.net/forum?id=ItkxLQU01lD": {
    "title": "Convergent Graph Solvers",
    "volume": "poster",
    "abstract": "We propose the convergent graph solver (CGS), a deep learning method that learns iterative mappings to predict the properties of a graph system at its stationary state (fixed point) with guaranteed convergence. The forward propagation of CGS proceeds in three steps: (1) constructing the input-dependent linear contracting iterative maps, (2) computing the fixed points of the iterative maps, and (3) decoding the fixed points to estimate the properties. The contractivity of the constructed linear maps guarantees the existence and uniqueness of the fixed points following the Banach fixed point theorem. To train CGS efficiently, we also derive a tractable analytical expression for its gradient by leveraging the implicit function theorem. We evaluate the performance of CGS by applying it to various network-analytic and graph benchmark problems. The results indicate that CGS has competitive capabilities for predicting the stationary properties of graph systems, irrespective of whether the target systems are linear or non-linear. CGS also shows high performance for graph classification problems where the existence or the meaning of a fixed point is hard to be clearly defined, which highlights the potential of CGS as a general graph neural network architecture",
    "checked": true,
    "id": "7a35a6061ac036dad9ae03c3b1a27435630611bf",
    "semantic_title": "convergent graph solvers",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=_F9xpOrqyX9": {
    "title": "Spread Spurious Attribute: Improving Worst-group Accuracy with Spurious Attribute Estimation",
    "volume": "poster",
    "abstract": "The paradigm of worst-group loss minimization has shown its promise in avoiding to learn spurious correlations, but requires costly additional supervision on spurious attributes. To resolve this, recent works focus on developing weaker forms of supervision---e.g., hyperparameters discovered with a small number of validation samples with spurious attribute annotation---but none of the methods retain comparable performance to methods using full supervision on the spurious attribute. In this paper, instead of searching for weaker supervisions, we ask: Given access to a fixed number of samples with spurious attribute annotations, what is the best achievable worst-group loss if we ''fully exploit'' them? To this end, we propose a pseudo-attribute-based algorithm, coined Spread Spurious Attribute (SSA), for improving the worst-group accuracy. In particular, we leverage samples both with and without spurious attribute annotations to train a model to predict the spurious attribute, then use the pseudo-attribute predicted by the trained model as supervision on the spurious attribute to train a new robust model having minimal worst-group loss. Our experiments on various benchmark datasets show that our algorithm consistently outperforms the baseline methods using the same number of validation samples with spurious attribute annotations. We also demonstrate that the proposed SSA can achieve comparable performances to methods using full (100%) spurious attribute supervision, by using a much smaller number of annotated samples---from 0.6% and up to 1.5%, depending on the dataset",
    "checked": true,
    "id": "d398aae4520ab684b87287b831fee244d5474e99",
    "semantic_title": "spread spurious attribute: improving worst-group accuracy with spurious attribute estimation",
    "citation_count": 97,
    "authors": []
  },
  "https://openreview.net/forum?id=06Wy2BtxXrz": {
    "title": "Learning Scenario Representation for Solving Two-stage Stochastic Integer Programs",
    "volume": "poster",
    "abstract": "Many practical combinatorial optimization problems under uncertainty can be modeled as stochastic integer programs (SIPs), which are extremely challenging to solve due to the high complexity. To solve two-stage SIPs efficiently, we propose a conditional variational autoencoder (CVAE) based method to learn scenario representation for a class of SIP instances. Specifically, we design a graph convolutional network based encoder to embed each scenario with the deterministic part of its instance (i.e. context) into a low-dimensional latent space, from which a decoder reconstructs the scenario from its latent representation conditioned on the context. Such a design effectively captures the dependencies of the scenarios on their corresponding instances. We apply the trained encoder to two tasks in typical SIP solving, i.e. scenario reduction and objective prediction. Experiments on two SIP problems show that the learned latent representation significantly boosts the solving performance to attain high-quality solutions in short computational time, and generalizes fairly well to problems of larger sizes or with more scenarios",
    "checked": true,
    "id": "e5a4ca43024028b1916baeeaa67797a0d3cc7d01",
    "semantic_title": "learning scenario representation for solving two-stage stochastic integer programs",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=7grkzyj89A_": {
    "title": "Generalization Through the Lens of Leave-One-Out Error",
    "volume": "poster",
    "abstract": "Despite the tremendous empirical success of deep learning models to solve various learning tasks, our theoretical understanding of their generalization ability is very limited. Classical generalization bounds based on tools such as the VC dimension or Rademacher complexity, are so far unsuitable for deep models and it is doubtful that these techniques can yield tight bounds even in the most idealistic settings~\\citep{nagarajan2019uniform}. In this work, we instead revisit the concept of leave-one-out (LOO) error to measure the generalization ability of deep models in the so-called kernel regime. While popular in statistics, the LOO error has been largely overlooked in the context of deep learning. By building upon the recently established connection between neural networks and kernel learning, we leverage the closed-form expression for the leave-one-out error, giving us access to an efficient proxy for the test error. We show both theoretically and empirically that the leave-one-out error is capable of capturing various phenomena in generalization theory, such as double descent, random labels or transfer learning. Our work therefore demonstrates that the leave-one-out error provides a tractable way to estimate the generalization ability of deep neural networks in the kernel regime, opening the door to potential, new research directions in the field of generalization",
    "checked": true,
    "id": "83a49f7f765335ab7897e95f2fde4b6a42893eb7",
    "semantic_title": "generalization through the lens of leave-one-out error",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=VPjw9KPWRSK": {
    "title": "Self-Supervised Inference in State-Space Models",
    "volume": "poster",
    "abstract": "We perform approximate inference in state-space models with nonlinear state transitions. Without parameterizing a generative model, we apply Bayesian update formulas using a local linearity approximation parameterized by neural networks. It comes accompanied by a maximum likelihood objective that requires no supervision via uncorrupt observations or ground truth latent states. The optimization backpropagates through a recursion similar to the classical Kalman filter and smoother. Additionally, using an approximate conditional independence, we can perform smoothing without having to parameterize a separate model. In scientific applications, domain knowledge can give a linear approximation of the latent transition maps, which we can easily incorporate into our model. Usage of such domain knowledge is reflected in excellent results (despite our model's simplicity) on the chaotic Lorenz system compared to fully supervised and variational inference methods. Finally, we show competitive results on an audio denoising experiment",
    "checked": true,
    "id": "2e7716c31962c629e0064a7c387d2cfd9792e467",
    "semantic_title": "self-supervised inference in state-space models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=SwIp410B6aQ": {
    "title": "On the Role of Neural Collapse in Transfer Learning",
    "volume": "poster",
    "abstract": "We study the ability of foundation models to learn representations for classification that are transferable to new, unseen classes. Recent results in the literature show that representations learned by a single classifier over many classes are competitive on few-shot learning problems with representations learned by special-purpose algorithms designed for such problems. In this paper, we provide an explanation for this behavior based on the recently observed phenomenon that the features learned by overparameterized classification networks show an interesting clustering property, called neural collapse. We demonstrate both theoretically and empirically that neural collapse generalizes to new samples from the training classes, and -- more importantly -- to new classes as well, allowing foundation models to provide feature maps that work well in transfer learning and, specifically, in the few-shot setting",
    "checked": true,
    "id": "8f0d609618838b20631469ffa7fc78928bba8ca0",
    "semantic_title": "on the role of neural collapse in transfer learning",
    "citation_count": 99,
    "authors": []
  },
  "https://openreview.net/forum?id=IpctgL7khPp": {
    "title": "Information-theoretic Online Memory Selection for Continual Learning",
    "volume": "poster",
    "abstract": "A challenging problem in task-free continual learning is the online selection of a representative replay memory from data streams. In this work, we investigate the online memory selection problem from an information-theoretic perspective. To gather the most information, we propose the \\textit{surprise} and the \\textit{learnability} criteria to pick informative points and to avoid outliers. We present a Bayesian model to compute the criteria efficiently by exploiting rank-one matrix structures. We demonstrate that these criteria encourage selecting informative points in a greedy algorithm for online memory selection. Furthermore, by identifying the importance of \\textit{the timing to update the memory}, we introduce a stochastic information-theoretic reservoir sampler (InfoRS), which conducts sampling among selective points with high information. Compared to reservoir sampling, InfoRS demonstrates improved robustness against data imbalance. Finally, empirical performances over continual learning benchmarks manifest its efficiency and efficacy",
    "checked": true,
    "id": "e8f9b92f802dffb64084e9920fd431ba861b5af1",
    "semantic_title": "information-theoretic online memory selection for continual learning",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=XHUxf5aRB3s": {
    "title": "Dealing with Non-Stationarity in MARL via Trust-Region Decomposition",
    "volume": "poster",
    "abstract": "Non-stationarity is one thorny issue in cooperative multi-agent reinforcement learning (MARL). One of the reasons is the policy changes of agents during the learning process. Some existing works have discussed various consequences caused by non-stationarity with several kinds of measurement indicators. This makes the objectives or goals of existing algorithms are inevitably inconsistent and disparate. In this paper, we introduce a novel notion, the $\\delta$-$stationarity$ measurement, to explicitly measure the non-stationarity of a policy sequence, which can be further proved to be bounded by the KL-divergence of consecutive joint policies. A straightforward but highly non-trivial way is to control the joint policies' divergence, which is difficult to estimate accurately by imposing the trust-region constraint on the joint policy. Although it has lower computational complexity to decompose the joint policy and impose trust-region constraints on the factorized policies, simple policy factorization like mean-field approximation will lead to more considerable policy divergence, which can be considered as the trust-region decomposition dilemma. We model the joint policy as a pairwise Markov random field and propose a trust-region decomposition network (TRD-Net) based on message passing to estimate the joint policy divergence more accurately. The Multi-Agent Mirror descent policy algorithm with Trust region decomposition, called MAMT, is established by adjusting the trust-region of the local policies adaptively in an end-to-end manner. MAMT can approximately constrain the consecutive joint policies' divergence to satisfy $\\delta$-stationarity and alleviate the non-stationarity problem. Our method can bring noticeable and stable performance improvement compared with baselines in cooperative tasks of different complexity",
    "checked": true,
    "id": "79eadadbb5251f74b017de36346833733a6896ce",
    "semantic_title": "dealing with non-stationarity in marl via trust-region decomposition",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=kF9DZQQrU0w": {
    "title": "Information Bottleneck: Exact Analysis of (Quantized) Neural Networks",
    "volume": "poster",
    "abstract": "The information bottleneck (IB) principle has been suggested as a way to analyze deep neural networks. The learning dynamics are studied by inspecting the mutual information (MI) between the hidden layers and the input and output. Notably, separate fitting and compression phases during training have been reported. This led to some controversy including claims that the observations are not reproducible and strongly dependent on the type of activation function used as well as on the way the MI is estimated. Our study confirms that different ways of binning when computing the MI lead to qualitatively different results, either supporting or refusing IB conjectures. To resolve the controversy, we study the IB principle in settings where MI is non-trivial and can be computed exactly. We monitor the dynamics of quantized neural networks, that is, we discretize the whole deep learning system so that no approximation is required when computing the MI. This allows us to quantify the information flow without measurement errors. In this setting, we observed a fitting phase for all layers and a compression phase for the output layer in all experiments; the compression in the hidden layers was dependent on the type of activation function. Our study shows that the initial IB results were not artifacts of binning when computing the MI. However, the critical claim that the compression phase may not be observed for some networks also holds true",
    "checked": true,
    "id": "0436bade813530e3f9f1441d8881a2d14c27ef9e",
    "semantic_title": "information bottleneck: exact analysis of (quantized) neural networks",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=XLxhEjKNbXj": {
    "title": "GLASS: GNN with Labeling Tricks for Subgraph Representation Learning",
    "volume": "poster",
    "abstract": "Despite the remarkable achievements of Graph Neural Networks (GNNs) on graph representation learning, few works have tried to use them to predict properties of subgraphs in the whole graph. The existing state-of-the-art method SubGNN introduces an overly complicated subgraph-level GNN model which synthesizes three artificial channels each of which has two carefully designed subgraph-level message passing modules, yet only slightly outperforms a plain GNN which performs node-level message passing and then pools node embeddings within the subgraph. By analyzing SubGNN and plain GNNs, we find that the key for subgraph representation learning might be to distinguish nodes inside and outside the subgraph. With this insight, we propose an expressive and scalable labeling trick, namely max-zero-one, to enhance plain GNNs for subgraph tasks. The resulting model is called GLASS (GNN with LAbeling trickS for Subgraph). We theoretically characterize GLASS's expressive power. Compared with SubGNN, GLASS is more expressive, more scalable, and easier to implement. Experiments on eight benchmark datasets show that GLASS outperforms the strongest baseline by $14.8\\%$ on average. And ablation analysis shows that our max-zero-one labeling trick can boost the performance of a plain GNN by up to $105\\%$ in maximum, which illustrates the effectiveness of labeling trick on subgraph tasks. Furthermore, training a GLASS model only takes $37\\%$ time needed for a SubGNN on average",
    "checked": true,
    "id": "90e76bbf2ff66de0e88fddefc0bcac176f86a3b5",
    "semantic_title": "glass: gnn with labeling tricks for subgraph representation learning",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=DnG75_KyHjX": {
    "title": "MoReL: Multi-omics Relational Learning",
    "volume": "poster",
    "abstract": "Multi-omics data analysis has the potential to discover hidden molecular interactions, revealing potential regulatory and/or signal transduction pathways for cellular processes of interest when studying life and disease systems. One of critical challenges when dealing with real-world multi-omics data is that they may manifest heterogeneous structures and data quality as often existing data may be collected from different subjects under different conditions for each type of omics data. We propose a novel deep Bayesian generative model to efficiently infer a multi-partite graph encoding molecular interactions across such heterogeneous views, using a fused Gromov-Wasserstein (FGW) regularization between latent representations of corresponding views for integrative analysis. With such an optimal transport regularization in the deep Bayesian generative model, it not only allows incorporating view-specific side information, either with graph-structured or unstructured data in different views, but also increases the model flexibility with the distribution-based regularization. This allows efficient alignment of heterogeneous latent variable distributions to derive reliable interaction predictions compared to the existing point-based graph embedding methods. Our experiments on several real-world datasets demonstrate enhanced performance of MoReL in inferring meaningful interactions compared to existing baselines",
    "checked": true,
    "id": "7e6de103891ff995967de5f8e40a4e948a566d41",
    "semantic_title": "morel: multi-omics relational learning",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=BwPaPxwgyQb": {
    "title": "Provable Learning-based Algorithm For Sparse Recovery",
    "volume": "poster",
    "abstract": "Recovering sparse parameters from observational data is a fundamental problem in machine learning with wide applications. Many classic algorithms can solve this problem with theoretical guarantees, but their performances rely on choosing the correct hyperparameters. Besides, hand-designed algorithms do not fully exploit the particular problem distribution of interest. In this work, we propose a deep learning method for algorithm learning called PLISA (Provable Learning-based Iterative Sparse recovery Algorithm). PLISA is designed by unrolling a classic path-following algorithm for sparse recovery, with some components being more flexible and learnable. We theoretically show the improved recovery accuracy achievable by PLISA. Furthermore, we analyze the empirical Rademacher complexity of PLISA to characterize its generalization ability to solve new problems outside the training set. This paper contains novel theoretical contributions to the area of learning-based algorithms in the sense that (i) PLISA is generically applicable to a broad class of sparse estimation problems, (ii) generalization analysis has received less attention so far, and (iii) our analysis makes novel connections between the generalization ability and algorithmic properties such as stability and convergence of the unrolled algorithm, which leads to a tighter bound that can explain the empirical observations. The techniques could potentially be applied to analyze other learning-based algorithms in the literature",
    "checked": true,
    "id": "0aa05b938282ebf4742777f3c03f1ac2a85e10cb",
    "semantic_title": "provable learning-based algorithm for sparse recovery",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jJOjjiZHy3h": {
    "title": "Defending Against Image Corruptions Through Adversarial Augmentations",
    "volume": "poster",
    "abstract": "Modern neural networks excel at image classification, yet they remain vulnerable to common image corruptions such as blur, speckle noise or fog. Recent methods that focus on this problem, such as AugMix and DeepAugment, introduce defenses that operate in expectation over a distribution of image corruptions. In contrast, the literature on Lp-norm bounded perturbations focuses on defenses against worst-case corruptions. In this work, we reconcile both approaches by proposing AdversarialAugment, a technique which optimizes the parameters of image-to-image models to generate adversarially corrupted augmented images. We theoretically motivate our method and give sufficient conditions for the consistency of its idealized version as well as that of DeepAugment. Our classifiers improve upon the state-of-the-art on common image corruption benchmarks conducted in expectation on CIFAR-10-C and improve worst-case performance against Lp-norm bounded perturbations on both CIFAR-10 and ImageNet",
    "checked": true,
    "id": "285018adc33d8b2735dc2bb918f9ef8bae36ba25",
    "semantic_title": "defending against image corruptions through adversarial augmentations",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=Zf4ZdI4OQPV": {
    "title": "Attacking deep networks with surrogate-based adversarial black-box methods is easy",
    "volume": "poster",
    "abstract": "A recent line of work on black-box adversarial attacks has revived the use of transfer from surrogate models by integrating it into query-based search. However, we find that existing approaches of this type underperform their potential, and can be overly complicated besides. Here, we provide a short and simple algorithm which achieves state-of-the-art results through a search which uses the surrogate network's class-score gradients, with no need for other priors or heuristics. The guiding assumption of the algorithm is that the studied networks are in a fundamental sense learning similar functions, and that a transfer attack from one to the other should thus be fairly \"easy\". This assumption is validated by the extremely low query counts and failure rates achieved: e.g. an untargeted attack on a VGG-16 ImageNet network using a ResNet-152 as the surrogate yields a median query count of 6 at a success rate of 99.9%. Code is available at https://github.com/fiveai/GFCS",
    "checked": true,
    "id": "7d3675344a3ae1b639cce4208779f914c3a9a68d",
    "semantic_title": "attacking deep networks with surrogate-based adversarial black-box methods is easy",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=Lm8T39vLDTE": {
    "title": "Autoregressive Diffusion Models",
    "volume": "poster",
    "abstract": "We introduce Autoregressive Diffusion Models (ARDMs), a model class encompassing and generalizing order-agnostic autoregressive models (Uria et al., 2014) and absorbing discrete diffusion (Austin et al., 2021), which we show are special cases of ARDMs under mild assumptions. ARDMs are simple to implement and easy to train. Unlike standard ARMs, they do not require causal masking of model representations, and can be trained using an efficient objective similar to modern probabilistic diffusion models that scales favourably to highly-dimensional data. At test time, ARDMs support parallel generation which can be adapted to fit any given generation budget. We find that ARDMs require significantly fewer steps than discrete diffusion models to attain the same performance. Finally, we apply ARDMs to lossless compression, and show that they are uniquely suited to this task. Contrary to existing approaches based on bits-back coding, ARDMs obtain compelling results not only on complete datasets, but also on compressing single data points. Moreover, this can be done using a modest number of network calls for (de)compression due to the model's adaptable parallel generation",
    "checked": true,
    "id": "599bc7cfe98c2b57ddbe111412203a636da57be0",
    "semantic_title": "autoregressive diffusion models",
    "citation_count": 172,
    "authors": []
  },
  "https://openreview.net/forum?id=H94a1_Pyr-6": {
    "title": "Auto-scaling Vision Transformers without Training",
    "volume": "poster",
    "abstract": "This work targets automated designing and scaling of Vision Transformers (ViTs). The motivation comes from two pain spots: 1) the lack of efficient and principled methods for designing and scaling ViTs; 2) the tremendous computational cost of training ViT that is much heavier than its convolution counterpart. To tackle these issues, we propose As-ViT, an auto-scaling framework for ViTs without training, which automatically discovers and scales up ViTs in an efficient and principled manner. Specifically, we first design a \"seed\" ViT topology by leveraging a training-free search process. This extremely fast search is fulfilled by a comprehensive study of ViT's network complexity, yielding a strong Kendall-tau correlation with ground-truth accuracies. Second, starting from the \"seed\" topology, we automate the scaling rule for ViTs by growing widths/depths to different ViT layers. This results in a series of architectures with different numbers of parameters in a single run. Finally, based on the observation that ViTs can tolerate coarse tokenization in early training stages, we propose a progressive tokenization strategy to train ViTs faster and cheaper. As a unified framework, As-ViT achieves strong performance on classification (83.5% top1 on ImageNet-1k) and detection (52.7% mAP on COCO) without any manual crafting nor scaling of ViT architectures: the end-to-end model design and scaling process costs only 12 hours on one V100 GPU. Our code is available at https://github.com/VITA-Group/AsViT",
    "checked": true,
    "id": "3c7f3b153c2b5b4074d95ac9d659a267a2bafa3f",
    "semantic_title": "auto-scaling vision transformers without training",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=KPEFXR1HdIo": {
    "title": "Fine-grained Differentiable Physics: A Yarn-level Model for Fabrics",
    "volume": "poster",
    "abstract": "Differentiable physics modeling combines physics models with gradient-based learning to provide model explicability and data efficiency. It has been used to learn dynamics, solve inverse problems and facilitate design, and is at its inception of impact. Current successes have concentrated on general physics models such as rigid bodies, deformable sheets, etc, assuming relatively simple structures and forces. Their granularity is intrinsically coarse and therefore incapable of modelling complex physical phenomena. Fine-grained models are still to be developed to incorporate sophisticated material structures and force interactions with gradient-based learning. Following this motivation, we propose a new differentiable fabrics model for composite materials such as cloths, where we dive into the granularity of yarns and model individual yarn physics and yarn-to-yarn interactions. To this end, we propose several differentiable forces, whose counterparts in empirical physics are indifferentiable, to facilitate gradient-based learning. These forces, albeit applied to cloths, are ubiquitous in various physical systems. Through comprehensive evaluation and comparison, we demonstrate our model's $\\textit{explicability}$ in learning meaningful physical parameters, $\\textit{versatility}$ in incorporating complex physical structures and heterogeneous materials, $\\textit{data-efficiency}$ in learning, and $\\textit{high-fidelity}$ in capturing subtle dynamics",
    "checked": true,
    "id": "8f32df9aa05cd0b1cd5c1b95508e85003f0e8bd6",
    "semantic_title": "fine-grained differentiable physics: a yarn-level model for fabrics",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=6y2KBh-0Fd9": {
    "title": "Revisiting flow generative models for Out-of-distribution detection",
    "volume": "poster",
    "abstract": "Deep generative models have been widely used in practical applications such as the detection of out-of-distribution (OOD) data. In this work, we aim to re-examine the potential of generative flow models in OOD detection. We first propose a simple combination of univariate one-sample statistical test (e.g., Kolmogorov-Smirnov) and random projections in the latent space of flow models to perform OOD detection. Then, we propose a two-sample version of our test to account for imperfect flow models. Quite distinctly, our method does not pose parametric assumptions on OOD data and is capable of exploiting any flow model. Experimentally, firstly we confirm the efficacy of our method against state-of-the-art baselines through extensive experiments on several image datasets; secondly we investigate the relationship between model accuracy (e.g., the generation quality) and the OOD detection performance, and found surprisingly that they are not always positively correlated; and thirdly we show that detection in the latent space of flow models generally outperforms detection in the sample space across various OOD datasets, hence highlighting the benefits of training a flow model",
    "checked": true,
    "id": "31278cec03fa87b45a02f43d275dd92b678fbc5b",
    "semantic_title": "revisiting flow generative models for out-of-distribution detection",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=Te5ytkqsnl": {
    "title": "Missingness Bias in Model Debugging",
    "volume": "poster",
    "abstract": "Missingness, or the absence of features from an input, is a concept fundamental to many model debugging tools. However, in computer vision, pixels cannot simply be removed from an image. One thus tends to resort to heuristics such as blacking out pixels, which may in turn introduce bias into the debugging process. We study such biases and, in particular, show how transformer-based architectures can enable a more natural implementation of missingness, which side-steps these issues and improves the reliability of model debugging in practice",
    "checked": true,
    "id": "fe8584c04819c38e62982c3de935900bef3fa9a0",
    "semantic_title": "missingness bias in model debugging",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=GQd7mXSPua": {
    "title": "Meta Learning Low Rank Covariance Factors for Energy Based Deterministic Uncertainty",
    "volume": "poster",
    "abstract": "Numerous recent works utilize bi-Lipschitz regularization of neural network layers to preserve relative distances between data instances in the feature spaces of each layer. This distance sensitivity with respect to the data aids in tasks such as uncertainty calibration and out-of-distribution (OOD) detection. In previous works, features extracted with a distance sensitive model are used to construct feature covariance matrices which are used in deterministic uncertainty estimation or OOD detection. However, in cases where there is a distribution over tasks, these methods result in covariances which are sub-optimal, as they may not leverage all of the meta information which can be shared among tasks. With the use of an attentive set encoder, we propose to meta learn either diagonal or diagonal plus low-rank factors to efficiently construct task specific covariance matrices. Additionally, we propose an inference procedure which utilizes scaled energy to achieve a final predictive distribution which is well calibrated under a distributional dataset shift",
    "checked": true,
    "id": "5e0d45d3a07a4330511fbb7f5bc06ee5ba7a1e0b",
    "semantic_title": "meta learning low rank covariance factors for energy-based deterministic uncertainty",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=aD7uesX1GF_": {
    "title": "Conditional Object-Centric Learning from Video",
    "volume": "poster",
    "abstract": "Object-centric representations are a promising path toward more systematic generalization by providing flexible abstractions upon which compositional world models can be built. Recent work on simple 2D and 3D datasets has shown that models with object-centric inductive biases can learn to segment and represent meaningful objects from the statistical structure of the data alone without the need for any supervision. However, such fully-unsupervised methods still fail to scale to diverse realistic data, despite the use of increasingly complex inductive biases such as priors for the size of objects or the 3D geometry of the scene. In this paper, we instead take a weakly-supervised approach and focus on how 1) using the temporal dynamics of video data in the form of optical flow and 2) conditioning the model on simple object location cues can be used to enable segmenting and tracking objects in significantly more realistic synthetic data. We introduce a sequential extension to Slot Attention which we train to predict optical flow for realistic looking synthetic scenes and show that conditioning the initial state of this model on a small set of hints, such as center of mass of objects in the first frame, is sufficient to significantly improve instance segmentation. These benefits generalize beyond the training distribution to novel objects, novel backgrounds, and to longer video sequences. We also find that such initial-state-conditioning can be used during inference as a flexible interface to query the model for specific objects or parts of objects, which could pave the way for a range of weakly-supervised approaches and allow more effective interaction with trained models",
    "checked": true,
    "id": "95805eb7ab0edcd05128cf0256feaea8e2497de9",
    "semantic_title": "conditional object-centric learning from video",
    "citation_count": 240,
    "authors": []
  },
  "https://openreview.net/forum?id=f2OYVDyfIB": {
    "title": "Scale Efficiently: Insights from Pretraining and Finetuning Transformers",
    "volume": "poster",
    "abstract": "There remain many open questions pertaining to the scaling behaviour of Transformer architectures. These scaling decisions and findings can be critical, as training runs often come with an associated computational cost which have both financial and/or environmental impact. The goal of this paper is to present scaling insights from pretraining and finetuning Transformers. While Kaplan et al. presents a comprehensive study of the scaling behaviour of Transformer language models, the scope is only on the upstream (pretraining) loss. Therefore, it is still unclear if these set of findings transfer to downstream task within the context of the pretrain-finetune paradigm. The key findings of this paper are as follows: (1) we show that aside from only the model size, model shape matters for downstream fine-tuning, (2) scaling protocols operate differently at different compute regions, (3) widely adopted T5-base and T5-large sizes are Pareto-inefficient. To this end, we present improved scaling protocols whereby our redesigned models achieve similar downstream fine-tuning quality while having 50\\% fewer parameters and training 40\\% faster compared to the widely adopted T5-base model. We publicly release over 100 pretrained checkpoints of different T5 configurations to facilitate future research and analysis",
    "checked": true,
    "id": "070ff40f38675cfa42a104a545a47584ad823e70",
    "semantic_title": "scale efficiently: insights from pretraining and finetuning transformers",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=Ow1C7s3UcY": {
    "title": "Vitruvion: A Generative Model of Parametric CAD Sketches",
    "volume": "poster",
    "abstract": "Parametric computer-aided design (CAD) tools are the predominant way that engineers specify physical structures, from bicycle pedals to airplanes to printed circuit boards. The key characteristic of parametric CAD is that design intent is encoded not only via geometric primitives, but also by parameterized constraints between the elements. This relational specification can be viewed as the construction of a constraint program, allowing edits to coherently propagate to other parts of the design. Machine learning offers the intriguing possibility of accelerating the design process via generative modeling of these structures, enabling new tools such as autocompletion, constraint inference, and conditional synthesis. In this work, we present such an approach to generative modeling of parametric CAD sketches, which constitute the basic computational building blocks of modern mechanical design. Our model, trained on real-world designs from the SketchGraphs dataset, autoregressively synthesizes sketches as sequences of primitives, with initial coordinates, and constraints that reference back to the sampled primitives. As samples from the model match the constraint graph representation used in standard CAD software, they may be directly imported, solved, and edited according to downstream design tasks. In addition, we condition the model on various contexts, including partial sketches (primers) and images of hand-drawn sketches. Evaluation of the proposed approach demonstrates its ability to synthesize realistic CAD sketches and its potential to aid the mechanical design workflow",
    "checked": true,
    "id": "cf290baa3f1d69cc2f08cba891403d168c598d88",
    "semantic_title": "vitruvion: a generative model of parametric cad sketches",
    "citation_count": 71,
    "authors": []
  },
  "https://openreview.net/forum?id=XJiajt89Omg": {
    "title": "Space-Time Graph Neural Networks",
    "volume": "poster",
    "abstract": "We introduce space-time graph neural network (ST-GNN), a novel GNN architecture, tailored to jointly process the underlying space-time topology of time-varying network data. The cornerstone of our proposed architecture is the composition of time and graph convolutional filters followed by pointwise nonlinear activation functions. We introduce a generic definition of convolution operators that mimic the diffusion process of signals over its underlying support. On top of this definition, we propose space-time graph convolutions that are built upon a composition of time and graph shift operators. We prove that ST-GNNs with multivariate integral Lipschitz filters are stable to small perturbations in the underlying graphs as well as small perturbations in the time domain caused by time warping. Our analysis shows that small variations in the network topology and time evolution of a system does not significantly affect the performance of ST-GNNs. Numerical experiments with decentralized control systems showcase the effectiveness and stability of the proposed ST-GNNs",
    "checked": true,
    "id": "b059345775719f19fd970bd05d07914e623f08e6",
    "semantic_title": "space-time graph neural networks",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=bjy5Zb2fo2": {
    "title": "Scattering Networks on the Sphere for Scalable and Rotationally Equivariant Spherical CNNs",
    "volume": "poster",
    "abstract": "Convolutional neural networks (CNNs) constructed natively on the sphere have been developed recently and shown to be highly effective for the analysis of spherical data. While an efficient framework has been formulated, spherical CNNs are nevertheless highly computationally demanding; typically they cannot scale beyond spherical signals of thousands of pixels. We develop scattering networks constructed natively on the sphere that provide a powerful representational space for spherical data. Spherical scattering networks are computationally scalable and exhibit rotational equivariance, while their representational space is invariant to isometries and provides efficient and stable signal representations. By integrating scattering networks as an additional type of layer in the generalized spherical CNN framework, we show how they can be leveraged to scale spherical CNNs to the high-resolution data typical of many practical applications, with spherical signals of many tens of megapixels and beyond",
    "checked": true,
    "id": "28193843c562cbb90c719ead9969ce1d394bd933",
    "semantic_title": "scattering networks on the sphere for scalable and rotationally equivariant spherical cnns",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=xkjqJYqRJy": {
    "title": "Bayesian Neural Network Priors Revisited",
    "volume": "poster",
    "abstract": "Isotropic Gaussian priors are the de facto standard for modern Bayesian neural network inference. However, it is unclear whether these priors accurately reflect our true beliefs about the weight distributions or give optimal performance. To find better priors, we study summary statistics of neural network weights in networks trained using stochastic gradient descent (SGD). We find that convolutional neural network (CNN) and ResNet weights display strong spatial correlations, while fully connected networks (FCNNs) display heavy-tailed weight distributions. We show that building these observations into priors can lead to improved performance on a variety of image classification datasets. Surprisingly, these priors mitigate the cold posterior effect in FCNNs, but slightly increase the cold posterior effect in ResNets",
    "checked": true,
    "id": "3299b02c20da6d68f66918c2dd2ff5e35b01ca7b",
    "semantic_title": "bayesian neural network priors revisited",
    "citation_count": 146,
    "authors": []
  },
  "https://openreview.net/forum?id=6NePxZwfae": {
    "title": "Goal-Directed Planning via Hindsight Experience Replay",
    "volume": "poster",
    "abstract": "We consider the problem of goal-directed planning under a deterministic transition model. Monte Carlo Tree Search has shown remarkable performance in solving deterministic control problems. It has been extended from complex continuous domains through function approximators to bias the search of the planning tree in AlphaZero. Nonetheless, these algorithms still struggle with control problems with sparse rewards, such as goal-directed domains, where a positive reward is awarded only when reaching a goal state. In this work, we recast AlphaZero with Hindsight Experience Replay to tackle complex goal-directed planning tasks. We perform a thorough empirical evaluation in several simulated domains, including a novel application to a quantum compiling domain",
    "checked": true,
    "id": "f2c954712914d75c752cb0950ef32043793d58b9",
    "semantic_title": "goal-directed planning via hindsight experience replay",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=EMigfE6ZeS": {
    "title": "Hybrid Random Features",
    "volume": "poster",
    "abstract": "We propose a new class of random feature methods for linearizing softmax and Gaussian kernels called hybrid random features (HRFs) that automatically adapt the quality of kernel estimation to provide most accurate approximation in the defined regions of interest. Special instantiations of HRFs lead to well-known methods such as trigonometric (Rahimi & Recht, 2007) or (recently introduced in the context of linear-attention Transformers) positive random features (Choromanski et al., 2021). By generalizing Bochner's Theorem for softmax/Gaussian kernels and leveraging random features for compositional kernels, the HRF-mechanism provides strong theoretical guarantees - unbiased approximation and strictly smaller worst-case relative errors than its counterparts. We conduct exhaustive empirical evaluation of HRF ranging from pointwise kernel estimation experiments, through tests on data admitting clustering structure to benchmarking implicit-attention Transformers (also for downstream Robotics applications), demonstrating its quality in a wide spectrum of machine learning problems",
    "checked": true,
    "id": "f27e8c4731c575bd5f5db4c93ad8588f684dcbd0",
    "semantic_title": "hybrid random features",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=figzpGMrdD": {
    "title": "Pretrained Language Model in Continual Learning: A Comparative Study",
    "volume": "poster",
    "abstract": "Continual learning (CL) is a setting in which a model learns from a stream of incoming data while avoiding to forget previously learned knowledge. Pre-trained language models (PLMs) have been successfully employed in continual learning of different natural language problems. With the rapid development of many continual learning methods and PLMs, understanding and disentangling their interactions become essential for continued improvement of continual learning performance. In this paper, we thoroughly compare the continual learning performance over the combination of 5 PLMs and 4 CL approaches on 3 benchmarks in 2 typical incremental settings. Our extensive experimental analyses reveal interesting performance differences across PLMs and across CL methods. Furthermore, our representativeness probing analyses dissect PLMs' performance characteristics in a layer-wise and task-wise manner, uncovering the extent to which their inner layers suffer from forgetting, and the effect of different CL approaches on each layer. Finally, our observations and analyses open up a number of important research questions that will inform and guide the design of effective continual learning techniques",
    "checked": true,
    "id": "8e125d392ea0d8240be654d90a28838711a5bd36",
    "semantic_title": "pretrained language model in continual learning: a comparative study",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=XVPqLyNxSyh": {
    "title": "Salient ImageNet: How to discover spurious features in Deep Learning?",
    "volume": "poster",
    "abstract": "Deep neural networks can be unreliable in the real world especially when they heavily use {\\it spurious} features for their predictions. Focusing on image classifications, we define {\\it core features} as the set of visual features that are always a part of the object definition while {\\it spurious features} are the ones that are likely to {\\it co-occur} with the object but not a part of it (e.g., attribute ``fingers\" for class ``band aid\"). Traditional methods for discovering spurious features either require extensive human annotations (thus, not scalable), or are useful on specific models. In this work, we introduce a {\\it general} framework to discover a subset of spurious and core visual features used in inferences of a general model and localize them on a large number of images with minimal human supervision. Our methodology is based on this key idea: to identify spurious or core \\textit{visual features} used in model predictions, we identify spurious or core \\textit{neural features} (penultimate layer neurons of a robust model) via limited human supervision (e.g., using top 5 activating images per feature). We then show that these neural feature annotations {\\it generalize} extremely well to many more images {\\it without} any human supervision. We use the activation maps for these neural features as the soft masks to highlight spurious or core visual features. Using this methodology, we introduce the {\\it Salient Imagenet} dataset containing core and spurious masks for a large set of samples from Imagenet. Using this dataset, we show that several popular Imagenet models rely heavily on various spurious features in their predictions, indicating the standard accuracy alone is not sufficient to fully assess model' performance specially in safety-critical applications. Code is available at \\url{https://github.com/singlasahil14/salient_imagenet}",
    "checked": true,
    "id": "5f893ad86470cb935d702f980f5af8d8e013c7ae",
    "semantic_title": "salient imagenet: how to discover spurious features in deep learning?",
    "citation_count": 130,
    "authors": []
  },
  "https://openreview.net/forum?id=9wOQOgNe-w": {
    "title": "Differentiable DAG Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "60ef888df689fb0f0503fffcf53c9ce980b19c10",
    "semantic_title": "differentiable dag sampling",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=SS8F6tFX3-": {
    "title": "Evaluating Model-Based Planning and Planner Amortization for Continuous Control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "90ab9f71262e03ba7429b3fc4b631aa8ab1ddd28",
    "semantic_title": "evaluating model-based planning and planner amortization for continuous control",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=xKZ4K0lTj_": {
    "title": "Hierarchical Few-Shot Imitation with Skill Transition Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "259b4f5ed43fda5dd3510821b40fac13021e7605",
    "semantic_title": "hierarchical few-shot imitation with skill transition models",
    "citation_count": 48,
    "authors": []
  },
  "https://openreview.net/forum?id=g2LCQwG7Of": {
    "title": "End-to-End Learning of Probabilistic Hierarchies on Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d4f5444c28a8fff5bb77a7b04681385d0dff3e12",
    "semantic_title": "end-to-end learning of probabilistic hierarchies on graphs",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=-w2oomO6qgc": {
    "title": "GeneDisco: A Benchmark for Experimental Design in Drug Discovery",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9afd22d6a636082dc977d05e0772b3c5749451af",
    "semantic_title": "genedisco: a benchmark for experimental design in drug discovery",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=MXEl7i-iru": {
    "title": "GraphENS: Neighbor-Aware Ego Network Synthesis for Class-Imbalanced Node Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4d0f0212ac509445983dab3032af8cbd14a7c3e3",
    "semantic_title": "graphens: neighbor-aware ego network synthesis for class-imbalanced node classification",
    "citation_count": 102,
    "authors": []
  },
  "https://openreview.net/forum?id=hcQHRHKfN_": {
    "title": "Continuously Discovering Novel Strategies via Reward-Switching Policy Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "89eed2e92270db2789cfb0cf00b387877809ad7a",
    "semantic_title": "continuously discovering novel strategies via reward-switching policy optimization",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=wwDg3bbYBIq": {
    "title": "Learning to Remember Patterns: Pattern Matching Memory Networks for Traffic Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b680663cef887d75a827ea0151dca2b9fe2885c6",
    "semantic_title": "learning to remember patterns: pattern matching memory networks for traffic forecasting",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=VTNjxbFRKly": {
    "title": "Why Propagate Alone? Parallel Use of Labels and Features on Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2f64df4e61994b127e36f8114da3a070535941f4",
    "semantic_title": "why propagate alone? parallel use of labels and features on graphs",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=5i7lJLuhTm": {
    "title": "Learning by Directional Gradient Descent",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "afd60ef58e012023903d2815273f21ce4e49a506",
    "semantic_title": "learning by directional gradient descent",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=PtSAD3caaA2": {
    "title": "Maximum Entropy RL (Provably) Solves Some Robust RL Problems",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b284afe9a7363b898661c9b3cfb7f015b158cc63",
    "semantic_title": "maximum entropy rl (provably) solves some robust rl problems",
    "citation_count": 204,
    "authors": []
  },
  "https://openreview.net/forum?id=XhF2VOMRHS": {
    "title": "A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2acee01e0614155ee2537535d88dc8577a4eda73",
    "semantic_title": "a unified contrastive energy-based model for understanding the generative ability of adversarial training",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=e95i1IHcWj": {
    "title": "Equivariant and Stable Positional Encoding for More Powerful Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "facf11419e149a03bd4a9bffdda2ebb433a59d85",
    "semantic_title": "equivariant and stable positional encoding for more powerful graph neural networks",
    "citation_count": 119,
    "authors": []
  },
  "https://openreview.net/forum?id=Mng8CQ9eBW": {
    "title": "BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786",
    "semantic_title": "badpre: task-agnostic backdoor attacks to pre-trained nlp foundation models",
    "citation_count": 123,
    "authors": []
  },
  "https://openreview.net/forum?id=AV8FPoMTTa": {
    "title": "Shallow and Deep Networks are Near-Optimal Approximators of Korobov Functions",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "197691e71bfeaef201111c42dd675cf2f0888db2",
    "semantic_title": "shallow and deep networks are near-optimal approximators of korobov functions",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=Ucx3DQbC9GH": {
    "title": "What Makes Better Augmentation Strategies? Augment Difficult but Not too Different",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e8b90fd68d8ebb398fd8527170724554ae00fe4b",
    "semantic_title": "what makes better augmentation strategies? augment difficult but not too different",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=Harn4_EZBw": {
    "title": "Generative Pseudo-Inverse Memory",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5bbc8681584b307eff08ebcaf1bbef4cc91bdd16",
    "semantic_title": "generative pseudo-inverse memory",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=RQ428ZptQfU": {
    "title": "A Deep Variational Approach to Clustering Survival Data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9af28c385cd8765c8d243c4ff1b31ca98db43549",
    "semantic_title": "a deep variational approach to clustering survival data",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=qaxhBG1UUaS": {
    "title": "GPT-Critic: Offline Reinforcement Learning for End-to-End Task-Oriented Dialogue Systems",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "49f8a31e13998ae431dde8092973e6bd0f8385be",
    "semantic_title": "gpt-critic: offline reinforcement learning for end-to-end task-oriented dialogue systems",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=JtBRnrlOEFN": {
    "title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e79d1206292bc5e67ba19737d87d4b2ea4a37105",
    "semantic_title": "charformer: fast character transformers via gradient-based subword tokenization",
    "citation_count": 177,
    "authors": []
  },
  "https://openreview.net/forum?id=mQxt8l7JL04": {
    "title": "Regularized Autoencoders for Isometric Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6ccf2df209c28fd6b1599131221ea28037be6306",
    "semantic_title": "regularized autoencoders for isometric representation learning",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=dTqOcTUOQO": {
    "title": "Knowledge Removal in Sampling-based Bayesian Inference",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "56a9f45906eb341dbd9f30f76dbbaa30a7213c72",
    "semantic_title": "knowledge removal in sampling-based bayesian inference",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=vEZyTBRPP6o": {
    "title": "Actor-critic is implicitly biased towards high entropy optimal policies",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2c7bdcd62f7ab957589931b4bf818d92fb3498bb",
    "semantic_title": "actor-critic is implicitly biased towards high entropy optimal policies",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=mfwdY3U_9ea": {
    "title": "Igeood: An Information Geometry Approach to Out-of-Distribution Detection",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2815a5e7ba661ae278aa7c19e08ac884cde17bf7",
    "semantic_title": "igeood: an information geometry approach to out-of-distribution detection",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=N0uJGWDw21d": {
    "title": "Bag of Instances Aggregation Boosts Self-supervised Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "30e864862e59221174e5c9dea7d0cf847463f71c",
    "semantic_title": "bag of instances aggregation boosts self-supervised distillation",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=6tmjoym9LR6": {
    "title": "Stability Regularization for Discrete Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ff37496238deb3277908bf9568032434a0bd960b",
    "semantic_title": "stability regularization for discrete representation learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=aBVxf5NaaRt": {
    "title": "Unrolling PALM for Sparse Semi-Blind Source Separation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "32e8ddbc580c39967501992b5a35a3cab20b7807",
    "semantic_title": "unrolling palm for sparse semi-blind source separation",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=fQTlgI2qZqE": {
    "title": "Fast Generic Interaction Detection for Model Interpretability and Compression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a8e91f0aa2ba327a4a32daf79a736c5f15a8ba6f",
    "semantic_title": "fast generic interaction detection for model interpretability and compression",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=cGDAkQo1C0p": {
    "title": "Reversible Instance Normalization for Accurate Time-Series Forecasting against Distribution Shift",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0e95008a8c49a4c2538aed62ff61977ff7b47ca5",
    "semantic_title": "reversible instance normalization for accurate time-series forecasting against distribution shift",
    "citation_count": 613,
    "authors": []
  },
  "https://openreview.net/forum?id=8uz0EWPQIMu": {
    "title": "On the Pitfalls of Analyzing Individual Neurons in Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "aaf3ebaf12baeb366ce6ff32aa36d608a7eab583",
    "semantic_title": "on the pitfalls of analyzing individual neurons in language models",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=4rLw09TgRw9": {
    "title": "Query Embedding on Hyper-Relational Knowledge Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1c652779d9b6a0700386295a428f21293d7b6d5b",
    "semantic_title": "query embedding on hyper-relational knowledge graphs",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=m8bypnj7Yl5": {
    "title": "Neural Solvers for Fast and Accurate Numerical Optimal Control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4d70ac690fcde8a720d6df5b106260240b353fb4",
    "semantic_title": "neural solvers for fast and accurate numerical optimal control",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=Ix_mh42xq5w": {
    "title": "PSA-GAN: Progressive Self Attention GANs for Synthetic Time Series",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "68043cccd2b620d44d6ffd4983b55060966acb23",
    "semantic_title": "psa-gan: progressive self attention gans for synthetic time series",
    "citation_count": 64,
    "authors": []
  },
  "https://openreview.net/forum?id=2t7CkQXNpuq": {
    "title": "ToM2C: Target-oriented Multi-agent Communication and Cooperation with Theory of Mind",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "52a027e80c24ecf9bcc468609dbb5be72478ec7a",
    "semantic_title": "tom2c: target-oriented multi-agent communication and cooperation with theory of mind",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=Iog0djAdbHj": {
    "title": "Better Supervisory Signals by Observing Learning Paths",
    "volume": "poster",
    "abstract": "Better-supervised models might have better performance. In this paper, we first clarify what makes for good supervision for a classification problem, and then explain two existing label refining methods, label smoothing and knowledge distillation, in terms of our proposed criterion. To further answer why and how better supervision emerges, we observe the learning path, i.e., the trajectory of the model's predictions during training, for each training sample. We find that the model can spontaneously refine \"bad\" labels through a \"zig-zag\" learning path, which occurs on both toy and real datasets. Observing the learning path not only provides a new perspective for understanding knowledge distillation, overfitting, and learning dynamics, but also reveals that the supervisory signal of a teacher network can be very unstable near the best points in training on real tasks. Inspired by this, we propose a new knowledge distillation scheme, Filter-KD, which improves downstream classification performance in various settings",
    "checked": true,
    "id": "91db2f1dae977f2cf139ce194173487133b1ad67",
    "semantic_title": "better supervisory signals by observing learning paths",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=izj68lUcBpt": {
    "title": "TAda! Temporally-Adaptive Convolutions for Video Understanding",
    "volume": "poster",
    "abstract": "Spatial convolutions are widely used in numerous deep video models. It fundamentally assumes spatio-temporal invariance, i.e., using shared weights for every location in different frames. This work presents Temporally-Adaptive Convolutions (TAdaConv) for video understanding, which shows that adaptive weight calibration along the temporal dimension is an efficient way to facilitate modelling complex temporal dynamics in videos. Specifically, TAdaConv empowers the spatial convolutions with temporal modelling abilities by calibrating the convolution weights for each frame according to its local and global temporal context. Compared to previous temporal modelling operations, TAdaConv is more efficient as it operates over the convolution kernels instead of the features, whose dimension is an order of magnitude smaller than the spatial resolutions. Further, the kernel calibration brings an increased model capacity. We construct TAda2D and TAdaConvNeXt networks by replacing the 2D convolutions in ResNet and ConvNeXt with TAdaConv, which leads to at least on par or better performance compared to state-of-the-art approaches on multiple video action recognition and localization benchmarks. We also demonstrate that as a readily plug-in operation with negligible computation overhead, TAdaConv can effectively improve many existing video models with a convincing margin",
    "checked": true,
    "id": "bb433f073e54e733fa9cc7d2acf32b43dd687449",
    "semantic_title": "tada! temporally-adaptive convolutions for video understanding",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=4Muj-t_4o4": {
    "title": "Learning a subspace of policies for online adaptation in Reinforcement Learning",
    "volume": "poster",
    "abstract": "Deep Reinforcement Learning (RL) is mainly studied in a setting where the training and the testing environments are similar. But in many practical applications, these environments may differ. For instance, in control systems, the robot(s) on which a policy is learned might differ from the robot(s) on which a policy will run. It can be caused by different internal factors (e.g., calibration issues, system attrition, defective modules) or also by external changes (e.g., weather conditions). There is a need to develop RL methods that generalize well to variations of the training conditions. In this article, we consider the simplest yet hard to tackle generalization setting where the test environment is unknown at train time, forcing the agent to adapt to the system's new dynamics. This online adaptation process can be computationally expensive (e.g., fine-tuning) and cannot rely on meta-RL techniques since there is just a single train environment. To do so, we propose an approach where we learn a subspace of policies within the parameter space. This subspace contains an infinite number of policies that are trained to solve the training environment while having different parameter values. As a consequence, two policies in that subspace process information differently and exhibit different behaviors when facing variations of the train environment. Our experiments carried out over a large variety of benchmarks compare our approach with baselines, including diversity-based methods. In comparison, our approach is simple to tune, does not need any extra component (e.g., discriminator) and learns policies able to gather a high reward on unseen environments",
    "checked": true,
    "id": "3dc6141adfdaa3c11ca672029002fa6eb11aba0d",
    "semantic_title": "learning a subspace of policies for online adaptation in reinforcement learning",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=2sDQwC_hmnM": {
    "title": "ZeroFL: Efficient On-Device Training for Federated Learning with Local Sparsity",
    "volume": "poster",
    "abstract": "When the available hardware cannot meet the memory and compute requirements to efficiently train high performing machine learning models, a compromise in either the training quality or the model complexity is needed. In Federated Learning (FL), nodes are orders of magnitude more constrained than traditional server-grade hardware and are often battery powered, severely limiting the sophistication of models that can be trained under this paradigm. While most research has focused on designing better aggregation strategies to improve convergence rates and in alleviating the communication costs of FL, fewer efforts have been devoted to accelerating on-device training. Such stage, which repeats hundreds of times (i.e. every round) and can involve thousands of devices, accounts for the majority of the time required to train federated models and, the totality of the energy consumption at the client side. In this work, we present the first study on the unique aspects that arise when introducing sparsity at training time in FL workloads. We then propose ZeroFL, a framework that relies on highly sparse operations to accelerate on-device training. Models trained with ZeroFL and 95% sparsity achieve up to 2.3% higher accuracy compared to competitive baselines obtained from adapting a state-of-the-art sparse training framework to the FL setting",
    "checked": true,
    "id": "c9ac807a8760ae2e35dff0967ad8f24440fadb7b",
    "semantic_title": "zerofl: efficient on-device training for federated learning with local sparsity",
    "citation_count": 80,
    "authors": []
  },
  "https://openreview.net/forum?id=Oxeka7Z7Hor": {
    "title": "Gaussian Mixture Convolution Networks",
    "volume": "poster",
    "abstract": "This paper proposes a novel method for deep learning based on the analytical convolution of multidimensional Gaussian mixtures. In contrast to tensors, these do not suffer from the curse of dimensionality and allow for a compact representation, as data is only stored where details exist. Convolution kernels and data are Gaussian mixtures with unconstrained weights, positions, and covariance matrices. Similar to discrete convolutional networks, each convolution step produces several feature channels, represented by independent Gaussian mixtures. Since traditional transfer functions like ReLUs do not produce Gaussian mixtures, we propose using a fitting of these functions instead. This fitting step also acts as a pooling layer if the number of Gaussian components is reduced appropriately. We demonstrate that networks based on this architecture reach competitive accuracy on Gaussian mixtures fitted to the MNIST and ModelNet data sets",
    "checked": true,
    "id": "ce108373d3458c2f27524483221aaad772c3edf3",
    "semantic_title": "gaussian mixture convolution networks",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=bwq6O4Cwdl": {
    "title": "How Does SimSiam Avoid Collapse Without Negative Samples? A Unified Understanding with Self-supervised Contrastive Learning",
    "volume": "poster",
    "abstract": "To avoid collapse in self-supervised learning (SSL), a contrastive loss is widely used but often requires a large number of negative samples. Without negative samples yet achieving competitive performance, a recent work~\\citep{chen2021exploring} has attracted significant attention for providing a minimalist simple Siamese (SimSiam) method to avoid collapse. However, the reason for how it avoids collapse without negative samples remains not fully clear and our investigation starts by revisiting the explanatory claims in the original SimSiam. After refuting their claims, we introduce vector decomposition for analyzing the collapse based on the gradient analysis of the $l_2$-normalized representation vector. This yields a unified perspective on how negative samples and SimSiam alleviate collapse. Such a unified perspective comes timely for understanding the recent progress in SSL",
    "checked": true,
    "id": "2b9455fceb0ff58f28a46aebfb8df6f7003e9e40",
    "semantic_title": "how does simsiam avoid collapse without negative samples? a unified understanding with self-supervised contrastive learning",
    "citation_count": 78,
    "authors": []
  },
  "https://openreview.net/forum?id=kAa9eDS0RdO": {
    "title": "Attention-based Interpretability with Concept Transformers",
    "volume": "poster",
    "abstract": "Attention is a mechanism that has been instrumental in driving remarkable performance gains of deep neural network models in a host of visual, NLP and multimodal tasks. One additional notable aspect of attention is that it conveniently exposes the ``reasoning'' behind each particular output generated by the model. Specifically, attention scores over input regions or intermediate features have been interpreted as a measure of the contribution of the attended element to the model inference. While the debate in regard to the interpretability of attention is still not settled, researchers have pointed out the existence of architectures and scenarios that afford a meaningful interpretation of the attention mechanism. Here we propose the generalization of attention from low-level input features to high-level concepts as a mechanism to ensure the interpretability of attention scores within a given application domain. In particular, we design the ConceptTransformer, a deep learning module that exposes explanations of the output of a model in which it is embedded in terms of attention over user-defined high-level concepts. Such explanations are \\emph{plausible} (i.e.\\ convincing to the human user) and \\emph{faithful} (i.e.\\ truly reflective of the reasoning process of the model). Plausibility of such explanations is obtained by construction by training the attention heads to conform with known relations between inputs, concepts and outputs dictated by domain knowledge. Faithfulness is achieved by design by enforcing a linear relation between the transformer value vectors that represent the concepts and their contribution to the classification log-probabilities. We validate our ConceptTransformer module on established explainability benchmarks and show how it can be used to infuse domain knowledge into classifiers to improve accuracy, and conversely to extract concept-based explanations of classification outputs. Code to reproduce our results is available at: \\url{https://github.com/ibm/concept_transformer}",
    "checked": true,
    "id": "2f47a4c37c01d3ad4e6c4b074ff61468f1e976b8",
    "semantic_title": "attention-based interpretability with concept transformers",
    "citation_count": 57,
    "authors": []
  },
  "https://openreview.net/forum?id=PTRo58zPt3P": {
    "title": "Inductive Relation Prediction Using Analogy Subgraph Embeddings",
    "volume": "poster",
    "abstract": "Prevailing methods for relation prediction in heterogeneous graphs aim at learning latent representations (i.e., embeddings) of observed nodes and relations, and thus are limited to the transductive setting where the relation types must be known during training. Here, we propose ANalogy SubGraphEmbeddingLearning (GraphANGEL), a novel relation prediction framework that predicts relations5between each node pair based on the subgraphs containing the pair, as well as other (analogy) subgraphs with the same graph patterns. Each graph pattern explicitly represents a specific logical rule, which contributes to an inductive bias that facilitates generalization to unseen relations and leads to more explainable predictive models. Moreover, our method also removes the limited neighborhood constraint of graph neural networks. Our model consistently outperforms existing models on heterogeneous graph based recommendation as well as knowledge graph completion. We also empirically demonstrate our model's capability in generalizing to new relations while producing explainable heat maps of attention scores across the discovered logic",
    "checked": true,
    "id": "271f40f4fad4d112e436565e668b79ede690d755",
    "semantic_title": "inductive relation prediction using analogy subgraph embeddings",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=CmsfC7u054S": {
    "title": "Reinforcement Learning in Presence of Discrete Markovian Context Evolution",
    "volume": "poster",
    "abstract": "We consider a context-dependent Reinforcement Learning (RL) setting, which is characterized by: a) an unknown finite number of not directly observable contexts; b) abrupt (discontinuous) context changes occurring during an episode; and c) Markovian context evolution. We argue that this challenging case is often met in applications and we tackle it using a Bayesian model-based approach and variational inference. We adapt a sticky Hierarchical Dirichlet Process (HDP) prior for model learning, which is arguably best-suited for infinite Markov chain modeling. We then derive a context distillation procedure, which identifies and removes spurious contexts in an unsupervised fashion. We argue that the combination of these two components allows inferring the number of contexts from data thus dealing with the context cardinality assumption. We then find the representation of the optimal policy enabling efficient policy learning using off-the-shelf RL algorithms. Finally, we demonstrate empirically (using gym environments cart-pole swing-up, drone, intersection) that our approach succeeds where state-of-the-art methods of other frameworks fail and elaborate on the reasons for such failures",
    "checked": true,
    "id": "69d042aaaf2fd7b15888aaf98009e184b54c32f8",
    "semantic_title": "reinforcement learning in presence of discrete markovian context evolution",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=t98k9ePQQpn": {
    "title": "Optimal Transport for Long-Tailed Recognition with Learnable Cost Matrix",
    "volume": "poster",
    "abstract": "It is attracting attention to the long-tailed recognition problem, a burning issue that has become very popular recently. Distinctive from conventional recognition is that it posits that the allocation of the training set is supremely distorted. Predictably, it will pose challenges to the generalisation behaviour of the model. Approaches to these challenges revolve into two groups: firstly, training-aware methods, with the aim of enhancing the generalisability of the model by exploiting its potential in the training period; and secondly, post-hoc correction, liberally coupled with training-aware methods, which is intended to refine the predictions to the extent possible in the post-processing stage, offering the advantages of simplicity and effectiveness. This paper introduces an alternative direction to do the post-hoc correction, which goes beyond the statistical methods. Mathematically, we approach this issue from the perspective of optimal transport (OT), yet, choosing the exact cost matrix when applying OT is challenging and requires expert knowledge of various tasks. To overcome this limitation, we propose to employ linear mapping to learn the cost matrix without necessary configurations adaptively. Testing our methods in practice, along with high efficiency and excellent performance, our method surpasses all previous methods and has the best performance to date",
    "checked": true,
    "id": "832c8e12e821db5fea16e486d40fd437d8302c6a",
    "semantic_title": "optimal transport for long-tailed recognition with learnable cost matrix",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=_BNiN4IjC5": {
    "title": "PriorGrad: Improving Conditional Denoising Diffusion Models with Data-Dependent Adaptive Prior",
    "volume": "poster",
    "abstract": "Denoising diffusion probabilistic models have been recently proposed to generate high-quality samples by estimating the gradient of the data density. The framework assumes the prior noise as a standard Gaussian distribution, whereas the corresponding data distribution may be more complicated than the standard Gaussian distribution, which potentially introduces inefficiency in denoising the prior noise into the data sample because of the discrepancy between the data and the prior. In this paper, we propose PriorGrad to improve the efficiency of the conditional diffusion model (for example, a vocoder using a mel-spectrogram as the condition) by applying an adaptive prior derived from the data statistics based on the conditional information. We formulate the training and sampling procedures of PriorGrad and demonstrate the advantages of an adaptive prior through a theoretical analysis. Focusing on the audio domain, we consider the recently proposed diffusion-based audio generative models based on both the spectral and time domains and show that PriorGrad achieves faster convergence and superior performance, leading to an improved perceptual quality and tolerance to a smaller network capacity, and thereby demonstrating the efficiency of a data-dependent adaptive prior",
    "checked": true,
    "id": "69614f326557928d9d142ca0de2e5f572d813f04",
    "semantic_title": "priorgrad: improving conditional denoising diffusion models with data-dependent adaptive prior",
    "citation_count": 97,
    "authors": []
  },
  "https://openreview.net/forum?id=pz1euXohm4H": {
    "title": "Target-Side Input Augmentation for Sequence to Sequence Generation",
    "volume": "poster",
    "abstract": "Autoregressive sequence generation, a prevalent task in machine learning and natural language processing, generates every target token conditioned on both a source input and previously generated target tokens. Previous data augmentation methods, which have been shown to be effective for the task, mainly enhance source inputs (e.g., injecting noise into the source sequence by random swapping or masking, back translation, etc.) while overlooking the target-side augmentation. In this work, we propose a target-side augmentation method for sequence generation. In training, we use the decoder output probability distributions as soft indicators, which are multiplied with target token embeddings, to build pseudo tokens. These soft pseudo tokens are then used as target tokens to enhance the training. We conduct comprehensive experiments on various sequence generation tasks, including dialog generation, machine translation, and abstractive summarization. Without using any extra labeled data or introducing additional model parameters, our method significantly outperforms strong baselines. The code is available at https://github.com/TARGET-SIDE-DATA-AUG/TSDASG",
    "checked": true,
    "id": "6883a825e8267459080fc0668df716853d1c83b0",
    "semantic_title": "target-side input augmentation for sequence to sequence generation",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=nBU_u6DLvoK": {
    "title": "UniFormer: Unified Transformer for Efficient Spatial-Temporal Representation Learning",
    "volume": "poster",
    "abstract": "It is a challenging task to learn rich and multi-scale spatiotemporal semantics from high-dimensional videos, due to large local redundancy and complex global dependency between video frames. The recent advances in this research have been mainly driven by 3D convolutional neural networks and vision transformers. Although 3D convolution can efficiently aggregate local context to suppress local redundancy from a small 3D neighborhood, it lacks the capability to capture global dependency because of the limited receptive field. Alternatively, vision transformers can effectively capture long-range dependency by self-attention mechanism, while having the limitation on reducing local redundancy with blind similarity comparison among all the tokens in each layer. Based on these observations, we propose a novel Unified transFormer (UniFormer) which seamlessly integrates merits of 3D convolution and spatiotemporal self-attention in a concise transformer format, and achieves a preferable balance between computation and accuracy. Different from traditional transformers, our relation aggregator can tackle both spatiotemporal redundancy and dependency, by learning local and global token affinity respectively in shallow and deep layers. We conduct extensive experiments on the popular video benchmarks, e.g., Kinetics-400, Kinetics-600, and Something-Something V1&V2. With only ImageNet-1K pretraining, our UniFormer achieves 82.9%/84.8% top-1 accuracy on Kinetics-400/Kinetics-600, while requiring 10x fewer GFLOPs than other state-of-the-art methods. For Something-Something V1 and V2, our UniFormer achieves new state-of-the-art performances of 60.9% and 71.2% top-1 accuracy respectively. Code is available at https://github.com/Sense-X/UniFormer",
    "checked": false,
    "id": "85e1bdcffda4cd02277297d3f45fe67653910464",
    "semantic_title": "multi-agent transformer networks for multimodal human activity recognition",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=DYypjaRdph2": {
    "title": "Inverse Online Learning: Understanding Non-Stationary and Reactionary Policies",
    "volume": "poster",
    "abstract": "Human decision making is well known to be imperfect and the ability to analyse such processes individually is crucial when attempting to aid or improve a decision-maker's ability to perform a task, e.g. to alert them to potential biases or oversights on their part. To do so, it is necessary to develop interpretable representations of how agents make decisions and how this process changes over time as the agent learns online in reaction to the accrued experience. To then understand the decision-making processes underlying a set of observed trajectories, we cast the policy inference problem as the inverse to this online learning problem. By interpreting actions within a potential outcomes framework, we introduce a meaningful mapping based on agents choosing an action they believe to have the greatest treatment effect. We introduce a practical algorithm for retrospectively estimating such perceived effects, alongside the process through which agents update them, using a novel architecture built upon an expressive family of deep state-space models. Through application to the analysis of UNOS organ donation acceptance decisions, we demonstrate that our approach can bring valuable insights into the factors that govern decision processes and how they change over time",
    "checked": true,
    "id": "2a8ce79d05232acfaad7753420cd99dcd5d718c8",
    "semantic_title": "inverse online learning: understanding non-stationary and reactionary policies",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=6YVIk0sAkF_": {
    "title": "Multi-Mode Deep Matrix and Tensor Factorization",
    "volume": "poster",
    "abstract": "Recently, deep linear and nonlinear matrix factorizations gain increasing attention in the area of machine learning. Existing deep nonlinear matrix factorization methods can only exploit partial nonlinearity of the data and are not effective in handling matrices of which the number of rows is comparable to the number of columns. On the other hand, there is still a gap between deep learning and tensor decomposition. This paper presents a framework of multi-mode deep matrix and tensor factorizations to explore and exploit the full nonlinearity of the data in matrices and tensors. We use the factorization methods to solve matrix and tensor completion problems and prove that our methods have tighter generalization error bounds than conventional matrix and tensor factorization methods. The experiments on synthetic data and real datasets showed that the proposed methods have much higher recovery accuracy than many baselines",
    "checked": true,
    "id": "1c7825a8cf4dabc866a6e5b0a88cff37f1d123d4",
    "semantic_title": "multi-mode deep matrix and tensor factorization",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=fCG75wd39ze": {
    "title": "LORD: Lower-Dimensional Embedding of Log-Signature in Neural Rough Differential Equations",
    "volume": "poster",
    "abstract": "The problem of processing very long time-series data (e.g., a length of more than 10,000) is a long-standing research problem in machine learning. Recently, one breakthrough, called neural rough differential equations (NRDEs), has been proposed and has shown that it is able to process such data. Their main concept is to use the log-signature transform, which is known to be more efficient than the Fourier transform for irregular long time-series, to convert a very long time-series sample into a relatively shorter series of feature vectors. However, the log-signature transform causes non-trivial spatial overheads. To this end, we present the method of LOweR-Dimensional embedding of log-signature (LORD), where we define an NRDE-based autoencoder to implant the higher-depth log-signature knowledge into the lower-depth log-signature. We show that the encoder successfully combines the higher-depth and the lower-depth log-signature knowledge, which greatly stabilizes the training process and increases the model accuracy. In our experiments with benchmark datasets, the improvement ratio by our method is up to 75\\% in terms of various classification and forecasting evaluation metrics",
    "checked": true,
    "id": "40aae7819918c596fd6a8a5caee2b9eb4cf4ec02",
    "semantic_title": "lord: lower-dimensional embedding of log-signature in neural rough differential equations",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=bsycpMi00R1": {
    "title": "Generalized Natural Gradient Flows in Hidden Convex-Concave Games and GANs",
    "volume": "poster",
    "abstract": "Game-theoretic formulations in machine learning have recently risen in prominence, whereby entire modeling paradigms are best captured as zero-sum games. Despite their popularity, however, their dynamics are still poorly understood. This lack of theory is often substantiated with painful empirical observations of volatile training dynamics and even divergence. Such results highlight the need to develop an appropriate theory with convergence guarantees that are powerful enough to inform practice. This paper studies the generalized Gradient Descent-Ascent (GDA) flow in a large class of non-convex non-concave Zero-Sum games dubbed Hidden Convex-Concave games, a class of games that includes GANs. We focus on two specific geometries: a novel geometry induced by the hidden convex-concave structure that we call the hidden mapping geometry and the Fisher information geometry. For the hidden mapping geometry, we prove global convergence under mild assumptions. In the case of Fisher information geometry, we provide a complete picture of the dynamics in an interesting special setting of team competition via invariant function analysis",
    "checked": true,
    "id": "4fe997e21a68b433f217007a12f9bd797cf2f1e2",
    "semantic_title": "generalized natural gradient flows in hidden convex-concave games and gans",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=sPIFuucA3F": {
    "title": "Offline Neural Contextual Bandits: Pessimism, Optimization and Generalization",
    "volume": "poster",
    "abstract": "Offline policy learning (OPL) leverages existing data collected a priori for policy optimization without any active exploration. Despite the prevalence and recent interest in this problem, its theoretical and algorithmic foundations in function approximation settings remain under-developed. In this paper, we consider this problem on the axes of distributional shift, optimization, and generalization in offline contextual bandits with neural networks. In particular, we propose a provably efficient offline contextual bandit with neural network function approximation that does not require any functional assumption on the reward. We show that our method provably generalizes over unseen contexts under a milder condition for distributional shift than the existing OPL works. Notably, unlike any other OPL method, our method learns from the offline data in an online manner using stochastic gradient descent, allowing us to leverage the benefits of online learning into an offline setting. Moreover, we show that our method is more computationally efficient and has a better dependence on the effective dimension of the neural network than an online counterpart. Finally, we demonstrate the empirical effectiveness of our method in a range of synthetic and real-world OPL problems",
    "checked": true,
    "id": "0c78bf086ae589d0a5a68b8e8326d2048b87018f",
    "semantic_title": "offline neural contextual bandits: pessimism, optimization and generalization",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=QDdJhACYrlX": {
    "title": "THOMAS: Trajectory Heatmap Output with learned Multi-Agent Sampling",
    "volume": "poster",
    "abstract": "In this paper, we propose THOMAS, a joint multi-agent trajectory prediction framework allowing for an efficient and consistent prediction of multi-agent multi-modal trajectories. We present a unified model architecture for simultaneous agent future heatmap estimation, in which we leverage hierarchical and sparse image generation for fast and memory-efficient inference. We propose a learnable trajectory recombination model that takes as input a set of predicted trajectories for each agent and outputs its consistent reordered recombination. This recombination module is able to realign the initially independent modalities so that they do no collide and are coherent with each other. We report our results on the Interaction multi-agent prediction challenge and rank $1^{st}$ on the online test leaderboard",
    "checked": true,
    "id": "9154d56389f9d5517fd721b456f9f07caac06ae9",
    "semantic_title": "thomas: trajectory heatmap output with learned multi-agent sampling",
    "citation_count": 154,
    "authors": []
  },
  "https://openreview.net/forum?id=rHMaBYbkkRJ": {
    "title": "CLEVA-Compass: A Continual Learning Evaluation Assessment Compass to Promote Research Transparency and Comparability",
    "volume": "poster",
    "abstract": "What is the state of the art in continual machine learning? Although a natural question for predominant static benchmarks, the notion to train systems in a lifelong manner entails a plethora of additional challenges with respect to set-up and evaluation. The latter have recently sparked a growing amount of critiques on prominent algorithm-centric perspectives and evaluation protocols being too narrow, resulting in several attempts at constructing guidelines in favor of specific desiderata or arguing against the validity of prevalent assumptions. In this work, we depart from this mindset and argue that the goal of a precise formulation of desiderata is an ill-posed one, as diverse applications may always warrant distinct scenarios. Instead, we introduce the Continual Learning EValuation Assessment Compass: the CLEVA-Compass. The compass provides the visual means to both identify how approaches are practically reported and how works can simultaneously be contextualized in the broader literature landscape. In addition to promoting compact specification in the spirit of recent replication trends, it thus provides an intuitive chart to understand the priorities of individual systems, where they resemble each other, and what elements are missing towards a fair comparison",
    "checked": true,
    "id": "0f51b10baca455068919915976b6b0e1ab62f2c3",
    "semantic_title": "cleva-compass: a continual learning evaluation assessment compass to promote research transparency and comparability",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=aisKPsMM3fg": {
    "title": "Neural Stochastic Dual Dynamic Programming",
    "volume": "poster",
    "abstract": "Stochastic dual dynamic programming (SDDP) is a state-of-the-art method for solving multi-stage stochastic optimization, widely used for modeling real-world process optimization tasks. Unfortunately, SDDP has a worst-case complexity that scales exponentially in the number of decision variables, which severely limits applicability to only low dimensional problems. To overcome this limitation, we extend SDDP by introducing a trainable neural model that learns to map problem instances to a piece-wise linear value function within intrinsic low-dimension space, which is architected specifically to interact with a base SDDP solver, so that can accelerate optimization performance on new instances. The proposed Neural Stochastic Dual Dynamic Programming ($$\\nu$$-SDDP) continually self-improves by solving successive problems. An empirical investigation demonstrates that $$\\nu$$-SDDP can significantly reduce problem solving cost without sacrificing solution quality over competitors such as SDDP and reinforcement learning algorithms, across a range of synthetic and real-world process optimization problems",
    "checked": true,
    "id": "e4dddc411739f85097c20aa71ebf27357b17bfb3",
    "semantic_title": "neural stochastic dual dynamic programming",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=BrPdX1bDZkQ": {
    "title": "DemoDICE: Offline Imitation Learning with Supplementary Imperfect Demonstrations",
    "volume": "poster",
    "abstract": "We consider offline imitation learning (IL), which aims to mimic the expert's behavior from its demonstration without further interaction with the environment. One of the main challenges in offline IL is to deal with the narrow support of the data distribution exhibited by the expert demonstrations that cover only a small fraction of the state and the action spaces. As a result, offline IL algorithms that rely only on expert demonstrations are very unstable since the situation easily deviates from those in the expert demonstrations. In this paper, we assume additional demonstration data of unknown degrees of optimality, which we call imperfect demonstrations. Under this setting, we propose DemoDICE, which effectively utilizes imperfect demonstrations by matching the stationary distribution of a policy with experts' distribution while penalizing its deviation from the overall demonstrations. Compared with the recent IL algorithms that adopt adversarial minimax training objectives, we substantially stabilize overall learning process by reducing minimax optimization to a direct convex optimization in a principled manner. Using extensive tasks, we show that DemoDICE achieves promising results in the offline IL from expert and imperfect demonstrations",
    "checked": true,
    "id": "8c16bd5d73372a48aec00c23312d66b0dc2044fa",
    "semantic_title": "demodice: offline imitation learning with supplementary imperfect demonstrations",
    "citation_count": 91,
    "authors": []
  },
  "https://openreview.net/forum?id=ZTsoE8G3GG": {
    "title": "Learning to Extend Molecular Scaffolds with Structural Motifs",
    "volume": "poster",
    "abstract": "Recent advancements in deep learning-based modeling of molecules promise to accelerate in silico drug discovery. A plethora of generative models is available, building molecules either atom-by-atom and bond-by-bond or fragment-by-fragment. However, many drug discovery projects require a fixed scaffold to be present in the generated molecule, and incorporating that constraint has only recently been explored. Here, we propose MoLeR, a graph-based model that naturally supports scaffolds as initial seed of the generative procedure, which is possible because it is not conditioned on the generation history. Our experiments show that MoLeR performs comparably to state-of-the-art methods on unconstrained molecular optimization tasks, and outperforms them on scaffold-based tasks, while being an order of magnitude faster to train and sample from than existing approaches. Furthermore, we show the influence of a number of seemingly minor design choices on the overall performance",
    "checked": true,
    "id": "faa6d14ee02cb7096a4ef5d7444742d065656a02",
    "semantic_title": "learning to extend molecular scaffolds with structural motifs",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=p98WJxUC3Ca": {
    "title": "Discrepancy-Based Active Learning for Domain Adaptation",
    "volume": "poster",
    "abstract": "The goal of the paper is to design active learning strategies which lead to domain adaptation under an assumption of Lipschitz functions. Building on previous work by Mansour et al. (2009) we adapt the concept of discrepancy distance between source and target distributions to restrict the maximization over the hypothesis class to a localized class of functions which are performing accurate labeling on the source domain. We derive generalization error bounds for such active learning strategies in terms of Rademacher average and localized discrepancy for general loss functions which satisfy a regularity condition. A practical K-medoids algorithm that can address the case of large data set is inferred from the theoretical bounds. Our numerical experiments show that the proposed algorithm is competitive against other state-of-the-art active learning techniques in the context of domain adaptation, in particular on large data sets of around one hundred thousand images",
    "checked": true,
    "id": "6d49abd4a9d563e6cf86ae5e2a808ecd4d67c3fa",
    "semantic_title": "discrepancy-based active learning for domain adaptation",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=vDwBW49HmO": {
    "title": "Gradient Matching for Domain Generalization",
    "volume": "poster",
    "abstract": "Machine learning systems typically assume that the distributions of training and test sets match closely. However, a critical requirement of such systems in the real world is their ability to generalize to unseen domains. Here, we propose an _inter-domain gradient matching_ objective that targets domain generalization by maximizing the inner product between gradients from different domains. Since direct optimization of the gradient inner product can be computationally prohibitive --- it requires computation of second-order derivatives ‚Äì-- we derive a simpler first-order algorithm named Fish that approximates its optimization. We perform experiments on the Wilds benchmark, which captures distribution shift in the real world, as well as the DomainBed benchmark that focuses more on synthetic-to-real transfer. Our method produces competitive results on both benchmarks, demonstrating its effectiveness across a wide range of domain generalization tasks",
    "checked": true,
    "id": "b7a4be5a703f251b2d341d30ccec5e201881981b",
    "semantic_title": "gradient matching for domain generalization",
    "citation_count": 328,
    "authors": []
  },
  "https://openreview.net/forum?id=d5SCUJ5t1k": {
    "title": "Objects in Semantic Topology",
    "volume": "poster",
    "abstract": "A more realistic object detection paradigm, Open-World Object Detection, has arised increasing research interests in the community recently. A qualified open-world object detector can not only identify objects of known categories, but also discover unknown objects, and incrementally learn to categorize them when their annotations progressively arrive. Previous works rely on independent modules to recognize unknown categories and perform incremental learning, respectively. In this paper, we provide a unified perspective: Semantic Topology. During the life-long learning of an open-world object detector, all object instances from the same category are assigned to their corresponding pre-defined node in the semantic topology, including the `unknown' category. This constraint builds up discriminative feature representations and consistent relationships among objects, thus enabling the detector to distinguish unknown objects out of the known categories, as well as making learned features of known objects undistorted when learning new categories incrementally. Extensive experiments demonstrate that semantic topology, either randomly-generated or derived from a well-trained language model, could outperform the current state-of-the-art open-world object detectors by a large margin, e.g., the absolute open-set error (the number of unknown instances that are wrongly labeled as known) is reduced from 7832 to 2546, exhibiting the inherent superiority of semantic topology on open-world object detection",
    "checked": true,
    "id": "9218a3b3d9314bc1e9b4c03ce50fff1147d8fdab",
    "semantic_title": "objects in semantic topology",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=ds8yZOUsea": {
    "title": "Hidden Parameter Recurrent State Space Models For Changing Dynamics Scenarios",
    "volume": "poster",
    "abstract": "Recurrent State-space models (RSSMs) are highly expressive models for learning patterns in time series data and for system identification. However, these models are often based on the assumption that the dynamics are fixed and unchanging, which is rarely the case in real-world scenarios. Many control applications often exhibit tasks with similar, but not identical dynamics, that can be modelled as having a common latent structure. We introduce the Hidden Parameter Recurrent State Space Models (HiP-RSSMs), a framework that parametrizes a family of related state-space models with a low-dimensional set of latent factors. We present a simple and effective way of performing learning and inference over this Gaussian graphical model that avoids approximations like variational inference. We show that HiP-RSSMs outperforms RSSMs and competing multi-task models on several challenging robotic benchmarks both on real systems and simulations",
    "checked": true,
    "id": "b66f6ef8c4ff9b5fce38396d43c7891ad4e6969f",
    "semantic_title": "hidden parameter recurrent state space models for changing dynamics scenarios",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=ar92oEosBIg": {
    "title": "Graph Neural Network Guided Local Search for the Traveling Salesperson Problem",
    "volume": "poster",
    "abstract": "Solutions to the Traveling Salesperson Problem (TSP) have practical applications to processes in transportation, logistics, and automation, yet must be computed with minimal delay to satisfy the real-time nature of the underlying tasks. However, solving large TSP instances quickly without sacrificing solution quality remains challenging for current approximate algorithms. To close this gap, we present a hybrid data-driven approach for solving the TSP based on Graph Neural Networks (GNNs) and Guided Local Search (GLS). Our model predicts the regret of including each edge of the problem graph in the solution; GLS uses these predictions in conjunction with the original problem graph to find solutions. Our experiments demonstrate that this approach converges to optimal solutions at a faster rate than three recent learning based approaches for the TSP. Notably, we reduce the mean optimality gap on the 100-node problem set from 1.534% to 0.705%, a 2x improvement. When generalizing from 20-node instances to the 100-node problem set, we reduce the optimality gap from 18.845% to 2.622%, a 7x improvement",
    "checked": true,
    "id": "7e827e96b96e4c359fd3636b12c925c0ee6b9c4e",
    "semantic_title": "graph neural network guided local search for the traveling salesperson problem",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=aPOpXlnV1T": {
    "title": "On the Pitfalls of Heteroscedastic Uncertainty Estimation with Probabilistic Neural Networks",
    "volume": "poster",
    "abstract": "Capturing aleatoric uncertainty is a critical part of many machine learning systems. In deep learning, a common approach to this end is to train a neural network to estimate the parameters of a heteroscedastic Gaussian distribution by maximizing the logarithm of the likelihood function under the observed data. In this work, we examine this approach and identify potential hazards associated with the use of log-likelihood in conjunction with gradient-based optimizers. First, we present a synthetic example illustrating how this approach can lead to very poor but stable parameter estimates. Second, we identify the culprit to be the log-likelihood loss, along with certain conditions that exacerbate the issue. Third, we present an alternative formulation, termed $\\beta$-NLL, in which each data point's contribution to the loss is weighted by the $\\beta$-exponentiated variance estimate. We show that using an appropriate $\\beta$ largely mitigates the issue in our illustrative example. Fourth, we evaluate this approach on a range of domains and tasks and show that it achieves considerable improvements and performs more robustly concerning hyperparameters, both in predictive RMSE and log-likelihood criteria",
    "checked": true,
    "id": "2b65a87983774bd86a362b73c6e21df1bb7cbfc7",
    "semantic_title": "on the pitfalls of heteroscedastic uncertainty estimation with probabilistic neural networks",
    "citation_count": 96,
    "authors": []
  },
  "https://openreview.net/forum?id=SlxSY2UZQT": {
    "title": "Label-Efficient Semantic Segmentation with Diffusion Models",
    "volume": "poster",
    "abstract": "Denoising diffusion probabilistic models have recently received much research attention since they outperform alternative approaches, such as GANs, and currently provide state-of-the-art generative performance. The superior performance of diffusion models has made them an appealing tool in several applications, including inpainting, super-resolution, and semantic editing. In this paper, we demonstrate that diffusion models can also serve as an instrument for semantic segmentation, especially in the setup when labeled data is scarce. In particular, for several pretrained diffusion models, we investigate the intermediate activations from the networks that perform the Markov step of the reverse diffusion process. We show that these activations effectively capture the semantic information from an input image and appear to be excellent pixel-level representations for the segmentation problem. Based on these observations, we describe a simple segmentation method, which can work even if only a few training images are provided. Our approach significantly outperforms the existing alternatives on several datasets for the same amount of human supervision",
    "checked": true,
    "id": "42f2271cebb7f272b0066c1f22d33381f139ee68",
    "semantic_title": "label-efficient semantic segmentation with diffusion models",
    "citation_count": 586,
    "authors": []
  },
  "https://openreview.net/forum?id=uPv9Y3gmAI5": {
    "title": "Language model compression with weighted low-rank factorization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d151ced8700d84a2efe411a234a4cb2c595e8ca9",
    "semantic_title": "language model compression with weighted low-rank factorization",
    "citation_count": 139,
    "authors": []
  },
  "https://openreview.net/forum?id=QuObT9BTWo": {
    "title": "Pareto Set Learning for Neural Multi-Objective Combinatorial Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "aab686e814f078144941cff57a5e0aaf4989aa97",
    "semantic_title": "pareto set learning for neural multi-objective combinatorial optimization",
    "citation_count": 83,
    "authors": []
  },
  "https://openreview.net/forum?id=8la28hZOwug": {
    "title": "Prototypical Contrastive Predictive Coding",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "da4deb6a7594782d3aa507f9121c3375b5f9f2a2",
    "semantic_title": "prototypical contrastive predictive coding",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=cZAi1yWpiXQ": {
    "title": "Adversarial Robustness Through the Lens of Causality",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "68b7532be018dbaf4fe7f500b19b46fd31b82ab9",
    "semantic_title": "adversarial robustness through the lens of causality",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=9NVd-DMtThY": {
    "title": "Distributionally Robust Fair Principal Components via Geodesic Descents",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bbf64d3561c3dd89c254483bf10facdfc322907f",
    "semantic_title": "distributionally robust fair principal components via geodesic descents",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=wkMG8cdvh7-": {
    "title": "Understanding and Improving Graph Injection Attack by Promoting Unnoticeability",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1b24fbc2189aca1d17f66eac7c8b09397eaf336f",
    "semantic_title": "understanding and improving graph injection attack by promoting unnoticeability",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=swiyAeGzFhQ": {
    "title": "Learning to Guide and to be Guided in the Architect-Builder Problem",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5df950eadbdd3c9955d1212f91165a2914f89887",
    "semantic_title": "learning to guide and to be guided in the architect-builder problem",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=iPHLcmtietq": {
    "title": "Phase Collapse in Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "85df0f58599676a3fe663f7601bdcc2eb79e49cc",
    "semantic_title": "phase collapse in neural networks",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=TBpg4PnXhYH": {
    "title": "SPIRAL: Self-supervised Perturbation-Invariant Representation Learning for Speech Pre-Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0228d04512e04306ed5971117a4e07d11df458b8",
    "semantic_title": "spiral: self-supervised perturbation-invariant representation learning for speech pre-training",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=J_PHjw4gvXJ": {
    "title": "Improving the Accuracy of Learning Example Weights for Imbalance Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1c90744253b2737712da17230d1c7f12a17f26c8",
    "semantic_title": "improving the accuracy of learning example weights for imbalance classification",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=Czsdv-S4-w9": {
    "title": "Generating Videos with Dynamics-aware Implicit Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ef4741b12cb8f01bcc68708c8cffcdc4237383f7",
    "semantic_title": "generating videos with dynamics-aware implicit generative adversarial networks",
    "citation_count": 213,
    "authors": []
  },
  "https://openreview.net/forum?id=0cgU-BZp2ky": {
    "title": "Efficient Learning of Safe Driving Policy via Human-AI Copilot Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f3e8c42b56bf8406726eaaccc68398df2eaccc61",
    "semantic_title": "efficient learning of safe driving policy via human-ai copilot optimization",
    "citation_count": 65,
    "authors": []
  },
  "https://openreview.net/forum?id=OjPmfr9GkVv": {
    "title": "Enhancing Cross-lingual Transfer by Manifold Mixup",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fd708dc43c0ed70ed03b2818a3f50fedda6d7f6e",
    "semantic_title": "enhancing cross-lingual transfer by manifold mixup",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=74x5BXs4bWD": {
    "title": "Evolutionary Diversity Optimization with Clustering-based Selection for Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fed0701afdfa6896057f7d04bd30ab1328eff110",
    "semantic_title": "evolutionary diversity optimization with clustering-based selection for reinforcement learning",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=_Wzj0J2xs2D": {
    "title": "CURVATURE-GUIDED DYNAMIC SCALE NETWORKS FOR MULTI-VIEW STEREO",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "60896adc1336150d63e896a24a3cf150092c15f3",
    "semantic_title": "curvature-guided dynamic scale networks for multi-view stereo",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=KLaDXLAzzFT": {
    "title": "Near-optimal Offline Reinforcement Learning with Linear Representation: Leveraging Variance Information with Pessimism",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "dc94a18c69c71434590299fad9aeb2c932f45e15",
    "semantic_title": "near-optimal offline reinforcement learning with linear representation: leveraging variance information with pessimism",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=RftryyYyjiG": {
    "title": "Exploring extreme parameter compression for pre-trained language models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "05d70085d1b580b2369942410ae77c48d1eeacca",
    "semantic_title": "exploring extreme parameter compression for pre-trained language models",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=Sq0-tgDyHe4": {
    "title": "Local Feature Swapping for Generalization in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "56a73bb9748e4a09994fe8aedc645eded638109e",
    "semantic_title": "local feature swapping for generalization in reinforcement learning",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=lL3lnMbR4WU": {
    "title": "Open-vocabulary Object Detection via Vision and Language Knowledge Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cf9b8da26d9b92e75ba49616ed2a1033f59fce14",
    "semantic_title": "open-vocabulary object detection via vision and language knowledge distillation",
    "citation_count": 1031,
    "authors": []
  },
  "https://openreview.net/forum?id=EBn0uInJZWh": {
    "title": "Model-Based Offline Meta-Reinforcement Learning with Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "02ad21eea9ec32783ba529487e74a76e85499a53",
    "semantic_title": "model-based offline meta-reinforcement learning with regularization",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=YVPBh4k78iZ": {
    "title": "Scale Mixtures of Neural Network Gaussian Processes",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "162f332bf7c0f51cc1cab62ce72f138a45b25029",
    "semantic_title": "scale mixtures of neural network gaussian processes",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=YX0lrvdPQc": {
    "title": "A Johnson-Lindenstrauss Framework for Randomly Initialized CNNs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d6b95c8bde98e25695a8140a6d21425e5d39ed6f",
    "semantic_title": "a johnson-lindenstrauss framework for randomly initialized cnns",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=Vr_BTpw3wz": {
    "title": "Hindsight: Posterior-guided training of retrievers for improved open-ended generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "753fd6952c9f06f3bbd46e37129acc3f7a984896",
    "semantic_title": "hindsight: posterior-guided training of retrievers for improved open-ended generation",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=k9bx1EfHI_-": {
    "title": "Self-Supervised Graph Neural Networks for Improved Electroencephalographic Seizure Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0a4d5fdfaba62390b25c725badffee524bbcf0a6",
    "semantic_title": "self-supervised graph neural networks for improved electroencephalographic seizure analysis",
    "citation_count": 115,
    "authors": []
  },
  "https://openreview.net/forum?id=cw-EmNq5zfD": {
    "title": "Group-based Interleaved Pipeline Parallelism for Large-scale DNN Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b49d495de440b5afffa0f1fde6bd8a832f9ed9cb",
    "semantic_title": "group-based interleaved pipeline parallelism for large-scale dnn training",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=nc0ETaieux": {
    "title": "Minimax Optimality (Probably) Doesn't Imply Distribution Learning for GANs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0e442e1d0e37abca83e83ca02ba7a74088daa55c",
    "semantic_title": "minimax optimality (probably) doesn't imply distribution learning for gans",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=RCZqv9NXlZ": {
    "title": "Offline Reinforcement Learning with Value-based Episodic Memory",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b7ba291af983b5e31f3adfa9a0ceb7a7b3114c7f",
    "semantic_title": "offline reinforcement learning with value-based episodic memory",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=C54V-xTWfi": {
    "title": "MonoDistill: Learning Spatial Features for Monocular 3D Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cfb38cc7825ea0e2260191771057213b6cf14bed",
    "semantic_title": "monodistill: learning spatial features for monocular 3d object detection",
    "citation_count": 110,
    "authors": []
  },
  "https://openreview.net/forum?id=vkaMaq95_rX": {
    "title": "EXACT: Scalable Graph Neural Networks Training via Extreme Activation Compression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "72567f3953c5479e09aacf48dfd888e38000b699",
    "semantic_title": "exact: scalable graph neural networks training via extreme activation compression",
    "citation_count": 66,
    "authors": []
  },
  "https://openreview.net/forum?id=MP904TiHqJ-": {
    "title": "Provably convergent quasistatic dynamics for mean-field two-player zero-sum games",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4e94df220b460da799073ab22f1102f602374461",
    "semantic_title": "provably convergent quasistatic dynamics for mean-field two-player zero-sum games",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=0RqDp8FCW5Z": {
    "title": "W-CTC: a Connectionist Temporal Classification Loss with Wild Cards",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7467b27d83a4f1f7adb9f5e90a8e49282c5177ea",
    "semantic_title": "w-ctc: a connectionist temporal classification loss with wild cards",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=Q83vFlie_Pr": {
    "title": "Bandit Learning with Joint Effect of Incentivized Sampling, Delayed Sampling Feedback, and Self-Reinforcing User Preferences",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c9fbfabd7250e1ab935b0486cf7db9783eff35a9",
    "semantic_title": "bandit learning with joint effect of incentivized sampling, delayed sampling feedback, and self-reinforcing user preferences",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=rWXfFogxRJN": {
    "title": "AdaAug: Learning Class- and Instance-adaptive Data Augmentation Policies",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "84dbb0f24bc6856ff353d2f5e393e9b756fe4531",
    "semantic_title": "adaaug: learning class- and instance-adaptive data augmentation policies",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=SaKO6z6Hl0c": {
    "title": "Unsupervised Semantic Segmentation by Distilling Feature Correspondences",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "918646809c7e22c94994fe80c3a2840b4c951a3c",
    "semantic_title": "unsupervised semantic segmentation by distilling feature correspondences",
    "citation_count": 268,
    "authors": []
  },
  "https://openreview.net/forum?id=TqNsv1TuCX9": {
    "title": "Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning",
    "volume": "poster",
    "abstract": "Visual search, recommendation, and contrastive similarity learning power technologies that impact billions of users worldwide. Modern model architectures can be complex and difficult to interpret, and there are several competing techniques one can use to explain a search engine's behavior. We show that the theory of fair credit assignment provides a unique axiomatic solution that generalizes several existing recommendation- and metric-explainability techniques in the literature. Using this formalism, we show when existing approaches violate \"fairness\" and derive methods that sidestep these shortcomings and naturally handle counterfactual information. More specifically, we show existing approaches implicitly approximate second-order Shapley-Taylor indices and extend CAM, GradCAM, LIME, SHAP, SBSM, and other methods to search engines. These extensions can extract pairwise correspondences between images from trained opaque-box models. We also introduce a fast kernel-based method for estimating Shapley-Taylor indices that require orders of magnitude fewer function evaluations to converge. Finally, we show that these game-theoretic measures yield more consistent explanations for image similarity architectures",
    "checked": true,
    "id": "4922e89201273b4040cfa5c90a5ab2906d725146",
    "semantic_title": "axiomatic explanations for visual search, retrieval, and similarity learning",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=kcwyXtt7yDJ": {
    "title": "Graph-Relational Domain Adaptation",
    "volume": "poster",
    "abstract": "Existing domain adaptation methods tend to treat every domain equally and align them all perfectly. Such uniform alignment ignores topological structures among different domains; therefore it may be beneficial for nearby domains, but not necessarily for distant domains. In this work, we relax such uniform alignment by using a domain graph to encode domain adjacency, e.g., a graph of states in the US with each state as a domain and each edge indicating adjacency, thereby allowing domains to align flexibly based on the graph structure. We generalize the existing adversarial learning framework with a novel graph discriminator using encoding-conditioned graph embeddings. Theoretical analysis shows that at equilibrium, our method recovers classic domain adaptation when the graph is a clique, and achieves non-trivial alignment for other types of graphs. Empirical results show that our approach successfully generalizes uniform alignment, naturally incorporates domain information represented by graphs, and improves upon existing domain adaptation methods on both synthetic and real-world datasets",
    "checked": true,
    "id": "d590b67abd90934abc7d75224eb3594064383afc",
    "semantic_title": "graph-relational domain adaptation",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=LdEhiMG9WLO": {
    "title": "Revisit Kernel Pruning with Lottery Regulated Grouped Convolutions",
    "volume": "poster",
    "abstract": "Structured pruning methods which are capable of delivering a densely pruned network are among the most popular techniques in the realm of neural network pruning, where most methods prune the original network at a filter or layer level. Although such methods may provide immediate compression and acceleration benefits, we argue that the blanket removal of an entire filter or layer may result in undesired accuracy loss. In this paper, we revisit the idea of kernel pruning (to only prune one or several $k \\times k$ kernels out of a 3D-filter), a heavily overlooked approach under the context of structured pruning. This is because kernel pruning will naturally introduce sparsity to filters within the same convolutional layer ‚Äî thus, making the remaining network no longer dense. We address this problem by proposing a versatile grouped pruning framework where we first cluster filters from each convolutional layer into equal-sized groups, prune the grouped kernels we deem unimportant from each filter group, then permute the remaining filters to form a densely grouped convolutional architecture (which also enables the parallel computing capability) for fine-tuning. Specifically, we consult empirical findings from a series of literature regarding $\\textit{Lottery Ticket Hypothesis}$ to determine the optimal clustering scheme per layer, and develop a simple yet cost-efficient greedy approximation algorithm to determine which group kernels to keep within each filter group. Extensive experiments also demonstrate our method often outperforms comparable SOTA methods with lesser data augmentation needed, smaller fine-tuning budget required, and sometimes even much simpler procedure executed (e.g., one-shot v. iterative). Please refer to our GitHub repository (https://github.com/choH/lottery_regulated_grouped_kernel_pruning) for code",
    "checked": true,
    "id": "2930b9c8e276fc530fdeae83ab76b05348a8a88b",
    "semantic_title": "revisit kernel pruning with lottery regulated grouped convolutions",
    "citation_count": 35,
    "authors": []
  },
  "https://openreview.net/forum?id=LedObtLmCjS": {
    "title": "Bi-linear Value Networks for Multi-goal Reinforcement Learning",
    "volume": "poster",
    "abstract": "Universal value functions are a core component of off-policy multi-goal reinforcement learning. The de-facto paradigm is to approximate Q(s, a, g) using monolithic neural networks which lack inductive biases to produce complex interactions between the state s and the goal g. In this work, we propose a bilinear decomposition that represents the Q-value via a low-rank approximation in the form of a dot product between two vector fields. The first vector field, f(s, a), captures the environment's local dynamics at the state s; whereas the second component, œï(s, g), captures the global relationship between the current state and the goal. We show that our bilinear decomposition scheme improves sample efficiency over the original monolithic value approximators, and transfer better to unseen goals. We demonstrate significant learning speed-up over a variety of tasks on a simulated robot arm, and the challenging task of dexterous manipulation with a Shadow hand",
    "checked": true,
    "id": "eae2cf052f034a6cbbb35f56cb3a86329cf1edff",
    "semantic_title": "bi-linear value networks for multi-goal reinforcement learning",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=BK-4qbGgIE3": {
    "title": "No One Representation to Rule Them All: Overlapping Features of Training Methods",
    "volume": "poster",
    "abstract": "Despite being able to capture a range of features of the data, high accuracy models trained with supervision tend to make similar predictions. This seemingly implies that high-performing models share similar biases regardless of training methodology, which would limit ensembling benefits and render low-accuracy models as having little practical use. Against this backdrop, recent work has developed quite different training techniques, such as large-scale contrastive learning, yielding competitively high accuracy on generalization and robustness benchmarks. This motivates us to revisit the assumption that models necessarily learn similar functions. We conduct a large-scale empirical study of models across hyper-parameters, architectures, frameworks, and datasets. We find that model pairs that diverge more in training methodology display categorically different generalization behavior, producing increasingly uncorrelated errors. We show these models specialize in subdomains of the data, leading to higher ensemble performance: with just 2 models (each with ImageNet accuracy \\~76.5\\%), we can create ensembles with 83.4\\% (+7\\% boost). Surprisingly, we find that even significantly low-accuracy models can be used to improve high-accuracy models. Finally, we show diverging training methodology yield representations that capture overlapping (but not supersetting) feature sets which, when combined, lead to increased downstream performance",
    "checked": true,
    "id": "5bcc379da187b69d705a81e93bf5ddbb90cda1b1",
    "semantic_title": "no one representation to rule them all: overlapping features of training methods",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=IfNu7Dr-3fQ": {
    "title": "Generalized Kernel Thinning",
    "volume": "poster",
    "abstract": "The kernel thinning (KT) algorithm of Dwivedi and Mackey (2021) compresses a probability distribution more effectively than independent sampling by targeting a reproducing kernel Hilbert space (RKHS) and leveraging a less smooth square-root kernel. Here we provide four improvements. First, we show that KT applied directly to the target RKHS yields tighter, dimension-free guarantees for any kernel, any distribution, and any fixed function in the RKHS. Second, we show that, for analytic kernels like Gaussian, inverse multiquadric, and sinc, target KT admits maximum mean discrepancy (MMD) guarantees comparable to or better than those of square-root KT without making explicit use of a square-root kernel. Third, we prove that KT with a fractional power kernel yields better-than-Monte-Carlo MMD guarantees for non-smooth kernels, like Laplace and Matern, that do not have square-roots. Fourth, we establish that KT applied to a sum of the target and power kernels (a procedure we call KT+) simultaneously inherits the improved MMD guarantees of power KT and the tighter individual function guarantees of target KT. In our experiments with target KT and KT+, we witness significant improvements in integration error even in 100 dimensions and when compressing challenging differential equation posteriors",
    "checked": true,
    "id": "8fcff9e70e8dd39b7b04807487a13069aa748214",
    "semantic_title": "generalized kernel thinning",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=zf_Ll3HZWgy": {
    "title": "How Much Can CLIP Benefit Vision-and-Language Tasks?",
    "volume": "poster",
    "abstract": "Most existing Vision-and-Language (V&L) models rely on pre-trained visual encoders, using a relatively small set of manually-annotated data (as compared to web-crawled data), to perceive the visual world. However, it has been observed that large-scale pretraining usually can result in better generalization performance, e.g., CLIP (Contrastive Language-Image Pre-training), trained on a massive amount of image-caption pairs, has shown a strong zero-shot capability on various vision tasks. To further study the advantage brought by CLIP, we propose to use CLIP as the visual encoder in various V&L models in two typical scenarios: 1) plugging CLIP into task-specific fine-tuning; 2) combining CLIP with V&L pre-training and transferring to downstream tasks. We show that CLIP significantly outperforms widely-used visual encoders trained with in-domain annotated data, such as BottomUp-TopDown. We achieve competitive or better results on diverse V&L tasks, while establishing new state-of-the-art results on Visual Question Answering, Visual Entailment, and V&L Navigation tasks",
    "checked": true,
    "id": "8f167ec1149921fac63b1ea855443de109bb013a",
    "semantic_title": "how much can clip benefit vision-and-language tasks?",
    "citation_count": 440,
    "authors": []
  },
  "https://openreview.net/forum?id=3tbDrs77LJ5": {
    "title": "Large Learning Rate Tames Homogeneity: Convergence and Balancing Effect",
    "volume": "poster",
    "abstract": "Recent empirical advances show that training deep models with large learning rate often improves generalization performance. However, theoretical justifications on the benefits of large learning rate are highly limited, due to challenges in analysis. In this paper, we consider using Gradient Descent (GD) with a large learning rate on a homogeneous matrix factorization problem, i.e., $\\min_{X, Y} \\|A - XY^\\top\\|_{\\sf F}^2$. We prove a convergence theory for constant large learning rates well beyond $2/L$, where $L$ is the largest eigenvalue of Hessian at the initialization. Moreover, we rigorously establish an implicit bias of GD induced by such a large learning rate, termed `balancing', meaning that magnitudes of $X$ and $Y$ at the limit of GD iterations will be close even if their initialization is significantly unbalanced. Numerical experiments are provided to support our theory",
    "checked": true,
    "id": "0b4108af924be64650e205d77f10e57aacbcbd5f",
    "semantic_title": "large learning rate tames homogeneity: convergence and balancing effect",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=l5aSHXi8jG5": {
    "title": "Demystifying Limited Adversarial Transferability in Automatic Speech Recognition Systems",
    "volume": "poster",
    "abstract": "The targeted transferability of adversarial samples enables attackers to exploit black-box models in the real-world. The most popular method to produce these adversarial samples is optimization attacks, which have been shown to achieve a high level of transferability in some domains. However, recent research has demonstrated that these attack samples fail to transfer when applied to Automatic Speech Recognition Systems (ASRs). In this paper, we investigate factors preventing this transferability via exhaustive experimentation. To do so, we perform an ablation study on each stage of the ASR pipeline. We discover and quantify six factors (i.e., input type, MFCC, RNN, output type, and vocabulary and sequence sizes) that impact the targeted transferability of optimization attacks against ASRs. Future research can leverage our findings to build ASRs that are more robust to other transferable attack types (e.g., signal processing attacks), or to modify architectures in other domains to reduce their exposure to targeted transferability of optimization attacks",
    "checked": true,
    "id": "d4a548aefe19fe788fa89641c5eeea026c3a8078",
    "semantic_title": "demystifying limited adversarial transferability in automatic speech recognition systems",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=kSwqMH0zn1F": {
    "title": "PipeGCN: Efficient Full-Graph Training of Graph Convolutional Networks with Pipelined Feature Communication",
    "volume": "poster",
    "abstract": "Graph Convolutional Networks (GCNs) is the state-of-the-art method for learning graph-structured data, and training large-scale GCNs requires distributed training across multiple accelerators such that each accelerator is able to hold a partitioned subgraph. However, distributed GCN training incurs prohibitive overhead of communicating node features and feature gradients among partitions for every GCN layer during each training iteration, limiting the achievable training efficiency and model scalability. To this end, we propose PipeGCN, a simple yet effective scheme that hides the communication overhead by pipelining inter-partition communication with intra-partition computation. It is non-trivial to pipeline for efficient GCN training, as communicated node features/gradients will become stale and thus can harm the convergence, negating the pipeline benefit. Notably, little is known regarding the convergence rate of GCN training with both stale features and stale feature gradients. This work not only provides a theoretical convergence analysis but also finds the convergence rate of PipeGCN to be close to that of the vanilla distributed GCN training without any staleness. Furthermore, we develop a smoothing method to further improve PipeGCN's convergence. Extensive experiments show that PipeGCN can largely boost the training throughput (1.7√ó~28.5√ó) while achieving the same accuracy as its vanilla counterpart and existing full-graph training methods. The code is available at https://github.com/RICE-EIC/PipeGCN",
    "checked": true,
    "id": "286d371febea99ec19044e69e163e8bd53137a7f",
    "semantic_title": "pipegcn: efficient full-graph training of graph convolutional networks with pipelined feature communication",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=7inCJ3MhXt3": {
    "title": "Learning Neural Contextual Bandits through Perturbed Rewards",
    "volume": "poster",
    "abstract": "Thanks to the power of representation learning, neural contextual bandit algorithms demonstrate remarkable performance improvement against their classical counterparts. But because their exploration has to be performed in the entire neural network parameter space to obtain nearly optimal regret, the resulting computational cost is prohibitively high. We propose to perturb the rewards when updating the neural network to eliminate the need of explicit exploration and the corresponding computational overhead. We prove that a $\\tilde{O}(\\tilde{d}\\sqrt{T})$ regret upper bound is still achievable under standard regularity conditions, where $T$ is the number of rounds of interactions and $\\tilde{d}$ is the effective dimension of a neural tangent kernel matrix. Extensive comparisons with several benchmark contextual bandit algorithms, including two recent neural contextual bandit models, demonstrate the effectiveness and computational efficiency of our proposed neural bandit algorithm",
    "checked": true,
    "id": "efd619218ab764f78acf070a9c39adf376f0abd6",
    "semantic_title": "learning neural contextual bandits through perturbed rewards",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=MeeQkFYVbzW": {
    "title": "Adversarial Unlearning of Backdoors via Implicit Hypergradient",
    "volume": "poster",
    "abstract": "We propose a minimax formulation for removing backdoors from a given poisoned model based on a small set of clean data. This formulation encompasses much of prior work on backdoor removal. We propose the Implicit Backdoor Adversarial Unlearning (I-BAU) algorithm to solve the minimax. Unlike previous work, which breaks down the minimax into separate inner and outer problems, our algorithm utilizes the implicit hypergradient to account for the interdependence between inner and outer optimization. We theoretically analyze its convergence and the generalizability of the robustness gained by solving minimax on clean data to unseen test data. In our evaluation, we compare I-BAU with six state-of-art backdoor defenses on eleven backdoor attacks over two datasets and various attack settings, including the common setting where the attacker targets one class as well as important but underexplored settings where multiple classes are targeted. I-BAU's performance is comparable to and most often significantly better than the best baseline. Particularly, its performance is more robust to the variation on triggers, attack settings, poison ratio, and clean data size. Moreover, I-BAU requires less computation to take effect; particularly, it is more than $13\\times$ faster than the most efficient baseline in the single-target attack setting. Furthermore, it can remain effective in the extreme case where the defender can only access 100 clean samples---a setting where all the baselines fail to produce acceptable results",
    "checked": true,
    "id": "10202457eca5845ce8f72d9684721498f76e6150",
    "semantic_title": "adversarial unlearning of backdoors via implicit hypergradient",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hjd-kcpDpf2": {
    "title": "Maximizing Ensemble Diversity in Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "Modern deep reinforcement learning (DRL) has been successful in solving a range of challenging sequential decision-making problems. Most of these algorithms use an ensemble of neural networks as their backbone structure and benefit from the diversity among the neural networks to achieve optimal results. Unfortunately, the members of the ensemble can converge to the same point either the parametric space or representation space during the training phase, therefore, losing all the leverage of an ensemble. In this paper, we describe Maximize Ensemble Diversity in Reinforcement Learning (MED-RL), a set of regularization methods inspired from the economics and consensus optimization to improve diversity in the ensemble-based deep reinforcement learning methods by encouraging inequality between the networks during training. We integrated MED-RL in five of the most common ensemble-based deep RL algorithms for both continuous and discrete control tasks and evaluated on six Mujoco environments and six Atari games. Our results show that MED-RL augmented algorithms outperform their un-regularized counterparts significantly and in some cases achieved more than 300$\\%$ in performance gains",
    "checked": true,
    "id": "b2960eeaab35c5a02d8a8dfd7636402ec0d2b7cd",
    "semantic_title": "maximizing ensemble diversity in deep reinforcement learning",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=wTTjnvGphYj": {
    "title": "Graph Neural Networks with Learnable Structural and Positional Representations",
    "volume": "poster",
    "abstract": "Graph neural networks (GNNs) have become the standard learning architectures for graphs. GNNs have been applied to numerous domains ranging from quantum chemistry, recommender systems to knowledge graphs and natural language processing. A major issue with arbitrary graphs is the absence of canonical positional information of nodes, which decreases the representation power of GNNs to distinguish e.g. isomorphic nodes and other graph symmetries. An approach to tackle this issue is to introduce Positional Encoding (PE) of nodes, and inject it into the input layer, like in Transformers. Possible graph PE are Laplacian eigenvectors. In this work, we propose to decouple structural and positional representations to make easy for the network to learn these two essential properties. We introduce a novel generic architecture which we call \\texttt{LSPE} (Learnable Structural and Positional Encodings). We investigate several sparse and fully-connected (Transformer-like) GNNs, and observe a performance increase for molecular datasets, from $1.79\\%$ up to $64.14\\%$ when considering learnable PE for both GNN classes",
    "checked": true,
    "id": "454304628bf10f02aba1c2cfc95891e94d09208e",
    "semantic_title": "graph neural networks with learnable structural and positional representations",
    "citation_count": 373,
    "authors": []
  },
  "https://openreview.net/forum?id=085y6YPaYjP": {
    "title": "Zero-Shot Self-Supervised Learning for MRI Reconstruction",
    "volume": "poster",
    "abstract": "Deep learning (DL) has emerged as a powerful tool for accelerated MRI reconstruction, but often necessitates a database of fully-sampled measurements for training. Recent self-supervised and unsupervised learning approaches enable training without fully-sampled data. However, a database of undersampled measurements may not be available in many scenarios, especially for scans involving contrast or translational acquisitions in development. Moreover, recent studies show that database-trained models may not generalize well when the unseen measurements differ in terms of sampling pattern, acceleration rate, SNR, image contrast, and anatomy. Such challenges necessitate a new methodology to enable subject-specific DL MRI reconstruction without external training datasets, since it is clinically imperative to provide high-quality reconstructions that can be used to identify lesions/disease for $\\textit{every individual}$. In this work, we propose a zero-shot self-supervised learning approach to perform subject-specific accelerated DL MRI reconstruction to tackle these issues. The proposed approach partitions the available measurements from a single scan into three disjoint sets. Two of these sets are used to enforce data consistency and define loss during training for self-supervision, while the last set serves to self-validate, establishing an early stopping criterion. In the presence of models pre-trained on a database with different image characteristics, we show that the proposed approach can be combined with transfer learning for faster convergence time and reduced computational complexity",
    "checked": true,
    "id": "7c830468dceeb9ab4cc8ca12b78165745e6fdca9",
    "semantic_title": "zero-shot self-supervised learning for mri reconstruction",
    "citation_count": 82,
    "authors": []
  },
  "https://openreview.net/forum?id=mwdfai8NBrJ": {
    "title": "Policy Smoothing for Provably Robust Reinforcement Learning",
    "volume": "poster",
    "abstract": "The study of provable adversarial robustness for deep neural networks (DNNs) has mainly focused on $\\textit{static}$ supervised learning tasks such as image classification. However, DNNs have been used extensively in real-world $\\textit{adaptive}$ tasks such as reinforcement learning (RL), making such systems vulnerable to adversarial attacks as well. Prior works in provable robustness in RL seek to certify the behaviour of the victim policy at every time-step against a non-adaptive adversary using methods developed for the static setting. But in the real world, an RL adversary can infer the defense strategy used by the victim agent by observing the states, actions, etc. from previous time-steps and adapt itself to produce stronger attacks in future steps (e.g., by focusing more on states critical to the agent's performance). We present an efficient procedure, designed specifically to defend against an adaptive RL adversary, that can directly certify the total reward without requiring the policy to be robust at each time-step. Focusing on randomized smoothing based defenses, our main theoretical contribution is to prove an $\\textit{adaptive version}$ of the Neyman-Pearson Lemma -- a key lemma for smoothing-based certificates -- where the adversarial perturbation at a particular time can be a stochastic function of current and previous observations and states as well as previous actions. Building on this result, we propose $\\textit{policy smoothing}$ where the agent adds a Gaussian noise to its observation at each time-step before passing it through the policy function. Our robustness certificates guarantee that the final total reward obtained by policy smoothing remains above a certain threshold, even though the actions at intermediate time-steps may change under the attack. We show that our certificates are $\\textit{tight}$ by constructing a worst-case scenario that achieves the bounds derived in our analysis. Our experiments on various environments like Cartpole, Pong, Freeway and Mountain Car show that our method can yield meaningful robustness guarantees in practice",
    "checked": true,
    "id": "7f2f8042750df1be7562023760148a391d247904",
    "semantic_title": "policy smoothing for provably robust reinforcement learning",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=gICys3ITSmj": {
    "title": "The Close Relationship Between Contrastive Learning and Meta-Learning",
    "volume": "poster",
    "abstract": "Contrastive learning has recently taken off as a paradigm for learning from unlabeled data. In this paper, we discuss the close relationship between contrastive learning and meta-learning under a certain task distribution. We complement this observation by showing that established meta-learning methods, such as Prototypical Networks, achieve comparable performance to SimCLR when paired with this task distribution. This relationship can be leveraged by taking established techniques from meta-learning, such as task-based data augmentation, and showing that they benefit contrastive learning as well. These tricks also benefit state-of-the-art self-supervised learners without using negative pairs such as BYOL, which achieves 94.6\\% accuracy on CIFAR-10 using a self-supervised ResNet-18 feature extractor trained with our meta-learning tricks. We conclude that existing advances designed for contrastive learning or meta-learning can be exploited to benefit the other, and it is better for contrastive learning researchers to take lessons from the meta-learning literature (and vice-versa) than to reinvent the wheel",
    "checked": true,
    "id": "5942115e923dcd5b3eae63865a59c05160ad1ad7",
    "semantic_title": "the close relationship between contrastive learning and meta-learning",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=rS9-7AuPKWK": {
    "title": "Towards Understanding Generalization via Decomposing Excess Risk Dynamics",
    "volume": "poster",
    "abstract": "Generalization is one of the fundamental issues in machine learning. However, traditional techniques like uniform convergence may be unable to explain generalization under overparameterization \\citep{nagarajan2019uniform}. As alternative approaches, techniques based on stability analyze the training dynamics and derive algorithm-dependent generalization bounds. Unfortunately, the stability-based bounds are still far from explaining the surprising generalization in deep learning since neural networks usually suffer from unsatisfactory stability. This paper proposes a novel decomposition framework to improve the stability-based bounds via a more fine-grained analysis of the signal and noise, inspired by the observation that neural networks converge relatively slowly when fitting noise (which indicates better stability). Concretely, we decompose the excess risk dynamics and apply the stability-based bound only on the noise component. The decomposition framework performs well in both linear regimes (overparameterized linear regression) and non-linear regimes (diagonal matrix recovery). Experiments on neural networks verify the utility of the decomposition framework",
    "checked": true,
    "id": "83add0326380fe4a3324d0eb838693ea4f3213b7",
    "semantic_title": "towards understanding generalization via decomposing excess risk dynamics",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=ATUh28lnSuW": {
    "title": "Graph Auto-Encoder via Neighborhood Wasserstein Reconstruction",
    "volume": "poster",
    "abstract": "Graph neural networks (GNNs) have drawn significant research attention recently, mostly under the setting of semi-supervised learning. When task-agnostic representations are preferred or supervision is simply unavailable, the auto-encoder framework comes in handy with a natural graph reconstruction objective for unsupervised GNN training. However, existing graph auto-encoders are designed to reconstruct the direct links, so GNNs trained in this way are only optimized towards proximity-oriented graph mining tasks, and will fall short when the topological structures matter. In this work, we revisit the graph encoding process of GNNs which essentially learns to encode the neighborhood information of each node into an embedding vector, and propose a novel graph decoder to reconstruct the entire neighborhood information regarding both proximity and structure via Neighborhood Wasserstein Reconstruction (NWR). Specifically, from the GNN embedding of each node, NWR jointly predicts its node degree and neighbor feature distribution, where the distribution prediction adopts an optimal-transport loss based on the Wasserstein distance. Extensive experiments on both synthetic and real-world network datasets show that the unsupervised node representations learned with NWR have much more advantageous in structure-oriented graph mining tasks, while also achieving competitive performance in proximity-oriented ones",
    "checked": true,
    "id": "2ab0630d0a5e209708e03dc6e21a477c3099f282",
    "semantic_title": "graph auto-encoder via neighborhood wasserstein reconstruction",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=nRj0NcmSuxb": {
    "title": "FairCal: Fairness Calibration for Face Verification",
    "volume": "poster",
    "abstract": "Despite being widely used, face recognition models suffer from bias: the probability of a false positive (incorrect face match) strongly depends on sensitive attributes such as the ethnicity of the face. As a result, these models can disproportionately and negatively impact minority groups, particularly when used by law enforcement. The majority of bias reduction methods have several drawbacks: they use an end-to-end retraining approach, may not be feasible due to privacy issues, and often reduce accuracy. An alternative approach is post-processing methods that build fairer decision classifiers using the features of pre-trained models, thus avoiding the cost of retraining. However, they still have drawbacks: they reduce accuracy (AGENDA, FTC), or require retuning for different false positive rates (FSN). In this work, we introduce the Fairness Calibration (FairCal) method, a post-training approach that simultaneously: (i) increases model accuracy (improving the state-of-the-art), (ii) produces fairly-calibrated probabilities, (iii) significantly reduces the gap in the false positive rates, (iv) does not require knowledge of the sensitive attribute, and (v) does not require retraining, training an additional model or retuning. We apply it to the task of Face Verification, and obtain state-of-the-art results with all the above advantages",
    "checked": true,
    "id": "3cf69ac8cae3d365db9139faee93d1631f42b5ed",
    "semantic_title": "faircal: fairness calibration for face verification",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=k7-s5HSSPE5": {
    "title": "Cross-Lingual Transfer with Class-Weighted Language-Invariant Representations",
    "volume": "poster",
    "abstract": "Recent advances in neural modeling have produced deep multilingual language models capable of extracting cross-lingual knowledge from non-parallel texts and enabling zero-shot downstream transfer. While their success is often attributed to shared representations, quantitative analyses are limited. Towards a better understanding, through empirical analyses, we show that the invariance of feature representations across languages‚Äîan effect of shared representations‚Äîstrongly correlates with transfer performance. We also observe that distributional shifts in class priors between source and target language task data negatively affect performance, a largely overlooked issue that could cause negative transfer with existing unsupervised approaches. Based on these findings, we propose and evaluate a method for unsupervised transfer, called importance-weighted domain alignment (IWDA), that performs representation alignment with prior shift estimation and correction using unlabeled target language task data. Experiments demonstrate its superiority under large prior shifts, and show further performance gains when combined with existing semi-supervised learning techniques",
    "checked": true,
    "id": "415f924c5c79e300891881af367e4d77602f9f39",
    "semantic_title": "cross-lingual transfer with class-weighted language-invariant representations",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=PgNEYaIc81Q": {
    "title": "ComPhy: Compositional Physical Reasoning of Objects and Events from Videos",
    "volume": "poster",
    "abstract": "Objects' motions in nature are governed by complex interactions and their properties. While some properties, such as shape and material, can be identified via the object's visual appearances, others like mass and electric charge are not directly visible. The compositionality between the visible and hidden properties poses unique challenges for AI models to reason from the physical world, whereas humans can effortlessly infer them with limited observations. Existing studies on video reasoning mainly focus on visually observable elements such as object appearance, movement, and contact interaction. In this paper, we take an initial step to highlight the importance of inferring the hidden physical properties not directly observable from visual appearances, by introducing the Compositional Physical Reasoning (ComPhy) dataset. For a given set of objects, ComPhy includes few videos of them moving and interacting under different initial conditions. The model is evaluated based on its capability to unravel the compositional hidden properties, such as mass and charge, and use this knowledge to answer a set of questions posted on one of the videos. Evaluation results of several state-of-the-art video reasoning models on ComPhy show unsatisfactory performance as they fail to capture these hidden properties. We further propose an oracle neural-symbolic framework named Compositional Physics Learner (CPL), combining visual perception, physical property learning, dynamic prediction, and symbolic execution into a unified framework. CPL can effectively identify objects' physical properties from their interactions and predict their dynamics to answer questions",
    "checked": true,
    "id": "e0c2db93e1aa2c9f089423390169cb60bc175ba3",
    "semantic_title": "comphy: compositional physical reasoning of objects and events from videos",
    "citation_count": 57,
    "authors": []
  },
  "https://openreview.net/forum?id=ecH2FKaARUp": {
    "title": "An Information Fusion Approach to Learning with Instance-Dependent Label Noise",
    "volume": "poster",
    "abstract": "Instance-dependent label noise (IDN) widely exists in real-world datasets and usually misleads the training of deep neural networks. Noise transition matrix (NTM) (i.e., the probability that clean labels flip into noisy labels) is used to characterize the label noise and can be adopted to bridge the gap between clean and noisy underlying data distributions. However, most instances are long-tail, i.e., the number of occurrences of each instance is usually limited, which leads to the gap between the underlying distribution and the empirical distribution. Therefore, the genuine problem caused by IDN is \\emph{empirical}, instead of underlying, \\emph{data distribution mismatch} during training. To directly tackle the empirical distribution mismatch problem, we propose \\emph{posterior transition matrix} (PTM) to posteriorly model label noise given limited observed noisy labels, which achieves \\emph{statistically consistent classifiers}. Note that even if an instance is corrupted by the same NTM, the intrinsic randomness incurs different noisy labels, and thus requires different correction methods. Motivated by this observation, we propose an \\textbf{I}nformation \\textbf{F}usion (IF) approach to fine-tune the NTM based on the estimated PTM. Specifically, we adopt the noisy labels and model predicted probabilities to estimate the PTM and then correct the NTM in \\emph{forward propagation}. Empirical evaluations on synthetic and real-world datasets demonstrate that our method is superior to the state-of-the-art approaches, and achieves more stable training for instance-dependent label noise",
    "checked": true,
    "id": "7810007373538a00d1f928670d8b0d00b31a5ec2",
    "semantic_title": "an information fusion approach to learning with instance-dependent label noise",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=rFJWoYoxrDB": {
    "title": "On Redundancy and Diversity in Cell-based Neural Architecture Search",
    "volume": "poster",
    "abstract": "Searching for the architecture cells is a dominant paradigm in NAS. However, little attention has been devoted to the analysis of the cell-based search spaces even though it is highly important for the continual development of NAS. In this work, we conduct an empirical post-hoc analysis of architectures from the popular cell-based search spaces and find that the existing search spaces contain a high degree of redundancy: the architecture performance is less sensitive to changes at large parts of the cells, and universally adopted design rules, like the explicit search for a reduction cell, significantly increase the complexities but have very limited impact on the performance. Across architectures found by a diverse set of search strategies, we consistently find that the parts of the cells that do matter for architecture performance often follow similar and simple patterns. By constraining cells to include these patterns, randomly sampled architectures can match or even outperform the state of the art. These findings cast doubts into our ability to discover truly novel architectures in the existing cell-based search spaces and, inspire our suggestions for improvement to guide future NAS research. Code is available at https://github.com/xingchenwan/cell-based-NAS-analysis",
    "checked": true,
    "id": "820a2446977e410b34d17dd73b3fe40d928db97d",
    "semantic_title": "on redundancy and diversity in cell-based neural architecture search",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=U0k7XNTiFEq": {
    "title": "Deep Learning without Shortcuts: Shaping the Kernel with Tailored Rectifiers",
    "volume": "poster",
    "abstract": "Training very deep neural networks is still an extremely challenging task. The common solution is to use shortcut connections and normalization layers, which are both crucial ingredients in the popular ResNet architecture. However, there is strong evidence to suggest that ResNets behave more like ensembles of shallower networks than truly deep ones. Recently, it was shown that deep vanilla networks (i.e.~networks without normalization layers or shortcut connections) can be trained as fast as ResNets by applying certain transformations to their activation functions. However, this method (called Deep Kernel Shaping) isn't fully compatible with ReLUs, and produces networks that overfit significantly more than ResNets on ImageNet. In this work, we rectify this situation by developing a new type of transformation that is fully compatible with a variant of ReLUs -- Leaky ReLUs. We show in experiments that our method, which introduces negligible extra computational cost, achieves validation accuracies with deep vanilla networks that are competitive with ResNets (of the same width/depth), and significantly higher than those obtained with the Edge of Chaos (EOC) method. And unlike with EOC, the validation accuracies we obtain do not get worse with depth",
    "checked": true,
    "id": "d7664cd386f70a9838c3c3c83128f4d22bb9c8f7",
    "semantic_title": "deep learning without shortcuts: shaping the kernel with tailored rectifiers",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=y_op4lLLaWL": {
    "title": "Variational autoencoders in the presence of low-dimensional data: landscape and implicit bias",
    "volume": "poster",
    "abstract": "Variational Autoencoders (VAEs) are one of the most commonly used generative models, particularly for image data. A prominent difficulty in training VAEs is data that is supported on a lower dimensional manifold. Recent work by Dai and Wipf (2020) proposes a two-stage training algorithm for VAEs, based on a conjecture that in standard VAE training the generator will converge to a solution with 0 variance which is correctly supported on the ground truth manifold. They gave partial support for this conjecture by showing that some optima of the VAE loss do satisfy this property, but did not analyze the training dynamics. In this paper, we show that for linear encoders/decoders, the conjecture is true‚Äîthat is the VAE training does recover a generator with support equal to the ground truth manifold‚Äîand does so due to an implicit bias of gradient descent rather than merely the VAE loss itself. In the nonlinear case, we show that VAE training frequently learns a higher-dimensional manifold which is a superset of the ground truth manifold",
    "checked": true,
    "id": "431d2847ab714367b10b5f429217064f4b274432",
    "semantic_title": "variational autoencoders in the presence of low-dimensional data: landscape and implicit bias",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=cuvga_CiVND": {
    "title": "No Parameters Left Behind: Sensitivity Guided Adaptive Learning Rate for Training Large Transformer Models",
    "volume": "poster",
    "abstract": "Recent research has shown the existence of significant redundancy in large Transformer models. One can prune the redundant parameters without significantly sacrificing the generalization performance. However, we question whether the redundant parameters could have contributed more if they were properly trained. To answer this question, we propose a novel training strategy that encourages all parameters to be trained sufficiently. Specifically, we adaptively adjust the learning rate for each parameter according to its sensitivity, a robust gradient-based measure reflecting this parameter's contribution to the model performance. A parameter with low sensitivity is redundant, and we improve its fitting by increasing its learning rate. In contrast, a parameter with high sensitivity is well-trained, and we regularize it by decreasing its learning rate to prevent further overfitting. We conduct extensive experiments on natural language understanding, neural machine translation, and image classification to demonstrate the effectiveness of the proposed schedule. Analysis shows that the proposed schedule indeed reduces the redundancy and improves generalization performance",
    "checked": true,
    "id": "52e8102e070dbed745c39fd518f4f6aa3daffb3c",
    "semantic_title": "no parameters left behind: sensitivity guided adaptive learning rate for training large transformer models",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=aBsCjcPu_tE": {
    "title": "SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations",
    "volume": "poster",
    "abstract": "Guided image synthesis enables everyday users to create and edit photo-realistic images with minimum effort. The key challenge is balancing faithfulness to the user inputs (e.g., hand-drawn colored strokes) and realism of the synthesized images. Existing GAN-based methods attempt to achieve such balance using either conditional GANs or GAN inversions, which are challenging and often require additional training data or loss functions for individual applications. To address these issues, we introduce a new image synthesis and editing method, Stochastic Differential Editing (SDEdit), based on a diffusion model generative prior, which synthesizes realistic images by iteratively denoising through a stochastic differential equation (SDE). Given an input image with user guide in a form of manipulating RGB pixels, SDEdit first adds noise to the input, then subsequently denoises the resulting image through the SDE prior to increase its realism. SDEdit does not require task-specific training or inversions and can naturally achieve the balance between realism and faithfulness. SDEdit outperforms state-of-the-art GAN-based methods by up to 98.09% on realism and 91.72% on overall satisfaction scores, according to a human perception study, on multiple tasks, including stroke-based image synthesis and editing as well as image compositing",
    "checked": true,
    "id": "f671a09e3e5922e6d38cb77dda8d76d5ceac2a27",
    "semantic_title": "sdedit: guided image synthesis and editing with stochastic differential equations",
    "citation_count": 1707,
    "authors": []
  },
  "https://openreview.net/forum?id=xNOVfCCvDpM": {
    "title": "Post hoc Explanations may be Ineffective for Detecting Unknown Spurious Correlation",
    "volume": "poster",
    "abstract": "We investigate whether three types of post hoc model explanations‚Äìfeature attribution, concept activation, and training point ranking‚Äìare effective for detecting a model's reliance on spurious signals in the training data. Specifically, we consider the scenario where the spurious signal to be detected is unknown, at test-time, to the user of the explanation method. We design an empirical methodology that uses semi-synthetic datasets along with pre-specified spurious artifacts to obtain models that verifiably rely on these spurious training signals. We then provide a suite of metrics that assess an explanation method's reliability for spurious signal detection under various conditions. We find that the post hoc explanation methods tested are ineffective when the spurious artifact is unknown at test-time especially for non-visible artifacts like a background blur. Further, we find that feature attribution methods are susceptible to erroneously indicating dependence on spurious signals even when the model being explained does not rely on spurious artifacts. This finding casts doubt on the utility of these approaches, in the hands of a practitioner, for detecting a model's reliance on spurious signals",
    "checked": true,
    "id": "d3fb854e4e97cab40d1c076cd6e88439a0227249",
    "semantic_title": "post hoc explanations may be ineffective for detecting unknown spurious correlation",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=_jMtny3sMKU": {
    "title": "Generalizing Few-Shot NAS with Gradient Matching",
    "volume": "poster",
    "abstract": "Efficient performance estimation of architectures drawn from large search spaces is essential to Neural Architecture Search. One-Shot methods tackle this challenge by training one supernet to approximate the performance of every architecture in the search space via weight-sharing, thereby drastically reducing the search cost. However, due to coupled optimization between child architectures caused by weight-sharing, One-Shot supernet's performance estimation could be inaccurate, leading to degraded search outcomes. To address this issue, Few-Shot NAS reduces the level of weight-sharing by splitting the One-Shot supernet into multiple separated sub-supernets via edge-wise (layer-wise) exhaustive partitioning. Since each partition of the supernet is not equally important, it necessitates the design of a more effective splitting criterion. In this work, we propose a gradient matching score (GM) that leverages gradient information at the shared weight for making informed splitting decisions. Intuitively, gradients from different child models can be used to identify whether they agree on how to update the shared modules, and subsequently to decide if they should share weight. Compared with exhaustive partitioning, the proposed criterion significantly reduces the branching factor per edge. This allows us to split more edges (layers) for a given budget, resulting in substantially improved performance as NAS search spaces usually include dozens of edges (layers). Extensive empirical evaluations of the proposed method on a wide range of search spaces (NASBench-201, DARTS, MobileNet Space), datasets (cifar10, cifar100, ImageNet) and search algorithms (DARTS, SNAS, RSPS, ProxylessNAS, OFA) demonstrate that it significantly outperforms its Few-Shot counterparts while surpassing previous comparable methods in terms of the accuracy of derived architectures. Our code is available at https://github.com/skhu101/GM-NAS",
    "checked": true,
    "id": "b1109de87e5f43a88c0de566044591b4c74364bd",
    "semantic_title": "generalizing few-shot nas with gradient matching",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=VBZJ_3tz-t": {
    "title": "The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "821b08d595b6482e3d1f5bab6835b72d67ebd894",
    "semantic_title": "the unreasonable effectiveness of random pruning: return of the most naive baseline for sparse training",
    "citation_count": 116,
    "authors": []
  },
  "https://openreview.net/forum?id=5HvpvYd68b": {
    "title": "switch-GLAT: Multilingual Parallel Machine Translation Via Code-Switch Decoder",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5962ff4e4adbdb52e45955cce6a76f56493cf70a",
    "semantic_title": "switch-glat: multilingual parallel machine translation via code-switch decoder",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=GWQWAeE9EpB": {
    "title": "DictFormer: Tiny Transformer with Shared Dictionary",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bd55dc24da4110ca58286f1ef59c88b0290c9cd9",
    "semantic_title": "dictformer: tiny transformer with shared dictionary",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=6vkzF28Hur8": {
    "title": "Training Transition Policies via Distribution Matching for Complex Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8eb73addbf8c2b52637af040755cf3ca13cdbf40",
    "semantic_title": "training transition policies via distribution matching for complex tasks",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=3YqeuCVwy1d": {
    "title": "GDA-AM: ON THE EFFECTIVENESS OF SOLVING MIN-IMAX OPTIMIZATION VIA ANDERSON MIXING",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fb011c5468856f46ee8def942b3d22b51e358660",
    "semantic_title": "gda-am: on the effectiveness of solving min-imax optimization via anderson mixing",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=PQTW3iG4sC-": {
    "title": "On feature learning in neural networks with global convergence guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "63b82ca8518012a371d5ee29ce2de16bb1769c18",
    "semantic_title": "on feature learning in neural networks with global convergence guarantees",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=EQmAP4F859": {
    "title": "The Three Stages of Learning Dynamics in High-dimensional Kernel Methods",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d584236d863de4dbed78c8428f5d29de22def819",
    "semantic_title": "the three stages of learning dynamics in high-dimensional kernel methods",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=6MmiS0HUJHR": {
    "title": "When Can We Learn General-Sum Markov Games with a Large Number of Players Sample-Efficiently?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0136a33c07bc7003066a1f0ae8e0ab26f84c0cbe",
    "semantic_title": "when can we learn general-sum markov games with a large number of players sample-efficiently?",
    "citation_count": 74,
    "authors": []
  },
  "https://openreview.net/forum?id=1NvflqAdoom": {
    "title": "Neural Networks as Kernel Learners: The Silent Alignment Effect",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fe5378625a41c823a9726c73f3934b6c4ab9e95a",
    "semantic_title": "neural networks as kernel learners: the silent alignment effect",
    "citation_count": 91,
    "authors": []
  },
  "https://openreview.net/forum?id=B6EIcyp-Rb7": {
    "title": "Learning Object-Oriented Dynamics for Planning from Text",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "07cf239db20908b18227648df28cefda640806c5",
    "semantic_title": "learning object-oriented dynamics for planning from text",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=pWBNOgdeURp": {
    "title": "An Operator Theoretic View On Pruning Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6eb0518b1743487d351854825edd5b21c9ed11a3",
    "semantic_title": "an operator theoretic view on pruning deep neural networks",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=_4GFbtOuWq-": {
    "title": "Capacity of Group-invariant Linear Readouts from Equivariant Representations: How Many Objects can be Linearly Classified Under All Possible Views?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b8084f26fa37ca9fa51a0489490a6e382e67b67e",
    "semantic_title": "capacity of group-invariant linear readouts from equivariant representations: how many objects can be linearly classified under all possible views?",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=V0A5g83gdQ_": {
    "title": "Tuformer: Data-driven Design of Transformers for Improved Generalization or Efficiency",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "338d0501947b5fc7d92d09eed9a3e299f7b48ec1",
    "semantic_title": "tuformer: data-driven design of transformers for improved generalization or efficiency",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=MSwEFaztwkE": {
    "title": "Learning Weakly-supervised Contrastive Representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f581629c86f1769f6e4d65c5497c5ea7a97408e5",
    "semantic_title": "learning weakly-supervised contrastive representations",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=Vs5NK44aP9P": {
    "title": "Encoding Weights of Irregular Sparsity for Fixed-to-Fixed Model Compression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cfcc55816a7b6f952e3010d998240cbff7fdfdb7",
    "semantic_title": "encoding weights of irregular sparsity for fixed-to-fixed model compression",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=0no8Motr-zO": {
    "title": "An Experimental Design Perspective on Model-Based Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2bd2080efe63afd1fd0c17e04b80b79166ec5aa7",
    "semantic_title": "an experimental design perspective on model-based reinforcement learning",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=NdOoQnYPj_": {
    "title": "BAM: Bayes with Adaptive Memory",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "27fcffc50eba1ccf8fa3dbc181b9fde0cc24b652",
    "semantic_title": "bam: bayes with adaptive memory",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=izvwgBic9q": {
    "title": "Unsupervised Learning of Full-Waveform Inversion: Connecting CNN and Partial Differential Equation in a Loop",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "da7698b4387e8de6138c7cf279f34521a841226d",
    "semantic_title": "unsupervised learning of full-waveform inversion: connecting cnn and partial differential equation in a loop",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=AAJLBoGt0XM": {
    "title": "Conditional Contrastive Learning with Kernel",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9e9dc4b54b20882f871cf3b1438df33162bb5414",
    "semantic_title": "conditional contrastive learning with kernel",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=zRJu6mU2BaE": {
    "title": "ConFeSS: A Framework for Single Source Cross-Domain Few-Shot Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a8fc34772ed42b17f49580cb7e8372fc96e307ad",
    "semantic_title": "confess: a framework for single source cross-domain few-shot learning",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=nZOUYEN6Wvy": {
    "title": "Granger causal inference on DAGs identifies genomic loci regulating transcription",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3f75b8d436a7240189c9ce7a31c9bb3c09f99670",
    "semantic_title": "granger causal inference on dags identifies genomic loci regulating transcription",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=7QfLW-XZTl": {
    "title": "Energy-Inspired Molecular Conformation Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a46b9fe9d83f21b18cf18b350be6484828f7f3e8",
    "semantic_title": "energy-inspired molecular conformation optimization",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=tT9t_ZctZRL": {
    "title": "Towards Deepening Graph Neural Networks: A GNTK-based Optimization Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c1cfc08ea4daff9f2d989b7021162c59b45fe07f",
    "semantic_title": "towards deepening graph neural networks: a gntk-based optimization perspective",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=CJzi3dRlJE-": {
    "title": "Connectome-constrained Latent Variable Model of Whole-Brain Neural Activity",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "18ab60e2cd524d4d8eca056dad5a279a9697246c",
    "semantic_title": "connectome-constrained latent variable model of whole-brain neural activity",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=U4uFaLyg7PV": {
    "title": "T-WaveNet: A Tree-Structured Wavelet Neural Network for Time Series Signal Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8332f1ea4f84a0ca19df1842e9f3c4705aff2f47",
    "semantic_title": "t-wavenet: a tree-structured wavelet neural network for time series signal analysis",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=AmUhwTOHgm": {
    "title": "Trans-Encoder: Unsupervised sentence-pair modelling through self- and mutual-distillations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "24cf68b10a2dcec9437b8155344c8f221c907c34",
    "semantic_title": "trans-encoder: unsupervised sentence-pair modelling through self- and mutual-distillations",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=_uCb2ynRu7Y": {
    "title": "Path Integral Sampler: A Stochastic Control Approach For Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "713223e6a06773523195d1f3a9472f0fce940aea",
    "semantic_title": "path integral sampler: a stochastic control approach for sampling",
    "citation_count": 141,
    "authors": []
  },
  "https://openreview.net/forum?id=WfvgGBcgbE7": {
    "title": "Model Zoo: A Growing Brain That Learns Continually",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ff3d64942ee32c12816cd0d60d338d858edbc99f",
    "semantic_title": "model zoo: a growing brain that learns continually",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=XctLdNfCmP": {
    "title": "Predicting Physics in Mesh-reduced Space with Temporal Attention",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a9764bda40202717968fa1d8355dd04dba227534",
    "semantic_title": "predicting physics in mesh-reduced space with temporal attention",
    "citation_count": 89,
    "authors": []
  },
  "https://openreview.net/forum?id=qiMXBIf4NfB": {
    "title": "How unlabeled data improve generalization in self-training? A one-hidden-layer theoretical analysis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d208842ecfb0c356315a6393df471b2042e383fc",
    "semantic_title": "how does unlabeled data improve generalization in self-training? a one-hidden-layer theoretical analysis",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=fExcSKdDo_": {
    "title": "Learning to Dequantise with Truncated Flows",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "552e1aa0e0c842ae84b33d67cba297471d4b1048",
    "semantic_title": "learning to dequantise with truncated flows",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=TpJMvo0_pu-": {
    "title": "Curriculum learning as a tool to uncover learning principles in the brain",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a29d959673a16ccd8d2d402b4109f8ac11718cee",
    "semantic_title": "curriculum learning as a tool to uncover learning principles in the brain",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=VqzXzA9hjaX": {
    "title": "Optimizer Amalgamation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cb669ae378861bae9ca1ca64df361c29c6f5f576",
    "semantic_title": "optimizer amalgamation",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=Xo0lbDt975": {
    "title": "An Agnostic Approach to Federated Learning with Class Imbalance",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e6982936a30c8c3c4ec8161a864dcd4edaf08b3b",
    "semantic_title": "an agnostic approach to federated learning with class imbalance",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=ckZY7DGa7FQ": {
    "title": "A Fine-Tuning Approach to Belief State Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fffb1db211354c7838e24b069c423ac7c064bfbd",
    "semantic_title": "a fine-tuning approach to belief state modeling",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=Q42f0dfjECO": {
    "title": "Differentially Private Fine-tuning of Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "56874f9aef515902c5a49d84d10f629f8dcd5f40",
    "semantic_title": "differentially private fine-tuning of language models",
    "citation_count": 414,
    "authors": []
  },
  "https://openreview.net/forum?id=DhzIU48OcZh": {
    "title": "P-Adapters: Robustly Extracting Factual Information from Language Models with Diverse Prompts",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b3ca60698c5ce591659a7e3719f124ba61ca3a39",
    "semantic_title": "p-adapters: robustly extracting factual information from language models with diverse prompts",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=giBFoa-uS12": {
    "title": "Iterated Reasoning with Mutual Information in Cooperative and Byzantine Decentralized Teaming",
    "volume": "poster",
    "abstract": "Information sharing is key in building team cognition and enables coordination and cooperation. High-performing human teams also benefit from acting strategically with hierarchical levels of iterated communication and rationalizability, meaning a human agent can reason about the actions of their teammates in their decision-making. Yet, the majority of prior work in Multi-Agent Reinforcement Learning (MARL) does not support iterated rationalizability and only encourage inter-agent communication, resulting in a suboptimal equilibrium cooperation strategy. In this work, we show that reformulating an agent's policy to be conditional on the policies of its neighboring teammates inherently maximizes Mutual Information (MI) lower-bound when optimizing under Policy Gradient (PG). Building on the idea of decision-making under bounded rationality and cognitive hierarchy theory, we show that our modified PG approach not only maximizes local agent rewards but also implicitly reasons about MI between agents without the need for any explicit ad-hoc regularization terms. Our approach, InfoPG, outperforms baselines in learning emergent collaborative behaviors and sets the state-of-the-art in decentralized cooperative MARL tasks. Our experiments validate the utility of InfoPG by achieving higher sample efficiency and significantly larger cumulative reward in several complex cooperative multi-agent domains",
    "checked": true,
    "id": "bb110105320dce94791af083a3ecad85989b6e20",
    "semantic_title": "iterated reasoning with mutual information in cooperative and byzantine decentralized teaming",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=T0GpzBQ1Fg6": {
    "title": "Step-unrolled Denoising Autoencoders for Text Generation",
    "volume": "poster",
    "abstract": "In this paper we propose a new generative model of text, Step-unrolled Denoising Autoencoder (SUNDAE), that does not rely on autoregressive models. Similarly to denoising diffusion techniques, SUNDAE is repeatedly applied on a sequence of tokens, starting from random inputs and improving them each time until convergence. We present a simple new improvement operator that converges in fewer iterations than diffusion methods, while qualitatively producing better samples on natural language datasets. SUNDAE achieves state-of-the-art results (among non-autoregressive methods) on the WMT'14 English-to-German translation task and good qualitative results on unconditional language modeling on the Colossal Cleaned Common Crawl dataset and a dataset of Python code from GitHub. The non-autoregressive nature of SUNDAE opens up possibilities beyond left-to-right prompted generation, by filling in arbitrary blank patterns in a template",
    "checked": true,
    "id": "2da2a44f78e1bd9735d94fee3bd944d47d45742b",
    "semantic_title": "step-unrolled denoising autoencoders for text generation",
    "citation_count": 124,
    "authors": []
  },
  "https://openreview.net/forum?id=P7OVkHEoHOZ": {
    "title": "Hindsight Foresight Relabeling for Meta-Reinforcement Learning",
    "volume": "poster",
    "abstract": "Meta-reinforcement learning (meta-RL) algorithms allow for agents to learn new behaviors from small amounts of experience, mitigating the sample inefficiency problem in RL. However, while meta-RL agents can adapt quickly to new tasks at test time after experiencing only a few trajectories, the meta-training process is still sample-inefficient. Prior works have found that in the multi-task RL setting, relabeling past transitions and thus sharing experience among tasks can improve sample efficiency and asymptotic performance. We apply this idea to the meta-RL setting and devise a new relabeling method called Hindsight Foresight Relabeling (HFR). We construct a relabeling distribution using the combination of \"hindsight\", which is used to relabel trajectories using reward functions from the training task distribution, and \"foresight\", which takes the relabeled trajectories and computes the utility of each trajectory for each task. HFR is easy to implement and readily compatible with existing meta-RL algorithms. We find that HFR improves performance when compared to other relabeling methods on a variety of meta-RL tasks",
    "checked": true,
    "id": "4ab9a90bc1ce86359b69d9cf8d005e7de89985f7",
    "semantic_title": "hindsight foresight relabeling for meta-reinforcement learning",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=nZeVKeeFYf9": {
    "title": "LoRA: Low-Rank Adaptation of Large Language Models",
    "volume": "poster",
    "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by a factor of 10,000 and the GPU memory requirement by a factor of 3. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA",
    "checked": true,
    "id": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
    "semantic_title": "lora: low-rank adaptation of large language models",
    "citation_count": 12808,
    "authors": []
  },
  "https://openreview.net/forum?id=qRDQi3ocgR3": {
    "title": "Which Shortcut Cues Will DNNs Choose? A Study from the Parameter-Space Perspective",
    "volume": "poster",
    "abstract": "Deep neural networks (DNNs) often rely on easy‚Äìto‚Äìlearn discriminatory features, or cues, that are not necessarily essential to the problem at hand. For example, ducks in an image may be recognized based on their typical background scenery, such as lakes or streams. This phenomenon, also known as shortcut learning, is emerging as a key limitation of the current generation of machine learning models. In this work, we introduce a set of experiments to deepen our understanding of shortcut learning and its implications. We design a training setup with several shortcut cues, named WCST-ML, where each cue is equally conducive to the visual recognition problem at hand. Even under equal opportunities, we observe that (1) certain cues are preferred to others, (2) solutions biased to the easy‚Äìto‚Äìlearn cues tend to converge to relatively flat minima on the loss surface, and (3) the solutions focusing on those preferred cues are far more abundant in the parameter space. We explain the abundance of certain cues via their Kolmogorov (descriptional) complexity: solutions corresponding to Kolmogorov-simple cues are abundant in the parameter space and are thus preferred by DNNs. Our studies are based on the synthetic dataset DSprites and the face dataset UTKFace. In our WCST-ML, we observe that the inborn bias of models leans toward simple cues, such as color and ethnicity. Our findings emphasize the importance of active human intervention to remove the inborn model biases that may cause negative societal impacts",
    "checked": true,
    "id": "535131d7218e26360c09cd8f9a2e7198ec0e3e6f",
    "semantic_title": "which shortcut cues will dnns choose? a study from the parameter-space perspective",
    "citation_count": 57,
    "authors": []
  },
  "https://openreview.net/forum?id=tUMr0Iox8XW": {
    "title": "Efficient Computation of Deep Nonlinear Infinite-Width Neural Networks that Learn Features",
    "volume": "poster",
    "abstract": "While a popular limit of infinite-width neural networks, the Neural Tangent Kernel (NTK) often exhibits performance gaps from finite-width neural networks on standard datasets, due to lack of feature learning. Although the feature learning *maximal update limit*, or *Œº-limit* (Yang and Hu, 2020) of wide networks has closed the gap for 1-hidden-layer linear models, no one has been able to demonstrate this for deep nonlinear multi-layer perceptrons (MLP) because of Œº-limit's computational difficulty in this setting. Here, we solve this problem by proposing a novel feature learning limit, the *œÄ-limit*, that bypasses the computational issues. The œÄ-limit, in short, is the limit of a form of projected gradient descent, and the œÄ-limit of an MLP is roughly another MLP where gradients are appended to weights during training. We prove its almost sure convergence with width using the Tensor Programs technique. We evaluate it on CIFAR10 and Omniglot against NTK as well as finite networks, finding the œÄ-limit outperform finite-width models trained normally (without projection) in both settings, closing the performance gap between finite- and infinite-width neural networks previously left by NTK. Code for this work is available at github.com/santacml/pilim",
    "checked": true,
    "id": "a2b2d75389ace794a87a7b944f6820d2463936f5",
    "semantic_title": "efficient computation of deep nonlinear infinite-width neural networks that learn features",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=6q_2b6u0BnJ": {
    "title": "TRAIL: Near-Optimal Imitation Learning with Suboptimal Data",
    "volume": "poster",
    "abstract": "In imitation learning, one aims to learn task-solving policies using access to near-optimal expert trajectories collected from the task environment. However, high-quality trajectories -- e.g., from human experts -- can be expensive to obtain in practical settings. On the contrary, it is often much easier to obtain large amounts of suboptimal trajectories which can nevertheless provide insight into the structure of the environment, showing what \\emph{could} be done in the environment even if not what \\emph{should} be done. Is it possible to formalize these conceptual benefits and devise algorithms to use offline datasets to yield \\emph{provable} improvements to the sample-efficiency of imitation learning? In this work, we answer this question affirmatively and present training objectives which use an offline dataset to learn an approximate \\emph{factored} dynamics model whose structure enables the extraction of a \\emph{latent action space}. Our theoretical analysis shows that the learned latent action space can boost the sample-efficiency of downstream imitation learning, effectively reducing the need for large near-optimal expert datasets through the use of auxiliary non-expert data. We evaluate the practicality of our objective through experiments on a set of navigation and locomotion tasks. Our results verify the benefits suggested by our theory and show that our algorithms is able to recover near-optimal policies with fewer expert trajectories",
    "checked": true,
    "id": "d3c6e0b80c36c14f7d1761fb881f20c35165f507",
    "semantic_title": "trail: near-optimal imitation learning with suboptimal data",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=zrW-LVXj2k1": {
    "title": "On the benefits of maximum likelihood estimation for Regression and Forecasting",
    "volume": "poster",
    "abstract": "We advocate for a practical Maximum Likelihood Estimation (MLE) approach towards designing loss functions for regression and forecasting, as an alternative to the typical approach of direct empirical risk minimization on a specific target metric. The MLE approach is better suited to capture inductive biases such as prior domain knowledge in datasets, and can output post-hoc estimators at inference time that can optimize different types of target metrics. We present theoretical results to demonstrate that our approach is competitive with any estimator for the target metric under some general conditions. In two example practical settings, Poisson and Pareto regression, we show that our competitive results can be used to prove that the MLE approach has better excess risk bounds than directly minimizing the target metric. We also demonstrate empirically that our method instantiated with a well-designed general purpose mixture likelihood family can obtain superior performance for a variety of tasks across time-series forecasting and regression datasets with different data distributions",
    "checked": true,
    "id": "f4748a79d1228973a82119e2a8e5159db818be6a",
    "semantic_title": "on the benefits of maximum likelihood estimation for regression and forecasting",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=GhVS8_yPeEa": {
    "title": "Effect of scale on catastrophic forgetting in neural networks",
    "volume": "poster",
    "abstract": "Catastrophic forgetting presents a challenge in developing deep learning models capable of continual learning, i.e. learning tasks sequentially. Recently, both computer vision and natural-language processing have witnessed great progress through the use of large-scale pretrained models. In this work, we present an empirical study of catastrophic forgetting in this pretraining paradigm. Our experiments indicate that large, pretrained ResNets and Transformers are significantly more resistant to forgetting than randomly-initialized, trained-from-scratch models; this robustness systematically improves with scale of both model and pretraining dataset size. We take initial steps towards characterizing what aspect of model representations allows them to perform continual learning so well, finding that in the pretrained models, distinct class representations grow more orthogonal with scale. Our results suggest that, when possible, scale and a diverse pretraining dataset can be useful ingredients in mitigating catastrophic forgetting",
    "checked": true,
    "id": "b584309dccafe64a7d6b96f064554330cbe1bbd3",
    "semantic_title": "effect of scale on catastrophic forgetting in neural networks",
    "citation_count": 173,
    "authors": []
  },
  "https://openreview.net/forum?id=FndDxSz3LxQ": {
    "title": "Learn Locally, Correct Globally: A Distributed Algorithm for Training Graph Neural Networks",
    "volume": "poster",
    "abstract": "Despite the recent success of Graph Neural Networks (GNNs), training GNNs on large graphs remains challenging. The limited resource capacities of the existing servers, the dependency between nodes in a graph, and the privacy concern due to the centralized storage and model learning have spurred the need to design an effective distributed algorithm for GNN training. However, existing distributed GNN training methods impose either excessive communication costs or large memory overheads that hinders their scalability. To overcome these issues, we propose a communication-efficient distributed GNN training technique named $\\text{\\textit{Learn Locally, Correct Globally}}$ (LLCG). To reduce the communication and memory overhead, each local machine in LLCG first trains a GNN on its local data by ignoring the dependency between nodes among different machines, then sends the locally trained model to the server for periodic model averaging. However, ignoring node dependency could result in significant performance degradation. To solve the performance degradation, we propose to apply $\\text{\\textit{Global Server Corrections}}$ on the server to refine the locally learned models. We rigorously analyze the convergence of distributed methods with periodic model averaging for training GNNs and show that naively applying periodic model averaging but ignoring the dependency between nodes will suffer from an irreducible residual error. However, this residual error can be eliminated by utilizing the proposed global corrections to entail fast convergence rate. Extensive experiments on real-world datasets show that LLCG can significantly improve the efficiency without hurting the performance",
    "checked": true,
    "id": "e38b78b4a71451a143e10a252d3fd0de70ef47c3",
    "semantic_title": "learn locally, correct globally: a distributed algorithm for training graph neural networks",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=7MV6uLzOChW": {
    "title": "Conditional Image Generation by Conditioning Variational Auto-Encoders",
    "volume": "poster",
    "abstract": "We present a conditional variational auto-encoder (VAE) which, to avoid the substantial cost of training from scratch, uses an architecture and training objective capable of leveraging a foundation model in the form of a pretrained unconditional VAE. To train the conditional VAE, we only need to train an artifact to perform amortized inference over the unconditional VAE's latent variables given a conditioning input. We demonstrate our approach on tasks including image inpainting, for which it outperforms state-of-the-art GAN-based approaches at faithfully representing the inherent uncertainty. We conclude by describing a possible application of our inpainting model, in which it is used to perform Bayesian experimental design for the purpose of guiding a sensor",
    "checked": true,
    "id": "999c93898f38008ed9caa742f3c6555c8f5c5975",
    "semantic_title": "conditional image generation by conditioning variational auto-encoders",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=hm2tNDdgaFK": {
    "title": "Learning 3D Representations of Molecular Chirality with Invariance to Bond Rotations",
    "volume": "poster",
    "abstract": "Molecular chirality, a form of stereochemistry most often describing relative spatial arrangements of bonded neighbors around tetrahedral carbon centers, influences the set of 3D conformers accessible to the molecule without changing its 2D graph connectivity. Chirality can strongly alter (bio)chemical interactions, particularly protein-drug binding. Most 2D graph neural networks (GNNs) designed for molecular property prediction at best use atomic labels to na√Øvely treat chirality, while E(3)-invariant 3D GNNs are invariant to chirality altogether. To enable representation learning on molecules with defined stereochemistry, we design an SE(3)-invariant model that processes torsion angles of a 3D molecular conformer. We explicitly model conformational flexibility by integrating a novel type of invariance to rotations about internal molecular bonds into the architecture, mitigating the need for multi-conformer data augmentation. We test our model on four benchmarks: contrastive learning to distinguish conformers of different stereoisomers in a learned latent space, classification of chiral centers as R/S, prediction of how enantiomers rotate circularly polarized light, and ranking enantiomers by their docking scores in an enantiosensitive protein pocket. We compare our model, Chiral InterRoto-Invariant Neural Network (ChIRo), with 2D and 3D GNNs to demonstrate that our model achieves state of the art performance when learning chiral-sensitive functions from molecular structures",
    "checked": true,
    "id": "a5eb31131ec807648d6b61eacbce5b2deb0d3727",
    "semantic_title": "learning 3d representations of molecular chirality with invariance to bond rotations",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=tgcAoUVHRIB": {
    "title": "Neural Methods for Logical Reasoning over Knowledge Graphs",
    "volume": "poster",
    "abstract": "Reasoning is a fundamental problem for computers and deeply studied in Artificial Intelligence. In this paper, we specifically focus on answering multi-hop logical queries on Knowledge Graphs (KGs). This is a complicated task because, in real world scenarios, the graphs tend to be large and incomplete. Most previous works have been unable to create models that accept full First-Order Logical (FOL) queries, which includes negative queries, and have only been able to process a limited set of query structures. Additionally, most methods present logic operators that can only perform the logical operation they are made for. We introduce a set of models that use Neural Networks to create one-point vector embeddings to answer the queries. The versatility of neural networks allows the framework to handle FOL queries with Conjunction, Disjunction and Negation operators. We demonstrate experimentally the performance of our models through extensive experimentation on well-known benchmarking datasets. Besides having more versatile operators, the models achieve a 10% relative increase over best performing state of the art and more than 30% over the original method based on single-point vector embeddings",
    "checked": true,
    "id": "2d80d0b053179988f2155ea9eaf57b60a7742c16",
    "semantic_title": "neural methods for logical reasoning over knowledge graphs",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=St6eyiTEHnG": {
    "title": "Consistent Counterfactuals for Deep Models",
    "volume": "poster",
    "abstract": "Counterfactual examples are one of the most commonly-cited methods for explaining the predictions of machine learning models in key areas such as finance and medical diagnosis. Counterfactuals are often discussed under the assumption that the model on which they will be used is static, but in deployment models may be periodically retrained or fine-tuned. This paper studies the consistency of model prediction on counterfactual examples in deep networks under small changes to initial training conditions, such as weight initialization and leave-one-out variations in data, as often occurs during model deployment. We demonstrate experimentally that counterfactual examples for deep models are often inconsistent across such small changes, and that increasing the cost of the counterfactual, a stability-enhancing mitigation suggested by prior work in the context of simpler models, is not a reliable heuristic in deep networks. Rather, our analysis shows that a model's Lipschitz continuity around the counterfactual, along with confidence of its prediction, is key to its consistency across related models. To this end, we propose Stable Neighbor Search as a way to generate more consistent counterfactual explanations, and illustrate the effectiveness of this approach on several benchmark datasets",
    "checked": true,
    "id": "b1fe45eda204847f5f4c0b3b8eafaecaf184859c",
    "semantic_title": "consistent counterfactuals for deep models",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=9jsZiUgkCZP": {
    "title": "Unified Visual Transformer Compression",
    "volume": "poster",
    "abstract": "Vision transformers (ViTs) have gained popularity recently. Even without customized image operators such as convolutions, ViTs can yield competitive performance when properly trained on massive data. However, the computational overhead of ViTs remains prohibitive, due to stacking multi-head self-attention modules and else. Compared to the vast literature and prevailing success in compressing convolutional neural networks, the study of Vision Transformer compression has also just emerged, and existing works focused on one or two aspects of compression. This paper proposes a unified ViT compression framework that seamlessly assembles three effective techniques: pruning, layer skipping, and knowledge distillation. We formulate a budget-constrained, end-to-end optimization framework, targeting jointly learning model weights, layer-wise pruning ratios/masks, and skip configurations, under a distillation loss. The optimization problem is then solved using the primal-dual algorithm. Experiments are conducted with several ViT variants, e.g. DeiT and T2T-ViT backbones on the ImageNet dataset, and our approach consistently outperforms recent competitors. For example, DeiT-Tiny can be trimmed down to 50\\% of the original FLOPs almost without losing accuracy. Codes are available online:~\\url{https://github.com/VITA-Group/UVC}",
    "checked": true,
    "id": "4c69fdca6e8a1f10871ab9dc47f62c81ba7ead4a",
    "semantic_title": "unified visual transformer compression",
    "citation_count": 104,
    "authors": []
  },
  "https://openreview.net/forum?id=IDwN6xjHnK8": {
    "title": "Transformer-based Transform Coding",
    "volume": "poster",
    "abstract": "Neural data compression based on nonlinear transform coding has made great progress over the last few years, mainly due to improvements in prior models, quantization methods and nonlinear transforms. A general trend in many recent works pushing the limit of rate-distortion performance is to use ever more expensive prior models that can lead to prohibitively slow decoding. Instead, we focus on more expressive transforms that result in a better rate-distortion-computation trade-off. Specifically, we show that nonlinear transforms built on Swin-transformers can achieve better compression efficiency than transforms built on convolutional neural networks (ConvNets), while requiring fewer parameters and shorter decoding time. Paired with a compute-efficient Channel-wise Auto-Regressive Model prior, our SwinT-ChARM model outperforms VTM-12.1 by $3.68\\%$ in BD-rate on Kodak with comparable decoding speed. In P-frame video compression setting, we are able to outperform the popular ConvNet-based scale-space-flow model by $12.35\\%$ in BD-rate on UVG. We provide model scaling studies to verify the computational efficiency of the proposed solutions and conduct several analyses to reveal the source of coding gain of transformers over ConvNets, including better spatial decorrelation, flexible effective receptive field, and more localized response of latent pixels during progressive decoding",
    "checked": true,
    "id": "64bbfebfdc6e2ab6ea32a73baba3e0a296bb6d2c",
    "semantic_title": "transformer-based transform coding",
    "citation_count": 172,
    "authors": []
  },
  "https://openreview.net/forum?id=lbauk6wK2-y": {
    "title": "Object Pursuit: Building a Space of Objects via Discriminative Weight Generation",
    "volume": "poster",
    "abstract": "We propose a framework to continuously learn object-centric representations for visual learning and understanding. Existing object-centric representations either rely on supervisions that individualize objects in the scene, or perform unsupervised disentanglement that can hardly deal with complex scenes in the real world. To mitigate the annotation burden and relax the constraints on the statistical complexity of the data, our method leverages interactions to effectively sample diverse variations of an object and the corresponding training signals while learning the object-centric representations. Throughout learning, objects are streamed one by one in random order with unknown identities, and are associated with latent codes that can synthesize discriminative weights for each object through a convolutional hypernetwork. Moreover, re-identification of learned objects and forgetting prevention are employed to make the learning process efficient and robust. We perform an extensive study of the key features of the proposed framework and analyze the characteristics of the learned representations. Furthermore, we demonstrate the capability of the proposed framework in learning representations that can improve label efficiency in downstream tasks. Our code and trained models are made publicly available at: https://github.com/pptrick/Object-Pursuit",
    "checked": true,
    "id": "a3f828416f5c8400e268ef07b71b5b6fd9e73f96",
    "semantic_title": "object pursuit: building a space of objects via discriminative weight generation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=DhP9L8vIyLc": {
    "title": "PAC Prediction Sets Under Covariate Shift",
    "volume": "poster",
    "abstract": "An important challenge facing modern machine learning is how to rigorously quantify the uncertainty of model predictions. Conveying uncertainty is especially important when there are changes to the underlying data distribution that might invalidate the predictive model. Yet, most existing uncertainty quantification algorithms break down in the presence of such shifts. We propose a novel approach that addresses this challenge by constructing \\emph{probably approximately correct (PAC)} prediction sets in the presence of covariate shift. Our approach focuses on the setting where there is a covariate shift from the source distribution (where we have labeled training examples) to the target distribution (for which we want to quantify uncertainty). Our algorithm assumes given importance weights that encode how the probabilities of the training examples change under the covariate shift. In practice, importance weights typically need to be estimated; thus, we extend our algorithm to the setting where we are given confidence intervals for the importance weights. We demonstrate the effectiveness of our approach on covariate shifts based on DomainNet and ImageNet. Our algorithm satisfies the PAC constraint, and gives prediction sets with the smallest average normalized size among approaches that always satisfy the PAC constraint",
    "checked": true,
    "id": "1a7cc437c4fb4ae26919f8f5926670b5cba5605c",
    "semantic_title": "pac prediction sets under covariate shift",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=vJZ7dPIjip3": {
    "title": "Generalization of Neural Combinatorial Solvers Through the Lens of Adversarial Robustness",
    "volume": "poster",
    "abstract": "End-to-end (geometric) deep learning has seen first successes in approximating the solution of combinatorial optimization problems. However, generating data in the realm of NP-hard/-complete tasks brings practical and theoretical challenges, resulting in evaluation protocols that are too optimistic. Specifically, most datasets only capture a simpler subproblem and likely suffer from spurious features. We investigate these effects by studying adversarial robustness -a local generalization property- to reveal hard, model-specific instances and spurious features. For this purpose, we derive perturbation models for SAT and TSP. Unlike in other applications, where perturbation models are designed around subjective notions of imperceptibility, our perturbation models are efficient and sound, allowing us to determine the true label of perturbed samples without a solver. Surprisingly, with such perturbations, a sufficiently expressive neural solver does not suffer from the limitations of the accuracy-robustness trade-off common in supervised learning. Although such robust solvers exist, we show empirically that the assessed neural solvers do not generalize well w.r.t. small perturbations of the problem instance",
    "checked": true,
    "id": "70864c48e643e852355f4a79e23baf3614740df6",
    "semantic_title": "generalization of neural combinatorial solvers through the lens of adversarial robustness",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=dg79moSRqIo": {
    "title": "One After Another: Learning Incremental Skills for a Changing World",
    "volume": "poster",
    "abstract": "Reward-free, unsupervised discovery of skills is an attractive alternative to the bottleneck of hand-designing rewards in environments where task supervision is scarce or expensive. However, current skill pre-training methods, like many RL techniques, make a fundamental assumption -- stationary environments during training. Traditional methods learn all their skills simultaneously, which makes it difficult for them to both quickly adapt to changes in the environment, and to not forget earlier skills after such adaptation. On the other hand, in an evolving or expanding environment, skill learning must be able to adapt fast to new environment situations while not forgetting previously learned skills. These two conditions make it difficult for classic skill discovery to do well in an evolving environment. In this work, we propose a new framework for skill discovery, where skills are learned one after another in an incremental fashion. This framework allows newly learned skills to adapt to new environment or agent dynamics, while the fixed old skills ensure the agent doesn't forget a learned skill. We demonstrate experimentally that in both evolving and static environments, incremental skills significantly outperform current state-of-the-art skill discovery methods on both skill quality and the ability to solve downstream tasks. Videos for learned skills and code are made public on https://notmahi.github.io/disk",
    "checked": true,
    "id": "77d3d69f1c4c160e3765c416bc13aed863176197",
    "semantic_title": "one after another: learning incremental skills for a changing world",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=Kwm8I7dU-l5": {
    "title": "Graph-Guided Network for Irregularly Sampled Multivariate Time Series",
    "volume": "poster",
    "abstract": "In many domains, including healthcare, biology, and climate science, time series are irregularly sampled with varying time intervals between successive readouts and different subsets of variables (sensors) observed at different time points. Here, we introduce RAINDROP, a graph neural network that embeds irregularly sampled and multivariate time series while also learning the dynamics of sensors purely from observational data. RAINDROP represents every sample as a separate sensor graph and models time-varying dependencies between sensors with a novel message passing operator. It estimates the latent sensor graph structure and leverages the structure together with nearby observations to predict misaligned readouts. This model can be interpreted as a graph neural network that sends messages over graphs that are optimized for capturing time-varying dependencies among sensors. We use RAINDROP to classify time series and interpret temporal dynamics on three healthcare and human activity datasets. RAINDROP outperforms state-of-the-art methods by up to 11.4% (absolute F1-score points), including techniques that deal with irregular sampling using fixed discretization and set functions. RAINDROP shows superiority in diverse setups, including challenging leave-sensor-out settings",
    "checked": true,
    "id": "455bfc515eb279cc09023faa1f78c6efb61224ba",
    "semantic_title": "graph-guided network for irregularly sampled multivariate time series",
    "citation_count": 124,
    "authors": []
  },
  "https://openreview.net/forum?id=qI4542Y2s1D": {
    "title": "FILM: Following Instructions in Language with Modular Methods",
    "volume": "poster",
    "abstract": "Recent methods for embodied instruction following are typically trained end-to-end using imitation learning. This often requires the use of expert trajectories and low-level language instructions. Such approaches assume that neural states will integrate multimodal semantics to perform state tracking, building spatial memory, exploration, and long-term planning. In contrast, we propose a modular method with structured representations that (1) builds a semantic map of the scene and (2) performs exploration with a semantic search policy, to achieve the natural language goal. Our modular method achieves SOTA performance (24.46 %) with a substantial (8.17 % absolute) gap from previous work while using less data by eschewing both expert trajectories and low-level instructions. Leveraging low-level language, however, can further increase our performance (26.49 %). Our findings suggest that an explicit spatial memory and a semantic search policy can provide a stronger and more general representation for state-tracking and guidance, even in the absence of expert trajectories or low-level instructions",
    "checked": true,
    "id": "bef63d4f7656393b7bceb2ec704e86577c286166",
    "semantic_title": "film: following instructions in language with modular methods",
    "citation_count": 173,
    "authors": []
  },
  "https://openreview.net/forum?id=Fza94Y8VS4a": {
    "title": "The Evolution of Uncertainty of Learning in Games",
    "volume": "poster",
    "abstract": "Learning in games has become an object of intense interest for ML due to its connections to numerous AI architectures. We study standard online learning in games but from a non-standard perspective. Instead of studying the behavior of a single initial condition and whether it converges to equilibrium or not, we study the behavior of a probability distribution/measure over a set of initial conditions. This initial uncertainty is well-motivated both from a standard game-theoretic perspective (e.g. a modeler's uncertainty about the agents' initial beliefs) as well as from a ML one (e.g. noisy measurements, system initialization from a dataset distribution). Despite this, little is formally known about whether and under what conditions uncertainty is amplified or reduced in these systems. We use the popular measure of differential entropy to quantify the evolution of uncertainty. We find that such analysis shares an intimate relationship with volume analysis, a technique which was recently used to demonstrate the occurrence of Lyapunov chaos when using Multiplicative Weights Update (MWU) or Follow-the-Regularized-Leader (FTRL) algorithms in zero-sum games. This allows us to show that the differential entropy of these learning-in-game systems increases linearly with time, formalizing their increased unpredictability over time. We showcase the power of the framework by applying it in the study of multiple related systems, including different standard online optimization algorithms in numerous games and dynamics of evolutionary game theory",
    "checked": true,
    "id": "9e4cc2da11615c4b1feb701d78cc4234fa9d321c",
    "semantic_title": "the evolution of uncertainty of learning in games",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=CrCvGNHAIrz": {
    "title": "Explainable GNN-Based Models over Knowledge Graphs",
    "volume": "poster",
    "abstract": "Graph Neural Networks (GNNs) are often used to learn transformations of graph data. While effective in practice, such approaches make predictions via numeric manipulations so their output cannot be easily explained symbolically. We propose a new family of GNN-based transformations of graph data that can be trained effectively, but where all predictions can be explained symbolically as logical inferences in Datalog‚Äîa well-known rule-based formalism. In particular, we show how to encode an input knowledge graph into a graph with numeric feature vectors, process this graph using a GNN, and decode the result into an output knowledge graph. We use a new class of monotonic GNNs (MGNNs) to ensure that this process is equivalent to a round of application of a set of Datalog rules. We also show that, given an arbitrary MGNN, we can automatically extract rules that completely characterise the transformation. We evaluate our approach by applying it to classification tasks in knowledge graph completion",
    "checked": true,
    "id": "18637c36a2737e51051113e56b4438b06b568a3d",
    "semantic_title": "explainable gnn-based models over knowledge graphs",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=OY1A8ejQgEX": {
    "title": "Mention Memory: incorporating textual knowledge into Transformers through entity mention attention",
    "volume": "poster",
    "abstract": "Natural language understanding tasks such as open-domain question answering often require retrieving and assimilating factual information from multiple sources. We propose to address this problem by integrating a semi-parametric representation of a large text corpus into a Transformer model as a source of factual knowledge. Specifically, our method represents knowledge with ``mention memory'', a table of dense vector representations of every entity mention in a corpus. The proposed model - TOME - is a Transformer that accesses the information through internal memory layers in which each entity mention in the input passage attends to the mention memory. This approach enables synthesis of and reasoning over many disparate sources of information within a single Transformer model. In experiments using a memory of 150 million Wikipedia mentions, TOME achieves strong performance on several open-domain knowledge-intensive tasks, including the claim verification benchmarks HoVer and FEVER and several entity-based QA benchmarks. We also show that the model learns to attend to informative mentions without any direct supervision. Finally we demonstrate that the model can generalize to new unseen entities by updating the memory without retraining",
    "checked": true,
    "id": "7b7416c90e8d3fc9ad5c9fb3923a638f69294ed7",
    "semantic_title": "mention memory : incorporating textual knowledge into transformers through entity mention attention",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=dDo8druYppX": {
    "title": "Training Data Generating Networks: Shape Reconstruction via Bi-level Optimization",
    "volume": "poster",
    "abstract": "We propose a novel 3d shape representation for 3d shape reconstruction from a single image. Rather than predicting a shape directly, we train a network to generate a training set which will be fed into another learning algorithm to define the shape. The nested optimization problem can be modeled by bi-level optimization. Specifically, the algorithms for bi-level optimization are also being used in meta learning approaches for few-shot learning. Our framework establishes a link between 3D shape analysis and few-shot learning. We combine training data generating networks with bi-level optimization algorithms to obtain a complete framework for which all components can be jointly trained. We improve upon recent work on standard benchmarks for 3d shape reconstruction",
    "checked": true,
    "id": "97933bf50776a35e087078a00c21132ed3854690",
    "semantic_title": "training data generating networks: shape reconstruction via bi-level optimization",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=IcUWShptD7d": {
    "title": "Monotonic Differentiable Sorting Networks",
    "volume": "poster",
    "abstract": "Differentiable sorting algorithms allow training with sorting and ranking supervision, where only the ordering or ranking of samples is known. Various methods have been proposed to address this challenge, ranging from optimal transport-based differentiable Sinkhorn sorting algorithms to making classic sorting networks differentiable. One problem of current differentiable sorting methods is that they are non-monotonic. To address this issue, we propose a novel relaxation of conditional swap operations that guarantees monotonicity in differentiable sorting networks. We introduce a family of sigmoid functions and prove that they produce differentiable sorting networks that are monotonic. Monotonicity ensures that the gradients always have the correct sign, which is an advantage in gradient-based optimization. We demonstrate that monotonic differentiable sorting networks improve upon previous differentiable sorting methods",
    "checked": true,
    "id": "09a4e48ecfcf01a11dd78bef525255d683226345",
    "semantic_title": "monotonic differentiable sorting networks",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=qyTBxTztIpQ": {
    "title": "CrowdPlay: Crowdsourcing Human Demonstrations for Offline Learning",
    "volume": "poster",
    "abstract": "Crowdsourcing has been instrumental for driving AI advances that rely on large-scale data. At the same time, reinforcement learning has seen rapid progress through benchmark environments that strike a balance between tractability and real-world complexity, such as ALE and OpenAI Gym. In this paper, we aim to fill a gap at the intersection of these two: The use of crowdsourcing to generate large-scale human demonstration data in the support of advancing research into imitation learning and offline learning. To this end, we present CrowdPlay, a complete crowdsourcing pipeline for any standard RL environment including OpenAI Gym (made available under an open-source license); a large-scale publicly available crowdsourced dataset of human gameplay demonstrations in Atari 2600 games, including multimodal behavior and human-human and human-AI multiagent data; offline learning benchmarks with extensive human data evaluation; and a detailed study of incentives, including real-time feedback to drive high quality data. We hope that this will drive the improvement in design of algorithms that account for the complexity of human, behavioral data and thereby enable a step forward in direction of effective learning for real-world settings. Our code and dataset are available at https://mgerstgrasser.github.io/crowdplay/",
    "checked": true,
    "id": "b161d47256418fb262a5d11b64c9626212df32d4",
    "semantic_title": "crowdplay: crowdsourcing human demonstrations for offline learning",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=KSSfF5lMIAg": {
    "title": "Model Agnostic Interpretability for Multiple Instance Learning",
    "volume": "poster",
    "abstract": "In Multiple Instance Learning (MIL), models are trained using bags of instances, where only a single label is provided for each bag. A bag label is often only determined by a handful of key instances within a bag, making it difficult to interpret what information a classifier is using to make decisions. In this work, we establish the key requirements for interpreting MIL models. We then go on to develop several model-agnostic approaches that meet these requirements. Our methods are compared against existing inherently interpretable MIL models on several datasets, and achieve an increase in interpretability accuracy of up to 30%. We also examine the ability of the methods to identify interactions between instances and scale to larger datasets, improving their applicability to real-world problems",
    "checked": true,
    "id": "d6721fcf0173b8212f888a636f03c7d5456b0c85",
    "semantic_title": "model agnostic interpretability for multiple instance learning",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=Zq2G_VTV53T": {
    "title": "FastSHAP: Real-Time Shapley Value Estimation",
    "volume": "poster",
    "abstract": "Although Shapley values are theoretically appealing for explaining black-box models, they are costly to calculate and thus impractical in settings that involve large, high-dimensional models. To remedy this issue, we introduce FastSHAP, a new method for estimating Shapley values in a single forward pass using a learned explainer model. To enable efficient training without requiring ground truth Shapley values, we develop an approach to train FastSHAP via stochastic gradient descent using a weighted least-squares objective function. In our experiments with tabular and image datasets, we compare FastSHAP to existing estimation approaches and find that it generates accurate explanations with an orders-of-magnitude speedup",
    "checked": true,
    "id": "cce242dfa75492b02952ad2692b1eed4dc43c8c1",
    "semantic_title": "fastshap: real-time shapley value estimation",
    "citation_count": 144,
    "authors": []
  },
  "https://openreview.net/forum?id=4Ycr8oeCoIh": {
    "title": "When, Why, and Which Pretrained GANs Are Useful?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6d8601eddd073284ea56ecee042be0aa87823643",
    "semantic_title": "when, why, and which pretrained gans are useful?",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=R332S76RjxS": {
    "title": "A global convergence theory for deep ReLU implicit networks via over-parameterization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a69fd30be8ac2afc863a4210b38839947af11c67",
    "semantic_title": "a global convergence theory for deep relu implicit networks via over-parameterization",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=6VpeS27viTq": {
    "title": "Learnability Lock: Authorized Learnability Control Through Adversarial Invertible Transformations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3f8af18256f5b158510ec248974568db25495e53",
    "semantic_title": "learnability lock: authorized learnability control through adversarial invertible transformations",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=WHA8009laxu": {
    "title": "Federated Learning from Only Unlabeled Data with Class-conditional-sharing Clients",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7b939b22e815fd7e37835956b0e05d16fefe398b",
    "semantic_title": "federated learning from only unlabeled data with class-conditional-sharing clients",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=Rty5g9imm7H": {
    "title": "Transformer Embeddings of Irregularly Spaced Events and Their Participants",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e5dc40835036700ee4f4a9ca9899c9514b320fad",
    "semantic_title": "transformer embeddings of irregularly spaced events and their participants",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=0DcZxeWfOPt": {
    "title": "Fast Model Editing at Scale",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9286ac6e9b1aacd7d93496eb4615ae7678876d2a",
    "semantic_title": "fast model editing at scale",
    "citation_count": 416,
    "authors": []
  },
  "https://openreview.net/forum?id=rTAclwH46Tb": {
    "title": "Eigencurve: Optimal Learning Rate Schedule for SGD on Quadratic Objectives with Skewed Hessian Spectrums",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ae8210e2443572ad9a05e7e66058ea0919e6db9e",
    "semantic_title": "eigencurve: optimal learning rate schedule for sgd on quadratic objectives with skewed hessian spectrums",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=C03Ajc-NS5W": {
    "title": "An Autoregressive Flow Model for 3D Molecular Geometry Generation from Scratch",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0756fa74ec4bae4d1cf2b0f40b0d7fc843ee0fcc",
    "semantic_title": "an autoregressive flow model for 3d molecular geometry generation from scratch",
    "citation_count": 90,
    "authors": []
  },
  "https://openreview.net/forum?id=nzvbBD_3J-g": {
    "title": "On Incorporating Inductive Biases into VAEs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fc26d1bff24719f82ca268b096760c4f05564b94",
    "semantic_title": "on incorporating inductive biases into vaes",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=Kef8cKdHWpP": {
    "title": "DiffSkill: Skill Abstraction from Differentiable Physics for Deformable Object Manipulations with Tools",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4ad02c57c8867e0516e2288fba14d4a55fbc2ef4",
    "semantic_title": "diffskill: skill abstraction from differentiable physics for deformable object manipulations with tools",
    "citation_count": 78,
    "authors": []
  },
  "https://openreview.net/forum?id=SYB4WrJql1n": {
    "title": "On the Existence of Universal Lottery Tickets",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "95b241948a67518cc83ad864ea96abaf9a473881",
    "semantic_title": "on the existence of universal lottery tickets",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=xQUe1pOKPam": {
    "title": "Pre-training Molecular Graph Representation with 3D Geometry",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b8f816e23ff40d6afabccca2ee4770087ef0ef57",
    "semantic_title": "pre-training molecular graph representation with 3d geometry",
    "citation_count": 357,
    "authors": []
  },
  "https://openreview.net/forum?id=-HSOjDPfhBJ": {
    "title": "PER-ETD: A Polynomially Efficient Emphatic Temporal Difference Learning Method",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "916bce602655cf89ac834d646beb3091bcccd0fd",
    "semantic_title": "per-etd: a polynomially efficient emphatic temporal difference learning method",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=B72HXs80q4": {
    "title": "Taming Sparsely Activated Transformer with Stochastic Experts",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0e23cc159fd2fb34550600d60dd9148c93636183",
    "semantic_title": "taming sparsely activated transformer with stochastic experts",
    "citation_count": 124,
    "authors": []
  },
  "https://openreview.net/forum?id=i3RI65sR7N": {
    "title": "Hierarchical Variational Memory for Few-shot Learning Across Domains",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a69cf91f0a4aa8ce4306f7b3bc59629631fa35f8",
    "semantic_title": "hierarchical variational memory for few-shot learning across domains",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=Z1Qlm11uOM": {
    "title": "Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction",
    "volume": "poster",
    "abstract": "Video recordings of speech contain correlated audio and visual information, providing a strong signal for speech representation learning from the speaker's lip movements and the produced sound. We introduce Audio-Visual Hidden Unit BERT (AV-HuBERT), a self-supervised representation learning framework for audio-visual speech, which masks multi-stream video input and predicts automatically discovered and iteratively refined multimodal hidden units. AV-HuBERT learns powerful audio-visual speech representation benefiting both lip-reading and automatic speech recognition. On the largest public lip-reading benchmark LRS3 (433 hours), AV-HuBERT achieves 32.5% WER with only 30 hours of labeled data, outperforming the former state-of-the-art approach (33.6%) trained with a thousand times more transcribed video data (31K hours) (Makino et al., 2019). The lip-reading WER is further reduced to 26.9% when using all 433 hours of labeled data from LRS3 and combined with self-training. Using our audio-visual representation on the same benchmark for audio-only speech recognition leads to a 40% relative WER reduction over the state-of-the-art performance (1.3% vs 2.3%). Our code and models are available at https://github.com/facebookresearch/av_hubert",
    "checked": true,
    "id": "dc9a76b7cb690e6a95f0f07bb3d4fabb7181b68d",
    "semantic_title": "learning audio-visual speech representation by masked multimodal cluster prediction",
    "citation_count": 357,
    "authors": []
  },
  "https://openreview.net/forum?id=RdJVFCHjUMI": {
    "title": "An Explanation of In-context Learning as Implicit Bayesian Inference",
    "volume": "poster",
    "abstract": "Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning",
    "checked": true,
    "id": "10bd4160b44803ada6a3d2e366c44b7e2a4ffe90",
    "semantic_title": "an explanation of in-context learning as implicit bayesian inference",
    "citation_count": 847,
    "authors": []
  },
  "https://openreview.net/forum?id=w_drCosT76": {
    "title": "Differentiable Scaffolding Tree for Molecule Optimization",
    "volume": "poster",
    "abstract": "The structural design of functional molecules, also called molecular optimization, is an essential chemical science and engineering task with important applications, such as drug discovery. Deep generative models and combinatorial optimization methods achieve initial success but still struggle with directly modeling discrete chemical structures and often heavily rely on brute-force enumeration. The challenge comes from the discrete and non-differentiable nature of molecule structures. To address this, we propose differentiable scaffolding tree (DST) that utilizes a learned knowledge network to convert discrete chemical structures to locally differentiable ones. DST enables a gradient-based optimization on a chemical graph structure by back-propagating the derivatives from the target properties through a graph neural network (GNN). Our empirical studies show the gradient-based molecular optimizations are both effective and sample efficient (in terms of oracle calling number). Furthermore, the learned graph parameters can also provide an explanation that helps domain experts understand the model output. The code repository (including processed data, trained model, demonstration, molecules with the highest property) is available at https://github.com/futianfan/DST",
    "checked": true,
    "id": "3f87530bf87a3ed3a7a803f0aa7815484d5bc7e6",
    "semantic_title": "differentiable scaffolding tree for molecular optimization",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=B3Nde6lvab": {
    "title": "Eliminating Sharp Minima from SGD with Truncated Heavy-tailed Noise",
    "volume": "poster",
    "abstract": "The empirical success of deep learning is often attributed to SGD's mysterious ability to avoid sharp local minima in the loss landscape, as sharp minima are known to lead to poor generalization. Recently, empirical evidence of heavy-tailed gradient noise was reported in many deep learning tasks; and it was shown in (Simsekli et al., 2019a;b) that SGD can escape sharp local minima under the presence of such heavy-tailed gradient noise, providing a partial solution to the mystery. In this work, we analyze a popular variant of SGD where gradients are truncated above a fixed threshold. We show that it achieves a stronger notion of avoiding sharp minima: it can effectively eliminate sharp local minima entirely from its training trajectory. We characterize the dynamics of truncated SGD driven by heavy-tailed noises. First, we show that the truncation threshold and width of the attraction field dictate the order of the first exit time from the associated local minimum. Moreover, when the objective function satisfies appropriate structural conditions, we prove that as the learning rate decreases, the dynamics of the heavy-tailed truncated SGD closely resemble those of a continuous-time Markov chain that never visits any sharp minima. Real data experiments on deep learning confirm our theoretical prediction that heavy-tailed SGD with gradient clipping finds a flatter local minima and achieves better generalization",
    "checked": true,
    "id": "68e193fe7536b5298986376e3e721d97d1688de1",
    "semantic_title": "eliminating sharp minima from sgd with truncated heavy-tailed noise",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=uxxFrDwrE7Y": {
    "title": "Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System",
    "volume": "poster",
    "abstract": "Humans excel at continually learning from an ever-changing environment whereas it remains a challenge for deep neural networks which exhibit catastrophic forgetting. The complementary learning system (CLS) theory suggests that the interplay between rapid instance-based learning and slow structured learning in the brain is crucial for accumulating and retaining knowledge. Here, we propose CLS-ER, a novel dual memory experience replay (ER) method which maintains short-term and long-term semantic memories that interact with the episodic memory. Our method employs an effective replay mechanism whereby new knowledge is acquired while aligning the decision boundaries with the semantic memories. CLS-ER does not utilize the task boundaries or make any assumption about the distribution of the data which makes it versatile and suited for ``general continual learning''. Our approach achieves state-of-the-art performance on standard benchmarks as well as more realistic general continual learning settings",
    "checked": true,
    "id": "0138e7d5bfb9b47106d9a3e8821820fb53964956",
    "semantic_title": "learning fast, learning slow: a general continual learning method based on complementary learning system",
    "citation_count": 148,
    "authors": []
  },
  "https://openreview.net/forum?id=ZaVVVlcdaN": {
    "title": "FedChain: Chained Algorithms for Near-optimal Communication Cost in Federated Learning",
    "volume": "poster",
    "abstract": "Federated learning (FL) aims to minimize the communication complexity of training a model over heterogeneous data distributed across many clients. A common approach is local methods, where clients take multiple optimization steps over local data before communicating with the server (e.g., FedAvg). Local methods can exploit similarity between clients' data. However, in existing analyses, this comes at the cost of slow convergence in terms of the dependence on the number of communication rounds R. On the other hand, global methods, where clients simply return a gradient vector in each round (e.g., SGD), converge faster in terms of R but fail to exploit the similarity between clients even when clients are homogeneous. We propose FedChain, an algorithmic framework that combines the strengths of local methods and global methods to achieve fast convergence in terms of R while leveraging the similarity between clients. Using FedChain, we instantiate algorithms that improve upon previously known rates in the general convex and PL settings, and are near-optimal (via an algorithm-independent lower bound that we show) for problems that satisfy strong convexity. Empirical results support this theoretical gain over existing methods",
    "checked": true,
    "id": "5c0e39934e1b57937e8702a47888b2404ae1345e",
    "semantic_title": "fedchain: chained algorithms for near-optimal communication cost in federated learning",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=VimqQq-i_Q": {
    "title": "What Do We Mean by Generalization in Federated Learning?",
    "volume": "poster",
    "abstract": "Federated learning data is drawn from a distribution of distributions: clients are drawn from a meta-distribution, and their data are drawn from local data distributions. Generalization studies in federated learning should separate performance gaps from unseen client data (out-of-sample gap) from performance gaps from unseen client distributions (participation gap). In this work, we propose a framework for disentangling these performance gaps. Using this framework, we observe and explain differences in behavior across natural and synthetic federated datasets, indicating that dataset synthesis strategy can be important for realistic simulations of generalization in federated learning. We propose a semantic synthesis strategy that enables realistic simulation without naturally partitioned data. Informed by our Ô¨Åndings, we call out community suggestions for future federated learning works",
    "checked": true,
    "id": "58dd7435865a37e5e3fb67bf42a025b4b6491d7e",
    "semantic_title": "what do we mean by generalization in federated learning?",
    "citation_count": 85,
    "authors": []
  },
  "https://openreview.net/forum?id=ibqTBNfJmi": {
    "title": "Frequency-aware SGD for Efficient Embedding Learning with Provable Benefits",
    "volume": "poster",
    "abstract": "Embedding learning has found widespread applications in recommendation systems and natural language modeling, among other domains. To learn quality embeddings efficiently, adaptive learning rate algorithms have demonstrated superior empirical performance over SGD, largely accredited to their token-dependent learning rate. However, the underlying mechanism for the efficiency of token-dependent learning rate remains underexplored. We show that incorporating frequency information of tokens in the embedding learning problems leads to provably efficient algorithms, and demonstrate that common adaptive algorithms implicitly exploit the frequency information to a large extent. Specifically, we propose (Counter-based) Frequency-aware Stochastic Gradient Descent, which applies a frequency-dependent learning rate for each token, and exhibits provable speed-up compared to SGD when the token distribution is imbalanced. Empirically, we show the proposed algorithms are able to improve or match the performance of adaptive algorithms on benchmark recommendation tasks and a large-scale industrial recommendation system, closing the performance gap between SGD and adaptive algorithms. Our results are the first to show token-dependent learning rate provably improves convergence for non-convex embedding learning problems",
    "checked": true,
    "id": "9075e67a8711ba29bc67c6d75118b6031ecfc31a",
    "semantic_title": "frequency-aware sgd for efficient embedding learning with provable benefits",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=KeI9E-gsoB": {
    "title": "Learning Curves for Gaussian Process Regression with Power-Law Priors and Targets",
    "volume": "poster",
    "abstract": "We characterize the power-law asymptotics of learning curves for Gaussian process regression (GPR) under the assumption that the eigenspectrum of the prior and the eigenexpansion coefficients of the target function follow a power law. Under similar assumptions, we leverage the equivalence between GPR and kernel ridge regression (KRR) to show the generalization error of KRR. Infinitely wide neural networks can be related to GPR with respect to the neural network GP kernel and the neural tangent kernel, which in several cases is known to have a power-law spectrum. Hence our methods can be applied to study the generalization error of infinitely wide neural networks. We present toy experiments demonstrating the theory",
    "checked": true,
    "id": "90fa397436474f9b8937c185b85e83b8982b7038",
    "semantic_title": "learning curves for gaussian process regression with power-law priors and targets",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=0kPL3xO4R5": {
    "title": "Fast topological clustering with Wasserstein distance",
    "volume": "poster",
    "abstract": "The topological patterns exhibited by many real-world networks motivate the development of topology-based methods for assessing the similarity of networks. However, extracting topological structure is difficult, especially for large and dense networks whose node degrees range over multiple orders of magnitude. In this paper, we propose a novel and computationally practical topological clustering method that clusters complex networks with intricate topology using principled theory from persistent homology and optimal transport. Such networks are aggregated into clusters through a centroid-based clustering strategy based on both their topological and geometric structure, preserving correspondence between nodes in different networks. The notions of topological proximity and centroid are characterized using a novel and efficient approach to computation of the Wasserstein distance and barycenter for persistence barcodes associated with connected components and cycles. The proposed method is demonstrated to be effective using both simulated networks and measured functional brain networks",
    "checked": true,
    "id": "35c8dc360c4981a6926d606619a94466f4f1292f",
    "semantic_title": "fast topological clustering with wasserstein distance",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=nkaba3ND7B5": {
    "title": "Autonomous Reinforcement Learning: Formalism and Benchmarking",
    "volume": "poster",
    "abstract": "Reinforcement learning (RL) provides a naturalistic framing for learning through trial and error, which is appealing both because of its simplicity and effectiveness and because of its resemblance to how humans and animals acquire skills through experience. However, real-world embodied learning, such as that performed by humans and animals, is situated in a continual, non-episodic world, whereas common benchmark tasks in RL are episodic, with the environment resetting between trials to provide the agent with multiple attempts. This discrepancy presents a major challenge when we attempt to take RL algorithms developed for episodic simulated environments and run them on real-world platforms, such as robots. In this paper, we aim to address this discrepancy by laying out a framework for Autonomous Reinforcement Learning (ARL): reinforcement learning where the agent not only learns through its own experience, but also contends with lack of human supervision to reset between trials. We introduce a simulated benchmark EARL based on this framework, containing a set of diverse and challenging simulated tasks reflective of the hurdles introduced to learning when only a minimal reliance on extrinsic intervention can be assumed. We show that standard approaches to episodic RL and existing approaches struggle as interventions are minimized, underscoring the need for developing new algorithms for reinforcement learning with a greater focus on autonomy",
    "checked": true,
    "id": "5dd82eee3efefb96aeaaae8b817b6be2e204dc2f",
    "semantic_title": "autonomous reinforcement learning: formalism and benchmarking",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=EMxu-dzvJk": {
    "title": "GRAND++: Graph Neural Diffusion with A Source Term",
    "volume": "poster",
    "abstract": "We propose GRAph Neural Diffusion with a source term (GRAND++) for graph deep learning with a limited number of labeled nodes, i.e., low-labeling rate. GRAND++ is a class of continuous-depth graph deep learning architectures whose theoretical underpinning is the diffusion process on graphs with a source term. The source term guarantees two interesting theoretical properties of GRAND++: (i) the representation of graph nodes, under the dynamics of GRAND++, will not converge to a constant vector over all nodes even as the time goes to infinity, which mitigates the over-smoothing issue of graph neural networks and enables graph learning in very deep architectures. (ii) GRAND++ can provide accurate classification even when the model is trained with a very limited number of labeled training data. We experimentally verify the above two advantages on various graph deep learning benchmark tasks, showing a significant improvement over many existing graph neural networks",
    "checked": true,
    "id": "5635a74cf4ee75e8d7899232cd7c93d1549589e5",
    "semantic_title": "grand++: graph neural diffusion with a source term",
    "citation_count": 89,
    "authors": []
  },
  "https://openreview.net/forum?id=ZDaSIkWT-AP": {
    "title": "Case-based reasoning for better generalization in textual reinforcement learning",
    "volume": "poster",
    "abstract": "Text-based games (TBG) have emerged as promising environments for driving research in grounded language understanding and studying problems like generalization and sample efficiency. Several deep reinforcement learning (RL) methods with varying architectures and learning schemes have been proposed for TBGs. However, these methods fail to generalize efficiently, especially under distributional shifts. In a departure from deep RL approaches, in this paper, we propose a general method inspired by case-based reasoning to train agents and generalize out of the training distribution. The case-based reasoner collects instances of positive experiences from the agent's interaction with the world and later reuses the collected experiences to act efficiently. The method can be used in conjunction with any existing on-policy neural agent introduced in the literature for TBGs. Our experiments show that the proposed approach consistently improves existing methods, obtains good out-of-distribution generalization and achieves new state-of-the-art results on widely used environments",
    "checked": true,
    "id": "18e0cdc75e017b6112d674c3bd0ac5b3e35e4f82",
    "semantic_title": "case-based reasoning for better generalization in textual reinforcement learning",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=B0oHOwT5ENL": {
    "title": "Neural Deep Equilibrium Solvers",
    "volume": "poster",
    "abstract": "A deep equilibrium (DEQ) model abandons traditional depth by solving for the fixed point of a single nonlinear layer $f_\\theta$. This structure enables decoupling the internal structure of the layer (which controls representational capacity) from how the fixed point is actually computed (which impacts inference-time efficiency), which is usually via classic techniques such as Broyden's method or Anderson acceleration. In this paper, we show that one can exploit such decoupling and substantially enhance this fixed point computation using a custom neural solver. Specifically, our solver uses a parameterized network to both guess an initial value of the optimization and perform iterative updates, in a method that generalizes a learnable form of Anderson acceleration and can be trained end-to-end in an unsupervised manner. Such a solution is particularly well suited to the implicit model setting, because inference in these models requires repeatedly solving for a fixed point of the same nonlinear layer for different inputs, a task at which our network excels. Our experiments show that these neural equilibrium solvers are fast to train (only taking an extra 0.9-1.1% over the original DEQ's training time), require few additional parameters (1-3% of the original model size), yet lead to a $2\\times$ speedup in DEQ network inference without any degradation in accuracy across numerous domains and tasks",
    "checked": true,
    "id": "397912f6f31ab7099ab3ffa68645db131b32b158",
    "semantic_title": "neural deep equilibrium solvers",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=wMpS-Z_AI_E": {
    "title": "A Theoretical Analysis on Feature Learning in Neural Networks: Emergence from Inputs and Advantage over Fixed Features",
    "volume": "poster",
    "abstract": "An important characteristic of neural networks is their ability to learn representations of the input data with effective features for prediction, which is believed to be a key factor to their superior empirical performance. To better understand the source and benefit of feature learning in neural networks, we consider learning problems motivated by practical data, where the labels are determined by a set of class relevant patterns and the inputs are generated from these along with some background patterns. We prove that neural networks trained by gradient descent can succeed on these problems. The success relies on the emergence and improvement of effective features, which are learned among exponentially many candidates efficiently by exploiting the data (in particular, the structure of the input distribution). In contrast, no linear models on data-independent features of polynomial sizes can learn to as good errors. Furthermore, if the specific input structure is removed, then no polynomial algorithm in the Statistical Query model can learn even weakly. These results provide theoretical evidence showing that feature learning in neural networks depends strongly on the input structure and leads to the superior performance. Our preliminary experimental results on synthetic and real data also provide positive support",
    "checked": true,
    "id": "d2b297c553b5820ec114bfb1d037a537f2f66aad",
    "semantic_title": "a theoretical analysis on feature learning in neural networks: emergence from inputs and advantage over fixed features",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=6IYp-35L-xJ": {
    "title": "CADDA: Class-wise Automatic Differentiable Data Augmentation for EEG Signals",
    "volume": "poster",
    "abstract": "Data augmentation is a key element of deep learning pipelines, as it informs the network during training about transformations of the input data that keep the label unchanged. Manually finding adequate augmentation methods and parameters for a given pipeline is however rapidly cumbersome. In particular, while intuition can guide this decision for images, the design and choice of augmentation policies remains unclear for more complex types of data, such as neuroscience signals. Besides, class-dependent augmentation strategies have been surprisingly unexplored in the literature, although it is quite intuitive: changing the color of a car image does not change the object class to be predicted, but doing the same to the picture of an orange does. This paper investigates gradient-based automatic data augmentation algorithms amenable to class-wise policies with exponentially larger search spaces. Motivated by supervised learning applications using EEG signals for which good augmentation policies are mostly unknown, we propose a new differentiable relaxation of the problem. In the class-agnostic setting, results show that our new relaxation leads to optimal performance with faster training than competing gradient-based methods, while also outperforming gradient-free methods in the class-wise setting. This work proposes also novel differentiable augmentation operations relevant for sleep stage classification",
    "checked": true,
    "id": "80ad37dc3147702839a4415ca79855215350b1ac",
    "semantic_title": "cadda: class-wise automatic differentiable data augmentation for eeg signals",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=cOtBRgsf2fO": {
    "title": "Label Leakage and Protection in Two-party Split Learning",
    "volume": "poster",
    "abstract": "Two-party split learning is a popular technique for learning a model across feature-partitioned data. In this work, we explore whether it is possible for one party to steal the private label information from the other party during split training, and whether there are methods that can protect against such attacks. Specifically, we first formulate a realistic threat model and propose a privacy loss metric to quantify label leakage in split learning. We then show that there exist two simple yet effective methods within the threat model that can allow one party to accurately recover private ground-truth labels owned by the other party. To combat these attacks, we propose several random perturbation techniques, including $\\texttt{Marvell}$, an approach that strategically finds the structure of the noise perturbation by minimizing the amount of label leakage (measured through our quantification metric) of a worst-case adversary. We empirically demonstrate the effectiveness of our protection techniques against the identified attacks, and show that $\\texttt{Marvell}$ in particular has improved privacy-utility tradeoffs relative to baseline approaches",
    "checked": true,
    "id": "42cbf5de56f5bc4992b5ab181e9a7f0705ecd5df",
    "semantic_title": "label leakage and protection in two-party split learning",
    "citation_count": 159,
    "authors": []
  },
  "https://openreview.net/forum?id=RShaMexjc-x": {
    "title": "Semi-relaxed Gromov-Wasserstein divergence and applications on graphs",
    "volume": "poster",
    "abstract": "Comparing structured objects such as graphs is a fundamental operation involved in many learning tasks. To this end, the Gromov-Wasserstein (GW) distance, based on Optimal Transport (OT), has proven to be successful in handling the specific nature of the associated objects. More specifically, through the nodes connectivity relations, GW operates on graphs, seen as probability measures over specific spaces. At the core of OT is the idea of conservation of mass, which imposes a coupling between all the nodes from the two considered graphs. We argue in this paper that this property can be detrimental for tasks such as graph dictionary or partition learning, and we relax it by proposing a new semi-relaxed Gromov-Wasserstein divergence. Aside from immediate computational benefits, we discuss its properties, and show that it can lead to an efficient graph dictionary learning algorithm. We empirically demonstrate its relevance for complex tasks on graphs such as partitioning, clustering and completion",
    "checked": true,
    "id": "529a83b1d776657c7bef0979982689da6723b411",
    "semantic_title": "semi-relaxed gromov-wasserstein divergence and applications on graphs",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=WQc075jmBmf": {
    "title": "CodeTrek: Flexible Modeling of Code using an Extensible Relational Representation",
    "volume": "poster",
    "abstract": "Designing a suitable representation for code-reasoning tasks is challenging in aspects such as the kinds of program information to model, how to combine them, and how much context to consider. We propose CodeTrek, a deep learning approach that addresses these challenges by representing codebases as databases that conform to rich relational schemas. The relational representation not only allows CodeTrek to uniformly represent diverse kinds of program information, but also to leverage program-analysis queries to derive new semantic relations, which can be readily incorporated without further architectural engineering. CodeTrek embeds this relational representation using a set of walks that can traverse different relations in an unconstrained fashion, and incorporates all relevant attributes along the way. We evaluate CodeTrek on four diverse and challenging Python tasks: variable misuse, exception prediction, unused definition, and variable shadowing. CodeTrek achieves an accuracy of 91%, 63%, 98%, and 94% on these tasks respectively, and outperforms state-of-the-art neural models by 2-19% points",
    "checked": true,
    "id": "7bbfe2586d10d56081915a9edc44be2d29bbf8dc",
    "semantic_title": "codetrek: flexible modeling of code using an extensible relational representation",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=TZeArecH2Nf": {
    "title": "Bridging Recommendation and Marketing via Recurrent Intensity Modeling",
    "volume": "poster",
    "abstract": "This paper studies some under-explored connections between personalized recommendation and marketing systems. Obviously, these two systems are different, in two main ways. Firstly, personalized item-recommendation (ItemRec) is user-centric, whereas marketing recommends the best user-state segments (UserRec) on behalf of its item providers. (We treat different temporal states of the same user as separate marketing opportunities.) To overcome this difference, we realize a novel connection to Marked-Temporal Point Processes (MTPPs), where we view both problems as different projections from a unified temporal intensity model for all user-item pairs. Correspondingly, we derive Recurrent Intensity Models (RIMs) to extend from recurrent ItemRec models with minimal changes. The second difference between recommendation and marketing is in the temporal domains where they operate. While recommendation demands immediate responses in real-time, marketing campaigns are often long-term, setting goals to cover a given percentage of all opportunities for a given item in a given period of time. We formulate both considerations into a constrained optimization problem we call online match (OnlnMtch) and derive a solution we call Dual algorithm. Simply put, Dual modifies the real-time ItemRec scores such that the marketing constraints can be met with least compromises in user-centric utilities. Finally, our connections between recommendation and marketing may lead to novel applications. We run experiments where we use marketing as an alternative to cold-start item exploration, by setting a minimal-exposure constraint for every item in the audience base. Our experiments are available at \\url{https://github.com/awslabs/recurrent-intensity-model-experiments}",
    "checked": true,
    "id": "a4cca89cc692d83442c436d24464343cc4341f01",
    "semantic_title": "bridging recommendation and marketing via recurrent intensity modeling",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=VGnOJhd5Q1q": {
    "title": "Sparse Attention with Learning to Hash",
    "volume": "poster",
    "abstract": "Transformer has become ubiquitous in sequence modeling tasks. As a key component of Transformer, self-attention does not scale to long sequences due to its quadratic time and space complexity with respect to the sequence length. To tackle this problem, recent work developed dynamic attention sparsification techniques based on Approximate Nearest Neighbor (ANN) methods, where similar queries and keys are allocated to the same hash bucket with high probability. However, the effectiveness of those ANN methods relies on the assumption that queries and keys should lie in the same space, which is not well justified. Besides, some of the ANN methods such as Locality-Sensitive Hashing (LSH) are randomized and cannot fully utilize the available real data distributions. To overcome these issues, this paper proposes a new strategy for sparse attention, namely LHA (Learning-to-Hash Attention), which directly learns separate parameterized hash functions for queries and keys, respectively. Another advantage of LHA is that it does not impose extra constraints for queries and keys, which makes it applicable to the wide range of pre-trained Transformer models. Our experiments on evaluation of the WikiText-103 dataset for language modeling, the GLUE benchmark for natural language understanding, and the Lang-Range-Arena benchmark for multiple tasks (text/image classification, retrieval, etc.) show the superior performance of LHA over other strong Transformer variants",
    "checked": true,
    "id": "c49c292e1fb1d215c88828a52134b7ccfa52be44",
    "semantic_title": "sparse attention with learning to hash",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=dQ7Cy_ndl1s": {
    "title": "Controlling the Complexity and Lipschitz Constant improves Polynomial Nets",
    "volume": "poster",
    "abstract": "While the class of Polynomial Nets demonstrates comparable performance to neural networks (NN), it currently has neither theoretical generalization characterization nor robustness guarantees. To this end, we derive new complexity bounds for the set of Coupled CP-Decomposition (CCP) and Nested Coupled CP-decomposition (NCP) models of Polynomial Nets in terms of the $\\ell_\\infty$-operator-norm and the $\\ell_2$-operator norm. In addition, we derive bounds on the Lipschitz constant for both models to establish a theoretical certificate for their robustness. The theoretical results enable us to propose a principled regularization scheme that we also evaluate experimentally and show that it improves the accuracy as well as the robustness of the models to adversarial perturbations. We showcase how this regularization can be combined with adversarial training, resulting in further improvements",
    "checked": true,
    "id": "3de6c57094ceb108affaf6c7df73be4ff004ff64",
    "semantic_title": "controlling the complexity and lipschitz constant improves polynomial nets",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=Ug-bgjgSlKV": {
    "title": "Finding an Unsupervised Image Segmenter in each of your Deep Generative Models",
    "volume": "poster",
    "abstract": "Recent research has shown that numerous human-interpretable directions exist in the latent space of GANs. In this paper, we develop an automatic procedure for finding directions that lead to foreground-background image separation, and we use these directions to train an image segmentation model without human supervision. Our method is generator-agnostic, producing strong segmentation results with a wide range of different GAN architectures. Furthermore, by leveraging GANs pretrained on large datasets such as ImageNet, we are able to segment images from a range of domains without further training or finetuning. Evaluating our method on image segmentation benchmarks, we compare favorably to prior work while using neither human supervision nor access to the training data. Broadly, our results demonstrate that automatically extracting foreground-background structure from pretrained deep generative models can serve as a remarkably effective substitute for human supervision",
    "checked": true,
    "id": "6470d56e2b96542d191067a258261f92aa8ed82c",
    "semantic_title": "finding an unsupervised image segmenter in each of your deep generative models",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=vaRCHVj0uGI": {
    "title": "Solving Inverse Problems in Medical Imaging with Score-Based Generative Models",
    "volume": "poster",
    "abstract": "Reconstructing medical images from partial measurements is an important inverse problem in Computed Tomography (CT) and Magnetic Resonance Imaging (MRI). Existing solutions based on machine learning typically train a model to directly map measurements to medical images, leveraging a training dataset of paired images and measurements. These measurements are typically synthesized from images using a fixed physical model of the measurement process, which hinders the generalization capability of models to unknown measurement processes. To address this issue, we propose a fully unsupervised technique for inverse problem solving, leveraging the recently introduced score-based generative models. Specifically, we first train a score-based generative model on medical images to capture their prior distribution. Given measurements and a physical model of the measurement process at test time, we introduce a sampling method to reconstruct an image consistent with both the prior and the observed measurements. Our method does not assume a fixed measurement process during training, and can thus be flexibly adapted to different measurement processes at test time. Empirically, we observe comparable or better performance to supervised learning techniques in several medical imaging tasks in CT and MRI, while demonstrating significantly better generalization to unknown measurement processes",
    "checked": true,
    "id": "49f6dbf4ead6a8a3d26f9cf218a654f2f3d1d896",
    "semantic_title": "solving inverse problems in medical imaging with score-based generative models",
    "citation_count": 575,
    "authors": []
  },
  "https://openreview.net/forum?id=L7wzpQttNO": {
    "title": "BDDM: Bilateral Denoising Diffusion Models for Fast and High-Quality Speech Synthesis",
    "volume": "poster",
    "abstract": "Diffusion probabilistic models (DPMs) and their extensions have emerged as competitive generative models yet confront challenges of efficient sampling. We propose a new bilateral denoising diffusion model (BDDM) that parameterizes both the forward and reverse processes with a schedule network and a score network, which can train with a novel bilateral modeling objective. We show that the new surrogate objective can achieve a lower bound of the log marginal likelihood tighter than a conventional surrogate. We also find that BDDM allows inheriting pre-trained score network parameters from any DPMs and consequently enables speedy and stable learning of the schedule network and optimization of a noise schedule for sampling. Our experiments demonstrate that BDDMs can generate high-fidelity audio samples with as few as three sampling steps. Moreover, compared to other state-of-the-art diffusion-based neural vocoders, BDDMs produce comparable or higher quality samples indistinguishable from human speech, notably with only seven sampling steps (143x faster than WaveGrad and 28.6x faster than DiffWave). We release our code at https://github.com/tencent-ailab/bddm",
    "checked": true,
    "id": "2f67789df52a9747a64cdcd545640efd19ff199f",
    "semantic_title": "bddm: bilateral denoising diffusion models for fast and high-quality speech synthesis",
    "citation_count": 100,
    "authors": []
  },
  "https://openreview.net/forum?id=IvepFxYRDG": {
    "title": "Sample Efficient Stochastic Policy Extragradient Algorithm for Zero-Sum Markov Game",
    "volume": "poster",
    "abstract": "Two-player zero-sum Markov game is a fundamental problem in reinforcement learning and game theory. Although many algorithms have been proposed for solving zero-sum Markov games in the existing literature, many of them either require a full knowledge of the environment or are not sample-efficient. In this paper, we develop a fully decentralized and sample-efficient stochastic policy extragradient algorithm for solving tabular zero-sum Markov games. In particular, our algorithm utilizes multiple stochastic estimators to accurately estimate the value functions involved in the stochastic updates, and leverages entropy regularization to accelerate the convergence. Specifically, with a proper entropy-regularization parameter, we prove that the stochastic policy extragradient algorithm has a sample complexity of the order $\\widetilde{\\mathcal{O}}(\\frac{A_{\\max}}{\\mu_{\\text{min}}\\epsilon^{5.5}(1-\\gamma)^{13.5}})$ for finding a solution that achieves $\\epsilon$-Nash equilibrium duality gap, where $A_{\\max}$ is the maximum number of actions between the players, $\\mu_{\\min}$ is the lower bound of state stationary distribution, and $\\gamma$ is the discount factor. Such a sample complexity result substantially improves the state-of-the-art complexity result",
    "checked": true,
    "id": "056ac5a492788ff7d93fe4f7edd7971c4c658af9",
    "semantic_title": "sample efficient stochastic policy extragradient algorithm for zero-sum markov game",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=3wNcr5nq56": {
    "title": "The Uncanny Similarity of Recurrence and Depth",
    "volume": "poster",
    "abstract": "It is widely believed that deep neural networks contain layer specialization, wherein networks extract hierarchical features representing edges and patterns in shallow layers and complete objects in deeper layers. Unlike common feed-forward models that have distinct filters at each layer, recurrent networks reuse the same parameters at various depths. In this work, we observe that recurrent models exhibit the same hierarchical behaviors and the same performance benefits as depth despite reusing the same filters at every recurrence. By training models of various feed-forward and recurrent architectures on several datasets for image classification as well as maze solving, we show that recurrent networks have the ability to closely emulate the behavior of non-recurrent deep models, often doing so with far fewer parameters",
    "checked": true,
    "id": "0c9ea8f25c4f29a28e1c04c0c7121b22b6daa3bf",
    "semantic_title": "the uncanny similarity of recurrence and depth",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=l8It-0lE5e7": {
    "title": "Implicit Bias of Adversarial Training for Deep Neural Networks",
    "volume": "poster",
    "abstract": "We provide theoretical understandings of the implicit bias imposed by adversarial training for homogeneous deep neural networks without any explicit regularization. In particular, for deep linear networks adversarially trained by gradient descent on a linearly separable dataset, we prove that the direction of the product of weight matrices converges to the direction of the max-margin solution of the original dataset. Furthermore, we generalize this result to the case of adversarial training for non-linear homogeneous deep neural networks without the linear separability of the dataset. We show that, when the neural network is adversarially trained with $\\ell_2$ or $\\ell_{\\infty}$ FGSM, FGM and PGD perturbations, the direction of the limit point of normalized parameters of the network along the trajectory of the gradient flow converges to a KKT point of a constrained optimization problem that aims to maximize the margin for adversarial examples. Our results theoretically justify the longstanding conjecture that adversarial training modifies the decision boundary by utilizing adversarial examples to improve robustness, and potentially provides insights for designing new robust training strategies",
    "checked": true,
    "id": "c33ce165a65ce4372cfc64d1e6cbd6e9b6b1bf0d",
    "semantic_title": "implicit bias of adversarial training for deep neural networks",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=_SJ-_yyes8": {
    "title": "Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning",
    "volume": "poster",
    "abstract": "We present DrQ-v2, a model-free reinforcement learning (RL) algorithm for visual continuous control. DrQ-v2 builds on DrQ, an off-policy actor-critic approach that uses data augmentation to learn directly from pixels. We introduce several improvements that yield state-of-the-art results on the DeepMind Control Suite. Notably, DrQ-v2 is able to solve complex humanoid locomotion tasks directly from pixel observations, previously unattained by model-free RL. DrQ-v2 is conceptually simple, easy to implement, and provides significantly better computational footprint compared to prior work, with the majority of tasks taking just 8 hours to train on a single GPU. Finally, we publicly release DrQ-v2 's implementation to provide RL practitioners with a strong and computationally efficient baseline",
    "checked": true,
    "id": "e06c005e98281af455c454ce2478285f6f3afeca",
    "semantic_title": "mastering visual continuous control: improved data-augmented reinforcement learning",
    "citation_count": 392,
    "authors": []
  },
  "https://openreview.net/forum?id=MMAeCXIa89": {
    "title": "$\\pi$BO: Augmenting Acquisition Functions with User Beliefs for Bayesian Optimization",
    "volume": "poster",
    "abstract": "Bayesian optimization (BO) has become an established framework and popular tool for hyperparameter optimization (HPO) of machine learning (ML) algorithms. While known for its sample-efficiency, vanilla BO can not utilize readily available prior beliefs the practitioner has on the potential location of the optimum. Thus, BO disregards a valuable source of information, reducing its appeal to ML practitioners. To address this issue, we propose $\\pi$BO, an acquisition function generalization which incorporates prior beliefs about the location of the optimum in the form of a probability distribution, provided by the user. In contrast to previous approaches, $\\pi$BO is conceptually simple and can easily be integrated with existing libraries and many acquisition functions. We provide regret bounds when $\\pi$BO is applied to the common Expected Improvement acquisition function and prove convergence at regular rates independently of the prior. Further, our experiments show that $\\pi$BO outperforms competing approaches across a wide suite of benchmarks and prior characteristics. We also demonstrate that $\\pi$BO improves on the state-of-the-art performance for a popular deep learning task, with a $12.5\\times$ time-to-accuracy speedup over prominent BO approaches",
    "checked": true,
    "id": "5abbeaf5edb11e5d3026c143da94650b72b57e7b",
    "semantic_title": "œÄbo: augmenting acquisition functions with user beliefs for bayesian optimization",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=14F3fI6MGxX": {
    "title": "A Generalized Weighted Optimization Method for Computational Learning and Inversion",
    "volume": "poster",
    "abstract": "The generalization capacity of various machine learning models exhibits different phenomena in the under- and over-parameterized regimes. In this paper, we focus on regression models such as feature regression and kernel regression and analyze a generalized weighted least-squares optimization method for computational learning and inversion with noisy data. The highlight of the proposed framework is that we allow weighting in both the parameter space and the data space. The weighting scheme encodes both a priori knowledge on the object to be learned and a strategy to weight the contribution of different data points in the loss function. Here, we characterize the impact of the weighting scheme on the generalization error of the learning method, where we derive explicit generalization errors for the random Fourier feature model in both the under- and over-parameterized regimes. For more general feature maps, error bounds are provided based on the singular values of the feature matrix. We demonstrate that appropriate weighting from prior knowledge can improve the generalization capability of the learned model",
    "checked": true,
    "id": "3c211c62754794a97f14a08aab2bf14bc264cfed",
    "semantic_title": "a generalized weighted optimization method for computational learning and inversion",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=d_2lcDh0Y9c": {
    "title": "DriPP: Driven Point Processes to Model Stimuli Induced Patterns in M/EEG Signals",
    "volume": "poster",
    "abstract": "The quantitative analysis of non-invasive electrophysiology signals from electroencephalography (EEG) and magnetoencephalography (MEG) boils down to the identification of temporal patterns such as evoked responses, transient bursts of neural oscillations but also blinks or heartbeats for data cleaning. Several works have shown that these patterns can be extracted efficiently in an unsupervised way, e.g., using Convolutional Dictionary Learning. This leads to an event-based description of the data. Given these events, a natural question is to estimate how their occurrences are modulated by certain cognitive tasks and experimental manipulations. To address it, we propose a point process approach. While point processes have been used in neuroscience in the past, in particular for single cell recordings (spike trains), techniques such as Convolutional Dictionary Learning make them amenable to human studies based on EEG/MEG signals. We develop a novel statistical point process model ‚Äì called driven temporal point processes (DriPP) ‚Äì where the intensity function of the point process model is linked to a set of point processes corresponding to stimulation events. We derive a fast and principled expectation-maximization algorithm to estimate the parameters of this model. Simulations reveal that model parameters can be identified from long enough signals. Results on standard MEG datasets demonstrate that our methodology reveals event-related neural responses ‚Äì both evoked and induced ‚Äì and isolates non-task specific temporal patterns",
    "checked": true,
    "id": "b4a18dd21bb5cee2a9296eb09e863bccf5af950e",
    "semantic_title": "dripp: driven point processes to model stimuli induced patterns in m/eeg signals",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=uVXEKeqJbNa": {
    "title": "Stiffness-aware neural network for learning Hamiltonian systems",
    "volume": "poster",
    "abstract": "We propose stiffness-aware neural network (SANN), a new method for learning Hamiltonian dynamical systems from data. SANN identifies and splits the training data into stiff and nonstiff portions based on a stiffness-aware index, a simple, yet effective metric we introduce to quantify the stiffness of the dynamical system. This classification along with a resampling technique allows us to apply different time integration strategies such as step size adaptation to better capture the dynamical characteristics of the Hamiltonian vector fields. We evaluate SANN on complex physical systems including a three-body problem and billiard model. We show that SANN is more stable and can better preserve energy when compared with the state-of-the-art methods, leading to significant improvement in accuracy",
    "checked": true,
    "id": "df0dc45dbfdc6525cb210b16d83b7a4ef873b1ca",
    "semantic_title": "stiffness-aware neural network for learning hamiltonian systems",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=PilZY3omXV2": {
    "title": "CoST: Contrastive Learning of Disentangled Seasonal-Trend Representations for Time Series Forecasting",
    "volume": "poster",
    "abstract": "Deep learning has been actively studied for time series forecasting, and the mainstream paradigm is based on the end-to-end training of neural network architectures, ranging from classical LSTM/RNNs to more recent TCNs and Transformers. Motivated by the recent success of representation learning in computer vision and natural language processing, we argue that a more promising paradigm for time series forecasting, is to first learn disentangled feature representations, followed by a simple regression fine-tuning step -- we justify such a paradigm from a causal perspective. Following this principle, we propose a new time series representation learning framework for long sequence time series forecasting named CoST, which applies contrastive learning methods to learn disentangled seasonal-trend representations. CoST comprises both time domain and frequency domain contrastive losses to learn discriminative trend and seasonal representations, respectively. Extensive experiments on real-world datasets show that CoST consistently outperforms the state-of-the-art methods by a considerable margin, achieving a 21.3% improvement in MSE on multivariate benchmarks. It is also robust to various choices of backbone encoders, as well as downstream regressors. Code is available at https://github.com/salesforce/CoST",
    "checked": true,
    "id": "c090ff3dec01e06f46735b7b9ab133a5db8c73c3",
    "semantic_title": "cost: contrastive learning of disentangled seasonal-trend representations for time series forecasting",
    "citation_count": 431,
    "authors": []
  },
  "https://openreview.net/forum?id=oAy7yPmdNz": {
    "title": "CoordX: Accelerating Implicit Neural Representation with a Split MLP Architecture",
    "volume": "poster",
    "abstract": "Implicit neural representations with multi-layer perceptrons (MLPs) have recently gained prominence for a wide variety of tasks such as novel view synthesis and 3D object representation and rendering. However, a significant challenge with these representations is that both training and inference with an MLP over a large number of input coordinates to learn and represent an image, video, or 3D object, require large amounts of computation and incur long processing times. In this work, we aim to accelerate inference and training of coordinate-based MLPs for implicit neural representations by proposing a new split MLP architecture, CoordX. With CoordX, the initial layers are split to learn each dimension of the input coordinates separately. The intermediate features are then fused by the last layers to generate the learned signal at the corresponding coordinate point. This significantly reduces the amount of computation required and leads to large speedups in training and inference, while achieving similar accuracy as the baseline MLP. This approach thus aims at first learning functions that are a decomposition of the original signal and then fusing them to generate the learned signal. Our proposed architecture can be generally used for many implicit neural representation tasks with no additional memory overheads. We demonstrate a speedup of up to 2.92x compared to the baseline model for image, video, and 3D shape representation and rendering tasks",
    "checked": true,
    "id": "a6ba988fded4d8c6d66efa8744fccc1ad23733e7",
    "semantic_title": "coordx: accelerating implicit neural representation with a split mlp architecture",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=9n9c8sf0xm": {
    "title": "Plant 'n' Seek: Can You Find the Winning Ticket?",
    "volume": "poster",
    "abstract": "The lottery ticket hypothesis has sparked the rapid development of pruning algorithms that aim to reduce the computational costs associated with deep learning during training and model deployment. Currently, such algorithms are primarily evaluated on imaging data, for which we lack ground truth information and thus the understanding of how sparse lottery tickets could be. To fill this gap, we develop a framework that allows us to plant and hide winning tickets with desirable properties in randomly initialized neural networks. To analyze the ability of state-of-the-art pruning to identify tickets of extreme sparsity, we design and hide such tickets solving four challenging tasks. In extensive experiments, we observe similar trends as in imaging studies, indicating that our framework can provide transferable insights into realistic problems. Additionally, we can now see beyond such relative trends and highlight limitations of current pruning methods. Based on our results, we conclude that the current limitations in ticket sparsity are likely of algorithmic rather than fundamental nature. We anticipate that comparisons to planted tickets will facilitate future developments of efficient pruning algorithms",
    "checked": true,
    "id": "67618071e2e63921dde7471bc3c835f0cebe5a41",
    "semantic_title": "plant 'n' seek: can you find the winning ticket?",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=gjNcH0hj0LM": {
    "title": "Coherence-based Label Propagation over Time Series for Accelerated Active Learning",
    "volume": "poster",
    "abstract": "Time-series data are ubiquitous these days, but lack of the labels in time-series data is regarded as a hurdle for its broad applicability. Meanwhile, active learning has been successfully adopted to reduce the labeling efforts in various tasks. Thus, this paper addresses an important issue, time-series active learning. Inspired by the temporal coherence in time-series data, where consecutive data points tend to have the same label, our label propagation framework, called TCLP, automatically assigns a queried label to the data points within an accurately estimated time-series segment, thereby significantly boosting the impact of an individual query. Compared with traditional time-series active learning, TCLP is shown to improve the classification accuracy by up to 7.1 times when only 0.8% of data points in the entire time series are queried for their labels",
    "checked": true,
    "id": "b9c2697a3222651061c468f44eb4cb8b4f0e193e",
    "semantic_title": "coherence-based label propagation over time series for accelerated active learning",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=_X90SIKbHa": {
    "title": "A Class of Short-term Recurrence Anderson Mixing Methods and Their Applications",
    "volume": "poster",
    "abstract": "Anderson mixing (AM) is a powerful acceleration method for fixed-point iterations, but its computation requires storing many historical iterations. The extra memory footprint can be prohibitive when solving high-dimensional problems in a resource-limited machine. To reduce the memory overhead, we propose a novel class of short-term recurrence AM methods (ST-AM). The ST-AM methods only store two previous iterations with cheap corrections. We prove that the basic version of ST-AM is equivalent to the full-memory AM in strongly convex quadratic optimization, and with minor changes it has local linear convergence for solving general nonlinear fixed-point problems. We further analyze the convergence properties of the regularized ST-AM for nonconvex (stochastic) optimization. Finally, we apply ST-AM to several applications including solving root-finding problems and training neural networks. Experimental results show that ST-AM is competitive with the long-memory AM and outperforms many existing optimizers",
    "checked": true,
    "id": "fe3d665472d47df4ea60b953b81aea3ccd502ea4",
    "semantic_title": "a class of short-term recurrence anderson mixing methods and their applications",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=A05I5IvrdL-": {
    "title": "The Geometry of Memoryless Stochastic Policy Optimization in Infinite-Horizon POMDPs",
    "volume": "poster",
    "abstract": "We consider the problem of finding the best memoryless stochastic policy for an infinite-horizon partially observable Markov decision process (POMDP) with finite state and action spaces with respect to either the discounted or mean reward criterion. We show that the (discounted) state-action frequencies and the expected cumulative reward are rational functions of the policy, whereby the degree is determined by the degree of partial observability. We then describe the optimization problem as a linear optimization problem in the space of feasible state-action frequencies subject to polynomial constraints that we characterize explicitly. This allows us to address the combinatorial and geometric complexity of the optimization problem using recent tools from polynomial optimization. In particular, we demonstrate how the partial observability constraints can lead to multiple smooth and non-smooth local optimizers and we estimate the number of critical points",
    "checked": true,
    "id": "992fac6a20eb8ef5e921213d8518a82fe76d36fa",
    "semantic_title": "the geometry of memoryless stochastic policy optimization in infinite-horizon pomdps",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=n0OeTdNRG0Q": {
    "title": "Efficient Sharpness-aware Minimization for Improved Training of Neural Networks",
    "volume": "poster",
    "abstract": "Overparametrized Deep Neural Networks (DNNs) often achieve astounding performances, but may potentially result in severe generalization error. Recently, the relation between the sharpness of the loss landscape and the generalization error has been established by Foret et al. (2020), in which the Sharpness Aware Minimizer (SAM) was proposed to mitigate the degradation of the generalization. Unfortunately, SAM's computational cost is roughly double that of base optimizers, such as Stochastic Gradient Descent (SGD). This paper thus proposes Efficient Sharpness Aware Minimizer (ESAM), which boosts SAM's efficiency at no cost to its generalization performance. ESAM includes two novel and efficient training strategies‚ÄîStochasticWeight Perturbation and Sharpness-Sensitive Data Selection. In the former, the sharpness measure is approximated by perturbing a stochastically chosen set of weights in each iteration; in the latter, the SAM loss is optimized using only a judiciously selected subset of data that is sensitive to the sharpness. We provide theoretical explanations as to why these strategies perform well. We also show, via extensive experiments on the CIFAR and ImageNet datasets, that ESAM enhances the efficiency over SAM from requiring 100% extra computations to 40% vis-`a-vis base optimizers, while test accuracies are preserved or even improved",
    "checked": true,
    "id": "a82ae40ecc5ea5e33b52c87c9464510cab7bf9d9",
    "semantic_title": "efficient sharpness-aware minimization for improved training of neural networks",
    "citation_count": 150,
    "authors": []
  },
  "https://openreview.net/forum?id=BGvt0ghNgA": {
    "title": "Lipschitz-constrained Unsupervised Skill Discovery",
    "volume": "poster",
    "abstract": "We study the problem of unsupervised skill discovery, whose goal is to learn a set of diverse and useful skills with no external reward. There have been a number of skill discovery methods based on maximizing the mutual information (MI) between skills and states. However, we point out that their MI objectives usually prefer static skills to dynamic ones, which may hinder the application for downstream tasks. To address this issue, we propose Lipschitz-constrained Skill Discovery (LSD), which encourages the agent to discover more diverse, dynamic, and far-reaching skills. Another benefit of LSD is that its learned representation function can be utilized for solving goal-following downstream tasks even in a zero-shot manner ‚Äî i.e., without further training or complex planning. Through experiments on various MuJoCo robotic locomotion and manipulation environments, we demonstrate that LSD outperforms previous approaches in terms of skill diversity, state space coverage, and performance on seven downstream tasks including the challenging task of following multiple goals on Humanoid. Our code and videos are available at https://shpark.me/projects/lsd/",
    "checked": true,
    "id": "bf5d2d302f046cda8f75ceb09c842109e09c5862",
    "semantic_title": "lipschitz-constrained unsupervised skill discovery",
    "citation_count": 57,
    "authors": []
  },
  "https://openreview.net/forum?id=zBOI9LFpESK": {
    "title": "Learning Generalizable Representations for Reinforcement Learning via Adaptive Meta-learner of Behavioral Similarities",
    "volume": "poster",
    "abstract": "How to learn an effective reinforcement learning-based model for control tasks from high-level visual observations is a practical and challenging problem. A key to solving this problem is to learn low-dimensional state representations from observations, from which an effective policy can be learned. In order to boost the learning of state encoding, recent works are focused on capturing behavioral similarities between state representations or applying data augmentation on visual observations. In this paper, we propose a novel meta-learner-based framework for representation learning regarding behavioral similarities for reinforcement learning. Specifically, our framework encodes the high-dimensional observations into two decomposed embeddings regarding reward and dynamics in a Markov Decision Process (MDP). A pair of meta-learners are developed, one of which quantifies the reward similarity and the other quantifies dynamics similarity over the correspondingly decomposed embeddings. The meta-learners are self-learned to update the state embeddings by approximating two disjoint terms in on-policy bisimulation metric. To incorporate the reward and dynamics terms, we further develop a strategy to adaptively balance their impacts based on different tasks or environments. We empirically demonstrate that our proposed framework outperforms state-of-the-art baselines on several benchmarks, including conventional DM Control Suite, Distracting DM Control Suite and a self-driving task CARLA",
    "checked": true,
    "id": "855bcc08e94a5bd82865b79b6bb9af4727e65726",
    "semantic_title": "learning generalizable representations for reinforcement learning via adaptive meta-learner of behavioral similarities",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=xa6otUDdP2W": {
    "title": "Effective Model Sparsification by Scheduled Grow-and-Prune Methods",
    "volume": "poster",
    "abstract": "Deep neural networks (DNNs) are effective in solving many real-world problems. Larger DNN models usually exhibit better quality (e.g., accuracy) but their excessive computation results in long inference time. Model sparsification can reduce the computation and memory cost while maintaining model quality. Most existing sparsification algorithms unidirectionally remove weights, while others randomly or greedily explore a small subset of weights in each layer for pruning. The limitations of these algorithms reduce the level of achievable sparsity. In addition, many algorithms still require pre-trained dense models and thus suffer from large memory footprint. In this paper, we propose a novel scheduled grow-and-prune (GaP) methodology without having to pre-train a dense model. It addresses the shortcomings of the previous works by repeatedly growing a subset of layers to dense and then pruning them back to sparse after some training. Experiments show that the models pruned using the proposed methods match or beat the quality of the highly optimized dense models at 80% sparsity on a variety of tasks, such as image classification, objective detection, 3D object part segmentation, and translation. They also outperform other state-of-the-art (SOTA) methods for model sparsification. As an example, a 90% non-uniform sparse ResNet-50 model obtained via GaP achieves 77.9% top-1 accuracy on ImageNet, improving the previous SOTA results by 1.5%. Code available at: https://github.com/boone891214/GaP",
    "checked": true,
    "id": "62b8b5da4f10230567b14d98cd92ad858bc80bbd",
    "semantic_title": "effective model sparsification by scheduled grow-and-prune methods",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=cpDhcsEDC2": {
    "title": "FILIP: Fine-grained Interactive Language-Image Pre-Training",
    "volume": "poster",
    "abstract": "Unsupervised large-scale vision-language pre-training has shown promising advances on various downstream tasks. Existing methods often model the cross-modal interaction either via the similarity of the global feature of each modality which misses sufficient information, or finer-grained interactions using cross/self-attention upon visual and textual tokens. However, cross/self-attention suffers from inferior efficiency in both training and inference. In this paper, we introduce a large-scale Fine-grained Interactive Language-Image Pre-training (FILIP) to achieve finer-level alignment through a cross-modal late interaction mechanism, which uses a token-wise maximum similarity between visual and textual tokens to guide the contrastive objective. FILIP successfully leverages the finer-grained expressiveness between image patches and textual words by modifying only contrastive loss, while simultaneously gaining the ability to pre-compute image and text representations offline at inference, keeping both large-scale training and inference efficient. Furthermore, we construct a new large-scale image-text pair dataset called FILIP300M for pre-training. Experiments show that FILIP achieves state-of-the-art performance on multiple downstream vision-language tasks including zero-shot image classification and image-text retrieval. The visualization on word-patch alignment further shows that FILIP can learn meaningful fine-grained features with promising localization ability",
    "checked": true,
    "id": "f675c62abfa788ea0be85d3124eba15a14d5e9d6",
    "semantic_title": "filip: fine-grained interactive language-image pre-training",
    "citation_count": 694,
    "authors": []
  },
  "https://openreview.net/forum?id=DfUjyyRW90": {
    "title": "Information Prioritization through Empowerment in Visual Model-based RL",
    "volume": "poster",
    "abstract": "Model-based reinforcement learning (RL) algorithms designed for handling complex visual observations typically learn some sort of latent state representation, either explicitly or implicitly. Standard methods of this sort do not distinguish between functionally relevant aspects of the state and irrelevant distractors, instead aiming to represent all available information equally. We propose a modified objective for model-based RL that, in combination with mutual information maximization, allows us to learn representations and dynamics for visual model-based RL without reconstruction in a way that explicitly prioritizes functionally relevant factors. The key principle behind our design is to integrate a term inspired by variational empowerment into a state-space learning model based on mutual information. This term prioritizes information that is correlated with action, thus ensuring that functionally relevant factors are captured first. Furthermore, the same empowerment term also promotes faster exploration during the RL process, especially for sparse-reward tasks where the reward signal is insufficient to drive exploration in the early stages of learning. We evaluate the approach on a suite of vision-based robot control tasks with natural video backgrounds, and show that the proposed prioritized information objective outperforms state-of-the-art model based RL approaches by an average of 20\\% in terms of episodic returns at 1M environment interactions with 30\\% higher sample efficiency at 100k interactions",
    "checked": true,
    "id": "9229fe9049677b0d00a38713bf1642a1955a1f18",
    "semantic_title": "information prioritization through empowerment in visual model-based rl",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=nO5caZwFwYu": {
    "title": "Efficient Active Search for Combinatorial Optimization Problems",
    "volume": "poster",
    "abstract": "Recently numerous machine learning based methods for combinatorial optimization problems have been proposed that learn to construct solutions in a sequential decision process via reinforcement learning. While these methods can be easily combined with search strategies like sampling and beam search, it is not straightforward to integrate them into a high-level search procedure offering strong search guidance. Bello et al. (2016) propose active search, which adjusts the weights of a (trained) model with respect to a single instance at test time using reinforcement learning. While active search is simple to implement, it is not competitive with state-of-the-art methods because adjusting all model weights for each test instance is very time and memory intensive. Instead of updating all model weights, we propose and evaluate three efficient active search strategies that only update a subset of parameters during the search. The proposed methods offer a simple way to significantly improve the search performance of a given model and outperform state-of-the-art machine learning based methods on combinatorial problems, even surpassing the well-known heuristic solver LKH3 on the capacitated vehicle routing problem. Finally, we show that (efficient) active search enables learned models to effectively solve instances that are much larger than those seen during training",
    "checked": true,
    "id": "e3f453ee3d2e084cb0f24769ead763844dbc0661",
    "semantic_title": "efficient active search for combinatorial optimization problems",
    "citation_count": 101,
    "authors": []
  },
  "https://openreview.net/forum?id=FZoZ7a31GCW": {
    "title": "Ancestral protein sequence reconstruction using a tree-structured Ornstein-Uhlenbeck variational autoencoder",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8461c8b7dbbf71c0f3aea3e6a5852bcc21620401",
    "semantic_title": "ancestral protein sequence reconstruction using a tree-structured ornstein-uhlenbeck variational autoencoder",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=mdUYT5QV0O": {
    "title": "Training Structured Neural Networks Through Manifold Identification and Variance Reduction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "458a5aaf98a159e8c0d1280b0e034e3864000375",
    "semantic_title": "training structured neural networks through manifold identification and variance reduction",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=KBQP4A_J1K": {
    "title": "The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e528466e2aff981511d4ca6e063211297c0b4175",
    "semantic_title": "the neural data router: adaptive control flow in transformers improves systematic generalization",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=w-CPUXXrAj": {
    "title": "On the Limitations of Multimodal VAEs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9ff70d133e998a909d1b111f1ee4e86cb12a56ba",
    "semantic_title": "on the limitations of multimodal vaes",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=CSfcOznpDY": {
    "title": "Recursive Disentanglement Network",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e41f9d672ddd894f7ab531f1f6703ce7dd53de5a",
    "semantic_title": "recursive disentanglement network",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=CgIEctmcXx1": {
    "title": "ADAVI: Automatic Dual Amortized Variational Inference Applied To Pyramidal Bayesian Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "168996ed9f03045c5dfc5d669041e0a2e7622b12",
    "semantic_title": "adavi: automatic dual amortized variational inference applied to pyramidal bayesian models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=a34GrNaYEcS": {
    "title": "Distributionally Robust Models with Parametric Likelihood Ratios",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "51836dfa1542277ed982612caa90ecf31ead4ba8",
    "semantic_title": "distributionally robust models with parametric likelihood ratios",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=gbe1zHyA73": {
    "title": "Constrained Physical-Statistics Models for Dynamical System Identification and Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c8651d5306c1e30cf0636b05fa346dae0ae3f0dd",
    "semantic_title": "constrained physical-statistics models for dynamical system identification and prediction",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=HCelXXcSEuH": {
    "title": "Doubly Adaptive Scaled Algorithm for Machine Learning Using Second-Order Information",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "31f79d616785e4ce096953d6524b1032031cc82f",
    "semantic_title": "doubly adaptive scaled algorithm for machine learning using second-order information",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=rI0LYgGeYaw": {
    "title": "Understanding approximate and unrolled dictionary learning for pattern recovery",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1909e2176bbc8b2c3f88c7d78b37a5e00b697d11",
    "semantic_title": "understanding approximate and unrolled dictionary learning for pattern recovery",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=jbrgwbv8nD": {
    "title": "Constraining Linear-chain CRFs to Regular Languages",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d663220ba20b2896d7ccf7167fb622cedcf75f64",
    "semantic_title": "constraining linear-chain crfs to regular languages",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=vHVcB-ak3Si": {
    "title": "Dive Deeper Into Integral Pose Regression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4b9a056604982ce80ecff6a59a8f5f2b5592012c",
    "semantic_title": "dive deeper into integral pose regression",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=84NMXTHYe-": {
    "title": "Evidential Turing Processes",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "81314b15e4dd3a5c12f5a186beedd381a037d272",
    "semantic_title": "evidential turing processes",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=vJb4I2ANmy": {
    "title": "Noisy Feature Mixup",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a1653c7f5a3e136b382475d9ff82a80ef24262cf",
    "semantic_title": "noisy feature mixup",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=moHCzz6D5H3": {
    "title": "Peek-a-Boo: What (More) is Disguised in a Randomly Weighted Neural Network, and How to Find It Efficiently",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d7d1cee22d578a833dfd91282ff501b0a65a86ae",
    "semantic_title": "peek-a-boo: what (more) is disguised in a randomly weighted neural network, and how to find it efficiently",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=EwqEx5ipbOu": {
    "title": "How Well Does Self-Supervised Pre-Training Perform with Streaming Data?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2824c3ca577822d8201d48127585e0f86284f01b",
    "semantic_title": "how well does self-supervised pre-training perform with streaming data?",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=boJy41J-tnQ": {
    "title": "Subspace Regularizers for Few-Shot Class Incremental Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f8d77d2d33a86a02c7d2ddd3dcbb1dc48ccf265c",
    "semantic_title": "subspace regularizers for few-shot class incremental learning",
    "citation_count": 72,
    "authors": []
  },
  "https://openreview.net/forum?id=OtEDS2NWhqa": {
    "title": "Using Graph Representation Learning with Schema Encoders to Measure the Severity of Depressive Symptoms",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6080c4cd6847cbd1c756928639bab086f57415d9",
    "semantic_title": "using graph representation learning with schema encoders to measure the severity of depressive symptoms",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=DTXZqTNV5nW": {
    "title": "Actor-Critic Policy Optimization in a Large-Scale Imperfect-Information Game",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "70baefe23e4e0d2de3c1e7be689ce6092c4638ea",
    "semantic_title": "actor-critic policy optimization in a large-scale imperfect-information game",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=EHaUTlm2eHg": {
    "title": "Policy Gradients Incorporating the Future",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "32bcee7bafa179cf468b3ec0bd66a2c91104573c",
    "semantic_title": "policy gradients incorporating the future",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=rzvOQrnclO0": {
    "title": "Gradient Information Matters in Policy Optimization by Back-propagating through Model",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1930877e3bd43a684154f28fa4a226d030e1624b",
    "semantic_title": "gradient information matters in policy optimization by back-propagating through model",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=xm6YD62D1Ub": {
    "title": "VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0d0cf5f64c052aa7edc5bb638203616a620557f6",
    "semantic_title": "vicreg: variance-invariance-covariance regularization for self-supervised learning",
    "citation_count": 1014,
    "authors": []
  },
  "https://openreview.net/forum?id=gI7feJ9yXPz": {
    "title": "High Probability Generalization Bounds with Fast Rates for Minimax Problems",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "aaf43fc215965b0b7818e77a3b30cd21fc65bba8",
    "semantic_title": "high probability generalization bounds with fast rates for minimax problems",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=Z8FzvVU6_Kj": {
    "title": "SUMNAS: Supernet with Unbiased Meta-Features for Neural Architecture Search",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4f92a16b5e7786e1c4b89699d71094a37d9dc9bc",
    "semantic_title": "sumnas: supernet with unbiased meta-features for neural architecture search",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=_XNtisL32jv": {
    "title": "Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0faf7b08fc18a22ccd0eac8fae0eee13913755ba",
    "semantic_title": "temporal efficient training of spiking neural network via gradient re-weighting",
    "citation_count": 289,
    "authors": []
  },
  "https://openreview.net/forum?id=u6TRGdzhfip": {
    "title": "Reliable Adversarial Distillation with Unreliable Teachers",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6bb6359ad507653e2785d9fe924fc1feff39e0e9",
    "semantic_title": "reliable adversarial distillation with unreliable teachers",
    "citation_count": 72,
    "authors": []
  },
  "https://openreview.net/forum?id=NyJ2KIN8P17": {
    "title": "Neural Program Synthesis with Query",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e32f6c3997fe4945e84effb84b1fb79fb59bfdcb",
    "semantic_title": "neural program synthesis with query",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=HTVch9AMPa": {
    "title": "Delaunay Component Analysis for Evaluation of Data Representations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b24d290cea1cb2c34f83e374c28ec1cb84ef7c2e",
    "semantic_title": "delaunay component analysis for evaluation of data representations",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=p0rCmDEN_-": {
    "title": "Visual hyperacuity with moving sensor and recurrent neural computations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1bf45f35f6f32822d20c22715b1b9543d74b2c3f",
    "semantic_title": "visual hyperacuity with moving sensor and recurrent neural computations",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=2ggNjUisGyr": {
    "title": "Partial Wasserstein Adversarial Network for Non-rigid Point Set Registration",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a353c993af7fc69b9598d204bd3b5756a300641e",
    "semantic_title": "partial wasserstein adversarial network for non-rigid point set registration",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=xFOyMwWPkz": {
    "title": "Quantitative Performance Assessment of CNN Units via Topological Entropy Calculation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e72ecb27924838e0614834180955f5ae029fe045",
    "semantic_title": "quantitative performance assessment of cnn units via topological entropy calculation",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=1zwleytEpYx": {
    "title": "Imitation Learning by Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "08fead9e534ad82d46317284b070d0f17034bddb",
    "semantic_title": "imitation learning by reinforcement learning",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=81e1aeOt-sd": {
    "title": "On-Policy Model Errors in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c425b527bbeffc2e4b5bc7a42649cd16a3fa216f",
    "semantic_title": "on-policy model errors in reinforcement learning",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=O50443AsCP": {
    "title": "TAPEX: Table Pre-training via Learning a Neural SQL Executor",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2406cf39805c70264c4226b7325a09b506c70921",
    "semantic_title": "tapex: table pre-training via learning a neural sql executor",
    "citation_count": 293,
    "authors": []
  },
  "https://openreview.net/forum?id=9SDQB3b68K": {
    "title": "DARA: Dynamics-Aware Reward Augmentation in Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "Offline reinforcement learning algorithms promise to be applicable in settings where a fixed dataset is available and no new experience can be acquired. However, such formulation is inevitably offline-data-hungry and, in practice, collecting a large offline dataset for one specific task over one specific environment is also costly and laborious. In this paper, we thus 1) formulate the offline dynamics adaptation by using (source) offline data collected from another dynamics to relax the requirement for the extensive (target) offline data, 2) characterize the dynamics shift problem in which prior offline methods do not scale well, and 3) derive a simple dynamics-aware reward augmentation (DARA) framework from both model-free and model-based offline settings. Specifically, DARA emphasizes learning from those source transition pairs that are adaptive for the target environment and mitigates the offline dynamics shift by characterizing state-action-next-state pairs instead of the typical state-action distribution sketched by prior offline RL methods. The experimental evaluation demonstrates that DARA, by augmenting rewards in the source offline dataset, can acquire an adaptive policy for the target environment and yet significantly reduce the requirement of target offline data. With only modest amounts of target offline data, our performance consistently outperforms the prior offline RL methods in both simulated and real-world tasks",
    "checked": true,
    "id": "33fae337de4cbf73dcf55acac1a2605fb727eadb",
    "semantic_title": "dara: dynamics-aware reward augmentation in offline reinforcement learning",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=P07dq7iSAGr": {
    "title": "Explaining Point Processes by Learning Interpretable Temporal Logic Rules",
    "volume": "poster",
    "abstract": "We propose a principled method to learn a set of human-readable logic rules to explain temporal point processes. We assume that the generative mechanisms underlying the temporal point processes are governed by a set of first-order temporal logic rules, as a compact representation of domain knowledge. Our method formulates the rule discovery process from noisy event data as a maximum likelihood problem, and designs an efficient and tractable branch-and-price algorithm to progressively search for new rules and expand existing rules. The proposed algorithm alternates between the rule generation stage and the rule evaluation stage, and uncovers the most important collection of logic rules within a fixed time limit for both synthetic and real event data. In a real healthcare application, we also had human experts (i.e., doctors) verify the learned temporal logic rules and provide further improvements. These expert-revised interpretable rules lead to a point process model which outperforms previous state-of-the-arts for symptom prediction, both in their occurrence times and types",
    "checked": true,
    "id": "07938ad12ced90fe381803b9ad754ea51fb77120",
    "semantic_title": "explaining point processes by learning interpretable temporal logic rules",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=eBCmOocUejf": {
    "title": "On Robust Prefix-Tuning for Text Classification",
    "volume": "poster",
    "abstract": "Recently, prefix-tuning has gained increasing attention as a parameter-efficient finetuning method for large-scale pretrained language models. The method keeps the pretrained models fixed and only updates the prefix token parameters for each downstream task. Despite being lightweight and modular, prefix-tuning still lacks robustness to textual adversarial attacks. However, most currently developed defense techniques necessitate auxiliary model update and storage, which inevitably hamper the modularity and low storage of prefix-tuning. In this work, we propose a robust prefix-tuning framework that preserves the efficiency and modularity of prefix-tuning. The core idea of our framework is leveraging the layerwise activations of the language model by correctly-classified training data as the standard for additional prefix finetuning. During the test phase, an extra batch-level prefix is tuned for each batch and added to the original prefix for robustness enhancement. Extensive experiments on three text classification benchmarks show that our framework substantially improves robustness over several strong baselines against five textual attacks of different types while maintaining comparable accuracy on clean texts. We also interpret our robust prefix-tuning framework from the optimal control perspective and pose several directions for future research",
    "checked": true,
    "id": "4bcd4f8ef3f269562dce183ed0329f93b24fd4e6",
    "semantic_title": "on robust prefix-tuning for text classification",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=0sgntlpKDOz": {
    "title": "Learning Graphon Mean Field Games and Approximate Nash Equilibria",
    "volume": "poster",
    "abstract": "Recent advances at the intersection of dense large graph limits and mean field games have begun to enable the scalable analysis of a broad class of dynamical sequential games with large numbers of agents. So far, results have been largely limited to graphon mean field systems with continuous-time diffusive or jump dynamics, typically without control and with little focus on computational methods. We propose a novel discrete-time formulation for graphon mean field games as the limit of non-linear dense graph Markov games with weak interaction. On the theoretical side, we give extensive and rigorous existence and approximation properties of the graphon mean field solution in sufficiently large systems. On the practical side we provide general learning schemes for graphon mean field equilibria by either introducing agent equivalence classes or reformulating the graphon mean field system as a classical mean field system. By repeatedly finding a regularized optimal control solution and its generated mean field, we successfully obtain plausible approximate Nash equilibria in otherwise infeasible large dense graph games with many agents. Empirically, we are able to demonstrate on a number of examples that the finite-agent behavior comes increasingly close to the mean field behavior for our computed equilibria as the graph or system size grows, verifying our theory. More generally, we successfully apply policy gradient reinforcement learning in conjunction with sequential Monte Carlo methods",
    "checked": true,
    "id": "7656387415a75d0825f7f47c91f55223af1e26e4",
    "semantic_title": "learning graphon mean field games and approximate nash equilibria",
    "citation_count": 39,
    "authors": []
  },
  "https://openreview.net/forum?id=UtGtoS4CYU": {
    "title": "Measuring CLEVRness: Black-box Testing of Visual Reasoning Models",
    "volume": "poster",
    "abstract": "How can we measure the reasoning capabilities of intelligence systems? Visual question answering provides a convenient framework for testing the model's abilities by interrogating the model through questions about the scene. However, despite scores of various visual QA datasets and architectures, which sometimes yield even a super-human performance, the question of whether those architectures can actually reason remains open to debate. To answer this, we extend the visual question answering framework and propose the following behavioral test in the form of a two-player game. We consider black-box neural models of CLEVR. These models are trained on a diagnostic dataset benchmarking reasoning. Next, we train an adversarial player that re-configures the scene to fool the CLEVR model. We show that CLEVR models, which otherwise could perform at a ``human-level'', can easily be fooled by our agent. Our results put in doubt whether data-driven approaches can do reasoning without exploiting the numerous biases that are often present in those datasets. Finally, we also propose a controlled experiment measuring the efficiency of such models to learn and perform reasoning",
    "checked": true,
    "id": "6847e2ba6796b8a786b3b6d8d8a2d922a6c7c31d",
    "semantic_title": "measuring clevrness: blackbox testing of visual reasoning models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=qqdXHUGec9h": {
    "title": "Exploiting Class Activation Value for Partial-Label Learning",
    "volume": "poster",
    "abstract": "Partial-label learning (PLL) solves the multi-class classification problem, where each training instance is assigned a set of candidate labels that include the true label. Recent advances showed that PLL can be compatible with deep neural networks, which achieved state-of-the-art performance. However, most of the existing deep PLL methods focus on designing proper training objectives under various assumptions on the collected data, which may limit their performance when the collected data cannot satisfy the adopted assumptions. In this paper, we propose to exploit the learned intrinsic representation of the model to identify the true label in the training process, which does not rely on any assumptions on the collected data. We make two key contributions. As the first contribution, we empirically show that the class activation map (CAM), a simple technique for discriminating the learning patterns of each class in images, could surprisingly be utilized to make accurate predictions on selecting the true label from candidate labels. Unfortunately, as CAM is confined to image inputs with convolutional neural networks, we are yet unable to directly leverage CAM to address the PLL problem with general inputs and models. Thus, as the second contribution, we propose the class activation value (CAV), which owns similar properties of CAM, while CAV is versatile in various types of inputs and models. Building upon CAV, we propose a novel method named CAV Learning (CAVL) that selects the true label by the class with the maximum CAV for model training. Extensive experiments on various datasets demonstrate that our proposed CAVL method achieves state-of-the-art performance",
    "checked": true,
    "id": "6bb4e8e53b8c47d60d01de40dc5fcc4055007995",
    "semantic_title": "exploiting class activation value for partial-label learning",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=9-Rfew334N": {
    "title": "Givens Coordinate Descent Methods for Rotation Matrix Learning in Trainable Embedding Indexes",
    "volume": "poster",
    "abstract": "Product quantization (PQ) coupled with a space rotation, is widely used in modern approximate nearest neighbor (ANN) search systems to significantly compress the disk storage for embeddings and speed up the inner product computation. Existing rotation learning methods, however, minimize quantization distortion for fixed embeddings, which are not applicable to an end-to-end training scenario where embeddings are updated constantly. In this paper, based on geometric intuitions from Lie group theory, in particular the special orthogonal groupSO(n), we propose a family of block Givens coordinate descent algorithms to learn rotation matrix that are provably convergent on any convex objectives. Compared to the state-of-the-art SVD method, the Givens algorithms are much more parallelizable, reducing runtime by orders of magnitude on modern GPUs, and converge more stably according to experimental studies. They further improve upon vanilla product quantization significantly in an end-to-end training scenario",
    "checked": true,
    "id": "48e3b328bc8ba8ce78f2a0dc3be2adddf05b76fb",
    "semantic_title": "givens coordinate descent methods for rotation matrix learning in trainable embedding indexes",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=Bl8CQrx2Up4": {
    "title": "cosFormer: Rethinking Softmax In Attention",
    "volume": "poster",
    "abstract": "Transformer has shown great successes in natural language processing, computer vision, and audio processing. As one of its core components, the softmax attention helps to capture long-range dependencies yet prohibits its scale-up due to the quadratic space and time complexity to the sequence length. Kernel methods are often adopted to reduce the complexity by approximating the softmax operator. Nevertheless, due to the approximation errors, their performances vary in different tasks/corpus and suffer crucial performance drops when compared with the vanilla softmax attention. In this paper, we propose a linear transformer called cosFormer that can achieve comparable or better accuracy to the vanilla transformer in both casual and cross attentions. cosFormer is based on two key properties of softmax attention: i). non-negativeness of the attention matrix; ii). a non-linear re-weighting scheme that can concentrate the distribution of the attention matrix. As its linear substitute, cosFormer fulfills these properties with a linear operator and a cosine-based distance re-weighting mechanism. Extensive experiments on language modeling and text understanding tasks demonstrate the effectiveness of our method. We further examine our method on long sequences and achieve state-of-the-art performance on the Long-Range Arena benchmark. The source code is available at https://github.com/OpenNLPLab/cosFormer",
    "checked": true,
    "id": "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
    "semantic_title": "cosformer: rethinking softmax in attention",
    "citation_count": 248,
    "authors": []
  },
  "https://openreview.net/forum?id=htWIlvDcY8": {
    "title": "FALCON: Fast Visual Concept Learning by Integrating Images, Linguistic descriptions, and Conceptual Relations",
    "volume": "poster",
    "abstract": "We present a meta-learning framework for learning new visual concepts quickly, from just one or a few examples, guided by multiple naturally occurring data streams: simultaneously looking at images, reading sentences that describe the objects in the scene, and interpreting supplemental sentences that relate the novel concept with other concepts. The learned concepts support downstream applications, such as answering questions by reasoning about unseen images. Our model, namely FALCON, represents individual visual concepts, such as colors and shapes, as axis-aligned boxes in a high-dimensional space (the ``box embedding space''). Given an input image and its paired sentence, our model first resolves the referential expression in the sentence and associates the novel concept with particular objects in the scene. Next, our model interprets supplemental sentences to relate the novel concept with other known concepts, such as ``X has property Y'' or ``X is a kind of Y''. Finally, it infers an optimal box embedding for the novel concept that jointly 1) maximizes the likelihood of the observed instances in the image, and 2) satisfies the relationships between the novel concepts and the known ones. We demonstrate the effectiveness of our model on both synthetic and real-world datasets",
    "checked": true,
    "id": "ae9f2d6a29daec0eaf5700d327df0f4f30558b74",
    "semantic_title": "falcon: fast visual concept learning by integrating images, linguistic descriptions, and conceptual relations",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=64trBbOhdGU": {
    "title": "HyAR: Addressing Discrete-Continuous Action Reinforcement Learning via Hybrid Action Representation",
    "volume": "poster",
    "abstract": "Discrete-continuous hybrid action space is a natural setting in many practical problems, such as robot control and game AI. However, most previous Reinforcement Learning (RL) works only demonstrate the success in controlling with either discrete or continuous action space, while seldom take into account the hybrid action space. One naive way to address hybrid action RL is to convert the hybrid action space into a unified homogeneous action space by discretization or continualization, so that conventional RL algorithms can be applied. However, this ignores the underlying structure of hybrid action space and also induces the scalability issue and additional approximation difficulties, thus leading to degenerated results. In this paper, we propose Hybrid Action Representation (HyAR) to learn a compact and decodable latent representation space for the original hybrid action space. HyAR constructs the latent space and embeds the dependence between discrete action and continuous parameter via an embedding table and conditional Variantional Auto-Encoder (VAE). To further improve the effectiveness, the action representation is trained to be semantically smooth through unsupervised environmental dynamics prediction. Finally, the agent then learns its policy with conventional DRL algorithms in the learned representation space and interacts with the environment by decoding the hybrid action embeddings to the original action space. We evaluate HyAR in a variety of environments with discrete-continuous action space. The results demonstrate the superiority of HyAR when compared with previous baselines, especially for high-dimensional action spaces",
    "checked": true,
    "id": "c73157e860991103ea56a336ed241c74b5ac4a6f",
    "semantic_title": "hyar: addressing discrete-continuous action reinforcement learning via hybrid action representation",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=DesNW4-5ai9": {
    "title": "Transferable Adversarial Attack based on Integrated Gradients",
    "volume": "poster",
    "abstract": "The vulnerability of deep neural networks to adversarial examples has drawn tremendous attention from the community. Three approaches, optimizing standard objective functions, exploiting attention maps, and smoothing decision surfaces, are commonly used to craft adversarial examples. By tightly integrating the three approaches, we propose a new and simple algorithm named Transferable Attack based on Integrated Gradients (TAIG) in this paper, which can find highly transferable adversarial examples for black-box attacks. Unlike previous methods using multiple computational terms or combining with other methods, TAIG integrates the three approaches into one single term. Two versions of TAIG that compute their integrated gradients on a straight-line path and a random piecewise linear path are studied. Both versions offer strong transferability and can seamlessly work together with the previous methods. Experimental results demonstrate that TAIG outperforms the state-of-the-art methods",
    "checked": true,
    "id": "848db0c9842cc66379ec7975ca535adc8d90c92d",
    "semantic_title": "transferable adversarial attack based on integrated gradients",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=J7b4BCtDm4": {
    "title": "How to deal with missing data in supervised deep learning?",
    "volume": "poster",
    "abstract": "The issue of missing data in supervised learning has been largely overlooked, especially in the deep learning community. We investigate strategies to adapt neural architectures for handling missing values. Here, we focus on regression and classification problems where the features are assumed to be missing at random. Of particular interest are schemes that allow reusing as-is a neural discriminative architecture. To address supervised deep learning with missing values, we propose to marginalize over missing values in a joint model of covariates and outcomes. Thereby, we leverage both the flexibility of deep generative models to describe the distribution of the covariates and the power of purely discriminative models to make predictions. More precisely, a deep latent variable model can be learned jointly with the discriminative model, using importance-weighted variational inference, essentially using importance sampling to mimick averaging over multiple imputations. In low-capacity regimes, or when the discriminative model has a strong inductive bias, we find that our hybrid generative/discriminative approach generally outperforms single imputations methods",
    "checked": true,
    "id": "acae90bc6bcde932e32a5fedda90b1f5b750e436",
    "semantic_title": "how to deal with missing data in supervised deep learning?",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=oxxUMeFwEHd": {
    "title": "Topological Graph Neural Networks",
    "volume": "poster",
    "abstract": "Graph neural networks (GNNs) are a powerful architecture for tackling graph learning tasks, yet have been shown to be oblivious to eminent substructures such as cycles. We present TOGL, a novel layer that incorporates global topological information of a graph using persistent homology. TOGL can be easily integrated into any type of GNN and is strictly more expressive (in terms the Weisfeiler‚ÄìLehman graph isomorphism test) than message-passing GNNs. Augmenting GNNs with TOGL leads to improved predictive performance for graph and node classification tasks, both on synthetic data sets, which can be classified by humans using their topology but not by ordinary GNNs, and on real-world data",
    "checked": true,
    "id": "2db550467862c3bea38901cef84d86efed31b8d2",
    "semantic_title": "topological graph neural networks",
    "citation_count": 106,
    "authors": []
  },
  "https://openreview.net/forum?id=6Pe99Juo9gd": {
    "title": "Learning Value Functions from Undirected State-only Experience",
    "volume": "poster",
    "abstract": "This paper tackles the problem of learning value functions from undirected state-only experience (state transitions without action labels i.e. (s,s',r) tuples). We first theoretically characterize the applicability of Q-learning in this setting. We show that tabular Q-learning in discrete Markov decision processes (MDPs) learns the same value function under any arbitrary refinement of the action space. This theoretical result motivates the design of Latent Action Q-learning or LAQ, an offline RL method that can learn effective value functions from state-only experience. Latent Action Q-learning (LAQ) learns value functions using Q-learning on discrete latent actions obtained through a latent-variable future prediction model. We show that LAQ can recover value functions that have high correlation with value functions learned using ground truth actions. Value functions learned using LAQ lead to sample efficient acquisition of goal-directed behavior, can be used with domain-specific low-level controllers, and facilitate transfer across embodiments. Our experiments in 5 environments ranging from 2D grid world to 3D visual navigation in realistic environments demonstrate the benefits of LAQ over simpler alternatives, imitation learning oracles, and competing methods",
    "checked": true,
    "id": "c3053ff94d26b1416ac8828b8cce0f1cdd47df99",
    "semantic_title": "learning value functions from undirected state-only experience",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=_l_QjPGN5ye": {
    "title": "The Boltzmann Policy Distribution: Accounting for Systematic Suboptimality in Human Models",
    "volume": "poster",
    "abstract": "Models of human behavior for prediction and collaboration tend to fall into two categories: ones that learn from large amounts of data via imitation learning, and ones that assume human behavior to be noisily-optimal for some reward function. The former are very useful, but only when it is possible to gather a lot of human data in the target environment and distribution. The advantage of the latter type, which includes Boltzmann rationality, is the ability to make accurate predictions in new environments without extensive data when humans are actually close to optimal. However, these models fail when humans exhibit systematic suboptimality, i.e. when their deviations from optimal behavior are not independent, but instead consistent over time. Our key insight is that systematic suboptimality can be modeled by predicting policies, which couple action choices over time, instead of trajectories. We introduce the Boltzmann policy distribution (BPD), which serves as a prior over human policies and adapts via Bayesian inference to capture systematic deviations by observing human actions during a single episode. The BPD is difficult to compute and represent because policies lie in a high-dimensional continuous space, but we leverage tools from generative and sequence modeling to enable efficient sampling and inference. We show that the BPD enables prediction of human behavior and human-AI collaboration equally as well as imitation learning-based human models while using far less data",
    "checked": true,
    "id": "1405b02da0fbd87b181b8ba7fcef558327596a23",
    "semantic_title": "the boltzmann policy distribution: accounting for systematic suboptimality in human models",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=ahi2XSHpAUZ": {
    "title": "WeakM3D: Towards Weakly Supervised Monocular 3D Object Detection",
    "volume": "poster",
    "abstract": "Monocular 3D object detection is one of the most challenging tasks in 3D scene understanding. Due to the ill-posed nature of monocular imagery, existing monocular 3D detection methods highly rely on training with the manually annotated 3D box labels on the LiDAR point clouds. This annotation process is very laborious and expensive. To dispense with the reliance on 3D box labels, in this paper we explore the weakly supervised monocular 3D detection. Specifically, we first detect 2D boxes on the image. Then, we adopt the generated 2D boxes to select corresponding RoI LiDAR points as the weak supervision. Eventually, we adopt a network to predict 3D boxes which can tightly align with associated RoI LiDAR points. This network is learned by minimizing our newly-proposed 3D alignment loss between the 3D box estimates and the corresponding RoI LiDAR points. We will illustrate the potential challenges of the above learning problem and resolve these challenges by introducing several effective designs into our method. Codes are available at https://github.com/SPengLiang/WeakM3D",
    "checked": true,
    "id": "cfec17d737cd9b6573c54a21cfec025fe7be3bb2",
    "semantic_title": "weakm3d: towards weakly supervised monocular 3d object detection",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=7gE9V9GBZaI": {
    "title": "Exploring Memorization in Adversarial Training",
    "volume": "poster",
    "abstract": "Deep learning models have a propensity for fitting the entire training set even with random labels, which requires memorization of every training sample. In this paper, we explore the memorization effect in adversarial training (AT) for promoting a deeper understanding of model capacity, convergence, generalization, and especially robust overfitting of the adversarially trained models. We first demonstrate that deep networks have sufficient capacity to memorize adversarial examples of training data with completely random labels, but not all AT algorithms can converge under the extreme circumstance. Our study of AT with random labels motivates further analyses on the convergence and generalization of AT. We find that some AT approaches suffer from a gradient instability issue and the recently suggested complexity measures cannot explain robust generalization by considering models trained on random labels. Furthermore, we identify a significant drawback of memorization in AT that it could result in robust overfitting. We then propose a new mitigation algorithm motivated by detailed memorization analyses. Extensive experiments on various datasets validate the effectiveness of the proposed method",
    "checked": true,
    "id": "abbe3a82bb11a9f28eba39ff6dc17982a724c2fd",
    "semantic_title": "exploring memorization in adversarial training",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=pETy-HVvGtt": {
    "title": "Disentanglement Analysis with Partial Information Decomposition",
    "volume": "poster",
    "abstract": "We propose a framework to analyze how multivariate representations disentangle ground-truth generative factors. A quantitative analysis of disentanglement has been based on metrics designed to compare how one variable explains each generative factor. Current metrics, however, may fail to detect entanglement that involves more than two variables, e.g., representations that duplicate and rotate generative factors in high dimensional spaces. In this work, we establish a framework to analyze information sharing in a multivariate representation with Partial Information Decomposition and propose a new disentanglement metric. This framework enables us to understand disentanglement in terms of uniqueness, redundancy, and synergy. We develop an experimental protocol to assess how increasingly entangled representations are evaluated with each metric and confirm that the proposed metric correctly responds to entanglement. Through experiments on variational autoencoders, we find that models with similar disentanglement scores have a variety of characteristics in entanglement, for each of which a distinct strategy may be required to obtain a disentangled representation",
    "checked": true,
    "id": "9d31879efaea84a237bd1abec678d3315818a2ef",
    "semantic_title": "disentanglement analysis with partial information decomposition",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=U8pbd00cCWB": {
    "title": "Differentiable Gradient Sampling for Learning Implicit 3D Scene Reconstructions from a Single Image",
    "volume": "poster",
    "abstract": "Implicit shape models are promising 3D representations for modeling arbitrary locations, with Signed Distance Functions (SDFs) particularly suitable for clear mesh surface reconstruction. Existing approaches for single object reconstruction impose supervision signals based on the loss of the signed distance value from all locations in a scene, posing difficulties when extending to real-world scenarios. The spatial gradient of the signed distance field, rather than the SDF value itself, has not been typically employed as a source of supervision for single-view reconstruction, in part due to the difficulties of differentiable sampling a spatial gradient from the feature map. In this study, we derive a novel closed-form gradient sampling solution for Differentialble Gradient Sampling (DGS) that enables backpropagation of the loss of the spatial gradient back to the feature map pixels, thus allowing the imposition of the loss efficiently on the spatial gradient. As a result, we achieve high-quality single view indoor scene reconstruction results learning directly from a real-world scanned dataset (e.g. ScannetV2). Our model also performs well when generalizing to unseen images downloaded directly from the internet (Fig. 1). We comfortably advanced the state-of-the-art results with several established datasets including ShapeNet and ScannetV2; extensive quantitative analysis confirmed that our proposed DGS module plays an essential role in achieving this performance improvement. Full codes are available in MaskedURL",
    "checked": true,
    "id": "2bd4758e990dce87a37c6a7179c1fd5f4798f8aa",
    "semantic_title": "differentiable gradient sampling for learning implicit 3d scene reconstructions from a single image",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=3ILxkQ7yElm": {
    "title": "Learning Continuous Environment Fields via Implicit Functions",
    "volume": "poster",
    "abstract": "We propose a novel scene representation that encodes reaching distance -- the distance between any position in the scene to a goal along a feasible trajectory. We demonstrate that this environment field representation can directly guide the dynamic behaviors of agents in 2D mazes or 3D indoor scenes. Our environment field is a continuous representation and learned via a neural implicit function using discretely sampled training data. We showcase its application for agent navigation in 2D mazes, and human trajectory prediction in 3D indoor environments. To produce physically plausible and natural trajectories for humans, we additionally learn a generative model that predicts regions where humans commonly appear, and enforce the environment field to be defined within such regions. Extensive experiments demonstrate that the proposed method can generate both feasible and plausible trajectories efficiently and accurately",
    "checked": true,
    "id": "2cba4246d75a950fecf4c3ee5ea812a2f6e608ff",
    "semantic_title": "learning continuous environment fields via implicit functions",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=F5Em8ASCosV": {
    "title": "Causal Contextual Bandits with Targeted Interventions",
    "volume": "poster",
    "abstract": "We study a contextual bandit setting where the learning agent has the ability to perform interventions on targeted subsets of the population, apart from possessing qualitative causal side-information. This novel formalism captures intricacies in real-world scenarios such as software product experimentation where targeted experiments can be conducted. However, this fundamentally changes the set of options that the agent has, compared to standard contextual bandit settings, necessitating new techniques. This is also the first work that integrates causal side-information in a contextual bandit setting, where the agent aims to learn a policy that maps contexts to arms (as opposed to just identifying one best arm). We propose a new algorithm, which we show empirically performs better than baselines on experiments that use purely synthetic data and on real world-inspired experiments. We also prove a bound on regret that theoretically guards performance",
    "checked": true,
    "id": "313e6a4b0ae9c8fb701a56451f6f1f5952138357",
    "semantic_title": "causal contextual bandits with targeted interventions",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=xS8AMYiEav3": {
    "title": "Sound and Complete Neural Network Repair with Minimality and Locality Guarantees",
    "volume": "poster",
    "abstract": "We present a novel methodology for repairing neural networks that use ReLU activation functions. Unlike existing methods that rely on modifying the weights of a neural network which can induce a global change in the function space, our approach applies only a localized change in the function space while still guaranteeing the removal of the buggy behavior. By leveraging the piecewise linear nature of ReLU networks, our approach can efficiently construct a patch network tailored to the linear region where the buggy input resides, which when combined with the original network, provably corrects the behavior on the buggy input. Our method is both sound and complete -- the repaired network is guaranteed to fix the buggy input, and a patch is guaranteed to be found for any buggy input. Moreover, our approach preserves the continuous piecewise linear nature of ReLU networks, automatically generalizes the repair to all the points including other undetected buggy inputs inside the repair region, is minimal in terms of changes in the function space, and guarantees that outputs on inputs away from the repair region are unaltered. On several benchmarks, we show that our approach significantly outperforms existing methods in terms of locality and limiting negative side effects",
    "checked": true,
    "id": "17b68e384b607208747606033b1dbe3f6a9262fd",
    "semantic_title": "sound and complete neural network repair with minimality and locality guarantees",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=JJxiD-kg-oK": {
    "title": "Blaschke Product Neural Networks (BPNN): A Physics-Infused Neural Network for Phase Retrieval of Meromorphic Functions",
    "volume": "poster",
    "abstract": "Numerous physical systems are described by ordinary or partial differential equations whose solutions are given by holomorphic or meromorphic functions in the complex domain. In many cases, only the magnitude of these functions are observed on various points on the purely imaginary $j\\omega$-axis since coherent measurement of their phases is often expensive. However, it is desirable to retrieve the lost phases from the magnitudes when possible. To this end, we propose a physics-infused deep neural network based on the Blaschke products for phase retrieval. Inspired by the Helson and Sarason Theorem, we recover coefficients of a rational function of Blaschke products using a Blaschke Product Neural Network (BPNN), based upon the magnitude observations as input. The resulting rational function is then used for phase retrieval. We compare the BPNN to conventional deep neural networks (NNs) on several phase retrieval problems, comprising both synthetic and contemporary real-world problems (e.g., metamaterials for which data collection requires substantial expertise and is time consuming). On each phase retrieval problem, we compare against a population of conventional NNs of varying size and hyperparameter settings. Even without any hyper-parameter search, we find that BPNNs consistently outperform the population of optimized NNs in scarce data scenarios, and do so despite being much smaller models. The results can in turn be applied to calculate the refractive index of metamaterials, which is an important problem in emerging areas of material science",
    "checked": true,
    "id": "386b0941e07369cd5ffe5ea3b3a822e755acb130",
    "semantic_title": "blaschke product neural networks (bpnn): a physics-infused neural network for phase retrieval of meromorphic functions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rFbR4Fv-D6-": {
    "title": "Automated Self-Supervised Learning for Graphs",
    "volume": "poster",
    "abstract": "Graph self-supervised learning has gained increasing attention due to its capacity to learn expressive node representations. Many pretext tasks, or loss functions have been designed from distinct perspectives. However, we observe that different pretext tasks affect downstream tasks differently cross datasets, which suggests that searching pretext tasks is crucial for graph self-supervised learning. Different from existing works focusing on designing single pretext tasks, this work aims to investigate how to automatically leverage multiple pretext tasks effectively. Nevertheless, evaluating representations derived from multiple pretext tasks without direct access to ground truth labels makes this problem challenging. To address this obstacle, we make use of a key principle of many real-world graphs, i.e., homophily, or the principle that ``like attracts like,'' as the guidance to effectively search various self-supervised pretext tasks. We provide theoretical understanding and empirical evidence to justify the flexibility of homophily in this search task. Then we propose the AutoSSL framework which can automatically search over combinations of various self-supervised tasks. By evaluating the framework on 7 real-world datasets, our experimental results show that AutoSSL can significantly boost the performance on downstream tasks including node clustering and node classification compared with training under individual tasks",
    "checked": true,
    "id": "940f3234bb388adcb7e77ff887c0e5f444e70e79",
    "semantic_title": "automated self-supervised learning for graphs",
    "citation_count": 81,
    "authors": []
  },
  "https://openreview.net/forum?id=m8uJvVgwRci": {
    "title": "Creating Training Sets via Weak Indirect Supervision",
    "volume": "poster",
    "abstract": "Creating labeled training sets has become one of the major roadblocks in machine learning. To address this, recent Weak Supervision (WS) frameworks synthesize training labels from multiple potentially noisy supervision sources. However, existing frameworks are restricted to supervision sources that share the same output space as the target task. To extend the scope of usable sources, we formulate Weak Indirect Supervision (WIS), a new research problem for automatically synthesizing training labels based on indirect supervision sources that have different output label spaces. To overcome the challenge of mismatched output spaces, we develop a probabilistic modeling approach, PLRM, which uses user-provided label relations to model and leverage indirect supervision sources. Moreover, we provide a theoretically-principled test of the distinguishability of PLRM for unseen labels, along with an generalization bound. On both image and text classification tasks as well as an industrial advertising application, we demonstrate the advantages of PLRM by outperforming baselines by a margin of 2%-9%",
    "checked": true,
    "id": "fd8e176087335355ff5e81821a616d15ec8d3346",
    "semantic_title": "creating training sets via weak indirect supervision",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=aTzMi4yV_RO": {
    "title": "Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs",
    "volume": "poster",
    "abstract": "The discovery of the disentanglement properties of the latent space in GANs motivated a lot of research to find the semantically meaningful directions on it. In this paper, we suggest that the disentanglement property is closely related to the geometry of the latent space. In this regard, we propose an unsupervised method for finding the semantic-factorizing directions on the intermediate latent space of GANs based on the local geometry. Intuitively, our proposed method, called $\\textit{Local Basis}$, finds the principal variation of the latent space in the neighborhood of the base latent variable. Experimental results show that the local principal variation corresponds to the semantic factorization and traversing along it provides strong robustness to image traversal. Moreover, we suggest an explanation for the limited success in finding the global traversal directions in the latent space, especially $\\mathcal{W}$-space of StyleGAN2. We show that $\\mathcal{W}$-space is warped globally by comparing the local geometry, discovered from Local Basis, through the metric on Grassmannian Manifold. The global warpage implies that the latent space is not well-aligned globally and therefore the global traversal directions are bound to show limited success on it",
    "checked": true,
    "id": "1e2770f58ba8b307d3048a4471cc42c489614484",
    "semantic_title": "do not escape from the manifold: discovering the local coordinates on the latent space of gans",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=HObMhrCeAAF": {
    "title": "GradSign: Model Performance Inference with Theoretical Insights",
    "volume": "poster",
    "abstract": "A key challenge in neural architecture search (NAS) is quickly inferring the predictive performance of a broad spectrum of networks to discover statistically accurate and computationally efficient ones. We refer to this task as model performance inference (MPI). The current practice for efficient MPI is gradient-based methods that leverage the gradients of a network at initialization to infer its performance. However, existing gradient-based methods rely only on heuristic metrics and lack the necessary theoretical foundations to consolidate their designs. We propose GradSign, an accurate, simple, and flexible metric for model performance inference with theoretical insights. The key idea behind GradSign is a quantity Œ® to analyze the sample-wise optimization landscape of different networks. Theoretically, we show that Œ® is an upper bound for both the training and true population losses of a neural network under reasonable assumptions. However, it is computationally prohibitive to directly calculate Œ® for modern neural networks. To address this challenge, we design GradSign, an accurate and simple approximation of Œ® using the gradients of a network evaluated at a random initialization state. Evaluation on seven NAS benchmarks across three training datasets shows that GradSign generalizes well to real-world networks and consistently outperforms state-of-the-art gradient-based methods for MPI evaluated by Spearman's œÅ and Kendall's Tau. Additionally, we integrate GradSign into four existing NAS algorithms and show that the GradSign-assisted NAS algorithms outperform their vanilla counterparts by improving the accuracies of best-discovered networks by up to 0.3%, 1.1%, and 1.0% on three real-world tasks. Code is available at https://github.com/JackFram/GradSign",
    "checked": true,
    "id": "4e9c42d09b692c3100242cf442dcb19b33a1cd75",
    "semantic_title": "gradsign: model performance inference with theoretical insights",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=hpBTIv2uy_E": {
    "title": "You are AllSet: A Multiset Function Framework for Hypergraph Neural Networks",
    "volume": "poster",
    "abstract": "Hypergraphs are used to model higher-order interactions amongst agents and there exist many practically relevant instances of hypergraph datasets. To enable the efficient processing of hypergraph data, several hypergraph neural network platforms have been proposed for learning hypergraph properties and structure, with a special focus on node classification tasks. However, almost all existing methods use heuristic propagation rules and offer suboptimal performance on benchmarking datasets. We propose AllSet, a new hypergraph neural network paradigm that represents a highly general framework for (hyper)graph neural networks and for the first time implements hypergraph neural network layers as compositions of two multiset functions that can be efficiently learned for each task and each dataset. The proposed AllSet framework also for the first time integrates Deep Sets and Set Transformers with hypergraph neural networks for the purpose of learning multiset functions and therefore allows for significant modeling flexibility and high expressive power. To evaluate the performance of AllSet, we conduct the most extensive experiments to date involving ten known benchmarking datasets and three newly curated datasets that represent significant challenges for hypergraph node classification. The results demonstrate that our method has the unique ability to either match or outperform all other hypergraph neural networks across the tested datasets: As an example, the performance improvements over existing methods and a new method based on heterogeneous graph neural networks are close to $4\\%$ on the Yelp and Zoo datasets, and $3\\%$ on the Walmart dataset",
    "checked": true,
    "id": "d87585d2e9164a074d68e4e66eb8070cfb1138eb",
    "semantic_title": "you are allset: a multiset function framework for hypergraph neural networks",
    "citation_count": 150,
    "authors": []
  },
  "https://openreview.net/forum?id=KmtVD97J43e": {
    "title": "Synchromesh: Reliable Code Generation from Pre-trained Language Models",
    "volume": "poster",
    "abstract": "Large pre-trained language models have been used to generate code, providing a flexible interface for synthesizing programs from natural language specifications. However, they often violate syntactic and semantic rules of their output language, limiting their practical usability. In this paper, we propose Synchromesh: a framework for substantially improving the reliability of pre-trained models for code generation. Synchromesh comprises two components. First, it retrieves few-shot examples from a training bank using Target Similarity Tuning (TST), a novel method for semantic example selection. TST learns to recognize utterances that describe similar target programs despite of differences in surface natural language features. Then, Synchromesh feeds the examples to a pre-trained language model and samples programs using Constrained Semantic Decoding (CSD): a general framework for constraining the output to a set of valid programs in the target language. CSD leverages constraints on partial outputs to sample complete correct programs, and needs neither re-training nor fine-tuning of the language model. We evaluate our methods by synthesizing code from natural language descriptions using GPT-3 and Codex in three real-world languages: SQL queries, Vega-Lite visualizations and SMCalFlow programs. These domains showcase rich constraints that CSD is able to enforce, including syntax, scoping and typing rules. Across all languages, we observe complementary gains from CSD and TST in prediction accuracy and in effectively preventing parsing, type and run-time errors",
    "checked": true,
    "id": "b62d63580b81a2cbb20c3c1593dd62d118e4cb07",
    "semantic_title": "synchromesh: reliable code generation from pre-trained language models",
    "citation_count": 173,
    "authors": []
  },
  "https://openreview.net/forum?id=tFgdrQbbaa": {
    "title": "Learning curves for continual learning in neural networks: Self-knowledge transfer and forgetting",
    "volume": "poster",
    "abstract": "Sequential training from task to task is becoming one of the major objects in deep learning applications such as continual learning and transfer learning. Nevertheless, it remains unclear under what conditions the trained model's performance improves or deteriorates. To deepen our understanding of sequential training, this study provides a theoretical analysis of generalization performance in a solvable case of continual learning. We consider neural networks in the neural tangent kernel (NTK) regime that continually learn target functions from task to task, and investigate the generalization by using an established statistical mechanical analysis of kernel ridge-less regression. We first show characteristic transitions from positive to negative transfer. More similar targets above a specific critical value can achieve positive knowledge transfer for the subsequent task while catastrophic forgetting occurs even with very similar targets. Next, we investigate a variant of continual learning which supposes the same target function in multiple tasks. Even for the same target, the trained model shows some transfer and forgetting depending on the sample size of each task. We can guarantee that the generalization error monotonically decreases from task to task for equal sample sizes while unbalanced sample sizes deteriorate the generalization. We respectively refer to these improvement and deterioration as self-knowledge transfer and forgetting, and empirically confirm them in realistic training of deep neural networks as well",
    "checked": true,
    "id": "a46a42d3a713a8e3843203764654c6d0219422a5",
    "semantic_title": "learning curves for continual learning in neural networks: self-knowledge transfer and forgetting",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=xLfAgCroImw": {
    "title": "Energy-Based Learning for Cooperative Games, with Applications to Valuation Problems in Machine Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c6f18b19d73909e13a7970962b4309ee137c923d",
    "semantic_title": "energy-based learning for cooperative games, with applications to valuation problems in machine learning",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=tyrJsbKAe6": {
    "title": "Pessimistic Model-based Offline Reinforcement Learning under Partial Coverage",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4ff8454c524163bbc5d25f6c8984b1c31ad057e4",
    "semantic_title": "pessimistic model-based offline reinforcement learning under partial coverage",
    "citation_count": 153,
    "authors": []
  },
  "https://openreview.net/forum?id=1ugNpm7W6E": {
    "title": "Cold Brew: Distilling Graph Node Representations with Incomplete or Missing Neighborhoods",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d4745f8e80d0cf75ae80c33ab5833b467704d46e",
    "semantic_title": "cold brew: distilling graph node representations with incomplete or missing neighborhoods",
    "citation_count": 70,
    "authors": []
  },
  "https://openreview.net/forum?id=v-v1cpNNK_v": {
    "title": "NASI: Label- and Data-agnostic Neural Architecture Search at Initialization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e3f732c15c0932e08cd51cab282d7cc7a1739c75",
    "semantic_title": "nasi: label- and data-agnostic neural architecture search at initialization",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=49h_IkpJtaE": {
    "title": "How to Train Your MAML to Excel in Few-Shot Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0999abf5e150a76120e54abfe89f48114095636c",
    "semantic_title": "how to train your maml to excel in few-shot classification",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=xy_2w3J3kH": {
    "title": "Communication-Efficient Actor-Critic Methods for Homogeneous Markov Games",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9ef78757ea89fef489e404d83049ae60f45e88f5",
    "semantic_title": "communication-efficient actor-critic methods for homogeneous markov games",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=vh-0sUt8HlG": {
    "title": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e",
    "semantic_title": "mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer",
    "citation_count": 1482,
    "authors": []
  },
  "https://openreview.net/forum?id=kavTY__jxp": {
    "title": "Spatial Graph Attention and Curiosity-driven Policy for Antiviral Drug Discovery",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5d7cb59281f7152a0a7c7d4ac80117fb2e77d38b",
    "semantic_title": "spatial graph attention and curiosity-driven policy for antiviral drug discovery",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=OnpFa95RVqs": {
    "title": "Surrogate NAS Benchmarks: Going Beyond the Limited Search Spaces of Tabular NAS Benchmarks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9ed4321e552d069ff6aa6f88480809e23927131d",
    "semantic_title": "surrogate nas benchmarks: going beyond the limited search spaces of tabular nas benchmarks",
    "citation_count": 88,
    "authors": []
  },
  "https://openreview.net/forum?id=y1PXylgrXZ": {
    "title": "Certified Robustness for Deep Equilibrium Models via Interval Bound Propagation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6e9577c4b4518b9976b9d421755ce37f8ea3ed7f",
    "semantic_title": "certified robustness for deep equilibrium models via interval bound propagation",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=03RLpj-tc_": {
    "title": "Crystal Diffusion Variational Autoencoder for Periodic Material Generation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f50f877b07d64f116de7bf161cf009d2ebad7d15",
    "semantic_title": "crystal diffusion variational autoencoder for periodic material generation",
    "citation_count": 286,
    "authors": []
  },
  "https://openreview.net/forum?id=u2GZOiUTbt": {
    "title": "Task Affinity with Maximum Bipartite Matching in Few-Shot Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "87397e5d3eaede08709f5d1d8b83da9823e6c1f8",
    "semantic_title": "task affinity with maximum bipartite matching in few-shot learning",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=7r6kDq0mK_": {
    "title": "Latent Image Animator: Learning to Animate Images via Latent Space Navigation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a3672263121f8e6409d3afbcc367001997f1312c",
    "semantic_title": "latent image animator: learning to animate images via latent space navigation",
    "citation_count": 170,
    "authors": []
  },
  "https://openreview.net/forum?id=o0ehFykKVtr": {
    "title": "Know Thyself: Transferable Visual Control Policies Through Robot-Awareness",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fac89076a97674fff11043ced4974ef49eca6810",
    "semantic_title": "know thyself: transferable visuomotor control through robot-awareness",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=KJggliHbs8": {
    "title": "Node Feature Extraction by Self-Supervised Multi-scale Neighborhood Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "259cbc1492c51d985bdafb67e48fa170471ee446",
    "semantic_title": "node feature extraction by self-supervised multi-scale neighborhood prediction",
    "citation_count": 149,
    "authors": []
  },
  "https://openreview.net/forum?id=givsRXsOt9r": {
    "title": "Spherical Message Passing for 3D Molecular Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f8079b38314b9a78e2558d26126ed649e9c3d2dc",
    "semantic_title": "spherical message passing for 3d molecular graphs",
    "citation_count": 218,
    "authors": []
  },
  "https://openreview.net/forum?id=wbPObLm6ueA": {
    "title": "Fairness Guarantees under Demographic Shift",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d9a7c8a454bebabd9d443a343092dae4aa183fec",
    "semantic_title": "fairness guarantees under demographic shift",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=j3krplz_4w6": {
    "title": "Fooling Explanations in Text Classifiers",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "43ad62d1e64f2301eb2294ab79b3979a470e7ed4",
    "semantic_title": "fooling explanations in text classifiers",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=y0VvIg25yk": {
    "title": "On the Learning and Learnability of Quasimetrics",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0cd1955bb1949c232c4d15a3c4c94a77a95a3f7d",
    "semantic_title": "on the learning and learnability of quasimetrics",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=WH6u2SvlLp4": {
    "title": "Learning Prototype-oriented Set Representations for Meta-Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b2dcd0c533b59baf8edfd7c82b93ea87d99d05e6",
    "semantic_title": "learning prototype-oriented set representations for meta-learning",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=9pEJSVfDbba": {
    "title": "Embedded-model flows: Combining the inductive biases of model-free deep learning and explicit probabilistic modeling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "21a75213c0d3dad6c6d891960099196496e03a14",
    "semantic_title": "embedded-model flows: combining the inductive biases of model-free deep learning and explicit probabilistic modeling",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=YRq0ZUnzKoZ": {
    "title": "A Relational Intervention Approach for Unsupervised Dynamics Generalization in Model-Based Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "955536024c5db4166e63d41406c290fcf7ade696",
    "semantic_title": "a relational intervention approach for unsupervised dynamics generalization in model-based reinforcement learning",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=2f1z55GVQN": {
    "title": "Critical Points in Quantum Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "dac72943052100bfd0eb882439249d8ede4c6e6e",
    "semantic_title": "critical points in quantum generative models",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=TW7d65uYu5M": {
    "title": "VOS: Learning What You Don't Know by Virtual Outlier Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f0e6aed660c541544063c0608326c6f19ba48f6d",
    "semantic_title": "vos: learning what you don't know by virtual outlier synthesis",
    "citation_count": 350,
    "authors": []
  },
  "https://openreview.net/forum?id=EcGGFkNTxdJ": {
    "title": "Trust Region Policy Optimisation in Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "27302766f8d0eb6c052eb400e234c5be0e7a767e",
    "semantic_title": "trust region policy optimisation in multi-agent reinforcement learning",
    "citation_count": 283,
    "authors": []
  },
  "https://openreview.net/forum?id=neqU3HWDgE": {
    "title": "Unsupervised Disentanglement with Tensor Product Representations on the Torus",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5e7cd7421d9f43c3171e792e8884feb5e02c7b5f",
    "semantic_title": "unsupervised disentanglement with tensor product representations on the torus",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=_hszZbt46bT": {
    "title": "Anomaly Detection for Tabular Data with Internal Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ab4d553afd613373f56520da52c83239be32422e",
    "semantic_title": "anomaly detection for tabular data with internal contrastive learning",
    "citation_count": 102,
    "authors": []
  },
  "https://openreview.net/forum?id=CpTuR2ECuW": {
    "title": "LIGS: Learnable Intrinsic-Reward Generation Selection for Multi-Agent Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f23a418ddb2f40fb6bd2abd79ffd8c198cb63a79",
    "semantic_title": "ligs: learnable intrinsic-reward generation selection for multi-agent learning",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=EVVadRFRgL7": {
    "title": "Bayesian Modeling and Uncertainty Quantification for Learning to Optimize: What, Why, and How",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f36b907ec78cc7f1ec8fecaae902a737b4bb6c0f",
    "semantic_title": "bayesian modeling and uncertainty quantification for learning to optimize: what, why, and how",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=RLtqs6pzj1-": {
    "title": "Deep Ensembling with No Overhead for either Training or Testing: The All-Round Blessings of Dynamic Sparsity",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "439d158e3ab3910d836535dd1aec693f5c0420cf",
    "semantic_title": "deep ensembling with no overhead for either training or testing: the all-round blessings of dynamic sparsity",
    "citation_count": 55,
    "authors": []
  },
  "https://openreview.net/forum?id=X0nrKAXu7g-": {
    "title": "HyperDQN: A Randomized Exploration Method for Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4c4d41a977e6d8d82b6cc19ac28e50de3c814fd8",
    "semantic_title": "hyperdqn: a randomized exploration method for deep reinforcement learning",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=3rULBvOJ8D2": {
    "title": "Unraveling Model-Agnostic Meta-Learning via The Adaptation Learning Rate",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fa50e56cf95e7c660fcce05dcc740b3266f86558",
    "semantic_title": "unraveling model-agnostic meta-learning via the adaptation learning rate",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=MsHnJPaBUZE": {
    "title": "iFlood: A Stable and Effective Regularizer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bc14b1ad9a707e605a38026ac0f5f2c61c486ee1",
    "semantic_title": "iflood: a stable and effective regularizer",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=3jooF27-0Wy": {
    "title": "FlexConv: Continuous Kernel Convolutions With Differentiable Kernel Sizes",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e62000bf455daf9d02626d9708c497290e1826fa",
    "semantic_title": "flexconv: continuous kernel convolutions with differentiable kernel sizes",
    "citation_count": 86,
    "authors": []
  },
  "https://openreview.net/forum?id=nxcABL7jbQh": {
    "title": "Zero Pixel Directional Boundary by Vector Transform",
    "volume": "poster",
    "abstract": "Boundaries or contours are among the primary visual cues used by human and computer vision systems. One of the key problems in boundary detection is the loss formulation, which typically leads to class imbalance and, as a consequence, to thick boundaries which require non-differential post-processing steps to be thinned. In this paper, we re-interpret boundaries as 1-D surfaces and formulate a one-to-one vector transform function that allows for training of boundary prediction completely avoiding the class imbalance issue. Specifically, we define the boundary representation at any point as the unit vector pointing to the closest boundary surface. Our problem formulation leads to the estimation of direction as well as richer contextual information of the boundary, and, if desired, the availability of zero-pixel thin boundaries also at training time. Our method uses no hyper-parameter in the training loss and a fixed stable hyper-parameter at inference. We provide theoretical justification/discussions of the vector transform representation. We evaluate the proposed loss method using a standard architecture and show the excellent performance over other losses and representations on several datasets",
    "checked": true,
    "id": "40b6dc974aa5b4b1a02e2fbb1b6dc681e0890ae6",
    "semantic_title": "zero pixel directional boundary by vector transform",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=wqD6TfbYkrn": {
    "title": "A Conditional Point Diffusion-Refinement Paradigm for 3D Point Cloud Completion",
    "volume": "poster",
    "abstract": "3D point clouds are an important data format that captures 3D information for real world objects. Since 3D point clouds scanned in the real world are often incomplete, it is important to recover the complete point cloud for many downstreaming applications. Most existing point cloud completion methods use the Chamfer Distance (CD) loss for training. The CD loss estimates correspondences between two point clouds by searching nearest neighbors, which does not capture the overall point distribution on the generated shape, and therefore likely leads to non-uniform point cloud generation. To tackle this problem, we propose a novel Point Diffusion-Refinement (PDR) paradigm for point cloud completion. PDR consists of a Conditional Generation Network (CGNet) and a ReFinement Network (RFNet). The CGNet uses a conditional generative model called the denoising diffusion probabilistic model (DDPM) to generate a coarse completion conditioned on the partial observation. DDPM establishes a one-to-one pointwise mapping between the generated point cloud and the uniform ground truth, and then optimizes the mean squared error loss to realize uniform generation. The RFNet refines the coarse output of the CGNet and further improves quality of the completed point cloud. In terms of the architecture, we develop a novel dual-path architecture for both networks. The architecture can (1) effectively and efficiently extract multi-level features from partially observed point clouds to guide completion, and (2) accurately manipulate spatial locations of 3D points to obtain smooth surfaces and sharp details. Extensive experimental results on various benchmark datasets show that our PDR paradigm outperforms previous state-of-the-art methods for point cloud completion. In addition, with the help of the RFNet, we can accelerate the iterative generation process of the DDPM by up to 50 times without much performance drop",
    "checked": true,
    "id": "c940509c5b1ee8db9e4ce70254726719b8d56c54",
    "semantic_title": "a conditional point diffusion-refinement paradigm for 3d point cloud completion",
    "citation_count": 133,
    "authors": []
  },
  "https://openreview.net/forum?id=SIKV0_MrZlr": {
    "title": "Auto-Transfer: Learning to Route Transferable Representations",
    "volume": "poster",
    "abstract": "Knowledge transfer between heterogeneous source and target networks and tasks has received a lot of attention in recent times as large amounts of quality labeled data can be difficult to obtain in many applications. Existing approaches typically constrain the target deep neural network (DNN) feature representations to be close to the source DNNs feature representations, which can be limiting. We, in this paper, propose a novel adversarial multi-armed bandit approach that automatically learns to route source representations to appropriate target representations following which they are combined in meaningful ways to produce accurate target models. We see upwards of 5\\% accuracy improvements compared with the state-of-the-art knowledge transfer methods on four benchmark (target) image datasets CUB200, Stanford Dogs, MIT67, and Stanford40 where the source dataset is ImageNet. We qualitatively analyze the goodness of our transfer scheme by showing individual examples of the important features focused on by our target network at different layers compared with the (closest) competitors. We also observe that our improvement over other methods is higher for smaller target datasets making it an effective tool for small data applications that may benefit from transfer learning",
    "checked": true,
    "id": "15e437c7be0f9eca8a74c92bafeeb9de892d02b4",
    "semantic_title": "auto-transfer: learning to route transferrable representations",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=9jInD9JjicF": {
    "title": "PoNet: Pooling Network for Efficient Token Mixing in Long Sequences",
    "volume": "poster",
    "abstract": "Transformer-based models have achieved great success in various NLP, vision, and speech tasks. However, the core of Transformer, the self-attention mechanism, has a quadratic time and memory complexity with respect to the sequence length, which hinders applications of Transformer-based models to long sequences. Many approaches have been proposed to mitigate this problem, such as sparse attention mechanisms, low-rank matrix approximations and scalable kernels, and token mixing alternatives to self-attention. We propose a novel Pooling Network (PoNet) for token mixing in long sequences with linear complexity. We design multi-granularity pooling and pooling fusion to capture different levels of contextual information and combine their interactions with tokens. On the Long Range Arena benchmark, PoNet significantly outperforms Transformer and achieves competitive accuracy, while being only slightly slower than the fastest model, FNet, across all sequence lengths measured on GPUs. We also conduct systematic studies on the transfer learning capability of PoNet and observe that PoNet achieves 95.7 percent of the accuracy of BERT on the GLUE benchmark, outperforming FNet by 4.5 percent relative. Comprehensive ablation analysis demonstrates effectiveness of the designed multi-granularity pooling and pooling fusion for token mixing in long sequences and efficacy of the designed pre-training tasks for PoNet to learn transferable contextualized language representations",
    "checked": true,
    "id": "c88e2d70e44493d5508bfe517be978a9040be6a5",
    "semantic_title": "ponet: pooling network for efficient token mixing in long sequences",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=9kpuB2bgnim": {
    "title": "Huber Additive Models for Non-stationary Time Series Analysis",
    "volume": "poster",
    "abstract": "Sparse additive models have shown promising Ô¨Çexibility and interpretability in processing time series data. However, existing methods usually assume the time series data to be stationary and the innovation is sampled from a Gaussian distribution. Both assumptions are too stringent for heavy-tailed and non-stationary time series data that frequently arise in practice, such as Ô¨Ånance and medical Ô¨Åelds. To address these problems, we propose an adaptive sparse Huber additive model for robust forecasting in both non-Gaussian data and (non)stationary data. In theory, the generalization bounds of our estimator are established for both stationary and nonstationary time series data, which are independent of the widely used mixing conditions in learning theory of dependent observations. Moreover, the error bound for non-stationary time series contains a discrepancy measure for the shifts of the data distributions over time. Such a discrepancy measure can be estimated empirically and used as a penalty in our method. Experimental results on both synthetic and real-world benchmark datasets validate the effectiveness of the proposed method. The code is available at https://github.com/xianruizhong/SpHAM",
    "checked": true,
    "id": "3259ccb8a56723bbfa5bc30f40dd4125082c7023",
    "semantic_title": "huber additive models for non-stationary time series analysis",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=WuEiafqdy9H": {
    "title": "Model-augmented Prioritized Experience Replay",
    "volume": "poster",
    "abstract": "Experience replay is an essential component in off-policy model-free reinforcement learning (MfRL). Due to its effectiveness, various methods for calculating priority scores on experiences have been proposed for sampling. Since critic networks are crucial to policy learning, TD-error, directly correlated to $Q$-values, is one of the most frequently used features to compute the scores. However, critic networks often under- or overestimate $Q$-values, so it is often ineffective to learn to predict $Q$-values by sampled experiences based heavily on TD-error. Accordingly, it is valuable to find auxiliary features, which positively support TD-error in calculating the scores for efficient sampling. Motivated by this, we propose a novel experience replay method, which we call model-augmented prioritized experience replay (MaPER), that employs new learnable features driven from components in model-based RL (MbRL) to calculate the scores on experiences. The proposed MaPER brings the effect of curriculum learning for predicting $Q$-values better by the critic network with negligible memory and computational overhead compared to the vanilla PER. Indeed, our experimental results on various tasks demonstrate that MaPER can significantly improve the performance of the state-of-the-art off-policy MfRL and MbRL which includes off-policy MfRL algorithms in its policy optimization procedure",
    "checked": true,
    "id": "fa7b186382cfc93454830b5d947a4ac1a9453cff",
    "semantic_title": "model-augmented prioritized experience replay",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=MSgB8D4Hy51": {
    "title": "Post-Training Detection of Backdoor Attacks for Two-Class and Multi-Attack Scenarios",
    "volume": "poster",
    "abstract": "Backdoor attacks (BAs) are an emerging threat to deep neural network classifiers. A victim classifier will predict to an attacker-desired target class whenever a test sample is embedded with the same backdoor pattern (BP) that was used to poison the classifier's training set. Detecting whether a classifier is backdoor attacked is not easy in practice, especially when the defender is, e.g., a downstream user without access to the classifier's training set. This challenge is addressed here by a reverse-engineering defense (RED), which has been shown to yield state-of-the-art performance in several domains. However, existing REDs are not applicable when there are only two classes or when multiple attacks are present. These scenarios are first studied in the current paper, under the practical constraints that the defender neither has access to the classifier's training set nor to supervision from clean reference classifiers trained for the same domain. We propose a detection framework based on BP reverse-engineering and a novel expected transferability (ET) statistic. We show that our ET statistic is effective using the same detection threshold, irrespective of the classification domain, the attack configuration, and the BP reverse-engineering algorithm that is used. The excellent performance of our method is demonstrated on six benchmark datasets. Notably, our detection framework is also applicable to multi-class scenarios with multiple attacks. Code is available at https://github.com/zhenxianglance/2ClassBADetection",
    "checked": true,
    "id": "c85f7d1e65be9465ac76ea2e1711dbb57d733285",
    "semantic_title": "post-training detection of backdoor attacks for two-class and multi-attack scenarios",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=9otKVlgrpZG": {
    "title": "Multi-Task Processes",
    "volume": "poster",
    "abstract": "Neural Processes (NPs) consider a task as a function realized from a stochastic process and flexibly adapt to unseen tasks through inference on functions. However, naive NPs can model data from only a single stochastic process and are designed to infer each task independently. Since many real-world data represent a set of correlated tasks from multiple sources (e.g., multiple attributes and multi-sensor data), it is beneficial to infer them jointly and exploit the underlying correlation to improve the predictive performance. To this end, we propose Multi-Task Neural Processes (MTNPs), an extension of NPs designed to jointly infer tasks realized from multiple stochastic processes. We build MTNPs in a hierarchical way such that inter-task correlation is considered by conditioning all per-task latent variables on a single global latent variable. In addition, we further design our MTNPs so that they can address multi-task settings with incomplete data (i.e., not all tasks share the same set of input points), which has high practical demands in various applications. Experiments demonstrate that MTNPs can successfully model multiple tasks jointly by discovering and exploiting their correlations in various real-world data such as time series of weather attributes and pixel-aligned visual modalities. We release our code at https://github.com/GitGyun/multi_task_neural_processes",
    "checked": true,
    "id": "5d23a0bdac52ec017a58291368f127fb1e765cde",
    "semantic_title": "multi-task processes",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=f9MHpAGUyMn": {
    "title": "Dynamic Token Normalization improves Vision Transformers",
    "volume": "poster",
    "abstract": "Vision Transformer (ViT) and its variants (e.g., Swin, PVT) have achieved great success in various computer vision tasks, owing to their capability to learn long-range contextual information. Layer Normalization (LN) is an essential ingredient in these models. However, we found that the ordinary LN makes tokens at different positions similar in magnitude because it normalizes embeddings within each token. It is difficult for Transformers to capture inductive bias such as the positional context in an image with LN. We tackle this problem by proposing a new normalizer, termed Dynamic Token Normalization (DTN), where normalization is performed both within each token (intra-token) and across different tokens (inter-token). DTN has several merits. Firstly, it is built on a unified formulation and thus can represent various existing normalization methods. Secondly, DTN learns to normalize tokens in both intra-token and inter-token manners, enabling Transformers to capture both the global contextual information and the local positional context. Thirdly, by simply replacing LN layers, DTN can be readily plugged into various vision transformers, such as ViT, Swin, and PVT. Extensive experiments show that the transformer equipped with DTN consistently outperforms baseline model with minimal extra parameters and computational overhead. For example, DTN outperforms LN on small ViT by $1.1\\%$ top-1 accuracy on ImageNet",
    "checked": true,
    "id": "7c8c6286a62a023f5d0d71fb315f9a0d4b9a2058",
    "semantic_title": "dynamic token normalization improves vision transformer",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=ef0nInZHKIC": {
    "title": "Symbolic Learning to Optimize: Towards Interpretability and Scalability",
    "volume": "poster",
    "abstract": "Recent studies on Learning to Optimize (L2O) suggest a promising path to automating and accelerating the optimization procedure for complicated tasks. Existing L2O models parameterize optimization rules by neural networks, and learn those numerical rules via meta-training. However, they face two common pitfalls: (1) scalability: the numerical rules represented by neural networks create extra memory overhead for applying L2O models, and limits their applicability to optimizing larger tasks; (2) interpretability: it is unclear what each L2O model has learned in its black-box optimization rule, nor is it straightforward to compare different L2O models in an explainable way. To avoid both pitfalls, this paper proves the concept that we can \"kill two birds by one stone\", by introducing the powerful tool of symbolic regression to L2O. In this paper, we establish a holistic symbolic representation and analysis framework for L2O, which yields a series of insights for learnable optimizers. Leveraging our findings, we further propose a lightweight L2O model that can be meta-trained on large-scale problems and outperformed human-designed and tuned optimizers. Our work is set to supply a brand-new perspective to L2O research. Codes are available at: https://github.com/VITA-Group/Symbolic-Learning-To-Optimize",
    "checked": true,
    "id": "a0f487ac7852a10769729f7a530f5fcfaf7466a0",
    "semantic_title": "symbolic learning to optimize: towards interpretability and scalability",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=ivQruZvXxtz": {
    "title": "Sequential Reptile: Inter-Task Gradient Alignment for Multilingual Learning",
    "volume": "poster",
    "abstract": "Multilingual models jointly pretrained on multiple languages have achieved remarkable performance on various multilingual downstream tasks. Moreover, models finetuned on a single monolingual downstream task have shown to generalize to unseen languages. In this paper, we first show that it is crucial for those tasks to align gradients between them in order to maximize knowledge transfer while minimizing negative transfer. Despite its importance, the existing methods for gradient alignment either have a completely different purpose, ignore inter-task alignment, or aim to solve continual learning problems in rather inefficient ways. As a result of the misaligned gradients between tasks, the model suffers from severe negative transfer in the form of catastrophic forgetting of the knowledge acquired from the pretraining. To overcome the limitations, we propose a simple yet effective method that can efficiently align gradients between tasks. Specifically, we perform each inner-optimization by sequentially sampling batches from all the tasks, followed by a Reptile outer update. Thanks to the gradients aligned between tasks by our method, the model becomes less vulnerable to negative transfer and catastrophic forgetting. We extensively validate our method on various multi-task learning and zero-shot cross-lingual transfer tasks, where our method largely outperforms all the relevant baselines we consider",
    "checked": true,
    "id": "e830ea765e17541f4c98e0853565459f66dc5d90",
    "semantic_title": "sequential reptile: inter-task gradient alignment for multilingual learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PlKWVd2yBkY": {
    "title": "Pseudo Numerical Methods for Diffusion Models on Manifolds",
    "volume": "poster",
    "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) can generate high-quality samples such as image and audio samples. However, DDPMs require hundreds to thousands of iterations to produce a sample. Several prior works have successfully accelerated DDPMs through adjusting the variance schedule (e.g., Improved Denoising Diffusion Probabilistic Models) or the denoising equation (e.g., Denoising Diffusion Implicit Models (DDIMs)). However, these acceleration methods cannot maintain the quality of samples and even introduce new noise at high speedup rate, which limit their practicability. To accelerate the inference process while keeping the sample quality, we provide a new perspective that DDPMs should be treated as solving differential equations on manifolds. Under such a perspective, we propose pseudo numerical methods for diffusion models (PNDMs). Specifically, we figure out how to solve differential equations on manifolds and show that DDIMs are simple cases of pseudo numerical methods. We change several classical numerical methods to corresponding pseudo numerical methods and find that pseudo linear multi-step method is the best method in most situations. According to our experiments, by directly using pre-trained models on Cifar10, CelebA and LSUN, PNDMs can generate higher quality synthetic images with only 50 steps compared with 1000-step DDIMs (20x speedup), significantly outperform DDIMs with 250 steps (by around 0.4 in FID) and have good generalization on different variance schedules",
    "checked": true,
    "id": "82482585e94192b4e9913727e461f89cd08e9725",
    "semantic_title": "pseudo numerical methods for diffusion models on manifolds",
    "citation_count": 722,
    "authors": []
  },
  "https://openreview.net/forum?id=zq1iJkNk3uN": {
    "title": "Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm",
    "volume": "poster",
    "abstract": "Recently, large-scale Contrastive Language-Image Pre-training (CLIP) has attracted unprecedented attention for its impressive zero-shot recognition ability and excellent transferability to downstream tasks. However, CLIP is quite data-hungry and requires 400M image-text pairs for pre-training, thereby restricting its adoption. This work proposes a novel training paradigm, Data efficient CLIP (DeCLIP), to alleviate this limitation. We demonstrate that by carefully utilizing the widespread supervision among the image-text pairs, our De-CLIP can learn generic visual features more efficiently. Instead of using the single image-text contrastive supervision, we fully exploit data potential through the use of (1) self-supervision within each modality; (2) multi-view supervision across modalities; (3) nearest-neighbor supervision from other similar pairs. Benefiting from intrinsic supervision, our DeCLIP-ResNet50 can achieve 60.4% zero-shot top1 accuracy on ImageNet, which is 0.8% above the CLIP-ResNet50 while using 7.1√ófewer data. Our DeCLIP-ResNet50 outperforms its counterpart in 8 out of 11 visual datasets when transferred to downstream tasks. Moreover, Scaling up the model and computing also works well in our framework",
    "checked": true,
    "id": "767923635f2fd4467d848dba9655866e4f9b55c8",
    "semantic_title": "supervision exists everywhere: a data efficient contrastive language-image pre-training paradigm",
    "citation_count": 484,
    "authors": []
  },
  "https://openreview.net/forum?id=DBiQQYWykyy": {
    "title": "Environment Predictive Coding for Visual Navigation",
    "volume": "poster",
    "abstract": "We introduce environment predictive coding, a self-supervised approach to learn environment-level representations for embodied agents. In contrast to prior work on self-supervised learning for individual images, we aim to encode a 3D environment using a series of images observed by an agent moving in it. We learn these representations via a masked-zone prediction task, which segments an agent's trajectory into zones and then predicts features of randomly masked zones, conditioned on the agent's camera poses. This explicit spatial conditioning encourages learning representations that capture the geometric and semantic regularities of 3D environments. We learn such representations on a collection of video walkthroughs and demonstrate successful transfer to multiple downstream navigation tasks. Our experiments on the real-world scanned 3D environments of Gibson and Matterport3D show that our method obtains 2 - 6√ó higher sample-efÔ¨Åciency and up to 57% higher performance over standard image-representation learning",
    "checked": true,
    "id": "905043837109bc9b6437aef683d72de07e169fae",
    "semantic_title": "environment predictive coding for visual navigation",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=OXRZeMmOI7a": {
    "title": "Topological Experience Replay",
    "volume": "poster",
    "abstract": "State-of-the-art deep Q-learning methods update Q-values using state transition tuples sampled from the experience replay buffer. This strategy often randomly samples or prioritizes data sampling based on measures such as the temporal difference (TD) error. Such sampling strategies can be inefficient at learning Q-function since a state's correct Q-value preconditions on the accurate successor states' Q-value. Disregarding such a successor's value dependency leads to useless updates and even learning wrong values. To expedite Q-learning, we maintain states' dependency by organizing the agent's experience into a graph. Each edge in the graph represents a transition between two connected states. We perform value backups via a breadth-first search that expands vertices in the graph starting from the set of terminal states successively moving backward. We empirically show that our method is substantially more data-efficient than several baselines on a diverse range of goal-reaching tasks. Notably, the proposed method also outperforms baselines that consume more batches of training experience",
    "checked": true,
    "id": "a2ed2715d0175e93939e0d9f46f2580968c98020",
    "semantic_title": "topological experience replay",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=SYuJXrXq8tw": {
    "title": "Sparsity Winning Twice: Better Robust Generalization from More Efficient Training",
    "volume": "poster",
    "abstract": "Recent studies demonstrate the deep networks, even robustified by the state-of-the-art adversarial training (AT), still suffer from large robust generalization gaps, in addition to the much more expensive training costs than standard training. In this paper, we investigate this intriguing problem from a new perspective, i.e., $\\textit{injecting appropriate forms of sparsity}$ during adversarial training. We introduce two alternatives for sparse adversarial training: (i) $\\textit{static sparsity}$, by leveraging recent results from the lottery ticket hypothesis to identify critical sparse subnetworks arising from the early training; (ii) $\\textit{dynamic sparsity}$, by allowing the sparse subnetwork to adaptively adjust its connectivity pattern (while sticking to the same sparsity ratio) throughout training. We find both static and dynamic sparse methods to yield win-win: substantially shrinking the robust generalization gap and alleviating the robust overfitting, meanwhile significantly saving training and inference FLOPs. Extensive experiments validate our proposals with multiple network architectures on diverse datasets, including CIFAR-10/100 and Tiny-ImageNet. For example, our methods reduce robust generalization gap and overfitting by $34.44\\%$ and $4.02\\%$, with comparable robust/standard accuracy boosts and $87.83\\%$/$87.82\\%$ training/inference FLOPs savings on CIFAR-100 with ResNet-18. Besides, our approaches can be organically combined with existing regularizers, establishing new state-of-the-art results in AT. All codes are included",
    "checked": true,
    "id": "01594f00b0deed32cba4fc4ea8c74b60be31db4a",
    "semantic_title": "sparsity winning twice: better robust generalization from more efficient training",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=48RBsJwGkJf": {
    "title": "CrossMatch: Cross-Classifier Consistency Regularization for Open-Set Single Domain Generalization",
    "volume": "poster",
    "abstract": "Single domain generalization (SDG) is a challenging scenario of domain generalization, where only one source domain is available to train the model. Typical SDG methods are based on the adversarial data augmentation strategy, which complements the diversity of source domain to learn a robust model. Existing SDG methods require the source and target domains to have the same label space. However, as target domains may contain novel categories unseen in source label space, this assumption is not practical in many real-world applications. In this paper, we propose a challenging and untouched problem: \\textit{Open-Set Single Domain Generalization} (OS-SDG), where target domains include unseen categories out of source label space. The goal of OS-SDG is to learn a model, with only one source domain, to classify a target sample with correct class if it belongs to source label space, or assign it to unknown classes. We design a \\textit{CrossMatch} approach to improve the performance of SDG methods on identifying unknown classes by leveraging a multi-binary classifier. CrossMatch generates auxiliary samples out of source label space by using an adversarial data augmentation strategy. We also adopt a consistency regularization on generated auxiliary samples between multi-binary classifiers and the model trained by SDG methods, to improve the model's capability on unknown class identification. Experimental results on benchmark datasets prove the effectiveness of CrossMatch on enhancing the performance of SDG methods in the OS-SDG setting",
    "checked": true,
    "id": "c1e10c15172b94d057f2bcdbcd6f3ddc766d8c62",
    "semantic_title": "crossmatch: cross-classifier consistency regularization for open-set single domain generalization",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=baUQQPwQiAg": {
    "title": "Robust Unlearnable Examples: Protecting Data Privacy Against Adversarial Learning",
    "volume": "poster",
    "abstract": "The tremendous amount of accessible data in cyberspace face the risk of being unauthorized used for training deep learning models. To address this concern, methods are proposed to make data unlearnable for deep learning models by adding a type of error-minimizing noise. However, such conferred unlearnability is found fragile to adversarial training. In this paper, we design new methods to generate robust unlearnable examples that are protected from adversarial training. We first find that the vanilla error-minimizing noise, which suppresses the informative knowledge of data via minimizing the corresponding training loss, could not effectively minimize the adversarial training loss. This explains the vulnerability of error-minimizing noise in adversarial training. Based on the observation, robust error-minimizing noise is then introduced to reduce the adversarial training loss. Experiments show that the unlearnability brought by robust error-minimizing noise can effectively protect data from adversarial training in various scenarios. The code is available at \\url{https://github.com/fshp971/robust-unlearnable-examples}",
    "checked": true,
    "id": "f943391013a0436b084d6e11b8527e1465cfff53",
    "semantic_title": "robust unlearnable examples: protecting data privacy against adversarial learning",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=bZJbzaj_IlP": {
    "title": "A NON-PARAMETRIC REGRESSION VIEWPOINT : GENERALIZATION OF OVERPARAMETRIZED DEEP RELU NETWORK UNDER NOISY OBSERVATIONS",
    "volume": "poster",
    "abstract": "We study the generalization properties of the overparameterized deep neural network (DNN) with Rectified Linear Unit (ReLU) activations. Under the non-parametric regression framework, it is assumed that the ground-truth function is from a reproducing kernel Hilbert space (RKHS) induced by a neural tangent kernel (NTK) of ReLU DNN, and a dataset is given with the noises. Without a delicate adoption of early stopping, we prove that the overparametrized DNN trained by vanilla gradient descent does not recover the ground-truth function. It turns out that the estimated DNN's $L_{2}$ prediction error is bounded away from $0$. As a complement of the above result, we show that the $\\ell_{2}$-regularized gradient descent enables the overparametrized DNN achieve the minimax optimal convergence rate of the $L_{2}$ prediction error, without early stopping. Notably, the rate we obtained is faster than $\\mathcal{O}(n^{-1/2})$ known in the literature",
    "checked": true,
    "id": "cb63525f1a63ddae95d449c7ad3f70645d425d53",
    "semantic_title": "a non-parametric regression viewpoint : generalization of overparametrized deep relu network under noisy observations",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=sNuFKTMktcY": {
    "title": "Active Hierarchical Exploration with Stable Subgoal Representation Learning",
    "volume": "poster",
    "abstract": "Goal-conditioned hierarchical reinforcement learning (GCHRL) provides a promising approach to solving long-horizon tasks. Recently, its success has been extended to more general settings by concurrently learning hierarchical policies and subgoal representations. Although GCHRL possesses superior exploration ability by decomposing tasks via subgoals, existing GCHRL methods struggle in temporally extended tasks with sparse external rewards, since the high-level policy learning relies on external rewards. As the high-level policy selects subgoals in an online learned representation space, the dynamic change of the subgoal space severely hinders effective high-level exploration. In this paper, we propose a novel regularization that contributes to both stable and efficient subgoal representation learning. Building upon the stable representation, we design measures of novelty and potential for subgoals, and develop an active hierarchical exploration strategy that seeks out new promising subgoals and states without intrinsic rewards. Experimental results show that our approach significantly outperforms state-of-the-art baselines in continuous control tasks with sparse rewards",
    "checked": true,
    "id": "3a56dc25b3cba2cb438be8b5b95f4ff63f8c02ff",
    "semantic_title": "active hierarchical exploration with stable subgoal representation learning",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=St-53J9ZARf": {
    "title": "Deep AutoAugment",
    "volume": "poster",
    "abstract": "While recent automated data augmentation methods lead to state-of-the-art results, their design spaces and the derived data augmentation strategies still incorporate strong human priors. In this work, instead of fixing a set of hand-picked default augmentations alongside the searched data augmentations, we propose a fully automated approach for data augmentation search named Deep AutoAugment (DeepAA). DeepAA progressively builds a multi-layer data augmentation pipeline from scratch by stacking augmentation layers one at a time until reaching convergence. For each augmentation layer, the policy is optimized to maximize the cosine similarity between the gradients of the original and augmented data along the direction with low variance. Our experiments show that even without default augmentations, we can learn an augmentation policy that achieves strong performance with that of previous works. Extensive ablation studies show that the regularized gradient matching is an effective search method for data augmentation policies. Our code is available at: https://github.com/MSU-MLSys-Lab/DeepAA",
    "checked": true,
    "id": "03fb0ee3129371e8ed2a58f7b548201e434862d8",
    "semantic_title": "deep autoaugment",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=p3DKPQ7uaAi": {
    "title": "Temporal Alignment Prediction for Supervised Representation Learning and Few-Shot Sequence Classification",
    "volume": "poster",
    "abstract": "Explainable distances for sequence data depend on temporal alignment to tackle sequences with different lengths and local variances. Most sequence alignment methods infer the optimal alignment by solving an optimization problem under pre-defined feasible alignment constraints, which not only is time-consuming, but also makes end-to-end sequence learning intractable. In this paper, we propose a learnable sequence distance called Temporal Alignment Prediction (TAP). TAP employs a lightweight convolutional neural network to directly predict the optimal alignment between two sequences, so that only straightforward calculations are required and no optimization is involved in inference. TAP can be applied in different distance-based machine learning tasks. For supervised sequence representation learning, we show that TAP trained with various metric learning losses achieves completive performances with much faster inference speed. For few-shot action classification, we apply TAP as the distance measure in the metric learning-based episode-training paradigm. This simple strategy achieves comparable results with state-of-the-art few-shot action recognition methods",
    "checked": true,
    "id": "5318b619f72bbe3aa2d1bb7b9ffad1b69fe18754",
    "semantic_title": "temporal alignment prediction for supervised representation learning and few-shot sequence classification",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=O476oWmiNNp": {
    "title": "Anti-Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis: From Theory to Practice",
    "volume": "poster",
    "abstract": "Vision Transformer (ViT) has recently demonstrated promise in computer vision problems. However, unlike Convolutional Neural Networks (CNN), it is known that the performance of ViT saturates quickly with depth increasing, due to the observed attention collapse or patch uniformity. Despite a couple of empirical solutions, a rigorous framework studying on this scalability issue remains elusive. In this paper, we first establish a rigorous theory framework to analyze ViT features from the Fourier spectrum domain. We show that the self-attention mechanism inherently amounts to a low-pass filter, which indicates when ViT scales up its depth, excessive low-pass filtering will cause feature maps to only preserve their Direct-Current (DC) component. We then propose two straightforward yet effective techniques to mitigate the undesirable low-pass limitation. The first technique, termed AttnScale, decomposes a self-attention block into low-pass and high-pass components, then rescales and combines these two filters to produce an all-pass self-attention matrix. The second technique, termed FeatScale, re-weights feature maps on separate frequency bands to amplify the high-frequency signals. Both techniques are efficient and hyperparameter-free, while effectively overcoming relevant ViT training artifacts such as attention collapse and patch uniformity. By seamlessly plugging in our techniques to multiple ViT variants, we demonstrate that they consistently help ViTs benefit from deeper architectures, bringing up to 1.1% performance gains \"for free\" (e.g., with little parameter overhead). We publicly release our codes and pre-trained models at https://github.com/VITA-Group/ViT-Anti-Oversmoothing",
    "checked": true,
    "id": "b9225c672a5078409d890393780a5eb90f2ec3ca",
    "semantic_title": "anti-oversmoothing in deep vision transformers via the fourier domain analysis: from theory to practice",
    "citation_count": 161,
    "authors": []
  },
  "https://openreview.net/forum?id=oU3aTsmeRQV": {
    "title": "Self-ensemble Adversarial Training for Improved Robustness",
    "volume": "poster",
    "abstract": "Due to numerous breakthroughs in real-world applications brought by machine intelligence, deep neural networks (DNNs) are widely employed in critical applications. However, predictions of DNNs are easily manipulated with imperceptible adversarial perturbations, which impedes the further deployment of DNNs and may result in profound security and privacy implications. By incorporating adversarial samples into the training data pool, adversarial training is the strongest principled strategy against various adversarial attacks among all sorts of defense methods. Recent works mainly focus on developing new loss functions or regularizers, attempting to find the unique optimal point in the weight space. But none of them taps the potentials of classifiers obtained from standard adversarial training, especially states on the searching trajectory of training. In this work, we are dedicated to the weight states of models through the training process and devise a simple but powerful \\emph{Self-Ensemble Adversarial Training} (SEAT) method for yielding a robust classifier by averaging weights of history models. This considerably improves the robustness of the target model against several well known adversarial attacks, even merely utilizing the naive cross-entropy loss to supervise. We also discuss the relationship between the ensemble of predictions from different adversarially trained models and the prediction of weight-ensembled models, as well as provide theoretical and empirical evidence that the proposed self-ensemble method provides a smoother loss landscape and better robustness than both individual models and the ensemble of predictions from different classifiers. We further analyze a subtle but fatal issue in the general settings for the self-ensemble model, which causes the deterioration of the weight-ensembled method in the late phases",
    "checked": true,
    "id": "fbe2f3bc13d56aa24c3c7e5817fedd1cc859758c",
    "semantic_title": "self-ensemble adversarial training for improved robustness",
    "citation_count": 53,
    "authors": []
  },
  "https://openreview.net/forum?id=Fn7i_r5rR0q": {
    "title": "Do deep networks transfer invariances across classes?",
    "volume": "poster",
    "abstract": "In order to generalize well, classifiers must learn to be invariant to nuisance transformations that do not alter an input's class. Many problems have \"class-agnostic\" nuisance transformations that apply similarly to all classes, such as lighting and background changes for image classification. Neural networks can learn these invariances given sufficient data, but many real-world datasets are heavily class imbalanced and contain only a few examples for most of the classes. We therefore pose the question: how well do neural networks transfer class-agnostic invariances learned from the large classes to the small ones? Through careful experimentation, we observe that invariance to class-agnostic transformations is still heavily dependent on class size, with the networks being much less invariant on smaller classes. This result holds even when using data balancing techniques, and suggests poor invariance transfer across classes. Our results provide one explanation for why classifiers generalize poorly on unbalanced and long-tailed distributions. Based on this analysis, we show how a generative approach for learning the nuisance transformations can help transfer invariances across classes and improve performance on a set of imbalanced image classification benchmarks",
    "checked": true,
    "id": "1128662539971d7217cc775b1b09398553c0809b",
    "semantic_title": "do deep networks transfer invariances across classes?",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=XOh5x-vxsrV": {
    "title": "Cross-Trajectory Representation Learning for Zero-Shot Generalization in RL",
    "volume": "poster",
    "abstract": "A highly desirable property of a reinforcement learning (RL) agent -- and a major difficulty for deep RL approaches -- is the ability to generalize policies learned on a few tasks over a high-dimensional observation space to similar tasks not seen during training. Many promising approaches to this challenge consider RL as a process of training two functions simultaneously: a complex nonlinear encoder that maps high-dimensional observations to a latent representation space, and a simple linear policy over this space. We posit that a superior encoder for zero-shot generalization in RL can be trained by using solely an auxiliary SSL objective if the training process encourages the encoder to map behaviorally similar observations to similar representations, as reward-based signal can cause overfitting in the encoder (Raileanu et al., 2021). We propose Cross-Trajectory Representation Learning (CTRL), a method that runs within an RL agent and conditions its encoder to recognize behavioral similarity in observations by applying a novel SSL objective to pairs of trajectories from the agent's policies. CTRL can be viewed as having the same effect as inducing a pseudo-bisimulation metric but, crucially, avoids the use of rewards and associated overfitting risks. Our experiments ablate various components of CTRL and demonstrate that in combination with PPO it achieves better generalization performance on the challenging Procgen benchmark suite (Cobbe et al., 2020)",
    "checked": true,
    "id": "f496a4934ae20d7da7869b4c7c5c47b61fccb299",
    "semantic_title": "cross-trajectory representation learning for zero-shot generalization in rl",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=w01vBAcewNX": {
    "title": "On Covariate Shift of Latent Confounders in Imitation and Reinforcement Learning",
    "volume": "poster",
    "abstract": "We consider the problem of using expert data with unobserved confounders for imitation and reinforcement learning. We begin by defining the problem of learning from confounded expert data in a contextual MDP setup. We analyze the limitations of learning from such data with and without external reward and propose an adjustment of standard imitation learning algorithms to fit this setup. In addition, we discuss the problem of distribution shift between the expert data and the online environment when partial observability is present in the data. We prove possibility and impossibility results for imitation learning under arbitrary distribution shift of the missing covariates. When additional external reward is provided, we propose a sampling procedure that addresses the unknown shift and prove convergence to an optimal solution. Finally, we validate our claims empirically on challenging assistive healthcare and recommender system simulation tasks",
    "checked": true,
    "id": "c3d9aeed8d61a69bfc077a498436c180b529ef13",
    "semantic_title": "on covariate shift of latent confounders in imitation and reinforcement learning",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=S874XAIpkR-": {
    "title": "RvS: What is Essential for Offline RL via Supervised Learning?",
    "volume": "poster",
    "abstract": "Recent work has shown that supervised learning alone, without temporal difference (TD) learning, can be remarkably effective for offline RL. When does this hold true, and which algorithmic components are necessary? Through extensive experiments, we boil supervised learning for offline RL down to its essential elements. In every environment suite we consider, simply maximizing likelihood with a two-layer feedforward MLP is competitive with state-of-the-art results of substantially more complex methods based on TD learning or sequence modeling with Transformers. Carefully choosing model capacity (e.g., via regularization or architecture) and choosing which information to condition on (e.g., goals or rewards) are critical for performance. These insights serve as a field guide for practitioners doing Reinforcement Learning via Supervised Learning (which we coin RvS learning). They also probe the limits of existing RvS methods, which are comparatively weak on random data, and suggest a number of open problems",
    "checked": true,
    "id": "63afc8d1a187d2f2faf603a51d3987db89574308",
    "semantic_title": "rvs: what is essential for offline rl via supervised learning?",
    "citation_count": 193,
    "authors": []
  },
  "https://openreview.net/forum?id=dpXL6lz4mOQ": {
    "title": "LEARNING GUARANTEES FOR GRAPH CONVOLUTIONAL NETWORKS ON THE STOCHASTIC BLOCK MODEL",
    "volume": "poster",
    "abstract": "An abundance of neural network models and algorithms for diverse tasks on graphs have been developed in the past five years. However, very few provable guarantees have been available for the performance of graph neural network models. This state of affairs is in contrast with the steady progress on the theoretical underpinnings of traditional dense and convolutional neural networks. In this paper we present the first provable guarantees for one of the best-studied families of graph neural network models, Graph Convolutional Networks (GCNs), for semi- supervised community detection tasks. We show that with high probability over the initialization and training data, a GCN will efficiently learn to detect communities on graphs drawn from a stochastic block model. Our proof relies on a fine-grained analysis of the training dynamics in order to overcome the complexity of a non-convex optimization landscape with many poorly-performing local minima",
    "checked": true,
    "id": "0e7f3ee94083170392b27a7198a47d39ce311b1c",
    "semantic_title": "learning guarantees for graph convolutional networks on the stochastic block model",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=KEQl-MZ5fg7": {
    "title": "Learning Versatile Neural Architectures by Propagating Network Codes",
    "volume": "poster",
    "abstract": "This work explores how to design a single neural network capable of adapting to multiple heterogeneous vision tasks, such as image segmentation, 3D detection, and video recognition. This goal is challenging because both network architecture search (NAS) spaces and methods in different tasks are inconsistent. We solve this challenge from both sides. We first introduce a unified design space for multiple tasks and build a multitask NAS benchmark (NAS-Bench-MR) on many widely used datasets, including ImageNet, Cityscapes, KITTI, and HMDB51. We further propose Network Coding Propagation (NCP), which back-propagates gradients of neural predictors to directly update architecture codes along the desired gradient directions to solve various tasks. In this way, optimal architecture configurations can be found by NCP in our large search space in seconds. Unlike prior arts of NAS that typically focus on a single task, NCP has several unique benefits. (1) NCP transforms architecture optimization from data-driven to architecture-driven, enabling joint search an architecture among multitasks with different data distributions. (2) NCP learns from network codes but not original data, enabling it to update the architecture efficiently across datasets. (3) In addition to our NAS-Bench-MR, NCP performs well on other NAS benchmarks, such as NAS-Bench-201. (4) Thorough studies of NCP on inter-, cross-, and intra-tasks highlight the importance of cross-task neural architecture design, i.e., multitask neural architectures and architecture transferring between different tasks. Code is available at https://github.com/dingmyu/NCP",
    "checked": true,
    "id": "4dc868c50900050e18f8d9685b4a0414f598d663",
    "semantic_title": "learning versatile neural architectures by propagating network codes",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=OzyXtIZAzFv": {
    "title": "Task-Induced Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "821eda22a55fecd25d1cf6c9d120274cb6ef36ba",
    "semantic_title": "task-induced representation learning",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=USIgIY6TNDe": {
    "title": "Graph-based Nearest Neighbor Search in Hyperbolic Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ebc55b11825ce9b017946f0b503bcfd975ee9a93",
    "semantic_title": "graph-based nearest neighbor search in hyperbolic spaces",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=qhAeZjs7dCL": {
    "title": "Generative Models as a Data Source for Multiview Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fc0b21675cc7ec8d9cd39c3ca5257008bbcec4df",
    "semantic_title": "generative models as a data source for multiview representation learning",
    "citation_count": 136,
    "authors": []
  },
  "https://openreview.net/forum?id=cBu4ElJfneV": {
    "title": "GiraffeDet: A Heavy-Neck Paradigm for Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2a8b79d45f39dbeb1a3af2f8ae78e169def8a2fa",
    "semantic_title": "giraffedet: a heavy-neck paradigm for object detection",
    "citation_count": 99,
    "authors": []
  },
  "https://openreview.net/forum?id=Dzpe9C1mpiv": {
    "title": "A Unified Wasserstein Distributional Robustness Framework for Adversarial Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9420398975d0989c638d47c6f059d09272b6992f",
    "semantic_title": "a unified wasserstein distributional robustness framework for adversarial training",
    "citation_count": 50,
    "authors": []
  },
  "https://openreview.net/forum?id=9ZPegFuFTFv": {
    "title": "miniF2F: a cross-system benchmark for formal Olympiad-level mathematics",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7ba98b00a224094c09676090f5d6d69498f5b299",
    "semantic_title": "minif2f: a cross-system benchmark for formal olympiad-level mathematics",
    "citation_count": 230,
    "authors": []
  },
  "https://openreview.net/forum?id=lQI_mZjvBxj": {
    "title": "Towards Model Agnostic Federated Learning Using Knowledge Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e96a94b7494b70685d6c3658018da900d82a9278",
    "semantic_title": "towards model agnostic federated learning using knowledge distillation",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=541PxiEKN3F": {
    "title": "Acceleration of Federated Learning with Alleviated Forgetting in Local Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d90f81bd97db5052ac9056f3f5a46eb06006e6fb",
    "semantic_title": "acceleration of federated learning with alleviated forgetting in local training",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=hGXij5rfiHw": {
    "title": "Discovering Invariant Rationales for Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bd15a322c20f891f38e247bd5ed6e9d2f0b637eb",
    "semantic_title": "discovering invariant rationales for graph neural networks",
    "citation_count": 252,
    "authors": []
  },
  "https://openreview.net/forum?id=IYMuTbGzjFU": {
    "title": "Representing Mixtures of Word Embeddings with Mixtures of Topic Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f397feec40db0192f09b5bc1c455e482f7cee432",
    "semantic_title": "representing mixtures of word embeddings with mixtures of topic embeddings",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=5JdLZg346Lw": {
    "title": "Generative Modeling with Optimal Transport Maps",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "987a879358bdc2150965d50396c0eb0159ffdf86",
    "semantic_title": "generative modeling with optimal transport maps",
    "citation_count": 74,
    "authors": []
  },
  "https://openreview.net/forum?id=irARV_2VFs4": {
    "title": "Focus on the Common Good: Group Distributional Robustness Follows",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "1e57462f93d78279549a8508e691dc4920151b35",
    "semantic_title": "focus on the common good: group distributional robustness follows",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=PDYs7Z2XFGv": {
    "title": "Omni-Scale CNNs: a simple and effective kernel size configuration for time series classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fe9d361fd7a024507c9d0016f129766f3a3cb19b",
    "semantic_title": "omni-scale cnns: a simple and effective kernel size configuration for time series classification",
    "citation_count": 115,
    "authors": []
  },
  "https://openreview.net/forum?id=QJWVP4CTmW4": {
    "title": "Ada-NETS: Face Clustering via Adaptive Neighbour Discovery in the Structure Space",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b9107ad6f914d01cada904e0ae801e7e12bbb1ea",
    "semantic_title": "ada-nets: face clustering via adaptive neighbour discovery in the structure space",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=VNqaB1g9393": {
    "title": "Decoupled Adaptation for Cross-Domain Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6f54d34b472c0f88fc062787d377a32244d9651e",
    "semantic_title": "decoupled adaptation for cross-domain object detection",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=3Pbra-_u76D": {
    "title": "Rethinking Network Design and Local Geometry in Point Cloud: A Simple Residual MLP Framework",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a7cc9851d78bd718e17f6fca05efa16710344952",
    "semantic_title": "rethinking network design and local geometry in point cloud: a simple residual mlp framework",
    "citation_count": 665,
    "authors": []
  },
  "https://openreview.net/forum?id=N8MaByOzUfb": {
    "title": "New Insights on Reducing Abrupt Representation Change in Online Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "fcc67b627d800c610f26f620c2697cdfbdeb476b",
    "semantic_title": "new insights on reducing abrupt representation change in online continual learning",
    "citation_count": 224,
    "authors": []
  },
  "https://openreview.net/forum?id=6XGgutacQ0B": {
    "title": "Demystifying Batch Normalization in ReLU Networks: Equivalent Convex Optimization Models and Implicit Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "560c2ea251a2818008c55a69fdd2599617699b66",
    "semantic_title": "demystifying batch normalization in relu networks: equivalent convex optimization models and implicit regularization",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=4N-17dske79": {
    "title": "Associated Learning: an Alternative to End-to-End Backpropagation that Works on CNN, RNN, and Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f11a560c7ce728fdd5f3cf5eca0199323e73b2b5",
    "semantic_title": "associated learning: an alternative to end-to-end backpropagation that works on cnn, rnn, and transformer",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=MTex8qKavoS": {
    "title": "MetaShift: A Dataset of Datasets for Evaluating Contextual Distribution Shifts and Training Conflicts",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "081964f4aea6bb4950b86b2603d3956df876af9b",
    "semantic_title": "metashift: a dataset of datasets for evaluating contextual distribution shifts and training conflicts",
    "citation_count": 92,
    "authors": []
  },
  "https://openreview.net/forum?id=yjMQuLLcGWK": {
    "title": "FP-DETR: Detection Transformer Advanced by Fully Pre-training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "64cb695fef260e36c1d3d8830b197923f1e865ea",
    "semantic_title": "fp-detr: detection transformer advanced by fully pre-training",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=Ht85_jyihxp": {
    "title": "Efficient and Differentiable Conformal Prediction with General Function Classes",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8b726e859a92c70319fe8699e4a824cccdae7742",
    "semantic_title": "efficient and differentiable conformal prediction with general function classes",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=NYBmJN4MyZ": {
    "title": "Safe Neurosymbolic Learning with Differentiable Symbolic Execution",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "66c96d6eceaefed1509dbc720ae3f997d3116432",
    "semantic_title": "safe neurosymbolic learning with differentiable symbolic execution",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=GUrhfTuf_3": {
    "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5e00596fa946670d894b1bdaeff5a98e3867ef13",
    "semantic_title": "simvlm: simple visual language model pretraining with weak supervision",
    "citation_count": 836,
    "authors": []
  },
  "https://openreview.net/forum?id=aBXzcPPOuX": {
    "title": "Bundle Networks: Fiber Bundles, Local Trivializations, and a Generative Approach to Exploring Many-to-one Maps",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "08d59e24c95c11bc676a478f7feb97ceaa69c9ef",
    "semantic_title": "bundle networks: fiber bundles, local trivializations, and a generative approach to exploring many-to-one maps",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=5i2f-aR6B8H": {
    "title": "Privacy Implications of Shuffling",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5eb4891433333237b06fbc60d4c2f3b28e73be61",
    "semantic_title": "privacy implications of shuffling",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=5Qkd7-bZfI": {
    "title": "On the role of population heterogeneity in emergent communication",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "61cbee9df90ede1fae2c7fb9af30cb855a50c1c6",
    "semantic_title": "on the role of population heterogeneity in emergent communication",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=qsZoGvFiJn1": {
    "title": "Hindsight is 20/20: Leveraging Past Traversals to Aid 3D Perception",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "992f940ecfbfd38aba53d0d7871547ccfad43961",
    "semantic_title": "hindsight is 20/20: leveraging past traversals to aid 3d perception",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=RriDjddCLN": {
    "title": "Language-driven Semantic Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "cc9826c222ac1e81b4b374dd9e0df130f298b1e8",
    "semantic_title": "language-driven semantic segmentation",
    "citation_count": 701,
    "authors": []
  },
  "https://openreview.net/forum?id=ydopy-e6Dg": {
    "title": "Image BERT Pre-training with Online Tokenizer",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ff169d09a933756e8798021dbf9e24a0bbfd9b38",
    "semantic_title": "image bert pre-training with online tokenizer",
    "citation_count": 234,
    "authors": []
  },
  "https://openreview.net/forum?id=ZSKRQMvttc": {
    "title": "Accelerated Policy Learning with Parallel Differentiable Simulation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "efbc2c6306ff1f3bfa282fc62f8467764fd41c25",
    "semantic_title": "accelerated policy learning with parallel differentiable simulation",
    "citation_count": 107,
    "authors": []
  },
  "https://openreview.net/forum?id=hl9ePdHO4_s": {
    "title": "Do We Need Anisotropic Graph Neural Networks?",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6d0096bc12d61c6f7bca4999bc0c4ce83f44bc72",
    "semantic_title": "do we need anisotropic graph neural networks?",
    "citation_count": 37,
    "authors": []
  },
  "https://openreview.net/forum?id=9xhgmsNVHu": {
    "title": "Is High Variance Unavoidable in RL? A Case Study in Continuous Control",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c578a295ba3456bebc651f1f516e34ea6e7a5fe3",
    "semantic_title": "is high variance unavoidable in rl? a case study in continuous control",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=1wVvweK3oIb": {
    "title": "Simple GNN Regularisation for 3D Molecular Property Prediction and Beyond",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e31fe33b0d45971b6c9daaf0b780ba521664008f",
    "semantic_title": "simple gnn regularisation for 3d molecular property prediction and beyond",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=2bO2x8NAIMB": {
    "title": "Should We Be Pre-training? An Argument for End-task Aware Training as an Alternative",
    "volume": "poster",
    "abstract": "In most settings of practical concern, machine learning practitioners know in advance what end-task they wish to boost with auxiliary tasks. However, widely used methods for leveraging auxiliary data like pre-training and its continued-pretraining variant are end-task agnostic: they rarely, if ever, exploit knowledge of the target task. We study replacing end-task agnostic continued training of pre-trained language models with end-task aware training of said models. We argue that for sufficiently important end-tasks, the benefits of leveraging auxiliary data in a task-aware fashion can justify forgoing the traditional approach of obtaining generic, end-task agnostic representations as with (continued) pre-training. On three different low-resource NLP tasks from two domains, we demonstrate that multi-tasking the end-task and auxiliary objectives results in significantly better downstream task performance than the widely-used task-agnostic continued pre-training paradigm of Gururangan et al. (2020). We next introduce an online meta-learning algorithm that learns a set of multi-task weights to better balance among our multiple auxiliary objectives, achieving further improvements on end-task performance and data efficiency",
    "checked": true,
    "id": "4e77a4d4bcc09f6b2f3bcb790d348f4dfdbf427b",
    "semantic_title": "should we be pre-training? an argument for end-task aware training as an alternative",
    "citation_count": 36,
    "authors": []
  },
  "https://openreview.net/forum?id=wogsFPHwftY": {
    "title": "Learning Super-Features for Image Retrieval",
    "volume": "poster",
    "abstract": "Methods that combine local and global features have recently shown excellent performance on multiple challenging deep image retrieval benchmarks, but their use of local features raises at least two issues. First, these local features simply boil down to the localized map activations of a neural network, and hence can be extremely redundant. Second, they are typically trained with a global loss that only acts on top of an aggregation of local features; by contrast, testing is based on local feature matching, which creates a discrepancy between training and testing. In this paper, we propose a novel architecture for deep image retrieval, based solely on mid-level features that we call Super-features. These Super-features are constructed by an iterative attention module and constitute an ordered set in which each element focuses on a localized and discriminant image pattern. For training, they require only image labels. A contrastive loss operates directly at the level of Super-features and focuses on those that match across images. A second complementary loss encourages diversity. Experiments on common landmark retrieval benchmarks validate that Super-features substantially outperform state-of-the-art methods when using the same number of features, and only require a significantly smaller memory footprint to match their performance. Code and models are available at: https://github.com/naver/FIRe",
    "checked": true,
    "id": "6c6d5e6baaa702a1dd0a18464a80acba8128ebb9",
    "semantic_title": "learning super-features for image retrieval",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=DSQHjibtgKR": {
    "title": "Online Facility Location with Predictions",
    "volume": "poster",
    "abstract": "We provide nearly optimal algorithms for online facility location (OFL) with predictions. In OFL, $n$ demand points arrive in order and the algorithm must irrevocably assign each demand point to an open facility upon its arrival. The objective is to minimize the total connection costs from demand points to assigned facilities plus the facility opening cost. We further assume the algorithm is additionally given for each demand point $x_i$ a natural prediction $f_{x_i}^{\\mathrm{pred}}$ which is supposed to be the facility $f_{x_i}^{\\mathrm{opt}}$ that serves $x_i$ in the offline optimal solution. Our main result is an $O(\\min\\{\\log {\\frac{n\\eta_\\infty}{\\mathrm{OPT}}}, \\log{n} \\})$-competitive algorithm where $\\eta_\\infty$ is the maximum prediction error (i.e., the distance between $f_{x_i}^{\\mathrm{pred}}$ and $f_{x_i}^{\\mathrm{opt}}$). Our algorithm overcomes the fundamental $\\Omega(\\frac{\\log n}{\\log \\log n})$ lower bound of OFL (without predictions) when $\\eta_\\infty$ is small, and it still maintains $O(\\log n)$ ratio even when $\\eta_\\infty$ is unbounded. Furthermore, our theoretical analysis is supported by empirical evaluations for the tradeoffs between $\\eta_\\infty$ and the competitive ratio on various real datasets of different types",
    "checked": true,
    "id": "a54d78ea8852836225d52e6bfb8156e772bcc32a",
    "semantic_title": "online facility location with predictions",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=qSV5CuSaK_a": {
    "title": "Few-Shot Backdoor Attacks on Visual Object Tracking",
    "volume": "poster",
    "abstract": "Visual object tracking (VOT) has been widely adopted in mission-critical applications, such as autonomous driving and intelligent surveillance systems. In current practice, third-party resources such as datasets, backbone networks, and training platforms are frequently used to train high-performance VOT models. Whilst these resources bring certain convenience, they also introduce new security threats into VOT models. In this paper, we reveal such a threat where an adversary can easily implant hidden backdoors into VOT models by tempering with the training process. Specifically, we propose a simple yet effective few-shot backdoor attack (FSBA) that optimizes two losses alternately: 1) a \\emph{feature loss} defined in the hidden feature space, and 2) the standard \\emph{tracking loss}. We show that, once the backdoor is embedded into the target model by our FSBA, it can trick the model to lose track of specific objects even when the \\emph{trigger} only appears in one or a few frames. We examine our attack in both digital and physical-world settings and show that it can significantly degrade the performance of state-of-the-art VOT trackers. We also show that our attack is resistant to potential defenses, highlighting the vulnerability of VOT models to potential backdoor attacks",
    "checked": true,
    "id": "23ae1ee3a1fd6a40b29978c32c56dff83d6f9d57",
    "semantic_title": "few-shot backdoor attacks on visual object tracking",
    "citation_count": 57,
    "authors": []
  },
  "https://openreview.net/forum?id=TySnJ-0RdKI": {
    "title": "Backdoor Defense via Decoupling the Training Process",
    "volume": "poster",
    "abstract": "Recent studies have revealed that deep neural networks (DNNs) are vulnerable to backdoor attacks, where attackers embed hidden backdoors in the DNN model by poisoning a few training samples. The attacked model behaves normally on benign samples, whereas its prediction will be maliciously changed when the backdoor is activated. We reveal that poisoned samples tend to cluster together in the feature space of the attacked DNN model, which is mostly due to the end-to-end supervised training paradigm. Inspired by this observation, we propose a novel backdoor defense via decoupling the original end-to-end training process into three stages. Specifically, we first learn the backbone of a DNN model via \\emph{self-supervised learning} based on training samples without their labels. The learned backbone will map samples with the same ground-truth label to similar locations in the feature space. Then, we freeze the parameters of the learned backbone and train the remaining fully connected layers via standard training with all (labeled) training samples. Lastly, to further alleviate side-effects of poisoned samples in the second stage, we remove labels of some `low-credible' samples determined based on the learned model and conduct a \\emph{semi-supervised fine-tuning} of the whole model. Extensive experiments on multiple benchmark datasets and DNN models verify that the proposed defense is effective in reducing backdoor threats while preserving high accuracy in predicting benign samples. Our code is available at \\url{https://github.com/SCLBD/DBD}",
    "checked": true,
    "id": "6c20a12376619a3119e53202692b091635ff03c5",
    "semantic_title": "backdoor defense via decoupling the training process",
    "citation_count": 210,
    "authors": []
  },
  "https://openreview.net/forum?id=q79uMSC6ZBT": {
    "title": "Learning to Complete Code with Sketches",
    "volume": "poster",
    "abstract": "Code completion is usually cast as a language modelling problem, i.e., continuing an input in a left-to-right fashion. However, in practice, some parts of the completion (e.g., string literals) may be very hard to predict, whereas subsequent parts directly follow from the context. To handle this, we instead consider the scenario of generating code completions with \"holes\" inserted in places where a model is uncertain. We develop Grammformer, a Transformer-based model that guides the code generation by the programming language grammar, and compare it to a variety of more standard sequence models. We train the models on code completion for C# and Python given partial code context. To evaluate models, we consider both ROUGE as well as a new metric RegexAcc that measures success of generating completions matching long outputs with as few holes as possible. In our experiments, Grammformer generates 10-50% more accurate completions compared to traditional generative models and 37-50% longer sketches compared to sketch-generating baselines trained with similar techniques",
    "checked": true,
    "id": "3f791ea6584b78fdf0ed80560d09d9496cf5a353",
    "semantic_title": "learning to complete code with sketches",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=gpp7cf0xdfN": {
    "title": "Reverse Engineering of Imperceptible Adversarial Image Perturbations",
    "volume": "poster",
    "abstract": "It has been well recognized that neural network based image classifiers are easily fooled by images with tiny perturbations crafted by an adversary. There has been a vast volume of research to generate and defend such adversarial attacks. However, the following problem is left unexplored: How to reverse-engineer adversarial perturbations from an adversarial image? This leads to a new adversarial learning paradigm‚ÄîReverse Engineering of Deceptions (RED). If successful, RED allows us to estimate adversarial perturbations and recover the original images. However, carefully crafted, tiny adversarial perturbations are difficult to recover by optimizing a unilateral RED objective. For example, the pure image denoising method may overfit to minimizing the reconstruction error but hardly preserve the classification properties of the true adversarial perturbations. To tackle this challenge, we formalize the RED problem and identify a set of principles crucial to the RED approach design. Particularly, we find that prediction alignment and proper data augmentation (in terms of spatial transformations) are two criteria to achieve a generalizable RED approach. By integrating these RED principles with image denoising, we propose a new Class-Discriminative Denoising based RED framework, termed CDD-RED. Extensive experiments demonstrate the effectiveness of CDD-RED under different evaluation metrics (ranging from the pixel-level, prediction-level to the attribution-level alignment) and a variety of attack generation methods (e.g., FGSM, PGD, CW, AutoAttack, and adaptive attacks)",
    "checked": true,
    "id": "2693b44fa7d291581e9c3f23630cf5c402caf06d",
    "semantic_title": "reverse engineering of imperceptible adversarial image perturbations",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=oMI9PjOb9Jl": {
    "title": "DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR",
    "volume": "poster",
    "abstract": "We present in this paper a novel query formulation using dynamic anchor boxes for DETR (DEtection TRansformer) and offer a deeper understanding of the role of queries in DETR. This new formulation directly uses box coordinates as queries in Transformer decoders and dynamically updates them layer by layer. Using box coordinates not only helps using explicit positional priors to improve the query-to-feature similarity and eliminate the slow training convergence issue in DETR, but also allows us to modulate the positional attention map using the box width and height information. Such a design makes it clear that queries in DETR can be implemented as performing soft ROI pooling layer by layer in a cascade manner. As a result, it leads to the best performance on the MS-COCO benchmark among the DETR-like detection models under the same setting, e.g., AP 45.7\\% using ResNet50-DC5 as backbone trained in 50 epochs. We also conducted extensive experiments to confirm our analysis and verify the effectiveness of our methods. Code is available at \\url{https://github.com/IDEA-opensource/DAB-DETR}",
    "checked": true,
    "id": "004f1d2b1b7d7dcecafdd94daee9c1b0aa3e65cf",
    "semantic_title": "dab-detr: dynamic anchor boxes are better queries for detr",
    "citation_count": 882,
    "authors": []
  },
  "https://openreview.net/forum?id=tUa4REjGjTf": {
    "title": "On the Certified Robustness for Ensemble Models and Beyond",
    "volume": "poster",
    "abstract": "Recent studies show that deep neural networks (DNN) are vulnerable to adversarial examples, which aim to mislead DNNs by adding perturbations with small magnitude. To defend against such attacks, both empirical and theoretical defense approaches have been extensively studied for a single ML model. In this work, we aim to analyze and provide the certified robustness for ensemble ML models, together with the sufficient and necessary conditions of robustness for different ensemble protocols. Although ensemble models are shown more robust than a single model empirically; surprisingly, we find that in terms of the certified robustness the standard ensemble models only achieve marginal improvement compared to a single model. Thus, to explore the conditions that guarantee to provide certifiably robust ensemble ML models, we first prove that diversified gradient and large confidence margin are sufficient and necessary conditions for certifiably robust ensemble models under the model-smoothness assumption. We then provide the bounded model-smoothness analysis based on the proposed Ensemble-before-Smoothing strategy. We also prove that an ensemble model can always achieve higher certified robustness than a single base model under mild conditions. Inspired by the theoretical findings, we propose the lightweight Diversity Regularized Training (DRT) to train certifiably robust ensemble ML models. Extensive experiments show that our DRT enhanced ensembles can consistently achieve higher certified robustness than existing single and ensemble ML models, demonstrating the state-of-the-art certified $L_2$-robustness on MNIST, CIFAR-10, and ImageNet datasets",
    "checked": true,
    "id": "d4aa4fd1d0ea6da1905640adb17c67db435f9f12",
    "semantic_title": "on the certified robustness for ensemble models and beyond",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=eYciPrLuUhG": {
    "title": "Efficient Neural Causal Discovery without Acyclicity Constraints",
    "volume": "poster",
    "abstract": "Learning the structure of a causal graphical model using both observational and interventional data is a fundamental problem in many scientific fields. A promising direction is continuous optimization for score-based methods, which, however, require constrained optimization to enforce acyclicity or lack convergence guarantees. In this paper, we present ENCO, an efficient structure learning method for directed, acyclic causal graphs leveraging observational and interventional data. ENCO formulates the graph search as an optimization of independent edge likelihoods, with the edge orientation being modeled as a separate parameter. Consequently, we provide for ENCO convergence guarantees under mild conditions, without having to constrain the score function with respect to acyclicity. In experiments, we show that ENCO can efficiently recover graphs with hundreds of nodes, an order of magnitude larger than what was previously possible, while handling deterministic variables and discovering latent confounders",
    "checked": true,
    "id": "bc14baa908c94b00aa4fee713c639a6f1c0a301a",
    "semantic_title": "efficient neural causal discovery without acyclicity constraints",
    "citation_count": 77,
    "authors": []
  },
  "https://openreview.net/forum?id=6Q52pZ-Th7N": {
    "title": "Pseudo-Labeled Auto-Curriculum Learning for Semi-Supervised Keypoint Localization",
    "volume": "poster",
    "abstract": "Localizing keypoints of an object is a basic visual problem. However, supervised learning of a keypoint localization network often requires a large amount of data, which is expensive and time-consuming to obtain. To remedy this, there is an ever-growing interest in semi-supervised learning (SSL), which leverages a small set of labeled data along with a large set of unlabeled data. Among these SSL approaches, pseudo-labeling (PL) is one of the most popular. PL approaches apply pseudo-labels to unlabeled data, and then train the model with a combination of the labeled and pseudo-labeled data iteratively. The key to the success of PL is the selection of high-quality pseudo-labeled samples. Previous works mostly select training samples by manually setting a single confidence threshold. We propose to automatically select reliable pseudo-labeled samples with a series of dynamic thresholds, which constitutes a learning curriculum.Extensive experiments on five keypoint localization benchmark datasets demonstrate that the proposed approach significantly outperforms the previous state-of-the-art SSL approaches",
    "checked": true,
    "id": "ada6d6057e232ba86d050e7c5b8ff2611ce1cd4c",
    "semantic_title": "pseudo-labeled auto-curriculum learning for semi-supervised keypoint localization",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=e0jtGTfPihs": {
    "title": "Signing the Supermask: Keep, Hide, Invert",
    "volume": "poster",
    "abstract": "The exponential growth in numbers of parameters of neural networks over the past years has been accompanied by an increase in performance across several fields. However, due to their sheer size, the networks not only became difficult to interpret but also problematic to train and use in real-world applications, since hardware requirements increased accordingly. Tackling both issues, we present a novel approach that either drops a neural network's initial weights or inverts their respective sign. Put simply, a network is trained by weight selection and inversion without changing their absolute values. Our contribution extends previous work on masking by additionally sign-inverting the initial weights and follows the findings of the Lottery Ticket Hypothesis. Through this extension and adaptations of initialization methods, we achieve a pruning rate of up to 99%, while still matching or exceeding the performance of various baseline and previous models. Our approach has two main advantages. First, and most notable, signed Supermask models drastically simplify a model's structure, while still performing well on given tasks. Second, by reducing the neural network to its very foundation, we gain insights into which weights matter for performance. The code is available on GitHub",
    "checked": true,
    "id": "53b4feab1858f39744220b61717b7f848ff36822",
    "semantic_title": "signing the supermask: keep, hide, invert",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=6u6N8WWwYSM": {
    "title": "Bootstrapping Semantic Segmentation with Regional Contrast",
    "volume": "poster",
    "abstract": "We present ReCo, a contrastive learning framework designed at a regional level to assist learning in semantic segmentation. ReCo performs pixel-level contrastive learning on a sparse set of hard negative pixels, with minimal additional memory footprint. ReCo is easy to implement, being built on top of off-the-shelf segmentation networks, and consistently improves performance, achieving more accurate segmentation boundaries and faster convergence. The strongest effect is in semi-supervised learning with very few labels. With ReCo, we achieve high quality semantic segmentation model, requiring only 5 examples of each semantic class",
    "checked": true,
    "id": "d181371dd23a15d83f323a3887649d3a98b81bf6",
    "semantic_title": "bootstrapping semantic segmentation with regional contrast",
    "citation_count": 137,
    "authors": []
  },
  "https://openreview.net/forum?id=pgir5f7ekAL": {
    "title": "Generative Principal Component Analysis",
    "volume": "poster",
    "abstract": "In this paper, we study the problem of principal component analysis with generative modeling assumptions, adopting a general model for the observed matrix that encompasses notable special cases, including spiked matrix recovery and phase retrieval. The key assumption is that the first principal eigenvector lies near the range of an $L$-Lipschitz continuous generative model with bounded $k$-dimensional inputs. We propose a quadratic estimator, and show that it enjoys a statistical rate of order $\\sqrt{\\frac{k\\log L}{m}}$, where $m$ is the number of samples. Moreover, we provide a variant of the classic power method, which projects the calculated data onto the range of the generative model during each iteration. We show that under suitable conditions, this method converges exponentially fast to a point achieving the above-mentioned statistical rate. This rate is conjectured in~\\citep{aubin2019spiked,cocola2020nonasymptotic} to be the best possible even when we only restrict to the special case of spiked matrix models. We perform experiments on various image datasets for spiked matrix and phase retrieval models, and illustrate performance gains of our method to the classic power method and the truncated power method devised for sparse principal component analysis",
    "checked": true,
    "id": "48111a61b287b4bf938c9053b273afae9f2e6160",
    "semantic_title": "generative principal component analysis",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=OqcZu8JIIzS": {
    "title": "Pareto Policy Pool for Model-based Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "Online reinforcement learning (RL) can suffer from poor exploration, sparse reward, insufficient data, and overhead caused by inefficient interactions between an immature policy and a complicated environment. Model-based offline RL instead trains an environment model using a dataset of pre-collected experiences so online RL methods can learn in an offline manner by solely interacting with the model. However, the uncertainty and accuracy of the environment model can drastically vary across different state-action pairs so the RL agent may achieve high model return but perform poorly in the true environment. Unlike previous works that need to carefully tune the trade-off between the model return and uncertainty in a single objective, we study a bi-objective formulation for model-based offline RL that aims at producing a pool of diverse policies on the Pareto front performing different levels of trade-offs, which provides the flexibility to select the best policy for each realistic environment from the pool. Our method, ''Pareto policy pool (P3)'', does not need to tune the trade-off weight but can produce policies allocated at different regions of the Pareto front. For this purpose, we develop an efficient algorithm that solves multiple bi-objective optimization problems with distinct constraints defined by reference vectors targeting diverse regions of the Pareto front. We theoretically prove that our algorithm can converge to the targeted regions. In order to obtain more Pareto optimal policies without linearly increasing the cost, we leverage the achieved policies as initialization to find more Pareto optimal policies in their neighborhoods. On the D4RL benchmark for offline RL, P3 substantially outperforms several recent baseline methods over multiple tasks, especially when the quality of pre-collected experiences is low",
    "checked": true,
    "id": "1e650cb12aaad371a49b0c8c4514e1b988a5178c",
    "semantic_title": "pareto policy pool for model-based offline reinforcement learning",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=kOu3-S3wJ7": {
    "title": "Filling the G_ap_s: Multivariate Time Series Imputation by Graph Neural Networks",
    "volume": "poster",
    "abstract": "Dealing with missing values and incomplete time series is a labor-intensive, tedious, inevitable task when handling data coming from real-world applications. Effective spatio-temporal representations would allow imputation methods to reconstruct missing temporal data by exploiting information coming from sensors at different locations. However, standard methods fall short in capturing the nonlinear time and space dependencies existing within networks of interconnected sensors and do not take full advantage of the available - and often strong - relational information. Notably, most state-of-the-art imputation methods based on deep learning do not explicitly model relational aspects and, in any case, do not exploit processing frameworks able to adequately represent structured spatio-temporal data. Conversely, graph neural networks have recently surged in popularity as both expressive and scalable tools for processing sequential data with relational inductive biases. In this work, we present the first assessment of graph neural networks in the context of multivariate time series imputation. In particular, we introduce a novel graph neural network architecture, named GRIN, which aims at reconstructing missing data in the different channels of a multivariate time series by learning spatio-temporal representations through message passing. Empirical results show that our model outperforms state-of-the-art methods in the imputation task on relevant real-world benchmarks with mean absolute error improvements often higher than 20%",
    "checked": true,
    "id": "2e08a508fa9c6ae7195aa14dfe6c9e695e19aa33",
    "semantic_title": "filling the g_ap_s: multivariate time series imputation by graph neural networks",
    "citation_count": 127,
    "authors": []
  },
  "https://openreview.net/forum?id=WZ3yjh8coDg": {
    "title": "An Unconstrained Layer-Peeled Perspective on Neural Collapse",
    "volume": "poster",
    "abstract": "Neural collapse is a highly symmetric geometry of neural networks that emerges during the terminal phase of training, with profound implications on the generalization performance and robustness of the trained networks. To understand how the last-layer features and classifiers exhibit this recently discovered implicit bias, in this paper, we introduce a surrogate model called the unconstrained layer-peeled model (ULPM). We prove that gradient flow on this model converges to critical points of a minimum-norm separation problem exhibiting neural collapse in its global minimizer. Moreover, we show that the ULPM with the cross-entropy loss has a benign global landscape for its loss function, which allows us to prove that all the critical points are strict saddle points except the global minimizers that exhibit the neural collapse phenomenon. Empirically, we show that our results also hold during the training of neural networks in real-world tasks when explicit regularization or weight decay is not used",
    "checked": true,
    "id": "571faa33a05d0de7f634b49b72e63c9e47b5fe5f",
    "semantic_title": "an unconstrained layer-peeled perspective on neural collapse",
    "citation_count": 91,
    "authors": []
  },
  "https://openreview.net/forum?id=pN1JOdrSY9": {
    "title": "Contrastive Clustering to Mine Pseudo Parallel Data for Unsupervised Translation",
    "volume": "poster",
    "abstract": "Modern unsupervised machine translation systems mostly train their models by generating synthetic parallel training data from large unlabeled monolingual corpora of different languages through various means, such as iterative back-translation. However, there may exist small amount of actual parallel data hidden in the sea of unlabeled data, which has not been exploited. We develop a new fine-tuning objective, called Language-Agnostic Constraint for SwAV loss, or LAgSwAV, which enables a pre-trained model to extract such pseudo-parallel data from the monolingual corpora in a fully unsupervised manner. We then propose an effective strategy to utilize the obtained synthetic data to augment unsupervised machine translation. Our method achieves the state of the art in the WMT'14 English-French, WMT'16 German-English and English-Romanian bilingual unsupervised translation tasks, with 40.2, 36.8, 37.0 BLEU, respectively. We also achieve substantial improvements in the FLoRes low-resource English-Nepali and English-Sinhala unsupervised tasks with 5.3 and 5.4 BLEU, respectively",
    "checked": true,
    "id": "18977cf1e4a3c200529f8c01d89a63fceeff026f",
    "semantic_title": "contrastive clustering to mine pseudo parallel data for unsupervised translation",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=QRX0nCX_gk": {
    "title": "Multimeasurement Generative Models",
    "volume": "poster",
    "abstract": "We formally map the problem of sampling from an unknown distribution with a density in $\\mathbb{R}^d$ to the problem of learning and sampling a smoother density in $\\mathbb{R}^{Md}$ obtained by convolution with a fixed factorial kernel: the new density is referred to as M-density and the kernel as multimeasurement noise model (MNM). The M-density in $\\mathbb{R}^{Md}$ is smoother than the original density in $\\mathbb{R}^d$, easier to learn and sample from, yet for large $M$ the two problems are mathematically equivalent since clean data can be estimated exactly given a multimeasurement noisy observation using the Bayes estimator. To formulate the problem, we derive the Bayes estimator for Poisson and Gaussian MNMs in closed form in terms of the unnormalized M-density. This leads to a simple least-squares objective for learning parametric energy and score functions. We present various parametrization schemes of interest including one in which studying Gaussian M-densities directly leads to multidenoising autoencoders‚Äîthis is the first theoretical connection made between denoising autoencoders and empirical Bayes in the literature. Samples in $\\mathbb{R}^d$ are obtained by walk-jump sampling (Saremi & Hyvarinen, 2019) via underdamped Langevin MCMC (walk) to sample from M-density and the multimeasurement Bayes estimation (jump). We study permutation invariant Gaussian M-densities on MNIST, CIFAR-10, and FFHQ-256 datasets, and demonstrate the effectiveness of this framework for realizing fast-mixing stable Markov chains in high dimensions",
    "checked": true,
    "id": "54a876282b9eb2b98ad174e944eb2e6b8c8518e1",
    "semantic_title": "multimeasurement generative models",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=USC0-nvGPK": {
    "title": "Information Gain Propagation: a New Way to Graph Active Learning with Soft Labels",
    "volume": "poster",
    "abstract": "Graph Neural Networks (GNNs) have achieved great success in various tasks, but their performance highly relies on a large number of labeled nodes, which typically requires considerable human effort. GNN-based Active Learning (AL) methods are proposed to improve the labeling efficiency by selecting the most valuable nodes to label. Existing methods assume an oracle can correctly categorize all the selected nodes and thus just focus on the node selection. However, such an exact labeling task is costly, especially when the categorization is out of the domain of individual expert (oracle). The paper goes further, presenting a soft-label approach to AL on GNNs. Our key innovations are: i) relaxed queries where a domain expert (oracle) only judges the correctness of the predicted labels (a binary question) rather than identifying the exact class (a multi-class question), and ii) new criteria of maximizing information gain propagation for active learner with relaxed queries and soft labels. Empirical studies on public datasets demonstrate that our method significantly outperforms the state-of-the-art GNN-based AL methods in terms of both accuracy and labeling cost",
    "checked": true,
    "id": "1a4d112a0460577e60542dac7008c302cb0052b6",
    "semantic_title": "information gain propagation: a new way to graph active learning with soft labels",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=Zr5W2LSRhD": {
    "title": "Constructing Orthogonal Convolutions in an Explicit Manner",
    "volume": "poster",
    "abstract": "Convolutions with orthogonal input-output Jacobian matrix, i.e., orthogonal convolution, have recently attracted substantial attention. A convolution layer with an orthogonal Jacobian matrix is 1-Lipschitz in the 2-norm, making the output robust to the perturbation in input. Meanwhile, an orthogonal Jacobian matrix preserves the gradient norm in back-propagation, which is critical for stable training deep networks. Nevertheless, existing orthogonal convolutions are burdened by high computational costs for preserving orthogonality. In this work, we exploit the relation between the singular values of the convolution layer's Jacobian and the structure of the convolution kernel. To achieve orthogonality, we explicitly construct the convolution kernel for enforcing all singular values of the convolution layer's Jacobian to be $1$s. After training, the explicitly constructed orthogonal (ECO) convolution is constructed only once, and their weights are stored. Then, in evaluation, we only need to load the stored weights of the trained ECO convolution, and the computational cost of ECO convolution is the same as the standard dilated convolution. It is more efficient than the recent state-of-the-art approach, skew orthogonal convolution (SOC) in evaluation. Experiments on CIFAR-10 and CIFAR-100 demonstrate that the proposed ECO convolution is faster than SOC in evaluation while leading to competitive standard and certified robust accuracies",
    "checked": true,
    "id": "b22fa597c74c692fe7f38a032f39865758c793ec",
    "semantic_title": "constructing orthogonal convolutions in an explicit manner",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=P3Bh01hBYTH": {
    "title": "X-model: Improving Data Efficiency in Deep Learning with A Minimax Model",
    "volume": "poster",
    "abstract": "To mitigate the burden of data labeling, we aim at improving data efficiency for both classification and regression setups in deep learning. However, the current focus is on classification problems while rare attention has been paid to deep regression, which usually requires more human effort to labeling. Further, due to the intrinsic difference between categorical and continuous label space, the common intuitions for classification, \\textit{e.g.} cluster assumptions or pseudo labeling strategies, cannot be naturally adapted into deep regression. To this end, we first delved into the existing data-efficient methods in deep learning and found that they either encourage invariance to \\textit{data stochasticity} (\\textit{e.g.}, consistency regularization under different augmentations) or \\textit{model stochasticity} (\\textit{e.g.}, difference penalty for predictions of models with different dropout). To take the power of both worlds, we propose a novel \\Chi-model by simultaneously encouraging the invariance to {data stochasticity} and {model stochasticity}. Further, the \\Chi-model plays a minimax game between the feature extractor and task-specific heads to further enhance the invariance to model stochasticity. Extensive experiments verify the superiority of the \\Chi-model among various tasks, from a single-value prediction task of age estimation to a dense-value prediction task of keypoint localization, a 2D synthetic and a 3D realistic dataset, as well as a multi-category object recognition task",
    "checked": true,
    "id": "2a5ee2b57b3c3dd60880e9bf9405f9fd87c02784",
    "semantic_title": "x-model: improving data efficiency in deep learning with a minimax model",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=2-mkiUs9Jx7": {
    "title": "Stein Latent Optimization for Generative Adversarial Networks",
    "volume": "poster",
    "abstract": "Generative adversarial networks (GANs) with clustered latent spaces can perform conditional generation in a completely unsupervised manner. In the real world, the salient attributes of unlabeled data can be imbalanced. However, most of existing unsupervised conditional GANs cannot cluster attributes of these data in their latent spaces properly because they assume uniform distributions of the attributes. To address this problem, we theoretically derive Stein latent optimization that provides reparameterizable gradient estimations of the latent distribution parameters assuming a Gaussian mixture prior in a continuous latent space. Structurally, we introduce an encoder network and novel unsupervised conditional contrastive loss to ensure that data generated from a single mixture component represent a single attribute. We confirm that the proposed method, named Stein Latent Optimization for GANs (SLOGAN), successfully learns balanced or imbalanced attributes and achieves state-of-the-art unsupervised conditional generation performance even in the absence of attribute information (e.g., the imbalance ratio). Moreover, we demonstrate that the attributes to be learned can be manipulated using a small amount of probe data",
    "checked": true,
    "id": "264472cf2a687c2b7d247a69a891be22a096c7ee",
    "semantic_title": "stein latent optimization for generative adversarial networks",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=RRGVCN8kjim": {
    "title": "Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity",
    "volume": "poster",
    "abstract": "DETR is the first end-to-end object detector using a transformer encoder-decoder architecture and demonstrates competitive performance but low computational efficiency. The subsequent work, Deformable DETR, enhances the efficiency of DETR by replacing dense attention with deformable attention, which achieves 10x faster convergence and improved performance. Using the multiscale feature to ameliorate performance, however, the number of encoder queries increases by 20x compared to DETR, and the computation cost of the encoder attention remains a bottleneck. We observe that the encoder queries referenced by the decoder account for only 45% of the total, and find out the detection accuracy does not deteriorate significantly even if only the referenced queries are polished in the encoder block. Inspired by this observation, we propose Sparse DETR that selectively updates only the queries expected to be referenced by the decoder, thus help the model effectively detect objects. In addition, we show that applying an auxiliary detection loss on the selected queries in the encoder improves the performance while minimizing computational overhead. We validate that Sparse DETR achieves better performance than Deformable DETR even with only 10% encoder queries on the COCO dataset. Albeit only the encoder queries are sparsified, the total computation cost decreases by 38% and the frames per second (FPS) increases by 42% compared to Deformable DETR. Code will be released",
    "checked": true,
    "id": "e0d272e01929024f28f0f7cacf26177cd60b3ee7",
    "semantic_title": "sparse detr: efficient end-to-end object detection with learnable sparsity",
    "citation_count": 172,
    "authors": []
  },
  "https://openreview.net/forum?id=HMJdXzbWKH": {
    "title": "Online Target Q-learning with Reverse Experience Replay: Efficiently finding the Optimal Policy for Linear MDPs",
    "volume": "poster",
    "abstract": "Q-learning is a popular Reinforcement Learning (RL) algorithm which is widely used in practice with function approximation (Mnih et al., 2015). In contrast, existing theoretical results are pessimistic about Q-learning. For example, (Baird, 1995) shows that Q-learning does not converge even with linear function approximation for linear MDPs. Furthermore, even for tabular MDPs with synchronous updates, Q-learning was shown to have sub-optimal sample complexity (Li et al., 2021, Azar et al., 2013). The goal of this work is to bridge the gap between practical success of Q-learning and the relatively pessimistic theoretical results. The starting point of our work is the observation that in practice, Q-learning is used with two important modifications: (i) training with two networks, called online network and target network simultaneously (online target learning, or OTL) , and (ii) experience replay (ER) (Mnih et al., 2015). While they have been observed to play a significant role in the practical success of Q-learning, a thorough theoretical understanding of how these two modifications improve the convergence behavior of Q-learning has been missing in literature. By carefully combining the Q-learning with OTL and reverse experience replay (RER) (a form of experience replay), we present novel methods Q-Rex and Q-RexDaRe (Q-Rex+data reuse). We show that Q-Rex efficiently finds the optimal policy for linear MDPs and provide non-asymptotic bounds on sample complexity -- the first such result for a Q-learning method with linear MDPs. Furthermore, we demonstrate that Q-RexDaRe in fact achieves near optimal sample complexity in the tabular setting, improving upon the existing results for vanilla Q-learning",
    "checked": true,
    "id": "e410c3d2b6d7a333b1ca5a2f97f76d41ad5f4914",
    "semantic_title": "online target q-learning with reverse experience replay: efficiently finding the optimal policy for linear mdps",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=7I8LPkcx8V": {
    "title": "Differentially Private Fractional Frequency Moments Estimation with Polylogarithmic Space",
    "volume": "poster",
    "abstract": "We prove that $\\mathbb{F}_p$ sketch, a well-celebrated streaming algorithm for frequency moments estimation, is differentially private as is when $p\\in(0, 1]$. $\\mathbb{F}_p$ sketch uses only polylogarithmic space, exponentially better than existing DP baselines and only worse than the optimal non-private baseline by a logarithmic factor. The evaluation shows that $\\mathbb{F}_p$ sketch can achieve reasonable accuracy with strong privacy guarantees. The code for evaluation is included in the supplementary material",
    "checked": true,
    "id": "115c3aa15ca5b07b067ce020f6cd0582d4f8e7f2",
    "semantic_title": "differentially private fractional frequency moments estimation with polylogarithmic space",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=YpSxqy_RE84": {
    "title": "How Low Can We Go: Trading Memory for Error in Low-Precision Training",
    "volume": "poster",
    "abstract": "Low-precision arithmetic trains deep learning models using less energy, less memory and less time. However, we pay a price for the savings: lower precision may yield larger round-off error and hence larger prediction error. As applications proliferate, users must choose which precision to use to train a new model, and chip manufacturers must decide which precisions to manufacture. We view these precision choices as a hyperparameter tuning problem, and borrow ideas from meta-learning to learn the tradeoff between memory and error. In this paper, we introduce Pareto Estimation to Pick the Perfect Precision (PEPPP). We use matrix factorization to find non-dominated configurations (the Pareto frontier) with a limited number of network evaluations. For any given memory budget, the precision that minimizes error is a point on this frontier. Practitioners can use the frontier to trade memory for error and choose the best precision for their goals",
    "checked": true,
    "id": "1a686569481fa360c7ed613443f8c557e044b907",
    "semantic_title": "how low can we go: trading memory for error in low-precision training",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=rUwm9wCjURV": {
    "title": "In a Nutshell, the Human Asked for This: Latent Goals for Following Temporal Specifications",
    "volume": "poster",
    "abstract": "We address the problem of building agents whose goal is to learn to execute out-of distribution (OOD) multi-task instructions expressed in temporal logic (TL) by using deep reinforcement learning (DRL). Recent works provided evidence that the agent's neural architecture is a key feature when DRL agents are learning to solve OOD tasks in TL. Yet, the studies on this topic are still in their infancy. In this work, we propose a new deep learning configuration with inductive biases that lead agents to generate latent representations of their current goal, yielding a stronger generalization performance. We use these latent-goal networks within a neuro-symbolic framework that executes multi-task formally-defined instructions and contrast the performance of the proposed neural networks against employing different state-of-the-art (SOTA) architectures when generalizing to unseen instructions in OOD environments",
    "checked": true,
    "id": "5b7d0f5fde3f8c8ee97a0d8b13dd4d225c48a987",
    "semantic_title": "in a nutshell, the human asked for this: latent goals for following temporal specifications",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=8hWs60AZcWk": {
    "title": "Discrete Representations Strengthen Vision Transformer Robustness",
    "volume": "poster",
    "abstract": "Vision Transformer (ViT) is emerging as the state-of-the-art architecture for image recognition. While recent studies suggest that ViTs are more robust than their convolutional counterparts, our experiments find that ViTs are overly reliant on local features (\\eg, nuisances and texture) and fail to make adequate use of global context (\\eg, shape and structure). As a result, ViTs fail to generalize to out-of-distribution, real-world data. To address this deficiency, we present a simple and effective architecture modification to ViT's input layer by adding discrete tokens produced by a vector-quantized encoder. Different from the standard continuous pixel tokens, discrete tokens are invariant under small perturbations and contain less information individually, which promote ViTs to learn global information that is invariant. Experimental results demonstrate that adding discrete representation on four architecture variants strengthens ViT robustness by up to 12\\% across seven ImageNet robustness benchmarks while maintaining the performance on ImageNet",
    "checked": true,
    "id": "601ab36b6f077ff57472f4a0cf2e061dd05b9b85",
    "semantic_title": "discrete representations strengthen vision transformer robustness",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=JzNB0eA2-M4": {
    "title": "On the Convergence of the Monte Carlo Exploring Starts Algorithm for Reinforcement Learning",
    "volume": "poster",
    "abstract": "A simple and natural algorithm for reinforcement learning (RL) is Monte Carlo Exploring Starts (MCES), where the Q-function is estimated by averaging the Monte Carlo returns, and the policy is improved by choosing actions that maximize the current estimate of the Q-function. Exploration is performed by \"exploring starts\", that is, each episode begins with a randomly chosen state and action, and then follows the current policy to the terminal state. In the classic book on RL by Sutton & Barto (2018), it is stated that establishing convergence for the MCES algorithm is one of the most important remaining open theoretical problems in RL. However, the convergence question for MCES turns out to be quite nuanced. Bertsekas & Tsitsiklis (1996) provide a counter-example showing that the MCES algorithm does not necessarily converge. Tsitsiklis (2002) further shows that if the original MCES algorithm is modified so that the Q-function estimates are updated at the same rate for all state-action pairs, and the discount factor is strictly less than one, then the MCES algorithm converges. In this paper we make headway with the original and more efficient MCES algorithm given in Sutton et al. (1998), establishing almost sure convergence for Optimal Policy Feed-Forward MDPs, which are MDPs whose states are not revisited within any episode when using an optimal policy. Such MDPs include a large class of environments such as all deterministic environments and all episodic environments with a timestep or any monotonically changing values as part of the state. Different from the previous proofs using stochastic approximations, we introduce a novel inductive approach, which is very simple and only makes use of the strong law of large numbers",
    "checked": true,
    "id": "d3c45d5403563c765b387c351be86281ba7ad730",
    "semantic_title": "on the convergence of the monte carlo exploring starts algorithm for reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rw1mZl_ss3L": {
    "title": "Concurrent Adversarial Learning for Large-Batch Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "dece6a5bd2246a09777f5201e7f530b00c4b5ba8",
    "semantic_title": "concurrent adversarial learning for large-batch training",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=5K7RRqZEjoS": {
    "title": "Multiset-Equivariant Set Prediction with Approximate Implicit Differentiation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7a8493a6d744f621945457bd0f5428a399ba5333",
    "semantic_title": "multiset-equivariant set prediction with approximate implicit differentiation",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=msRBojTz-Nh": {
    "title": "Learned Simulators for Turbulence",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8049cb71b76be847c1c4886a79a9cbd4d9b1d821",
    "semantic_title": "learned simulators for turbulence",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=5XmLzdslFNN": {
    "title": "Modular Lifelong Reinforcement Learning via Neural Composition",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0b3ca2a700085a877c560e20558566536f18d5a2",
    "semantic_title": "modular lifelong reinforcement learning via neural composition",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=7B3IJMM1k_M": {
    "title": "Optimal ANN-SNN Conversion for High-accuracy and Ultra-low-latency Spiking Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "f791cf3dbd0a1c97d340869988370fe086bfca03",
    "semantic_title": "optimized potential initialization for low-latency spiking neural networks",
    "citation_count": 96,
    "authors": []
  },
  "https://openreview.net/forum?id=fvLLcIYmXb": {
    "title": "AS-MLP: An Axial Shifted MLP Architecture for Vision",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "71363797140647ebb3f540584de0a8758d2f7aa2",
    "semantic_title": "as-mlp: an axial shifted mlp architecture for vision",
    "citation_count": 196,
    "authors": []
  },
  "https://openreview.net/forum?id=nrGGfMbY_qK": {
    "title": "Online Continual Learning on Class Incremental Blurry Task Configuration with Anytime Inference",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "272e9e9fc81b4620f2226e77a59ad899fdcadb5b",
    "semantic_title": "online continual learning on class incremental blurry task configuration with anytime inference",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=TBWA6PLJZQm": {
    "title": "Learning with Noisy Labels Revisited: A Study Using Real-World Human Annotations",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ab8510d2675a94c9d29e49d949820a2eb136df10",
    "semantic_title": "learning with noisy labels revisited: a study using real-world human annotations",
    "citation_count": 279,
    "authors": []
  },
  "https://openreview.net/forum?id=nbC8iTTXIrk": {
    "title": "Optimization inspired Multi-Branch Equilibrium Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3a0ef15417a40192870834a4344a30b9f8d0d1e3",
    "semantic_title": "optimization inspired multi-branch equilibrium models",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=zNR43c03lRy": {
    "title": "Learning to Annotate Part Segmentation with Gradient Matching",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b18072fa828f4d19319ed76d97cb2b356a29f67c",
    "semantic_title": "learning to annotate part segmentation with gradient matching",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=pfNyExj7z2": {
    "title": "Vector-quantized Image Modeling with Improved VQGAN",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9c7a2cd13b783bb73ad2d1ec2880bdd9b995cbdc",
    "semantic_title": "vector-quantized image modeling with improved vqgan",
    "citation_count": 597,
    "authors": []
  },
  "https://openreview.net/forum?id=R8sQPpGCv0": {
    "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "9ca329408813d209b1dcb36936f7f9cba82506bd",
    "semantic_title": "train short, test long: attention with linear biases enables input length extrapolation",
    "citation_count": 880,
    "authors": []
  },
  "https://openreview.net/forum?id=J1rhANsCY9": {
    "title": "Learning Representation from Neural Fisher Kernel with Low-rank Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a737ce16dbb4db48e50e3a4861d6f7b9e002d753",
    "semantic_title": "learning representation from neural fisher kernel with low-rank approximation",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=RDlLMjLJXdq": {
    "title": "Learning Temporally Causal Latent Processes from General Temporal Data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "62c77adfd98fd2fd0b58ee900ab5156750ae577d",
    "semantic_title": "learning temporally causal latent processes from general temporal data",
    "citation_count": 91,
    "authors": []
  },
  "https://openreview.net/forum?id=DXPftn5kjQK": {
    "title": "The Rich Get Richer: Disparate Impact of Semi-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "76e95fc9c312e40e59cd3034a4d1bcde4acac511",
    "semantic_title": "the rich get richer: disparate impact of semi-supervised learning",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=HBsJNesj2S": {
    "title": "Neural Relational Inference with Node-Specific Information",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "0a9a6bb422e831b422d68dfa96d7683f2d7f41d6",
    "semantic_title": "neural relational inference with node-specific information",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=ZU-zFnTum1N": {
    "title": "Bregman Gradient Policy Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8cca25bf5259f86dcb8c7999c3ab743558fdac0f",
    "semantic_title": "bregman gradient policy optimization",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=hgKtwSb4S2": {
    "title": "A generalization of the randomized singular value decomposition",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f4b1076c5b5c392ed2e887c97a8cde9aa00e6a6c",
    "semantic_title": "a generalization of the randomized singular value decomposition",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=xCVJMsPv3RT": {
    "title": "Dropout Q-Functions for Doubly Efficient Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b2d931da61559c528c5d4eadcb939425a2531652",
    "semantic_title": "dropout q-functions for doubly efficient reinforcement learning",
    "citation_count": 132,
    "authors": []
  },
  "https://openreview.net/forum?id=ySQH0oDyp7": {
    "title": "QDrop: Randomly Dropping Quantization for Extremely Low-bit Post-Training Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "edfea69d9cd1a4d40f4d879aa36f93ad7d26a659",
    "semantic_title": "qdrop: randomly dropping quantization for extremely low-bit post-training quantization",
    "citation_count": 196,
    "authors": []
  },
  "https://openreview.net/forum?id=POxF-LEqnF": {
    "title": "You Mostly Walk Alone: Analyzing Feature Attribution in Trajectory Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "781e22916f75c1340cfae83caaba8aff5fa219a2",
    "semantic_title": "you mostly walk alone: analyzing feature attribution in trajectory prediction",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=aYAA-XHKyk": {
    "title": "Rethinking Class-Prior Estimation for Positive-Unlabeled Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "240d50efd3a57a6d34b9e48e1a27a44a095b0f99",
    "semantic_title": "rethinking class-prior estimation for positive-unlabeled learning",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=bfuGjlCwAq": {
    "title": "Learning Efficient Online 3D Bin Packing on Packing Configuration Trees",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "669e33deca3245e27f9c01805210b6022581ba80",
    "semantic_title": "learning efficient online 3d bin packing on packing configuration trees",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=6HN7LHyzGgC": {
    "title": "Uncertainty Modeling for Out-of-Distribution Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "3485aa52bf7a8310e80e139907d8c2e649e3ab66",
    "semantic_title": "uncertainty modeling for out-of-distribution generalization",
    "citation_count": 197,
    "authors": []
  },
  "https://openreview.net/forum?id=bYGSzbCM_i": {
    "title": "Online Adversarial Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c02ade047261fb3a199219ae1274d70529d0a26c",
    "semantic_title": "online adversarial attacks",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=kNKFOXleuC": {
    "title": "Anytime Dense Prediction with Confidence Adaptivity",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "48d70a9662cd2aef09ec2ddf8d91f4358e7d96ac",
    "semantic_title": "anytime dense prediction with confidence adaptivity",
    "citation_count": 22,
    "authors": []
  },
  "https://openreview.net/forum?id=q4HaTeMO--y": {
    "title": "Declarative nets that are equilibrium models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "120d537965ef4c0eb560c1bce866ae9815065cd5",
    "semantic_title": "declarative nets that are equilibrium models",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=AcrlgZ9BKed": {
    "title": "A Reduction-Based Framework for Conservative Bandits and Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "390efde41d805b8dbc7e2f2c4c3e92643063a500",
    "semantic_title": "a reduction-based framework for conservative bandits and reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MvO2t0vbs4-": {
    "title": "Wisdom of Committees: An Overlooked Approach To Faster and More Accurate Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f61855fa6acf7bd28b20187ad1cf219e96429534",
    "semantic_title": "wisdom of committees: an overlooked approach to faster and more accurate models",
    "citation_count": 54,
    "authors": []
  },
  "https://openreview.net/forum?id=rwE8SshAlxw": {
    "title": "Unsupervised Discovery of Object Radiance Fields",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "7c690f7c0597ef8e966ab61281ecb552aa0b37a4",
    "semantic_title": "unsupervised discovery of object radiance fields",
    "citation_count": 125,
    "authors": []
  },
  "https://openreview.net/forum?id=fPhKeld3Okz": {
    "title": "Gradient Step Denoiser for convergent Plug-and-Play",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "854926f19cd2426fe6b0bef5ed834c83fd56f97b",
    "semantic_title": "gradient step denoiser for convergent plug-and-play",
    "citation_count": 115,
    "authors": []
  },
  "https://openreview.net/forum?id=edONMAnhLu-": {
    "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "58ab7d67f2eeb4d753cf443da3bccaeaa7897fde",
    "semantic_title": "surrogate gap minimization improves sharpness-aware training",
    "citation_count": 179,
    "authors": []
  },
  "https://openreview.net/forum?id=MQ2sAGunyBP": {
    "title": "R4D: Utilizing Reference Objects for Long-Range Distance Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c104e14a13846680c675c1782a97a3be46b8064a",
    "semantic_title": "r4d: utilizing reference objects for long-range distance estimation",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=YevsQ05DEN7": {
    "title": "Understanding Dimensional Collapse in Contrastive Self-supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "28c17db217f2d7af12482a087d197851f0a97db0",
    "semantic_title": "understanding dimensional collapse in contrastive self-supervised learning",
    "citation_count": 390,
    "authors": []
  },
  "https://openreview.net/forum?id=d71n4ftoCBy": {
    "title": "FedPara: Low-rank Hadamard Product for Communication-Efficient Federated Learning",
    "volume": "poster",
    "abstract": "In this work, we propose a communication-efficient parameterization, $\\texttt{FedPara}$, for federated learning (FL) to overcome the burdens on frequent model uploads and downloads. Our method re-parameterizes weight parameters of layers using low-rank weights followed by the Hadamard product. Compared to the conventional low-rank parameterization, our $\\texttt{FedPara}$ method is not restricted to low-rank constraints, and thereby it has a far larger capacity. This property enables to achieve comparable performance while requiring 3 to 10 times lower communication costs than the model with the original layers, which is not achievable by the traditional low-rank methods. The efficiency of our method can be further improved by combining with other efficient FL optimizers. In addition, we extend our method to a personalized FL application, $\\texttt{pFedPara}$, which separates parameters into global and local ones. We show that $\\texttt{pFedPara}$ outperforms competing personalized FL methods with more than three times fewer parameters",
    "checked": true,
    "id": "e991a2047cc0640f32439bafa879726041129c96",
    "semantic_title": "fedpara: low-rank hadamard product for communication-efficient federated learning",
    "citation_count": 143,
    "authors": []
  },
  "https://openreview.net/forum?id=T__V3uLix7V": {
    "title": "RegionViT: Regional-to-Local Attention for Vision Transformers",
    "volume": "poster",
    "abstract": "Vision transformer (ViT) has recently shown its strong capability in achieving comparable results to convolutional neural networks (CNNs) on image classification. However, vanilla ViT simply inherits the same architecture from the natural language processing directly, which is often not optimized for vision applications. Motivated by this, in this paper, we propose a new architecture that adopts the pyramid structure and employ novel regional-to-local attention rather than global self-attention in vision transformers. More specifically, our model first generates regional tokens and local tokens from an image with different patch sizes, where each regional token is associated with a set of local tokens based on the spatial location. The regional-to-local attention includes two steps: first, the regional self-attention extracts global information among all regional tokens and then the local self-attention exchanges the information among one regional token and the associated local tokens via self-attention. Therefore, even though local self-attention confines the scope in a local region but it can still receive global information. Extensive experiments on four vision tasks, including image classification, object and keypoint detection, semantics segmentation and action recognition, show that our approach outperforms or is on par with state-of-the-art ViT variants including many concurrent works. Our source codes and models are available at \\url{https://github.com/IBM/RegionViT}",
    "checked": true,
    "id": "2e8149dafb864ec3675087c99bf5572fcf4eb170",
    "semantic_title": "regionvit: regional-to-local attention for vision transformers",
    "citation_count": 207,
    "authors": []
  },
  "https://openreview.net/forum?id=fR-EnKWL_Zb": {
    "title": "Quadtree Attention for Vision Transformers",
    "volume": "poster",
    "abstract": "Transformers have been successful in many vision tasks, thanks to their capability of capturing long-range dependency. However, their quadratic computational complexity poses a major obstacle for applying them to vision tasks requiring dense predictions, such as object detection, feature matching, stereo, etc. We introduce QuadTree Attention, which reduces the computational complexity from quadratic to linear. Our quadtree transformer builds token pyramids and computes attention in a coarse-to-fine manner. At each level, the top K patches with the highest attention scores are selected, such that at the next level, attention is only evaluated within the relevant regions corresponding to these top K patches. We demonstrate that quadtree attention achieves state-of-the-art performance in various vision tasks, e.g. with 4.0% improvement in feature matching on ScanNet, about 50% flops reduction in stereo matching, 0.4-1.5% improvement in top-1 accuracy on ImageNet classification, 1.2-1.8% improvement on COCO object detection, and 0.7-2.4% improvement on semantic segmentation over previous state-of-the-art transformers. The codes are available at https://github.com/Tangshitao/QuadtreeAttention",
    "checked": true,
    "id": "0d9b8ccb1135b8e380dd8015b080158c6aae3ae5",
    "semantic_title": "quadtree attention for vision transformers",
    "citation_count": 174,
    "authors": []
  },
  "https://openreview.net/forum?id=jaLDP8Hp_gc": {
    "title": "Visual Correspondence Hallucination",
    "volume": "poster",
    "abstract": "Given a pair of partially overlapping source and target images and a keypoint in the source image, the keypoint's correspondent in the target image can be either visible, occluded or outside the field of view. Local feature matching methods are only able to identify the correspondent's location when it is visible, while humans can also hallucinate its location when it is occluded or outside the field of view through geometric reasoning. In this paper, we bridge this gap by training a network to output a peaked probability distribution over the correspondent's location, regardless of this correspondent being visible, occluded, or outside the field of view. We experimentally demonstrate that this network is indeed able to hallucinate correspondences on pairs of images captured in scenes that were not seen at training-time. We also apply this network to an absolute camera pose estimation problem and find it is significantly more robust than state-of-the-art local feature matching-based competitors",
    "checked": true,
    "id": "12975ab22e9bee532404337a944b5c509a35cde5",
    "semantic_title": "visual correspondence hallucination: towards geometric reasoning",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=mk0HzdqY7i1": {
    "title": "What's Wrong with Deep Learning in Tree Search for Combinatorial Optimization",
    "volume": "poster",
    "abstract": "Combinatorial optimization lies at the core of many real-world problems. Especially since the rise of graph neural networks (GNNs), the deep learning community has been developing solvers that derive solutions to NP-hard problems by learning the problem-specific solution structure. However, reproducing the results of these publications proves to be difficult. We make three contributions. First, we present an open-source benchmark suite for the NP-hard Maximum Independent Set problem, in both its weighted and unweighted variants. The suite offers a unified interface to various state-of-the-art traditional and machine learning-based solvers. Second, using our benchmark suite, we conduct an in-depth analysis of the popular guided tree search algorithm by Li et al. [NeurIPS 2018], testing various configurations on small and large synthetic and real-world graphs. By re-implementing their algorithm with a focus on code quality and extensibility, we show that the graph convolution network used in the tree search does not learn a meaningful representation of the solution structure, and can in fact be replaced by random values. Instead, the tree search relies on algorithmic techniques like graph kernelization to find good solutions. Thus, the results from the original publication are not reproducible. Third, we extend the analysis to compare the tree search implementations to other solvers, showing that the classical algorithmic solvers often are faster, while providing solutions of similar quality. Additionally, we analyze a recent solver based on reinforcement learning and observe that for this solver, the GNN is responsible for the competitive solution quality",
    "checked": true,
    "id": "e6bf8b804d1fa49f90150b26ee1e4eb92157657f",
    "semantic_title": "what's wrong with deep learning in tree search for combinatorial optimization",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=T4-65DNlDij": {
    "title": "Deep Attentive Variational Inference",
    "volume": "poster",
    "abstract": "Stochastic Variational Inference is a powerful framework for learning large-scale probabilistic latent variable models. However, typical assumptions on the factorization or independence of the latent variables can substantially restrict its capacity for inference and generative modeling. A major line of active research aims at building more expressive variational models by designing deep hierarchies of interdependent latent variables. Although these models exhibit superior performance and enable richer latent representations, we show that they incur diminishing returns: adding more stochastic layers to an already very deep model yields small predictive improvement while substantially increasing the inference and training time. Moreover, the architecture for this class of models favors local interactions among the latent variables between neighboring layers when designing the conditioning factors of the involved distributions. This is the first work that proposes attention mechanisms to build more expressive variational distributions in deep probabilistic models by explicitly modeling both local and global interactions in the latent space. Specifically, we propose deep attentive variational autoencoder and test it on a variety of established datasets. We show it achieves state-of-the-art log-likelihoods while using fewer latent layers and requiring less training time than existing models. The proposed non-local inference reduces computational footprint by alleviating the need for deep hierarchies. Project code: https://github.com/ifiaposto/Deep_Attentive_VI",
    "checked": true,
    "id": "d52031ffd5d5b6704752a7dc051dbc04ea47e712",
    "semantic_title": "deep attentive variational inference",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=CVfLvQq9gLo": {
    "title": "ARTEMIS: Attention-based Retrieval with Text-Explicit Matching and Implicit Similarity",
    "volume": "poster",
    "abstract": "An intuitive way to search for images is to use queries composed of an example image and a complementary text. While the first provides rich and implicit context for the search, the latter explicitly calls for new traits, or specifies how some elements of the example image should be changed to retrieve the desired target image. Current approaches typically combine the features of each of the two elements of the query into a single representation, which can then be compared to the ones of the potential target images. Our work aims at shedding new light on the task by looking at it through the prism of two familiar and related frameworks: text-to-image and image-to-image retrieval. Taking inspiration from them, we exploit the specific relation of each query element with the targeted image and derive light-weight attention mechanisms which enable to mediate between the two complementary modalities. We validate our approach on several retrieval benchmarks, querying with images and their associated free-form text modifiers. Our method obtains state-of-the-art results without resorting to side information, multi-level features, heavy pre-training nor large architectures as in previous works",
    "checked": true,
    "id": "69778262994c3183ac02c7e535a3e9256c5231fa",
    "semantic_title": "artemis: attention-based retrieval with text-explicit matching and implicit similarity",
    "citation_count": 117,
    "authors": []
  },
  "https://openreview.net/forum?id=C_vsGwEIjAr": {
    "title": "Trivial or Impossible --- dichotomous data difficulty masks model differences (on ImageNet and beyond)",
    "volume": "poster",
    "abstract": "The power of a generalization system follows directly from its biases\" (Mitchell 1980). Today, CNNs are incredibly powerful generalisation systems---but to what degree have we understood how their inductive bias influences model decisions? We here attempt to disentangle the various aspects that determine how a model decides. In particular, we ask: what makes one model decide differently from another? In a meticulously controlled setting, we find that (1.) irrespective of the network architecture or objective (e.g. self-supervised, semi-supervised, vision transformers, recurrent models) all models end up with a similar decision boundary. (2.) To understand these findings, we analysed model decisions on the ImageNet validation set from epoch to epoch and image by image. We find that the ImageNet validation set, among others, suffers from dichotomous data difficulty (DDD): For the range of investigated models and their accuracies, it is dominated by 46.0% \"trivial\" and 11.5% \"impossible\" images (beyond label errors). Only 42.5% of the images could possibly be responsible for the differences between two models' decision boundaries. (3.) Only removing the \"impossible\" and \"trivial\" images allows us to see pronounced differences between models. (4.) Humans are highly accurate at predicting which images are \"trivial\" and \"impossible\" for CNNs (81.4%). This implies that in future comparisons of brains, machines and behaviour, much may be gained from investigating the decisive role of images and the distribution of their difficulties",
    "checked": true,
    "id": "68aca52dc55878a345420f9c32eaaec77794481b",
    "semantic_title": "trivial or impossible - dichotomous data difficulty masks model differences (on imagenet and beyond)",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=u6s8dSporO8": {
    "title": "Group equivariant neural posterior estimation",
    "volume": "poster",
    "abstract": "Simulation-based inference with conditional neural density estimators is a powerful approach to solving inverse problems in science. However, these methods typically treat the underlying forward model as a black box, with no way to exploit geometric properties such as equivariances. Equivariances are common in scientific models, however integrating them directly into expressive inference networks (such as normalizing flows) is not straightforward. We here describe an alternative method to incorporate equivariances under joint transformations of parameters and data. Our method---called group equivariant neural posterior estimation (GNPE)---is based on self-consistently standardizing the \"pose\" of the data while estimating the posterior over parameters. It is architecture-independent, and applies both to exact and approximate equivariances. As a real-world application, we use GNPE for amortized inference of astrophysical binary black hole systems from gravitational-wave observations. We show that GNPE achieves state-of-the-art accuracy while reducing inference times by three orders of magnitude",
    "checked": true,
    "id": "e1bce54a3f17eeb22e41a397fae4907c65a40866",
    "semantic_title": "group equivariant neural posterior estimation",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=-AOEi-5VTU8": {
    "title": "Fast Differentiable Matrix Square Root",
    "volume": "poster",
    "abstract": "Computing the matrix square root or its inverse in a differentiable manner is important in a variety of computer vision tasks. Previous methods either adopt the Singular Value Decomposition (SVD) to explicitly factorize the matrix or use the Newton-Schulz iteration (NS iteration) to derive the approximate solution. However, both methods are not computationally efficient enough in either the forward pass or in the backward pass. In this paper, we propose two more efficient variants to compute the differentiable matrix square root. For the forward propagation, one method is to use Matrix Taylor Polynomial (MTP), and the other method is to use Matrix Pad\\'e Approximants (MPA). The backward gradient is computed by iteratively solving the continuous-time Lyapunov equation using the matrix sign function. Both methods yield considerable speed-up compared with the SVD or the Newton-Schulz iteration. Experimental results on the de-correlated batch normalization and second-order vision transformer demonstrate that our methods can also achieve competitive and even slightly better performances. The code is available at \\href{https://github.com/KingJamesSong/FastDifferentiableMatSqrt}{https://github.com/KingJamesSong/FastDifferentiableMatSqrt}",
    "checked": true,
    "id": "e10f14defd9613cf5ab7a953cd834cb6f8fb2b63",
    "semantic_title": "fast differentiable matrix square root",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=JXhROKNZzOc": {
    "title": "SQuant: On-the-Fly Data-Free Quantization via Diagonal Hessian Approximation",
    "volume": "poster",
    "abstract": "Quantization of deep neural networks (DNN) has been proven effective for compressing and accelerating DNN models. Data-free quantization (DFQ) is a promising approach without the original datasets under privacy-sensitive and confidential scenarios. However, current DFQ solutions degrade accuracy, need synthetic data to calibrate networks, and are time-consuming and costly. This paper proposes an on-the-fly DFQ framework with sub-second quantization time, called SQuant, which can quantize networks on inference-only devices with low computation and memory requirements. With the theoretical analysis of the second-order information of DNN task loss, we decompose and approximate the Hessian-based optimization objective into three diagonal sub-items, which have different areas corresponding to three dimensions of weight tensor: element-wise, kernel-wise, and output channel-wise. Then, we progressively compose sub-items and propose a novel data-free optimization objective in the discrete domain, minimizing Constrained Absolute Sum of Error (or CASE in short), which surprisingly does not need any dataset and is even not aware of network architecture. We also design an efficient algorithm without back-propagation to further reduce the computation complexity of the objective solver. Finally, without fine-tuning and synthetic datasets, SQuant accelerates the data-free quantization process to a sub-second level with >30% accuracy improvement over the existing data-free post-training quantization works, with the evaluated models under 4-bit quantization. We have open-sourced the SQuant framework at https://github.com/clevercool/SQuant",
    "checked": true,
    "id": "583f353972ed917772f3f1fc62f4b7cadc8f1e81",
    "semantic_title": "squant: on-the-fly data-free quantization via diagonal hessian approximation",
    "citation_count": 79,
    "authors": []
  },
  "https://openreview.net/forum?id=lyLVzukXi08": {
    "title": "Neural Variational Dropout Processes",
    "volume": "poster",
    "abstract": "Learning to infer the conditional posterior model is a key step for robust meta-learning. This paper presents a new Bayesian meta-learning approach called Neural Variational Dropout Processes (NVDPs). NVDPs model the conditional posterior distribution based on a task-specific dropout; a low-rank product of Bernoulli experts meta-model is utilized for a memory-efficient mapping of dropout rates from a few observed contexts. It allows for a quick reconfiguration of a globally learned and shared neural network for new tasks in multi-task few-shot learning. In addition, NVDPs utilize a novel prior conditioned on the whole task data to optimize the conditional dropout posterior in the amortized variational inference. Surprisingly, this enables the robust approximation of task-specific dropout rates that can deal with a wide range of functional ambiguities and uncertainties. We compared the proposed method with other meta-learning approaches in the few-shot learning tasks such as 1D stochastic regression, image inpainting, and classification. The results show the excellent performance of NVDPs",
    "checked": true,
    "id": "1fe05706b9ac70805929b1fac1311d8f1b3d1e9f",
    "semantic_title": "neural variational dropout processes",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=kQ2SOflIOVC": {
    "title": "Towards Better Understanding and Better Generalization of Low-shot Classification in Histology Images with Contrastive Learning",
    "volume": "poster",
    "abstract": "Few-shot learning is an established topic in natural images for years, but few work is attended to histology images, which is of high clinical value since well-labeled datasets and rare abnormal samples are expensive to collect. Here, we facilitate the study of few-shot learning in histology images by setting up three cross-domain tasks that simulate real clinics problems. To enable label-efficient learning and better generalizability, we propose to incorporate contrastive learning (CL) with latent augmentation (LA) to build a few-shot system. CL learns useful representations without manual labels, while LA transfers semantic variations of the base dataset in an unsupervised way. These two components fully exploit unlabeled training data and can scale gracefully to other label-hungry problems. In experiments, we find i) models learned by CL generalize better than supervised learning for histology images in unseen classes, and ii) LA brings consistent gains over baselines. Prior studies of self-supervised learning mainly focus on ImageNet-like images, which only present a dominant object in their centers. Recent attention has been paid to images with multi-objects and multi-textures. Histology images are a natural choice for such a study. We show the superiority of CL over supervised learning in terms of generalization for such data and provide our empirical understanding for this observation. The findings in this work could contribute to understanding how the model generalizes in the context of both representation learning and histological image analysis. Code is available",
    "checked": true,
    "id": "620611fb99f47be149ff2b4b2f5f73432dda5557",
    "semantic_title": "towards better understanding and better generalization of few-shot classification in histology images with contrastive learning",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=QjOQkpzKbNk": {
    "title": "Distilling GANs with Style-Mixed Triplets for X2I Translation with Limited Data",
    "volume": "poster",
    "abstract": "Conditional image synthesis is an integral part of many X2I translation systems, including image-to-image, text-to-image and audio-to-image translation systems. Training these large systems generally requires huge amounts of training data. Therefore, we investigate knowledge distillation to transfer knowledge from a high-quality unconditioned generative model (e.g., StyleGAN) to a conditioned synthetic image generation modules in a variety of systems. To initialize the conditional and reference branch (from a unconditional GAN) we exploit the style mixing characteristics of high-quality GANs to generate an infinite supply of style-mixed triplets to perform the knowledge distillation. Extensive experimental results in a number of image generation tasks (i.e., image-to-image, semantic segmentation-to-image, text-to-image and audio-to-image) demonstrate qualitatively and quantitatively that our method successfully transfers knowledge to the synthetic image generation modules, resulting in more realistic images than previous methods as confirmed by a significant drop in the FID",
    "checked": true,
    "id": "2e1f585dedc7eef1e47c31d07c7bb7d767d75fb4",
    "semantic_title": "distilling gans with style-mixed triplets for x2i translation with limited data",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=FQOC5u-1egI": {
    "title": "Handling Distribution Shifts on Graphs: An Invariance Perspective",
    "volume": "poster",
    "abstract": "There is increasing evidence suggesting neural networks' sensitivity to distribution shifts, so that research on out-of-distribution (OOD) generalization comes into the spotlight. Nonetheless, current endeavors mostly focus on Euclidean data, and its formulation for graph-structured data is not clear and remains under-explored, given two-fold fundamental challenges: 1) the inter-connection among nodes in one graph, which induces non-IID generation of data points even under the same environment, and 2) the structural information in the input graph, which is also informative for prediction. In this paper, we formulate the OOD problem on graphs and develop a new invariant learning approach, Explore-to-Extrapolate Risk Minimization (EERM), that facilitates graph neural networks to leverage invariance principles for prediction. EERM resorts to multiple context explorers (specified as graph structure editers in our case) that are adversarially trained to maximize the variance of risks from multiple virtual environments. Such a design enables the model to extrapolate from a single observed environment which is the common case for node-level prediction. We prove the validity of our method by theoretically showing its guarantee of a valid OOD solution and further demonstrate its power on various real-world datasets for handling distribution shifts from artificial spurious features, cross-domain transfers and dynamic graph evolution",
    "checked": true,
    "id": "c559fd26624401e8bf1586c1f08e8de560b41a21",
    "semantic_title": "handling distribution shifts on graphs: an invariance perspective",
    "citation_count": 212,
    "authors": []
  },
  "https://openreview.net/forum?id=hSktDu-h94": {
    "title": "Automatic Loss Function Search for Predict-Then-Optimize Problems with Strong Ranking Property",
    "volume": "poster",
    "abstract": "Combinatorial optimization problems with parameters to be predicted from side information are commonly seen in a variety of problems during the paradigm shift from reactive decision making to proactive decision making. Due to the misalignment between the continuous prediction results and the discrete decisions in optimization problems, it is hard to achieve a satisfactory prediction result with the ordinary $l_2$ loss in the prediction phase. To properly connect the prediction loss with the optimization goal, in this paper we propose a total group preorder (TGP) loss and its differential version called approximated total group preorder (ATGP) loss for predict-then-optimize (PTO) problems with strong ranking property. These new losses are provably more robust than the usual $l_2$ loss in a linear regression setting and have great potential to extend to other settings. We also propose an automatic searching algorithm that adapts the ATGP loss to PTO problems with different combinatorial structures. Extensive experiments on the ranking problem, the knapsack problem, and the shortest path problem have demonstrated that our proposed method can achieve a significant performance compared to the other methods designed for PTO problems",
    "checked": true,
    "id": "d42b866ad0b57225916a96e1a50b44c00adaf47b",
    "semantic_title": "automatic loss function search for predict-then-optimize problems with strong ranking property",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YigKlMJwjye": {
    "title": "Generalized Demographic Parity for Group Fairness",
    "volume": "poster",
    "abstract": "This work aims to generalize demographic parity to continuous sensitive attributes while preserving tractable computation. Current fairness metrics for continuous sensitive attributes largely rely on intractable statistical independence between variables, such as Hirschfeld-Gebelein-Renyi (HGR) and mutual information. Statistical fairness metrics estimation relying on either tractable bounds or neural network approximation, however, are not sufficiently trustful to rank algorithms prediction bias due to lack of estimation accuracy guarantee. To make fairness metrics trustable, we propose \\textit{\\underline{G}eneralized \\underline{D}emographic \\underline{P}arity} (GDP), a group fairness metric for continuous and discrete attributes. We show the understanding of GDP from the probability perspective and theoretically reveal the connection between GDP regularizer and adversarial debiasing. To estimate GDP, we adopt hard and soft group strategies via the one-hot or the soft group indicator, representing the membership of each sample in different groups of the sensitive attribute. We provably and numerically show that the soft group strategy achieves a faster estimation error convergence rate. Experiments show the better bias mitigation performance of GDP regularizer, compared with adversarial debiasing, for regression and classification tasks in tabular and graph benchmarks",
    "checked": true,
    "id": "71629202ad8ad4550520726148afe7305b7387b8",
    "semantic_title": "generalized demographic parity for group fairness",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=ljxWpdBl4V": {
    "title": "Closed-form Sample Probing for Learning Generative Models in Zero-shot Learning",
    "volume": "poster",
    "abstract": "Generative model based approaches have led to significant advances in zero-shot learning (ZSL) over the past few years. These approaches typically aim to learn a conditional generator that synthesizes training samples of classes conditioned on class definitions. The final zero-shot learning model is then obtained by training a supervised classification model over the real and/or synthesized training samples of seen and unseen classes, combined. Therefore, naturally, the generative model needs to produce not only relevant samples, but also those that are sufficiently rich for classifier training purposes, which is handled by various heuristics in existing works. In this paper, we introduce a principled approach for training generative models {\\em directly} for training data generation purposes. Our main observation is that the use of closed-form models opens doors to end-to-end training thanks to the differentiability of the solvers. In our approach, at each generative model update step, we fit a task-specific closed-form ZSL model from generated samples, and measure its loss on novel samples all within the compute graph, a procedure that we refer to as {\\em sample probing}. In this manner, the generator receives feedback directly based on the value of its samples for model training purposes. Our experimental results show that the proposed sample probing approach improves the ZSL results even when integrated into state-of-the-art generative models",
    "checked": true,
    "id": "f114c76d5eca4747bb8af5e8cdb9c16a4ddc3406",
    "semantic_title": "closed-form sample probing for learning generative models in zero-shot learning",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=J_F_qqCE3Z5": {
    "title": "DKM: Differentiable k-Means Clustering Layer for Neural Network Compression",
    "volume": "poster",
    "abstract": "Deep neural network (DNN) model compression for efficient on-device inference is becoming increasingly important to reduce memory requirements and keep user data on-device. To this end, we propose a novel differentiable k-means clustering layer (DKM) and its application to train-time weight clustering-based DNN model compression. DKM casts k-means clustering as an attention problem and enables joint optimization of the DNN parameters and clustering centroids. Unlike prior works that rely on additional regularizers and parameters, DKM-based compression keeps the original loss function and model architecture fixed. We evaluated DKM-based compression on various DNN models for computer vision and natural language processing (NLP) tasks. Our results demonstrate that DKM delivers superior compression and accuracy trade-off on ImageNet1k and GLUE benchmarks. For example, DKM-based compression can offer 74.5% top-1 ImageNet1k accuracy on ResNet50 DNN model with 3.3MB model size (29.4x model compression factor). For MobileNet-v1, which is a challenging DNN to compress, DKM delivers 63.9% top-1 ImageNet1k accuracy with 0.72 MB model size (22.4x model compression factor). This result is 6.8% higher top-1accuracy and 33% relatively smaller model size than the current state-of-the-art DNN compression algorithms. Additionally, DKM enables compression of DistilBERT model by 11.8x with minimal (1.1%) accuracy loss on GLUE NLP benchmarks",
    "checked": true,
    "id": "3f0574949341836549783869ac16e8082729a05d",
    "semantic_title": "dkm: differentiable k-means clustering layer for neural network compression",
    "citation_count": 34,
    "authors": []
  },
  "https://openreview.net/forum?id=hcMvApxGSzZ": {
    "title": "Fixed Neural Network Steganography: Train the images, not the network",
    "volume": "poster",
    "abstract": "Recent attempts at image steganography make use of advances in deep learning to train an encoder-decoder network pair to hide and retrieve secret messages in images. These methods are able to hide large amounts of data, but they also incur high decoding error rates (around 20%). In this paper, we propose a novel algorithm for steganography that takes advantage of the fact that neural networks are sensitive to tiny perturbations. Our method, Fixed Neural Network Steganography (FNNS), yields significantly lower error rates when compared to prior state-of-the-art methods and achieves 0% error reliably for hiding up to 3 bits per pixel (bpp) of secret information in images. FNNS also successfully evades existing statistical steganalysis systems and can be modified to evade neural steganalysis systems as well. Recovering every bit correctly, up to 3 bpp, enables novel applications that requires encryption. We introduce one specific use case for facilitating anonymized and safe image sharing. Our code is available at https://github.com/varshakishore/FNNS",
    "checked": true,
    "id": "042bfdccaebd07b47085408bf6baa3566112e538",
    "semantic_title": "fixed neural network steganography: train the images, not the network",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=N9W24a4zU": {
    "title": "Steerable Partial Differential Operators for Equivariant Neural Networks",
    "volume": "poster",
    "abstract": "Recent work in equivariant deep learning bears strong similarities to physics. Fields over a base space are fundamental entities in both subjects, as are equivariant maps between these fields. In deep learning, however, these maps are usually defined by convolutions with a kernel, whereas they are partial differential operators (PDOs) in physics. Developing the theory of equivariant PDOs in the context of deep learning could bring these subjects even closer together and lead to a stronger flow of ideas. In this work, we derive a $G$-steerability constraint that completely characterizes when a PDO between feature vector fields is equivariant, for arbitrary symmetry groups $G$. We then fully solve this constraint for several important groups. We use our solutions as equivariant drop-in replacements for convolutional layers and benchmark them in that role. Finally, we develop a framework for equivariant maps based on Schwartz distributions that unifies classical convolutions and differential operators and gives insight about the relation between the two",
    "checked": true,
    "id": "8d1118549b20c451abd0f412eb48a925922d98f8",
    "semantic_title": "steerable partial differential operators for equivariant neural networks",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=oVE1z8NlNe": {
    "title": "Divergence-aware Federated Self-Supervised Learning",
    "volume": "poster",
    "abstract": "Self-supervised learning (SSL) is capable of learning remarkable representations from centrally available data. Recent works further implement federated learning with SSL to learn from rapidly growing decentralized unlabeled images (e.g., from cameras and phones), often resulted from privacy constraints. Extensive attention has been paid to SSL approaches based on Siamese networks. However, such an effort has not yet revealed deep insights into various fundamental building blocks for the federated self-supervised learning (FedSSL) architecture. We aim to fill in this gap via in-depth empirical study and propose a new method to tackle the non-independently and identically distributed (non-IID) data problem of decentralized data. Firstly, we introduce a generalized FedSSL framework that embraces existing SSL methods based on Siamese networks and presents flexibility catering to future methods. In this framework, a server coordinates multiple clients to conduct SSL training and periodically updates local models of clients with the aggregated global model. Using the framework, our study uncovers unique insights of FedSSL: 1) stop-gradient operation, previously reported to be essential, is not always necessary in FedSSL; 2) retaining local knowledge of clients in FedSSL is particularly beneficial for non-IID data. Inspired by the insights, we then propose a new approach for model update, Federated Divergence-aware Exponential Moving Average update (FedEMA). FedEMA updates local models of clients adaptively using EMA of the global model, where the decay rate is dynamically measured by model divergence. Extensive experiments demonstrate that FedEMA outperforms existing methods by 3-4% on linear evaluation. We hope that this work will provide useful insights for future research",
    "checked": true,
    "id": "eb9104bb72f3856751d8c1b9c9573768d5f3df35",
    "semantic_title": "divergence-aware federated self-supervised learning",
    "citation_count": 108,
    "authors": []
  },
  "https://openreview.net/forum?id=0rcbOaoBXbg": {
    "title": "Neural Spectral Marked Point Processes",
    "volume": "poster",
    "abstract": "Self- and mutually-exciting point processes are popular models in machine learning and statistics for dependent discrete event data. To date, most existing models assume stationary kernels (including the classical Hawkes processes) and simple parametric models. Modern applications with complex event data require more general point process models that can incorporate contextual information of the events, called marks, besides the temporal and location information. Moreover, such applications often require non-stationary models to capture more complex spatio-temporal dependence. To tackle these challenges, a key question is to devise a versatile influence kernel in the point process model. In this paper, we introduce a novel and general neural network-based non-stationary influence kernel with high expressiveness for handling complex discrete events data while providing theoretical performance guarantees. We demonstrate the superior performance of our proposed method compared with the state-of-the-art on synthetic and real data",
    "checked": true,
    "id": "c4e9ca295f59a80d4059ddaafa5534fa781f054d",
    "semantic_title": "neural spectral marked point processes",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=Bn09TnDngN": {
    "title": "How to Inject Backdoors with Better Consistency: Logit Anchoring on Clean Data",
    "volume": "poster",
    "abstract": "Since training a large-scale backdoored model from scratch requires a large training dataset, several recent attacks have considered to inject backdoors into a trained clean model without altering model behaviors on the clean data. Previous work finds that backdoors can be injected into a trained clean model with Adversarial Weight Perturbation (AWP), which means the variation of parameters are small in backdoor learning. In this work, we observe an interesting phenomenon that the variations of parameters are always AWPs when tuning the trained clean model to inject backdoors. We further provide theoretical analysis to explain this phenomenon. We are the first to formulate the behavior of maintaining accuracy on clean data as the consistency of backdoored models, which includes both global consistency and instance-wise consistency. We extensively analyze the effects of AWPs on the consistency of backdoored models. In order to achieve better consistency, we propose a novel anchoring loss to anchor or freeze the model behaviors on the clean data, with a theoretical guarantee",
    "checked": true,
    "id": "5223b4b3d19d553a66eca98b4b31122d37468154",
    "semantic_title": "how to inject backdoors with better consistency: logit anchoring on clean data",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=Lwr8We4MIxn": {
    "title": "A Biologically Interpretable Graph Convolutional Network to Link Genetic Risk Pathways and Imaging Phenotypes of Disease",
    "volume": "poster",
    "abstract": "We propose a novel end-to-end framework for whole-brain and whole-genome imaging-genetics. Our genetics network uses hierarchical graph convolution and pooling operations to embed subject-level data onto a low-dimensional latent space. The hierarchical network implicitly tracks the convergence of genetic risk across well-established biological pathways, while an attention mechanism automatically identifies the salient edges of this network at the subject level. In parallel, our imaging network projects multimodal data onto a set of latent embeddings. For interpretability, we implement a Bayesian feature selection strategy to extract the discriminative imaging biomarkers; these feature weights are optimized alongside the other model parameters. We couple the imaging and genetic embeddings with a predictor network, to ensure that the learned representations are linked to phenotype. We evaluate our framework on a schizophrenia dataset that includes two functional MRI paradigms and gene scores derived from Single Nucleotide Polymorphism data. Using repeated 10-fold cross-validation, we show that our imaging-genetics fusion achieves the better classification performance than state-of-the-art baselines. In an exploratory analysis, we further show that the biomarkers identified by our model are reproducible and closely associated with deficits in schizophrenia",
    "checked": true,
    "id": "d1bf6fa13b2013b0f6275acf8554fb4f9449d610",
    "semantic_title": "a biologically interpretable graph convolutional network to link genetic risk pathways and imaging phenotypes of disease",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=ek9a0qIafW": {
    "title": "Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners",
    "volume": "poster",
    "abstract": "Large-scale pre-trained language models have contributed significantly to natural language processing by demonstrating remarkable abilities as few-shot learners. However, their effectiveness depends mainly on scaling the model parameters and prompt design, hindering their implementation in most real-world applications. This study proposes a novel pluggable, extensible, and efficient approach named DifferentiAble pRompT (DART), which can convert small language models into better few-shot learners. The main principle behind this approach involves reformulating potential natural language processing tasks into the task of a pre-trained language model and differentially optimizing the prompt template as well as the target label with backpropagation. Furthermore, the proposed approach can be: (i) Plugged to any pre-trained language models; (ii) Extended to widespread classification tasks. A comprehensive evaluation of standard NLP tasks demonstrates that the proposed approach achieves a better few-shot performance",
    "checked": true,
    "id": "9b56086e420ecb216f85d408a25264f640e46705",
    "semantic_title": "differentiable prompt makes pre-trained language models better few-shot learners",
    "citation_count": 185,
    "authors": []
  },
  "https://openreview.net/forum?id=yfe1VMYAXa4": {
    "title": "OntoProtein: Protein Pretraining With Gene Ontology Embedding",
    "volume": "poster",
    "abstract": "Self-supervised protein language models have proved their effectiveness in learning the proteins representations. With the increasing computational power, current protein language models pre-trained with millions of diverse sequences can advance the parameter scale from million-level to billion-level and achieve remarkable improvement. However, those prevailing approaches rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better protein representations. We argue that informative biology knowledge in KGs can enhance protein representation with external knowledge. In this work, we propose OntoProtein, the first general framework that makes use of structure in GO (Gene Ontology) into protein pre-training models. We construct a novel large-scale knowledge graph that consists of GO and its related proteins, and gene annotation texts or protein sequences describe all nodes in the graph. We propose novel contrastive learning with knowledge-aware negative sampling to jointly optimize the knowledge graph and protein embedding during pre-training. Experimental results show that OntoProtein can surpass state-of-the-art methods with pre-trained protein language models in TAPE benchmark and yield better performance compared with baselines in protein-protein interaction and protein function prediction",
    "checked": true,
    "id": "a03ac04948846190a5915feb3ab3ede1a4b26b15",
    "semantic_title": "ontoprotein: protein pretraining with gene ontology embedding",
    "citation_count": 107,
    "authors": []
  },
  "https://openreview.net/forum?id=GugZ5DzzAu": {
    "title": "Permutation Compressors for Provably Faster Distributed Nonconvex Optimization",
    "volume": "poster",
    "abstract": "In this work we study the MARINA method of Gorbunov et al (ICML, 2021) -- the current state-of-the-art distributed non-convex optimization method in terms of theoretical communication complexity. Theoretical superiority of this method can be largely attributed to two sources: a carefully engineered biased stochastic gradient estimator, which leads to a reduction in the number of communication rounds, and the reliance on {\\em independent} stochastic communication compression, which leads to a reduction in the number of transmitted bits within each communication round. In this paper we i) extend the theory of MARINA to support a much wider class of potentially {\\em correlated} compressors, extending the reach of the method beyond the classical independent compressors setting, ii) show that a new quantity, for which we coin the name {\\em Hessian variance}, allows us to significantly refine the original analysis of MARINA without any additional assumptions, and iii) identify a special class of correlated compressors based on the idea of {\\em random permutations}, for which we coin the term Perm$K$, the use of which leads to up to $O(\\sqrt{n})$ (resp. $O(1 + d/\\sqrt{n})$) improvement in the theoretical communication complexity of MARINA in the low Hessian variance regime when $d\\geq n$ (resp. $d \\leq n$), where $n$ is the number of workers and $d$ is the number of parameters describing the model we are learning. We corroborate our theoretical results with carefully engineered synthetic experiments with minimizing the average of nonconvex quadratics, and on autoencoder training with the MNIST dataset",
    "checked": true,
    "id": "6e8fe46a27900de93a8d2c2e5f232b6b5adff9f2",
    "semantic_title": "permutation compressors for provably faster distributed nonconvex optimization",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=6kCiVaoQdx9": {
    "title": "Few-shot Learning via Dirichlet Tessellation Ensemble",
    "volume": "poster",
    "abstract": "Few-shot learning (FSL) is the process of rapid generalization from abundant base samples to inadequate novel samples. Despite extensive research in recent years, FSL is still not yet able to generate satisfactory solutions for a wide range of real-world applications. To confront this challenge, we study the FSL problem from a geometric point of view in this paper. One observation is that the widely embraced ProtoNet model is essentially a Voronoi Diagram (VD) in the feature space. We retrofit it by making use of a recent advance in computational geometry called Cluster-induced Voronoi Diagram (CIVD). Starting from the simplest nearest neighbor model, CIVD gradually incorporates cluster-to-point and then cluster-to-cluster relationships for space subdivision, which is used to improve the accuracy and robustness at multiple stages of FSL. Specifically, we use CIVD (1) to integrate parametric and nonparametric few-shot classifiers; (2) to combine feature representation and surrogate representation; (3) and to leverage feature-level, transformation-level, and geometry-level heterogeneities for a better ensemble. Our CIVD-based workflow enables us to achieve new state-of-the-art results on mini-ImageNet, CUB, and tiered-ImagenNet datasets, with ${\\sim}2\\%{-}5\\%$ improvements upon the next best. To summarize, CIVD provides a mathematically elegant and geometrically interpretable framework that compensates for extreme data insufficiency, prevents overfitting, and allows for fast geometric ensemble for thousands of individual VD. These together make FSL stronger",
    "checked": true,
    "id": "ab3ad1b6d55a3ca0ca9183e9a1a96735fe1c500d",
    "semantic_title": "few-shot learning via dirichlet tessellation ensemble",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=mKDtUtxIGJ": {
    "title": "Deep Point Cloud Reconstruction",
    "volume": "poster",
    "abstract": "Point cloud obtained from 3D scanning is often sparse, noisy, and irregular. To cope with these issues, recent studies have been separately conducted to densify, denoise, and complete inaccurate point cloud. In this paper, we advocate that jointly solving these tasks leads to significant improvement for point cloud reconstruction. To this end, we propose a deep point cloud reconstruction network consisting of two stages: 1) a 3D sparse stacked-hourglass network as for the initial densification and denoising, 2) a refinement via transformers converting the discrete voxels into continuous 3D points. In particular, we further improve the performance of the transformers by a newly proposed module called amplified positional encoding. This module has been designed to differently amplify the magnitude of positional encoding vectors based on the points' distances for adaptive refinements. Extensive experiments demonstrate that our network achieves state-of-the-art performance among the recent studies in the ScanNet, ICL-NUIM, and ShapeNet datasets. Moreover, we underline the ability of our network to generalize toward real-world and unmet scenes",
    "checked": true,
    "id": "cc48dd21e92f5f154f6c86495a224e4d69e29ea2",
    "semantic_title": "deep point cloud reconstruction",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=q7n2RngwOM": {
    "title": "$\\beta$-Intact-VAE: Identifying and Estimating Causal Effects under Limited Overlap",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "919656373ce96831fbe8dc91f1b5535b256d20d6",
    "semantic_title": "$\\beta$-intact-vae: identifying and estimating causal effects under limited overlap",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=BZnnMbt0pW": {
    "title": "Promoting Saliency From Depth: Deep Unsupervised RGB-D Saliency Detection",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a0d19f43c75f231fd631babbf5d9125b4dff311c",
    "semantic_title": "promoting saliency from depth: deep unsupervised rgb-d saliency detection",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=AXWygMvuT6Q": {
    "title": "Retriever: Learning Content-Style Representation as a Token-Level Bipartite Graph",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5f2e17546d47f0ee4ae70b05cc6a61e4c0562f43",
    "semantic_title": "retriever: learning content-style representation as a token-level bipartite graph",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=7DI6op61AY": {
    "title": "Neural Markov Controlled SDE: Stochastic Optimization for Continuous-Time Data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "425bd431cd48ae4dbe15d83bd433b11b4ba84c79",
    "semantic_title": "neural markov controlled sde: stochastic optimization for continuous-time data",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=_PHymLIxuI": {
    "title": "CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a5c41f188b0eb0acb444cb4899bf6af378ee9ede",
    "semantic_title": "crossformer: a versatile vision transformer hinging on cross-scale attention",
    "citation_count": 291,
    "authors": []
  },
  "https://openreview.net/forum?id=9L1BsI4wP1H": {
    "title": "Adversarially Robust Conformal Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "acc8f7cc17ce5009bd2504572e8b9b76148e63a7",
    "semantic_title": "adversarially robust conformal prediction",
    "citation_count": 38,
    "authors": []
  },
  "https://openreview.net/forum?id=HTp-6yLGGX": {
    "title": "Hot-Refresh Model Upgrades with Regression-Free Compatible Training in Image Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e03629b93f92e2e80e7d9020ca60ab93db378d3a",
    "semantic_title": "hot-refresh model upgrades with regression-free compatible training in image retrieval",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=kG0AtPi6JI1": {
    "title": "Visual Representation Learning over Latent Domains",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e0071927a83f29ab191c603f3a3bf9b6b1b35e96",
    "semantic_title": "visual representation learning over latent domains",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=6sh3pIzKS-": {
    "title": "Chemical-Reaction-Aware Molecule Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "309049d5003f0876a759c983fce4edf510f1b006",
    "semantic_title": "chemical-reaction-aware molecule representation learning",
    "citation_count": 69,
    "authors": []
  },
  "https://openreview.net/forum?id=jeLW-Fh9bV": {
    "title": "Skill-based Meta-Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "23bac2542b145bf2fcd17d7fa0a02ae03d0a45f7",
    "semantic_title": "skill-based meta-reinforcement learning",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=ufGMqIM0a4b": {
    "title": "InfinityGAN: Towards Infinite-Pixel Image Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "e64447d336f7eb3a205f93cbc8fd461c44b51ad7",
    "semantic_title": "infinitygan: towards infinite-pixel image synthesis",
    "citation_count": 75,
    "authors": []
  },
  "https://openreview.net/forum?id=DrZXuTGg2A-": {
    "title": "Shuffle Private Stochastic Convex Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "63b14309ee6ec539499636ec7f51c4d43d9a4a12",
    "semantic_title": "shuffle private stochastic convex optimization",
    "citation_count": 28,
    "authors": []
  },
  "https://openreview.net/forum?id=MljXVdp4A3N": {
    "title": "Know Your Action Set: Learning Action Relations for Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "abeb9aa8b4ec2fd1a8a8908a4b0c18ee7f1fa117",
    "semantic_title": "know your action set: learning action relations for reinforcement learning",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=3eIrli0TwQ": {
    "title": "On the Importance of Difficulty Calibration in Membership Inference Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5d96e9061337fcaedcfba2ee2ceb6daa85151568",
    "semantic_title": "on the importance of difficulty calibration in membership inference attacks",
    "citation_count": 150,
    "authors": []
  },
  "https://openreview.net/forum?id=VrjOFfcnSV8": {
    "title": "Entroformer: A Transformer-based Entropy Model for Learned Image Compression",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a7de7968bd9d4d97daaac74c3c1d466f3342ff45",
    "semantic_title": "entroformer: a transformer-based entropy model for learned image compression",
    "citation_count": 153,
    "authors": []
  },
  "https://openreview.net/forum?id=fOsN52jn25l": {
    "title": "Dual Lottery Ticket Hypothesis",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "94d353a313021f2237afb28d93965c4767263352",
    "semantic_title": "dual lottery ticket hypothesis",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=hzmQ4wOnSb": {
    "title": "GNN is a Counter? Revisiting GNN for Question Answering",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "ed02d4a08a50d23709b7ce5df9878a8bc34ac12c",
    "semantic_title": "gnn is a counter? revisiting gnn for question answering",
    "citation_count": 32,
    "authors": []
  },
  "https://openreview.net/forum?id=OT3mLgR8Wg8": {
    "title": "IFR-Explore: Learning Inter-object Functional Relationships in 3D Indoor Scenes",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "6551c77fd3a99b3ea00bdc54075357c8cc5a2150",
    "semantic_title": "ifr-explore: learning inter-object functional relationships in 3d indoor scenes",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=iEx3PiooLy": {
    "title": "VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D ARTiculated Objects",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "883ac0fcebca7f8df1a8a253c4d27ed83494efae",
    "semantic_title": "vat-mart: learning visual action trajectory proposals for manipulating 3d articulated objects",
    "citation_count": 99,
    "authors": []
  },
  "https://openreview.net/forum?id=SsHBkfeRF9L": {
    "title": "Neural graphical modelling in continuous-time: consistency guarantees and algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "5de29a2474abb0888c13aab0f6e56a761ee69602",
    "semantic_title": "neural graphical modelling in continuous-time: consistency guarantees and algorithms",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=K2JfSnLBD9": {
    "title": "C-Planning: An Automatic Curriculum for Learning Goal-Reaching Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "52aeb38922f1f60ef4032012c70f9d5363547e03",
    "semantic_title": "c-planning: an automatic curriculum for learning goal-reaching tasks",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=0DLwqQLmqV": {
    "title": "NAS-Bench-Suite: NAS Evaluation is (Now) Surprisingly Easy",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "4857d4de584ed6f15e7ef12a96823ec8b4a17ec1",
    "semantic_title": "nas-bench-suite: nas evaluation is (now) surprisingly easy",
    "citation_count": 46,
    "authors": []
  },
  "https://openreview.net/forum?id=mhYUBYNoGz": {
    "title": "Machine Learning For Elliptic PDEs: Fast Rate Generalization Bound, Neural Scaling Law and Minimax Optimality",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "16bf2241414f01bd3c98316cc75ef733fb6a1980",
    "semantic_title": "machine learning for elliptic pdes: fast rate generalization bound, neural scaling law and minimax optimality",
    "citation_count": 44,
    "authors": []
  },
  "https://openreview.net/forum?id=pjqqxepwoMy": {
    "title": "Variational oracle guiding for reinforcement learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "18ea0cc03be311dc5d24e082d96aed021419b2b2",
    "semantic_title": "variational oracle guiding for reinforcement learning",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=XGzk5OKWFFc": {
    "title": "CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "f734996bd0300856f6fabd3dde15ef080967cb02",
    "semantic_title": "cdtrans: cross-domain transformer for unsupervised domain adaptation",
    "citation_count": 243,
    "authors": []
  },
  "https://openreview.net/forum?id=QkRV50TZyP": {
    "title": "Beyond ImageNet Attack: Towards Crafting Adversarial Examples for Black-box Domains",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "956d22ab5f6e450f56c545e1f817bc5f3c78d439",
    "semantic_title": "beyond imagenet attack: towards crafting adversarial examples for black-box domains",
    "citation_count": 72,
    "authors": []
  },
  "https://openreview.net/forum?id=k7efTb0un9z": {
    "title": "Learning to Schedule Learning rate with Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a4bea03d7a62917bf46ece2eb1e24f1fcacbc6ed",
    "semantic_title": "learning to schedule learning rate with graph neural networks",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=c-4HSDAWua5": {
    "title": "SketchODE: Learning neural sketch representation in continuous time",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "726e069bfa66dac36c5abc12b6237a88ca777f60",
    "semantic_title": "sketchode: learning neural sketch representation in continuous time",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=HFPTzdwN39": {
    "title": "Measuring the Interpretability of Unsupervised Representations via Quantized Reversed Probing",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a8196d00d8b956d7d67b7ec1723f2d3e7b8eb6f3",
    "semantic_title": "measuring the interpretability of unsupervised representations via quantized reversed probing",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=qjN4h_wwUO": {
    "title": "GradMax: Growing Neural Networks using Gradient Information",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "eeb29f35b1a4e2cbe2c76e6c63df858194ed5d5d",
    "semantic_title": "gradmax: growing neural networks using gradient information",
    "citation_count": 60,
    "authors": []
  },
  "https://openreview.net/forum?id=f9D-5WNG4Nv": {
    "title": "Online Coreset Selection for Rehearsal-based Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8436293cd98a3df769151ac8d7c81e0fe9d8d813",
    "semantic_title": "online coreset selection for rehearsal-based continual learning",
    "citation_count": 158,
    "authors": []
  },
  "https://openreview.net/forum?id=H-iABMvzIc": {
    "title": "Switch to Generalize: Domain-Switch Learning for Cross-Domain Few-Shot Classification",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "a01ccf2e3ca3a694e75a2fedc5f9d9d83db06db6",
    "semantic_title": "switch to generalize: domain-switch learning for cross-domain few-shot classification",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=RAW9tCdVxLj": {
    "title": "Zero-CL: Instance and Feature decorrelation for negative-free symmetric contrastive learning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bbcbe9a52010f06cf3e037d2d304fb20339a2e01",
    "semantic_title": "zero-cl: instance and feature decorrelation for negative-free symmetric contrastive learning",
    "citation_count": 47,
    "authors": []
  },
  "https://openreview.net/forum?id=qwULHx9zld": {
    "title": "Random matrices in service of ML footprint: ternary random features with no performance loss",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "eb1164159adfcfe421587fdcf01a946f4da1c0ff",
    "semantic_title": "random matrices in service of ml footprint: ternary random features with no performance loss",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=gfwON7rAm4": {
    "title": "Global Convergence of Multi-Agent Policy Gradient in Markov Potential Games",
    "volume": "poster",
    "abstract": "Potential games are arguably one of the most important and widely studied classes of normal form games. They define the archetypal setting of multi-agent coordination in which all agents utilities are perfectly aligned via a common potential function. Can this intuitive framework be transplanted in the setting of Markov games? What are the similarities and differences between multi-agent coordination with and without state dependence? To answer these questions, we study a natural class of Markov Potential Games (MPGs) that generalize prior attempts at capturing complex stateful multi-agent coordination. Counter-intuitively, insights from normal-form potential games do not carry over as MPGs involve settings where state-games can be zero-sum games. In the opposite direction, Markov games where every state-game is a potential game are not necessarily MPGs. Nevertheless, MPGs showcase standard desirable properties such as the existence of deterministic Nash policies. In our main technical result, we prove convergence of independent policy gradient and its stochastic counterpart to Nash policies (polynomially fast in the approximation error) by adapting recent gradient dominance property arguments developed for single-agent Markov decision processes to multi-agent learning settings",
    "checked": true,
    "id": "766f25e1e6adb4d76403e2a59733257b77d5db12",
    "semantic_title": "global convergence of multi-agent policy gradient in markov potential games",
    "citation_count": 130,
    "authors": []
  },
  "https://openreview.net/forum?id=gVRhIEajG1k": {
    "title": "Rethinking Adversarial Transferability from a Data Distribution Perspective",
    "volume": "poster",
    "abstract": "Adversarial transferability enables attackers to generate adversarial examples from the source model to attack the target model, which has raised security concerns about the deployment of DNNs in practice. In this paper, we rethink adversarial transferability from a data distribution perspective and further enhance transferability by score matching based optimization. We identify that some samples with injecting small Gaussian noise can fool different target models, and their adversarial examples under different source models have much stronger transferability. We hypothesize that these samples are in the low-density region of the ground truth distribution where models are not well trained. To improve the attack success rate of adversarial examples, we match the adversarial attacks with the directions which effectively decrease the ground truth density. We propose Intrinsic Adversarial Attack (IAA), which smooths the activation function and decreases the impact of the later layers of a given normal model, to increase the alignment of adversarial attack and the gradient of joint data distribution. We conduct comprehensive transferable attacks against multiple DNNs and show that our IAA can boost the transferability of the crafted attacks in all cases and go beyond state-of-the-art methods",
    "checked": true,
    "id": "d2a7640459ac5fb1809089f8a98142d75357d6b9",
    "semantic_title": "rethinking adversarial transferability from a data distribution perspective",
    "citation_count": 43,
    "authors": []
  },
  "https://openreview.net/forum?id=KSugKcbNf9": {
    "title": "Transformers Can Do Bayesian Inference",
    "volume": "poster",
    "abstract": "Currently, it is hard to reap the benefits of deep learning for Bayesian methods, which allow the explicit specification of prior knowledge and accurately capture model uncertainty. We present Prior-Data Fitted Networks (PFNs). PFNs leverage large-scale machine learning techniques to approximate a large set of posteriors. The only requirement for PFNs to work is the ability to sample from a prior distribution over supervised learning tasks (or functions). Our method restates the objective of posterior approximation as a supervised classification problem with a set-valued input: it repeatedly draws a task (or function) from the prior, draws a set of data points and their labels from it, masks one of the labels and learns to make probabilistic predictions for it based on the set-valued input of the rest of the data points. Presented with a set of samples from a new supervised learning task as input, PFNs make probabilistic predictions for arbitrary other data points in a single forward propagation, having learned to approximate Bayesian inference. We demonstrate that PFNs can near-perfectly mimic Gaussian processes and also enable efficient Bayesian inference for intractable problems, with over 200-fold speedups in multiple setups compared to current methods. We obtain strong results in very diverse areas such as Gaussian process regression, Bayesian neural networks, classification for small tabular data sets, and few-shot image classification, demonstrating the generality of PFNs. Code and trained PFNs are released at https://github.com/automl/TransformersCanDoBayesianInference",
    "checked": true,
    "id": "d88a5ae1673f2009704186acf2890163e6ddf4ca",
    "semantic_title": "transformers can do bayesian inference",
    "citation_count": 202,
    "authors": []
  },
  "https://openreview.net/forum?id=JJCjv4dAbyL": {
    "title": "Learning Discrete Structured Variational Auto-Encoder using Natural Evolution Strategies",
    "volume": "poster",
    "abstract": "Discrete variational auto-encoders (VAEs) are able to represent semantic latent spaces in generative learning. In many real-life settings, the discrete latent space consists of high-dimensional structures, and propagating gradients through the relevant structures often requires enumerating over an exponentially large latent space. Recently, various approaches were devised to propagate approximated gradients without enumerating over the space of possible structures. In this work, we use Natural Evolution Strategies (NES), a class of gradient-free black-box optimization algorithms, to learn discrete structured VAEs. The NES algorithms are computationally appealing as they estimate gradients with forward pass evaluations only, thus they do not require to propagate gradients through their discrete structures. We demonstrate empirically that optimizing discrete structured VAEs using NES is as effective as gradient-based approximations. Lastly, we prove NES converges for non-Lipschitz functions as appear in discrete structured VAEs",
    "checked": true,
    "id": "762911860a91b5fb24f16a7ca52de3c86d563567",
    "semantic_title": "learning discrete structured variational auto-encoder using natural evolution strategies",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=bCrdi4iVvv": {
    "title": "Learning Features with Parameter-Free Layers",
    "volume": "poster",
    "abstract": "Trainable layers such as convolutional building blocks are the standard network design choices by learning parameters to capture the global context through successive spatial operations. When designing an efficient network, trainable layers such as the depthwise convolution is the source of efficiency in the number of parameters and FLOPs, but there was little improvement to the model speed in practice. This paper argues that simple built-in parameter-free operations can be a favorable alternative to the efficient trainable layers replacing spatial operations in a network architecture. We aim to break the stereotype of organizing the spatial operations of building blocks into trainable layers. Extensive experimental analyses based on layer-level studies with fully-trained models and neural architecture searches are provided to investigate whether parameter-free operations such as the max-pool are functional. The studies eventually give us a simple yet effective idea for redesigning network architectures, where the parameter-free operations are heavily used as the main building block without sacrificing the model accuracy as much. Experimental results on the ImageNet dataset demonstrate that the network architectures with parameter-free operations could enjoy the advantages of further efficiency in terms of model speed, the number of the parameters, and FLOPs. Code and ImageNet pretrained models are available at https://github.com/naver-ai/PfLayer",
    "checked": true,
    "id": "237776a7902f275c34b1ac389cfc38e974916fea",
    "semantic_title": "learning features with parameter-free layers",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=LcF-EEt8cCC": {
    "title": "Denoising Likelihood Score Matching for Conditional Score-based Data Generation",
    "volume": "poster",
    "abstract": "Many existing conditional score-based data generation methods utilize Bayes' theorem to decompose the gradients of a log posterior density into a mixture of scores. These methods facilitate the training procedure of conditional score models, as a mixture of scores can be separately estimated using a score model and a classifier. However, our analysis indicates that the training objectives for the classifier in these methods may lead to a serious score mismatch issue, which corresponds to the situation that the estimated scores deviate from the true ones. Such an issue causes the samples to be misled by the deviated scores during the diffusion process, resulting in a degraded sampling quality. To resolve it, we theoretically formulate a novel training objective, called Denoising Likelihood Score Matching (DLSM) loss, for the classifier to match the gradients of the true log likelihood density. Our experimental evidences show that the proposed method outperforms the previous methods on both Cifar-10 and Cifar-100 benchmarks noticeably in terms of several key evaluation metrics. We thus conclude that, by adopting DLSM, the conditional scores can be accurately modeled, and the effect of the score mismatch issue is alleviated",
    "checked": true,
    "id": "183aec8376b39ae2a1707a436266cddaf8f05596",
    "semantic_title": "denoising likelihood score matching for conditional score-based data generation",
    "citation_count": 45,
    "authors": []
  },
  "https://openreview.net/forum?id=a7H7OucbWaU": {
    "title": "Memory Replay with Data Compression for Continual Learning",
    "volume": "poster",
    "abstract": "Continual learning needs to overcome catastrophic forgetting of the past. Memory replay of representative old training samples has been shown as an effective solution, and achieves the state-of-the-art (SOTA) performance. However, existing work is mainly built on a small memory buffer containing a few original data, which cannot fully characterize the old data distribution. In this work, we propose memory replay with data compression to reduce the storage cost of old training samples and thus increase their amount that can be stored in the memory buffer. Observing that the trade-off between the quality and quantity of compressed data is highly nontrivial for the efficacy of memory replay, we propose a novel method based on determinantal point processes (DPPs) to efficiently determine an appropriate compression quality for currently-arrived training samples. In this way, using a naive data compression algorithm with a properly selected quality can largely boost recent strong baselines by saving more compressed data in a limited storage space. We extensively validate this across several benchmarks of class-incremental learning and in a realistic scenario of object detection for autonomous driving",
    "checked": true,
    "id": "45d49272a3ec51b61ff88af04b38dbce29b05dfc",
    "semantic_title": "memory replay with data compression for continual learning",
    "citation_count": 113,
    "authors": []
  },
  "https://openreview.net/forum?id=LDAwu17QaJz": {
    "title": "MAML is a Noisy Contrastive Learner in Classification",
    "volume": "poster",
    "abstract": "Model-agnostic meta-learning (MAML) is one of the most popular and widely adopted meta-learning algorithms, achieving remarkable success in various learning problems. Yet, with the unique design of nested inner-loop and outer-loop updates, which govern the task-specific and meta-model-centric learning, respectively, the underlying learning objective of MAML remains implicit, impeding a more straightforward understanding of it. In this paper, we provide a new perspective of the working mechanism of MAML. We discover that MAML is analogous to a meta-learner using a supervised contrastive objective in classification. The query features are pulled towards the support features of the same class and against those of different classes. Such contrastiveness is experimentally verified via an analysis based on the cosine similarity. Moreover, we reveal that vanilla MAML has an undesirable interference term originating from the random initialization and the cross-task interaction. We thus propose a simple but effective technique, the zeroing trick, to alleviate the interference. Extensive experiments are conducted on both mini-ImageNet and Omniglot datasets to validate the consistent improvement brought by our proposed method",
    "checked": true,
    "id": "73e1b848711c304dbf16ed6f390d0d765ca1c7e1",
    "semantic_title": "maml is a noisy contrastive learner in classification",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=afoV8W3-IYp": {
    "title": "RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning",
    "volume": "poster",
    "abstract": "Reasoning about visual relationships is central to how humans interpret the visual world. This task remains challenging for current deep learning algorithms since it requires addressing three key technical problems jointly: 1) identifying object entities and their properties, 2) inferring semantic relations between pairs of entities, and 3) generalizing to novel object-relation combinations, i.e., systematic generalization. In this work, we use vision transformers (ViTs) as our base model for visual reasoning and make better use of concepts defined as object entities and their relations to improve the reasoning ability of ViTs. Specifically, we introduce a novel concept-feature dictionary to allow flexible image feature retrieval at training time with concept keys. This dictionary enables two new concept-guided auxiliary tasks: 1) a global task for promoting relational reasoning, and 2) a local task for facilitating semantic object-centric correspondence learning. To examine the systematic generalization of visual reasoning models, we introduce systematic splits for the standard HICO and GQA benchmarks. We show the resulting model, Concept-guided Vision Transformer (or RelViT for short) significantly outperforms prior approaches on HICO and GQA by 16% and 13% in the original split, and by 43% and 18% in the systematic split. Our ablation analyses also reveal our model's compatibility with multiple ViT variants and robustness to hyper-parameters",
    "checked": true,
    "id": "2e69d97ae22c6a3685cc548f8c19c696d5d7d363",
    "semantic_title": "relvit: concept-guided vision transformer for visual relational reasoning",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=anbBFlX1tJ1": {
    "title": "Boosted Curriculum Reinforcement Learning",
    "volume": "poster",
    "abstract": "Curriculum value-based reinforcement learning (RL) solves a complex target task by reusing action-values across a tailored sequence of related tasks of increasing difficulty. However, finding an exact way of reusing action-values in this setting is still a poorly understood problem. In this paper, we introduce the concept of boosting to curriculum value-based RL, by approximating the action-value function as a sum of residuals trained on each task. This approach, which we refer to as boosted curriculum reinforcement learning (BCRL), has the benefit of naturally increasing the representativeness of the functional space by adding a new residual each time a new task is presented. This procedure allows reusing previous action-values while promoting expressiveness of the action-value function. We theoretically study BCRL as an approximate value iteration algorithm, discussing advantages over regular curriculum RL in terms of approximation accuracy and convergence to the optimal action-value function. Finally, we provide detailed empirical evidence of the benefits of BCRL in problems requiring curricula for accurate action-value estimation and targeted exploration",
    "checked": true,
    "id": "1909317ccf9d01052bbe9eafbbe06380848effb8",
    "semantic_title": "boosted curriculum reinforcement learning",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=w4cXZDDib1H": {
    "title": "ViDT: An Efficient and Effective Fully Transformer-based Object Detector",
    "volume": "poster",
    "abstract": "Transformers are transforming the landscape of computer vision, especially for recognition tasks. Detection transformers are the first fully end-to-end learning systems for object detection, while vision transformers are the first fully transformer-based architecture for image classification. In this paper, we integrate Vision and Detection Transformers (ViDT) to build an effective and efficient object detector. ViDT introduces a reconfigured attention module to extend the recent Swin Transformer to be a standalone object detector, followed by a computationally efficient transformer decoder that exploits multi-scale features and auxiliary techniques essential to boost the detection performance without much increase in computational load. Extensive evaluation results on the Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP and latency trade-off among existing fully transformer-based object detectors, and achieves 49.2AP owing to its high scalability for large models. We release the code and trained models at https://github.com/naver-ai/vidt",
    "checked": true,
    "id": "38c493ed7121ec136516b9a93f76e4c680999a93",
    "semantic_title": "vidt: an efficient and effective fully transformer-based object detector",
    "citation_count": 85,
    "authors": []
  },
  "https://openreview.net/forum?id=5xEgrl_5FAJ": {
    "title": "BiBERT: Accurate Fully Binarized BERT",
    "volume": "poster",
    "abstract": "The large pre-trained BERT has achieved remarkable performance on Natural Language Processing (NLP) tasks but is also computation and memory expensive. As one of the powerful compression approaches, binarization extremely reduces the computation and memory consumption by utilizing 1-bit parameters and bitwise operations. Unfortunately, the full binarization of BERT (i.e., 1-bit weight, embedding, and activation) usually suffer a significant performance drop, and there is rare study addressing this problem. In this paper, with the theoretical justification and empirical analysis, we identify that the severe performance drop can be mainly attributed to the information degradation and optimization direction mismatch respectively in the forward and backward propagation, and propose BiBERT, an accurate fully binarized BERT, to eliminate the performance bottlenecks. Specifically, BiBERT introduces an efficient Bi-Attention structure for maximizing representation information statistically and a Direction-Matching Distillation (DMD) scheme to optimize the full binarized BERT accurately. Extensive experiments show that BiBERT outperforms both the straightforward baseline and existing state-of-the-art quantized BERTs with ultra-low bit activations by convincing margins on the NLP benchmark. As the first fully binarized BERT, our method yields impressive 56.3 times and 31.2 times saving on FLOPs and model size, demonstrating the vast advantages and potential of the fully binarized BERT model in real-world resource-constrained scenarios",
    "checked": true,
    "id": "fb2307f7ce7c6868429ee3ee15d6eaf311ecba5c",
    "semantic_title": "bibert: accurate fully binarized bert",
    "citation_count": 107,
    "authors": []
  },
  "https://openreview.net/forum?id=tBIQEvApZK5": {
    "title": "Feature Kernel Distillation",
    "volume": "poster",
    "abstract": "Trained Neural Networks (NNs) can be viewed as data-dependent kernel machines, with predictions determined by the inner product of last-layer representations across inputs, referred to as the feature kernel. We explore the relevance of the feature kernel for Knowledge Distillation (KD), using a mechanistic understanding of an NN's optimisation process. We extend the theoretical analysis of Allen-Zhu & Li (2020) to show that a trained NN's feature kernel is highly dependent on its parameter initialisation, which biases different initialisations of the same architecture to learn different data attributes in a multi-view data setting. This enables us to prove that KD using only pairwise feature kernel comparisons can improve NN test accuracy in such settings, with both single & ensemble teacher models, whereas standard training without KD fails to generalise. We further use our theory to motivate practical considerations for improving student generalisation when using distillation with feature kernels, which allows us to propose a novel approach: Feature Kernel Distillation (FKD). Finally, we experimentally corroborate our theory in the image classification setting, showing that FKD is amenable to ensemble distillation, can transfer knowledge across datasets, and outperforms both vanilla KD & other feature kernel based KD baselines across a range of standard architectures & datasets",
    "checked": true,
    "id": "1c5ac60e4af26048812bf01e45a4e71df07d3a87",
    "semantic_title": "feature kernel distillation",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=-ngwPqanCEZ": {
    "title": "Representation-Agnostic Shape Fields",
    "volume": "poster",
    "abstract": "3D shape analysis has been widely explored in the era of deep learning. Numerous models have been developed for various 3D data representation formats, e.g., MeshCNN for meshes, PointNet for point clouds and VoxNet for voxels. In this study, we present Representation-Agnostic Shape Fields (RASF), a generalizable and computation-efficient shape embedding module for 3D deep learning. RASF is implemented with a learnable 3D grid with multiple channels to store local geometry. Based on RASF, shape embeddings for various 3D shape representations (point clouds, meshes and voxels) are retrieved by coordinate indexing. While there are multiple ways to optimize the learnable parameters of RASF, we provide two effective schemes among all in this paper for RASF pre-training: shape reconstruction and normal estimation. Once trained, RASF becomes a plug-and-play performance booster with negligible cost. Extensive experiments on diverse 3D representation formats, networks and applications, validate the universal effectiveness of the proposed RASF. Code and pre-trained models are publicly available\\footnote{\\url{https://github.com/seanywang0408/RASF}}",
    "checked": true,
    "id": "27910a4e2da98c11f639259e6d8d61f122513756",
    "semantic_title": "representation-agnostic shape fields",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=C1_esHN6AVn": {
    "title": "Learning Synthetic Environments and Reward Networks for Reinforcement Learning",
    "volume": "poster",
    "abstract": "We introduce Synthetic Environments (SEs) and Reward Networks (RNs), represented by neural networks, as proxy environment models for training Reinforcement Learning (RL) agents. We show that an agent, after being trained exclusively on the SE, is able to solve the corresponding real environment. While an SE acts as a full proxy to a real environment by learning about its state dynamics and rewards, an RN is a partial proxy that learns to augment or replace rewards. We use bi-level optimization to evolve SEs and RNs: the inner loop trains the RL agent, and the outer loop trains the parameters of the SE / RN via an evolution strategy. We evaluate our proposed new concept on a broad range of RL algorithms and classic control environments. In a one-to-one comparison, learning an SE proxy requires more interactions with the real environment than training agents only on the real environment. However, once such an SE has been learned, we do not need any interactions with the real environment to train new agents. Moreover, the learned SE proxies allow us to train agents with fewer interactions while maintaining the original task performance. Our empirical results suggest that SEs achieve this result by learning informed representations that bias the agents towards relevant states. Moreover, we find that these proxies are robust against hyperparameter variation and can also transfer to unseen agents",
    "checked": true,
    "id": "0c529ba981b90b2d1536d342bf92a7451686164a",
    "semantic_title": "learning synthetic environments and reward networks for reinforcement learning",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=NH29920YEmj": {
    "title": "Who Is Your Right Mixup Partner in Positive and Unlabeled Learning",
    "volume": "poster",
    "abstract": "Positive and Unlabeled (PU) learning targets inducing a binary classifier from weak training datasets of positive and unlabeled instances, which arise in many real-world applications. In this paper, we propose a novel PU learning method, namely Positive and unlabeled learning with Partially Positive Mixup (P3Mix), which simultaneously benefits from data augmentation and supervision correction with a heuristic mixup technique. To be specific, we take inspiration from the directional boundary deviation phenomenon observed in our preliminary experiments, where the learned PU boundary tends to deviate from the fully supervised boundary towards the positive side. For the unlabeled instances with ambiguous predictive results, we select their mixup partners from the positive instances around the learned PU boundary, so as to transform them into augmented instances near to the boundary yet with more precise supervision. Accordingly, those augmented instances may push the learned PU boundary towards the fully supervised boundary, thereby improving the classification performance. Comprehensive experimental results demonstrate the effectiveness of the heuristic mixup technique in PU learning and show that P3Mix can consistently outperform the state-of-the-art PU learning methods",
    "checked": true,
    "id": "bd30aa7245e49c49e3cd8f1bd62a8a0cadfe8aeb",
    "semantic_title": "who is your right mixup partner in positive and unlabeled learning",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=dDjSKKA5TP1": {
    "title": "Incremental False Negative Detection for Contrastive Learning",
    "volume": "poster",
    "abstract": "Self-supervised learning has recently shown great potential in vision tasks through contrastive learning, which aims to discriminate each image, or instance, in the dataset. However, such instance-level learning ignores the semantic relationship among instances and sometimes undesirably repels the anchor from the semantically similar samples, termed as \"false negatives\". In this work, we show that the unfavorable effect from false negatives is more significant for the large-scale datasets with more semantic concepts. To address the issue, we propose a novel self-supervised contrastive learning framework that incrementally detects and explicitly removes the false negative samples. Specifically, following the training process, our method dynamically detects increasing high-quality false negatives considering that the encoder gradually improves and the embedding space becomes more semantically structural. Next, we discuss two strategies to explicitly remove the detected false negatives during contrastive learning. Extensive experiments show that our framework outperforms other self-supervised contrastive learning methods on multiple benchmarks in a limited resource setup",
    "checked": true,
    "id": "7c1d2b33480cfb6da5b2573a8f8d113dabc39cfe",
    "semantic_title": "incremental false negative detection for contrastive learning",
    "citation_count": 61,
    "authors": []
  },
  "https://openreview.net/forum?id=rJvY_5OzoI": {
    "title": "Multi-Critic Actor Learning: Teaching RL Policies to Act with Style",
    "volume": "poster",
    "abstract": "Using a single value function (critic) shared over multiple tasks in Actor-Critic multi-task reinforcement learning (MTRL) can result in negative interference between tasks, which can compromise learning performance. Multi-Critic Actor Learning (MultiCriticAL) proposes instead maintaining separate critics for each task being trained while training a single multi-task actor. Explicitly distinguishing between tasks also eliminates the need for critics to learn to do so and mitigates interference between task-value estimates. MultiCriticAL is tested in the context of multi-style learning, a special case of MTRL where agents are trained to behave with different distinct behavior styles, and yields up to 56% performance gains over the single-critic baselines and even successfully learns behavior styles in cases where single-critic approaches may simply fail to learn. In a simulated real-world use case, MultiCriticAL enables learning policies that smoothly transition between multiple fighting styles on an experimental build of EA's UFC game",
    "checked": true,
    "id": "753d10c000e0df1c25ba20833d48b0d60d148467",
    "semantic_title": "multi-critic actor learning: teaching rl policies to act with style",
    "citation_count": 27,
    "authors": []
  },
  "https://openreview.net/forum?id=kezNJydWvE": {
    "title": "Clean Images are Hard to Reblur: Exploiting the Ill-Posed Inverse Task for Dynamic Scene Deblurring",
    "volume": "poster",
    "abstract": "The goal of dynamic scene deblurring is to remove the motion blur in a given image. Typical learning-based approaches implement their solutions by minimizing the L1 or L2 distance between the output and the reference sharp image. Recent attempts adopt visual recognition features in training to improve the perceptual quality. However, those features are primarily designed to capture high-level contexts rather than low-level structures such as blurriness. Instead, we propose a more direct way to make images sharper by exploiting the inverse task of deblurring, namely, reblurring. Reblurring amplifies the remaining blur to rebuild the original blur, however, a well-deblurred clean image with zero-magnitude blur is hard to reblur. Thus, we design two types of reblurring loss functions for better deblurring. The supervised reblurring loss at training stage compares the amplified blur between the deblurred and the sharp images. The self-supervised reblurring loss at inference stage inspects if noticeable blur remains in the deblurred. Our experimental results on large-scale benchmarks and real images demonstrate the effectiveness of the reblurring losses in improving the perceptual quality of the deblurred images in terms of NIQE and LPIPS scores as well as visual sharpness",
    "checked": true,
    "id": "9a531d3f6eee43b93349bf1f1e7f8c2c3b6def61",
    "semantic_title": "clean images are hard to reblur: exploiting the ill-posed inverse task for dynamic scene deblurring",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=j-63FSNcO5a": {
    "title": "Learning Disentangled Representation by Exploiting Pretrained Generative Models: A Contrastive Learning View",
    "volume": "poster",
    "abstract": "From the intuitive notion of disentanglement, the image variations corresponding to different generative factors should be distinct from each other, and the disentangled representation should reflect those variations with separate dimensions. To discover the generative factors and learn disentangled representation, previous methods typically leverage an extra regularization term when learning to generate realistic images. However, the term usually results in a trade-off between disentanglement and generation quality. For the generative models pretrained without any disentanglement term, the generated images show semantically meaningful variations when traversing along different directions in the latent space. Based on this observation, we argue that it is possible to mitigate the trade-off by (i) leveraging the pretrained generative models with high generation quality, (ii) focusing on discovering the traversal directions as generative factors for disentangled representation learning. To achieve this, we propose Disentaglement via Contrast (DisCo) as a framework to model the variations based on the target disentangled representations, and contrast the variations to jointly discover disentangled directions and learn disentangled representations. DisCo achieves the state-of-the-art disentangled representation learning and distinct direction discovering, given pretrained non-disentangled generative models including GAN, VAE, and Flow. Source code is at https://github.com/xrenaa/DisCo",
    "checked": true,
    "id": "77064160e80b260d044c91b44134df4de08e4c38",
    "semantic_title": "learning disentangled representation by exploiting pretrained generative models: a contrastive learning view",
    "citation_count": 42,
    "authors": []
  },
  "https://openreview.net/forum?id=YgPqNctmyd": {
    "title": "Towards Building A Group-based Unsupervised Representation Disentanglement Framework",
    "volume": "poster",
    "abstract": "Disentangled representation learning is one of the major goals of deep learning, and is a key step for achieving explainable and generalizable models. The key idea of the state-of-the-art VAE-based unsupervised representation disentanglement methods is to minimize the total correlation of the joint distribution of the latent variables. However, it has been proved that their goal can not be achieved without introducing other inductive biases. The Group Theory based definition of representation disentanglement mathematically connects the data transformations to the representations using the formalism of group. In this paper, built on the group-based definition and inspired by the \\emph{n-th dihedral group}, we first propose a theoretical framework towards achieving unsupervised representation disentanglement. We then propose a model based on existing VAE-based methods to tackle the unsupervised learning problem of the framework. In the theoretical framework, we prove three sufficient conditions on model, group structure, and data respectively in an effort to achieve, in an unsupervised way, disentangled representation per group-based definition. With these conditions, we offer an option, from the perspective of the group-based definition, for the inductive bias that existing VAE-based models lack. Experimentally, we train 1800 models covering the most prominent VAE-based methods on five datasets to verify the effectiveness of our theoretical framework. Compared to the original VAE-based methods, these Groupified VAEs consistently achieve better mean performance with smaller variances",
    "checked": true,
    "id": "bd71fc4689d265c54506ce6ca352363e926bba0b",
    "semantic_title": "towards building a group-based unsupervised representation disentanglement framework",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=AjGC97Aofee": {
    "title": "Learning Efficient Image Super-Resolution Networks via Structure-Regularized Pruning",
    "volume": "poster",
    "abstract": "Several image super-resolution (SR) networks have been proposed of late for efficient SR, achieving promising results. However, they are still not lightweight enough and neglect to be extended to larger networks. At the same time, model compression techniques, like neural architecture search and knowledge distillation, typically consume considerable computation resources. In contrast, network pruning is a cheap and effective model compression technique. However, it is hard to be applied to SR networks directly because filter pruning for residual blocks is well-known tricky. To address the above issues, we propose structure-regularized pruning (SRP), which imposes regularization on the pruned structure to ensure the locations of pruned filters are aligned across different layers. Specifically, for the layers connected by the same residual, we select the filters of the same indices as unimportant filters. To transfer the expressive power in the unimportant filters to the rest of the network, we employ $L_2$ regularization to drive the weights towards zero so that eventually, their absence will cause minimal performance degradation. We apply SRP to train efficient image SR networks, resulting in a lightweight network SRPN-Lite and a very deep one SRPN. We conduct extensive comparisons with both lightweight and larger networks. SRPN-Lite and SRPN perform favorably against other recent efficient SR approaches quantitatively and visually",
    "checked": true,
    "id": "c7e110d392a19387ac8cda76343e256ce6add4ce",
    "semantic_title": "learning efficient image super-resolution networks via structure-regularized pruning",
    "citation_count": 45,
    "authors": []
  }
}