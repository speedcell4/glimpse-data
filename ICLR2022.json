{
  "https://openreview.net/forum?id=wQfgfb8VKTn": {
    "title": "Context-Aware Sparse Deep Coordination Graphs",
    "abstract": "3 × 5 matrix. Sensors can scan the eight nearby points. Each scan induces a cost of -1, and agents can do noop to save the cost. Three targets wander randomly in the gird. If k ≥ 2 sensors scan a target simultaneously, the system gets a constant reward of , which is independent of the number of sensors. Agents can observe the id and position of targets nearby",
    "volume": "main",
    "checked": true,
    "id": "1f259952671f58d08822327cf7aa5ccb0a7d70af",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=_5js_8uTrx1": {
    "title": "Towards Evaluating the Robustness of Neural Networks Learned by Transduction",
    "abstract": "There has been emerging interest in using transductive learning for adversarial robustness (Goldwasser et al., NeurIPS 2020; Wu et al., ICML 2020; Wang et al., ArXiv 2021). Compared to traditional defenses, these defense mechanisms \"dynamically learn\" the model based on test-time input; and theoretically, attacking these defenses reduces to solving a bilevel optimization problem, which poses difficulty in crafting adaptive attacks. In this paper, we examine these defense mechanisms from a principled threat analysis perspective. We formulate and analyze threat models for transductive-learning based defenses, and point out important subtleties. We propose the principle of attacking model space for solving bilevel attack objectives, and present Greedy Model Space Attack (GMSA), an attack framework that can serve as a new baseline for evaluating transductivelearning based defenses. Through systematic evaluation, we show that GMSA, even with weak instantiations, can break previous transductive-learning based defenses, which were resilient to previous attacks, such as AutoAttack. On the positive side, we report a somewhat surprising empirical result of \"transductive adversarial training\": Adversarially retraining the model using fresh randomness at the test time gives a significant increase in robustness against attacks we consider",
    "volume": "main",
    "checked": true,
    "id": "90fe9785e1ff58c1d7aa22319009e0d0e3077d29",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=XhF2VOMRHS": {
    "title": "A Unified Contrastive Energy-based Model for Understanding the Generative Ability of Adversarial Training",
    "abstract": "robustness to verify our probabilistic understandings of AT. We show adversarial training objectives derived from our",
    "volume": "main",
    "checked": true,
    "id": "2acee01e0614155ee2537535d88dc8577a4eda73",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=41e9o6cQPj": {
    "title": "GreaseLM: Graph REASoning Enhanced Language Models",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "e3298ca96b207b4de80f4fc17b3f3a1a766e9590",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=mhYUBYNoGz": {
    "title": "Machine Learning For Elliptic PDEs: Fast Rate Generalization Bound, Neural Scaling Law and Minimax Optimality",
    "abstract": "In this paper, we study the statistical limits of deep learning techniques for solving elliptic partial differential equations (PDEs) from random samples using the Deep Ritz Method (DRM) and Physics-Informed Neural Networks (PINNs). To simplify the problem, we focus on a prototype elliptic PDE: the Schr\\\"odinger equation on a hypercube with zero Dirichlet boundary condition, which has wide application in the quantum-mechanical systems. We establish upper and lower bounds for both methods, which improves upon concurrently developed upper bounds for this problem via a fast rate generalization bound. We discover that the current Deep Ritz Methods is sub-optimal and propose a modified version of it. We also prove that PINN and the modified version of DRM can achieve minimax optimal bounds over Sobolev spaces. Empirically, following recent work which has shown that the deep model accuracy will improve with growing training sets according to a power law, we supply computational experiments to show a similar behavior of dimension dependent power law for deep PDE solvers",
    "volume": "main",
    "checked": true,
    "id": "16bf2241414f01bd3c98316cc75ef733fb6a1980",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=auOPcdAcoy": {
    "title": "Hybrid Memoised Wake-Sleep: Approximate Inference at the Discrete-Continuous Interface",
    "abstract": "Modeling complex phenomena typically involves the use of both discrete and continuous variables. Such a setting applies across a wide range of problems, from identifying trends in time-series data to performing effective compositional scene understanding in images. Here, we propose Hybrid Memoised Wake-Sleep (HMWS), an algorithm for effective inference in such hybrid discrete-continuous models. Prior approaches to learning suffer as they need to perform repeated expensive inner-loop discrete inference. We build on a recent approach, Memoised Wake-Sleep (MWS), which alleviates part of the problem by memoising discrete variables, and extend it to allow for a principled and effective way to handle continuous variables by learning a separate recognition model used for importance-sampling based approximate inference and marginalization. We evaluate HMWS in the GP-kernel learning and 3D scene understanding domains, and show that it outperforms current state-of-the-art inference methods",
    "volume": "main",
    "checked": true,
    "id": "5a533bce8c685b67d5279ed72e25835520177727",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=xMJWUKJnFSw": {
    "title": "NodePiece: Compositional and Parameter-Efficient Representations of Large Knowledge Graphs",
    "abstract": "Conventional representation learning algorithms for knowledge graphs (KG) map each entity to a unique embedding vector. Such a shallow lookup results in a linear growth of memory consumption for storing the embedding matrix and incurs high computational costs when working with real-world KGs. Drawing parallels with subword tokenization commonly used in NLP, we explore the landscape of more parameter-efficient node embedding strategies with possibly sublinear memory requirements. To this end, we propose NodePiece, an anchor-based approach to learn a fixed-size entity vocabulary. In NodePiece, a vocabulary of subword/subentity units is constructed from anchor nodes in a graph with known relation types. Given such a fixed-size vocabulary, it is possible to bootstrap an encoding and embedding for any entity, including those unseen during training. Experiments show that NodePiece performs competitively in node classification, link prediction, and relation prediction tasks while retaining less than 10% of explicit nodes in a graph as anchors and often having 10x fewer parameters",
    "volume": "main",
    "checked": true,
    "id": "f6c4d57c710b461ece658a0b8e7427e862fef116",
    "citation_count": 17
  },
  "https://openreview.net/forum?id=SHbhHHfePhP": {
    "title": "Equivariant Graph Mechanics Networks with Constraints",
    "abstract": "Learning to reason about relations and dynamics over multiple interacting objects is a challenging topic in machine learning. The challenges mainly stem from that the interacting systems are exponentially-compositional, symmetrical, and commonly geometrically-constrained. Current methods, particularly the ones based on equivariant Graph Neural Networks (GNNs), have targeted on the first two challenges but remain immature for constrained systems. In this paper, we propose Graph Mechanics Network (GMN) which is combinatorially efficient, equivariant and constraint-aware. The core of GMN is that it represents, by generalized coordinates, the forward kinematics information (positions and velocities) of a structural object. In this manner, the geometrical constraints are implicitly and naturally encoded in the forward kinematics. Moreover, to allow equivariant message passing in GMN, we have developed a general form of orthogonality-equivariant functions, given that the dynamics of constrained systems are more complicated than the unconstrained counterparts. Theoretically, the proposed equivariant formulation is proved to be universally expressive under certain conditions. Extensive experiments support the advantages of GMN compared to the state-of-the-art GNNs in terms of prediction accuracy, constraint satisfaction and data efficiency on the simulated systems consisting of particles, sticks and hinges, as well as two real-world datasets for molecular dynamics prediction and human motion capture",
    "volume": "main",
    "checked": true,
    "id": "1407ea6d9ada333e9fc8de9416b203d58e9d9282",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=y0VvIg25yk": {
    "title": "On the Learning and Learnability of Quasimetrics",
    "abstract": "Our world is full of asymmetries. Gravity and wind can make reaching a place easier than coming back. Social artifacts such as genealogy charts and citation graphs are inherently directed. In reinforcement learning and control, optimal goal-reaching strategies are rarely reversible (symmetrical). Distance functions supported on these asymmetrical structures are called quasimetrics. Despite their common appearance, little research has been done on the learning of quasimetrics. Our theoretical analysis reveals that a common class of learning algorithms, including unconstrained multilayer perceptrons (MLPs), provably fails to learn a quasimetric consistent with training data. In contrast, our proposed Poisson Quasimetric Embedding (PQE) is the first quasimetric learning formulation that both is learnable with gradient-based optimization and enjoys strong performance guarantees. Experiments on random graphs, social graphs, and offline Q-learning demonstrate its effectiveness over many common baselines. Project Page: ssnl.github.io/quasimetric. Code: github.com/SsnL/poisson_quasimetric_embedding",
    "volume": "main",
    "checked": true,
    "id": "0cd1955bb1949c232c4d15a3c4c94a77a95a3f7d",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=AmUhwTOHgm": {
    "title": "Trans-Encoder: Unsupervised sentence-pair modelling through self- and mutual-distillations",
    "abstract": "pairwise between two sequences sentence similarity and paraphrase identification). two formulations for sentence-pair tasks: bi-encoders and cross-encoders. Bi-encoders fixed-dimensional sentence representations computationally efficient, underperform cross-encoders. Cross-encoders their attention heads to exploit inter-sentence interactions for better performance they require task finetuning computationally more expensive. In this paper, we present a completely unsupervised sentence-pair model termed as T RANS -E NCODER that combines the two learning paradigms into an iterative joint framework to simultaneously learn enhanced bi- and cross-encoders. Specifically, on top of a pre-trained language model (PLM), we start with converting it to an unsupervised bi-encoder, and then alternate between the bi- and cross-encoder task formulations. In each alternation, one task formulation will produce pseudo-labels which are used as learning signals for the other task formulation. We then propose an extension to conduct such self-distillation approach on multiple PLMs in parallel and use the average of their pseudo-labels for mutual-distillation. T RANS -E NCODER creates, to the best of our knowledge, the first completely unsupervised cross-encoder and also a state-of-the-art unsupervised bi-encoder for sentence similarity. Both the bi-encoder and cross-encoder formulations of T RANS E NCODER outperform recently proposed state-of-the-art",
    "volume": "main",
    "checked": true,
    "id": "24cf68b10a2dcec9437b8155344c8f221c907c34",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=ucASPPD9GKN": {
    "title": "Is Homophily a Necessity for Graph Neural Networks?",
    "abstract": "Graph neural networks (GNNs) have shown great prowess in learning representations suitable for numerous graph-based machine learning tasks. When applied to semi-supervised node classification, GNNs are widely believed to work well due to the homophily assumption (\"like attracts like\"), and fail to generalize to heterophilous graphs where dissimilar nodes connect. Recent works design new architectures to overcome such heterophily-related limitations, citing poor baseline performance and new architecture improvements on a few heterophilous graph benchmark datasets as evidence for this notion. In our experiments, we empirically find that standard graph convolutional networks (GCNs) can actually achieve better performance than such carefully designed methods on some commonly used heterophilous graphs. This motivates us to reconsider whether homophily is truly necessary for good GNN performance. We find that this claim is not quite true, and in fact, GCNs can achieve strong performance on heterophilous graphs under certain conditions. Our work carefully characterizes these conditions, and provides supporting theoretical understanding and empirical observations. Finally, we examine existing heterophilous graphs benchmarks and reconcile how the GCN (under)performs on them based on this understanding",
    "volume": "main",
    "checked": true,
    "id": "9f3dd3d44bab4cedb5b8831deadd5d6dbc1b4237",
    "citation_count": 46
  },
  "https://openreview.net/forum?id=wbPObLm6ueA": {
    "title": "Fairness Guarantees under Demographic Shift",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "44244bc52f17d2b04dc3ad35d63a614f583f8826",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=T__V3uLix7V": {
    "title": "RegionViT: Regional-to-Local Attention for Vision Transformers",
    "abstract": "Vision transformer (ViT) has recently shown its strong capability in achieving comparable results to convolutional neural networks (CNNs) on image classification. However, vanilla ViT simply inherits the same architecture from the natural language processing directly, which is often not optimized for vision applications. Motivated by this, in this paper, we propose a new architecture that adopts the pyramid structure and employ a novel regional-to-local attention rather than global self-attention in vision transformers. More specifically, our model first generates regional tokens and local tokens from an image with different patch sizes, where each regional token is associated with a set of local tokens based on the spatial location. The regional-to-local attention includes two steps: first, the regional selfattention extract global information among all regional tokens and then the local self-attention exchanges the information among one regional token and the associated local tokens via self-attention. Therefore, even though local self-attention confines the scope in a local region but it can still receive global information. Extensive experiments on four vision tasks, including image classification, object and keypoint detection, semantics segmentation and action recognition, show that our approach outperforms or is on par with state-of-the-art ViT variants including many concurrent works. Our source codes and models are available at https://github.com/ibm/regionvit",
    "volume": "main",
    "checked": true,
    "id": "2e8149dafb864ec3675087c99bf5572fcf4eb170",
    "citation_count": 49
  },
  "https://openreview.net/forum?id=iulEMLYh1uR": {
    "title": "The Efficiency Misnomer",
    "abstract": "Model efficiency is a critical aspect of developing and deploying machine learning models. Inference time and latency directly affect the user experience, and some applications have hard requirements. In addition to inference costs, model training also have direct financial and environmental impacts. Although there are numerous wellestablished metrics (cost indicators) for measuring model efficiency, researchers and practitioners often assume that these metrics are correlated with each other and report only few of them. In this paper, we thoroughly discuss common cost indicators, their advantages and disadvantages, and how they can contradict each other. We demonstrate how incomplete reporting of cost indicators can lead to partial conclusions and a blurred or incomplete picture of the practical considerations of different models. We further present suggestions to improve reporting of efficiency metrics",
    "volume": "main",
    "checked": true,
    "id": "66d735987a31d666a6459566ae026c40ab9a1c3a",
    "citation_count": 31
  },
  "https://openreview.net/forum?id=p3DKPQ7uaAi": {
    "title": "Temporal Alignment Prediction for Supervised Representation Learning and Few-Shot Sequence Classification",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "5318b619f72bbe3aa2d1bb7b9ffad1b69fe18754",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=sX3XaHwotOg": {
    "title": "Pretraining Text Encoders with Adversarial Mixture of Training Signal Generators",
    "abstract": "We present a new framework AMOS that pretrains text encoders with an Adversarial learning curriculum via a Mixture Of Signals from multiple auxiliary generators. Following ELECTRA-style pretraining, the main encoder is trained as a discriminator to detect replaced tokens generated by auxiliary masked language models (MLMs). Different from ELECTRA which trains one MLM as the generator, we jointly train multiple MLMs of different sizes to provide training signals at various levels of difficulty. To push the discriminator to learn better with challenging replaced tokens, we learn mixture weights over the auxiliary MLMs' outputs to maximize the discriminator loss by backpropagating the gradient from the discriminator via Gumbel-Softmax. For better pretraining efficiency, we propose a way to assemble multiple MLMs into one unified auxiliary model. AMOS outperforms ELECTRA and recent state-of-the-art pretrained models by about 1 point on the GLUE benchmark for BERT base-sized models",
    "volume": "main",
    "checked": true,
    "id": "02f36289e227595dd2786b32c3975ce7e61dff48",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=QuObT9BTWo": {
    "title": "Pareto Set Learning for Neural Multi-Objective Combinatorial Optimization",
    "abstract": "Multiobjective combinatorial optimization (MOCO) problems can be found in many real-world applications. However, exactly solving these problems would be very challenging, particularly when they are NP-hard. Many handcrafted heuristic methods have been proposed to tackle different MOCO problems over the past decades. In this work, we generalize the idea of neural combinatorial optimization, and develop a learning-based approach to approximate the whole Pareto set for a given MOCO problem without further search procedure. We propose a single preference-conditioned model to directly generate approximate Pareto solutions for any trade-off preference, and design an efﬁcient multiobjective reinforcement learning algorithm to train this model. Our proposed method can be treated as a learning-based extension for the widely-used decomposition-based multiobjective evolutionary algorithm (MOEA/D). It uses a single model to accommodate all the possible preferences, whereas other methods use a ﬁnite number of solutions to approximate the Pareto set. Experimental results show that our proposed method signiﬁcantly outperforms some other methods on the multiobjective traveling salesman problem, multiobjective vehicle routing problem, and multiobjective knapsack problem in terms of solution quality, speed, and model efﬁciency",
    "volume": "main",
    "checked": true,
    "id": "aab686e814f078144941cff57a5e0aaf4989aa97",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=_PHymLIxuI": {
    "title": "CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention",
    "abstract": "Transformers have made great progress in dealing with computer vision tasks. However, existing vision transformers do not yet possess the ability of building the interactions among features of different scales, which is perceptually important to visual inputs. The reasons are two-fold: (1) Input embeddings of each layer are equal-scale, so no cross-scale feature can be extracted; (2) to lower the computational cost, some vision transformers merge adjacent embeddings inside the selfattention module, thus sacrificing small-scale (fine-grained) features of the embeddings and also disabling the cross-scale interactions. To this end, we propose Cross-scale Embedding Layer (CEL) and Long Short Distance Attention (LSDA). On the one hand, CEL blends each embedding with multiple patches of different scales, providing the self-attention module itself with cross-scale features. On the other hand, LSDA splits the self-attention module into a short-distance one and a long-distance counterpart, which not only reduces the computational burden but also keeps both small-scale and large-scale features in the embeddings. Through the above two designs, we achieve cross-scale attention. Besides, we put forward a dynamic position bias for vision transformers to make the popular relative position bias apply to variable-sized images. Hinging on the cross-scale attention module, we construct a versatile vision architecture, dubbed CrossFormer, which accommodates variable-sized inputs. Extensive experiments show that CrossFormer outperforms the other vision transformers on image classification, object detection, instance segmentation, and semantic segmentation tasks.1",
    "volume": "main",
    "checked": true,
    "id": "a5c41f188b0eb0acb444cb4899bf6af378ee9ede",
    "citation_count": 35
  },
  "https://openreview.net/forum?id=ECvgmYVyeUz": {
    "title": "Chaos is a Ladder: A New Theoretical Understanding of Contrastive Learning via Augmentation Overlap",
    "abstract": "Recently, contrastive learning has risen to be a promising approach for large-scale self-supervised learning. However, theoretical understanding of how it works is still unclear. In this paper, we propose a new guarantee on the downstream performance without resorting to the conditional independence assumption that is widely adopted in previous work but hardly holds in practice. Our new theory hinges on the insight that the support of different intra-class samples will become more overlapped under aggressive data augmentations, thus simply aligning the positive samples (augmented views of the same sample) could make contrastive learning cluster intra-class samples together. Based on this augmentation overlap perspective, theoretically, we obtain asymptotically closed bounds for downstream performance under weaker assumptions, and empirically, we propose an unsupervised model selection metric ARC that aligns well with downstream accuracy. Our theory suggests an alternative understanding of contrastive learning: the role of aligning positive samples is more like a surrogate task than an ultimate goal, and the overlapped augmented views (i.e., the chaos) create a ladder for contrastive learning to gradually learn class-separated representations. The code for computing ARC is available at https://github.com/zhangq327/ARC",
    "volume": "main",
    "checked": true,
    "id": "13beab6bad06631d177e274e4d70b95c4b103423",
    "citation_count": 14
  },
  "https://openreview.net/forum?id=CyKHoKyvgnp": {
    "title": "Transition to Linearity of Wide Neural Networks is an Emerging Property of Assembling Weak Models",
    "abstract": "Wide neural networks with linear output layer have been shown to be near-linear, and to have near-constant neural tangent kernel (NTK), in a region containing the optimization path of gradient descent. These findings seem counter-intuitive since in general neural networks are highly complex models. Why does a linear structure emerge when the networks become wide? In this work, we provide a new perspective on this \"transition to linearity\" by considering a neural network as an assembly model recursively built from a set of sub-models corresponding to individual neurons. In this view, we show that the linearity of wide neural networks is, in fact, an emerging property of assembling a large number of diverse \"weak\" sub-models, none of which dominate the assembly",
    "volume": "main",
    "checked": true,
    "id": "e4e153dcd5d261efcf1647264e367c32e446c658",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=uSE03demja": {
    "title": "RISP: Rendering-Invariant State Predictor with Differentiable Simulation and Rendering for Cross-Domain Parameter Estimation",
    "abstract": "This work considers identifying parameters characterizing a physical system's dynamic motion directly from a video whose rendering conﬁgurations are inaccessible. Existing solutions require massive training data or lack generalizability to unknown rendering conﬁgurations. We propose a novel approach that marries domain randomization and differentiable rendering gradients to address this problem. Our core idea is to train a rendering-invariant state-prediction (RISP) network that transforms image differences into state differences independent of rendering conﬁgurations, e.g., lighting, shadows, or material reﬂectance. To train this predictor, we formulate a new loss on rendering variances using gradients from differentiable rendering. Moreover, we present an efﬁcient, second-order method to compute the gradients of this loss, allowing it to be integrated seamlessly into modern deep learning frameworks. We evaluate our method in rigid-body and deformable-body simulation environments using four tasks: state estimation, system identiﬁcation, imitation learning, and visuomotor control. We further demonstrate the efﬁcacy of our approach on a real-world example: inferring the state and action sequences of a quadrotor from a video of its motion sequences. Compared with existing methods, our approach achieves signiﬁcantly lower reconstruction errors and has better generalizability among unknown rendering conﬁgurations 1",
    "volume": "main",
    "checked": true,
    "id": "da98d61a5ed63abe34d52bc9d500f702f9239e87",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=XVPqLyNxSyh": {
    "title": "Salient ImageNet: How to discover spurious features in Deep Learning?",
    "abstract": "we identify spurious or core neural features (penultimate layer neurons of a robust model) via limited human supervision (e.g., using top 5 activating images per feature). We then show that these neural feature annotations generalize extremely well to many more images without any human supervision. We use the activation maps for these neural features as the soft masks to highlight spurious or core visual features. Using this methodology, we introduce the Salient Imagenet dataset containing core and spurious masks for a large set of samples from Imagenet. Using this dataset, we show that several popular Imagenet models rely heavily on various spurious features in their predictions, in-dicating the standard accuracy alone is not sufﬁcient to fully assess model perfor-1",
    "volume": "main",
    "checked": true,
    "id": "5f893ad86470cb935d702f980f5af8d8e013c7ae",
    "citation_count": 20
  },
  "https://openreview.net/forum?id=hcQHRHKfN_": {
    "title": "Continuously Discovering Novel Strategies via Reward-Switching Policy Optimization",
    "abstract": "We present Reward-Switching Policy Optimization (RSPO), a paradigm to discover diverse strategies in complex RL environments by iteratively ﬁnding novel policies that are both locally optimal and sufﬁciently different from existing ones. To encourage the learning policy to consistently converge towards a previously undiscovered local optimum, RSPO switches between extrinsic and intrinsic rewards via a trajectory-based novelty measurement during the optimization process. When a sampled trajectory is sufﬁciently distinct, RSPO performs standard policy optimization with extrinsic rewards. For trajectories with high likelihood under existing policies, RSPO utilizes an intrinsic diversity reward to promote exploration. Experiments show that RSPO is able to discover a wide spectrum of strategies in a variety of domains, ranging from single-agent particle-world tasks and MuJoCo continuous control to multi-agent stag-hunt games and StarCraftII challenges",
    "volume": "main",
    "checked": true,
    "id": "0ac9e357f2599dc4f66bf74f511c717fdc9e3a20",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=fCG75wd39ze": {
    "title": "LORD: Lower-Dimensional Embedding of Log-Signature in Neural Rough Differential Equations",
    "abstract": "The problem of processing very long time-series data (e.g., a length of more than 10,000) is a long-standing research problem in machine learning. Recently, one breakthrough, called neural rough differential equations (NRDEs), has been proposed and has shown that it is able to process such data. Their main concept is to use the log-signature transform, which is known to be more efﬁcient than the Fourier transform for irregular long time-series, to convert a very long time-series sample into a relatively shorter series of feature vectors. However, the log-signature transform causes non-trivial spatial overheads. To this end, we present the method of LO we R - D imensional embedding of log-signature (LORD), where we deﬁne an NRDE-based autoencoder to implant the higher-depth log-signature knowledge into the lower-depth log-signature. We show that the encoder successfully combines the higher-depth and the lower-depth log-signature knowledge, which greatly stabilizes the training process and increases the model accuracy. In our experiments with benchmark datasets, the improvement ratio by our method is up to 75% in terms of various classiﬁcation and forecasting evaluation metrics",
    "volume": "main",
    "checked": true,
    "id": "40aae7819918c596fd6a8a5caee2b9eb4cf4ec02",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=TXsjU8BaibT": {
    "title": "Trigger Hunting with a Topological Prior for Trojan Detection",
    "abstract": "Despite deep neural (DNNs) when backdoor This impedes their adoption, especially in mission critical applications. This paper tackles the problem of Trojan detection, namely, identifying Trojaned models – models trained with poisoned data. One popular approach is reverse engineering, i.e., recovering the triggers on a clean image by manipulating the model's prediction. One major challenge of reverse engineering approach is the enormous search space of triggers. To this end, we propose innovative priors such as diversity and topological simplicity to not only increase the chances of ﬁnding the appropriate triggers but also improve the quality of the found triggers. Moreover, by encouraging a diverse set of trigger candidates, our method can perform effectively in cases with unknown target labels. We demonstrate that these priors can signiﬁcantly improve the quality of the recovered triggers, resulting in substantially improved Trojan detection accuracy as validated on both synthetic and publicly available TrojAI benchmarks",
    "volume": "main",
    "checked": true,
    "id": "400ecda0ce56b6ef77c51247167680103666fc41",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=IK9ap6nxXr2": {
    "title": "Interacting Contour Stochastic Gradient Langevin Dynamics",
    "abstract": "We propose an interacting contour stochastic gradient Langevin dynamics (ICSGLD) sampler, an embarrassingly parallel multiple-chain contour stochastic gradient Langevin dynamics (CSGLD) sampler with efficient interactions. We show that ICSGLD can be theoretically more efficient than a single-chain CSGLD with an equivalent computational budget. We also present a novel random-field function, which facilitates the estimation of self-adapting parameters in big data and obtains free mode explorations. Empirically, we compare the proposed algorithm with popular benchmark methods for posterior sampling. The numerical results show a great potential of ICSGLD for large-scale uncertainty estimation tasks",
    "volume": "main",
    "checked": true,
    "id": "4210a4e8e3396e8099ef457ca3312d6d653c9d2f",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=IvepFxYRDG": {
    "title": "Sample Efficient Stochastic Policy Extragradient Algorithm for Zero-Sum Markov Game",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "056ac5a492788ff7d93fe4f7edd7971c4c658af9",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=VimqQq-i_Q": {
    "title": "What Do We Mean by Generalization in Federated Learning?",
    "abstract": "Federated learning data is drawn from a distribution of distributions: clients are drawn from a meta-distribution, and their data are drawn from local data distributions. Thus generalization studies in federated learning should separate performance gaps from unseen client data (out-of-sample gap) from performance gaps from unseen client distributions (participation gap). In this work, we propose a framework for disentangling these performance gaps. Using this framework, we observe and explain differences in behavior across natural and synthetic federated datasets, indicating that dataset synthesis strategy can be important for realistic simulations of generalization in federated learning. We propose a semantic synthesis strategy that enables realistic simulation without naturally-partitioned data. Informed by our findings, we call out community suggestions for future federated learning works",
    "volume": "main",
    "checked": true,
    "id": "58dd7435865a37e5e3fb67bf42a025b4b6491d7e",
    "citation_count": 13
  },
  "https://openreview.net/forum?id=7MV6uLzOChW": {
    "title": "Conditional Image Generation by Conditioning Variational Auto-Encoders",
    "abstract": "We present a conditional variational auto-encoder (VAE) which, to avoid the substantial cost of training from scratch, uses an architecture and training objective capable of leveraging a foundation model in the form of a pretrained unconditional VAE. To train the conditional VAE, we only need to train an artifact to perform amortized inference over the unconditional VAE's latent variables given a conditioning input. We demonstrate our approach on tasks including image inpainting, for which it outperforms state-of-the-art GAN-based approaches at faithfully representing the inherent uncertainty. We conclude by describing a possible application of our inpainting model, in which it is used to perform Bayesian experimental design for the purpose of guiding a sensor",
    "volume": "main",
    "checked": true,
    "id": "9260377808078724f969b8e80ec89e94d440ce22",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=qhAeZjs7dCL": {
    "title": "Generative Models as a Data Source for Multiview Representation Learning",
    "abstract": "Generative models are now capable of producing highly realistic images that look nearly indistinguishable from the data on which they are trained. This raises the question: if we have good enough generative models, do we still need datasets? We investigate this question in the setting of learning general-purpose visual representations from a black-box generative model rather than directly from data. Given an off-the-shelf image generator without any access to its training data, we train representations from the samples output by this generator. We compare several representation learning methods that can be applied to this setting, using the latent space of the generator to generate multiple \"views\" of the same semantic content. We show that for contrastive methods, this multiview data can naturally be used to identify positive pairs (nearby in latent space) and negative pairs (far apart in latent space). We find that the resulting representations rival those learned directly from real data, but that good performance requires care in the sampling strategy applied and the training method. Generative models can be viewed as a compressed and organized copy of a dataset, and we envision a future where more and more \"model zoos\" proliferate while datasets become increasingly unwieldy, missing, or private. This paper suggests several techniques for dealing with visual representation learning in such a future. Code is released on our project page https://ali-design.github",
    "volume": "main",
    "checked": true,
    "id": "3743249bf829cbe0de72cc49371f51c40d7cf56c",
    "citation_count": 21
  },
  "https://openreview.net/forum?id=tD7eCtaSkR": {
    "title": "Improved deterministic l2 robustness on CIFAR-10 and CIFAR-100",
    "abstract": "Training convolutional neural networks (CNNs) with a strict Lipschitz constraint under the l2 norm is useful for provable adversarial robustness, interpretable gradients and stable training. While 1-Lipschitz CNNs can be designed by enforcing a 1-Lipschitz constraint on each layer, training such networks requires each layer to have an orthogonal Jacobian matrix (for all inputs) to prevent the gradients from vanishing during backpropagation. A layer with this property is said to be Gradient Norm Preserving (GNP). In this work, we introduce a procedure to certify the robustness of 1-Lipschitz CNNs by relaxing the orthogonalization of the last linear layer of the network that significantly advances the state of the art for both standard and provable robust accuracies on CIFAR-100 (gains of 4.80% and 4.71%, respectively). We further boost their robustness by introducing (i) a novel Gradient Norm preserving activation function called the Householder activation function (that includes every GroupSort activation) and (ii) a certificate regularization. On CIFAR-10, we achieve significant improvements over prior works in provable robust accuracy (5.81%) with only a minor drop in standard accuracy (−0.29%). Code for reproducing all experiments in the paper is available at https://github.com/singlasahil14/SOC",
    "volume": "main",
    "checked": true,
    "id": "a3c052386f0fae0c84c6743271ddb7a938fd755c",
    "citation_count": 13
  },
  "https://openreview.net/forum?id=xnYACQquaGV": {
    "title": "Neural Contextual Bandits with Deep Representation and Shallow Exploration",
    "abstract": "We study a general class of contextual bandits, where each context-action pair is associated with a raw feature vector, but the reward generating function is unknown. We propose a novel learning algorithm that transforms the raw feature vector using the last hidden layer of a deep ReLU neural network (deep representation learning), and uses an upper confidence bound (UCB) approach to explore in the last linear layer (shallow exploration). We prove that under standard assumptions, our proposed algorithm achieves $\\tilde{O}(\\sqrt{T})$ finite-time regret, where $T$ is the learning time horizon. Compared with existing neural contextual bandit algorithms, our approach is computationally much more efficient since it only needs to explore in the last layer of the deep neural network",
    "volume": "main",
    "checked": true,
    "id": "1355d22b6f85a28d46ce5ec31b1a4ba38e38147e",
    "citation_count": 20
  },
  "https://openreview.net/forum?id=14F3fI6MGxX": {
    "title": "A Generalized Weighted Optimization Method for Computational Learning and Inversion",
    "abstract": "The generalization capacity of various machine learning models exhibits different phenomena in the underand over-parameterized regimes. In this paper, we focus on regression models such as feature regression and kernel regression and analyze a generalized weighted least-squares optimization method for computational learning and inversion with noisy data. The highlight of the proposed framework is that we allow weighting in both the parameter space and the data space. The weighting scheme encodes both a priori knowledge on the object to be learned and a strategy to weight the contribution of different data points in the loss function. Here, we characterize the impact of the weighting scheme on the generalization error of the learning method, where we derive explicit generalization errors for the random Fourier feature model in both the underand over-parameterized regimes. For more general feature maps, error bounds are provided based on the singular values of the feature matrix. We demonstrate that appropriate weighting from prior knowledge can improve the generalization capability of the learned model",
    "volume": "main",
    "checked": true,
    "id": "3c211c62754794a97f14a08aab2bf14bc264cfed",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=wkMG8cdvh7-": {
    "title": "Understanding and Improving Graph Injection Attack by Promoting Unnoticeability",
    "abstract": "Recently Graph Injection Attack (GIA) emerges as a practical attack scenario on Graph Neural Networks (GNNs), where the adversary can merely inject few malicious nodes instead of modifying existing nodes or edges, i.e., Graph Modification Attack (GMA). Although GIA has achieved promising results, little is known about why it is successful and whether there is any pitfall behind the success. To understand the power of GIA, we compare it with GMA and find that GIA can be provably more harmful than GMA due to its relatively high flexibility. However, the high flexibility will also lead to great damage to the homophily distribution of the original graph, i.e., similarity among neighbors. Consequently, the threats of GIA can be easily alleviated or even prevented by homophily-based defenses designed to recover the original homophily. To mitigate the issue, we introduce a novel constraint – homophily unnoticeability that enforces GIA to preserve the homophily, and propose Harmonious Adversarial Objective (HAO) to instantiate it. Extensive experiments verify that GIA with HAO can break homophily-based defenses and outperform previous GIA attacks by a significant margin. We believe our methods can serve for a more reliable evaluation of the robustness of GNNs",
    "volume": "main",
    "checked": true,
    "id": "1b24fbc2189aca1d17f66eac7c8b09397eaf336f",
    "citation_count": 13
  },
  "https://openreview.net/forum?id=IYMuTbGzjFU": {
    "title": "Representing Mixtures of Word Embeddings with Mixtures of Topic Embeddings",
    "abstract": "A topic model is often formulated as a generative model that explains how each word of a document is generated given a set of topics and document-specific topic proportions. It is focused on capturing the word co-occurrences in a document and hence often suffers from poor performance in analyzing short documents. In addition, its parameter estimation often relies on approximate posterior inference that is either not scalable or suffers from large approximation error. This paper introduces a new topic-modeling framework where each document is viewed as a set of word embedding vectors and each topic is modeled as an embedding vector in the same embedding space. Embedding the words and topics in the same vector space, we define a method to measure the semantic difference between the embedding vectors of the words of a document and these of the topics, and optimize the topic embeddings to minimize the expected difference over all documents. Experiments on text analysis demonstrate that the proposed method, which is amenable to mini-batch stochastic gradient descent based optimization and hence scalable to big corpora, provides competitive performance in discovering more coherent and diverse topics and extracting better document representations. The code is available at https://github.com/BoChenGroup/WeTe",
    "volume": "main",
    "checked": true,
    "id": "4102043f373446a92b81c2e8c7e9e747e7f5d434",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=pz1euXohm4H": {
    "title": "Target-Side Input Augmentation for Sequence to Sequence Generation",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "6883a825e8267459080fc0668df716853d1c83b0",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=JXhROKNZzOc": {
    "title": "SQuant: On-the-Fly Data-Free Quantization via Diagonal Hessian Approximation",
    "abstract": "Quantization of deep neural networks (DNN) has been proven effective for compressing and accelerating DNN models. Data-free quantization (DFQ) is a promising approach without the original datasets under privacy-sensitive and confidential scenarios. However, current DFQ solutions degrade accuracy, need synthetic data to calibrate networks, and are time-consuming and costly. This paper proposes an on-the-fly DFQ framework with sub-second quantization time, called SQuant, which can quantize networks on inference-only devices with low computation and memory requirements. With the theoretical analysis of the second-order information of DNN task loss, we decompose and approximate the Hessian-based optimization objective into three diagonal sub-items, which have different areas corresponding to three dimensions of weight tensor: element-wise, kernel-wise, and output channel-wise. Then, we progressively compose sub-items and propose a novel data-free optimization objective in the discrete domain, minimizing Constrained Absolute Sum of Error (or CASE in short), which surprisingly does not need any dataset and is even not aware of network architecture. We also design an efficient algorithm without back-propagation to further reduce the computation complexity of the objective solver. Finally, without fine-tuning and synthetic datasets, SQuant accelerates the data-free quantization process to a sub-second level with > 30% accuracy improvement over the existing data-free post-training quantization works, with the evaluated models under 4-bit quantization. We have open-sourced the SQuant framework1",
    "volume": "main",
    "checked": true,
    "id": "583f353972ed917772f3f1fc62f4b7cadc8f1e81",
    "citation_count": 12
  },
  "https://openreview.net/forum?id=CSfcOznpDY": {
    "title": "Recursive Disentanglement Network",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "e41f9d672ddd894f7ab531f1f6703ce7dd53de5a",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=kNKFOXleuC": {
    "title": "Anytime Dense Prediction with Confidence Adaptivity",
    "abstract": "Anytime inference requires a model to make a progression of predictions which might be halted at any time. Prior research on anytime visual recognition has mostly focused on image classiﬁcation. We propose the ﬁrst uniﬁed and end-to-end approach for anytime dense prediction. A cascade of \"exits\" is attached to the model to make multiple predictions. We redesign the exits to account for the depth and spatial resolution of the features for each exit. To reduce total computation, and make full use of prior predictions, we develop a novel spatially adaptive approach to avoid further computation on regions where early predictions are already sufﬁciently conﬁdent. Our full method, named anytime dense prediction with conﬁdence (ADP-C), achieves the same level of ﬁnal accuracy as the base model, and meanwhile signiﬁcantly reduces total computation. We evaluate our method on Cityscapes semantic segmentation and MPII human pose estimation: ADP-C en-ables anytime inference without sacriﬁcing accuracy while also reducing the total FLOPs of its base models by 44.4% and 59.1%. We compare with anytime inference by deep equilibrium networks and feature-based stochastic sampling, show-ing that ADP-C dominates both across the accuracy-computation curve. Our code is available at https://github.com/liuzhuang13/anytime . ﬁnal We",
    "volume": "main",
    "checked": true,
    "id": "48d70a9662cd2aef09ec2ddf8d91f4358e7d96ac",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=c-4HSDAWua5": {
    "title": "SketchODE: Learning neural sketch representation in continuous time",
    "abstract": "Learning meaningful representations for chirographic drawing data such as sketches, handwriting, and flowcharts is a gateway for understanding and emulating human creative expression. Despite being inherently continuous-time data, existing works have treated these as discrete-time sequences, disregarding their true nature. In this work, we model such data as continuous-time functions and learn compact representations by virtue of Neural Ordinary Differential Equations. To this end, we introduce the first continuous-time Seq2Seq model and demonstrate some remarkable properties that set it apart from traditional discrete-time analogues. We also provide solutions for some practical challenges for such models, including introducing a family of parameterized ODE dynamics & continuoustime data augmentation particularly suitable for the task. Our models are validated on several datasets including VectorMNIST, DiDi and Quick, Draw!",
    "volume": "main",
    "checked": true,
    "id": "726e069bfa66dac36c5abc12b6237a88ca777f60",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=FLA55mBee6Q": {
    "title": "COptiDICE: Offline Constrained Reinforcement Learning via Stationary Distribution Correction Estimation",
    "abstract": "We consider the ofﬂine constrained reinforcement learning (RL) problem, in which the agent aims to compute a policy that maximizes expected return while satisfying given cost constraints, learning only from a pre-collected dataset. This problem setting is appealing in many real-world scenarios, where direct interaction with the environment is costly or risky, and where the resulting policy should comply with safety constraints. However, it is challenging to compute a policy that guarantees satisfying the cost constraints in the ofﬂine RL setting, since the off-policy evaluation inherently has an estimation error. In this paper, we present an ofﬂine constrained RL algorithm that optimizes the policy in the space of the stationary distribution. Our algorithm, COptiDICE, directly estimates the stationary distribution corrections of the optimal policy with respect to returns, while constraining the cost upper bound, with the goal of yielding a cost-conservative policy for actual constraint satisfaction. Experimental results show that COptiDICE attains better policies in terms of constraint satisfaction and return-maximization, outperforming baseline algorithms",
    "volume": "main",
    "checked": true,
    "id": "d1a596b5dce1ff183a763cdd1610a95f7f05e170",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=gpp7cf0xdfN": {
    "title": "Reverse Engineering of Imperceptible Adversarial Image Perturbations",
    "abstract": "It has been well recognized that neural network based image classifiers are easily fooled by images with tiny perturbations crafted by an adversary. There has been a vast volume of research to generate and defend such adversarial attacks. However, the following problem is left unexplored: How to reverse-engineer adversarial perturbations from an adversarial image? This leads to a new adversarial learning paradigm—Reverse Engineering of Deceptions (RED). If successful, RED allows us to estimate adversarial perturbations and recover the original images. However, carefully crafted, tiny adversarial perturbations are difficult to recover by optimizing a unilateral RED objective. For example, the pure image denoising method may overfit to minimizing the reconstruction error but hardly preserves the classification properties of the true adversarial perturbations. To tackle this challenge, we formalize the RED problem and identify a set of principles crucial to the RED approach design. Particularly, we find that prediction alignment and proper data augmentation (in terms of spatial transformations) are two criteria to achieve a generalizable RED approach. By integrating these RED principles with image denoising, we propose a new Class-Discriminative Denoising based RED framework, termed CDD-RED. Extensive experiments demonstrate the effectiveness of CDD-RED under different evaluation metrics (ranging from the pixel-level, prediction-level to the attribution-level alignment) and a variety of attack generation methods (e.g., FGSM, PGD, CW, AutoAttack, and adaptive attacks). Codes are available at link",
    "volume": "main",
    "checked": true,
    "id": "2693b44fa7d291581e9c3f23630cf5c402caf06d",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=qSV5CuSaK_a": {
    "title": "Few-Shot Backdoor Attacks on Visual Object Tracking",
    "abstract": "We",
    "volume": "main",
    "checked": true,
    "id": "23ae1ee3a1fd6a40b29978c32c56dff83d6f9d57",
    "citation_count": 14
  },
  "https://openreview.net/forum?id=siCt4xZn5Ve": {
    "title": "What Happens after SGD Reaches Zero Loss? --A Mathematical Framework",
    "abstract": "Understanding the implicit bias of Stochastic Gradient Descent (SGD) is one of the key challenges in deep learning, especially for overparametrized models, where the local minimizers of the loss function L can form a manifold. Intuitively, with a sufficiently small learning rate η, SGD tracks Gradient Descent (GD) until it gets close to such manifold, where the gradient noise prevents further convergence. In such a regime, Blanc et al. (2020) proved that SGD with label noise locally decreases a regularizer-like term, the sharpness of loss, tr[∇L]. The current paper gives a general framework for such analysis by adapting ideas from Katzenberger (1991). It allows in principle a complete characterization for the regularization effect of SGD around such manifold—i.e., the \"implicit bias\"—using a stochastic differential equation (SDE) describing the limiting dynamics of the parameters, which is determined jointly by the loss function and the noise covariance. This yields some new results: (1) a global analysis of the implicit bias valid for η−2 steps, in contrast to the local analysis of Blanc et al. (2020) that is only valid for η−1.6 steps and (2) allowing arbitrary noise covariance. As an application, we show with arbitrary large initialization, label noise SGD can always escape the kernel regime and only requires O(κ ln d) samples for learning an κ-sparse overparametrized linear model in R (Woodworth et al., 2020), while GD initialized in the kernel regime requires Ω(d) samples. This upper bound is minimax optimal and improves the previous Õ(κ) upper bound (HaoChen et al., 2020)",
    "volume": "main",
    "checked": true,
    "id": "40c115c43adee6bc7a00ffd444ee9c045360d97d",
    "citation_count": 22
  },
  "https://openreview.net/forum?id=KSugKcbNf9": {
    "title": "Transformers Can Do Bayesian Inference",
    "abstract": "Currently, it hard to reap the beneﬁts of deep learning for Bayesian methods, which allow the explicit speciﬁcation of prior knowledge and accurately capture model uncertainty. We present Prior-Data Fitted Networks (PFNs) . PFNs leverage large-scale machine learning techniques to approximate a large set of posteriors. The only requirement for PFNs to work is the ability to sample from a prior distribution over supervised learning tasks (or functions). Our method restates the objective of posterior approximation as a supervised classiﬁcation problem with a set-valued input: it repeatedly draws a task (or function) from the prior, draws a set of data points and their labels from it, masks one of the labels and learns to make probabilistic predictions for it based on the set-valued input of the rest of the data points. Presented with a set of samples from a new supervised learning task as input, PFNs make probabilistic predictions for arbitrary other data points in a single forward propagation, having learned to approximate Bayesian inference. We demonstrate that PFNs can near-perfectly mimic Gaussian processes and also enable efﬁcient Bayesian inference for intractable problems, with over 200-fold speedups in multiple setups compared to current methods. We obtain strong results in very diverse areas such as Gaussian process regression, Bayesian neural networks, classiﬁcation for small tabular data sets, and few-shot image classiﬁcation, demonstrating the generality of PFNs. Code and trained PFNs are released",
    "volume": "main",
    "checked": true,
    "id": "9143c6351113c19904301938fb887be1991500d0",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=UcDUxjPYWSr": {
    "title": "Transform2Act: Learning a Transform-and-Control Policy for Efficient Agent Design",
    "abstract": "An agent's functionality is largely determined by its design, i.e., skeletal structure and joint attributes (e.g., length, size, strength). However, finding the optimal agent design for a given function is extremely challenging since the problem is inherently combinatorial and the design space is prohibitively large. Additionally, it can be costly to evaluate each candidate design which requires solving for its optimal controller. To tackle these problems, our key idea is to incorporate the design procedure of an agent into its decision-making process. Specifically, we learn a conditional policy that, in an episode, first applies a sequence of transform actions to modify an agent's skeletal structure and joint attributes, and then applies control actions under the new design. To handle a variable number of joints across designs, we use a graph-based policy where each graph node represents a joint and uses message passing with its neighbors to output joint-specific actions. Using policy gradient methods, our approach enables joint optimization of agent design and control as well as experience sharing across different designs, which improves sample efficiency substantially. Experiments show that our approach, Transform2Act, outperforms prior methods significantly in terms of convergence speed and final performance. Notably, Transform2Act can automatically discover plausible designs similar to giraffes, squids, and spiders. Code and videos are available at https://sites.google.com/view/transform2act",
    "volume": "main",
    "checked": true,
    "id": "4a26969478c80e15a68dda8712da34c8a6a441ef",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=iLHOIDsPv1P": {
    "title": "PAC-Bayes Information Bottleneck",
    "abstract": "Understanding the source of the superior generalization ability of NNs remains one of the most important problems in ML research. There have been a series of theoretical works trying to derive non-vacuous bounds for NNs. Recently, the compression of information stored in weights (IIW) is proved to play a key role in NNs generalization based on the PAC-Bayes theorem. However, no solution of IIW has ever been provided, which builds a barrier for further investigation of the IIW's property and its potential in practical deep learning. In this paper, we propose an algorithm for the efficient approximation of IIW. Then, we build an IIW-based information bottleneck on the trade-off between accuracy and information complexity of NNs, namely PIB. From PIB, we can empirically identify the fitting to compressing phase transition during NNs' training and the concrete connection between the IIW compression and the generalization. Besides, we verify that IIW is able to explain NNs in broad cases, e.g., varying batch sizes, overparameterization, and noisy labels. Moreover, we propose an MCMC-based algorithm to sample from the optimal weight posterior characterized by PIB, which fulfills the potential of IIW in enhancing NNs in practice",
    "volume": "main",
    "checked": true,
    "id": "39000df05781f49365cb1b223d228dd9b859efc4",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=AJAR-JgNw__": {
    "title": "DEPTS: Deep Expansion Learning for Periodic Time Series Forecasting",
    "abstract": "Periodic time series (PTS) forecasting plays a crucial role in a variety of industries to foster critical tasks, such as early warning, pre-planning, resource scheduling, etc. However, the complicated dependencies of the PTS signal on its inherent periodicity as well as the sophisticated composition of various periods hinder the performance of PTS forecasting. In this paper, we introduce a deep expansion learning framework, DEPTS, for PTS forecasting. DEPTS starts with a decoupled formulation by introducing the periodic state as a hidden variable, which stimulates us to make two dedicated modules to tackle the aforementioned two challenges. First, we develop an expansion module on top of residual learning to perform a layer-by-layer expansion of those complicated dependencies. Second, we introduce a periodicity module with a parameterized periodic function that holds sufficient capacity to capture diversified periods. Moreover, our two customized modules also have certain interpretable capabilities, such as attributing the forecasts to either local momenta or global periodicity and characterizing certain core periodic properties, e.g., amplitudes and frequencies. Extensive experiments on both synthetic data and real-world data demonstrate the effectiveness of DEPTS on handling PTS. In most cases, DEPTS achieves significant improvements over the best baseline. Specifically, the error reduction can even reach up to 20% for a few cases. All codes are publicly available at https://github.com/weifantt/DEPTS",
    "volume": "main",
    "checked": true,
    "id": "d62fdf39a501f79df7c514a023e02a4dd0bcc967",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=vaRCHVj0uGI": {
    "title": "Solving Inverse Problems in Medical Imaging with Score-Based Generative Models",
    "abstract": "Reconstructing in Tomography (CT) Resonance Imaging (MRI). leveraging of paired physical model of the process, the generalization capability of to measurement processes. we propose a technique for problem solving, leveraging generative models. Speciﬁcally, we a score-based generative model on medical images to capture their prior distribution. measurements and a physical model of the measurement process at test time, we introduce a sampling method to reconstruct an image consistent with both the prior and the observed measurements. Our method does not assume a measurement process during and thus be to processes at test time. Empirically, we comparable performance to techniques in imaging CT and MRI, while demonstrating better generalization to unknown measurement processes",
    "volume": "main",
    "checked": true,
    "id": "49f6dbf4ead6a8a3d26f9cf218a654f2f3d1d896",
    "citation_count": 47
  },
  "https://openreview.net/forum?id=MP904TiHqJ-": {
    "title": "Provably convergent quasistatic dynamics for mean-field two-player zero-sum games",
    "abstract": "In this paper, we study the problem of finding mixed Nash equilibrium for meanfield two-player zero-sum games. Solving this problem requires optimizing over two probability distributions. We consider a quasistatic Wasserstein gradient flow dynamics in which one probability distribution follows the Wasserstein gradient flow, while the other one is always at the equilibrium. Theoretical analysis are conducted on this dynamics, showing its convergence to the mixed Nash equilibrium under mild conditions. Inspired by the continuous dynamics of probability distributions, we derive a quasistatic Langevin gradient descent method with inner-outer iterations, and test the method on different problems, including training mixture of GANs",
    "volume": "main",
    "checked": true,
    "id": "4e94df220b460da799073ab22f1102f602374461",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=qiMXBIf4NfB": {
    "title": "How unlabeled data improve generalization in self-training? A one-hidden-layer theoretical analysis",
    "abstract": "Self-training, a semi-supervised learning algorithm, leverages a large amount of unlabeled data to improve learning when the labeled data are limited. Despite empirical successes, its theoretical characterization remains elusive. To the best of our knowledge, this work establishes the ﬁrst theoretical analysis for the known iterative self-training paradigm and proves the beneﬁts of unlabeled data in both training convergence and generalization ability. To make our theoretical analysis feasible, we focus on the case of one-hidden-layer neural networks. However, theoretical understanding of iterative self-training is non-trivial even for a shallow neural network. One of the key challenges is that existing neural network landscape analysis built upon supervised learning no longer holds in the (semi-supervised) self-training paradigm. We address this challenge and prove that iterative self-training converges linearly with both convergence rate and generalization accuracy improved in the order of 1 / √ M , where M is the number of unlabeled samples. Experiments from shallow neural networks to deep neural networks are also provided to justify the correctness of our established theoretical insights on self-training",
    "volume": "main",
    "checked": true,
    "id": "d208842ecfb0c356315a6393df471b2042e383fc",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=dZPgfwaTaXv": {
    "title": "Relational Surrogate Loss Learning",
    "abstract": "Evaluation metrics in machine learning are often hardly taken as loss functions, as they could be non-differentiable and non-decomposable, e.g., average precision and F1 score. This paper aims to address this problem by revisiting the surrogate loss learning, where a deep neural network is employed to approximate the evaluation metrics. Instead of pursuing an exact recovery of the evaluation metric through a deep neural network, we are reminded of the purpose of the existence of these evaluation metrics, which is to distinguish whether one model is better or worse than another. In this paper, we show that directly maintaining the relation of models between surrogate losses and metrics suffices, and propose a rank correlation-based optimization method to maximize this relation and learn surrogate losses. Compared to previous works, our method is much easier to optimize and enjoys significant efficiency and performance gains. Extensive experiments show that our method achieves improvements on various tasks including image classification and neural machine translation, and even outperforms state-of-theart methods on human pose estimation and machine reading comprehension tasks. Code is available at: https://github.com/hunto/ReLoss",
    "volume": "main",
    "checked": true,
    "id": "96f79556c4a6f11f88ede749f2e379cc97d237c2",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=ZSKRQMvttc": {
    "title": "Accelerated Policy Learning with Parallel Differentiable Simulation",
    "abstract": "Deep reinforcement learning can generate complex control policies, but requires large amounts of training data to work effectively. Recent work has attempted to address this issue by leveraging differentiable simulators. However, inherent problems such as local minima and exploding/vanishing numerical gradients prevent these methods from being generally applied to control tasks with complex contact-rich dynamics, such as humanoid locomotion in classical RL benchmarks. In this work we present a high-performance differentiable simulator and a new policy learning algorithm (SHAC) that can effectively leverage simulation gradients, even in the presence of non-smoothness. Our learning algorithm alleviates problems with local minima through a smooth critic function, avoids vanish-ing/exploding gradients through a truncated learning window, and allows many physical environments to be run in parallel. We evaluate our method on classical RL control tasks, and show substantial improvements in sample efﬁciency and wall-clock time over state-of-the-art RL and differentiable simulation-based algorithms. In addition, we demonstrate the scalability of our method by applying it to the challenging high-dimensional problem of muscle-actuated locomotion with a large action space, achieving a greater than 17 × reduction in training time over the best-performing established RL algorithm. More visual results are provided at: https://short-horizon-actor-critic.github.io/",
    "volume": "main",
    "checked": true,
    "id": "efbc2c6306ff1f3bfa282fc62f8467764fd41c25",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=givsRXsOt9r": {
    "title": "Spherical Message Passing for 3D Molecular Graphs",
    "abstract": "We consider representation learning of 3D molecular graphs in which each atom is associated with a spatial position in 3D. This is an under-explored area of research, and a principled message passing framework is currently lacking. In this work, we conduct analyses in the spherical coordinate system (SCS) for the complete identification of 3D graph structures. Based on such observations, we propose the spherical message passing (SMP) as a novel and powerful scheme for 3D molecular learning. SMP dramatically reduces training complexity, enabling it to perform efficiently on large-scale molecules. In addition, SMP is capable of distinguishing almost all molecular structures, and the uncovered cases may not exist in practice. Based on meaningful physically-based representations of 3D information, we further propose the SphereNet for 3D molecular learning. Experimental results demonstrate that the use of meaningful 3D information in SphereNet leads to significant performance improvements in prediction tasks. Our results also demonstrate the advantages of SphereNet in terms of capability, efficiency, and scalability. Our code is publicly available as part of the DIG library (https://github.com/divelab/DIG)",
    "volume": "main",
    "checked": true,
    "id": "d6dfd7548e85435880e252627dbe9a45432236f4",
    "citation_count": 26
  },
  "https://openreview.net/forum?id=eW5R4Cek6y6": {
    "title": "On Predicting Generalization using GANs",
    "abstract": "Research on generalization bounds for deep networks seeks to give ways to predict test error using just the training dataset and the network parameters. While generalization bounds can give many insights about architecture design, training algorithms etc., what they do not currently do is yield good predictions for actual test error. A recently introduced Predicting Generalization in Deep Learning competition (Jiang et al., 2020) aims to encourage discovery of methods to better predict test error. The current paper investigates a simple idea: can test error be predicted using synthetic data, produced using a Generative Adversarial Network (GAN) that was trained on the same training dataset? Upon investigating several GAN models and architectures, we find that this turns out to be the case. In fact, using GANs pre-trained on standard datasets, the test error can be predicted without requiring any additional hyperparameter tuning. This result is surprising because GANs have well-known limitations (e.g. mode collapse) and are known to not learn the data distribution accurately. Yet the generated samples are good enough to substitute for test data. Several additional experiments are presented to explore reasons why GANs do well at this task. In addition to a new approach for predicting generalization, the counter-intuitive phenomena presented in our work may also call for a better understanding of GANs' strengths and limitations",
    "volume": "main",
    "checked": true,
    "id": "0951c005254b3947be88bacadaa4f9c5c1806ead",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=l5aSHXi8jG5": {
    "title": "Demystifying Limited Adversarial Transferability in Automatic Speech Recognition Systems",
    "abstract": "The targeted transferability of adversarial samples enables attackers to exploit black-box models in the real world. Optimization attacks are the most popular means of producing such transferable samples. This is because these samples have high levels of transferability in some domains. However, recent research has shown that samples from these attacks do not transfer when applied to Automatic Speech Recognition systems (ASRs). In this paper, we study this phenomenon, perform exhaustive experiments, and identify the factors that are preventing transferability in ASRs. To do so, we perform an ablation study on each stage of the ASR pipeline. We dis-cover and quantify six factors (i.e., input type, MFCC, RNN, output type, and vocabulary and sequence sizes) that impact the targeted transferability of optimization attacks against ASRs. Our ﬁndings can be leveraged to design ASRs that are more robust to other transferable attack types (e.g., signal processing attacks), or to modify architectures in other domains to reduce their vulnerability to targeted transferability",
    "volume": "main",
    "checked": true,
    "id": "d4a548aefe19fe788fa89641c5eeea026c3a8078",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=V3C8p78sDa": {
    "title": "Exploring the Limits of Large Scale Pre-training",
    "abstract": "Recent developments in large-scale machine learning suggest that by scaling up data, model size and training time properly, one might observe that improvements in pre-training would transfer favorably to most downstream tasks. In this work, we systematically study this phenomena and establish that, as we increase the upstream accuracy, the performance of downstream tasks saturates. In particular, we investigate more than 4800 experiments on Vision Transformers, MLP-Mixers and ResNets with number of parameters ranging from ten million to ten billion, trained on the largest scale of available image data (JFT, ImageNet21K) and evaluated on more than 20 downstream image recognition tasks. We propose a model for downstream performance that reflects the saturation phenomena and captures the nonlinear relationship in performance of upstream and downstream tasks. Delving deeper to understand the reasons that give rise to these phenomena, we show that the saturation behavior we observe is closely related to the way that representations evolve through the layers of the models. We showcase an even more extreme scenario where performance on upstream and downstream are at odds with each other. That is, to have a better downstream performance, we need to hurt upstream accuracy",
    "volume": "main",
    "checked": true,
    "id": "c206a6e7f51f5e1b6bfc479a174b66ad88ada2db",
    "citation_count": 41
  },
  "https://openreview.net/forum?id=Nh7CtbyoqV5": {
    "title": "Normalization of Language Embeddings for Cross-Lingual Alignment",
    "abstract": "Learning a good transfer function to map the word vectors from two languages 1 into a shared cross-lingual word vector space plays a crucial role in cross-lingual 2 NLP. It is useful in translation tasks and important in allowing complex models 3 built on a high-resource language like English to be directly applied on an aligned 4 low resource language. While Procrustes and other techniques can align language 5 models with some success, it has recently been identiﬁed that structural differences 6 (for instance, due to differing word frequency) create different proﬁles for various 7 monolingual embedding. When these proﬁles differ across languages, it corre- 8 lates with how well languages can align and their performance on cross-lingual 9 downstream tasks. In this work, we develop a very general language embedding 10 normalization procedure, building and subsuming various previous approaches, 11 which removes these structural proﬁles across languages without destroying their 12 intrinsic meaning. We demonstrate that meaning is retained and alignment is 13 improved on similarity, translation, and cross-language classiﬁcation tasks. Our 14 proposed normalization clearly outperforms all prior approaches like centering and 15 vector normalization on each task and with each alignment approach. 16 19 1 all are to 200K most frequent words",
    "volume": "main",
    "checked": true,
    "id": "1b9b1ea97a421cad079c87540d1b2bf1a24ee7af",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=AwgtcUAhBq": {
    "title": "Domain Adversarial Training: A Game Perspective",
    "abstract": "The dominant line of work in domain adaptation has focused on learning invariant representations using domain-adversarial training. In this paper, we interpret this approach from a game theoretical perspective . Deﬁning optimal solutions in domain-adversarial training as local Nash equilibria, we show that gradient descent in domain-adversarial training can violate the asymptotic convergence guarantees of the optimizer, oftentimes hindering the transfer performance. Our analysis leads us to replace gradient descent with high-order ODE solvers (i.e., Runge–Kutta), for which we derive asymptotic convergence guarantees. This family of optimizers is signiﬁcantly more stable and allows more aggressive learning rates, leading to high performance gains when used as a drop-in replacement over standard optimizers. Our experiments show that in conjunction with state-of-the-art domain-adversarial methods, we achieve up to 3.5% improvement with less than half of training iterations. Our optimizers are easy to implement, free of additional parameters, and can be plugged into any domain-adversarial framework",
    "volume": "main",
    "checked": true,
    "id": "ffeaa70ae5e1280bafb29cc2d9fe80aae340eb70",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=hm2tNDdgaFK": {
    "title": "Learning 3D Representations of Molecular Chirality with Invariance to Bond Rotations",
    "abstract": "Molecular chirality, a form of stereochemistry most often describing relative spatial arrangements of bonded neighbors around tetrahedral carbon centers, influences the set of 3D conformers accessible to the molecule without changing its 2D graph connectivity. Chirality can strongly alter (bio)chemical interactions, particularly protein-drug binding. Most 2D graph neural networks (GNNs) designed for molecular property prediction at best use atomic labels to naı̈vely treat chirality, while E(3)-invariant 3D GNNs are invariant to chirality altogether. To enable representation learning on molecules with defined stereochemistry, we design an SE(3)-invariant model that processes torsion angles of a 3D molecular conformer. We explicitly model conformational flexibility by integrating a novel type of invariance to rotations about internal molecular bonds into the architecture, mitigating the need for multi-conformer data augmentation. We test our model on four benchmarks: contrastive learning to distinguish conformers of different stereoisomers in a learned latent space, classification of chiral centers as R/S, prediction of how enantiomers rotate circularly polarized light, and ranking enantiomers by their docking scores in an enantiosensitive protein pocket. We compare our model, Chiral InterRoto-Invariant Neural Network (ChIRo), with 2D and 3D GNNs to demonstrate that our model achieves state of the art performance when learning chiral-sensitive functions from molecular structures",
    "volume": "main",
    "checked": true,
    "id": "a5eb31131ec807648d6b61eacbce5b2deb0d3727",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=xNOVfCCvDpM": {
    "title": "Post hoc Explanations may be Ineffective for Detecting Unknown Spurious Correlation",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "e3228e30641f5ed04f5f1861909b3f2b20582301",
    "citation_count": 20
  },
  "https://openreview.net/forum?id=lQI_mZjvBxj": {
    "title": "Towards Model Agnostic Federated Learning Using Knowledge Distillation",
    "abstract": "Is it possible to design an universal API for federated learning using which an ad-hoc group of data-holders (agents) collaborate with each other and perform federated learning? Such an API would necessarily need to be model-agnostic i.e. make no assumption about the model architecture being used by the agents, and also cannot rely on having representative public data at hand. Knowledge distillation (KD) is the obvious tool of choice to design such protocols. However, surprisingly, we show that most natural KD-based federated learning protocols have poor performance. To investigate this, we propose a new theoretical framework, Federated Kernel ridge regression, which can capture both model heterogeneity as well as data heterogeneity. Our analysis shows that the degradation is largely due to a fundamental limitation of knowledge distillation under data heterogeneity. We further validate our framework by analyzing and designing new protocols based on KD. Their performance on real world experiments using neural networks, though still unsatisfactory, closely matches our theoretical predictions",
    "volume": "main",
    "checked": true,
    "id": "e96a94b7494b70685d6c3658018da900d82a9278",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=HMJdXzbWKH": {
    "title": "Online Target Q-learning with Reverse Experience Replay: Efficiently finding the Optimal Policy for Linear MDPs",
    "abstract": "Q-learning is a popular Reinforcement Learning (RL) algorithm which is widely used in practice with function approximation (Mnih et al., 2015). In contrast, existing theoretical results are pessimistic about Q-learning. For example, (Baird, 1995) shows that Q-learning does not converge even with linear function approximation for linear MDPs. Furthermore, even for tabular MDPs with synchronous updates, Q-learning was shown to have sub-optimal sample complexity (Li et al., 2021; Azar et al., 2013). The goal of this work is to bridge the gap between practical success of Q-learning and the relatively pessimistic theoretical results. The starting point of our work is the observation that in practice, Q-learning is used with two important modifications: (i) training with two networks, called online network and target network simultaneously (online target learning, or OTL) , and (ii) experience replay (ER) (Mnih et al., 2015). While they have been observed to play a significant role in the practical success of Q-learning, a thorough theoretical understanding of how these two modifications improve the convergence behavior of Q-learning has been missing in literature. By carefully combining Q-learning with OTL and reverse experience replay (RER) (a form of experience replay), we present novel methods Q-Rex and Q-RexDaRe (Q-Rex+ data reuse). We show that Q-Rex efficiently finds the optimal policy for linear MDPs (or more generally for MDPs with zero inherent Bellman error with linear approximation (ZIBEL)) and provide non-asymptotic bounds on sample complexity – the first such result for a Q-learning method for this class of MDPs under standard assumptions. Furthermore, we demonstrate that Q-RexDaRe in fact achieves near optimal sample complexity in the tabular setting, improving upon the existing results for vanilla Q-learning",
    "volume": "main",
    "checked": true,
    "id": "e410c3d2b6d7a333b1ca5a2f97f76d41ad5f4914",
    "citation_count": 13
  },
  "https://openreview.net/forum?id=P-pPW1nxf1r": {
    "title": "HTLM: Hyper-Text Pre-Training and Prompting of Language Models",
    "abstract": "We introduce HTLM , a hyper-text language model trained on a large-scale web crawl. Modeling hyper-text has a number of advantages: (1) it is easily gathered at scale, (2) it provides rich document-level and end-task-adjacent supervision (e.g. class and id attributes often encode document category infor-mation), and (3) it allows for new structured prompting that follows the established semantics of HTML (e.g. to do zero-shot summarization by inﬁlling <title> tags for a webpage that contains the input text). We show that pretraining with a BART-style denoising loss directly on simpliﬁed HTML provides highly effective transfer for a wide range of end tasks and supervision levels. HTLM matches or exceeds the performance of comparably sized text-only LMs for zero-shot prompting and ﬁne-tuning for classiﬁcation benchmarks, while also setting new state-of-the-art performance levels for zero-shot summarization. We also ﬁnd that hyper-text prompts provide more value to HTLM , in terms of data efﬁciency, than plain text prompts do for existing LMs, and that HTLM is highly effective at autoprompting itself, by simply generating the most likely hyper-text formatting for any available training data. We will release all code and models to support future HTLM research",
    "volume": "main",
    "checked": true,
    "id": "e596b8adbffa546dbc163e817fb3de72744ec4f6",
    "citation_count": 34
  },
  "https://openreview.net/forum?id=w60btE_8T2m": {
    "title": "Spanning Tree-based Graph Generation for Molecules",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "2761e45bb1901c3723cd7af9fa9f6deb65ff22fc",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=g5ynW-jMq4M": {
    "title": "Properties from mechanisms: an equivariance perspective on identifiable representation learning",
    "abstract": "A key goal of unsupervised representation learning is \"inverting\" a data generating process to recover its latent properties. Existing work that provably achieves this goal relies on strong assumptions on relationships between the latent variables (e.g., independence conditional on auxiliary information). In this paper, we take a very different perspective on the problem and ask, \"Can we instead identify latent properties by leveraging knowledge of the mechanisms that govern their evolution?\" We provide a complete characterization of the sources of non-identifiability as we vary knowledge about a set of possible mechanisms. In particular, we prove that if we know the exact mechanisms under which the latent properties evolve, then identification can be achieved up to any equivariances that are shared by the underlying mechanisms. We generalize this characterization to settings where we only know some hypothesis class over possible mechanisms, as well as settings where the mechanisms are stochastic. We demonstrate the power of this mechanism-based perspective by showing that we can leverage our results to generalize existing identifiable representation learning results. These results suggest that by exploiting inductive biases on mechanisms, it is possible to design a range of new identifiable representation learning approaches",
    "volume": "main",
    "checked": true,
    "id": "bb12ccf1860719560eb41a5e69ce420de6438c57",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=boJy41J-tnQ": {
    "title": "Subspace Regularizers for Few-Shot Class Incremental Learning",
    "abstract": "Few-shot class incremental learning—the problem of updating a trained classifier to discriminate among an expanded set of classes with limited labeled data—is a key challenge for machine learning systems deployed in non-stationary environments. Existing approaches to the problem rely on complex model architectures and training procedures that are difficult to tune and re-use. In this paper, we present an extremely simple approach that enables the use of ordinary logistic regression classifiers for few-shot incremental learning. The key to this approach is a new family of subspace regularization schemes that encourage weight vectors for new classes to lie close to the subspace spanned by the weights of existing classes. When combined with pretrained convolutional feature extractors, logistic regression models trained with subspace regularization outperform specialized, state-of-the-art approaches to few-shot incremental image classification by up to 22% on the miniImageNet dataset. Because of its simplicity, subspace regularization can be straightforwardly extended to incorporate additional background information about the new classes (including class names and descriptions specified in natural language); these further improve accuracy by up to 2%. Our results show that simple geometric regularization of class representations offers an effective tool for continual learning.1",
    "volume": "main",
    "checked": true,
    "id": "f8d77d2d33a86a02c7d2ddd3dcbb1dc48ccf265c",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=qwULHx9zld": {
    "title": "Random matrices in service of ML footprint: ternary random features with no performance loss",
    "abstract": "In this article, we investigate the spectral behavior of random features kernel matrices of the type K = E w [ σ ( w T x i ) σ ( w T x j )] ni,j =1 , with nonlinear function σ ( · ) , data x 1 , . . . , x n ∈ R p , and random projection vector w ∈ R p having i.i.d. entries. In a high-dimensional setting where the number of data n and their dimension p are both large and comparable, we show, under a Gaussian mixture model for the data, that the eigenspectrum of K is independent of the distribution of the i.i.d. (zero-mean and unit-variance) entries of w and only depends on σ ( · ) via its (generalized) Gaussian moments E z ∼N (0 , 1) [ σ ′ ( z )] and E z ∼N (0 , 1) [ σ ′′ ( z )] . As a result, for any kernel matrix K of the form above, we propose a novel random features technique, called Ternary Random Feature (TRF), that (i) asymptotically yields the same limiting kernel as the original K in a spectral sense and (ii) can be computed and stored much more efﬁciently, by wisely tuning (in a data-dependent counterpart expensive kernels. Our article comes along with (Couillet et al., 2021) as ﬁrst steps in re-designing machine learning algorithms using Random Matrix Theory, in order to be able to perform computations on massive data using desktop computers instead of relying on energy consuming giant servers",
    "volume": "main",
    "checked": true,
    "id": "eb1164159adfcfe421587fdcf01a946f4da1c0ff",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=4rLw09TgRw9": {
    "title": "Query Embedding on Hyper-Relational Knowledge Graphs",
    "abstract": "Multi-hop logical reasoning is an established problem in the field of representation learning on knowledge graphs (KGs). It subsumes both one-hop link prediction as well as other more complex types of logical queries. Existing algorithms operate only on classical, triple-based graphs, whereas modern KGs often employ a hyperrelational modeling paradigm. In this paradigm, typed edges may have several key-value pairs known as qualifiers that provide fine-grained context for facts. In queries, this context modifies the meaning of relations, and usually reduces the answer set. Hyper-relational queries are often observed in real-world KG applications, and existing approaches for approximate query answering cannot make use of qualifier pairs. In this work, we bridge this gap and extend the multi-hop reasoning problem to hyper-relational KGs allowing to tackle this new type of complex queries. Building upon recent advancements in Graph Neural Networks and query embedding techniques, we study how to embed and answer hyper-relational conjunctive queries. Besides that, we propose a method to answer such queries and demonstrate in our experiments that qualifiers improve query answering on a diverse set of query patterns",
    "volume": "main",
    "checked": true,
    "id": "1c652779d9b6a0700386295a428f21293d7b6d5b",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=AIgn9uwfcD1": {
    "title": "Prospect Pruning: Finding Trainable Weights at Initialization using Meta-Gradients",
    "abstract": "Pruning neural networks at initialization would enable us to find sparse models that retain the accuracy of the original network while consuming fewer computational resources for training and inference. However, current methods are insufficient to enable this optimization and lead to a large degradation in model performance. In this paper, we identify a fundamental limitation in the formulation of current methods, namely that their saliency criteria look at a single step at the start of training without taking into account the trainability of the network. While pruning iteratively and gradually has been shown to improve pruning performance, explicit consideration of the training stage that will immediately follow pruning has so far been absent from the computation of the saliency criterion. To overcome the short-sightedness of existing methods, we propose Prospect Pruning (ProsPr), which uses meta-gradients through the first few steps of optimization to determine which weights to prune. ProsPr combines an estimate of the higherorder effects of pruning on the loss and the optimization trajectory to identify the trainable sub-network. Our method achieves state-of-the-art pruning performance on a variety of vision classification tasks, with less data and in a single shot compared to existing pruning-at-initialization methods. Our code is available online at https://github.com/mil-ad/prospr",
    "volume": "main",
    "checked": true,
    "id": "be1210aa1ddbe7d6a654045a5aabbdc2a4827e6f",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=d_2lcDh0Y9c": {
    "title": "DriPP: Driven Point Processes to Model Stimuli Induced Patterns in M/EEG Signals",
    "abstract": "The quantitative analysis of non-invasive electrophysiology signals from electroencephalography (EEG) and magnetoencephalography (MEG) boils down to the identiﬁcation of temporal patterns such as evoked responses, transient bursts of neural oscillations but also blinks or heartbeats for data cleaning. Several works have shown that these patterns can be extracted efﬁciently in an unsupervised way, e.g., using Convolutional Dictionary Learning. This leads to an event-based description of the data. Given these events, a natural question is to estimate how their occurrences are modulated by certain cognitive tasks and experimental manipulations. To address it, we propose a point process approach. While point processes have been used in neuroscience in the past, in particular for single cell recordings (spike trains), techniques such as Convolutional Dictionary Learning make them amenable to human studies based on EEG/MEG signals. We develop a novel statistical point process model – called driven temporal point processes (DriPP) – where the intensity function of the point process model is linked to a set of point processes corresponding to stimulation events. We derive a fast and principled expectation-maximization (EM) algorithm to estimate the parameters of this model. Simulations reveal that model parameters can be identiﬁed from long enough signals. Results on standard MEG datasets demonstrate that our methodology reveals event-related neural responses – both evoked and induced – and isolates non-task-speciﬁc temporal patterns. datasets, gradiometer The in the CDL. CDL alphacsc GreedyCDL For the sample dataset, 40 atoms of duration 1 are and for the somato dataset, 20 of 0 . activations a of (resp. somato times of the to of 400 and initialization",
    "volume": "main",
    "checked": true,
    "id": "b4a18dd21bb5cee2a9296eb09e863bccf5af950e",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=7IWGzQ6gZ1D": {
    "title": "Constructing a Good Behavior Basis for Transfer using Generalized Policy Updates",
    "abstract": "We study the problem of learning a good set of policies, so that when combined together, they can solve a wide variety of unseen reinforcement learning tasks with no or very little new data. Specifically, we consider the framework of generalized policy evaluation and improvement, in which the rewards for all tasks of interest are assumed to be expressible as a linear combination of a fixed set of features. We show theoretically that, under certain assumptions, having access to a specific set of diverse policies, which we call a set of independent policies, can allow for instantaneously achieving high-level performance on all possible downstream tasks which are typically more complex than the ones on which the agent was trained. Based on this theoretical analysis, we propose a simple algorithm that iteratively constructs this set of policies. In addition to empirically validating our theoretical results, we compare our approach with recently proposed diverse policy set construction methods and show that, while others fail, our approach is able to build a behavior basis that enables instantaneous transfer to all possible downstream tasks. We also show empirically that having access to a set of independent policies can better bootstrap the learning process on downstream tasks where the new reward function cannot be described as a linear combination of the features. Finally, we demonstrate how this policy set can be useful in a lifelong reinforcement learning setting",
    "volume": "main",
    "checked": true,
    "id": "46168a5ff72e9f9e0c98e3d63f5758f475af5183",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=tgcAoUVHRIB": {
    "title": "Neural Methods for Logical Reasoning over Knowledge Graphs",
    "abstract": "Reasoning is a fundamental problem for computers and deeply studied in Artiﬁcial Intelligence. In this paper, we speciﬁcally focus on answering multi-hop logical queries on Knowledge Graphs (KGs). This is a complicated task because, in real-world scenarios, the graphs tend to be large and incomplete. Most previous works have been unable to create models that accept full First-Order Logical (FOL) queries, which include negative queries, and have only been able to process a limited set of query structures. Additionally, most methods present logic operators that can only perform the logical operation they are made for. We introduce a set of models that use Neural Networks to create one-point vector embeddings to answer the queries. The versatility of neural networks allows the framework to handle FOL queries with Conjunction ( ∧ ), Disjunction ( ∨ ) and Negation ( ¬ ) operators. We demonstrate experimentally the performance of our model through extensive experimentation on well-known benchmarking datasets. Besides having more versatile operators, the models achieve a 10% relative increase over the best performing state of the art and more than 30% over the original method based on single-point vector embeddings",
    "volume": "main",
    "checked": true,
    "id": "2d80d0b053179988f2155ea9eaf57b60a7742c16",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=FmBegXJToY": {
    "title": "Procedural generalization by planning with self-supervised world models",
    "abstract": "One of the key promises of model-based reinforcement learning is the ability to generalize using an internal model of the world to make predictions in novel environments and tasks. However, the generalization ability of model-based agents is not well understood because existing work has focused on model-free agents when benchmarking generalization. Here, we explicitly measure the generalization ability of model-based agents in comparison to their model-free counterparts. We focus our analysis on MuZero [60], a powerful model-based agent, and evaluate its performance on both procedural and task generalization. We identify three factors of procedural generalization—planning, self-supervised representation learning, and procedural data diversity—and show that by combining these techniques, we achieve state-of-the art generalization performance and data efficiency on Procgen [9]. However, we find that these factors do not always provide the same benefits for the task generalization benchmarks in Meta-World [74], indicating that transfer remains a challenge and may require different approaches than procedural generalization. Overall, we suggest that building generalizable agents requires moving beyond the single-task, model-free paradigm and towards self-supervised model-based agents that are trained in rich, procedural, multi-task environments",
    "volume": "main",
    "checked": true,
    "id": "44164c068499fbe387a1765104d69a8cbc5f0327",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=2f1z55GVQN": {
    "title": "Critical Points in Quantum Generative Models",
    "abstract": "One of the most important properties of neural networks is the clustering of local minima of the loss function near the global minimum, enabling efficient training. Though generative models implemented on quantum computers are known to be more expressive than their traditional counterparts, it has empirically been observed that these models experience a transition in the quality of their local minima. Namely, below some critical number of parameters, all local minima are far from the global minimum in function value; above this critical parameter count, all local minima are good approximators of the global minimum. Furthermore, for a certain class of quantum generative models, this transition has empirically been observed to occur at parameter counts exponentially large in the problem size, meaning practical training of these models is out of reach. Here, we give the first proof of this transition in trainability, specializing to this latter class of quantum generative models. We use techniques inspired by those used to study the loss landscapes of classical neural networks. We also verify that our analytic results hold experimentally even at modest model sizes",
    "volume": "main",
    "checked": true,
    "id": "ec8b14c76481364249451d8aea4df983c647f239",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=X6D9bAHhBQ1": {
    "title": "Planning in Stochastic Environments with a Learned Model",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "899b22558b417e5afbdf013963a393d3daf2dabc",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=8uz0EWPQIMu": {
    "title": "On the Pitfalls of Analyzing Individual Neurons in Language Models",
    "abstract": "results imply they may get better",
    "volume": "main",
    "checked": true,
    "id": "aaf3ebaf12baeb366ce6ff32aa36d608a7eab583",
    "citation_count": 14
  },
  "https://openreview.net/forum?id=T4-65DNlDij": {
    "title": "Deep Attentive Variational Inference",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "d52031ffd5d5b6704752a7dc051dbc04ea47e712",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=uxxFrDwrE7Y": {
    "title": "Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System",
    "abstract": "Humans excel at continually learning from an ever-changing environment whereas it remains a challenge for deep neural networks which exhibit catastrophic forgetting. The complementary learning system (CLS) theory suggests that the interplay between rapid instance-based learning and slow structured learning in the brain is crucial for accumulating and retaining knowledge. Here, we propose CLS-ER, a novel dual memory experience replay (ER) method which maintains short-term and long-term semantic memories that interact with the episodic memory. Our method employs an effective replay mechanism whereby new knowledge is acquired while aligning the decision boundaries with the semantic memories. CLS-ER does not utilize the task boundaries or make any assumption about the distribution of the data which makes it versatile and suited for \"general continual learning\". Our approach achieves state-of-the-art performance on standard bench-marks as well as more realistic general continual learning settings. 1",
    "volume": "main",
    "checked": true,
    "id": "0138e7d5bfb9b47106d9a3e8821820fb53964956",
    "citation_count": 12
  },
  "https://openreview.net/forum?id=3PN4iyXBeF": {
    "title": "Amortized Implicit Differentiation for Stochastic Bilevel Optimization",
    "abstract": "We study a class of algorithms for solving bilevel optimization problems in both stochastic and deterministic settings when the inner-level objective is strongly convex. Speciﬁcally, we consider algorithms based on inexact implicit differentiation and we exploit a warm-start strategy to amortize the estimation of the exact gradient. We then introduce a uniﬁed theoretical framework inspired by the study of singularly perturbed systems (Habets, 1974) to analyze such amortized algorithms. By using this framework, our analysis shows these algorithms to match the computational complexity of oracle methods that have access to an unbiased estimate of the gradient, thus outperforming many existing results for bilevel optimization. We illustrate these ﬁndings on synthetic experiments and demonstrate the efﬁciency of these algorithms on hyper-parameter optimization experiments involving several thousands of variables",
    "volume": "main",
    "checked": true,
    "id": "dab2cd1db9f2182d77df4d6fa689f37e93b506a7",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=Vzh1BFUCiIX": {
    "title": "ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning",
    "abstract": "Despite the recent success of multi-task learning and transfer learning for natural language processing (NLP), few works have systematically studied the effect of scaling up the number of tasks during pre-training. Towards this goal, this paper introduces EXMIX (Extreme Mixture): a massive collection of 107 supervised NLP tasks across diverse domains and task-families. Using EXMIX, we study the effect of multi-task pre-training at the largest scale to date, and analyze cotraining transfer amongst common families of tasks. Through this analysis, we show that manually curating an ideal set of tasks for multi-task pre-training is not straightforward, and that multi-task scaling can vastly improve models on its own. Finally, we propose EXT5: a model pre-trained using a multi-task objective of self-supervised span denoising and supervised EXMIX. Via extensive experiments, we show that EXT5 outperforms strong T5 baselines on SuperGLUE, GEM, Rainbow, Closed-Book QA tasks, and several tasks outside of EXMIX. EXT5 also significantly improves sample efficiency while pre-training",
    "volume": "main",
    "checked": true,
    "id": "cbf98ebe967e0f3f3236e7932f37013b98244e94",
    "citation_count": 65
  },
  "https://openreview.net/forum?id=PRZoSmCinhf": {
    "title": "Constrained Policy Optimization via Bayesian World Models",
    "abstract": "Improving sample-efficiency and safety are crucial challenges when deploying reinforcement learning in high-stakes real world applications. We propose LAMBDA, a novel model-based approach for policy optimization in safety critical tasks modeled via constrained Markov decision processes. Our approach utilizes Bayesian world models, and harnesses the resulting uncertainty to maximize optimistic upper bounds on the task objective, as well as pessimistic upper bounds on the safety constraints. We demonstrate LAMBDA's state of the art performance on the Safety-Gym benchmark suite in terms of sample efficiency and constraint violation",
    "volume": "main",
    "checked": true,
    "id": "f1e48bfb4464fedb94ced2d85b74991efcfe2856",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=RXQ-FPbQYVn": {
    "title": "Anti-Concentrated Confidence Bonuses For Scalable Exploration",
    "abstract": "Intrinsic rewards play a central role in handling the exploration-exploitation trade-off when designing sequential decision-making algorithms, in both foundational theory and state-of-the-art deep reinforcement learning. The LinUCB algorithm, a centerpiece of the stochastic linear bandits literature, prescribes an elliptical bonus which addresses the challenge of leveraging shared information in large action spaces. This bonus scheme cannot be directly transferred to high-dimensional exploration problems, however, due to the computational cost of maintaining the inverse covariance matrix of action features. We introduce anti-concentrated conﬁdence bounds for efﬁciently approximating the elliptical bonus, using an ensemble of regressors trained to predict random noise from policy network-derived features. Using this approximation, we obtain stochastic linear bandit algorithms which obtain ˜ O ( d √ T ) regret bounds for poly( d ) ﬁxed actions. We develop a practical variant for deep reinforcement learning that is competitive with contemporary intrinsic reward heuristics on Atari benchmarks",
    "volume": "main",
    "checked": true,
    "id": "c4d91bca6066282da421671ab06ac1e156f19851",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=1NvflqAdoom": {
    "title": "Neural Networks as Kernel Learners: The Silent Alignment Effect",
    "abstract": "Neural networks in the lazy training regime converge to kernel machines. Can neural networks in the rich feature learning regime learn a kernel machine with a data-dependent kernel? We demonstrate that this can indeed happen due to a phenomenon we term silent alignment, which requires that the tangent kernel of a network evolves in eigenstructure while small and before the loss appreciably decreases, and grows only in overall scale afterwards. We show that such an effect takes place in homogenous neural networks with small initialization and whitened data. We provide an analytical treatment of this effect in the linear network case. In general, we find that the kernel develops a low-rank contribution in the early phase of training, and then evolves in overall scale, yielding a function equivalent to a kernel regression solution with the final network's tangent kernel. The early spectral learning of the kernel depends on both depth and on relative learning rates in each layer. We also demonstrate that non-whitened data can weaken the silent alignment effect",
    "volume": "main",
    "checked": true,
    "id": "ccd3631a4509aac2d71c320a6ac677f311d94b05",
    "citation_count": 14
  },
  "https://openreview.net/forum?id=ZDaSIkWT-AP": {
    "title": "Case-based reasoning for better generalization in textual reinforcement learning",
    "abstract": "Text-based games (TBG) have emerged as promising environments for driving research in grounded language understanding and studying problems like generalization and sample efﬁciency. Several deep reinforcement learning (RL) methods with varying architectures and learning schemes have been proposed for TBGs. However, these methods fail to generalize efﬁciently, especially under distributional shifts. In a departure from deep RL approaches, in this paper, we propose a general method inspired by case-based reasoning to train agents and generalize out of the training distribution. The case-based reasoner collects instances of positive experiences from the agent's interaction with the world in the past and later reuses the collected experiences to act efﬁciently. The method can be applied in conjunction with any existing on-policy neural agent in the literature for TBGs. Our experiments show that the proposed approach consistently improves existing methods, obtains good out-of-distribution generalization, and achieves new state-of-the-art results on widely used environments. a successful action. shows that, for the out-of-distribution games, the neural agent would struggle to select good actions when the CBR is used",
    "volume": "main",
    "checked": true,
    "id": "18e0cdc75e017b6112d674c3bd0ac5b3e35e4f82",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=zrW-LVXj2k1": {
    "title": "On the benefits of maximum likelihood estimation for Regression and Forecasting",
    "abstract": "We advocate for a practical Maximum Likelihood Estimation (MLE) approach towards designing loss functions for regression and forecasting, as an alternative to the typical approach of direct empirical risk minimization on a speciﬁc target metric. The MLE approach is better suited to capture inductive biases such as prior domain knowledge in datasets, and can output post-hoc estimators at inference time that can optimize different types of target metrics. We present theoretical results to demonstrate that our approach is competitive with any estimator for the target metric under some general conditions. In two example practical settings, Poisson and Pareto regression, we show that our competitive results can be used to prove that the MLE approach has better excess risk bounds than directly minimizing the target metric. We also demonstrate empirically that our method instantiated with a well-designed general purpose mixture likelihood family can obtain superior performance for a variety of tasks across time-series forecasting and regression datasets with different data distributions",
    "volume": "main",
    "checked": true,
    "id": "f4748a79d1228973a82119e2a8e5159db818be6a",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=B7ZbqNLDn-_": {
    "title": "Recycling Model Updates in Federated Learning: Are Gradient Subspaces Low-Rank?",
    "abstract": "In this paper, we question the rationale behind propagating large numbers of parameters through a distributed system during federated learning. We start by examining the rank characteristics of the subspace spanned by gradients across epochs (i.e., the gradient-space) in centralized model training, and observe that this gradient-space often consists of a few leading principal components accounting for an overwhelming majority ( 95 − 99% ) of the explained variance. Motivated by this, we propose the \"Look-back Gradient Multiplier\" ( LBGM ) algorithm, which exploits this low-rank property to enable gradient recycling between model update rounds of federated learning, reducing transmissions of large parameters to single scalars for aggregation. We analytically characterize the convergence behavior of LBGM , revealing the nature of the trade-off between communication savings and model performance. Our subsequent experimental results demonstrate the improvement LBGM obtains in communication overhead compared to conventional federated learning on several datasets and deep learning models. Additionally, we show that LBGM is a general plug-and-play algorithm that can be used standalone or stacked on top of existing sparsification techniques for distributed model training",
    "volume": "main",
    "checked": true,
    "id": "4c99c73c14176159a6cd19e2366e98e87a9a2aa0",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=Qycd9j5Qp9J": {
    "title": "Understanding the Variance Collapse of SVGD in High Dimensions",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "9de2faee84881212450715bfabf8b91763e0adde",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=7grkzyj89A_": {
    "title": "Generalization Through the Lens of Leave-One-Out Error",
    "abstract": "Despite the tremendous empirical success of deep learning models to solve various learning tasks, our theoretical understanding of their generalization ability is very limited. Classical generalization bounds based on tools such as the VC dimension or Rademacher complexity, are so far unsuitable for deep models and it is doubtful that these techniques can yield tight bounds even in the most idealistic settings (Nagarajan & Kolter, 2019). In this work, we instead revisit the concept of leave-one-out (LOO) error to measure the generalization ability of deep models in the so-called kernel regime. While popular in statistics, the LOO error has been largely overlooked in the context of deep learning. By building upon the recently established connection between neural networks and kernel learning, we leverage the closed-form expression for the leave-one-out error, giving us access to an efficient proxy for the test error. We show both theoretically and empirically that the leave-one-out error is capable of capturing various phenomena in generalization theory, such as double descent, random labels or transfer learning. Our work therefore demonstrates that the leave-one-out error provides a tractable way to estimate the generalization ability of deep neural networks in the kernel regime, opening the door to potential, new research directions in the field of generalization",
    "volume": "main",
    "checked": true,
    "id": "36992d4df8216f7ad04e773a92f7cc5b6ba2d90e",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=CuV_qYkmKb3": {
    "title": "Scarf: Self-Supervised Contrastive Learning using Random Feature Corruption",
    "abstract": "Self-supervised contrastive representation learning has proved incredibly successful in the vision and natural language domains, enabling state-of-the-art performance with orders of magnitude less labeled data. However, such methods are domain-speciﬁc and little has been done to leverage this technique on real-world tabular datasets. We propose S CARF , a simple, widely-applicable technique for contrastive learning, where views are formed by corrupting a random subset of features. When applied to pre-train deep neural networks on the 69 real-world, tabular classiﬁcation datasets from the OpenML-CC18 benchmark, S CARF not only improves classiﬁcation accuracy in the fully-supervised setting but does so also in the presence of label noise and in the semi-supervised setting where only a fraction of the available training data is labeled. We show that S CARF complements existing strategies and outperforms alternatives like autoencoders. We conduct comprehensive ablations, detailing the importance of a range of factors",
    "volume": "main",
    "checked": true,
    "id": "08bd0ebcf5e0cd08a9748683692678c36dee9c07",
    "citation_count": 16
  },
  "https://openreview.net/forum?id=Y4cs1Z3HnqL": {
    "title": "Pessimistic Bootstrapping for Uncertainty-Driven Offline Reinforcement Learning",
    "abstract": "Offline Reinforcement Learning (RL) aims to learn policies from previously collected datasets without exploring the environment. Directly applying off-policy algorithms to offline RL usually fails due to the extrapolation error caused by the out-of-distribution (OOD) actions. Previous methods tackle such problems by penalizing the Q-values of OOD actions or constraining the trained policy to be close to the behavior policy. Nevertheless, such methods typically prevent the generalization of value functions beyond the offline data and also lack a precise characterization of OOD data. In this paper, we propose Pessimistic Bootstrapping for offline RL (PBRL), a purely uncertainty-driven offline algorithm without explicit policy constraints. Specifically, PBRL conducts uncertainty quantification via the disagreement of bootstrapped Q-functions, and performs pessimistic updates by penalizing the value function based on the estimated uncertainty. To tackle the extrapolating error, we further propose a novel OOD sampling method. We show that such OOD sampling and pessimistic bootstrapping yields a provable uncertainty quantifier in linear MDPs, thus providing the theoretical underpinning for PBRL. Extensive experiments on D4RL benchmark show that PBRL has better performance compared to the state-of-the-art algorithms",
    "volume": "main",
    "checked": true,
    "id": "23abe79046d7f7b430d5d21b6a93598d0aa1b9c2",
    "citation_count": 16
  },
  "https://openreview.net/forum?id=B0oHOwT5ENL": {
    "title": "Neural Deep Equilibrium Solvers",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "397912f6f31ab7099ab3ffa68645db131b32b158",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=Ht85_jyihxp": {
    "title": "Efficient and Differentiable Conformal Prediction with General Function Classes",
    "abstract": "Quantifying the data uncertainty in learning tasks is often done by learning a prediction interval or prediction set of the label given the input. Two commonly desired properties for learned prediction sets are valid coverage and good eﬃciency (such as low length or low car-dinality). Conformal prediction is a powerful technique for learning prediction sets with valid coverage, yet by default its conformalization step only learns a single parameter, and does not optimize the eﬃciency over more expressive function classes. In this paper, we propose a generalization of conformal prediction to multiple learnable parameters, by considering the constrained empirical risk minimization (ERM) problem of ﬁnding the most eﬃcient prediction set subject to valid empirical coverage. This meta-algorithm generalizes existing conformal prediction algorithms, and we show that it achieves approximate valid population coverage and near-optimal eﬃciency within class, whenever the function class in the conformalization step is low-capacity in a certain sense. Next, this ERM problem is challenging to optimize as it involves a non-diﬀerentiable coverage constraint. We develop a gradient-based algorithm for it by approximating the original constrained ERM using diﬀerentiable surrogate losses and Lagrangians. Experiments show that our algorithm is able to learn valid prediction sets and improve the eﬃciency signiﬁcantly over existing approaches in several applications such as prediction intervals with improved length, minimum-volume prediction sets for multi-output regression, and label prediction sets for image classiﬁcation",
    "volume": "main",
    "checked": true,
    "id": "5d0fab8771f3f564ba7f081ad8d597eb8d7d028d",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=fOsN52jn25l": {
    "title": "Dual Lottery Ticket Hypothesis",
    "abstract": "Fully exploiting the learning capacity of neural networks requires overparameterized dense networks. On the other side, directly training sparse neural networks typically results in unsatisfactory performance. Lottery Ticket Hypothesis (LTH) provides a novel view to investigate sparse network training and maintain its capacity. Concretely, it claims there exist winning tickets from a randomly initialized network found by iterative magnitude pruning and preserving promising trainability (or we say being in trainable condition). In this work, we regard the winning ticket from LTH as the subnetwork which is in trainable condition and its performance as our benchmark, then go from a complementary direction to articulate the Dual Lottery Ticket Hypothesis (DLTH): Randomly selected subnetworks from a randomly initialized dense network can be transformed into a trainable condition and achieve admirable performance compared with LTH — random tickets in a given lottery pool can be transformed into winning tickets. Specifically, by using uniform-randomly selected subnetworks to represent the general cases, we propose a simple sparse network training strategy, Random Sparse Network Transformation (RST), to substantiate our DLTH. Concretely, we introduce a regularization term to borrow learning capacity and realize information extrusion from the weights which will be masked. After finishing the transformation for the randomly selected subnetworks, we conduct the regular finetuning to evaluate the model using fair comparisons with LTH and other strong baselines. Extensive experiments on several public datasets and comparisons with competitive approaches validate our DLTH as well as the effectiveness of the proposed model RST. Our work is expected to pave a way for inspiring new research directions of sparse network training in the future. Our code is available at https://github.com/yueb17/DLTH",
    "volume": "main",
    "checked": true,
    "id": "94d353a313021f2237afb28d93965c4767263352",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=nwKXyFvaUm": {
    "title": "Diverse Client Selection for Federated Learning via Submodular Maximization",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "91a5751912175f3a9cdf754f774839a0489f076e",
    "citation_count": 13
  },
  "https://openreview.net/forum?id=f2lrIbGx3x7": {
    "title": "Bayesian Framework for Gradient Leakage",
    "abstract": "Federated learning is an established method for training machine learning models without sharing training data. However, recent work has shown that it cannot guarantee data privacy as shared gradients can still leak sensitive information. To formalize the problem of gradient leakage, we propose a theoretical framework that enables, for the first time, analysis of the Bayes optimal adversary phrased as an optimization problem. We demonstrate that existing leakage attacks can be seen as approximations of this optimal adversary with different assumptions on the probability distributions of the input data and gradients. Our experiments confirm the effectiveness of the Bayes optimal adversary when it has knowledge of the underlying distribution. Further, our experimental evaluation shows that several existing heuristic defenses are not effective against stronger attacks, especially early in the training process. Thus, our findings indicate that the construction of more effective defenses and their evaluation remains an open problem",
    "volume": "main",
    "checked": true,
    "id": "2e9064208fd23c998f67f79531346504c9cfc7f3",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=BrFIKuxrZE": {
    "title": "Fair Normalizing Flows",
    "abstract": "Fair representation learning is an attractive approach that promises fairness of downstream predictors by encoding sensitive data. Unfortunately, recent work has shown that strong adversarial predictors can still exhibit unfairness by recovering sensitive attributes from these representations. In this work, we present Fair Normalizing Flows (FNF), a new approach offering more rigorous fairness guarantees for learned representations. Specifically, we consider a practical setting where we can estimate the probability density for sensitive groups. The key idea is to model the encoder as a normalizing flow trained to minimize the statistical distance between the latent representations of different groups. The main advantage of FNF is that its exact likelihood computation allows us to obtain guarantees on the maximum unfairness of any potentially adversarial downstream predictor. We experimentally demonstrate the effectiveness of FNF in enforcing various group fairness notions, as well as other attractive properties such as interpretability and transfer learning, on a variety of challenging real-world datasets",
    "volume": "main",
    "checked": true,
    "id": "947cf3e0d055bcafe5848afe10fca4e2484da2f8",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=X_ch3VrNSRg": {
    "title": "EE-Net: Exploitation-Exploration Neural Networks in Contextual Bandits",
    "abstract": "In this paper, we propose a novel neural exploration strategy in contextual bandits, EE-Net, distinct from the standard UCB-based and TS-based approaches. Contextual multi-armed bandits have been studied for decades with various applications. To solve the exploitation-exploration tradeoff in bandits, there are three main techniques: epsilon-greedy, Thompson Sampling (TS), and Upper Conﬁdence Bound (UCB). In recent literature, linear contextual bandits have adopted ridge regression to estimate the reward function and combine it with TS or UCB strategies for exploration. However, this line of works explicitly assumes the reward is based on a linear function of arm vectors, which may not be true in real-world datasets. To overcome this challenge, a series of neural bandit algorithms have been proposed, where a neural network is used to learn the underlying reward function and TS or UCB are adapted for exploration. Instead of calculating a large-deviation based statistical bound for exploration like previous methods, we propose \"EE-Net\", a novel neural-based exploration strategy. In addition to using a neural network (Exploitation network) to learn the reward function, EE-Net uses another neural network (Exploration network) to adaptively learn potential gains compared to the currently estimated reward for exploration. Then, a decision-maker is constructed to combine the outputs from the Exploitation and Exploration networks. We prove that EE-Net can achieve O ( √ T log T ) regret and show that EE-Net outperforms existing linear and neural contextual bandit baselines on real-world datasets. advantages together, we propose the hybrid approach, EE-Net, achieving the best performance with strong stability",
    "volume": "main",
    "checked": true,
    "id": "64839c2bbfcf40d4ece312da523906d177d56c3c",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=HBsJNesj2S": {
    "title": "Neural Relational Inference with Node-Specific Information",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "0a9a6bb422e831b422d68dfa96d7683f2d7f41d6",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=sRZ3GhmegS": {
    "title": "CoBERL: Contrastive BERT for Reinforcement Learning",
    "abstract": "Many reinforcement learning (RL) agents require a large amount of experience to solve tasks. We propose Contrastive BERT for RL (COBERL), an agent that combines a new contrastive loss and a hybrid LSTM-transformer architecture to tackle the challenge of improving data efficiency. COBERL enables efficient and robust learning from pixels across a wide variety of domains. We use bidirectional masked prediction in combination with a generalization of a recent contrastive method to learn better representations for RL, without the need of hand engineered data augmentations. We find that COBERL consistently improves data efficiency across the full Atari suite, a set of control tasks and a challenging 3D environment, and often it also increases final score performance",
    "volume": "main",
    "checked": true,
    "id": "f908112125aaf7505237737b2f08e40dbdd9a110",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=5ECQL05ub0J": {
    "title": "Resonance in Weight Space: Covariate Shift Can Drive Divergence of SGD with Momentum",
    "abstract": "Most convergence guarantees for stochastic gradient descent with momentum (SGDm) rely on iid sampling. Yet, SGDm is often used outside this regime, in settings with temporally correlated input samples such as continual learning and reinforcement learning. Existing work has shown that SGDm with a decaying stepsize can converge under Markovian temporal correlation. In this work, we show that SGDm under covariate shift with a fixed step-size can be unstable and diverge. In particular, we show SGDm under covariate shift is a parametric oscillator, and so can suffer from a phenomenon known as resonance. We approximate the learning system as a time varying system of ordinary differential equations, and leverage existing theory to characterize the system's divergence/convergence as resonant/nonresonant modes. The theoretical result is limited to the linear setting with periodic covariate shift, so we empirically supplement this result to show that resonance phenomena persist even under non-periodic covariate shift, nonlinear dynamics with neural networks, and optimizers other than SGDm",
    "volume": "main",
    "checked": true,
    "id": "a8b150013942d949a03ecf20545a3b5d66ca369a",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=p-BhZSz59o4": {
    "title": "BEiT: BERT Pre-Training of Image Transformers",
    "abstract": "We introduce a self-supervised vision representation model BE I T , which stands for B idirectional E ncoder representation from I mage T ransformers. Following BERT [DCLT19] developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Speciﬁcally, each image has two views in our pre-training, i.e., image patches (such as 16 × 16 pixels), and visual tokens (i.e., discrete tokens). We ﬁrst \"tokenize\" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BE I T, we directly ﬁne-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classiﬁcation and semantic segmentation show that our model achieves competitive results with previous pre-training methods. BE I T outperforms both from-scratch training and previous strong self-supervised models. Moreover, BE I T is complementary to supervised pre-training. Performance of BE I T can be further improved by intermediate ﬁne-tuning with ImageNet labels. Ablation studies show that our proposed techniques are critical to the effectiveness of BERT-style pre-training for image data. Apart from performance, the improvements of convergence speed and stability of ﬁne-tuning reduce training costs on end tasks. addition, we demonstrate that self-supervised BE I T can learn reasonable semantic regions via pre-training, unleashing the rich supervision signals contained in images",
    "volume": "main",
    "checked": true,
    "id": "722ad6ac92286507437b31486f47987d6ece05c9",
    "citation_count": 562
  },
  "https://openreview.net/forum?id=BmJV7kyAmg": {
    "title": "Towards Understanding the Robustness Against Evasion Attack on Categorical Data",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "f01b92624046cadffe6d8a26805e03502ed3fd81",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=0xiJLKH-ufZ": {
    "title": "Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models",
    "abstract": "Diffusion probabilistic models (DPMs) represent a class of powerful generative models. Despite their success, the inference of DPMs is expensive since it generally needs to iterate over thousands of timesteps. A key problem in the inference is to estimate the variance in each timestep of the reverse process. In this work, we present a surprising result that both the optimal reverse variance and the corresponding optimal KL divergence of a DPM have analytic forms w.r.t. its score function. Building upon it, we propose Analytic-DPM, a training-free inference framework that estimates the analytic forms of the variance and KL divergence using the Monte Carlo method and a pretrained score-based model. Further, to correct the potential bias caused by the score-based model, we derive both lower and upper bounds of the optimal variance and clip the estimate for a better result. Empirically, our analytic-DPM improves the log-likelihood of various DPMs, produces high-quality samples, and meanwhile enjoys a 20× to 80× speed up",
    "volume": "main",
    "checked": true,
    "id": "9b7b218b0f4e14f97260b6192add37da5e9ae2c5",
    "citation_count": 38
  },
  "https://openreview.net/forum?id=SlxSY2UZQT": {
    "title": "Label-Efficient Semantic Segmentation with Diffusion Models",
    "abstract": "Denoising diffusion probabilistic models have recently received much research attention since they outperform alternative approaches, such as GANs, and currently provide state-of-the-art generative performance. The superior performance of diffusion models has made them an appealing tool in several applications, including inpainting, super-resolution, and semantic editing. In this paper, we demonstrate that diffusion models can also serve as an instrument for semantic segmentation, especially in the setup when labeled data is scarce. In particular, for several pretrained diffusion models, we investigate the intermediate activations from the networks that perform the Markov step of the reverse diffusion process. We show that these activations effectively capture the semantic information from an input image and appear to be excellent pixel-level representations for the segmentation problem. Based on these observations, we describe a simple segmentation method, which can work even if only a few training images are provided. Our approach significantly outperforms the existing alternatives on several datasets for the same amount of human supervision. The source code of the project is publicly available",
    "volume": "main",
    "checked": true,
    "id": "42f2271cebb7f272b0066c1f22d33381f139ee68",
    "citation_count": 31
  },
  "https://openreview.net/forum?id=swiyAeGzFhQ": {
    "title": "Learning to Guide and to be Guided in the Architect-Builder Problem",
    "abstract": "in interactive agents that learn to coordinate, namely, a builder – which performs actions but ignores the goal of the task, i.e. has no access to rewards – and an architect which guides the builder towards the goal of the task. We deﬁne and explore a formal setting where artiﬁcial agents are equipped with mechanisms that allow them to simultaneously learn a task while at the same time evolving a shared communication protocol. Ideally, such learning should only rely on high-level communication priors and be able to handle a large variety of tasks and meanings while deriving communication protocols that can be reused across tasks. The ﬁeld of Experimental Semiotics has shown the extent of human proﬁciency at learning from a priori unknown instructions meanings. Therefore, we take inspiration from it and present the Architect-Builder Problem ( ABP ): an asymmetrical setting in which an architect must learn to guide a builder towards constructing a speciﬁc structure. The architect knows the target structure but cannot act in the environment and can only send arbitrary messages to the builder. The builder on the other hand can act in the environment, but receives no rewards nor has any knowledge about the task, and must learn to solve it relying only on the messages sent by the architect. Crucially, the meaning of messages is initially not deﬁned nor shared between the agents but must be negotiated throughout learning",
    "volume": "main",
    "checked": true,
    "id": "5df950eadbdd3c9955d1212f91165a2914f89887",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=xm6YD62D1Ub": {
    "title": "VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning",
    "abstract": "Recent self-supervised methods for image representation learning are based on maximizing the agreement between embedding vectors from different views of the same image. A trivial solution is obtained when the encoder outputs constant vectors. This collapse problem is often avoided through implicit biases in the learning architecture, that often lack a clear justification or interpretation. In this paper, we introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with a simple regularization term on the variance of the embeddings along each dimension individually. VICReg combines the variance term with a decorrelation mechanism based on redundancy reduction and covariance regularization, and achieves results on par with the state of the art on several downstream tasks. In addition, we show that incorporating our new variance term into other methods helps stabilize the training and leads to performance improvements",
    "volume": "main",
    "checked": true,
    "id": "0d0cf5f64c052aa7edc5bb638203616a620557f6",
    "citation_count": 198
  },
  "https://openreview.net/forum?id=SsHBkfeRF9L": {
    "title": "Neural graphical modelling in continuous-time: consistency guarantees and algorithms",
    "abstract": "The discovery of structure from time series data is a key problem in fields of study working with complex systems. Most identifiability results and learning algorithms assume the underlying dynamics to be discrete in time. Comparatively few, in contrast, explicitly define dependencies in infinitesimal intervals of time, independently of the scale of observation and of the regularity of sampling. In this paper, we consider score-based structure learning for the study of dynamical systems. We prove that for vector fields parameterized in a large class of neural networks, least squares optimization with adaptive regularization schemes consistently recovers directed graphs of local independencies in systems of stochastic differential equations. Using this insight, we propose a score-based learning algorithm based on penalized Neural Ordinary Differential Equations (modelling the mean process) that we show to be applicable to the general setting of irregularly-sampled multivariate time series and to outperform the state of the art across a range of dynamical systems",
    "volume": "main",
    "checked": true,
    "id": "5de29a2474abb0888c13aab0f6e56a761ee69602",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=NlObxR0rosG": {
    "title": "Practical Integration via Separable Bijective Networks",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "9ae51d683e131995ed684cfb9122dafbbce36b50",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=JJCjv4dAbyL": {
    "title": "Learning Discrete Structured Variational Auto-Encoder using Natural Evolution Strategies",
    "abstract": "Discrete variational auto-encoders (VAEs) are able to represent semantic latent spaces in generative learning. In many real-life settings, the discrete latent space consists of high-dimensional structures, and propagating gradients through the relevant structures often requires enumerating over an exponentially large latent space. Recently, various approaches were devised to propagate approximated gradients without enumerating over the space of possible structures. In this work, we use Natural Evolution Strategies (NES), a class of gradient-free black-box optimization algorithms, to learn discrete structured VAEs. The NES algorithms are computationally appealing as they estimate gradients with forward pass evaluations only, thus they do not require to propagate gradients through their discrete structures. We demonstrate empirically that optimizing discrete structured VAEs using NES is as effective as gradient-based approximations. Lastly, we prove NES converges for non-Lipschitz functions as appear in discrete structured VAEs.1",
    "volume": "main",
    "checked": true,
    "id": "762911860a91b5fb24f16a7ca52de3c86d563567",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=PVJ6j87gOHz": {
    "title": "CoMPS: Continual Meta Policy Search",
    "abstract": ", analogously to PPO and other importance-sampled policy gradient algorithms. We use this estimator for the inner loop update in Algorithm 1 line 5. We show in our ablation experiments that this approach is needed to enable successful meta-training using the exhaustive off-policy experience collected by CoMPS",
    "volume": "main",
    "checked": true,
    "id": "b64b3880198289fca95e54a001da3dd336502d7a",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=Q5uh1Nvv5dm": {
    "title": "AdaMatch: A Unified Approach to Semi-Supervised Learning and Domain Adaptation",
    "abstract": "We extend semi-supervised learning to the problem of domain adaptation to learn significantly higher-accuracy models that train on one data distribution and test on a different one. With the goal of generality, we introduce AdaMatch, a method that unifies the tasks of unsupervised domain adaptation (UDA), semi-supervised learning (SSL), and semi-supervised domain adaptation (SSDA). In an extensive experimental study, we compare its behavior with respective state-of-the-art techniques from SSL, SSDA, and UDA on vision classification tasks. We find AdaMatch either matches or significantly exceeds the state-of-the-art in each case using the same hyper-parameters regardless of the dataset or task. For example, AdaMatch nearly doubles the accuracy compared to that of the prior state-of-the-art on the UDA task for DomainNet and even exceeds the accuracy of the prior state-ofthe-art obtained with pre-training by 6.4% when AdaMatch is trained completely from scratch. Furthermore, by providing AdaMatch with just one labeled example per class from the target domain (i.e., the SSDA setting), we increase the target accuracy by an additional 6.1%, and with 5 labeled examples, by 13.6%. 2",
    "volume": "main",
    "checked": true,
    "id": "a9e865dc4fe66c4259206b439322c9f45badd50c",
    "citation_count": 27
  },
  "https://openreview.net/forum?id=m8bypnj7Yl5": {
    "title": "Neural Solvers for Fast and Accurate Numerical Optimal Control",
    "abstract": "Synthesizing optimal controllers for dynamical systems in practice involves solving real–time optimization problems with hard time constraints. These constraints restrict the class of numerical methods that can be applied; indeed, computationally expensive but accurate numerical routines often have to be replaced with fast and inaccurate methods, trading inference time for worse theoretical guarantees on solution accuracy. This paper proposes a novel methodology to accelerate numerical optimization of optimal control policies via hypersolvers , hybrids of a base solver and a neural network. In particular, we apply low–order explicit numerical methods for the ordinary differential equation (ODE) associated to the numerical optimal control problem, augmented with an additional parametric approximator trained to reduce local truncation errors introduced by the base solver. Given a target system to control, we ﬁrst pre-train hypersolvers to approximate base solver residuals by sampling plausible control inputs. Then, we use the trained hypersolver to obtain fast and accurate solutions of the target system during optimization of the controller. The performance of our approach is evaluated in direct and model predictive optimal control settings, where we show consistent Pareto improvements in terms of solution accuracy and control performance",
    "volume": "main",
    "checked": true,
    "id": "4d70ac690fcde8a720d6df5b106260240b353fb4",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=Sq0-tgDyHe4": {
    "title": "Local Feature Swapping for Generalization in Reinforcement Learning",
    "abstract": "of capable from the over-parametrization of neural architectures leads to memorization of the data used during training and thus to a lack of generalization. Reinforcement learning agents based on visual inputs also suffer from this phenomenon by erroneously correlating rewards with unrelated visual features such as background elements. To alleviate this problem, we introduce a new regularization technique consisting of channel-consistent local permutations (CLOP) of the feature maps. The proposed permutations induce robustness to spatial correlations and help prevent overﬁtting behaviors in RL. We demonstrate, on the OpenAI Procgen Benchmark, that RL agents trained with the CLOP method exhibit robustness to visual changes and better generalization properties than agents trained using other state-of-the-art regularization techniques. We also demonstrate the effectiveness of CLOP as a general regularization technique in supervised learning",
    "volume": "main",
    "checked": true,
    "id": "56a73bb9748e4a09994fe8aedc645eded638109e",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=dFbKQaRk15w": {
    "title": "Equivariant Subgraph Aggregation Networks",
    "abstract": "Message-passing neural networks (MPNNs) are the leading architecture for deep learning on graph-structured data, in large part due to their simplicity and scalability. Unfortunately, it was shown that these architectures are limited in their expressive power. This paper proposes a novel framework called Equivariant Subgraph Aggregation Networks (ESAN) to address this issue. Our main observation is that while two graphs may not be distinguishable by an MPNN, they often contain distinguishable subgraphs. Thus, we propose to represent each graph as a set of subgraphs derived by some predefined policy, and to process it using a suitable equivariant architecture. We develop novel variants of the 1-dimensional Weisfeiler-Leman (1-WL) test for graph isomorphism, and prove lower bounds on the expressiveness of ESAN in terms of these new WL variants. We further prove that our approach increases the expressive power of both MPNNs and more expressive architectures. Moreover, we provide theoretical results that describe how design choices such as the subgraph selection policy and equivariant neural architecture affect our architecture's expressive power. To deal with the increased computational cost, we propose a subgraph sampling scheme, which can be viewed as a stochastic version of our framework. A comprehensive set of experiments on real and synthetic datasets demonstrates that our framework improves the expressive power and overall performance of popular GNN architectures",
    "volume": "main",
    "checked": true,
    "id": "6667a57c8ffa3f3c0d724b1e8e986758995df2b8",
    "citation_count": 42
  },
  "https://openreview.net/forum?id=DfUjyyRW90": {
    "title": "Information Prioritization through Empowerment in Visual Model-based RL",
    "abstract": "Model-based algorithms designed for handling complex visual typically learn some sort of latent representation, implicitly. Standard methods of this do functionally relevant aspects of the state and irrelevant distractors, instead aiming to represent all available information We propose a modiﬁed objective for model-based RL that, in combination with mutual information maximization, allows us to learn representations and dynamics for visual model-based RL without reconstruction in a way that explicitly prioritizes functionally relevant factors. The key principle behind our design is to integrate a term inspired by variational empowerment into a state-space model based on mutual information. This term prioritizes information that is correlated with action, thus ensur-ing that functionally relevant factors are captured ﬁrst. Furthermore, the same empowerment term also promotes faster exploration during the RL process, es-pecially for sparse-reward tasks where the reward signal is insufﬁcient to drive exploration in the early stages of learning. We evaluate the approach on a suite of vision-based robot control tasks with natural video backgrounds, and show that the proposed prioritized information objective outperforms state-of-the-art model based RL approaches with higher sample efﬁciency and episodic returns",
    "volume": "main",
    "checked": true,
    "id": "9229fe9049677b0d00a38713bf1642a1955a1f18",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=xLfAgCroImw": {
    "title": "Energy-Based Learning for Cooperative Games, with Applications to Valuation Problems in Machine Learning",
    "abstract": "Valuation problems, such as feature interpretation, data valuation and model valuation for ensembles, become increasingly more important in many machine learning applications. Such problems are commonly addressed via well-known game-theoretic criteria, such as the Shapley value or Banzhaf value. In this work, we present a novel energy-based treatment for cooperative games, with a theo-retical justiﬁcation via the maximum entropy principle. Surprisingly, through mean-ﬁeld variational inference in the energy-based model, we recover classical game-theoretic valuation criteria by conducting one-step of ﬁxed point iteration for maximizing the ELBO objective. This observation also further supports existing criteria, as they can be seen as attempting to decouple the correlations among players. By running the ﬁxed point iteration for multiple steps, we achieve a trajectory of the variational valuations, among which we deﬁne the valuation with the best conceivable decoupling error as the Variational Index . We prove that under uniform initialization, these variational valuations all satisfy a set of game-theoretic axioms. We empirically demonstrate that the proposed variational valuations enjoy lower decoupling error and better valuation performance on certain synthetic and real-world valuation problems. 1 i i.e., | x i ← k x x i e i i . For two sets S and T , S + T and S − T represent set union and set difference, respectively. | S | is the cardinality of S . i is used to denote the singleton { i } with a bit abuse of notation",
    "volume": "main",
    "checked": true,
    "id": "abc4a4ba79799f7d123b3afbcf24476b9b541f95",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=lrocYB-0ST2": {
    "title": "Approximation and Learning with Deep Convolutional Models: a Kernel Perspective",
    "abstract": "The empirical success of deep convolutional networks on tasks involving highdimensional data such as images or audio suggests that they can efficiently approximate certain functions that are well-suited for such tasks. In this paper, we study this through the lens of kernel methods, by considering simple hierarchical kernels with two or three convolution and pooling layers, inspired by convolutional kernel networks. These achieve good empirical performance on standard vision datasets, while providing a precise description of their functional space that yields new insights on their inductive bias. We show that the RKHS consists of additive models of interaction terms between patches, and that its norm encourages spatial similarities between these terms through pooling layers. We then provide generalization bounds which illustrate how pooling and patches yield improved sample complexity guarantees when the target function presents such regularities",
    "volume": "main",
    "checked": true,
    "id": "88965ac9e0666e94174a54b0490679eee4dfc7be",
    "citation_count": 14
  },
  "https://openreview.net/forum?id=9xhgmsNVHu": {
    "title": "Is High Variance Unavoidable in RL? A Case Study in Continuous Control",
    "abstract": "Reinforcement learning (RL) experiments have notoriously high variance, and minor details can have disproportionately large effects on measured outcomes. This is problematic for creating reproducible research and also serves as an obstacle when applying RL to sensitive real-world applications. In this paper, we investigate causes for this perceived instability. To allow for an in-depth analysis, we focus on a specifically popular setup with high variance – continuous control from pixels with an actor-critic agent. In this setting, we demonstrate that poor outlier runs which completely fail to learn are an important source of variance, but that weight initialization and initial exploration are not at fault. We show that one cause for these outliers is unstable network parametrization which leads to saturating nonlinearities. We investigate several fixes to this issue and find that simply normalizing penultimate features is surprisingly effective. For sparse tasks, we also find that partially disabling clipped double Q-learning decreases variance. By combining fixes we significantly decrease variances, lowering the average standard deviation across 21 tasks by a factor > 3 for a state-of-the-art agent. This demonstrates that the perceived variance is not necessarily inherent to RL. Instead, it may be addressed via simple modifications and we argue that developing low-variance agents is an important goal for the RL community",
    "volume": "main",
    "checked": true,
    "id": "c578a295ba3456bebc651f1f516e34ea6e7a5fe3",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=HfUyCRBeQc": {
    "title": "Selective Ensembles for Consistent Predictions",
    "abstract": "Recent work has shown that models trained to the same objective, and which achieve similar measures of accuracy on consistent test data, may nonetheless behave very differently on individual predictions. This inconsistency is undesirable in high-stakes contexts, such as medical diagnosis and finance. We show that this inconsistent behavior extends beyond predictions to feature attributions, which may likewise have negative implications for the intelligibility of a model, and one's ability to find recourse for subjects. We then introduce selective ensembles to mitigate such inconsistencies by applying hypothesis testing to the predictions of a set of models trained using randomly-selected starting conditions; importantly, selective ensembles can abstain in cases where a consistent outcome cannot be achieved up to a specified confidence level. We prove that that prediction disagreement between selective ensembles is bounded, and empirically demonstrate that selective ensembles achieve consistent predictions and feature attributions while maintaining low abstention rates. On several benchmark datasets, selective ensembles reach zero inconsistently predicted points, with abstention rates as low 1.5%",
    "volume": "main",
    "checked": true,
    "id": "9cf305d6c9e8ffb571b28aea718ff42d1a9efdc7",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=St6eyiTEHnG": {
    "title": "Consistent Counterfactuals for Deep Models",
    "abstract": "Counterfactual examples are one of the most commonly-cited methods for explaining the predictions of machine learning models in key areas such as finance and medical diagnosis. Counterfactuals are often discussed under the assumption that the model on which they will be used is static, but in deployment models may be periodically retrained or fine-tuned. This paper studies the consistency of model prediction on counterfactual examples in deep networks under small changes to initial training conditions, such as weight initialization and leave-one-out variations in data, as often occurs during model deployment. We demonstrate experimentally that counterfactual examples for deep models are often inconsistent across such small changes, and that increasing the cost of the counterfactual, a stability-enhancing mitigation suggested by prior work in the context of simpler models, is not a reliable heuristic in deep networks. Rather, our analysis shows that a model's Lipschitz continuity around the counterfactual, along with confidence of its prediction, is key to its consistency across related models. To this end, we propose Stable Neighbor Search as a way to generate more consistent counterfactual explanations, and illustrate the effectiveness of this approach on several benchmark datasets",
    "volume": "main",
    "checked": true,
    "id": "b1fe45eda204847f5f4c0b3b8eafaecaf184859c",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=AV8FPoMTTa": {
    "title": "Shallow and Deep Networks are Near-Optimal Approximators of Korobov Functions",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "197691e71bfeaef201111c42dd675cf2f0888db2",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=WPI2vbkAl3Q": {
    "title": "Learning Curves for SGD on Structured Features",
    "abstract": "The generalization performance of a machine learning algorithm such as a neural network depends in a non-trivial way on the structure of the data distribution. To analyze the influence of data structure on test loss dynamics, we study an exactly solveable model of stochastic gradient descent (SGD) which predicts test loss when training on features with arbitrary covariance structure. We solve the theory exactly for both Gaussian features and arbitrary features and we show that the simpler Gaussian model accurately predicts test loss of nonlinear random-feature models and deep neural networks trained with SGD on real datasets such as MNIST and CIFAR-10. We show that the optimal batch size at a fixed compute budget is typically small and depends on the feature correlation structure, demonstrating the computational benefits of SGD with small batch sizes. Lastly, we extend our theory to the more usual setting of stochastic gradient descent on a fixed subsampled training set, showing that both training and test error can be accurately predicted in our framework on real data",
    "volume": "main",
    "checked": true,
    "id": "a8f103c9af08697d7a5bf509325f96c1e15f490b",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=mk0HzdqY7i1": {
    "title": "What's Wrong with Deep Learning in Tree Search for Combinatorial Optimization",
    "abstract": "Combinatorial optimization lies at the core of many real-world problems. Especially since the rise of graph neural networks (GNNs), the deep learning community has been developing solvers that derive solutions to NP-hard problems by learning the problem-specific solution structure. However, reproducing the results of these publications proves to be difficult. We make three contributions. First, we present an open-source benchmark suite for the NP-hard MAXIMUM INDEPENDENT SET problem, in both its weighted and unweighted variants. The suite offers a unified interface to various state-of-the-art traditional and machine learning-based solvers. Second, using our benchmark suite, we conduct an in-depth analysis of the popular guided tree search algorithm by Li et al. [NeurIPS 2018], testing various configurations on small and large synthetic and real-world graphs. By re-implementing their algorithm with a focus on code quality and extensibility, we show that the graph convolution network used in the tree search does not learn a meaningful representation of the solution structure, and can in fact be replaced by random values. Instead, the tree search relies on algorithmic techniques like graph kernelization to find good solutions. Thus, the results from the original publication are not reproducible. Third, we extend the analysis to compare the tree search implementations to other solvers, showing that the classical algorithmic solvers often are faster, while providing solutions of similar quality. Additionally, we analyze a recent solver based on reinforcement learning and observe that for this solver, the GNN is responsible for the competitive solution quality",
    "volume": "main",
    "checked": true,
    "id": "e6bf8b804d1fa49f90150b26ee1e4eb92157657f",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=hgKtwSb4S2": {
    "title": "A generalization of the randomized singular value decomposition",
    "abstract": "The randomized singular value decomposition (SVD) is a popular and effective algorithm for computing a near-best rank k approximation of a matrix A using matrix-vector products with standard Gaussian vectors. Here, we generalize the theory of randomized SVD to multivariable Gaussian vectors, allowing one to incorporate prior knowledge of A into the algorithm. This enables us to explore the continuous analogue of the randomized SVD for Hilbert–Schmidt (HS) operators using operator-function products with functions drawn from a Gaussian process (GP). We then construct a new covariance kernel for GPs, based on weighted Jacobi polynomials, which allows us to rapidly sample the GP and control the smoothness of the randomly generated functions. Numerical examples on matrices and HS operators demonstrate the applicability of the algorithm",
    "volume": "main",
    "checked": true,
    "id": "f4b1076c5b5c392ed2e887c97a8cde9aa00e6a6c",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=VLgmhQDVBV": {
    "title": "Implicit Bias of MSE Gradient Optimization in Underparameterized Neural Networks",
    "abstract": "We study the dynamics of a neural network in function space when optimizing the mean squared error via gradient flow. We show that in the underparameterized regime the network learns eigenfunctions of an integral operator TK∞ determined by the Neural Tangent Kernel (NTK) at rates corresponding to their eigenvalues. For example, for uniformly distributed data on the sphere Sd−1 and rotation invariant weight distributions, the eigenfunctions of TK∞ are the spherical harmonics. Our results can be understood as describing a spectral bias in the underparameterized regime. The proofs use the concept of \"Damped Deviations\", where deviations of the NTK matter less for eigendirections with large eigenvalues due to the occurence of a damping factor. Aside from the underparameterized regime, the damped deviations point-of-view can be used to track the dynamics of the empirical risk in the overparameterized setting, allowing us to extend certain results in the literature. We conclude that damped deviations offers a simple and unifying perspective of the dynamics when optimizing the squared error",
    "volume": "main",
    "checked": true,
    "id": "0301a9186a35d9085f8db7f1847fcbcfcfa26232",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=_xwr8gOBeV1": {
    "title": "Geometric and Physical Quantities improve E(3) Equivariant Message Passing",
    "abstract": "Including covariant information, such as position, force, velocity or spin is important in many tasks in computational physics and chemistry. We introduce Steerable E( 3 ) Equivariant Graph Neural Networks (SEGNNs) that generalise equivariant graph networks, such that node and edge attributes are not restricted to invariant scalars, but can contain covariant information, such as vectors or tensors. This model, composed of steerable MLPs, is able to incorporate geometric and physical information in both the message and update functions. Through the deﬁnition of steerable node attributes, the MLPs provide a new class of activation functions for general use with steerable feature ﬁelds. We discuss ours and related work through the lens of equivariant non-linear convolutions , which further allows us to pin-point the successful components of SEGNNs: non-linear message aggregation improves upon classic linear (steerable) point convolutions; steerable messages improve upon recent equivariant graph networks that send invariant messages. We demonstrate the effectiveness of our method on several tasks in computational physics and chemistry and provide extensive ablation studies. but can also be vector- or tensor-valued. SEGNNs are the ﬁrst networks which allow the steering of node updates by leveraging geometric and physical cues, introducing a new class of equivariant activation functions. We demonstrate the potential of SEGNNs by applying it to a wide range of different tasks. Extensive ablation studies have further shown the beneﬁt of steerable over non-steerable (invariant) message passing, and the beneﬁt of non-linear over linear convolutions. On the OC20 ISRE taks, SEGNNs outperform all competitors",
    "volume": "main",
    "checked": true,
    "id": "d420253d38e08881969ed1e3afbe6e1ef3fe1368",
    "citation_count": 37
  },
  "https://openreview.net/forum?id=vSix3HPYKSU": {
    "title": "Message Passing Neural PDE Solvers",
    "abstract": "The numerical solution of partial differential equations (PDEs) is difficult, having led to a century of research so far. Recently, there have been pushes to build neural–numerical hybrid solvers, which piggy-backs the modern trend towards fully end-to-end learned systems. Most works so far can only generalize over a subset of properties to which a generic solver would be faced, including: resolution, topology, geometry, boundary conditions, domain discretization regularity, dimensionality, etc. In this work, we build a solver, satisfying these properties, where all the components are based on neural message passing, replacing all heuristically designed components in the computation graph with backpropoptimized neural function approximators. We show that neural message passing solvers representationally contain some classical methods, such as finite differences, finite volumes, and WENO schemes. In order to encourage stability in training autoregressive models, we put forward a method that is based on the principle of zero-stability, posing stability as a domain adaptation problem. We validate our method on various fluid-like flow problems, demonstrating fast, stable, and accurate performance across different domain topologies, discretization, etc. in 1D and 2D. Our model outperforms state-of-the-art numerical solvers in the low resolution regime in terms of speed and accuracy",
    "volume": "main",
    "checked": true,
    "id": "b3a56eae6ee1676247c8fad50faac02874edcda7",
    "citation_count": 37
  },
  "https://openreview.net/forum?id=T0B9AoM_bFg": {
    "title": "Improving Mutual Information Estimation with Annealed and Energy-Based Bounds",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "af7a0aa310a2277999e3144365b422afcc7e99dd",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=ziRLU3Y2PN_": {
    "title": "Generalized rectifier wavelet covariance models for texture synthesis",
    "abstract": "State-of-the-art maximum entropy models for texture synthesis are built from statistics relying on image representations defined by convolutional neural networks (CNN). Such representations capture rich structures in texture images, outperforming wavelet-based representations in this regard. However, conversely to neural networks, wavelets offer meaningful representations, as they are known to detect structures at multiple scales (e.g. edges) in images. In this work, we propose a family of statistics built upon non-linear wavelet based representations, that can be viewed as a particular instance of a one-layer CNN, using a generalized rectifier non-linearity. These statistics significantly improve the visual quality of previous classical wavelet-based models, and allow one to produce syntheses of similar quality to state-of-the-art models, on both gray-scale and color textures. We further provide insights on memorization effects in these models",
    "volume": "main",
    "checked": true,
    "id": "0c13593e5024f2d84d089c0f0bec03f6f5d35d20",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=F72ximsx7C1": {
    "title": "How Attentive are Graph Attention Networks?",
    "abstract": "Graph Attention Networks (GATs) are one of the most popular GNN architectures and are considered as the state-of-the-art architecture for representation learning with graphs. In GAT, every node attends to its neighbors given its own representation as the query. However, in this paper we show that GATs can only compute a restricted kind of attention where the ranking of attended nodes is unconditioned on the query node. We formally define this restricted kind of attention as static attention and distinguish it from a strictly more expressive dynamic attention. Because GATs use a static attention mechanism, there are simple graph problems that GAT cannot express: in a controlled problem, we show that static attention hinders GAT from even fitting the training data. To remove this limitation, we introduce a simple fix by modifying the order of operations and propose GATv2: a dynamic graph attention variant that is strictly more expressive than GAT. We perform an extensive evaluation and show that GATv2 outperforms GAT across 11 OGB and other benchmarks while we match their parametric costs. Our code is available at https://github.com/tech-srl/how_attentive_are_gats",
    "volume": "main",
    "checked": true,
    "id": "ab30672c8c5e4787f6a5985f26a8f281f0db2fb8",
    "citation_count": 142
  },
  "https://openreview.net/forum?id=af1eUDdUVz": {
    "title": "Evading Adversarial Example Detection Defenses with Orthogonal Projected Gradient Descent",
    "abstract": "(a) to multiple simultaneous one at the of Projected Gradient Descent and Orthogonal Projected Gradient Descent , improved attack to adversarial examples this by orthogonalizing the when running standard gradient-based attacks. We use our technique to evade four state-of-the-art detection defenses, reducing their accuracy to 0% while maintaining a detection",
    "volume": "main",
    "checked": true,
    "id": "13a5aedf89c0e6c10b18350e4b228708f22a6605",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=7B3IJMM1k_M": {
    "title": "Optimal ANN-SNN Conversion for High-accuracy and Ultra-low-latency Spiking Neural Networks",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "5682daa63e1ffa2153384ba40a4a204b2bdc5446",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=-8sBpe7rDiV": {
    "title": "Network Insensitivity to Parameter Noise via Parameter Attack During Training",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "dbf898f4935340449883fef8ba505059525bd98a",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=Dzpe9C1mpiv": {
    "title": "A Unified Wasserstein Distributional Robustness Framework for Adversarial Training",
    "abstract": "It is well-known that deep neural networks (DNNs) are susceptible to adversarial attacks, exposing a severe fragility of deep learning systems. As the result, adversarial training (AT) method, by incorporating adversarial examples during training, represents a natural and effective approach to strengthen the robustness of a DNN-based classifier. However, most AT-based methods, notably PGD-AT and TRADES, typically seek a pointwise adversary that generates the worst-case adversarial example by independently perturbing each data sample, as a way to \"probe\" the vulnerability of the classifier. Arguably, there are unexplored benefits in considering such adversarial effects from an entire distribution. To this end, this paper presents a unified framework that connects Wasserstein distributional robustness with current state-of-the-art AT methods. We introduce a new Wasserstein cost function and a new series of risk functions, with which we show that standard AT methods are special cases of their counterparts in our framework. This connection leads to an intuitive relaxation and generalization of existing AT methods and facilitates the development of a new family of distributional robustness AT-based algorithms. Extensive experiments show that our distributional robustness AT algorithms robustify further their standard AT counterparts in various settings.1",
    "volume": "main",
    "checked": true,
    "id": "9420398975d0989c638d47c6f059d09272b6992f",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=noaG7SrPVK0": {
    "title": "Counterfactual Plans under Distributional Ambiguity",
    "abstract": "Counterfactual explanations are attracting signiﬁcant attention due to the ﬂourish-ing applications of machine learning models in consequential domains. A counterfactual plan consists of multiple possibilities to modify a given instance so that the model's prediction will be altered. As the predictive model can be updated subject to the future arrival of new data, a counterfactual plan may become inef-fective or infeasible with respect to the future values of the model parameters. In this work, we study the counterfactual plans under model uncertainty, in which the distribution of the model parameters is partially prescribed using only the ﬁrst- and second-moment information. First, we propose an uncertainty quantiﬁcation tool to compute the lower and upper bounds of the probability of validity for any given counterfactual plan. We then provide corrective methods to adjust the counterfactual plan to improve the validity measure. The numerical experiments validate our bounds and demonstrate that our correction increases the robustness of the counterfactual plans in different real-world datasets",
    "volume": "main",
    "checked": true,
    "id": "262e060ddfa7db0563b083d31552b2855e2daa1e",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=SYB4WrJql1n": {
    "title": "On the Existence of Universal Lottery Tickets",
    "abstract": "The lottery ticket hypothesis conjectures the existence of sparse subnetworks of large randomly initialized deep neural networks that can be successfully trained in isolation. Recent work has experimentally observed that some of these tickets can be practically reused across a variety of tasks, hinting at some form of universality. We formalize this concept and theoretically prove that not only do such universal tickets exist but they also do not require further training. Our proofs introduce a couple of technical innovations related to pruning for strong lottery tickets, including extensions of subset sum results and a strategy to leverage higher amounts of depth. Our explicit sparse constructions of universal function families might be of independent interest, as they highlight representational benefits induced by univariate convolutional architectures",
    "volume": "main",
    "checked": true,
    "id": "95b241948a67518cc83ad864ea96abaf9a473881",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=SS8F6tFX3-": {
    "title": "Evaluating Model-Based Planning and Planner Amortization for Continuous Control",
    "abstract": "There is a widespread intuition that model-based control methods should be able to surpass the data efficiency of model-free approaches. In this paper we attempt to evaluate this intuition on various challenging locomotion tasks. We take a hybrid approach, combining model predictive control (MPC) with a learned model and model-free policy learning; the learned policy serves as a proposal for MPC. We find that well-tuned model-free agents are strong baselines even for high DoF control problems but MPC with learned proposals and models (trained on the fly or transferred from related tasks) can significantly improve performance and data efficiency in hard multi-task/multi-goal settings. Finally, we show that it is possible to distil a model-based planner into a policy that amortizes the planning computation without any loss of performance. Videos of agents performing different tasks can be seen on our website",
    "volume": "main",
    "checked": true,
    "id": "90ab9f71262e03ba7429b3fc4b631aa8ab1ddd28",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=6vkzF28Hur8": {
    "title": "Training Transition Policies via Distribution Matching for Complex Tasks",
    "abstract": "Humans decompose novel complex tasks into simpler ones to exploit previously learned skills. Analogously, hierarchical reinforcement learning seeks to leverage lower-level policies for simple tasks to solve complex ones. However, because each lower-level policy induces a different distribution of states, transitioning from one lower-level policy to another may fail due to an unexpected starting state. We introduce transition policies that smoothly connect lower-level policies by producing a distribution of states and actions that matches what is expected by the next policy. Training transition policies is challenging because the natural reward signal—whether the next policy can execute its subtask successfully—is sparse. By training transition policies via adversarial inverse reinforcement learning to match the distribution of expected states and actions, we avoid relying on taskbased reward. To further improve performance, we use deep Q-learning with a binary action space to determine when to switch from a transition policy to the next pre-trained policy, using the success or failure of the next subtask as the reward. Although the reward is still sparse, the problem is less severe due to the simple binary action space. We demonstrate our method on continuous bipedal locomotion and arm manipulation tasks that require diverse skills. We show that it smoothly connects the lower-level policies, achieving higher success rates than previous methods that search for successful trajectories based on a reward function, but do not match the state distribution",
    "volume": "main",
    "checked": true,
    "id": "8eb73addbf8c2b52637af040755cf3ca13cdbf40",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=N8MaByOzUfb": {
    "title": "New Insights on Reducing Abrupt Representation Change in Online Continual Learning",
    "abstract": "In the online continual learning paradigm, agents must learn from a changing distribution while respecting memory and compute constraints. Experience Replay (ER), where a small subset of past data is stored and replayed alongside new data, has emerged as a simple and effective learning strategy. In this work, we focus on the change in representations of observed data that arises when previously unobserved classes appear in the incoming data stream, and new classes must be distinguished from previous ones. We shed new light on this question by showing that applying ER causes the newly added classes' representations to overlap signiﬁcantly with the previous classes, leading to highly disruptive parameter updates. Based on this empirical analysis, we propose a new method which mitigates this issue by shielding the learned representations from drastic adaptation to accommodate new classes. We show that using an asymmetric update rule pushes new classes to adapt to the older ones (rather than the reverse), which is more effective especially at task boundaries, where much of the forgetting typically occurs. Empirical results show signiﬁcant gains over strong baselines on standard continual learning benchmarks 1",
    "volume": "main",
    "checked": true,
    "id": "fcc67b627d800c610f26f620c2697cdfbdeb476b",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=TYw3-OlrRm-": {
    "title": "Network Augmentation for Tiny Deep Learning",
    "abstract": "We introduce Network Augmentation (NetAug), a new training method for improving the performance of tiny neural networks. Existing regularization techniques (e.g., data augmentation, dropout) have shown much success on large neural networks by adding noise to overcome over-ﬁtting. However, we found these techniques hurt the performance of tiny neural networks. We argue that training tiny models are different from large models: rather than augmenting the data, we should augment the model, since tiny models tend to suffer from under-ﬁtting rather than over-ﬁtting due to limited capacity. To alleviate this issue, NetAug augments the network (reverse dropout) instead of inserting noise into the dataset or the network. It puts the tiny model into larger models and encourages it to work as a sub-model of larger models to get extra supervision, in addition to functioning as an independent model. At test time, only the tiny model is used for inference, incurring zero inference overhead. We demonstrate the effectiveness of NetAug on image classiﬁcation and object detection. NetAug consistently improves the performance of tiny models, achieving up to 2.2% accuracy improvement on ImageNet. On object detection, achieving the same level of performance, NetAug requires 41% fewer MACs on Pascal VOC and 38% fewer MACs on COCO than the baseline",
    "volume": "main",
    "checked": true,
    "id": "942b963c5ad89e20720ec38fe0c4861072d20aa3",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=0RqDp8FCW5Z": {
    "title": "W-CTC: a Connectionist Temporal Classification Loss with Wild Cards",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "7467b27d83a4f1f7adb9f5e90a8e49282c5177ea",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=jJOjjiZHy3h": {
    "title": "Defending Against Image Corruptions Through Adversarial Augmentations",
    "abstract": "Modern neural networks excel at image classification, yet they remain vulnerable to common image corruptions such as blur, speckle noise or fog. Recent methods that focus on this problem, such as AugMix and DeepAugment, introduce defenses that operate in expectation over a distribution of image corruptions. In contrast, the literature on `p-norm bounded perturbations focuses on defenses against worst-case corruptions. In this work, we reconcile both approaches by proposing AdversarialAugment, a technique which optimizes the parameters of image-to-image models to generate adversarially corrupted augmented images. We theoretically motivate our method and give sufficient conditions for the consistency of its idealized version as well as that of DeepAugment. Our classifiers improve upon the state-ofthe-art on common image corruption benchmarks conducted in expectation on CIFAR-10-C and improve worst-case performance against `p-norm bounded perturbations on both CIFAR-10 and IMAGENET",
    "volume": "main",
    "checked": true,
    "id": "285018adc33d8b2735dc2bb918f9ef8bae36ba25",
    "citation_count": 13
  },
  "https://openreview.net/forum?id=xZ6H7wydGl": {
    "title": "Robust and Scalable SDE Learning: A Functional Perspective",
    "abstract": "Stochastic differential equations provide a rich class of flexible generative models, capable of describing a wide range of spatio-temporal processes. A host of recent work looks to learn data-representing SDEs, using neural networks and other flexible function approximators. Despite these advances, learning remains computationally expensive due to the sequential nature of SDE integrators. In this work, we propose an importance-sampling estimator for probabilities of observations of SDEs for the purposes of learning. Crucially, the approach we suggest does not rely on such integrators. The proposed method produces lower-variance gradient estimates compared to algorithms based on SDE integrators and has the added advantage of being embarrassingly parallelizable. This facilitates the effective use of large-scale parallel hardware for massive decreases in computation time",
    "volume": "main",
    "checked": true,
    "id": "95088cb4b573cde65e748d9764dbbced0157309c",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=O-r8LOR-CCA": {
    "title": "Open-World Semi-Supervised Learning",
    "abstract": "Supervised and semi-supervised learning methods have been traditionally designed for the closedworld setting based on the assumption that unlabeled test data contains only classes previously encountered in the labeled training data. However, the real world is inherently open and dynamic, and thus novel, previously unseen classes may appear in the test data or during the model deployment. Here, we introduce a new open-world semisupervised learning setting in which the model is required to recognize previously seen classes, as well as to discover novel classes never seen in the labeled dataset. To tackle the problem, we propose ORCA, an approach that learns to simultaneously classify and cluster the data. ORCA classifies examples from the unlabeled dataset to previously seen classes, or forms a novel class by grouping similar examples together. The key idea in ORCA is in introducing uncertainty based adaptive margin that effectively circumvents the bias caused by the imbalance of variance between seen and novel classes/clusters. We demonstrate that ORCA accurately discovers novel classes and assigns samples to previously seen classes on benchmark image classification datasets, including CIFAR and ImageNet. Remarkably, despite solving the harder task ORCA outperforms semisupervised methods on seen classes, as well as novel class discovery methods on novel classes, achieving 7% and 151% improvements on seen and novel classes in the ImageNet dataset",
    "volume": "main",
    "checked": true,
    "id": "0016122bc5dfe0684baaa672c53014d48b79a65f",
    "citation_count": 26
  },
  "https://openreview.net/forum?id=8Py-W8lSUgy": {
    "title": "Relational Multi-Task Learning: Modeling Relations between Data and Tasks",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "70e192974552e2ace78d8378ef8b7443ab6fd091",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=iC4UHbQ01Mp": {
    "title": "Poisoning and Backdooring Contrastive Learning",
    "abstract": "Multimodal contrastive learning methods like CLIP train on noisy and uncurated training datasets. This is cheaper than labeling datasets manually, and even improves out-of-distribution robustness. We show that this practice makes backdoor and poisoning attacks a signiﬁcant threat. By poisoning just 0 . 01% of a dataset (e.g., just 300 images of the 3 million-example Conceptual Captions dataset), we can cause the model to misclassify test images by overlaying a small patch. Targeted poisoning attacks, whereby the model misclassiﬁes a particular test input with an adversarially-desired label, are even easier requiring control of 0 . 0001% of the dataset (e.g., just three out of the 3 million images). Our attacks call into question whether training on noisy and uncurated Internet scrapes is desirable",
    "volume": "main",
    "checked": true,
    "id": "6c50e8db8d44a3399a78adb8fab2d7f81a029c33",
    "citation_count": 37
  },
  "https://openreview.net/forum?id=Oxeka7Z7Hor": {
    "title": "Gaussian Mixture Convolution Networks",
    "abstract": "This paper proposes a novel method for deep learning based on the analytical convolution of multidimensional Gaussian mixtures. In contrast to tensors, these do not suffer from the curse of dimensionality and allow for a compact representation, as data is only stored where details exist. Convolution kernels and data are Gaussian mixtures with unconstrained weights, positions, and covariance matrices. Similar to discrete convolutional networks, each convolution step produces several feature channels, represented by independent Gaussian mixtures. Since traditional transfer functions like ReLUs do not produce Gaussian mixtures, we propose using a fitting of these functions instead. This fitting step also acts as a pooling layer if the number of Gaussian components is reduced appropriately. We demonstrate that networks based on this architecture reach competitive accuracy on Gaussian mixtures fitted to the MNIST and ModelNet data sets",
    "volume": "main",
    "checked": true,
    "id": "ce108373d3458c2f27524483221aaad772c3edf3",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=WE4qe9xlnQw": {
    "title": "A Program to Build E(N)-Equivariant Steerable CNNs",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "24a0cc377b41f361b2ef6500f28446c986486717",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=ljxWpdBl4V": {
    "title": "Closed-form Sample Probing for Learning Generative Models in Zero-shot Learning",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "f114c76d5eca4747bb8af5e8cdb9c16a4ddc3406",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=AUGBfDIV9rL": {
    "title": "Emergent Communication at Scale",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "051c85b10124bd2de6fcc29519a187f479d451bf",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=DYypjaRdph2": {
    "title": "Inverse Online Learning: Understanding Non-Stationary and Reactionary Policies",
    "abstract": "Human decision making is well known to be imperfect and the ability to analyse such processes individually is crucial when attempting to aid or improve a decision-maker's ability to perform a task, e.g. to alert them to potential biases or oversights on their part. To do so, it is necessary to develop interpretable representations of how agents make decisions and how this process changes over time as the agent learns online in reaction to the accrued experience. To then understand the decision-making processes underlying a set of observed trajectories, we cast the policy inference problem as the inverse to this online learning problem. By interpreting actions within a potential outcomes framework, we introduce a meaningful mapping based on agents choosing an action they believe to have the greatest treatment effect. We introduce a practical algorithm for retrospectively estimating such perceived effects, alongside the process through which agents update them, using a novel architecture built upon an expressive family of deep state-space models. Through application to the analysis of UNOS organ donation acceptance decisions, we demonstrate that our approach can bring valuable insights into the factors that govern decision processes and how they change over time",
    "volume": "main",
    "checked": true,
    "id": "009a6834cf8fccf2c93e7dbb766437b0ce96a33d",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=g8NJR6fCCl8": {
    "title": "NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning",
    "abstract": "Deployment of machine learning models in real high-risk settings (e.g. healthcare) often depends not only on model's accuracy but also on its fairness, robustness and interpretability. Generalized Additive Models (GAMs) have a long history of use in these high-risk domains, but lack desirable features of deep learning such as differentiability and scalability. In this work, we propose a neural GAM (NODE-GAM) and neural GAM (NODE-GAM) that scale well to large datasets, while remaining interpretable and accurate. We show that our proposed models have comparable accuracy to other non-interpretable models, and outperform other GAMs on large datasets. We also show that our models are more accurate in self-supervised learning setting when access to labeled data is limited",
    "volume": "main",
    "checked": true,
    "id": "44cb23a83d1ebc347dfb16174e0eb59e713ed27c",
    "citation_count": 16
  },
  "https://openreview.net/forum?id=6Pe99Juo9gd": {
    "title": "Learning Value Functions from Undirected State-only Experience",
    "abstract": "This paper tackles the problem of learning value functions from undirected state-only experience (state transitions without action labels i.e. ( s, s (cid:48) , r ) tuples). We ﬁrst theoretically characterize the applicability of Q-learning in this setting. We show that tabular Q-learning in discrete Markov decision processes (MDPs) learns the same value function under any arbitrary reﬁnement of the action space. This theoretical result motivates the design of Latent Action Q-learning or LAQ, an ofﬂine RL method that can learn effective value functions from state-only experience. Latent Action Q-learning (LAQ) learns value functions using Q-learning on discrete latent actions obtained through a latent-variable future prediction model. We show that LAQ can recover value functions that have high correlation with value functions learned using ground truth actions. Value functions learned using LAQ lead to sample efﬁcient acquisition of goal-directed behavior, can be used with domain-speciﬁc low-level controllers, and facilitate transfer across embodiments. Our experiments in 5 environments ranging from 2D grid world to 3D visual navigation in realistic environments demonstrate the beneﬁts of LAQ over simpler alternatives, imitation learning oracles, and competing methods",
    "volume": "main",
    "checked": true,
    "id": "c3053ff94d26b1416ac8828b8cce0f1cdd47df99",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=LcF-EEt8cCC": {
    "title": "Denoising Likelihood Score Matching for Conditional Score-based Data Generation",
    "abstract": "Many existing conditional score-based data generation methods utilize Bayes' theorem to decompose the gradients of a log posterior density into a mixture of scores. These methods facilitate the training procedure of conditional score models, as a mixture of scores can be separately estimated using a score model and a classifier. However, our analysis indicates that the training objectives for the classifier in these methods may lead to a serious score mismatch issue, which corresponds to the situation that the estimated scores deviate from the true ones. Such an issue causes the samples to be misled by the deviated scores during the diffusion process, resulting in a degraded sampling quality. To resolve it, we formulate a novel training objective, called Denoising Likelihood Score Matching (DLSM) loss, for the classifier to match the gradients of the true log likelihood density. Our experimental evidence shows that the proposed method outperforms the previous methods on both Cifar-10 and Cifar-100 benchmarks noticeably in terms of several key evaluation metrics. We thus conclude that, by adopting DLSM, the conditional scores can be accurately modeled, and the effect of the score mismatch issue is alleviated",
    "volume": "main",
    "checked": true,
    "id": "183aec8376b39ae2a1707a436266cddaf8f05596",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=tV3N0DWMxCg": {
    "title": "Natural Posterior Network: Deep Bayesian Predictive Uncertainty for Exponential Family Distributions",
    "abstract": "Uncertainty awareness is crucial to develop reliable machine learning models. In this work, we propose the Natural Posterior Network (NatPN) for fast and highquality uncertainty estimation for any task where the target distribution belongs to the exponential family. Thus, NatPN finds application for both classification and general regression settings. Unlike many previous approaches, NatPN does not require out-of-distribution (OOD) data at training time. Instead, it leverages Normalizing Flows to fit a single density on a learned low-dimensional and taskdependent latent space. For any input sample, NatPN uses the predicted likelihood to perform a Bayesian update over the target distribution. Theoretically, NatPN assigns high uncertainty far away from training data. Empirically, our extensive experiments on calibration and OOD detection show that NatPN delivers highly competitive performance for classification, regression and count prediction tasks",
    "volume": "main",
    "checked": true,
    "id": "b5ad8e8d9ab0ea57f448fe58847d5bce3ce5cb0c",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=9wOQOgNe-w": {
    "title": "Differentiable DAG Sampling",
    "abstract": "We propose a new differentiable probabilistic model over DAGs (DP-DAG). DP-DAG allows fast and differentiable DAG sampling suited to continuous optimization. To this end, DP-DAG samples a DAG by successively (1) sampling a linear ordering of the node and (2) sampling edges consistent with the sampled linear ordering. We further propose VI-DP-DAG, a new method for DAG learning from observational data which combines DP-DAG with variational inference. Hence, VI-DP-DAG approximates the posterior probability over DAG edges given the observed data. VI-DP-DAG is guaranteed to output a valid DAG at any time during training and does not require any complex augmented Lagrangian optimization scheme in contrast to existing differentiable DAG learning approaches. In our extensive experiments, we compare VI-DP-DAG to other differentiable DAG learning baselines on synthetic and real datasets. VI-DP-DAG signiﬁcantly improves DAG structure and causal mechanism learning while training faster than competitors. DAG structure. We evaluate the learning of the DAG structure by comparing the ground-truth adjacency matrix A ∗ ij to the learned score S ij . For baselines, we use entries from the weighted adjacency matrix S ij = A ij as scores for directed edges, and S ij = A ij + A ji as scores for undirected edges. For DP-DAG, we use the edge probability S ij = P φ ( U π ( i ) π ( j ) ) as scores for directed edges and S ij = P φ ( U π ( i ) π ( j ) ) + P φ ( U π ( j ) π ( i ) ) as scores for undirected edges. In this case, the permutation π (or equivalently Π ) is deterministically computed by removing Gumbel noise in the forward sampling step. The directed scores (Dir-) indicate if an edge with a speciﬁc direction exists between two nodes. The undirected scores (Un-) indicates if an edge without a speciﬁc direction exists between two nodes. We compare the directed and undirected scores to the ground-truth binary adjacency matrix by using the area under the curve of precision-recall (AUC-PR) and the area under the receiver operating characteristic curve (AUC-ROC) . These metrics have the important advantage to be independent of a threshold choice (Vowels et al., 2021). Thus, AUC-PR and AUC-ROC are better indicators of the true performance of the models contrary to other structure metrics like Structural Hamming Distance which needs to arbitrary select a threshold",
    "volume": "main",
    "checked": true,
    "id": "60ef888df689fb0f0503fffcf53c9ce980b19c10",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=nc0ETaieux": {
    "title": "Minimax Optimality (Probably) Doesn't Imply Distribution Learning for GANs",
    "abstract": "Arguably the most fundamental question in the theory of generative adversarial networks (GANs) is to understand to what extent GANs can actually learn the underlying distribution. Theoretical and empirical evidence (see e.g. [ARZ18]) suggests local optimality of the empirical training objective is insufficient. Yet, it does not rule out the possibility that achieving a true population minimax optimal solution might imply distribution learning. In this paper, we show that standard cryptographic assumptions imply that this stronger condition is still insufficient. Namely, we show that if local pseudorandom generators (PRGs) exist, then for a large family of natural continuous target distributions, there are ReLU network generators of constant depth and polynomial size which take Gaussian random seeds so that (i) the output is far in Wasserstein distance from the target distribution, but (ii) no polynomially large Lipschitz discriminator ReLU network can detect this. This implies that even achieving a population minimax optimal solution to the Wasserstein GAN objective is likely insufficient for distribution learning in the usual statistical sense. Our techniques reveal a deep connection between GANs and PRGs, which we believe will lead to further insights into the computational landscape of GANs",
    "volume": "main",
    "checked": true,
    "id": "0e442e1d0e37abca83e83ca02ba7a74088daa55c",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=I1hQbx10Kxn": {
    "title": "On Bridging Generic and Personalized Federated Learning for Image Classification",
    "abstract": "Federated learning is promising for its capability to collaboratively train models with multiple clients without accessing their data, but vulnerable when clients' data distributions diverge from each other. This divergence further leads to a dilemma: \"Should we prioritize the learned model's generic performance (for future use at the server) or its personalized performance (for each client)?\" These two, seemingly competing goals have divided the community to focus on one or the other, yet in this paper we show that it is possible to approach both at the same time. Concretely, we propose a novel federated learning framework that explicitly decouples a model's dual duties with two prediction tasks . On the one hand, we introduce a family of losses that are robust to non-identical class distributions, enabling clients to train a generic predictor with a consistent objective across them. On the other hand, we formulate the personalized predictor as a lightweight adaptive module that is learned to minimize each client's empirical risk on top of the generic predictor. With this two-loss, two-predictor framework which we name Federated Robust Decoupling (F ED -R O D) , the learned model can simultaneously achieve state-of-the-art generic and personalized performance, essentially bridging the two tasks",
    "volume": "main",
    "checked": true,
    "id": "77a27cd55900bc12c9e548fffcac160227d8448b",
    "citation_count": 14
  },
  "https://openreview.net/forum?id=ltM1RMZntpu": {
    "title": "Weighted Training for Cross-Task Learning",
    "abstract": "In this paper, we introduce Target-Aware Weighted Training (TAWT), a weighted training algorithm for cross-task learning based on minimizing a representationbased task distance between the source and target tasks. We show that TAWT is easy to implement, is computationally efficient, requires little hyperparameter tuning, and enjoys non-asymptotic learning-theoretic guarantees. The effectiveness of TAWT is corroborated through extensive experiments with BERT on four sequence tagging tasks in natural language processing (NLP), including part-ofspeech (PoS) tagging, chunking, predicate detection, and named entity recognition (NER). As a byproduct, the proposed representation-based task distance allows one to reason in a theoretically principled way about several critical aspects of cross-task learning, such as the choice of the source data and the impact of fine-tuning",
    "volume": "main",
    "checked": true,
    "id": "0992638c434257f4b9691637dd5ada046eb14b17",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=Nfl-iXa-y7R": {
    "title": "Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models",
    "abstract": "Overparameterized neural networks generalize well but are expensive to train. Ideally, one would like to reduce their computational cost while retaining their generalization beneﬁts. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or diﬃculty in sparsifying all model components. The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is diﬃcult and expensive. To address this, our main insight is to optimize over a continuous superset of sparse matrices with a ﬁxed structure known as products of butterﬂy matrices. As butterﬂy matrices are not hardware eﬃcient, we propose simple variants of butterﬂy (block and ﬂat) to take advantage of modern hardware. Our method (Pixelated Butterﬂy) uses a simple ﬁxed sparsity pattern based on ﬂat block butterﬂy and low-rank matrices to sparsify most network layers (e.g., attention, MLP). We empirically validate that Pixelated Butterﬂy is 3 × faster than butterﬂy and speeds up training to achieve favorable accuracy–eﬃciency tradeoﬀs. On the ImageNet classiﬁcation and WikiText-103 language modeling tasks, our sparse models train up to 2.5 × faster than the dense MLP-Mixer, Vision Transformer, and GPT-2 medium with no drop in accuracy. These results relate the generalization bound of sparse models to that of dense models",
    "volume": "main",
    "checked": true,
    "id": "90b21dbad8969b74d704eed15a3d98722a88e464",
    "citation_count": 15
  },
  "https://openreview.net/forum?id=8in_5gN9I0": {
    "title": "Triangle and Four Cycle Counting with Predictions in Graph Streams",
    "abstract": "We propose data-driven one-pass streaming algorithms for estimating the number of triangles and four cycles, two fundamental problems in graph analytics that are widely studied in the graph data stream literature. Recently, Hsu et al. (2019a) and Jiang et al. (2020) applied machine learning techniques in other data stream problems, using a trained oracle that can predict certain properties of the stream elements to improve on prior \"classical\" algorithms that did not use oracles. In this paper, we explore the power of a \"heavy edge\" oracle in multiple graph edge streaming models. In the adjacency list model, we present a one-pass triangle counting algorithm improving upon the previous space upper bounds without such an oracle. In the arbitrary order model, we present algorithms for both triangle and four cycle estimation with fewer passes and the same space complexity as in previous algorithms, and we show several of these bounds are optimal. We analyze our algorithms under several noise models, showing that the algorithms perform well even when the oracle errs. Our methodology expands upon prior work on \"classical\" streaming algorithms, as previous multi-pass and random order streaming algorithms can be seen as special cases of our algorithms, where the first pass or random order was used to implement the heavy edge oracle. Lastly, our experiments demonstrate advantages of the proposed method compared to state-of-theart streaming algorithms",
    "volume": "main",
    "checked": true,
    "id": "2abbbc44ac51362540cdac1e34a99c03091424da",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=SidzxAb9k30": {
    "title": "Near-Optimal Reward-Free Exploration for Linear Mixture MDPs with Plug-in Solver",
    "abstract": "Although model-based reinforcement learning (RL) approaches are considered more sample efficient, existing algorithms are usually relying on sophisticated planning algorithm to couple tightly with the model-learning procedure. Hence the learned models may lack the ability of being re-used with more specialized planners. In this paper we address this issue and provide approaches to learn an RL model efficiently without the guidance of a reward signal. In particular, we take a plug-in solver approach, where we focus on learning a model in the exploration phase and demand that any planning algorithm on the learned model can give a near-optimal policy. specifically, we focus on the linear mixture MDP setting, where the probability transition matrix is a (unknown) convex combination of a set of existing models. We show that, by establishing a novel exploration algorithm, the plug-in approach learns a model by taking Õ(dH/ǫ) episodes with the environment and any ǫ-optimal planner on the model gives an O(ǫ)-optimal policy on the original model. This sample complexity matches our lower bound for non-plug-in approaches and is statistically optimal. We achieve this result by leveraging a careful maximum total-variance bound using Bernstein inequality and properties specified to linear mixture MDPs",
    "volume": "main",
    "checked": true,
    "id": "558f2176a0023df3df5cc99c8cf41bae0f7427c6",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=H94a1_Pyr-6": {
    "title": "Auto-scaling Vision Transformers without Training",
    "abstract": "This work targets automated designing and scaling of Vision Transformers (ViTs). The motivation comes from two pain spots: 1) the lack of efﬁcient and principled methods for designing and scaling ViTs; 2) the tremendous computational cost of training ViT that is much heavier than its convolution counterpart. To tackle these issues, we propose As-ViT , an auto-scaling framework for ViTs without training, which automatically discovers and scales up ViTs in an efﬁcient and principled manner. Speciﬁcally, we ﬁrst design a \"seed\" ViT topology by leveraging a training-free search process. This extremely fast search is fulﬁlled by a comprehensive study of ViT's network complexity, yielding a strong Kendall-tau correlation with ground-truth accuracies. Second, starting from the \"seed\" topology, we automate the scaling rule for ViTs by growing widths/depths to different ViT layers. This results in a series of architectures with different numbers of parameters in a single run. Finally, based on the observation that ViTs can tolerate coarse tokenization in early training stages, we propose a progressive tokenization strategy to train ViTs faster and cheaper. As a uniﬁed framework, As-ViT achieves strong performance on classiﬁcation (83.5% top1 on ImageNet-1k) and detection (52.7% mAP on COCO) without any manual crafting nor scaling of ViT architectures: the end-to-end model design and scaling process costs only 12 hours on one V100 GPU . Our code is available at https://github.com/VITA-Group/AsViT",
    "volume": "main",
    "checked": true,
    "id": "3c7f3b153c2b5b4074d95ac9d659a267a2bafa3f",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=LtKcMgGOeLt": {
    "title": "When Vision Transformers Outperform ResNets without Pre-training or Strong Data Augmentations",
    "abstract": "Vision Transformers (ViTs) and MLPs signal further efforts on replacing hand-wired features or inductive biases with general-purpose neural architectures. Existing works empower the models by massive data, such as large-scale pre-training and/or repeated strong data augmentations, and still report optimization-related problems (e.g., sensitivity to initialization and learning rates). Hence, this paper investigates ViTs and MLP-Mixers from the lens of loss geometry, intending to improve the models' data efﬁciency at training and generalization at inference. Visualization and Hessian reveal extremely sharp local minima of converged models. By promoting smoothness with a recently proposed sharpness-aware optimizer, we substantially improve the accuracy and robustness of ViTs and MLP-Mixers on various tasks spanning supervised, adversarial, contrastive, and transfer learning (e.g., +5.3% and +11.0% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively, with the simple Inception-style prepro-cessing). We show that the improved smoothness attributes to sparser active neurons in the ﬁrst few layers. The resultant ViTs outperform ResNets of similar size and throughput when trained from scratch on ImageNet without large-scale pre-training or strong data augmentations. Model checkpoints are available at https://github.com/google-research/vision_transformer . and MLP-Mixer architectures used in this We train all the architectures with image resolution 224 × 224",
    "volume": "main",
    "checked": true,
    "id": "42a7015e48a1e00b70ebb442a82afb4b10017c0b",
    "citation_count": 115
  },
  "https://openreview.net/forum?id=T8vZHIRTrY": {
    "title": "Understanding Domain Randomization for Sim-to-real Transfer",
    "abstract": "Reinforcement learning encounters many challenges when applied directly in the real world. Sim-to-real transfer is widely used to transfer the knowledge learned from simulation to the real world. Domain randomization—one of the most popular algorithms for sim-to-real transfer—has been demonstrated to be effective in various tasks in robotics and autonomous driving. Despite its empirical successes, theoretical understanding on why this simple algorithm works is limited. In this paper, we propose a theoretical framework for sim-to-real transfers, in which the simulator is modeled as a set of MDPs with tunable parameters (corresponding to unknown physical parameters such as friction). We provide sharp bounds on the sim-to-real gap—the difference between the value of policy returned by domain randomization and the value of an optimal policy for the real world. We prove that sim-to-real transfer can succeed under mild conditions without any real-world training samples. Our theory also highlights the importance of using memory (i.e., history-dependent policies) in domain randomization. Our proof is based on novel techniques that reduce the problem of bounding the sim-to-real gap to the problem of designing efficient learning algorithms for infinite-horizon MDPs, which we believe are of independent interest",
    "volume": "main",
    "checked": true,
    "id": "2340e5b64224b66c10e603898c0cc24c8e2793c3",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=dDjSKKA5TP1": {
    "title": "Incremental False Negative Detection for Contrastive Learning",
    "abstract": "Self-supervised learning has recently shown great potential in vision tasks via contrastive learning, which aims to discriminate each image, or instance, in the dataset. However, such instance-level learning ignores the semantic relationship between instances and repels the anchor equally from the semantically similar samples, termed as \"false negatives\". In this work, we first empirically highlight that the unfavorable effect from false negatives is more significant for the datasets containing images with more semantic concepts. To address the issue, we introduce a novel incremental false negative detection for self-supervised contrastive learning. Following the training process, when the encoder is gradually better-trained and the embedding space becomes more semantically structural, our method incrementally detects more reliable false negatives. Subsequently, during contrastive learning, we discuss two strategies to explicitly remove the detected false negatives. Extensive experiments show that our proposed method outperforms other self-supervised contrastive learning frameworks on multiple benchmarks within a limited compute. The source code is available at https://github.com/tsaishien-chen/IFND",
    "volume": "main",
    "checked": true,
    "id": "7c1d2b33480cfb6da5b2573a8f8d113dabc39cfe",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=nioAdKCEdXB": {
    "title": "Likelihood Training of Schrödinger Bridge using Forward-Backward SDEs Theory",
    "abstract": "Schrödinger Bridge (SB) is an entropy-regularized optimal transport problem that has received increasing attention in deep generative modeling for its mathematical flexibility compared to the Scored-based Generative Model (SGM). However, it remains unclear whether the optimization principle of SB relates to the modern training of deep generative models, which often rely on constructing log-likelihood objectives.This raises questions on the suitability of SB models as a principled alternative for generative applications. In this work, we present a novel computational framework for likelihood training of SB models grounded on Forward-Backward Stochastic Differential Equations Theory – a mathematical methodology appeared in stochastic optimal control that transforms the optimality condition of SB into a set of SDEs. Crucially, these SDEs can be used to construct the likelihood objectives for SB that, surprisingly, generalizes the ones for SGM as special cases. This leads to a new optimization principle that inherits the same SB optimality yet without losing applications of modern generative training techniques, and we show that the resulting training algorithm achieves comparable results on generating realistic images on MNIST, CelebA, and CIFAR10. Our code is available at https://github.com/ghliu/SB-FBSDE",
    "volume": "main",
    "checked": true,
    "id": "7863b5390be3b55abfb8893c682d4ff88ea7149c",
    "citation_count": 22
  },
  "https://openreview.net/forum?id=xy_2w3J3kH": {
    "title": "Communication-Efficient Actor-Critic Methods for Homogeneous Markov Games",
    "abstract": "We develop",
    "volume": "main",
    "checked": true,
    "id": "9ef78757ea89fef489e404d83049ae60f45e88f5",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=nHpzE7DqAnG": {
    "title": "Does your graph need a confidence boost? Convergent boosted smoothing on graphs with tabular node features",
    "abstract": "For supervised learning with tabular data, decision tree ensembles produced via boosting techniques generally dominate real-world applications involving iid training/test sets. However for graph data where the iid assumption is violated due to structured relations between samples, it remains unclear how to best incorporate this structure within existing boosting pipelines. To this end, we propose a generalized framework for iterating boosting with graph propagation steps that share node/sample information across edges connecting related samples. Unlike previous efforts to integrate graph-based models with boosting, our approach is anchored in a principled meta loss function such that provable convergence can be guaranteed under relatively mild assumptions. Across a variety of non-iid graph datasets with tabular node features, our method achieves comparable or superior performance than both tabular and graph neural network models, as well as existing hybrid strategies that combine the two. Beyond producing better predictive performance than recently proposed graph models, our proposed techniques are easy to implement, computationally more efficient, and enjoy stronger theoretical guarantees (which make our results more reproducible)",
    "volume": "main",
    "checked": true,
    "id": "229e883a8575182dfe0b0469e059a20e5ff3d2b8",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=Mng8CQ9eBW": {
    "title": "BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models",
    "abstract": "Pre-trained Natural Language Processing (NLP) models can be easily adapted to a variety of downstream language tasks. This significantly accelerates the development of language models. However, NLP models have been shown to be vulnerable to backdoor attacks, where a pre-defined trigger word in the input text causes model misprediction. Previous NLP backdoor attacks mainly focus on some specific tasks. This makes those attacks less general and applicable to other kinds of NLP models and tasks. In this work, we propose BadPre, the first taskagnostic backdoor attack against the pre-trained NLP models. The key feature of our attack is that the adversary does not need prior information about the downstream tasks when implanting the backdoor to the pre-trained model. When this malicious model is released, any downstream models transferred from it will also inherit the backdoor, even after the extensive transfer learning process. We further design a simple yet effective strategy to bypass a state-of-the-art defense. Experimental results indicate that our approach can compromise a wide range of downstream NLP tasks in an effective and stealthy way",
    "volume": "main",
    "checked": true,
    "id": "6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=zBOI9LFpESK": {
    "title": "Learning Generalizable Representations for Reinforcement Learning via Adaptive Meta-learner of Behavioral Similarities",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "855bcc08e94a5bd82865b79b6bb9af4727e65726",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=wv6g8fWLX2q": {
    "title": "TAMP-S2GCNets: Coupling Time-Aware Multipersistence Knowledge Representation with Spatio-Supra Graph Convolutional Networks for Time-Series Forecasting",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "ee8d0023ec8e4fc6df19935a94e0d186432d0239",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=e42KbIw6Wb": {
    "title": "Pix2seq: A Language Modeling Framework for Object Detection",
    "abstract": "We present Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, we cast object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural network to perceive the image and generate the desired sequence. Our approach is based mainly on the intuition that if a neural network knows about where and what the objects are, we just need to teach it how to read them out. Beyond the use of task-specific data augmentations, our approach makes minimal assumptions about the task, yet it achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms.1 Pix2Seq ymin=9 xmin=7 ymax=67 xmax=98 train ...... ymin=8 xmin=4 ymax=99 xmax=97 motocycle ...... ymin=1 xmin=57 ymax=99 xmax=72 Person ...... Cmd: detect objects Figure 1: Illustration of Pix2Seq framework for object detection. The neural net perceives an image and generates a sequence of tokens that correspond to bounding boxes and class labels",
    "volume": "main",
    "checked": true,
    "id": "19b3b074d38b250d024920732ae51a8ffa0996dd",
    "citation_count": 60
  },
  "https://openreview.net/forum?id=BwPaPxwgyQb": {
    "title": "Provable Learning-based Algorithm For Sparse Recovery",
    "abstract": "Recovering sparse parameters from observational data is a fundamental problem in machine learning with wide applications. Many classic algorithms can solve this problem with theoretical guarantees, but their performances rely on choosing the correct hyperparameters. Besides, hand-designed algorithms do not fully exploit the particular problem distribution of interest. In this work, we propose a deep learning method for algorithm learning called PLISA ( P rovable L earning-based I terative S parse recovery A lgorithm). PLISA is designed by unrolling a classic path-following algorithm for sparse recovery, with some components being more ﬂexible and learnable. We theoretically show the improved recovery accuracy achievable by PLISA . Furthermore, we analyze the empirical Rademacher complexity of PLISA to characterize its generalization ability to solve new problems outside the training set. This paper contains novel theoretical contributions to the area of learning-based algorithms in the sense that (i) PLISA is generically applicable to a broad class of sparse estimation problems, (ii) generalization analysis has received less attention so far, and (iii) our analysis makes novel connections between the generalization ability and algorithmic properties such as stability and convergence of the unrolled algorithm, which leads to a tighter bound that can explain the empirical observations. The techniques could potentially be applied to analyze other learning-based algorithms in the literature",
    "volume": "main",
    "checked": true,
    "id": "0aa05b938282ebf4742777f3c03f1ac2a85e10cb",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=PQTW3iG4sC-": {
    "title": "On feature learning in neural networks with global convergence guarantees",
    "abstract": "We study the optimization of wide neural networks (NNs) via gradient ﬂow (GF) in setups that allow feature learning while admitting non-asymptotic global convergence guarantees. First, for wide shallow NNs under the mean-ﬁeld scaling and with a general class of activation functions, we prove that when the input dimension is no less than the size of the training set, the training loss converges to zero at a linear rate under GF. Building upon this analysis, we study a model of wide multi-layer NNs whose second-to-last layer is trained via GF, for which we also prove a linear-rate convergence of the training loss to zero, but regardless of the input dimension. We also show empirically that, unlike in the Neural Tangent Kernel (NTK) regime, our multi-layer model exhibits feature learning and can achieve better generalization performance than its NTK counterpart",
    "volume": "main",
    "checked": true,
    "id": "63b82ca8518012a371d5ee29ce2de16bb1769c18",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=NMEceG4v69Y": {
    "title": "CycleMLP: A MLP-like Architecture for Dense Prediction",
    "abstract": "This paper presents a simple MLP-like architecture, CycleMLP, which is a versatile backbone for visual recognition and dense predictions, unlike modern MLP architectures, e.g., MLP-Mixer [49], ResMLP [50], and gMLP [35], whose architectures are correlated to image size and thus are infeasible in object detection and segmentation. CycleMLP has two advantages compared to modern approaches. (1) It can cope with various image sizes. (2) It achieves linear computational complexity to image size by using local windows. In contrast, previous MLPs have quadratic computations because of their fully spatial connections. We build a family of models that surpass existing MLPs and achieve a comparable accuracy (83.2%) on ImageNet-1K classification compared to the state-of-the-art Transformer such as Swin Transformer [36] (83.3%), but using fewer parameters and FLOPs. We expand the MLPlike models' applicability, making them a versatile backbone for dense prediction tasks. CycleMLP aims to provide a competitive baseline on object detection, instance segmentation, and semantic segmentation for MLP models. In particular, CycleMLP achieves 45.1 mIoU on ADE20K val, comparable to Swin (45.2 mIOU). Code is available at https://github.com/ShoufaChen/CycleMLP",
    "volume": "main",
    "checked": true,
    "id": "f75cddf2d42ed01b34686704eb3504becef67442",
    "citation_count": 78
  },
  "https://openreview.net/forum?id=FEDfGWVZYIn": {
    "title": "RelaxLoss: Defending Membership Inference Attacks without Losing Utility",
    "abstract": "As a long-term threat to the privacy of training data, membership inference attacks (MIAs) emerge ubiquitously in machine learning models. Existing works evidence strong connection between the distinguishability of the training and testing loss distributions and the model's vulnerability to MIAs. Motivated by existing results, we propose a novel training framework based on a relaxed loss (RelaxLoss) with a more achievable learning target, which leads to narrowed generalization gap and reduced privacy leakage. RelaxLoss is applicable to any classification model with added benefits of easy implementation and negligible overhead. Through extensive evaluations on five datasets with diverse modalities (images, medical data, transaction records), our approach consistently outperforms state-of-theart defense mechanisms in terms of resilience against MIAs as well as model utility. Our defense is the first that can withstand a wide range of attacks while preserving (or even improving) the target model's utility. Source code is available at https://github.com/DingfanChen/RelaxLoss",
    "volume": "main",
    "checked": true,
    "id": "9a8e8693fe5c0b889743a88b4f6c5a13e1475d78",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=iMqTLyfwnOO": {
    "title": "Augmented Sliced Wasserstein Distances",
    "abstract": "While theoretically appealing, the application of the Wasserstein distance to large-scale machine learning problems has been hampered by its prohibitive computational cost. The sliced Wasserstein distance and its variants improve the computational efficiency through random projection, yet they suffer from low projection efficiency because the majority of projections result in trivially small values. In this work, we propose a new family of distance metrics, called augmented sliced Wasserstein distances (ASWDs), constructed by first mapping samples to higher-dimensional hypersurfaces parameterized by neural networks. It is derived from a key observation that (random) linear projections of samples residing on these hypersurfaces would translate to much more flexible projections in the original sample space, so they can capture complex structures of the data distribution. We show that the hypersurfaces can be optimized by gradient ascent efficiently. We provide the condition under which the ASWD is a valid metric and show that this can be obtained by an injective neural network architecture. Numerical results demonstrate that the ASWD significantly outperforms other Wasserstein variants for both synthetic and real-world problems",
    "volume": "main",
    "checked": true,
    "id": "5c2e520e88203b1f4cb82c0f9a9682d9cf2e969a",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=PgNEYaIc81Q": {
    "title": "ComPhy: Compositional Physical Reasoning of Objects and Events from Videos",
    "abstract": "Objects' motions in nature are governed by complex interactions and their properties. While some properties, such as shape and material, can be identiﬁed via the object's visual appearances, others like mass and electric charge are not directly visible. The compositionality between the visible and hidden properties poses unique challenges for AI models to reason from the physical world, whereas humans can effortlessly infer them with limited observations. Existing studies on video reasoning mainly focus on visually observable elements such as object appearance, movement, and contact interaction. In this paper, we take an initial step to highlight the importance of inferring the hidden physical properties not directly observable from visual appearances, by introducing the Compositional Physical Reasoning (ComPhy) dataset 1 . For a given set of objects, ComPhy includes few videos of them moving and interacting under different initial conditions. The model is evaluated based on its capability to unravel the compositional hidden properties, such as mass and charge, and use this knowledge to answer a set of questions posted on one of the videos. Evaluation results of several state-of-the-art video reasoning models on ComPhy show unsatisfactory performance as they fail to capture these hidden properties. We further propose an oracle neural-symbolic framework named Compositional Physics Learner (CPL), combining visual perception, physical property learning, dynamic prediction, and symbolic execution into a uniﬁed framework. CPL can effectively identify objects' physical properties from their interactions and predict their dynamics to answer questions",
    "volume": "main",
    "checked": true,
    "id": "e0c2db93e1aa2c9f089423390169cb60bc175ba3",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=gFDFKC4gHL4": {
    "title": "How Did the Model Change? Efficiently Assessing Machine Learning API Shifts",
    "abstract": "Machine learning (ML) prediction APIs are increasingly widely used. An ML API can change over time due to model updates or retraining. This presents a key challenge in the usage of the API because it's often not clear to the user if and how the ML model has changed. Model shifts can affect downstream application performance and also create oversight issues (e.g. if consistency is desired). In this paper, we initiate a systematic investigation of ML API shifts. We first quantify the performance shifts from 2020 to 2021 of popular ML APIs from Google, Microsoft, Amazon, and others on a variety of datasets. We identified significant model shifts in 12 out of 36 cases we investigated. Interestingly, we found several datasets where the API's predictions became significantly worse over time. This motivated us to formulate the API shift assessment problem at a more fine-grained level as estimating how the API model's confusion matrix changes over time when the data distribution is constant. Monitoring confusion matrix shifts using standard random sampling can require a large number of samples, which is expensive as each API call costs a fee. We propose a principled adaptive sampling algorithm, MASA, to efficiently estimate confusion matrix shifts. MASA can accurately estimate the confusion matrix shifts in commercial ML APIs using up to 90% fewer samples compared to random sampling. This work establishes ML API shifts as an important problem to study and provides a cost-effective approach to monitor such shifts",
    "volume": "main",
    "checked": true,
    "id": "1110ee7aad23df7edd95701b80ecd99631d5764f",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=moHCzz6D5H3": {
    "title": "Peek-a-Boo: What (More) is Disguised in a Randomly Weighted Neural Network, and How to Find It Efficiently",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "d7d1cee22d578a833dfd91282ff501b0a65a86ae",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=SYuJXrXq8tw": {
    "title": "Sparsity Winning Twice: Better Robust Generalization from More Efficient Training",
    "abstract": "Recent studies demonstrate that deep networks, even robustiﬁed by the state-of-the-art adversarial training (AT), still suffer from large robust generalization gaps, in addition to the much more expensive training costs than standard training. In this paper, we investigate this intriguing problem from a new perspective, i.e., injecting appropriate forms of sparsity during adversarial training. We introduce two alternatives for sparse adversarial training: (i) static sparsity , by leveraging recent results from the lottery ticket hypothesis to identify critical sparse subnetworks arising from the early training; (ii) dynamic sparsity , by allowing the sparse subnetwork to adaptively adjust its connectivity pattern (while sticking to the same sparsity ratio) throughout training. We ﬁnd both static and dynamic sparse methods to yield win-win: substantially shrinking the robust generalization gap and alleviating the robust overﬁtting, meanwhile signiﬁcantly saving training and inference FLOPs. Extensive experiments validate our proposals with multiple network architectures on diverse datasets, including CIFAR-10/100 and Tiny-ImageNet. For example, our methods reduce robust generalization gap and overﬁtting by 34 . 44% and 4 . 02% , with comparable robust/standard accuracy boosts and 87 . 83% / 87 . 82% training/inference FLOPs savings on CIFAR-100 with ResNet-18. Besides, our approaches can be organically combined",
    "volume": "main",
    "checked": true,
    "id": "01594f00b0deed32cba4fc4ea8c74b60be31db4a",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=DrZXuTGg2A-": {
    "title": "Shuffle Private Stochastic Convex Optimization",
    "abstract": "In shuffle privacy, each user sends a collection of randomized messages to a trusted shuffler, the shuffler randomly permutes these messages, and the resulting shuffled collection of messages must satisfy differential privacy. Prior work in this model has largely focused on protocols that use a single round of communication to compute algorithmic primitives like means, histograms, and counts. We present interactive shuffle protocols for stochastic convex optimization. Our protocols rely on a new noninteractive protocol for summing vectors of bounded `2 norm. By combining this sum subroutine with mini-batch stochastic gradient descent, accelerated gradient descent, and Nesterov's smoothing method, we obtain loss guarantees for a variety of convex loss functions that significantly improve on those of the local model and sometimes match those of the central model",
    "volume": "main",
    "checked": true,
    "id": "63b14309ee6ec539499636ec7f51c4d43d9a4a12",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=Fza94Y8VS4a": {
    "title": "The Evolution of Uncertainty of Learning in Games",
    "abstract": "Learning-in-games has become an object of intense interest for ML due to its connections to numerous AI architectures. We study standard online learning in games but from a non-standard perspective. Instead of studying the behavior of a single initial condition and whether it converges to equilibrium or not, we study the behavior of a probability distribution/measure over a set of initial conditions. This initial uncertainty is well motivated both from a standard game-theoretic perspective (e.g. a modeler's uncertainty about the agents' initial beliefs, random external signals) as well as from a ML one (e.g. noisy measurements, system initialization from a dataset distribution). Despite this, little is formally known about whether and under what conditions uncertainty is amplified or reduced in these systems. We use the popular measure of differential entropy to quantify the evolution of uncertainty. We find that such analysis shares an intimate relationship with volume analysis, a technique which was recently used to demonstrate the occurrence of Lyapunov chaos when using Multiplicative Weights Update (MWU) or Followthe-Regularized-Leader (FTRL) algorithms in zero-sum games. This allows us to show that the differential entropy of these learning-in-game systems increases linearly with time, formalizing their increased unpredictability over time. We showcase the power of the framework by applying it in the study of multiple related systems, including different standard online optimization algorithms in numerous games and dynamics of evolutionary game theory",
    "volume": "main",
    "checked": true,
    "id": "9e4cc2da11615c4b1feb701d78cc4234fa9d321c",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=rWXfFogxRJN": {
    "title": "AdaAug: Learning Class- and Instance-adaptive Data Augmentation Policies",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "84dbb0f24bc6856ff353d2f5e393e9b756fe4531",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=MEpKGLsY8f": {
    "title": "Meta Discovery: Learning to Discover Novel Classes given Very Limited Data",
    "abstract": "NCDL. We conduct experiments on four benchmarks and compare our method with ﬁve competitive baselines Empirical results show that our method outperforms these baselines signiﬁcantly when novel-class data are very limited",
    "volume": "main",
    "checked": true,
    "id": "2967375a78cbf845c2119b50dceb957d50d205e9",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=ieNJYujcGDO": {
    "title": "Towards Understanding the Data Dependency of Mixup-style Training",
    "abstract": "In the Mixup training paradigm, a model is trained using convex combinations of data points and their associated labels. Despite seeing very few true data points during training, models trained using Mixup seem to still minimize the original empirical risk and exhibit better generalization and robustness on various tasks when compared to standard training. In this paper, we investigate how these benefits of Mixup training rely on properties of the data in the context of classification. For minimizing the original empirical risk, we compute a closed form for the Mixup-optimal classification, which allows us to construct a simple dataset on which minimizing the Mixup loss can provably lead to learning a classifier that does not minimize the empirical loss on the data. On the other hand, we also give sufficient conditions for Mixup training to also minimize the original empirical risk. For generalization, we characterize the margin of a Mixup classifier, and use this to understand why the decision boundary of a Mixup classifier can adapt better to the full structure of the training data when compared to standard training. In contrast, we also show that, for a large class of linear models and linearly separable datasets, Mixup training leads to learning the same classifier as standard training",
    "volume": "main",
    "checked": true,
    "id": "e19e9cf2aa468267f1f1f1075840d6b77f594c14",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=KJggliHbs8": {
    "title": "Node Feature Extraction by Self-Supervised Multi-scale Neighborhood Prediction",
    "abstract": "Learning on graphs has attracted significant attention in the learning community due to numerous real-world applications. In particular, graph neural networks (GNNs), which take numerical node features and graph structure as inputs, have been shown to achieve state-of-the-art performance on various graph-related learning tasks. Recent works exploring the correlation between numerical node features and graph structure via self-supervised learning have paved the way for further performance improvements of GNNs. However, methods used for extracting numerical node features from raw data are still graph-agnostic within standard GNN pipelines. This practice is sub-optimal as it prevents one from fully utilizing potential correlations between graph topology and node attributes. To mitigate this issue, we propose a new self-supervised learning framework, Graph Information Aided Node feature exTraction (GIANT). GIANT makes use of the eXtreme Multi-label Classification (XMC) formalism, which is crucial for fine-tuning the language model based on graph information, and scales to large datasets. We also provide a theoretical analysis that justifies the use of XMC over link prediction and motivates integrating XR-Transformers, a powerful method for solving XMC problems, into the GIANT framework. We demonstrate the superior performance of GIANT over the standard GNN pipeline on Open Graph Benchmark datasets: For example, we improve the accuracy of the top-ranked method GAMLP from 68.25% to 69.67%, SGC from 63.29% to 66.10% and MLP from 47.24% to 61.10% on the ogbn-papers100M dataset by leveraging GIANT. Our implementation is public available1",
    "volume": "main",
    "checked": true,
    "id": "259cbc1492c51d985bdafb67e48fa170471ee446",
    "citation_count": 14
  },
  "https://openreview.net/forum?id=hpBTIv2uy_E": {
    "title": "You are AllSet: A Multiset Function Framework for Hypergraph Neural Networks",
    "abstract": "Hypergraphs are used to model higher-order interactions amongst agents and there exist many practically relevant instances of hypergraph datasets. To enable the efficient processing of hypergraph data, several hypergraph neural network platforms have been proposed for learning hypergraph properties and structure, with a special focus on node classification tasks. However, almost all existing methods use heuristic propagation rules and offer suboptimal performance on benchmarking datasets. We propose AllSet, a new hypergraph neural network paradigm that represents a highly general framework for (hyper)graph neural networks and for the first time implements hypergraph neural network layers as compositions of two multiset functions that can be efficiently learned for each task and each dataset. The proposed AllSet framework also for the first time integrates Deep Sets and Set Transformers with hypergraph neural networks for the purpose of learning multiset functions and therefore allows for significant modeling flexibility and high expressive power. To evaluate the performance of AllSet, we conduct the most extensive experiments to date involving ten known benchmarking datasets and three newly curated datasets that represent significant challenges for hypergraph node classification. The results demonstrate that our method has the unique ability to either match or outperform all other hypergraph neural networks across the tested datasets: As an example, the performance improvements over existing methods and a new method based on heterogeneous graph neural networks are close to 4% on the Yelp and Zoo datasets, and 3% on the Walmart dataset. Our implementation is available online.1",
    "volume": "main",
    "checked": true,
    "id": "28a1a6e0b84747ed8bee294edc3606d4451e3c01",
    "citation_count": 20
  },
  "https://openreview.net/forum?id=qj1IZ-6TInc": {
    "title": "Real-Time Neural Voice Camouflage",
    "abstract": "Automatic speech recognition systems have created exciting possibilities for applications, however they also enable opportunities for systematic eavesdropping. We propose a method to camouflage a person's voice over-the-air from these systems without inconveniencing the conversation between people in the room. Standard adversarial attacks are not effective in real-time streaming situations because the characteristics of the signal will have changed by the time the attack is executed. We introduce predictive attacks, which achieve real-time performance by forecasting the attack that will be the most effective in the future. Under real-time constraints, our method jams the established speech recognition system DeepSpeech 3.9x more than baselines as measured through word error rate, and 6.6x more as measured through character error rate. We furthermore demonstrate our approach is practically effective in realistic environments over physical distances",
    "volume": "main",
    "checked": true,
    "id": "4e2dfe2b54bcd5d5c8178aca868959568298f0c8",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=J_F_qqCE3Z5": {
    "title": "DKM: Differentiable k-Means Clustering Layer for Neural Network Compression",
    "abstract": "Deep neural network (DNN) model compression for efficient on-device inference is becoming increasingly important to reduce memory requirements and keep user data on-device. To this end, we propose a novel differentiable k-means clustering layer (DKM) and its application to train-time weight clustering based DNN model compression. DKM casts k-means clustering as an attention problem and enables joint optimization of the DNN parameters and clustering centroids. Unlike prior works that rely on additional regularizers and parameters, DKM-based compression keeps the original loss function and model architecture fixed. We evaluated DKM-based compression on various DNN models for computer vision and natural language processing (NLP) tasks. Our results demonstrate that DKM delivers superior compression and accuracy trade-off on ImageNet1k and GLUE benchmarks. For example, DKM-based compression can offer 74.5% top-1 ImageNet1k accuracy on ResNet50 DNN model with 3.3MB model size (29.4xmodel compression factor). For MobileNet-v1, which is a challenging DNN to compress, DKM delivers 62.8% top-1 ImageNet1k accuracy with 0.74 MB model size (22.4xmodel compression factor). This result is 6.8% higher top-1 accuracy and 33% relatively smaller model size than the current state-of-the-art DNN compression algorithms. Additionally, DKM enables compression of DistilBERT model by 11.8xwith minimal (1.1%) accuracy loss on GLUE NLP benchmarks",
    "volume": "main",
    "checked": true,
    "id": "3f0574949341836549783869ac16e8082729a05d",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=mKDtUtxIGJ": {
    "title": "Deep Point Cloud Reconstruction",
    "abstract": "Point cloud obtained from 3D scanning is often sparse, noisy, and irregular. To cope with these issues, recent studies have been separately conducted to densify, denoise, and complete inaccurate point cloud. In this paper, we advocate that jointly solving these tasks leads to significant improvement for point cloud reconstruction. To this end, we propose a deep point cloud reconstruction network consisting of two stages: 1) a 3D sparse stacked-hourglass network as for the initial densification and denoising, 2) a refinement via transformers converting the discrete voxels into 3D points. In particular, we further improve the performance of transformer by a newly proposed module called amplified positional encoding. This module has been designed to differently amplify the magnitude of positional encoding vectors based on the points' distances for adaptive refinements. Extensive experiments demonstrate that our network achieves state-of-the-art performance among the recent studies in the ScanNet, ICL-NUIM, and ShapeNetPart datasets. Moreover, we underline the ability of our network to generalize toward real-world and unmet scenes. Implementation will be released soon",
    "volume": "main",
    "checked": true,
    "id": "cc48dd21e92f5f154f6c86495a224e4d69e29ea2",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=aTzMi4yV_RO": {
    "title": "Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs",
    "abstract": "The discovery of the disentanglement properties of the latent space in GANs motivated a lot of research to ﬁnd the semantically meaningful directions on it. In this paper, we suggest that the disentanglement property is closely related to the geometry of the latent space. In this regard, we propose an unsupervised method for ﬁnding the semantic-factorizing directions on the intermediate latent space of GANs based on the local geometry. Intuitively, our proposed method, called Local Basis , ﬁnds the principal variation of the latent space in the neighborhood of the base latent variable. Experimental results show that the local principal variation corresponds to the semantic factorization and traversing along it provides strong robustness to image traversal. Moreover, we suggest an explanation for the limited success in ﬁnding the global traversal directions in the latent space, especially W -space of StyleGAN2. We show that W -space is warped globally by comparing the local geometry, discovered from Local Basis, through the metric on Grassmannian Manifold. The global warpage implies that the latent space is not well-aligned globally and therefore the global traversal directions are bound to show limited success on it",
    "volume": "main",
    "checked": true,
    "id": "1e2770f58ba8b307d3048a4471cc42c489614484",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=C54V-xTWfi": {
    "title": "MonoDistill: Learning Spatial Features for Monocular 3D Object Detection",
    "abstract": "3D object detection is a fundamental and challenging task for 3D scene understanding, and the monocular-based methods can serve as an economical alternative to the stereo-based or LiDAR-based methods. However, accurately detecting objects in the 3D space from a single image is extremely difficult due to the lack of spatial cues. To mitigate this issue, we propose a simple and effective scheme to introduce the spatial information from LiDAR signals to the monocular 3D detectors, without introducing any extra cost in the inference phase. In particular, we first project the LiDAR signals into the image plane and align them with the RGB images. After that, we use the resulting data to train a 3D detector (LiDAR Net) with the same architecture as the baseline model. Finally, this LiDAR Net can serve as the teacher to transfer the learned knowledge to the baseline model. Experimental results show that the proposed method can significantly boost the performance of the baseline model and ranks the 1 place among all monocularbased methods on the KITTI benchmark. Besides, extensive ablation studies are conducted, which further prove the effectiveness of each part of our designs and illustrate what the baseline model has learned from the LiDAR Net. Our code will be released at https://github.com/monster-ghost/MonoDistill",
    "volume": "main",
    "checked": true,
    "id": "cfb38cc7825ea0e2260191771057213b6cf14bed",
    "citation_count": 15
  },
  "https://openreview.net/forum?id=P7FLfMLTSEX": {
    "title": "The Spectral Bias of Polynomial Neural Networks",
    "abstract": "Polynomial neural networks (PNNs) have been recently shown to be particularly effective at image generation and face recognition, where high-frequency information is critical. Previous studies have revealed that neural networks demonstrate a spectral bias towards low-frequency functions, which yields faster learning of low-frequency components during training. Inspired by such studies, we conduct a spectral analysis of the Neural Tangent Kernel (NTK) of PNNs. We find that the Π-Net family, i.e., a recently proposed parametrization of PNNs, speeds up the learning of the higher frequencies. We verify the theoretical bias through extensive experiments. We expect our analysis to provide novel insights into designing architectures and learning frameworks by incorporating multiplicative interactions via polynomials",
    "volume": "main",
    "checked": true,
    "id": "4b510291d020f9bb7bb3acea9a0f78af78f824b9",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=EMigfE6ZeS": {
    "title": "Hybrid Random Features",
    "abstract": "We propose a new class of random feature methods for linearizing softmax and Gaussian kernels called hybrid random features (HRFs) that automatically adapt the quality of kernel estimation to provide most accurate approximation in the defined regions of interest. Special instantiations of HRFs lead to well-known methods such as trigonometric (Rahimi & Recht, 2007) or (recently introduced in the context of linear-attention Transformers) positive random features (Choro-manski et al., 2021b). By generalizing Bochner's Theorem for softmax/Gaussian kernels and leveraging random features for compositional kernels, the HRF-mechanism provides strong theoretical guarantees - unbiased approximation and strictly smaller worst-case relative errors than its counterparts. We conduct exhaustive empirical evaluation of HRF ranging from pointwise kernel estimation experiments, through tests on data admitting clustering structure to benchmark-ing implicit-attention Transformers (also for downstream Robotics applications), demonstrating its quality in a wide spectrum of machine learning problems. manipulation task. The additional five regular RF-configurations did not train by producing Nan loss due to large variance of the underlying softmax kernel estimators",
    "volume": "main",
    "checked": true,
    "id": "f27e8c4731c575bd5f5db4c93ad8588f684dcbd0",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=kOu3-S3wJ7": {
    "title": "Filling the G_ap_s: Multivariate Time Series Imputation by Graph Neural Networks",
    "abstract": "Dealing with missing values and incomplete time series is a labor-intensive, tedious, inevitable task when handling data coming from real-world applications. Effective spatio-temporal representations would allow imputation methods to reconstruct missing temporal data by exploiting information coming from sensors at different locations. However, standard methods fall short in capturing the nonlinear time and space dependencies existing within networks of interconnected sensors and do not take full advantage of the available – and often strong – relational information. Notably, most state-of-the-art imputation methods based on deep learning do not explicitly model relational aspects and, in any case, do not exploit processing frameworks able to adequately represent structured spatio-temporal data. Conversely, graph neural networks have recently surged in popularity as both expressive and scalable tools for processing sequential data with relational inductive biases. In this work, we present the first assessment of graph neural networks in the context of multivariate time series imputation. In particular, we introduce a novel graph neural network architecture, named GRIN, which aims at reconstructing missing data in the different channels of a multivariate time series by learning spatio-temporal representations through message passing. Empirical results show that our model outperforms state-of-the-art methods in the imputation task on relevant real-world benchmarks with mean absolute error improvements often higher than 20%",
    "volume": "main",
    "checked": true,
    "id": "2e08a508fa9c6ae7195aa14dfe6c9e695e19aa33",
    "citation_count": 12
  },
  "https://openreview.net/forum?id=1zwleytEpYx": {
    "title": "Imitation Learning by Reinforcement Learning",
    "abstract": "Imitation Learning algorithms learn a policy from demonstrations of expert behavior. Somewhat counterintuitively, we show that, for deterministic experts, imitation learning can be done by reduction to reinforcement learning, which is commonly considered more difficult. We conduct experiments which confirm that our reduction works well in practice for a continuous control task",
    "volume": "main",
    "checked": true,
    "id": "08fead9e534ad82d46317284b070d0f17034bddb",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=hfU7Ka5cfrC": {
    "title": "Scalable One-Pass Optimisation of High-Dimensional Weight-Update Hyperparameters by Implicit Differentiation",
    "abstract": "Machine learning training methods depend plentifully and intricately on hyperparameters, motivating automated strategies for their optimisation. Many existing algorithms restart training for each new hyperparameter choice, at considerable computational cost. Some hypergradient-based one-pass methods exist, but these either cannot be applied to arbitrary optimiser hyperparameters (such as learning rates and momenta) or take several times longer to train than their base models. We extend these existing methods to develop an approximate hypergradient-based hyperparameter optimiser which is applicable to any continuous hyperparameter appearing in a differentiable model weight update, yet requires only one training episode, with no restarts. We also provide a motivating argument for convergence to the true hypergradient, and perform tractable gradient-based optimisation of independent learning rates for each model parameter. Our method performs competitively from varied random hyperparameter initialisations on several UCI datasets and Fashion-MNIST (using a one-layer MLP), Penn Treebank (using an LSTM) and CIFAR-10 (using a ResNet-18), in time only 2–3x greater than vanilla training",
    "volume": "main",
    "checked": true,
    "id": "d5201a097dcd7681a46ae507c063c59f901a846e",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=yKIAXjkJc2F": {
    "title": "Imbedding Deep Neural Networks",
    "abstract": "Continuous-depth neural networks, such as Neural ODEs, have refashioned the understanding of residual neural networks in terms of non-linear vector-valued optimal control problems. The common solution is to use the adjoint sensitivity method to replicate a forward-backward pass optimisation problem. We propose a new approach which explicates the network's ‘depth' as a fundamental variable, thus reducing the problem to a system of forward-facing initial value problems. This new method is based on the principle of ‘Invariant Imbedding' for which we prove a general solution, applicable to all non-linear, vector-valued optimal control problems with both running and terminal loss. Our new architectures provide a tangible tool for inspecting the theoretical–and to a great extent unexplained– properties of network depth. They also constitute a resource of discrete implementations of Neural ODEs comparable to classes of imbedded residual neural networks. Through a series of experiments, we show the competitive performance of the proposed architectures for supervised learning and time series prediction. Accompanying code is made available at github.com/andrw3000/inimnet. 1 UNPACKING CONTINUOUS-DEPTH NEURAL NETWORKS The long-standing enigma surrounding machine learning still remains paramount today: What is it that machines are learning and how may we extract meaningful knowledge from trained algorithms? Deep Neural Networks (DNNs), whilst undeniably successful, are notorious black-box secret keepers. To solve a supervised learning process, mapping vector inputs x to their targets y, parameters are stored and updated in ever deepening layers with no facility to access the physical significance of the internal function approximating the global mapping x 7→ y. p1 t = pi pn h( t; p, x) Figure 1: Plotted are heights h(t; p,x) vs. lengths t of projectile curves initiated with identical initial velocities x at a range of points pi along the t-axis. The red curve depicts a regression fit to a t-varying sample set; contrast with the blue InImNet training paradigm which learns the endpoints (diamonds) from varying the input position pi. We propose a new class of DNNs obtained by imbedding multiple networks of varying depth whilst keeping the inputs, x, invariant; we call these ‘Invariant Imbedding Networks' (InImNets). To illustrate the concept, Figure 1 depicts a system of projectiles fired from a range of positions p1 < p2 < · · · < pn with the same initial velocity conditions x. The red curve (initiated at p1) is fit to a sample (circles) along a single trajectory, representing a traditional regression problem. InImNet architectures are trained on the output values y = y(pi,x) at pn (the diamonds) as the depth pi of the system varies. This analogy applies to DNN classifiers where increasing the depth from pi to pi−1 outputs a classification decision for each of the i-steps. As a machine learning tool, the use of deep hidden layers, whilst successful, was first considered ad hoc in implementations, such as in multilayer ∗Equal Contribution. 1 ar X iv :2 20 2. 00 11 3v 2 [ cs .L G ] 1 5 Fe b 20 22 Published as a conference paper at ICLR 2022 perceptrons. But following the advent of residual neural networks (He et al., 2015) which use ‘Euler-step' internal updates between layers, DNN evolution is seen to emulate a continuous dynamical system (Lu et al., 2018; Ruthotto & Haber, 2020). Thus was formed the notion of a ‘Neural ODE' (Chen et al., 2018) in which the hidden network state vector z(t) ∈ R , instead of being defined at fixed layers t ∈ N, is allowed to vary continuously over an interval [p, q] ⊂ R for a system of dimension N ≥ 1. Its evolution is governed by an Ordinary Differential Equation (ODE) ż(t) = f(t, z(t),θ(t)); z(p) = x (1) where the training function f is controlled by a parameter vector θ(t) ∈ R for t ∈ [p, q] of length M ≥ 1. Network outputs are retrieved at z(q) = y after fixing the endpoint t = q. As such, the enigmatic ‘depth' of a Neural ODE is controlled by varying t = p, at which point we insert the initial condition z(p) = x. This dynamical description has given the theory of DNNs a new home: the mathematical framework of optimal control (Massaroli et al., 2020; Bensoussan et al., 2020). In this framework, whether discrete or continuous, solution networks are sought after that: (A) satisfying their update law (1) over a fixed ‘depth' interval [p, q]; (B) minimise a loss function subject to terminal success and internal regulation (see §2.1). As initiated by Euler and Lagrange (Euler, 1766), this mathematical framework determines networks given by (A) satisfying condition (B) using, amongst other techniques, the adjoint method: here a new state, which we call λ(t) or the ‘Lagrange multiplier', is introduced containing the system losses with respect to both t and the parameters θ(t); see §2.3. The connection between the traditional method of ‘backpropagation-by-chain rule' and the rigorous ‘adjoint method' is quite brilliantly explicated in proof by Chen et al. (2018), in that they directly deduce the adjoint method using infinitesimal backpropagation–far from the train of thought of Lagrange's multiplier method (Liberzon, 2012, Ch. 2); see also §D and §E.2 in the appendix for extended discussion and derivation. Even within the above theory, the initial condition, z(p) = x, and location, t = p, remain implicit constraints; a clear understanding of network depth remains illusive. Our new class of InImNet architectures may be obtained by imbedding networks of varying depth p whilst keeping the inputs, x, invariant. Explicating these two variables throughout the network, writing z(t) = z(t; p,x), has exciting conceptual consequences: 1. Forward pass to construct multiple networks: InImNet state vectors z(t; p,x) are computed with respect to the depth variable p rather than t ∈ [p, q], which is considered fixed (in practice at t = q). We build from the bottom up: initiate at p = q with the trivial network z(q; q,x) = x and unwind the p-varying dynamics, as described in Theorem 1, by integrating ∇pz(q; p,x) = −∇xz(q; p,x) · f(p,x,θ(p)) (2) from p = q to a greater depth p. Note that at depth p an InImNet returns an external output z(q; p,x) ∼ y, subject to training. This contrasts with convention, where one would obtain z(q; p,x) by integrating from t = p to t = q, where t < q states are considered internal. A general algorithm to implement the forward pass is described in Algorithm 1. The gradient operator ∇ denotes the usual vector, or Jacobian, of partial derivatives. 2. Backpropagate independently from the forward pass: We generalise the adjoint method of Chen et al. (2018), who was able to do away with the backpropagation-by-chain rule method in favour of a continuous approach with at most bounded memory demand. With our bottom-up formulation, we are able to go one step further and do away with the initial forward pass altogether by initiating our ‘imbedded' adjoint Λ(p,x), generalising λ(t), with loss gradients for the trivial network z(q; q,x) = x and computing to depth p via ∇pΛ(p,x) = −[∇xΛ(p,x) · f(p,x,θ(p)) +∇xf(p,x,θ(p)) ·Λ(p,x)]. (3) See Theorem 2 for a precise, more general explication. Backward passes may be made independently of forward passing altogether; see Theorem 2 and Algorithm 1. 3. Pre-imposed optimality: Working in the framework of optimal control theory, we consider both running and terminal losses–a general ‘Bolza problem'–see §2.1. We give a necessary first-order criterion for optimal control (Theorem 3). In this way, we account for t-varying parameter controls θ(t) = θ(t; p,x), omitted from the original Neural ODEs, permitting future compatibility with the recent particle-shooting models of Vialard et al. (2020)",
    "volume": "main",
    "checked": true,
    "id": "e547ef1c681f17a79520ead66a5c265217695ebd",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=7b4zxUnrO2N": {
    "title": "Possibility Before Utility: Learning And Using Hierarchical Affordances",
    "abstract": "Reinforcement learning algorithms struggle on tasks with complex hierarchical dependency structures. Humans and other intelligent agents do not waste time assessing the utility of every high-level action in existence, but instead only consider ones they deem possible in the first place. By focusing only on what is feasible, or \"afforded\", at the present moment, an agent can spend more time both evaluating the utility of and acting on what matters. To this end, we present Hierarchical Affordance Learning (HAL), a method that learns a model of hierarchical affordances in order to prune impossible subtasks for more effective learning. Existing works in hierarchical reinforcement learning provide agents with structural representations of subtasks but are not affordance-aware, and by grounding our definition of hierarchical affordances in the present state, our approach is more flexible than the multitude of approaches that ground their subtask dependencies in a symbolic history. While these logic-based methods often require complete knowledge of the subtask hierarchy, our approach is able to utilize incomplete and varying symbolic specifications. Furthermore, we demonstrate that relative to non-affordance-aware methods, HAL agents are better able to efficiently learn complex tasks, navigate environment stochasticity, and acquire diverse skills in the absence of extrinsic supervision—all of which are hallmarks of human learning.1",
    "volume": "main",
    "checked": true,
    "id": "6343543986bd0b2800f0ef468604b659f0a24ec5",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=aBXzcPPOuX": {
    "title": "Bundle Networks: Fiber Bundles, Local Trivializations, and a Generative Approach to Exploring Many-to-one Maps",
    "abstract": "Many-to-one maps are ubiquitous in machine learning, from the image recognition model that assigns a multitude of distinct images to the concept of \"cat\" to the time series forecasting model which assigns a range of distinct time-series to a single scalar regression value. While the primary use of such models is naturally to associate correct output to each input, in many problems it is also useful to be able to explore, understand, and sample from a model's fibers, which are the set of input values x such that f(x) = y, for fixed y in the output space. In this paper we show that popular generative architectures are ill-suited to such tasks. Motivated by this we introduce a novel generative architecture, a Bundle Network, based on the concept of a fiber bundle from (differential) topology. BundleNets exploit the idea of a local trivialization wherein a space can be locally decomposed into a product space that cleanly encodes the many-to-one nature of the map. By enforcing this decomposition in BundleNets and by utilizing state-of-the-art invertible components, investigating a network's fibers becomes natural",
    "volume": "main",
    "checked": true,
    "id": "08d59e24c95c11bc676a478f7feb97ceaa69c9ef",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=KBQP4A_J1K": {
    "title": "The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization",
    "abstract": "Despite progress across a broad range of applications, Transformers have limited success in systematic generalization. The situation is especially frustrating in the case of algorithmic tasks, where they often fail to ﬁnd intuitive solutions that route relevant information to the right node/operation at the right time in the grid represented by Transformer columns. To facilitate the learning of useful control ﬂow, we propose two modiﬁcations to the Transformer architecture, copy gate and geometric attention. Our novel Neural Data Router (NDR) achieves 100% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect accuracy on the simple arithmetic task and a new variant of ListOps testing for generalization across computational depths. NDR's attention and gating patterns tend to be interpretable as an intuitive form of neural routing . Our code is public. 1",
    "volume": "main",
    "checked": true,
    "id": "e528466e2aff981511d4ca6e063211297c0b4175",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=CrCvGNHAIrz": {
    "title": "Explainable GNN-Based Models over Knowledge Graphs",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "18637c36a2737e51051113e56b4438b06b568a3d",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=0sgntlpKDOz": {
    "title": "Learning Graphon Mean Field Games and Approximate Nash Equilibria",
    "abstract": "Recent advances at the intersection of dense large graph limits and mean field games have begun to enable the scalable analysis of a broad class of dynamical sequential games with large numbers of agents. So far, results have been largely limited to graphon mean field systems with continuous-time diffusive or jump dynamics, typically without control and with little focus on computational methods. We propose a novel discrete-time formulation for graphon mean field games as the limit of non-linear dense graph Markov games with weak interaction. On the theoretical side, we give extensive and rigorous existence and approximation properties of the graphon mean field solution in sufficiently large systems. On the practical side, we provide general learning schemes for graphon mean field equilibria by either introducing agent equivalence classes or reformulating the graphon mean field system as a classical mean field system. By repeatedly finding a regularized optimal control solution and its generated mean field, we successfully obtain plausible approximate Nash equilibria in otherwise infeasible large dense graph games with many agents. Empirically, we are able to demonstrate on a number of examples that the finite-agent behavior comes increasingly close to the mean field behavior for our computed equilibria as the graph or system size grows, verifying our theory. More generally, we successfully apply policy gradient reinforcement learning in conjunction with sequential Monte Carlo methods",
    "volume": "main",
    "checked": true,
    "id": "7656387415a75d0825f7f47c91f55223af1e26e4",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=Vjki79-619-": {
    "title": "Proving the Lottery Ticket Hypothesis for Convolutional Neural Networks",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "af55b591775c43b987f23cd541cb059ea89e7f5c",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=45L_dgP48Vd": {
    "title": "Graph-Augmented Normalizing Flows for Anomaly Detection of Multiple Time Series",
    "abstract": "Anomaly detection is a widely studied task for a broad variety of data types; among them, multiple time series appear in applications, including for example, power grids and trafﬁc networks. Detecting anomalies for multiple time series, however, is a challenging subject, owing to the intricate interdependencies among the constituent series. We hypothesize that anomalies occur in low density regions of a distribution and explore the use of normalizing ﬂows for unsupervised anomaly detection, because of their superior quality in density estimation. More-over, we propose a novel ﬂow model by imposing a Bayesian network among constituent series. A Bayesian network is a directed acyclic graph (DAG) that models causal relationships; it factorizes the joint probability of the series into the product of easy-to-evaluate conditional probabilities. We call such a graph-augmented normalizing ﬂow approach GANF and propose joint estimation of the DAG with ﬂow parameters. We conduct extensive experiments on real-world datasets and demonstrate the effectiveness of GANF for density estimation, anomaly detection, and identiﬁcation of time series distribution drift",
    "volume": "main",
    "checked": true,
    "id": "c25975cc81949931f79ffb135354c474681d1ccc",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=aisKPsMM3fg": {
    "title": "Neural Stochastic Dual Dynamic Programming",
    "abstract": "Stochastic dual dynamic programming (SDDP) is a state-of-the-art method for solving multi-stage stochastic optimization, widely used for modeling real-world process optimization tasks. Unfortunately, SDDP has a worst-case complexity that scales exponentially in the number of decision variables, which severely limits applicability to only low dimensional problems. To overcome this limitation, we extend SDDP by introducing a trainable neural model that learns to map problem instances to a piece-wise linear value function within intrinsic low-dimension space , which is architected speciﬁcally to interact with a base SDDP solver, so that can accelerate optimization performance on new instances. The proposed Neural Stochastic Dual Dynamic Programming ( ν -SDDP) continually self-improves by solving successive problems. An empirical investigation demonstrates that ν -SDDP can signiﬁcantly reduce problem solving cost without sacriﬁcing solution quality over competitors such as SDDP and reinforcement learning algorithms, across a range of synthetic and real-world process optimization problems",
    "volume": "main",
    "checked": true,
    "id": "e4dddc411739f85097c20aa71ebf27357b17bfb3",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=POTMtpYI1xH": {
    "title": "Discovering Latent Concepts Learned in BERT",
    "abstract": "A large number of studies that analyze deep neural network models and their ability to encode various linguistic and non-linguistic concepts provide an interpretation of the inner mechanics of these models. The scope of the analyses is limited to pre-deﬁned concepts that reinforce the traditional linguistic knowledge and do not reﬂect on how novel concepts are learned by the model. We address this limitation by discovering and analyzing latent concepts learned in neural network models in an unsupervised fashion and provide interpretations from the model's perspective. In this work, we study: i) what latent concepts exist in the pre-trained BERT model, ii) how the discovered latent concepts align or diverge from classical linguistic hierarchy and iii) how the latent concepts evolve across layers. Our ﬁndings show: i) a model learns novel concepts (e.g. animal categories and de-mographic groups), which do not strictly adhere to any pre-deﬁned categorization (e.g. POS, semantic tags), ii) several latent concepts are based on multiple properties which may include semantics, syntax, and morphology, iii) the lower layers in the model dominate in learning shallow lexical concepts while the higher layers learn semantic relations and iv) the discovered latent concepts highlight potential biases learned in the model. We also release 1 a novel BERT ConceptNet dataset ( BCN ) consisting of 174 concept labels and 1M annotated instances",
    "volume": "main",
    "checked": true,
    "id": "e33b7282f1e547054a660377383b8ab8464f676e",
    "citation_count": 14
  },
  "https://openreview.net/forum?id=SLz5sZjacp": {
    "title": "Evaluating Disentanglement of Structured Representations",
    "abstract": "We introduce the first metric for evaluating disentanglement at individual hierarchy levels of a structured latent representation. Applied to object-centric generative models, this offers a systematic, unified approach to evaluating (i) object separation between latent slots (ii) disentanglement of object properties inside individual slots (iii) disentanglement of intrinsic and extrinsic object properties. We theoretically show that for structured representations, our framework gives stronger guarantees of selecting a good model than previous disentanglement metrics. Experimentally, we demonstrate that viewing object compositionality as a disentanglement problem addresses several issues with prior visual metrics of object separation. As a core technical component, we present the first representation probing algorithm handling slot permutation invariance",
    "volume": "main",
    "checked": true,
    "id": "11cd1909ce2ccffb9242f227cc3ba2d5529ec5c9",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=gKLAAfiytI": {
    "title": "Equivariant Self-Supervised Learning: Encouraging Equivariance in Representations",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "b81c1a10fb333b59d34fdf022690285b8ae7dbdc",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=bERaNdoegnO": {
    "title": "Policy improvement by planning with Gumbel",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "93e598ac56f29ed2638f393af9c93eec0ea07c1a",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=zRJu6mU2BaE": {
    "title": "ConFeSS: A Framework for Single Source Cross-Domain Few-Shot Learning",
    "abstract": "Most current few-shot learning methods train a model from abundantly labeled base category data and then transfer and adapt the model to sparsely labeled novel category data. These methods mostly generalize well on novel categories from the same domain as the base categories but perform poorly for distant domain categories. In this paper, we propose a framework for few-shot learning coined as ConFeSS (Contrastive Learning and Feature Selection System) that tackles large domain shift between base and novel categories. The first step of our framework trains a feature extracting backbone with the contrastive loss on the base category data. Since the contrastive loss does not use supervision, the features can generalize better to distant target domains. For the second step, we train a masking module to select relevant features that are more suited to target domain classification. Finally, a classifier is fine-tuned along with the backbone such that the backbone produces features similar to the relevant ones. To evaluate our framework, we tested it on a recently introduced cross-domain few-shot learning benchmark. Experimental results demonstrate that our framework outperforms all meta-learning approaches and produces competitive results against recent cross-domain methods. Additional analyses are also performed to better understand our framework",
    "volume": "main",
    "checked": true,
    "id": "a8fc34772ed42b17f49580cb7e8372fc96e307ad",
    "citation_count": 12
  },
  "https://openreview.net/forum?id=w-CPUXXrAj": {
    "title": "On the Limitations of Multimodal VAEs",
    "abstract": "Multimodal variational autoencoders (VAEs) have shown promise as efﬁcient generative models for weakly-supervised data. Yet, despite their advantage of weak supervision, they exhibit a gap in generative quality compared to unimodal VAEs, which are completely unsupervised. In an attempt to explain this gap, we un-cover a fundamental limitation that applies to a large family of mixture-based multimodal VAEs. We prove that the sub-sampling of modalities enforces an undesirable upper bound on the multimodal ELBO and thereby limits the generative quality of the respective models. Empirically, we showcase the generative quality gap on both synthetic and real data and present the tradeoffs between different variants of multimodal VAEs. We ﬁnd that none of the existing approaches fulﬁlls all desired criteria of an effective multimodal generative model when applied on more complex datasets than those used in previous benchmarks. In summary, we identify, formalize, and validate fundamental limitations of VAE-based approaches for modeling weakly-supervised data and discuss implications for real-world applications",
    "volume": "main",
    "checked": true,
    "id": "9ff70d133e998a909d1b111f1ee4e86cb12a56ba",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=FKp8-pIRo3y": {
    "title": "Wish you were here: Hindsight Goal Selection for long-horizon dexterous manipulation",
    "abstract": "Complex sequential tasks in continuous-control settings often require agents to successfully traverse a set of \"narrow passages\" in their state space. Solving such tasks with a sparse reward in a sample-efficient manner poses a challenge to modern reinforcement learning (RL) due to the associated long-horizon nature of the problem and the lack of sufficient positive signal during learning. Various tools have been applied to address this challenge. When available, large sets of demonstrations can guide agent exploration. Hindsight relabelling on the other hand does not require additional sources of information. However, existing strategies explore based on task-agnostic goal distributions, which can render the solution of long-horizon tasks impractical. In this work, we extend hindsight relabelling mechanisms to guide exploration along task-specific distributions implied by a small set of successful demonstrations. We evaluate the approach on four complex, single and dual arm, robotics manipulation tasks against strong suitable baselines. The method requires far fewer demonstrations to solve all tasks and achieves a significantly higher overall performance as task complexity increases. Finally, we investigate the robustness of the proposed solution with respect to the quality of input representations and the number of demonstrations",
    "volume": "main",
    "checked": true,
    "id": "bb77b81c4e817859fb831378d3c35ad0ea31bf4b",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=R79ZGjHhv6p": {
    "title": "Toward Faithful Case-based Reasoning through Learning Prototypes in a Nearest Neighbor-friendly Space",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "329edaadc5c83ed3c4b5cb4dbedce0976d2d7dbd",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=u6s8dSporO8": {
    "title": "Group equivariant neural posterior estimation",
    "abstract": "Simulation-based inference with conditional neural density estimators is a powerful approach to solving inverse problems in science. However, these methods typically treat the underlying forward model as a black box, with no way to exploit geometric properties such as equivariances. Equivariances are common in scientific models, however integrating them directly into expressive inference networks (such as normalizing flows) is not straightforward. We here describe an alternative method to incorporate equivariances under joint transformations of parameters and data. Our method—called group equivariant neural posterior estimation (GNPE)—is based on self-consistently standardizing the \"pose\" of the data while estimating the posterior over parameters. It is architecture-independent, and applies both to exact and approximate equivariances. As a real-world application, we use GNPE for amortized inference of astrophysical binary black hole systems from gravitationalwave observations. We show that GNPE achieves state-of-the-art accuracy while reducing inference times by three orders of magnitude",
    "volume": "main",
    "checked": true,
    "id": "4923ac78a01d019268b901c056c1336e22ec6f04",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=kG0AtPi6JI1": {
    "title": "Visual Representation Learning over Latent Domains",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "e0071927a83f29ab191c603f3a3bf9b6b1b35e96",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=57PipS27Km": {
    "title": "Continuous-Time Meta-Learning with Forward Mode Differentiation",
    "abstract": "Drawing inspiration from gradient-based meta-learning methods with infinitely small gradient steps, we introduce Continuous-Time Meta-Learning (COMLN), a meta-learning algorithm where adaptation follows the dynamics of a gradient vector field. Specifically, representations of the inputs are meta-learned such that a taskspecific linear classifier is obtained as a solution of an ordinary differential equation (ODE). Treating the learning process as an ODE offers the notable advantage that the length of the trajectory is now continuous, as opposed to a fixed and discrete number of gradient steps. As a consequence, we can optimize the amount of adaptation necessary to solve a new task using stochastic gradient descent, in addition to learning the initial conditions as is standard practice in gradient-based meta-learning. Importantly, in order to compute the exact meta-gradients required for the outer-loop updates, we devise an efficient algorithm based on forward mode differentiation, whose memory requirements do not scale with the length of the learning trajectory, thus allowing longer adaptation in constant memory. We provide analytical guarantees for the stability of COMLN, we show empirically its efficiency in terms of runtime and memory usage, and we illustrate its effectiveness on a range of few-shot image classification problems",
    "volume": "main",
    "checked": true,
    "id": "2947873e49f3185cf39be950c4f8603a1fa901cd",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=CVfLvQq9gLo": {
    "title": "ARTEMIS: Attention-based Retrieval with Text-Explicit Matching and Implicit Similarity",
    "abstract": "An intuitive way to search for images is to use queries composed of an example image and a complementary text. While the ﬁrst provides rich and implicit context for the search, the latter explicitly calls for new traits, or speciﬁes how some elements of the example image should be changed to retrieve the desired target image. Current approaches typically combine the features of each of the two elements of the query into a single representation, which can then be compared to the ones of the potential target images. Our work aims at shedding new light on the task by looking at it through the prism of two familiar and related frameworks: text-to-image and image-to-image retrieval. Taking inspiration from them, we ex-ploit the speciﬁc relation of each query element with the targeted image and derive light-weight attention mechanisms which enable to mediate between the two complementary modalities. We validate our approach on several retrieval benchmarks, querying with images and their associated free-form text modiﬁers. Our method obtains state-of-the-art results without resorting to side information, multi-level features, heavy pre-training nor large architectures as in previous works. Our code is available at https://github.com/naver/artemis",
    "volume": "main",
    "checked": true,
    "id": "69778262994c3183ac02c7e535a3e9256c5231fa",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=_XNtisL32jv": {
    "title": "Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting",
    "abstract": "Recently, brain-inspired spiking neuron networks (SNNs) have attracted widespread research interest because of their event-driven and energy-efﬁcient characteristics. Still, it is difﬁcult to efﬁciently train deep SNNs due to the non-differentiability of its activation function, which disables the typically used gradient descent approaches for traditional artiﬁcial neural networks (ANNs). Although the adoption of surrogate gradient (SG) formally allows for the back-propagation of losses, the discrete spiking mechanism actually differentiates the loss landscape of SNNs from that of ANNs, failing the surrogate gradient methods to achieve comparable accuracy as for ANNs. In this paper, we ﬁrst analyze why the current direct training approach with surrogate gradient results in SNNs with poor generalizability. Then we introduce the temporal efﬁcient training (TET) approach to compensate for the loss of momentum in the gradient descent with SG so that the training process can converge into ﬂatter minima with better generalizability. Meanwhile, we demonstrate that TET improves the temporal scalability of SNN and induces a temporal inheritable training for acceleration. Our method consistently outperforms the SOTA on all reported mainstream datasets, includ-ing CIFAR-10/100 and ImageNet. Remarkably on DVS-CIFAR10, we obtained 83 % top-1 accuracy, over 10 % improvement compared to existing state of the art. Codes are available at https://github.com/Gus-Lab/temporal_",
    "volume": "main",
    "checked": true,
    "id": "0faf7b08fc18a22ccd0eac8fae0eee13913755ba",
    "citation_count": 20
  },
  "https://openreview.net/forum?id=iRCUlgmdfHJ": {
    "title": "Discovering and Explaining the Representation Bottleneck of DNNS",
    "abstract": "This paper explores the bottleneck of feature representations of deep neural networks (DNNs), from the perspective of the complexity of interactions between input variables encoded in DNNs. To this end, we focus on the multi-order interaction between input variables, where the order represents the complexity of interactions. We discover that a DNN is more likely to encode both too simple interactions and too complex interactions, but usually fails to learn interactions of intermediate complexity. Such a phenomenon is widely shared by different DNNs for different tasks. This phenomenon indicates a cognition gap between DNNs and human beings, and we call it a representation bottleneck. We theoretically prove the underlying reason for the representation bottleneck. Furthermore, we propose a loss to encourage/penalize the learning of interactions of specific complexities, and analyze the representation capacities of interactions of different complexities",
    "volume": "main",
    "checked": true,
    "id": "d00d1a41a7e7ba09e1abf4f7c3b1ea05367039bf",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=2bO2x8NAIMB": {
    "title": "Should We Be Pre-training? An Argument for End-task Aware Training as an Alternative",
    "abstract": "In most settings of practical concern, machine learning practitioners know in advance what end-task they wish to boost with auxiliary tasks. However, widely used methods for leveraging auxiliary data like pre-training and its continuedpretraining variant are end-task agnostic: they rarely, if ever, exploit knowledge of the target task. We study replacing end-task agnostic continued training of pretrained language models with end-task aware training of said models. We argue that for sufficiently important end-tasks, the benefits of leveraging auxiliary data in a task-aware fashion can justify forgoing the traditional approach of obtaining generic, end-task agnostic representations as with (continued) pre-training. On three different low-resource NLP tasks from two domains, we demonstrate that multi-tasking the end-task and auxiliary objectives results in significantly better downstream task performance than the widely-used task-agnostic continued pre-training paradigm of Gururangan et al. (2020). We next introduce an online meta-learning algorithm that learns a set of multi-task weights to better balance among our multiple auxiliary objectives, achieving further improvements on endtask performance and data efficiency",
    "volume": "main",
    "checked": true,
    "id": "4e77a4d4bcc09f6b2f3bcb790d348f4dfdbf427b",
    "citation_count": 16
  },
  "https://openreview.net/forum?id=shpkpVXzo3h": {
    "title": "8-bit Optimizers via Block-wise Quantization",
    "abstract": "Stateful optimizers maintain gradient statistics over time, e.g., the exponentially smoothed sum (SGD with momentum) or squared sum (Adam) of past gradient values. This state can be used to accelerate optimization compared to plain stochastic gradient descent but uses memory that might otherwise be allocated to model parameters, thereby limiting the maximum size of models trained in practice. In this paper, we develop the first optimizers that use 8-bit statistics while maintaining the performance levels of using 32-bit optimizer states. To overcome the resulting computational, quantization, and stability challenges, we develop block-wise dynamic quantization. Block-wise quantization divides input tensors into smaller blocks that are independently quantized. Each block is processed in parallel across cores, yielding faster optimization and high precision quantization. To maintain stability and performance, we combine block-wise quantization with two additional changes: (1) dynamic quantization, a form of non-linear optimization that is precise for both large and small magnitude values, and (2) a stable embedding layer to reduce gradient variance that comes from the highly non-uniform distribution of input tokens in language models. As a result, our 8-bit optimizers maintain 32-bit performance with a small fraction of the memory footprint on a range of tasks, including 1.5B parameter language modeling, GLUE finetuning, ImageNet classification, WMT'14 machine translation, MoCo v2 contrastive ImageNet pretraining+finetuning, and RoBERTa pretraining, without changes to the original optimizer hyperparameters. We open-sourceour 8-bit optimizers as a drop-in replacement that only requires a two-line code change. Increasing model size is an effective way to achieve better performance for given resources (Kaplan et al., 2020; Henighan et al., 2020; Raffel et al., 2019; Lewis et al., 2021). However, training such large models requires storing the model, gradient, and state of the optimizer (e.g., exponentially smoothed sum and squared sum of previous gradients for Adam), all in a fixed amount of available memory. Although significant research has focused on enabling larger model training by reducing or efficiently distributing the memory required for the model parameters (Shoeybi et al., 2019; Lepikhin et al., 2020; Fedus et al., 2021; Brown et al., 2020; Rajbhandari et al., 2020), reducing the memory footprint of optimizer gradient statistics is much less studied. This is a significant missed opportunity since these optimizer states use 33-75% of the total memory footprint during training. For example, the Adam optimizer states for the largest GPT-2 (Radford et al., 2019) and T5 (Raffel et al., 2019) models are 11 GB and 41 GB in size. In this paper, we develop a fast, high-precision non-linear quantization method – block-wise dynamic quantization – that enables stable 8-bit optimizers (e.g., Adam, AdamW, and Momentum) which maintain 32-bit performance at a fraction of the memory footprint and without any changes to the original hyperparameters.1 While most current work uses 32-bit optimizer states, recent high-profile efforts to use 16-bit optimizers report difficultly for large models with more than 1B parameters (Ramesh et al., 2021). Going from 16-bit optimizers to 8-bit optimizers reduces the range of possible values from 2 = 65536 values to just 2 = 256. To our knowledge, this has not been attempted before. Effectively using this very limited range is challenging for three reasons: quantization accuracy, computational efficiency, and large-scale stability. To maintain accuracy, it is critical to introduce some form of non-linear quantization to reduce errors for both common small magnitude values We study 8-bit optimization with current best practice model and gradient representations (typically 16-bit mixed precision), to isolate optimization challenges. Future work could explore further compressing all three. 1 ar X iv :2 11 0. 02 86 1v 2 [ cs .L G ] 2 0 Ju n 20 22 Published as a conference paper at ICLR 2022",
    "volume": "main",
    "checked": true,
    "id": "11fe37ab6faf6bf85ad2f5746c154dec5412bd04",
    "citation_count": 23
  },
  "https://openreview.net/forum?id=UMfhoMtIaP5": {
    "title": "Provably Robust Adversarial Examples",
    "abstract": "We introduce the concept of provably robust adversarial examples for deep neural networks – connected input regions constructed from standard adversarial examples which are guaranteed to be robust to a set of real-world perturbations (such as changes in pixel intensity and geometric transformations). We present a novel method called PARADE for generating these regions in a scalable manner which works by iteratively reﬁning the region initially obtained via sampling until a reﬁned region is certiﬁed to be adversarial with existing state-of-the-art veriﬁers. At each step, a novel optimization procedure is applied to maximize the region's volume under the constraint that the convex relaxation of the network behavior with respect to the region implies a chosen bound on the certiﬁcation objective. Our experimental evaluation shows the effectiveness of PARADE: it successfully ﬁnds large provably robust regions including ones containing ≈ 10 573 adversarial examples for pixel intensity and ≈ 10 599 for geometric perturbations. The provability enables our robust examples to be signiﬁcantly more effective against state-of-the-art defenses based on randomized smoothing than the individual attacks used to construct the regions",
    "volume": "main",
    "checked": true,
    "id": "7cb6a6369c6b01de8f88539687cb4acc121edb94",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=9Nk6AJkVYB": {
    "title": "Audio Lottery: Speech Recognition Made Ultra-Lightweight, Noise-Robust, and Transferable",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "2cecb623ba2e2ee4872bb07b794552fd73a87976",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=KEQl-MZ5fg7": {
    "title": "Learning Versatile Neural Architectures by Propagating Network Codes",
    "abstract": "This work explores how to design a single neural network that is capable of adapting to multiple heterogeneous tasks of computer vision, such as image segmentation, 3D detection, and video recognition. This goal is challenging because network architecture designs in different tasks are inconsistent. We solve this challenge by proposing Network Coding Propagation (NCP), a novel \"neural predictor\", which is able to predict an architecture's performance in multiple datasets and tasks. Unlike prior arts of neural architecture search (NAS) that typically focus on a single task, NCP has several unique benefits. (1) NCP can be trained on different NAS benchmarks, such as NAS-Bench-201 and NAS-Bench-MR, which contains a novel network space designed by us for jointly searching an architecture among multiple tasks, including ImageNet, Cityscapes, KITTI, and HMDB51. (2) NCP learns from network codes but not original data, enabling it to update the architecture efficiently across datasets. (3) Extensive experiments evaluate NCP on object classification, detection, segmentation, and video recognition. For example, with 17% fewer FLOPs, a single architecture returned by NCP achieves 86% and 77.16% on ImageNet-50-1000 and Cityscapes respectively, outperforming its counterparts. More interestingly, NCP enables a single architecture applicable to both image segmentation and video recognition, which achieves competitive performance on both HMDB51 and ADE20K compared to the singular counterparts. Code is available at https://github.com/dingmyu/NCP",
    "volume": "main",
    "checked": true,
    "id": "4dc868c50900050e18f8d9685b4a0414f598d663",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=J_2xNmVcY4": {
    "title": "Optimizing Neural Networks with Gradient Lexicase Selection",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "0cf7d32bc9017842abb8752b279e2d31910623b4",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=CzceR82CYc": {
    "title": "Score-Based Generative Modeling with Critically-Damped Langevin Diffusion",
    "abstract": "Score-based generative models (SGMs) have demonstrated remarkable synthesis quality. SGMs rely on a diffusion process that gradually perturbs the data towards a tractable distribution, while the generative model learns to denoise. The complexity of this denoising task is, apart from the data distribution itself, uniquely determined by the diffusion process. We argue that current SGMs employ overly simplistic diffusions, leading to unnecessarily complex denoising processes, which limit generative modeling performance. Based on connections to statistical mechanics, we propose a novel critically-damped Langevin diffusion (CLD) and show that CLD-based SGMs achieve superior performance. CLD can be interpreted as running a joint diffusion in an extended space, where the auxiliary variables can be considered \"velocities\" that are coupled to the data variables as in Hamiltonian dynamics. We derive a novel score matching objective for CLD and show that the model only needs to learn the score function of the conditional distribution of the velocity given data, an easier task than learning scores of the data directly. We also derive a new sampling scheme for efﬁcient synthesis from CLD-based diffusion models. We ﬁnd that CLD outperforms previous SGMs in synthesis quality for similar network architectures and sampling compute budgets. We show that our novel sampler for CLD signiﬁcantly outperforms solvers such as Euler–Maruyama. Our framework provides new insights into score-based denoising diffusion models and can be readily used for high-resolution image synthesis. VPSDE. We leave the study of CLD with maximum likelihood training for high-dimensional (image) datasets to future work",
    "volume": "main",
    "checked": true,
    "id": "a28cdccba07dbf977795e15ff2c9b7ec80dac050",
    "citation_count": 52
  },
  "https://openreview.net/forum?id=LBvk4QWIUpm": {
    "title": "Tighter Sparse Approximation Bounds for ReLU Neural Networks",
    "abstract": "A well-known line of work [Barron, 1993, Breiman, 1993, Klusowski and Barron, 2018] provides bounds on the width n of a ReLU two-layer neural network needed to approximate a function f over the ball BR(R) up to error , when the Fourier based quantity Cf = 1 (2π)d/2 ∫ Rd ‖ξ‖ |f̂(ξ)| dξ is finite. More recently Ongie et al. [2019] used the Radon transform as a tool for analysis of infinite-width ReLU two-layer networks. In particular, they introduce the concept of Radon-based R-norms and show that a function defined on R can be represented as an infinite-width two-layer neural network if and only if its R-norm is finite. In this work, we extend the framework of [Ongie et al., 2019] and define similar Radonbased semi-norms (R,U-norms) such that a function admits an infinite-width neural network representation on a bounded open set U ⊆ R when its R,U-norm is finite. Building on this, we derive sparse (finite-width) neural network approximation bounds that refine those of Breiman [1993], Klusowski and Barron [2018]. Finally, we show that infinite-width neural network representations on bounded open sets are not unique and study their structure, providing a functional view of mode connectivity",
    "volume": "main",
    "checked": true,
    "id": "79650143c8f77b0f960db1b3b32bec5c00124338",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=gbe1zHyA73": {
    "title": "Constrained Physical-Statistics Models for Dynamical System Identification and Prediction",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "c8651d5306c1e30cf0636b05fa346dae0ae3f0dd",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=JJxiD-kg-oK": {
    "title": "Blaschke Product Neural Networks (BPNN): A Physics-Infused Neural Network for Phase Retrieval of Meromorphic Functions",
    "abstract": "Numerous physical systems are described by ordinary or partial differential equations whose solutions are given by holomorphic or meromorphic functions in the complex domain. In many cases, only the magnitude of these functions are observed on various points on the purely imaginary jω-axis since coherent measurement of their phases is often expensive. However, it is desirable to retrieve the lost phases from the magnitudes when possible. To this end, we propose a physics-infused deep neural network based on the Blaschke products for phase retrieval. Inspired by the Helson and Sarason Theorem, we recover coefficients of a rational function of Blaschke products using a Blaschke Product Neural Network (BPNN), based upon the magnitude observations as input. The resulting rational function is then used for phase retrieval. We compare the BPNN to conventional deep neural networks (NNs) on several phase retrieval problems, comprising both synthetic and contemporary real-world problems (e.g., metamaterials for which data collection requires substantial expertise and is time consuming). On each phase retrieval problem, we compare against a population of conventional NNs of varying size and hyperparameter settings. Even without any hyper-parameter search, we find that BPNNs consistently outperform the population of optimized NNs in scarce data scenarios, and do so despite being much smaller models. The results can in turn be applied to calculate the refractive index of metamaterials, which is an important problem in emerging areas of material science",
    "volume": "main",
    "checked": true,
    "id": "386b0941e07369cd5ffe5ea3b3a822e755acb130",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=7gE9V9GBZaI": {
    "title": "Exploring Memorization in Adversarial Training",
    "abstract": "It is well known that deep learning models have a propensity for fitting the entire training set even with random labels, which requires memorization of every training sample. In this paper, we investigate the memorization effect in adversarial training (AT) for promoting a deeper understanding of capacity, convergence, generalization, and especially robust overfitting of adversarially trained classifiers. We first demonstrate that deep networks have sufficient capacity to memorize adversarial examples of training data with completely random labels, but not all AT algorithms can converge under the extreme circumstance. Our study of AT with random labels motivates further analyses on the convergence and generalization of AT. We find that some AT methods suffer from a gradient instability issue, and the recently suggested complexity measures cannot explain robust generalization by considering models trained on random labels. Furthermore, we identify a significant drawback of memorization in AT that it could result in robust overfitting. We then propose a new mitigation algorithm motivated by detailed memorization analyses. Extensive experiments on various datasets validate the effectiveness of the proposed method",
    "volume": "main",
    "checked": true,
    "id": "abbe3a82bb11a9f28eba39ff6dc17982a724c2fd",
    "citation_count": 16
  },
  "https://openreview.net/forum?id=q4tZR1Y-UIs": {
    "title": "It Takes Four to Tango: Multiagent Self Play for Automatic Curriculum Generation",
    "abstract": "We are interested in training general-purpose reinforcement learning agents that can solve a wide variety of goals. Training such agents efficiently requires automatic generation of a goal curriculum. This is challenging as it requires (a) exploring goals of increasing difficulty, while ensuring that the agent (b) is exposed to a diverse set of goals in a sample efficient manner and (c) does not catastrophically forget previously solved goals. We propose Curriculum Self Play (CuSP), an automated goal generation framework that seeks to satisfy these desiderata by virtue of a multi-player game with 4 agents. We extend the asymmetric curricula learning in PAIRED (Dennis et al., 2020) to a symmetrized game that carefully balances cooperation and competition between two off-policy student learners and two regret-maximizing teachers. CuSP additionally introduces entropic goal coverage and accounts for the non-stationary nature of the students, allowing us to automatically induce a curriculum that balances progressive exploration with anticatastrophic exploitation. We demonstrate that our method succeeds at generating an effective curricula of goals for a range of control tasks, outperforming other methods at zero-shot test-time generalization to novel out-of-distribution goals",
    "volume": "main",
    "checked": true,
    "id": "9fc6b6d3fead719cdc95263e62209548223576d1",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=5LXw_QplBiF": {
    "title": "Learning Hierarchical Structures with Differentiable Nondeterministic Stacks",
    "abstract": "Learning hierarchical structures in sequential data—from simple algorithmic patterns to natural language—in a reliable, generalizable way remains a challenging problem for neural language models. Past work has shown that recurrent neural networks (RNNs) struggle to generalize on held-out algorithmic or syntactic patterns without supervision or some inductive bias. To remedy this, many papers have explored augmenting RNNs with various diﬀerentiable stacks, by analogy with ﬁnite automata and pushdown automata (PDAs). In this paper, we improve the performance of our recently proposed Nondeterministic Stack RNN (NS-RNN), which uses a diﬀerentiable data structure that simulates a nondeterministic PDA, with two important changes. First, the model now assigns unnormalized positive weights instead of probabilities to stack actions, and we provide an analysis of why this improves training. Second, the model can directly observe the state of the underlying PDA. Our model achieves lower cross-entropy than all previous stack RNNs on ﬁve context-free language modeling tasks (within 0.05 nats of the information-theoretic lower bound), including a task on which the NS-RNN previously failed to outperform a deterministic stack RNN baseline. Finally, we propose a restricted version of the NS-RNN that incrementally processes inﬁnitely long sequences, and we present language modeling results on the Penn Treebank",
    "volume": "main",
    "checked": true,
    "id": "14dbb988d3666e57496338cc78d175c55f4719b3",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=TW7d65uYu5M": {
    "title": "VOS: Learning What You Don't Know by Virtual Outlier Synthesis",
    "abstract": "Out-of-distribution (OOD) detection has received much attention lately due to its importance in the safe deployment of neural networks. One of the key challenges is that models lack supervision signals from unknown data, and as a result, can produce overconfident predictions on OOD data. Previous approaches rely on real outlier datasets for model regularization, which can be costly and sometimes infeasible to obtain in practice. In this paper, we present VOS, a novel framework for OOD detection by adaptively synthesizing virtual outliers that can meaningfully regularize the model's decision boundary during training. Specifically, VOS samples virtual outliers from the low-likelihood region of the classconditional distribution estimated in the feature space. Alongside, we introduce a novel unknown-aware training objective, which contrastively shapes the uncertainty space between the ID data and synthesized outlier data. VOS achieves stateof-the-art performance on both object detection and image classification models, reducing the FPR95 by up to 7.87% compared to the previous best method. Code is available at https://github.com/deeplearning-wisc/vos",
    "volume": "main",
    "checked": true,
    "id": "80ac4250b3cba55228684756acb55922042d7aaf",
    "citation_count": 40
  },
  "https://openreview.net/forum?id=n0OeTdNRG0Q": {
    "title": "Efficient Sharpness-aware Minimization for Improved Training of Neural Networks",
    "abstract": "optimizers, Stochastic at no to its generalization two novel and strategies—Stochastic Weight Perturbation and Sharpness-Sensitive Data Selection. In the the sharpness is approximated by perturbing a stochastically set of weights in each in the the SAM loss is optimized using only a judiciously selected subset of data that is sensitive to the sharpness. We provide theoretical explanations as to why these strategies perform well. We also show, via extensive experiments on the CIFAR",
    "volume": "main",
    "checked": true,
    "id": "a82ae40ecc5ea5e33b52c87c9464510cab7bf9d9",
    "citation_count": 24
  },
  "https://openreview.net/forum?id=i3RI65sR7N": {
    "title": "Hierarchical Variational Memory for Few-shot Learning Across Domains",
    "abstract": "explore the importance of different feature levels, we propose learning to weigh prototypes in a data-driven way, which further improves generalization performance. Extensive experiments on six benchmarks demonstrate the efﬁcacy of each component in our model and the effectiveness of hierarchical variational memory in handling both the domain shift and few-shot learning problems",
    "volume": "main",
    "checked": true,
    "id": "a69cf91f0a4aa8ce4306f7b3bc59629631fa35f8",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=bVvMOtLMiw": {
    "title": "DIVA: Dataset Derivative of a Learning Task",
    "abstract": "We present a method to compute the derivative of a learning task with respect to a dataset. A learning task is a function from a training set to the validation error, which can be represented by a trained deep neural network (DNN). The \"dataset derivative\" is a linear operator, computed around the trained model, that informs how perturbations of the weight of each training sample affect the validation error, usually computed on a separate validation dataset. Our method, DIVA (Differentiable Validation) hinges on a closed-form differentiable expression of the leave-one-out cross-validation error around a pre-trained DNN. Such expression constitutes the dataset derivative. DIVA could be used for dataset auto-curation, for example removing samples with faulty annotations, augmenting a dataset with additional relevant samples, or rebalancing. More generally, DIVA can be used to optimize the dataset, along with the parameters of the model, as part of the training process without the need for a separate validation dataset, unlike bi-level optimization methods customary in AutoML. To illustrate the ﬂexibility of DIVA, we report experiments on sample auto-curation tasks such as outlier rejection, dataset extension, and automatic aggregation of multi-modal data",
    "volume": "main",
    "checked": true,
    "id": "892e110278714f9a40c942f17ec2bd851e199895",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=js62_xuLDDv": {
    "title": "Is Fairness Only Metric Deep? Evaluating and Addressing Subgroup Gaps in Deep Metric Learning",
    "abstract": "Deep metric learning (DML) enables learning with less supervision through its emphasis on the similarity structure of representations. There has been much work on improving generalization of DML in settings like zero-shot retrieval, but little is known about its implications for fairness. In this paper, we are the first to evaluate state-of-the-art DML methods trained on imbalanced data, and to show the negative impact these representations have on minority subgroup performance when used for downstream tasks. In this work, we first define fairness in DML through an analysis of three properties of the representation space – interclass alignment, intra-class alignment, and uniformity – and propose finDML, the fairness in non-balanced DML benchmark to characterize representation fairness. Utilizing finDML, we find bias in DML representations to propagate to common downstream classification tasks. Surprisingly, this bias is propagated even when training data in the downstream task is re-balanced. To address this problem, we present Partial Attribute De-correlation (PARADE) to de-correlate feature representations from sensitive attributes and reduce performance gaps between subgroups in both embedding space and downstream metrics",
    "volume": "main",
    "checked": true,
    "id": "528fb2ab93bd2a63354513a191b83907ccf4ccb5",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=oh4TirnfSem": {
    "title": "PF-GNN: Differentiable particle filtering based approximation of universal graph representations",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "696fc28d71cf4d681568d89646cce12f4d1eea50",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=wTTjnvGphYj": {
    "title": "Graph Neural Networks with Learnable Structural and Positional Representations",
    "abstract": "Graph neural networks (GNNs) have become the standard learning architectures for graphs. GNNs have been applied to numerous domains ranging from quantum chemistry, recommender systems to knowledge graphs and natural language processing. A major issue with arbitrary graphs is the absence of canonical positional information of nodes, which decreases the representation power of GNNs to distinguish e.g. isomorphic nodes and other graph symmetries. An approach to tackle this issue is to introduce Positional Encoding (PE) of nodes, and inject it into the input layer, like in Transformers. Possible graph PE are Laplacian eigenvectors. In this work, we propose to decouple structural and positional representations to make easy for the network to learn these two essential properties. We introduce a novel generic architecture which we call LSPE (Learnable Structural and Positional Encodings). We investigate several sparse and fully-connected (Transformer-like) GNNs, and observe a performance increase for molecular datasets, from 1.79% up to 64.14% when considering learnable PE for both GNN classes. 1",
    "volume": "main",
    "checked": true,
    "id": "454304628bf10f02aba1c2cfc95891e94d09208e",
    "citation_count": 38
  },
  "https://openreview.net/forum?id=IfNu7Dr-3fQ": {
    "title": "Generalized Kernel Thinning",
    "abstract": "The kernel thinning (KT) algorithm of Dwivedi and Mackey (2021) compresses a probability distribution more effectively than independent sampling by targeting a reproducing kernel Hilbert space (RKHS) and leveraging a less smooth square-root kernel. Here we provide four improvements. First, we show that KT applied directly to the target RKHS yields tighter, dimension-free guarantees for any kernel, any distribution, and any ﬁxed function in the RKHS. Second, we show that, for analytic kernels like Gaussian, inverse multiquadric, and sinc, target KT admits maximum mean discrepancy (MMD) guarantees comparable to or better than those of square-root KT without making explicit use of a square-root kernel. Third, we prove that KT with a fractional power kernel yields better-than-Monte-Carlo MMD guarantees for non-smooth kernels, like Laplace and Mat ´ ern, that do not have square-roots. Fourth, we establish that KT applied to a sum of the target and power kernels (a procedure we call KT+) simultaneously inherits the improved MMD guarantees of power KT and the tighter individual function guarantees of target KT. In our experiments with target KT and KT+, we witness signiﬁcant improvements in integration error even in 100 dimensions and when compressing challenging differential equation posteriors",
    "volume": "main",
    "checked": true,
    "id": "8fcff9e70e8dd39b7b04807487a13069aa748214",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=EAy7C1cgE1L": {
    "title": "Increasing the Cost of Model Extraction with Calibrated Proof of Work",
    "abstract": "number of queries, and timing for legitimate users, and (2) timing to solve a puzzle for a number of leading zeros. Based on the data from legitimate users, we create a model that predicts the total privacy cost from the number of queries to the privacy This ﬁrst model extrapolates to any of queries. The more data on legitimate we have more the speciﬁcation of between the number of queries and the cumulative privacy cost. We interpolate the by creating a linear model that predicts the number of leading To we",
    "volume": "main",
    "checked": true,
    "id": "5a36cff04ca7d7a8d7b7c780665a45942fc89df5",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=KSSfF5lMIAg": {
    "title": "Model Agnostic Interpretability for Multiple Instance Learning",
    "abstract": "In Multiple Instance Learning (MIL), models are trained using bags of instances, where only a single label is provided for each bag. A bag label is often only determined by a handful of key instances within a bag, making it difficult to interpret what information a classifier is using to make decisions. In this work, we establish the key requirements for interpreting MIL models. We then go on to develop several model-agnostic approaches that meet these requirements. Our methods are compared against existing inherently interpretable MIL models on several datasets, and achieve an increase in interpretability accuracy of up to 30%. We also examine the ability of the methods to identify interactions between instances and scale to larger datasets, improving their applicability to real-world problems",
    "volume": "main",
    "checked": true,
    "id": "d6721fcf0173b8212f888a636f03c7d5456b0c85",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=1JDiK_TbV4S": {
    "title": "Source-Free Adaptation to Measurement Shift via Bottom-Up Feature Restoration",
    "abstract": "Source-free domain adaptation (SFDA) aims to adapt a model trained on labelled data in a source domain to unlabelled data in a target domain without access to the source-domain data during adaptation . Existing methods for SFDA leverage entropy-minimization techniques which: (i) apply only to classiﬁcation; (ii) destroy model calibration; and (iii) rely on the source model achieving a good level of feature-space class-separation in the target domain. We address these issues for a particularly pervasive type of domain shift called measurement shift which can be resolved by restoring the source features rather than extracting new ones. In particular, we propose Feature Restoration (FR) wherein we: (i) store a lightweight and ﬂexible approximation of the feature distribution under the source data; and (ii) adapt the feature-extractor such that the approximate feature distribution under the target data realigns with that saved on the source. We additionally propose a bottom-up training scheme which boosts performance, which we call Bottom-Up Feature Restoration (BUFR). On real and synthetic data, we demonstrate that BUFR outperforms existing SFDA methods in terms of accuracy, calibration, and data efﬁciency, while being less reliant on the performance of the source model in the target domain",
    "volume": "main",
    "checked": true,
    "id": "5cc61b4440ce5aa2a28fff7a57401e1f947a4f1a",
    "citation_count": 15
  },
  "https://openreview.net/forum?id=RQLLzMCefQu": {
    "title": "Provably Filtering Exogenous Distractors using Multistep Inverse Dynamics",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "b8879d5beb8242c7a08d8d86ce3e6216c99e5f88",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=S874XAIpkR-": {
    "title": "RvS: What is Essential for Offline RL via Supervised Learning?",
    "abstract": "Recent work has shown that supervised learning alone, without temporal differ-ence (TD) learning, can be remarkably effective for ofﬂine RL. When does this hold true, and which algorithmic components are necessary? Through extensive experiments, we boil supervised learning for ofﬂine RL down to its essential elements. In every environment suite we consider, simply maximizing likelihood with a two-layer feedforward MLP is competitive with state-of-the-art results of substantially more complex methods based on TD learning or sequence modeling with Transformers. Carefully choosing model capacity (e.g., via regularization or architecture) and choosing which information to condition on (e.g., goals or rewards) are critical for performance. These insights serve as a ﬁeld guide for practitioners doing Reinforcement Learning via Supervised Learning (which we coin RvS learning ). They also probe the limits of existing RvS methods, which are comparatively weak on random data, and suggest a number of open problems",
    "volume": "main",
    "checked": true,
    "id": "63afc8d1a187d2f2faf603a51d3987db89574308",
    "citation_count": 32
  },
  "https://openreview.net/forum?id=dNigytemkL": {
    "title": "The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks",
    "abstract": "In this paper, we conjecture that if the permutation invariance of neural networks is taken into account, SGD solutions will likely have no barrier in the linear interpolation between them. Although it is a bold conjecture, we show how extensive empirical attempts fall short of refuting it. We further provide a preliminary theoretical result to support our conjecture. Our conjecture has implications for lottery ticket hypothesis, distributed training and ensemble methods. The source code is available at https://github.com/rahimentezari/PermutationInvariance barrier if the test error is lower. Therefore, any changes in the architecture or the task that improves the test error, also improves the loss barrier. Effect of depth is stronger than (architecture, task) which leads to high barrier values for ResNets on MNIST, SVHN, CIFAR10, CIFAR100, and ImageNet",
    "volume": "main",
    "checked": true,
    "id": "4dc48bd4e1c0e5986b36eca8339bd45e944d8a82",
    "citation_count": 24
  },
  "https://openreview.net/forum?id=6XGgutacQ0B": {
    "title": "Demystifying Batch Normalization in ReLU Networks: Equivalent Convex Optimization Models and Implicit Regularization",
    "abstract": "Batch Normalization (BN) is a commonly used technique to accelerate and stabilize training of deep neural networks. Despite its empirical success, a full theoretical understanding of BN is yet to be developed. In this work, we analyze BN through the lens of convex optimization. We introduce an analytic framework based on convex duality to obtain exact convex representations of weight-decay regularized ReLU networks with BN, which can be trained in polynomial-time. Our analyses also show that optimal layer weights can be obtained as simple closed-form for-mulas in the high-dimensional and/or overparameterized regimes. Furthermore, we ﬁnd that Gradient Descent provides an algorithmic bias effect on the standard non-convex BN network, and we design an approach to explicitly encode this implicit regularization into the convex objective. Experiments with CIFAR image classiﬁcation highlight the effectiveness of this explicit regularization for mimicking and substantially improving the performance of standard BN networks. al. (2022). Extensions of our convex BN analysis to these architectures are left for future work",
    "volume": "main",
    "checked": true,
    "id": "218b3ae98621b6530a191164efd3f09b0fdb4708",
    "citation_count": 18
  },
  "https://openreview.net/forum?id=X8cLTHexYyY": {
    "title": "Learning-Augmented $k$-means Clustering",
    "abstract": "k-means clustering is a well-studied problem due to its wide applicability. Unfortunately, there exist strong theoretical limits on the performance of any algorithm for the k-means problem on worst-case inputs. To overcome this barrier, we consider a scenario where \"advice\" is provided to help perform clustering. Specifically, we consider the k-means problem augmented with a predictor that, given any point, returns its cluster label in an approximately optimal clustering up to some, possibly adversarial, error. We present an algorithm whose performance improves along with the accuracy of the predictor, even though näıvely following the accurate predictor can still lead to a high clustering cost. Thus if the predictor is sufficiently accurate, we can retrieve a close to optimal clustering with nearly optimal runtime, breaking known computational barriers for algorithms that do not have access to such advice. We evaluate our algorithms on real datasets and show significant improvements in the quality of clustering",
    "volume": "main",
    "checked": true,
    "id": "0d352c530dd041a4c7a525e8b3f0e94f60c8fb34",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=qjN4h_wwUO": {
    "title": "GradMax: Growing Neural Networks using Gradient Information",
    "abstract": "The architecture and the parameters of neural networks are often optimized in-dependently, which requires costly retraining of the parameters whenever the architecture is modiﬁed. In this work we instead focus on growing the architecture without requiring costly retraining. We present a method that adds new neurons during training without impacting what is already learned, while improving the training dynamics. We achieve the latter by maximizing the gradients of the new weights and efﬁciently ﬁnd the optimal initialization by means of the singular value decomposition (SVD). We call this technique Gradient Maximizing Growth (GradMax) and demonstrate its effectiveness in variety of vision tasks and architectures 1 growth step and grow a new neuron using GradMax ( f g ) and Random ( f r ). Then we continue training for 500 steps and plot the difference in training loss (i.e. L ( f r ) − L ( f g ) ). All experiments are repeated 5 times",
    "volume": "main",
    "checked": true,
    "id": "b03fe9e4f02a43078386df4c0b44116e400bf56a",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=PtSAD3caaA2": {
    "title": "Maximum Entropy RL (Provably) Solves Some Robust RL Problems",
    "abstract": "Many potential applications of reinforcement learning (RL) require guarantees that the agent will perform well in the face of disturbances to the dynamics or reward function. In this paper, we prove theoretically that standard maximum entropy RL is robust to some disturbances in the dynamics and the reward function. While this capability of MaxEnt RL has been observed empirically in prior work, to the best of our knowledge our work provides the first rigorous proof and theoretical characterization of the MaxEnt RL robust set. While a number of prior robust RL algorithms have been designed to handle similar disturbances to the reward function or dynamics, these methods typically require adding additional moving parts and hyperparameters on top of a base RL algorithm. In contrast, our theoretical results suggest that MaxEnt RL by itself is robust to certain disturbances, without requiring any additional modifications. While this does not imply that MaxEnt RL is the best available robust RL method, MaxEnt RL does possess a striking simplicity and appealing formal guarantees",
    "volume": "main",
    "checked": true,
    "id": "b284afe9a7363b898661c9b3cfb7f015b158cc63",
    "citation_count": 41
  },
  "https://openreview.net/forum?id=3wU2UX0voE": {
    "title": "The Information Geometry of Unsupervised Reinforcement Learning",
    "abstract": "How can a reinforcement learning (RL) agent prepare to solve downstream tasks if those tasks are not known a priori? One approach is unsupervised skill discovery, a class of algorithms that learn a set of policies without access to a reward function. Such algorithms bear a close resemblance to representation learning algorithms (e.g., contrastive learning) in supervised learning, in that both are pretraining algorithms that maximize some approximation to a mutual information objective. While prior work has shown that the set of skills learned by such methods can accelerate downstream RL tasks, prior work offers little analysis into whether these skill learning algorithms are optimal, or even what notion of optimality would be appropriate to apply to them. In this work, we show that unsupervised skill discovery algorithms based on mutual information maximization do not learn skills that are optimal for every possible reward function. However, we show that the distribution over skills provides an optimal initialization minimizing regret against adversarially-chosen reward functions, assuming a certain type of adaptation procedure. Our analysis also provides a geometric perspective on these skill learning methods. af- ter adaptation, which maximizes reward and minimizes information cost",
    "volume": "main",
    "checked": true,
    "id": "40888b859c5b40868943162e3c4769dae1aed716",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=FPCMqjI0jXN": {
    "title": "Domino: Discovering Systematic Errors with Cross-Modal Embeddings",
    "abstract": "Machine learning models that achieve high overall accuracy often make systematic errors on important subsets (or slices ) of data. Identifying underperforming slices is particularly challenging when working with high-dimensional inputs ( e.g. images, audio), where important slices are often unlabeled. In order to address this issue, recent studies have proposed automated slice discovery methods (SDMs), which leverage learned model representations to mine input data for slices on which a model performs poorly. To be useful to a practitioner, these methods must identify slices that are both underperforming and coherent ( i.e. united by a human-understandable concept). However, no quantitative evaluation framework currently exists for rigorously assessing SDMs with respect to these criteria. Ad-ditionally, prior qualitative evaluations have shown that SDMs often identify slices that are incoherent. In this work, we address these challenges by ﬁrst designing a principled evaluation framework that enables a quantitative comparison of SDMs across 1,235 slice discovery settings in three input domains (natural images, medical images, and time-series data). Then, motivated by the recent development of powerful cross-modal representation learning approaches, we present Domino , an SDM that leverages cross-modal embeddings and a novel error-aware mixture model to discover and describe coherent slices. We ﬁnd that Domino accurately identiﬁes 36% of the 1,235 slices in our framework – a 12 percentage point improvement over prior methods. Further, Domino is the ﬁrst SDM that can provide natural language descriptions of identiﬁed slices, correctly generating the exact name of the slice in 35% of settings",
    "volume": "main",
    "checked": true,
    "id": "0567131ec1f839240179927ceb61f51bdd173055",
    "citation_count": 21
  },
  "https://openreview.net/forum?id=aBVxf5NaaRt": {
    "title": "Unrolling PALM for Sparse Semi-Blind Source Separation",
    "abstract": "Sparse Blind Source Separation (BSS) has become a well established tool for a wide range of applications – for instance, in astrophysics and remote sensing. Classical sparse BSS methods, such as the Proximal Alternating Linearized Minimization (PALM) algorithm, nevertheless often suffer from a difficult hyperparameter choice, which undermines their results. To bypass this pitfall, we propose in this work to build on the thriving field of algorithm unfolding/unrolling. Unrolling PALM enables to leverage the data-driven knowledge stemming from realistic simulations or ground-truth data by learning both PALM hyperparameters and variables. In contrast to most existing unrolled algorithms, which assume a fixed known dictionary during the training and testing phases, this article further emphasizes on the ability to deal with variable mixing matrices (a.k.a. dictionaries). The proposed Learned PALM (LPALM) algorithm thus enables to perform semi-blind source separation, which is key to increase the generalization of the learnt model in real-world applications. We illustrate the relevance of LPALM in astrophysical multispectral imaging: the algorithm not only needs up to 10−10 times fewer iterations than PALM, but also improves the separation quality, while avoiding the cumbersome hyperparameter and initialization choice of PALM. We further show that LPALM outperforms other unrolled source separation methods in the semi-blind setting",
    "volume": "main",
    "checked": true,
    "id": "32e8ddbc580c39967501992b5a35a3cab20b7807",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=qhkFX-HLuHV": {
    "title": "Can an Image Classifier Suffice For Action Recognition?",
    "abstract": "We explore a new perspective on video understanding by casting the video recognition problem as an image recognition task. Our approach rearranges input video frames into super images, which allow for training an image classiﬁer directly to fulﬁll the task of action recognition, in exactly the same way as image classiﬁcation. With such a simple idea, we show that transformer-based image classiﬁers alone can sufﬁce for action recognition. In particular, our approach demonstrates strong and promising performance against SOTA methods on several pub-lic datasets including Kinetics400, Moments In Time, Something-Something V2 (SSV2), Jester and Diving48. We also experiment with the prevalent ResNet image classiﬁers in computer vision to further validate our idea. The results on both Kinetics400 and SSV2 are comparable to some of the best-performed CNN approaches based on spatio-temporal modeling. Our source codes and models are available at https://github.com/IBM/sifar-pytorch",
    "volume": "main",
    "checked": true,
    "id": "bd189ee2c5860461d0165a2971f5ff32b66b8c41",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=6YVIk0sAkF_": {
    "title": "Multi-Mode Deep Matrix and Tensor Factorization",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "1c7825a8cf4dabc866a6e5b0a88cff37f1d123d4",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=WAid50QschI": {
    "title": "Sparse Communication via Mixed Distributions",
    "abstract": "Neural networks and other machine learning models compute continuous representations, while humans communicate mostly through discrete symbols. Reconciling these two forms of communication is desirable for generating human-readable interpretations or learning discrete latent variable models, while maintaining endto-end differentiability. Some existing approaches (such as the Gumbel-Softmax transformation) build continuous relaxations that are discrete approximations in the zero-temperature limit, while others (such as sparsemax transformations and the Hard Concrete distribution) produce discrete/continuous hybrids. In this paper, we build rigorous theoretical foundations for these hybrids, which we call \"mixed random variables.\" Our starting point is a new \"direct sum\" base measure defined on the face lattice of the probability simplex. From this measure, we introduce new entropy and Kullback-Leibler divergence functions that subsume the discrete and differential cases and have interpretations in terms of code optimality. Our framework suggests two strategies for representing and sampling mixed random variables, an extrinsic (\"sample-and-project\") and an intrinsic one (based on face stratification). We experiment with both approaches on an emergent communication benchmark and on modeling MNIST and Fashion-MNIST data with variational auto-encoders with mixed latent variables. Our code is publicly available",
    "volume": "main",
    "checked": true,
    "id": "3bbb736d74fc7c96ebbe7ebf5bf82060840bb7f4",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=_4GFbtOuWq-": {
    "title": "Capacity of Group-invariant Linear Readouts from Equivariant Representations: How Many Objects can be Linearly Classified Under All Possible Views?",
    "abstract": "Equivariance has emerged as a desirable property of representations of objects subject to identity-preserving transformations that constitute a group, such as translations and rotations. However, the expressivity of a representation constrained by group equivariance is still not fully understood. We address this gap by providing a generalization of Cover's Function Counting Theorem that quantifies the number of linearly separable and group-invariant binary dichotomies that can be assigned to equivariant representations of objects. We find that the fraction of separable dichotomies is determined by the dimension of the space that is fixed by the group action. We show how this relation extends to operations such as convolutions, element-wise nonlinearities, and global and local pooling. While other operations do not change the fraction of separable dichotomies, local pooling decreases the fraction, despite being a highly nonlinear operation. Finally, we test our theory on intermediate representations of randomly initialized and fully trained convolutional neural networks and find perfect agreement",
    "volume": "main",
    "checked": true,
    "id": "b8084f26fa37ca9fa51a0489490a6e382e67b67e",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=c87d0TS4yX": {
    "title": "Orchestrated Value Mapping for Reinforcement Learning",
    "abstract": "We present a general convergent class of reinforcement learning algorithms that is founded on two distinct principles: (1) mapping value estimates to a different space using arbitrary functions from a broad class, and (2) linearly decomposing the reward signal into multiple channels. The first principle enables incorporating specific properties into the value estimator that can enhance learning. The second principle, on the other hand, allows for the value function to be represented as a composition of multiple utility functions. This can be leveraged for various purposes, e.g. dealing with highly varying reward scales, incorporating a priori knowledge about the sources of reward, and ensemble learning. Combining the two principles yields a general blueprint for instantiating convergent algorithms by orchestrating diverse mapping functions over multiple reward channels. This blueprint generalizes and subsumes algorithms such as Q-Learning, Log Q-Learning, and Q-Decomposition. In addition, our convergence proof for this general class relaxes certain required assumptions in some of these algorithms. Based on our theory, we discuss several interesting configurations as special cases. Finally, to illustrate the potential of the design space that our theory opens up, we instantiate a particular algorithm and evaluate its performance on the Atari suite",
    "volume": "main",
    "checked": true,
    "id": "32455a01db6a0054bb606358eab53596a99a64df",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=gRCCdgpVZf": {
    "title": "Provable Adaptation across Multiway Domains via Representation Learning",
    "abstract": "This paper studies zero-shot domain adaptation where each domain is indexed on a multidimensional array, and we only have data from a small subset of domains. Our goal is to produce predictors that perform well on unseen domains. We propose a model which consists of a domain-invariant latent representation layer and a domain-specific linear prediction layer with a low-rank tensor structure. Theoretically, we present explicit sample complexity bounds to characterize the prediction error on unseen domains in terms of the number of domains with training data and the number of data per domain. To our knowledge, this is the first finitesample guarantee for zero-shot domain adaptation. In addition, we provide experiments on two-way MNIST and four-way fiber sensing datasets to demonstrate the effectiveness of our proposed model",
    "volume": "main",
    "checked": true,
    "id": "11aacbe1c8534d2fcfa8eb1967c277144da82682",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=Jjcv9MTqhcq": {
    "title": "Rethinking Supervised Pre-Training for Better Downstream Transferring",
    "abstract": "The pretrain-finetune paradigm has shown outstanding performance on many applications of deep learning, where a model is pre-trained on an upstream large dataset (e.g. ImageNet), and is then fine-tuned to different downstream tasks. Though for most cases, the pre-training stage is conducted based on supervised methods, recent works on self-supervised pre-training have shown powerful transferability and even outperform supervised pre-training on multiple downstream tasks. It thus remains as an open question how to better generalize supervised pretraining model to downstream tasks. In this paper, we argue that the worse transferability of existing supervised pre-training methods arise from the negligence of valuable intra-class semantic difference. This is because these methods tend to push images from the same class close to each other despite of the large diversity in their visual contents, a problem to which referred as \"overfit of upstream tasks\". To alleviate this problem, we propose a new supervised pre-training method based on Leave-One-Out K-Nearest-Neighbor, or LOOK for short. It relieves the problem of overfitting upstream tasks by only requiring each image to share its class label with most of its k nearest neighbors, thus allowing each class to exhibit a multi-mode distribution and consequentially preserving part of intra-class difference for better transferring to downstream tasks. We developed efficient implementation of the proposed method that scales well to large datasets. Extensive empirical studies on multiple downstream tasks show that LOOK outperforms other state-of-the-art methods for supervised and self-supervised pre-training",
    "volume": "main",
    "checked": true,
    "id": "05700beec6388a11ff72cbe996c9f7634a8f8f2d",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=Ve0Wth3ptT_": {
    "title": "DEGREE: Decomposition Based Explanation for Graph Neural Networks",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "217e328eba3b03cea1f574f967ce58cbc923ace9",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=l_amHf1oaK": {
    "title": "Complete Verification via Multi-Neuron Relaxation Guided Branch-and-Bound",
    "abstract": "State-of-the-art neural network veriﬁers are fundamentally based on one of two paradigms: either encoding the whole veriﬁcation problem via tight multi-neuron convex relaxations or applying a Branch-and-Bound (BaB) procedure leveraging imprecise but fast bounding methods on a large number of easier subproblems. The former can capture complex multi-neuron dependencies but sacriﬁces com-pleteness due to the inherent limitations of convex relaxations. The latter enables complete veriﬁcation but becomes increasingly ineffective on larger and more challenging networks. In this work, we present a novel complete veriﬁer which combines the strengths of both paradigms: it leverages multi-neuron relaxations to drastically reduce the number of subproblems generated during the BaB process and an efﬁcient GPU-based dual optimizer to solve the remaining ones. An extensive evaluation demonstrates that our veriﬁer achieves a new state-of-the-art on both established benchmarks as well as networks with signiﬁcantly higher accuracy than previously considered. The latter result (up to 28% certiﬁcation gains) indicates meaningful progress towards creating veriﬁers that can handle practically relevant networks. constraints, a novel branching heuristic and an efﬁcient dual solver, able to utilize massively parallel hardware accelerators, to enable the veriﬁcation of particularly challenging networks. Our thorough empirical evaluation shows how MN-B A B is particularly effective in verifying challenging networks with high natural accuracy and practical relevance, reaching a new state-of-the-art in several settings",
    "volume": "main",
    "checked": true,
    "id": "d7aff3e7c84e8440b49a3cd36f686c6486094956",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=C1_esHN6AVn": {
    "title": "Learning Synthetic Environments and Reward Networks for Reinforcement Learning",
    "abstract": "We introduce Synthetic Environments (SEs) and Reward Networks (RNs), represented by neural networks, as proxy environment models for training Reinforcement Learning (RL) agents. We show that an agent, after being trained exclusively on the SE, is able to solve the corresponding real environment. While an SE acts as a full proxy to a real environment by learning about its state dynamics and rewards, an RN is a partial proxy that learns to augment or replace rewards. We use bi-level optimization to evolve SEs and RNs: the inner loop trains the RL agent, and the outer loop trains the parameters of the SE / RN via an evolution strategy. We evaluate our proposed new concept on a broad range of RL algorithms and classic control environments. In a one-to-one comparison, learning an SE proxy requires more interactions with the real environment than training agents only on the real environment. However, once such an SE has been learned, we do not need any interactions with the real environment to train new agents. Moreover, the learned SE proxies allow us to train agents with fewer interactions while maintaining the original task performance. Our empirical results suggest that SEs achieve this result by learning informed representations that bias the agents towards relevant states. Moreover, we find that these proxies are robust against hyperparameter variation and can also transfer to unseen agents",
    "volume": "main",
    "checked": true,
    "id": "0c529ba981b90b2d1536d342bf92a7451686164a",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=xP3cPq2hQC": {
    "title": "Cross-Domain Imitation Learning via Optimal Transport",
    "abstract": "Cross-domain imitation learning studies how to leverage expert demonstrations of one agent to train an imitation agent with a different embodiment or morphology. Comparing trajectories and stationary distributions between the expert and imitation agents is challenging because they live on different systems that may not even have the same dimensionality. We propose Gromov-Wasserstein Imitation Learning (GWIL) , a method for cross-domain imitation that uses the Gromov-Wasserstein distance to align and compare states between the different spaces of the agents. Our theory formally characterizes the scenarios where GWIL preserves optimality, revealing its possibilities and limitations. We demonstrate the effectiveness of GWIL in non-trivial continuous control domains ranging from simple rigid transformation of the expert domain to arbitrary transformation of the state-action space. 1",
    "volume": "main",
    "checked": true,
    "id": "09f9af75d631ff9e6b71726c9344b9bedb921b42",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=UdxJ2fJx7N0": {
    "title": "Minimax Optimization with Smooth Algorithmic Adversaries",
    "abstract": "This paper considers minimax optimization minx maxy f(x, y) in the challenging setting where f can be both nonconvex in x and nonconcave in y. Though such optimization problems arise in many machine learning paradigms including training generative adversarial networks (GANs) and adversarially robust models, many fundamental issues remain in theory, such as the absence of efficiently computable optimality notions, and cyclic or diverging behavior of existing algorithms. Our framework sprouts from the practical consideration that under a computational budget, the max-player can not fully maximize f(x, ·) since nonconcave maximization is NP-hard in general. So, we propose a new algorithm for the min-player to play against smooth algorithms deployed by the adversary (i.e., the max-player) instead of against full maximization. Our algorithm is guaranteed to make monotonic progress (thus having no limit cycles), and to find an appropriate \"stationary point\" in a polynomial number of iterations. Our framework covers practical settings where the smooth algorithms deployed by the adversary are multi-step stochastic gradient ascent, and its accelerated version. We further provide complementing experiments that confirm our theoretical findings and demonstrate the effectiveness of the proposed approach in practice",
    "volume": "main",
    "checked": true,
    "id": "dbdefb498b619912a726fec7c85533594a1c6a1b",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=9n9c8sf0xm": {
    "title": "Plant 'n' Seek: Can You Find the Winning Ticket?",
    "abstract": "The lottery ticket hypothesis has sparked the rapid development of pruning algorithms that aim to reduce the computational costs associated with deep learning during training and model deployment. Currently, such algorithms are primar-ily evaluated on imaging data, for which we lack ground truth information and thus the understanding of how sparse lottery tickets could be. To ﬁll this gap, we develop a framework that allows us to plant and hide winning tickets with desirable properties in randomly initialized neural networks. To analyze the ability of state-of-the-art pruning to identify tickets of extreme sparsity, we design and hide such tickets solving four challenging tasks. In extensive experiments, we observe similar trends as in imaging studies, indicating that our framework can provide transferable insights into realistic problems. Additionally, we can now see beyond such relative trends and highlight limitations of current pruning methods. Based on our results, we conclude that the current limitations in ticket sparsity are likely of algorithmic rather than fundamental nature. We anticipate that comparisons to planted tickets will facilitate future developments of efﬁcient pruning algorithms",
    "volume": "main",
    "checked": true,
    "id": "67618071e2e63921dde7471bc3c835f0cebe5a41",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=b-ny3x071E5": {
    "title": "Bootstrapped Meta-Learning",
    "abstract": "Meta-learning empowers artificial intelligence to increase its efficiency by learning how to learn. Unlocking this potential involves overcoming a challenging meta-optimisation problem that often exhibits ill-conditioning, and myopic metaobjectives. We propose an algorithm that tackles these issues by letting the metalearner teach itself. The algorithm first bootstraps a target from the meta-learner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. Focusing on meta-learning with gradients, we establish conditions that guarantee performance improvements and show that the improvement is related to the target distance. Thus, by controlling curvature, the distance measure can be used to ease meta-optimization, for instance by reducing ill-conditioning. Further, the bootstrapping mechanism can extend the effective meta-learning horizon without requiring backpropagation through all updates. The algorithm is versatile and easy to implement. We achieve a new state-of-the art for model-free agents on the Atari ALE benchmark, improve upon MAML in few-shot learning, and demonstrate how our approach opens up new possibilities by meta-learning efficient exploration in an ε-greedy Q-learning agent",
    "volume": "main",
    "checked": true,
    "id": "f8befa0bc3442979ff19e070f7c6b16d66a776c5",
    "citation_count": 19
  },
  "https://openreview.net/forum?id=xkjqJYqRJy": {
    "title": "Bayesian Neural Network Priors Revisited",
    "abstract": "study in We while We show on a in",
    "volume": "main",
    "checked": true,
    "id": "3299b02c20da6d68f66918c2dd2ff5e35b01ca7b",
    "citation_count": 54
  },
  "https://openreview.net/forum?id=fwzUgo0FM9v": {
    "title": "Robbing the Fed: Directly Obtaining Private Data in Federated Learning with Modified Models",
    "abstract": "Federated learning has quickly gained popularity with its promises of increased user privacy and efficiency. Previous works have shown that federated gradient updates contain information that can be used to approximately recover user data in some situations. These previous attacks on user privacy have been limited in scope and do not scale to gradient updates aggregated over even a handful of data points, leaving some to conclude that data privacy is still intact for realistic training regimes. In this work, we introduce a new threat model based on minimal but malicious modifications of the shared model architecture which enable the server to directly obtain a verbatim copy of user data from gradient updates without solving difficult inverse problems. Even user data aggregated over large batches – where previous methods fail to extract meaningful content – can be reconstructed by these minimally modified models",
    "volume": "main",
    "checked": true,
    "id": "1a9004ca6d1d5f18e3f2925c4ab1fbea5fb2fd68",
    "citation_count": 22
  },
  "https://openreview.net/forum?id=KxbhdyiPHE": {
    "title": "Learning Altruistic Behaviours in Reinforcement Learning without External Rewards",
    "abstract": "Can artificial agents learn to assist others in achieving their goals without knowing what those goals are? Generic reinforcement learning agents could be trained to behave altruistically towards others by rewarding them for altruistic behaviour, i.e., rewarding them for benefiting other agents in a given situation. Such an approach assumes that other agents' goals are known so that the altruistic agent can cooperate in achieving those goals. However, explicit knowledge of other agents' goals is often difficult to acquire. In the case of human agents, their goals and preferences may be difficult to express fully; they might be ambiguous or even contradictory. Thus, it is beneficial to develop agents that do not depend on external supervision and learn altruistic behaviour in a task-agnostic manner. We propose to act altruistically towards other agents by giving them more choice and allowing them to achieve their goals better. Some concrete examples include opening a door for others or safeguarding them to pursue their objectives without interference. We formalize this concept and propose an altruistic agent that learns to increase the choices another agent has by preferring to maximize the number of states that the other agent can reach in its future. We evaluate our approach in three different multi-agent environments where another agent's success depends on altruistic behaviour. Finally, we show that our unsupervised agents can perform comparably to agents explicitly trained to work cooperatively, in some cases even outperforming them",
    "volume": "main",
    "checked": true,
    "id": "4c5b2f64b661957a90945e58e8f35a8ca829d887",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=81e1aeOt-sd": {
    "title": "On-Policy Model Errors in Reinforcement Learning",
    "abstract": "Model-free reinforcement learning algorithms can compute policy gradients given sampled environment transitions, but require large amounts of data. In contrast, model-based methods can use the learned model to generate new data, but model errors and bias can render learning unstable or sub-optimal. In this paper, we present a novel method that combines real world data and a learned model in order to get the best of both worlds. The core idea is to exploit the real world data for onpolicy predictions and use the learned model only to generalize to different actions. Specifically, we use the data as time-dependent on-policy correction terms on top of a learned model, to retain the ability to generate data without accumulating errors over long prediction horizons. We motivate this method theoretically and show that it counteracts an error term for model-based policy improvement. Experiments on MuJoCoand PyBullet-benchmarks show that our method can drastically improve existing model-based approaches without introducing additional tuning parameters",
    "volume": "main",
    "checked": true,
    "id": "c425b527bbeffc2e4b5bc7a42649cd16a3fa216f",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=w_drCosT76": {
    "title": "Differentiable Scaffolding Tree for Molecule Optimization",
    "abstract": "The structural design of functional molecules, also called molecular optimization, is an essential chemical science and engineering task with important applications, such as drug discovery. Deep generative models and combinatorial optimization methods achieve initial success but still struggle with directly modeling discrete chemical structures and often heavily rely on brute-force enumeration. The challenge comes from the discrete and non-differentiable nature of molecule structures. To address this, we propose differentiable scaffolding tree (DST) that utilizes a learned knowledge network to convert discrete chemical structures to locally differentiable ones. DST enables a gradient-based optimization on a chemical graph structure by back-propagating the derivatives from the target properties through a graph neural network (GNN). Our empirical studies show the gradient-based molecular optimizations are both effective and sample efficient. Furthermore, the learned graph parameters can also provide an explanation that helps domain experts understand the model output",
    "volume": "main",
    "checked": true,
    "id": "3f87530bf87a3ed3a7a803f0aa7815484d5bc7e6",
    "citation_count": 24
  },
  "https://openreview.net/forum?id=baUQQPwQiAg": {
    "title": "Robust Unlearnable Examples: Protecting Data Privacy Against Adversarial Learning",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "f943391013a0436b084d6e11b8527e1465cfff53",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=dTqOcTUOQO": {
    "title": "Knowledge Removal in Sampling-based Bayesian Inference",
    "abstract": "The right to be forgotten has been legislated in many countries, but its enforcement in the AI industry would cause unbearable costs. When single data deletion requests come, companies may need to delete the whole models learned with massive resources. Existing works propose methods to remove knowledge learned from data for explicitly parameterized models, which however are not appliable to the sampling-based Bayesian inference, i.e., Markov chain Monte Carlo (MCMC), as MCMC can only infer implicit distributions. In this paper, we propose the first machine unlearning algorithm for MCMC. We first convert the MCMC unlearning problem into an explicit optimization problem. Based on this problem conversion, an MCMC influence function is designed to provably characterize the learned knowledge from data, which then delivers the MCMC unlearning algorithm. Theoretical analysis shows that MCMC unlearning would not compromise the generalizability of the MCMC models. Experiments on Gaussian mixture models and Bayesian neural networks confirm the effectiveness of the proposed algorithm. The code is available at https: //github.com/fshp971/mcmc-unlearning",
    "volume": "main",
    "checked": true,
    "id": "56a9f45906eb341dbd9f30f76dbbaa30a7213c72",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=xS8AMYiEav3": {
    "title": "Sound and Complete Neural Network Repair with Minimality and Locality Guarantees",
    "abstract": "We present a novel methodology for repairing neural networks that use ReLU activation functions. Unlike existing methods that rely on modifying the weights of a neural network which can induce a global change in the function space, our approach applies only a localized change in the function space while still guaranteeing the removal of the buggy behavior. By leveraging the piecewise linear nature of ReLU networks, our approach can efﬁciently construct a patch network tailored to the linear region where the buggy input resides, which when combined with the original network, provably corrects the behavior on the buggy input. Our method is both sound and complete – the repaired network is guaranteed to ﬁx the buggy input, and a patch is guaranteed to be found for any buggy input. Moreover, our approach preserves the continuous piecewise linear nature of ReLU networks, automatically generalizes the repair to all the points including other undetected buggy inputs inside the repair region, is minimal in terms of changes in the function space, and guarantees that outputs on inputs away from the repair region are unaltered. On several benchmarks, we show that our approach signiﬁcantly outperforms existing methods in terms of locality and limiting negative side effects. Our code is available on GitHub: https://github.com/BU-DEPEND-Lab/REASSURE",
    "volume": "main",
    "checked": true,
    "id": "17b68e384b607208747606033b1dbe3f6a9262fd",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=DTXZqTNV5nW": {
    "title": "Actor-Critic Policy Optimization in a Large-Scale Imperfect-Information Game",
    "abstract": "The deep policy gradient method has demonstrated promising results in many large-scale games, where the agent learns purely from its own experience. Yet, policy gradient methods with self-play suffer convergence problems to a Nash Equilibrium (NE) in multi-agent situations. Counterfactual regret minimization (CFR) has a convergence guarantee to a NE in 2-player zero-sum games, but it usually needs domain-speciﬁc abstractions to deal with large-scale games. Inheriting merits from both methods, in this paper we extend the actor-critic algorithm framework in deep reinforcement learning to tackle a large-scale 2-player zero-sum imperfect-information game, 1-on-1 Mahjong, whose information set size and game length are much larger than poker. The proposed algorithm, named Actor-Critic Hedge (ACH), modiﬁes the policy optimization objective from originally maximizing the discounted returns to minimizing a type of weighted cumulative counterfactual regret. This modiﬁcation is achieved by approximating the regret via a deep neural network and minimizing the regret via generating self-play policies using Hedge. ACH is theoretically justiﬁed as it is derived from a neural-based weighted CFR, for which we prove the convergence to a NE under certain conditions. Experimental results on the proposed 1-on-1 Mahjong benchmark and benchmarks from the literature demonstrate that ACH outperforms related state-of-the-art methods. Also, the agent obtained",
    "volume": "main",
    "checked": true,
    "id": "70baefe23e4e0d2de3c1e7be689ce6092c4638ea",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=28ib9tf6zhr": {
    "title": "Patch-Fool: Are Vision Transformers Always Robust Against Adversarial Perturbations?",
    "abstract": "Vision transformers (ViTs) have recently set off a new wave in neural architecture design thanks to their record-breaking performance in various vision tasks. In parallel, to fulﬁll the goal of deploying ViTs into real-world vision applica-tions, their robustness against potential malicious attacks has gained increasing attention. In particular, recent works show that ViTs are more robust against adversarial attacks as compared with convolutional neural networks (CNNs), and conjecture that this is because ViTs focus more on capturing global interactions among different input/feature patches, leading to their improved robustness to local perturbations imposed by adversarial attacks. In this work, we ask an intriguing question: \"Under what kinds of perturbations do ViTs become more vulnerable learners compared to CNNs?\" Driven by this question, we ﬁrst conduct a com-prehensive experiment regarding the robustness of both ViTs and CNNs under various existing adversarial attacks to understand the underlying reason favoring their robustness. Based on the drawn insights, we then propose a dedicated attack framework, dubbed Patch-Fool, that fools the self-attention mechanism by attacking its basic component (i.e., a single patch ) with a series of attention-aware optimization techniques. Interestingly, our Patch-Fool framework shows for the ﬁrst time that ViTs are not necessarily more robust than CNNs against adversarial perturbations. In particular, we ﬁnd that ViTs are more vulnerable learners compared with CNNs against our Patch-Fool attack which is consistent across extensive experiments, and the observations from Sparse/Mild Patch-Fool, two variants of Patch-Fool, indicate an intriguing insight that the perturbation density",
    "volume": "main",
    "checked": true,
    "id": "c24f4fcec3f1fba7a741f6348aafe5899d06ef77",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=CAjxVodl_v": {
    "title": "Generalized Decision Transformer for Offline Hindsight Information Matching",
    "abstract": "How to extract as much learning signal from each trajectory data has been a key problem in reinforcement learning (RL), where sample inefficiency has posed serious challenges for practical applications. Recent works have shown that using expressive policy function approximators and conditioning on future trajectory information – such as future states in hindsight experience replay (HER) or returnsto-go in Decision Transformer (DT) – enables efficient learning of multi-task policies, where at times online RL is fully replaced by offline behavioral cloning (BC), e.g. sequence modeling. We demonstrate that all these approaches are doing hindsight information matching (HIM) – training policies that can output the rest of trajectory that matches some statistics of future state information. We present Generalized Decision Transformer (GDT) for solving any HIM problem, and show how different choices for the feature function and the anti-causal aggregator not only recover DT as a special case, but also lead to novel Categorical DT (CDT) and Bi-directional DT (BDT) for matching different statistics of the future. For evaluating CDT and BDT, we define offline multi-task state-marginal matching (SMM) and imitation learning (IL) as two generic HIM problems, propose a Wasserstein distance loss as a metric for both, and empirically study them on MuJoCo continuous control benchmarks. Categorical DT, which simply replaces anti-causal summation with anti-causal binning in DT, enables arguably the first effective offline multi-task SMM algorithm that generalizes well to unseen (and even synthetic) multi-modal reward or state-feature distributions. Bi-directional DT, which uses an anti-causal second transformer as the aggregator, can learn to model any statistics of the future and outperforms DT variants in offline multi-task IL, i.e. one-shot IL. Our generalized formulations from HIM and GDT greatly expand the role of powerful sequence modeling architectures in modern RL",
    "volume": "main",
    "checked": true,
    "id": "387a17823d7c47c0bd3390a124708933032989e0",
    "citation_count": 29
  },
  "https://openreview.net/forum?id=SwIp410B6aQ": {
    "title": "On the Role of Neural Collapse in Transfer Learning",
    "abstract": "We study the ability of foundation models to learn representations for classification that are transferable to new, unseen classes. Recent results in the literature show that representations learned by a single classifier over many classes are competitive on few-shot learning problems with representations learned by special-purpose algorithms designed for such problems. In this paper we provide an explanation for this behavior based on the recently observed phenomenon that the features learned by overparameterized classification networks show an interesting clustering property, called neural collapse. We demonstrate both theoretically and empirically that neural collapse generalizes to new samples from the training classes, and – more importantly – to new classes as well, allowing foundation models to provide feature maps that work well in transfer learning and, specifically, in the few-shot setting",
    "volume": "main",
    "checked": true,
    "id": "8f0d609618838b20631469ffa7fc78928bba8ca0",
    "citation_count": 20
  },
  "https://openreview.net/forum?id=GQjaI9mLet": {
    "title": "Independent SE(3)-Equivariant Models for End-to-End Rigid Protein Docking",
    "abstract": "Protein complex formation is a central problem in biology, being involved in most of the cell's processes, and essential for applications, e.g. drug design or protein engineering. We tackle rigid body protein-protein docking, i.e., computationally predicting the 3D structure of a protein-protein complex from the individual unbound structures, assuming no conformational change within the proteins happens during binding. We design a novel pairwise-independent SE(3)-equivariant graph matching network to predict the rotation and translation to place one of the proteins at the right docked position relative to the second protein. We mathematically guarantee a basic principle: the predicted complex is always identical regardless of the initial locations and orientations of the two structures. Our model, named EQUIDOCK, approximates the binding pockets and predicts the docking poses using keypoint matching and alignment, achieved through optimal transport and a differentiable Kabsch algorithm. Empirically, we achieve significant running time improvements and often outperform existing docking software despite not relying on heavy candidate sampling, structure refinement, or templates. 1",
    "volume": "main",
    "checked": true,
    "id": "3f1ef7d7f1a8f50823ca848e222b28e407a9a5e4",
    "citation_count": 33
  },
  "https://openreview.net/forum?id=twv2QlJhXzo": {
    "title": "Imitation Learning from Observations under Transition Model Disparity",
    "abstract": "Learning to perform tasks by leveraging a dataset of expert observations, also known as imitation learning from observations (ILO), is an important paradigm for learning skills without access to the expert reward function or the expert actions. We consider ILO in the setting where the expert and the learner agents operate in different environments, with the source of the discrepancy being the transition dynamics model. Recent methods for scalable ILO utilize adversarial learning to match the state-transition distributions of the expert and the learner, an approach that becomes challenging when the dynamics are dissimilar. In this work, we propose an algorithm that trains an intermediary policy in the learner environment and uses it as a surrogate expert for the learner. The intermediary policy is learned such that the state transitions generated by it are close to the state transitions in the expert dataset. To derive a practical and scalable algorithm, we employ concepts from prior work on estimating the support of a probability distribution. Experiments using MuJoCo locomotion tasks highlight that our method compares favorably to the baselines for ILO with transition dynamics mismatch 1",
    "volume": "main",
    "checked": true,
    "id": "72289526642ab84890bb574d1eb1af48cfce09c3",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=fXHl76nO2AZ": {
    "title": "Gradient Importance Learning for Incomplete Observations",
    "abstract": "Though recent works have developed methods that can generate estimates (or imputations) of the missing entries in a dataset to facilitate downstream analysis, most depend on assumptions that may not align with real-world applications and could suffer from poor performance in subsequent tasks such as classification. This is particularly true if the data have large missingness rates or a small sample size. More importantly, the imputation error could be propagated into the prediction step that follows, which may constrain the capabilities of the prediction model. In this work, we introduce the gradient importance learning (GIL) method to train multilayer perceptrons (MLPs) and long short-term memories (LSTMs) to directly perform inference from inputs containing missing values without imputation. Specifically, we employ reinforcement learning (RL) to adjust the gradients used to train these models via back-propagation. This allows the model to exploit the underlying information behind missingness patterns. We test the approach on real-world time-series (i.e., MIMIC-III), tabular data obtained from an eye clinic, and a standard dataset (i.e., MNIST), where our imputation-free predictions outperform the traditional two-step imputation-based predictions using state-of-the-art imputation methods",
    "volume": "main",
    "checked": true,
    "id": "747feca620cdedc3456fe39bf8f7e621f3c8f784",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=apv504XsysP": {
    "title": "Ab-Initio Potential Energy Surfaces by Pairing GNNs with Neural Wave Functions",
    "abstract": "Solving the Schrödinger equation is key to many quantum mechanical properties. However, an analytical solution is only tractable for single-electron systems. Recently, neural networks succeeded at modeling wave functions of many-electron systems. Together with the variational Monte-Carlo (VMC) framework, this led to solutions on par with the best known classical methods. Still, these neural methods require tremendous amounts of computational resources as one has to train a separate model for each molecular geometry. In this work, we combine a Graph Neural Network (GNN) with a neural wave function to simultaneously solve the Schrödinger equation for multiple geometries via VMC. This enables us to model continuous subsets of the potential energy surface with a single training pass. Compared to existing state-of-the-art networks, our Potential Energy Surface Network (PESNet) speeds up training for multiple geometries by up to 40 times while matching or surpassing their accuracy. This may open the path to accurate and orders of magnitude cheaper quantum mechanical calculations",
    "volume": "main",
    "checked": true,
    "id": "5db6ef158cb0ca101b4d482f4f01ef94b7e7628f",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=R332S76RjxS": {
    "title": "A global convergence theory for deep ReLU implicit networks via over-parameterization",
    "abstract": "a (stochastic) gradient descent (SGD) training nonlinear implicit neural networks This paper ﬁlls the gap by analyzing the gradient ﬂow of Rectiﬁed Linear Unit (ReLU) activated implicit neural networks. For an m -width implicit neural network with ReLU activation and n training samples, we show that a randomly initialized gradient descent converges to a global minimum at a linear rate for the square loss function if the implicit neural network is over-parameterized . It is worth that, existing works on the convergence of (S)GD on ﬁnite-layer over-parameterized neural networks, our convergence results hold for neural networks, where the of layers",
    "volume": "main",
    "checked": true,
    "id": "a69fd30be8ac2afc863a4210b38839947af11c67",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=FRxhHdnxt1": {
    "title": "Amortized Tree Generation for Bottom-up Synthesis Planning and Synthesizable Molecular Design",
    "abstract": "Molecular design and synthesis planning are two critical steps in the process of molecular discovery that we propose to formulate as a single shared task of conditional synthetic pathway generation. We report an amortized approach to generate synthetic pathways as a Markov decision process conditioned on a target molecular embedding. This approach allows us to conduct synthesis planning in a bottom-up manner and design synthesizable molecules by decoding from optimized conditional codes, demonstrating the potential to solve both problems of design and synthesis simultaneously. The approach leverages neural networks to probabilistically model the synthetic trees, one reaction step at a time, according to reactivity rules encoded in a discrete action space of reaction templates. We train these networks on hundreds of thousands of artificial pathways generated from a pool of purchasable compounds and a list of expert-curated templates. We validate our method with (a) the recovery of molecules using conditional generation, (b) the identification of synthesizable structural analogs, and (c) the optimization of molecular structures given oracle functions relevant to drug discovery",
    "volume": "main",
    "checked": true,
    "id": "59186f8ca5ffc029c365bfe4fe6386f753d21694",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=iMSjopcOn0p": {
    "title": "MT3: Multi-Task Multitrack Music Transcription",
    "abstract": "Automatic Music Transcription (AMT), inferring musical notes from raw audio, is a challenging task at the core of music understanding. Unlike Automatic Speech Recognition (ASR), which typically focuses on the words of a single speaker, AMT often requires transcribing multiple instruments simultaneously, all while preserving ﬁne-scale pitch and timing information. Further, many AMT datasets are \"low-resource\", as even expert musicians ﬁnd music transcription difﬁcult and time-consuming. Thus, prior work has focused on task-speciﬁc architectures, tai-lored to the individual instruments of each task. In this work, motivated by the promising results of sequence-to-sequence transfer learning for low-resource Nat-ural Language Processing (NLP), we demonstrate that a general-purpose Transformer model can perform multi-task AMT, jointly transcribing arbitrary combinations of musical instruments across several transcription datasets. We show this uniﬁed training framework achieves high-quality transcription results across a range of datasets, dramatically improving performance for low-resource instruments (such as guitar), while preserving strong performance for abundant instruments (such as piano). Finally, by expanding the scope of AMT, we expose the need for more consistent evaluation metrics and better dataset alignment, and provide a strong baseline for this new direction of multi-task AMT. 1",
    "volume": "main",
    "checked": true,
    "id": "8f4bc7e92526faeb65fabd60e5d8c86392fce414",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=o_HsiMPYh_x": {
    "title": "Leveraging unlabeled data to predict out-of-distribution performance",
    "abstract": "Real-world machine learning deployments are characterized by mismatches between the source (training) and target (test) distributions that may cause performance drops. In this work, we investigate methods for predicting the target domain accuracy using only labeled source data and unlabeled target data. We propose Average Thresholded Confidence (ATC), a practical method that learns a threshold on the model's confidence, predicting accuracy as the fraction of unlabeled examples for which model confidence exceeds that threshold. ATC outperforms previous methods across several model architectures, types of distribution shifts (e.g., due to synthetic corruptions, dataset reproduction, or novel subpopulations), and datasets (WILDS, ImageNet, BREEDS, CIFAR, and MNIST). In our experiments, ATC estimates target performance 2–4ˆ more accurately than prior methods. We also explore the theoretical foundations of the problem, proving that, in general, identifying the accuracy is just as hard as identifying the optimal predictor and thus, the efficacy of any method rests upon (perhaps unstated) assumptions on the nature of the shift. Finally, analyzing our method on some toy distributions, we provide insights concerning when it works1",
    "volume": "main",
    "checked": true,
    "id": "042fe4e996a1133e2c4c11f70aa2654e88f31687",
    "citation_count": 21
  },
  "https://openreview.net/forum?id=dPyRNUlttBv": {
    "title": "Optimization and Adaptive Generalization of Three layer Neural Networks",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "bee817efbcd3e1e67995ea033462540bad5e66d4",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=4Muj-t_4o4": {
    "title": "Learning a subspace of policies for online adaptation in Reinforcement Learning",
    "abstract": "Deep Reinforcement Learning (RL) is mainly studied in a setting where the training and the testing environments are similar. But in many practical applications, these environments may differ. For instance, in control systems, the robot(s) on which a policy is learned might differ from the robot(s) on which a policy will run. It can be caused by different internal factors (e.g., calibration issues, system attrition, defective modules) or also by external changes (e.g., weather conditions). There is a need to develop RL methods that generalize well to variations of the training conditions. In this article, we consider the simplest yet hard to tackle generalization setting where the test environment is unknown at train time, forcing the agent to adapt to the system's new dynamics. This online adaptation process can be computationally expensive (e.g., fine-tuning) and cannot rely on meta-RL techniques since there is just a single train environment. To do so, we propose an approach where we learn a subspace of policies within the parameter space. This subspace contains an infinite number of policies that are trained to solve the training environment while having different parameter values. As a consequence, two policies in that subspace process information differently and exhibit different behaviors when facing variations of the train environment. Our experiments1 carried out over a large variety of benchmarks compare our approach with baselines, including diversity-based methods. In comparison, our approach is simple to tune, does not need any extra component (e.g., discriminator) and learns policies able to gather a high reward on unseen environments",
    "volume": "main",
    "checked": true,
    "id": "3dc6141adfdaa3c11ca672029002fa6eb11aba0d",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=wIzUeM3TAU": {
    "title": "Expressiveness and Approximation Properties of Graph Neural Networks",
    "abstract": "-tests - of of the of in",
    "volume": "main",
    "checked": true,
    "id": "307c18712c6fb9617a450cfb0c2d540d81a9bea9",
    "citation_count": 19
  },
  "https://openreview.net/forum?id=g1SzIRLQXMM": {
    "title": "Wiring Up Vision: Minimizing Supervised Synaptic Updates Needed to Produce a Primate Ventral Stream",
    "abstract": "After training on large datasets, certain deep neural networks are surprisingly good models of the neural mechanisms of adult primate visual object recognition. Nevertheless, these models are poor models of the development of the visual system because they posit millions of sequential, precisely coordinated synaptic updates, each based on a labeled image. While ongoing research is pursuing the use of unsupervised proxies for labels, we here explore a complementary strategy of reducing the required number of supervised synaptic updates to produce an adult-like ventral visual stream (as judged by the match to V1, V2, V4, IT, and behavior). Such models might require less precise machinery and energy expenditure to coordinate these updates and would thus move us closer to viable neuroscientific hypotheses about how the visual system wires itself up. Relative to the current leading model of the adult ventral stream, we here demonstrate that the total number of supervised weight updates can be substantially reduced using three complementary strategies: First, we find that only 2% of supervised updates (epochs and images) are needed to achieve ~80% of the match to adult ventral stream. Second, by improving the random distribution of synaptic connectivity, we find that 54% of the brain match can already be achieved \"at birth\" (i.e. no training at all). Third, we find that, by training only ~5% of model synapses, we can still achieve nearly 80% of the match to the ventral stream. When these three strategies are applied in combination, we find that these new models achieve ~80% of a fully trained model's match to the brain, while using two orders of magnitude fewer supervised synaptic updates. These results reflect first steps in modeling not just primate adult visual processing during inference, but also how the ventral visual stream might be \"wired up\" by evolution (a model's \"birth\" state) and by developmental learning (a model's updates based on visual experience)",
    "volume": "main",
    "checked": true,
    "id": "19d68db5346c837bb428160619356e36045b351c",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=ZBESeIUB5k": {
    "title": "Stochastic Training is Not Necessary for Generalization",
    "abstract": "It is widely believed that the implicit regularization of stochastic gradient descent (SGD) is fundamental to the impressive generalization behavior we observe in neural networks. In this work, we demonstrate that non-stochastic full-batch training can achieve strong performance on CIFAR-10 that is on-par with SGD, using modern architectures in settings with and without data augmentation. To this end, we utilize modified hyperparameters and show that the implicit regularization of SGD can be completely replaced with explicit regularization. This strongly suggests that theories that rely heavily on properties of stochastic sampling to explain generalization are incomplete, as strong generalization behavior is still observed in the absence of stochastic sampling. Fundamentally, deep learning can succeed without stochasticity. Our observations further indicate that the perceived difficulty of full-batch training is largely the result of its optimization properties and the disproportionate time and effort spent by the ML community tuning optimizers and hyperparameters for small-batch training",
    "volume": "main",
    "checked": true,
    "id": "60f9fa6d4f966c316353ee2753021c4587fa0273",
    "citation_count": 28
  },
  "https://openreview.net/forum?id=vJZ7dPIjip3": {
    "title": "Generalization of Neural Combinatorial Solvers Through the Lens of Adversarial Robustness",
    "abstract": "End-to-end (geometric) deep learning has seen first successes in approximating the solution of combinatorial optimization problems. However, generating data in the realm of NP-hard/-complete tasks brings practical and theoretical challenges, resulting in evaluation protocols that are too optimistic. Specifically, most datasets only capture a simpler subproblem and likely suffer from spurious features. We investigate these effects by studying adversarial robustness–a local generalization property–to reveal hard, model-specific instances and spurious features. For this purpose, we derive perturbation models for SAT and TSP. Unlike in other applications, where perturbation models are designed around subjective notions of imperceptibility, our perturbation models are efficient and sound, allowing us to determine the true label of perturbed samples without a solver. Surprisingly, with such perturbations, a sufficiently expressive neural solver does not suffer from the limitations of the accuracy-robustness trade-off common in supervised learning. Although such robust solvers exist, we show empirically that the assessed neural solvers do not generalize well w.r.t. small perturbations of the problem instance",
    "volume": "main",
    "checked": true,
    "id": "70864c48e643e852355f4a79e23baf3614740df6",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=So6YAqnqgMj": {
    "title": "EigenGame Unloaded: When playing games is better than optimizing",
    "abstract": "We build on the recently proposed EigenGame that views eigendecomposition as a competitive game. EigenGame's updates are biased if computed using minibatches of data, which hinders convergence and more sophisticated parallelism in the stochastic setting. In this work, we propose an unbiased stochastic update that is asymptotically equivalent to EigenGame, enjoys greater parallelism allowing computation on datasets of larger sample sizes, and outperforms EigenGame in experiments. We present applications to finding the principal components of massive datasets and performing spectral clustering of graphs. We analyze and discuss our proposed update in the context of EigenGame and the shift in perspective from optimization to games",
    "volume": "main",
    "checked": true,
    "id": "f9464bda6ca2e1ae20a335163b0172a3fcf599f8",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=9L1BsI4wP1H": {
    "title": "Adversarially Robust Conformal Prediction",
    "abstract": "Conformal prediction is a model-agnostic tool for constructing prediction sets that are valid under the common i.i.d. assumption, which has been applied to quan-tify the prediction uncertainty of deep net classiﬁers. In this paper, we generalize this framework to the case where adversaries exist during inference time, under which the i.i.d. assumption is grossly violated. By combining conformal prediction with randomized smoothing, our proposed method forms a prediction set with ﬁnite-sample coverage guarantee that holds for any data distribution with (cid:96) 2 norm bounded adversarial noise, generated by any adversarial attack algorithm. The core idea is to bound the Lipschitz constant of the non-conformity score by smoothing it with Gaussian noise and leverage this knowledge to account for the effect of the unknown adversarial perturbation. We demonstrate the necessity of our method in the adversarial setting and the validity of our theoretical guarantee on three widely used benchmark data sets: CIFAR10, CIFAR100, and ImageNet",
    "volume": "main",
    "checked": true,
    "id": "acc8f7cc17ce5009bd2504572e8b9b76148e63a7",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=swrMQttr6wN": {
    "title": "Learning to Map for Active Semantic Goal Navigation",
    "abstract": "We consider the problem of object goal navigation in unseen environments. In our view, solving this problem requires learning of contextual semantic priors, a challenging endeavour given the spatial and semantic variability of indoor environments. Current methods learn to implicitly encode these priors through goal-oriented navigation policy functions operating on spatial representations that are limited to the agent's observable areas. In this work, we propose a novel framework that actively learns to generate semantic maps outside the field of view of the agent and leverages the uncertainty over the semantic classes in the unobserved areas to decide on long term goals. We demonstrate that through this spatial prediction strategy, we are able to learn semantic priors in scenes that can be leveraged in unknown environments. Additionally, we show how different objectives can be defined by balancing exploration with exploitation during searching for semantic targets. Our method is validated in the visually realistic environments offered by the Matterport3D dataset and show state of the art results on the object goal navigation task",
    "volume": "main",
    "checked": true,
    "id": "cdfe59dd102d8f8e780c893f3438eaf7723aecea",
    "citation_count": 15
  },
  "https://openreview.net/forum?id=jaLDP8Hp_gc": {
    "title": "Visual Correspondence Hallucination",
    "abstract": "Given a pair of partially overlapping source and target images and a keypoint in the source image, the keypoint's correspondent in the target image can be either visible, occluded or outside the field of view. Local feature matching methods are only able to identify the correspondent's location when it is visible, while humans can also hallucinate its location when it is occluded or outside the field of view through geometric reasoning. In this paper, we bridge this gap by training a network to output a peaked probability distribution over the correspondent's location, regardless of this correspondent being visible, occluded, or outside the field of view. We experimentally demonstrate that this network is indeed able to hallucinate correspondences on unseen pairs of images. We also apply this network to a camera pose estimation problem and find it is significantly more robust than state-of-the-art local feature matching-based competitors. Code will be available at hugogermain.com/neurhal",
    "volume": "main",
    "checked": true,
    "id": "8c0d7220e294ee532fac1f993370c3ea8d3c319f",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=qyTBxTztIpQ": {
    "title": "CrowdPlay: Crowdsourcing Human Demonstrations for Offline Learning",
    "abstract": "Crowdsourcing has been instrumental for driving AI advances that rely on large-scale data. At the same time, reinforcement learning has seen rapid progress through benchmark environments that strike a balance between tractability and real-world complexity, such as ALE and OpenAI Gym. In this paper, we aim to ﬁll a gap at the intersection of these two: The use of crowdsourcing to generate large-scale human demonstration data in the support of advancing research into imitation learning and ofﬂine learning. To this end, we present CrowdPlay , a complete crowdsourcing pipeline for any standard RL environment including OpenAI Gym (made available under an open-source license); a large-scale publicly available crowdsourced dataset of human gameplay demonstrations in Atari 2600 games, including multimodal behavior and human-human and human-AI multiagent data; ofﬂine learning benchmarks with extensive human data evaluation; and a detailed study of incentives, including real-time feedback to drive high quality data. We hope that this will drive the improvement in design of algorithms that account for the complexity of human, behavioral data and thereby enable a step forward in direction of effective learning for real-world settings. Our code and dataset are available at https://mgerstgrasser.github.io/crowdplay/ ",
    "volume": "main",
    "checked": true,
    "id": "b161d47256418fb262a5d11b64c9626212df32d4",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=DNRADop4ksB": {
    "title": "On the Importance of Firth Bias Reduction in Few-Shot Classification",
    "abstract": "Learning accurate classiﬁers for novel categories from very few examples, known as few-shot image classiﬁcation, is a challenging task in statistical machine learning and computer vision. The performance in few-shot classiﬁcation suffers from the bias in the estimation of classiﬁer parameters; however, an effective underlying bias reduction technique that could alleviate this issue in training few-shot classiﬁers has been overlooked. In this work, we demonstrate the effectiveness of Firth bias reduction in few-shot classiﬁcation. Theoretically, Firth bias reduction removes the O ( N − 1 ) ﬁrst order term from the small-sample bias of the Maximum Likelihood Estimator. Here we show that the general Firth bias reduction technique simpliﬁes to encouraging uniform class assignment probabilities for multinomial logistic classiﬁcation, and almost has the same effect in cosine classiﬁers. We derive an easy-to-implement optimization objective for Firth penalized multinomial logistic and cosine classiﬁers, which is equivalent to penalizing the cross-entropy loss with a KL-divergence between the uniform label distribution and the predictions. Then, we empirically evaluate that it is consistently effective across the board for few-shot image classiﬁcation, regardless of (1) the feature representations from different backbones, (2) the number of samples per class, and (3) the number of classes. Finally, we show the robustness of Firth bias reduction, in the case of imbalanced data distribution. Our implementation is available at https://github.com/ehsansaleh/ﬁrth_bias_reduction",
    "volume": "main",
    "checked": true,
    "id": "1729f308d7874f7a51a19571d5c96c3cc1a53d4d",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=qY79G8jGsep": {
    "title": "DISSECT: Disentangled Simultaneous Explanations via Concept Traversals",
    "abstract": "Explaining deep learning model inferences is a promising venue for scientific understanding, improving safety, uncovering hidden biases, evaluating fairness, and beyond, as argued by many scholars. One of the principal benefits of counterfactual explanations is allowing users to explore \"what-if\" scenarios through what does not and cannot exist in the data, a quality that many other forms of explanation such as heatmaps and influence functions are inherently incapable of doing. However, most previous work on generative explainability cannot disentangle important concepts effectively, produces unrealistic examples, or fails to retain relevant information. We propose a novel approach, DISSECT, that jointly trains a generator, a discriminator, and a concept disentangler to overcome such challenges using little supervision. DISSECT generates Concept Traversals (CTs), defined as a sequence of generated examples with increasing degrees of concepts that influence a classifier's decision. By training a generative model from a classifier's signal, DISSECT offers a way to discover a classifier's inherent \"notion\" of distinct concepts automatically rather than rely on user-predefined concepts. We show that DISSECT produces CTs that (1) disentangle several concepts, (2) are influential to a classifier's decision and are coupled to its reasoning due to joint training (3), are realistic, (4) preserve relevant information, and (5) are stable across similar inputs. We validate DISSECT on several challenging synthetic and realistic datasets where previous methods fall short of satisfying desirable criteria for interpretability and show that it performs consistently well. Finally, we present experiments showing applications of DISSECT for detecting potential biases of a classifier and identifying spurious artifacts that impact predictions",
    "volume": "main",
    "checked": true,
    "id": "28fc865105bf91c70d13e7e19effc53da9d247b1",
    "citation_count": 14
  },
  "https://openreview.net/forum?id=hR_SMu8cxCV": {
    "title": "Scaling Laws for Neural Machine Translation",
    "abstract": "We present an empirical study of scaling properties of encoder-decoder Transformer models used in neural machine translation (NMT). We show that cross-entropy loss as a function of model size follows a certain scaling law. Specifically (i) We propose a formula which describes the scaling behavior of cross-entropy loss as a bivariate function of encoder and decoder size, and show that it gives accurate predictions under a variety of scaling approaches and languages; we show that the total number of parameters alone is not sufficient for such purposes. (ii) We observe different power law exponents when scaling the decoder vs scaling the encoder, and provide recommendations for optimal allocation of encoder/decoder capacity based on this observation. (iii) We also report that the scaling behavior of the model is acutely influenced by composition bias of the train/test sets, which we define as any deviation from naturally generated text (either via machine generated or human translated text). We observe that natural text on the target side enjoys scaling, which manifests as successful reduction of the cross-entropy loss. (iv) Finally, we investigate the relationship between the cross-entropy loss and the quality of the generated translations. We find two different behaviors, depending on the nature of the test data. For test sets which were originally translated from target language to source language, both loss and BLEU score improve as model size increases. In contrast, for test sets originally translated from source language to target language, the loss improves, but the BLEU score stops improving after a certain threshold. We release generated text from all models used in this study",
    "volume": "main",
    "checked": true,
    "id": "de1fdaf92488f2f33ddc0272628c8543778d0da9",
    "citation_count": 20
  },
  "https://openreview.net/forum?id=Lwr8We4MIxn": {
    "title": "A Biologically Interpretable Graph Convolutional Network to Link Genetic Risk Pathways and Imaging Phenotypes of Disease",
    "abstract": "We propose a novel deep neural network for whole-genome imaging-genetics. Our genetics module uses hierarchical graph convolution and pooling operations that mimic the organization of a well-established gene ontology to embed subject-level data into a latent space. The ontology implicitly tracks the convergence of genetic risk across biological pathways, and an attention mechanism automatically identiﬁes the salient edges in our network. We couple the imaging and genetics data using an autoencoder and predictor, which couples the latent embeddings learned for each modality. The predictor uses these embeddings for disease diagnosis, while the decoder regularizes the model. For interpretability, we implement a Bayesian feature selection strategy to extract the discriminative biomarkers of each modality. We evaluate our framework on a population study of schizophrenia that includes two functional MRI (fMRI) paradigms and gene scores derived from Single Nucleotide Polymorphism (SNP) data. Using 10-fold cross-validation, we show that our model achieves better classiﬁcation performance than the baselines. In an exploratory analysis, we further show that the biomarkers identiﬁed by our model are reproducible and closely associated with deﬁcits in schizophrenia",
    "volume": "main",
    "checked": true,
    "id": "72300756d1faa739234ca85113fc948798f1614e",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=EQmAP4F859": {
    "title": "The Three Stages of Learning Dynamics in High-dimensional Kernel Methods",
    "abstract": "To understand how deep learning works, it is crucial to understand the training dynamics of neural networks. Several interesting hypotheses about these dynamics have been made based on empirically observed phenomena, but there exists a limited theoretical understanding of when and why such phenomena occur. In this paper, we consider the training dynamics of gradient flow on kernel least-squares objectives, which is a limiting dynamics of SGD trained neural networks. Using precise highdimensional asymptotics, we characterize the dynamics of the fitted model in two \"worlds\": in the Oracle World the model is trained on the population distribution and in the Empirical World the model is trained on a sampled dataset. We show that under mild conditions on the kernel and L target regression function the training dynamics undergo three stages characterized by the behaviors of the models in the two worlds. Our theoretical results also mathematically formalize some interesting deep learning phenomena. Specifically, in our setting we show that SGD progressively learns more complex functions and that there is a \"deep bootstrap\" phenomenon: during the second stage, the test error of both worlds remain close despite the empirical training error being much smaller. Finally, we give a concrete example comparing the dynamics of two different kernels which shows that faster training is not necessary for better generalization",
    "volume": "main",
    "checked": true,
    "id": "d584236d863de4dbed78c8428f5d29de22def819",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=vA7doMdgi75": {
    "title": "Implicit Bias of Projected Subgradient Method Gives Provable Robust Recovery of Subspaces of Unknown Codimension",
    "abstract": "Robust subspace recovery (RSR) is a fundamental problem in robust representation learning. Here we focus on a recently proposed RSR method termed Dual Principal Component Pursuit (DPCP) approach, which aims to recover a basis of the orthogonal complement of the subspace and is amenable to handling subspaces of high relative dimension. Prior work has shown that DPCP can provably recover the correct subspace in the presence of outliers, as long as the true dimension of the subspace is known. We show that DPCP can provably solve RSR problems in the unknown subspace dimension regime, as long as orthogonality constraints -adopted in previous DPCP formulationsare relaxed and random initialization is used instead of spectral one. Namely, we propose a very simple algorithm based on running multiple instances of a projected sub-gradient descent method (PSGM), with each problem instance seeking to find one vector in the null space of the subspace. We theoretically prove that under mild conditions this approach will succeed with high probability. In particular, we show that 1) all of the problem instances will converge to a vector in the nullspace of the subspace and 2) the ensemble of problem instance solutions will be sufficiently diverse to fully span the nullspace of the subspace thus also revealing its true unknown codimension. We provide empirical results that corroborate our theoretical results and showcase the remarkable implicit rank regularization behavior of PSGM algorithm that allows us to perform RSR without being aware of the subspace dimension",
    "volume": "main",
    "checked": true,
    "id": "f1057950e69b736c35a4fe9c013083d6118811ed",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=_Wzj0J2xs2D": {
    "title": "Curvature-Guided Dynamic Scale Networks for Multi-View Stereo",
    "abstract": "Multi-view stereo (MVS) is a crucial task for precise 3D reconstruction. Most recent studies tried to improve the performance of matching cost volume in MVS by designing aggregated 3D cost volumes and their regularization. This paper focuses on learning a robust feature extraction network to enhance the performance of matching costs without heavy computation in the other steps. In particular, we present a dynamic scale feature extraction network, namely, CDSFNet. It is composed of multiple novel convolution layers, each of which can select a proper patch scale for each pixel guided by the normal curvature of the image surface. As a result, CDFSNet can estimate the optimal patch scales to learn discriminative features for accurate matching computation between reference and source images. By combining the robust extracted features with an appropriate cost formulation strategy, our resulting MVS architecture can estimate depth maps more precisely. Extensive experiments showed that the proposed method outperforms other methods on complex outdoor scenes. It significantly improves the completeness of reconstructed models. As a result, the method can process higher resolution inputs within faster run-time and lower memory than other MVS methods",
    "volume": "main",
    "checked": true,
    "id": "60896adc1336150d63e896a24a3cf150092c15f3",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=QDdJhACYrlX": {
    "title": "THOMAS: Trajectory Heatmap Output with learned Multi-Agent Sampling",
    "abstract": "In this paper, we propose THOMAS, a joint multi-agent trajectory prediction framework allowing for efficient and consistent prediction of multi-agent multimodal trajectories. We present a unified model architecture for fast and simultaneous agent future heatmap estimation leveraging hierarchical and sparse image generation. We demonstrate that heatmap output enables a higher level of control on the predicted trajectories compared to vanilla multi-modal trajectory regression, allowing to incorporate additional constraints for tighter sampling or collisionfree predictions in a deterministic way. However, we also highlight that generating scene-consistent predictions goes beyond the mere generation of collision-free trajectories. We therefore propose a learnable trajectory recombination model that takes as input a set of predicted trajectories for each agent and outputs its consistent reordered recombination. We report our results on the Interaction multi-agent prediction challenge and rank 1 on the online test leaderboard",
    "volume": "main",
    "checked": true,
    "id": "9154d56389f9d5517fd721b456f9f07caac06ae9",
    "citation_count": 16
  },
  "https://openreview.net/forum?id=OcKMT-36vUs": {
    "title": "A Loss Curvature Perspective on Training Instabilities of Deep Learning Models",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "f6e05a4269a60d5807d99872296be34e4e25ea17",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=Dup_dDqkZC5": {
    "title": "Latent Variable Sequential Set Transformers for Joint Multi-Agent Motion Prediction",
    "abstract": "Robust multi-agent trajectory prediction is essential for the safe control of robotic systems. A major challenge is to efficiently learn a representation that approximates the true joint distribution of contextual, social, and temporal information to enable planning. We propose Latent Variable Sequential Set Transformers which are encoder-decoder architectures that generate scene-consistent multi-agent trajectories. We refer to these architectures as \"AutoBots\". The encoder is a stack of interleaved temporal and social multi-head self-attention (MHSA) modules which alternately perform equivariant processing across the temporal and social dimensions. The decoder employs learnable seed parameters in combination with temporal and social MHSA modules allowing it to perform inference over the entire future scene in a single forward pass efficiently. AutoBots can produce either the trajectory of one ego-agent or a distribution over the future trajectories for all agents in the scene. For the single-agent prediction case, our model achieves top results on the global nuScenes vehicle motion prediction leaderboard, and produces strong results on the Argoverse vehicle prediction challenge. In the multi-agent setting, we evaluate on the synthetic partition of TrajNet++ dataset to showcase the model's socially-consistent predictions. We also demonstrate our model on general sequences of sets and provide illustrative experiments modelling the sequential structure of the multiple strokes that make up symbols in the Omniglot data. A distinguishing feature of AutoBots is that all models are trainable on a single desktop GPU (1080 Ti) in under 48h",
    "volume": "main",
    "checked": true,
    "id": "6c1bb8b017c469208bd7e3a80639bdb5f1726e2c",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=kZ0UYdhqkNY": {
    "title": "Variational methods for simulation-based inference",
    "abstract": "We present Sequential Neural Variational Inference (SNVI), an approach to perform Bayesian inference in models with intractable likelihoods. SNVI combines likelihood-estimation (or likelihood-ratio-estimation) with variational inference to achieve a scalable simulation-based inference approach. SNVI maintains the ﬂex-ibility of likelihood(-ratio) estimation to allow arbitrary proposals for simulations, while simultaneously providing a functional estimate of the posterior distribution without requiring MCMC sampling. We present several variants of SNVI and demonstrate that they are substantially more computationally efﬁcient than previous algorithms, without loss of accuracy on benchmark tasks. We apply SNVI to a neuroscience model of the pyloric network in the crab and demonstrate that it can infer the posterior distribution with one order of magnitude fewer simulations than previously reported. SNVI vastly reduces the computational cost of simulation-based inference while maintaining accuracy and ﬂexibility, making it possible to tackle problems that were previously inaccessible",
    "volume": "main",
    "checked": true,
    "id": "401782ba0dacfe3cc1c84d0d6054fa1ab416248c",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=1wVvweK3oIb": {
    "title": "Simple GNN Regularisation for 3D Molecular Property Prediction and Beyond",
    "abstract": "In this paper we show that simple noisy regularisation can be an effective way to address GNN oversmoothing. First we argue that regularisers addressing oversmoothing should both penalise node latent similarity and encourage meaningful node representations. From this observation we derive \"Noisy Nodes\", a simple technique in which we corrupt the input graph with noise, and add a noise correcting node-level loss. The diverse node level loss encourages latent node diversity, and the denoising objective encourages graph manifold learning. Our regulariser applies well-studied methods in simple, straightforward ways which allow even generic architectures to overcome oversmoothing and achieve state of the art results on quantum chemistry tasks, and improve results significantly on Open Graph Benchmark (OGB) datasets. Our results suggest Noisy Nodes can serve as a complementary building block in the GNN toolkit",
    "volume": "main",
    "checked": true,
    "id": "504132a027a91391f0c2206133bbacb36b43d053",
    "citation_count": 27
  },
  "https://openreview.net/forum?id=mfwdY3U_9ea": {
    "title": "Igeood: An Information Geometry Approach to Out-of-Distribution Detection",
    "abstract": "Reliable out-of-distribution (OOD) detection is fundamental to implementing safer modern machine learning (ML) systems. In this paper, we introduce IGEOOD, an effective method for detecting OOD samples. IGEOOD applies to any pre-trained neural network, works under various degrees of access to the ML model, does not require OOD samples or assumptions on the OOD data but can also benefit (if available) from OOD samples. By building on the geodesic (FisherRao) distance between the underlying data distributions, our discriminator can combine confidence scores from the logits outputs and the learned features of a deep neural network. Empirically, we show that IGEOOD outperforms competing state-of-the-art methods on a variety of network architectures and datasets",
    "volume": "main",
    "checked": true,
    "id": "2815a5e7ba661ae278aa7c19e08ac884cde17bf7",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=Qaw16njk6L": {
    "title": "NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "970ea9d2e69c0ba9f104a038ce15305cbd75a4b4",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=KPEFXR1HdIo": {
    "title": "Fine-grained Differentiable Physics: A Yarn-level Model for Fabrics",
    "abstract": "Differentiable physics modeling combines physics models with gradient-based learning to provide model explicability and data efficiency. It has been used to learn dynamics, solve inverse problems and facilitate design, and is at its inception of impact. Current successes have concentrated on general physics models such as rigid bodies, deformable sheets, etc, assuming relatively simple structures and forces. Their granularity is intrinsically coarse and therefore incapable of modelling complex physical phenomena. Fine-grained models are still to be developed to incorporate sophisticated material structures and force interactions with gradient-based learning. Following this motivation, we propose a new differentiable fabrics model for composite materials such as cloths, where we dive into the granularity of yarns and model individual yarn physics and yarn-to-yarn interactions. To this end, we propose several differentiable forces, whose counterparts in empirical physics are indifferentiable, to facilitate gradient-based learning. These forces, albeit applied to cloths, are ubiquitous in various physical systems. Through comprehensive evaluation and comparison, we demonstrate our model's explicability in learning meaningful physical parameters, versatility in incorporating complex physical structures and heterogeneous materials, data-efficiency in learning, and high-fidelity in capturing subtle dynamics. Code is available in: https://github.com/realcrane/Fine-grained-Differentiable-P hysics-A-Yarn-level-Model-for-Fabrics.git",
    "volume": "main",
    "checked": true,
    "id": "8f32df9aa05cd0b1cd5c1b95508e85003f0e8bd6",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=6PvWo1kEvlT": {
    "title": "Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis--Hastings",
    "abstract": "While recent work has shown that scores from models trained by the ubiquitous masked language modeling (MLM) objective effectively discriminate probable and improbable sequences, it is still an open question if these MLMs specify a principled probability distribution over the space of possible sequences. In this paper, we interpret MLMs as energy-based sequence models and propose two energy parametrizations derivable from the trained MLMs. In order to draw samples correctly from these models, we develop a tractable sampling scheme based on the Metropolis–Hastings Monte Carlo algorithm. In our approach, samples are proposed from the same masked conditionals used for training the masked language models, and they are accepted or rejected based on their energy values according to the target distribution. We validate the effectiveness of the proposed parametrizations by exploring the quality of samples drawn from these energybased models on the conditional generation task of machine translation. We theoretically and empirically justify our sampling algorithm by showing that the masked conditionals on their own do not yield a Markov chain whose stationary distribution is that of our target distribution, and our approach generates higher quality samples than other recently proposed undirected generation approaches [29, 12]",
    "volume": "main",
    "checked": true,
    "id": "f71d0c4cae964a71c8a2f2c0919075d3bb18ca63",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=XzTtHjgPDsT": {
    "title": "Coordination Among Neural Modules Through a Shared Global Workspace",
    "abstract": "Deep learning has seen a movement away from representing examples with a monolithic hidden state towards a richly structured state. For example, Transformers segment by position, and object-centric architectures decompose images into entities. In all these architectures, interactions between different elements are modeled via pairwise interactions: Transformers make use of self-attention to incorporate information from other positions and object-centric architectures make use of graph neural networks to model interactions among entities. We consider how to improve on pairwise interactions in terms of global coordination and a coherent, integrated representation that can be used for downstream tasks. In cognitive science, a global workspace architecture has been proposed in which functionally specialized components share information through a common, bandwidth-limited communication channel. We explore the use of such a communication channel in the context of deep learning for modeling the structure of complex environments. The proposed method includes a shared workspace through which communication among different specialist modules takes place but due to limits on the communication bandwidth, specialist modules must compete for access. We show that capacity limitations have a rational basis in that (1) they encourage specialization and compositionality and (2) they facilitate the synchronization of otherwise independent specialists",
    "volume": "main",
    "checked": true,
    "id": "78ea232dbabc67ca4d6d4a7c1bbf568e9b47cb8a",
    "citation_count": 37
  },
  "https://openreview.net/forum?id=4Ycr8oeCoIh": {
    "title": "When, Why, and Which Pretrained GANs Are Useful?",
    "abstract": "The literature has proposed several methods to finetune pretrained GANs on new datasets, which typically results in higher performance compared to training from scratch, especially in the limited-data regime. However, despite the apparent empirical benefits of GAN pretraining, its inner mechanisms were not analyzed indepth, and understanding of its role is not entirely clear. Moreover, the essential practical details, e.g., selecting a proper pretrained GAN checkpoint, currently do not have rigorous grounding and are typically determined by trial and error. This work aims to dissect the process of GAN finetuning. First, we show that initializing the GAN training process by a pretrained checkpoint primarily affects the model's coverage rather than the fidelity of individual samples. Second, we explicitly describe how pretrained generators and discriminators contribute to the finetuning process and explain the previous evidence on the importance of pretraining both of them. Finally, as an immediate practical benefit of our analysis, we describe a simple recipe to choose an appropriate GAN checkpoint that is the most suitable for finetuning to a particular target task. Importantly, for most of the target tasks, Imagenet-pretrained GAN, despite having poor visual quality, appears to be an excellent starting point for finetuning, resembling the typical pretraining scenario of discriminative computer vision models",
    "volume": "main",
    "checked": true,
    "id": "6d8601eddd073284ea56ecee042be0aa87823643",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=EDeVYpT42oS": {
    "title": "Deconstructing the Inductive Biases of Hamiltonian Neural Networks",
    "abstract": "Physics-inspired neural networks (NNs), such as Hamiltonian or Lagrangian NNs, dramatically outperform other learned dynamics models by leveraging strong inductive biases. These models, however, are challenging to apply to many real world systems, such as those that don't conserve energy or contain contacts, a common setting for robotics and reinforcement learning. In this paper, we examine the inductive biases that make physics-inspired models successful in practice. We show that, contrary to conventional wisdom, the improved generalization of HNNs is the result of modeling acceleration directly and avoiding artificial complexity from the coordinate system, rather than symplectic structure or energy conservation. We show that by relaxing the inductive biases of these models, we can match or exceed performance on energy-conserving systems while dramatically improving performance on practical, non-conservative systems. We extend this approach to constructing transition models for common Mujoco environments, showing that our model can appropriately balance inductive biases with the flexibility required for model-based control",
    "volume": "main",
    "checked": true,
    "id": "3f8dae850dfc1163990f9b513164b42908515a08",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=uYLFoz1vlAC": {
    "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
    "abstract": "A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of 10000 or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) x (cid:48) ( t ) = Ax ( t ) + Bu ( t ) , y ( t ) = Cx ( t ) + Du ( t ), and showed that for appropriate choices of the state matrix A , this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more eﬃciently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning A with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation 60 × faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as eﬃcient as all competitors. 1",
    "volume": "main",
    "checked": true,
    "id": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51",
    "citation_count": 47
  },
  "https://openreview.net/forum?id=iUuzzTMUw9K": {
    "title": "StyleNeRF: A Style-based 3D Aware Generator for High-resolution Image Synthesis",
    "abstract": "We propose StyleNeRF , a 3D-aware generative model for photo-realistic high-resolution image synthesis with high multi-view consistency, which can be trained on unstructured 2D images. Existing approaches either cannot synthesize high-resolution images with ﬁne details or yield noticeable 3D-inconsistent artifacts. In addition, many of them lack control over style attributes and explicit 3D camera poses. StyleNeRF integrates the neural radiance ﬁeld (NeRF) into a style-based generator to tackle the aforementioned challenges, i.e., improving rendering efﬁciency and 3D consistency for high-resolution image generation. We perform volume rendering only to produce a low-resolution feature map and progressively apply upsampling in 2D to address the ﬁrst issue. To mitigate the inconsistencies caused by 2D upsampling, we propose multiple designs, including a better upsampler and a new regularization loss. With these designs, StyleNeRF can synthesize high-resolution images at interactive rates while preserving 3D consistency at high quality. StyleNeRF also enables control of camera poses and different levels of styles, which can generalize to unseen views. It also supports challenging tasks, including style mixing and semantic editing. Code and pre-trained models are available at: https://github.com/facebookresearch/StyleNeRF",
    "volume": "main",
    "checked": true,
    "id": "3fdda879abf2462b09139fb1fd1c2c147c9a0ef0",
    "citation_count": 101
  },
  "https://openreview.net/forum?id=lL3lnMbR4WU": {
    "title": "Open-vocabulary Object Detection via Vision and Language Knowledge Distillation",
    "abstract": "We aim at advancing open-vocabulary object detection, which detects objects described by arbitrary text inputs. The fundamental challenge is the availability of training data. Existing object detection datasets only contain hundreds of categories, and it is costly to scale further. To overcome this challenge, we propose ViLD, a training method via Vision and Language knowledge Distillation. Our method distills the knowledge from a pretrained open-vocabulary image classification model (teacher) into a two-stage detector (student). Specifically, we use the teacher model to encode category texts and image regions of object proposals. Then we train a student detector, whose region embeddings of detected boxes are aligned with the text and image embeddings inferred by the teacher. We benchmark on LVIS by holding out all rare categories as novel categories not seen during training. ViLD obtains 16.1 mask APr, even outperforming the supervised counterpart by 3.8 with a ResNet-50 backbone. The model can directly transfer to other datasets without finetuning, achieving 72.2 AP50, 36.6 AP and 11.8 AP on PASCAL VOC, COCO and Objects365, respectively. On COCO, ViLD outperforms previous SOTA (Zareian et al., 2021) by 4.8 on novel AP and 11.4 on overall AP",
    "volume": "main",
    "checked": true,
    "id": "cf9b8da26d9b92e75ba49616ed2a1033f59fce14",
    "citation_count": 76
  },
  "https://openreview.net/forum?id=vHVcB-ak3Si": {
    "title": "Dive Deeper Into Integral Pose Regression",
    "abstract": "Integral pose regression combines an implicit heatmap with end-to-end training for human body and hand pose estimation. Unlike detection-based heatmap methods, which decode final joint positions from the heatmap with a non-differentiable argmax operation, integral regression methods apply a differentiable expectation operation. This paper offers a deep dive into the inference and back-propagation of integral pose regression to better understand the differences in performance and training compared to detection-based methods. For inference, we give theoretical support as to why expectation should always be better than the argmax operation, i.e. integral regression should always outperform detection. Yet, in practice, this is observed only in hard cases because the heatmap activation for regression shrinks in easy cases. We then experimentally show that activation shrinkage is one of the leading causes for integral regression's inferior performance. For backpropagation, we theoretically and empirically analyze the gradients to explain the slow training speed of integral regression. Based on these findings, we incorporate the supervision of a spatial prior to speed up training and improve performance",
    "volume": "main",
    "checked": true,
    "id": "4b9a056604982ce80ecff6a59a8f5f2b5592012c",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=18Ys0-PzyPI": {
    "title": "Online Ad Hoc Teamwork under Partial Observability",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "967d246f296166eaad859520eb3bf918fbc3a275",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=A3HHaEdqAJL": {
    "title": "Task Relatedness-Based Generalization Bounds for Meta Learning",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "2b70bb9d21d09199362bd98fced966481caf4646",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=7QfLW-XZTl": {
    "title": "Energy-Inspired Molecular Conformation Optimization",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "a46b9fe9d83f21b18cf18b350be6484828f7f3e8",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=-HSOjDPfhBJ": {
    "title": "PER-ETD: A Polynomially Efficient Emphatic Temporal Difference Learning Method",
    "abstract": "Emphatic temporal diﬀerence (ETD) learning [26] is a successful method to conduct the oﬀ-policy value function evaluation with function approximation. Although ETD has been shown to converge asymptotically to a desirable value function, it is well-known that ETD often encounters a large variance so that its sample complexity can increase exponentially fast with the number of iterations. In this work, we propose a new ETD method, called PER-ETD (i.e., PE riodically R estarted-ETD), which restarts and updates the follow-on trace only for a ﬁnite period for each iteration of the evaluation parameter. Further, PER-ETD features a design of the logarithmical increase of the restart period with the number of iterations, which guarantees the best trade-oﬀ between the variance and bias and keeps both vanishing sublinearly. We show that PER-ETD converges to the same desirable ﬁxed point as ETD, but improves the exponential sample complexity of ETD to be polynomials. Our experiments validate the superior performance of PER-ETD and its advantage over ETD",
    "volume": "main",
    "checked": true,
    "id": "916bce602655cf89ac834d646beb3091bcccd0fd",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=EXHG-A3jlM": {
    "title": "Efficient Token Mixing for Transformers via Adaptive Fourier Neural Operators",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "32e0057c9a06d23182ff41553c9df1c9a8c4b757",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=RhB1AdoFfGE": {
    "title": "Sample and Computation Redistribution for Efficient Face Detection",
    "abstract": "Although tremendous strides have been made in uncontrolled face detection, efficient face detection with a low computation cost as well as high precision remains an open challenge. In this paper, we point out that training data sampling and computation distribution strategies are the keys to efficient and accurate face detection. Motivated by these observations, we introduce two simple but effective methods (1) Sample Redistribution (SR), which augments training samples for the most needed stages, based on the statistics of benchmark datasets; and (2) Computation Redistribution (CR), which reallocates the computation between the backbone, neck and head of the model, based on a meticulously defined search methodology. Extensive experiments conducted on WIDER FACE demonstrate the state-of-the-art efficiency-accuracy trade-off for the proposed SCRFD family across a wide range of compute regimes. In particular, SCRFD-34GF outperforms the best competitor, TinaFace, by 3.86% (AP at hard set) while being more than 3× faster on GPUs with VGA-resolution images. We also release our code to facilitate future research. https://github.com/deepinsight/ insightface/tree/master/detection/scrfd",
    "volume": "main",
    "checked": true,
    "id": "99d171aa12175c4d5d97d91c4032757719e22ae0",
    "citation_count": 15
  },
  "https://openreview.net/forum?id=YRq0ZUnzKoZ": {
    "title": "A Relational Intervention Approach for Unsupervised Dynamics Generalization in Model-Based Reinforcement Learning",
    "abstract": "The generalization of model-based reinforcement learning (MBRL) methods to environments with unseen transition dynamics is an important yet challenging problem. Existing methods try to extract environment-speciﬁed information Z from past transition segments to make the dynamics prediction model generalizable to different dynamics. However, because environments are not labelled, the extracted information inevitably contains redundant information unrelated to the dynamics in transition segments and thus fails to maintain a crucial property of Z : Z should be similar in the same environment and dissimilar in different ones. As a result, the learned dynamics prediction function will deviate from the true one, which undermines the generalization ability. To tackle this problem, we introduce an interventional prediction module to estimate the probability of two estimated ˆ z i , ˆ z j belonging to the same environment. Furthermore, by utilizing the Z 's invariance within a single environment, a relational head is proposed to enforce the similarity between ˆ Z from the same environment. As a result, the redundant information will be reduced in ˆ Z . We empirically show that ˆ Z estimated by our method enjoy less redundant information than previous methods, and such ˆ Z can signiﬁcantly reduce dynamics prediction errors and improve the performance of model-based RL methods on zero-shot new environments with unseen dynamics. The codes of this method are available at https://github.com/CR-Gjx/RIA ",
    "volume": "main",
    "checked": true,
    "id": "955536024c5db4166e63d41406c290fcf7ade696",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=OM_lYiHXiCL": {
    "title": "AEVA: Black-box Backdoor Detection Using Adversarial Extreme Value Analysis",
    "abstract": "Deep neural networks (DNNs) are proved to be vulnerable against backdoor attacks. A backdoor is often embedded in the target DNNs through injecting a backdoor trigger into training examples, which can cause the target DNNs misclassify an input attached with the backdoor trigger. Existing backdoor detection methods often require the access to the original poisoned training data, the parameters of the target DNNs, or the predictive confidence for each given input, which are impractical in many real-world applications, e.g., on-device deployed DNNs. We address the black-box hard-label backdoor detection problem where the DNN is fully black-box and only its final output label is accessible. We approach this problem from the optimization perspective and show that the objective of backdoor detection is bounded by an adversarial objective. Further theoretical and empirical studies reveal that this adversarial objective leads to a solution with highly skewed distribution; a singularity is often observed in the adversarial map of a backdoor-infected example, which we call the adversarial singularity phenomenon. Based on this observation, we propose the adversarial extreme value analysis (AEVA) to detect backdoors in black-box neural networks. AEVA is based on an extreme value analysis of the adversarial map, computed from the monte-carlo gradient estimation. Evidenced by extensive experiments across multiple popular tasks and backdoor attacks, our approach is shown effective in detecting backdoor attacks under the black-box hard-label scenarios",
    "volume": "main",
    "checked": true,
    "id": "192b598c581ba0eabb4bbaa178eb8be6314c3943",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=WxuE_JWxjkW": {
    "title": "Expressivity of Emergent Languages is a Trade-off between Contextual Complexity and Unpredictability",
    "abstract": "Researchers are using deep learning models to explore the emergence of language in various language games, where agents interact and develop an emergent language to solve tasks. We focus on the factors that determine the expressivity of emergent languages, which reflects the amount of information about input spaces those languages are capable of encoding. We measure the expressivity of emergent languages based on the generalisation performance across different games, and demonstrate that the expressivity of emergent languages is a trade-off between the complexity and unpredictability of the context those languages emerged from. Another contribution of this work is the discovery of message type collapse, i.e. the number of unique messages is lower than that of inputs. We also show that using the contrastive loss proposed by Chen et al. (2020) can alleviate this problem",
    "volume": "main",
    "checked": true,
    "id": "2635b0d4b61ad5f2867aa2cfaac63c6aba185d43",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=H0oaWl6THa": {
    "title": "Hybrid Local SGD for Federated Learning with Heterogeneous Communications",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "0aa7cb075978ed8c24f3e2a8ddc3ccb14df3e9a5",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=q79uMSC6ZBT": {
    "title": "Learning to Complete Code with Sketches",
    "abstract": "Code completion is usually cast as a language modelling problem, i.e., continuing an input in a left-to-right fashion. However, in practice, some parts of the completion (e.g., string literals) may be very hard to predict, whereas subsequent parts directly follow from the context. To handle this, we instead consider the scenario of generating code completions with \"holes\" inserted in places where a model is uncertain. We develop GRAMMFORMER, a Transformer-based model that guides code generation by the programming language grammar, and compare it to a variety of more standard sequence models. We train the models on code completion for C# and Python given partial code context. To evaluate models, we consider both ROUGE as well as a new metric REGEXACC that measures success of generating completions matching long outputs with as few holes as possible. In our experiments, GRAMMFORMER generates 10-50% more accurate completions compared to traditional generative models and 37-50% longer sketches compared to sketch-generating baselines trained with similar techniques",
    "volume": "main",
    "checked": true,
    "id": "3f791ea6584b78fdf0ed80560d09d9496cf5a353",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=l4IHywGq6a": {
    "title": "Data-Efficient Graph Grammar Learning for Molecular Generation",
    "abstract": "The problem of molecular generation has received significant attention recently. Existing methods are typically based on deep neural networks and require training on large datasets with tens of thousands of samples. In practice, however, the size of class-specific chemical datasets is usually limited (e.g., dozens of samples) due to labor-intensive experimentation and data collection. This presents a considerable challenge for the deep learning generative models to comprehensively describe the molecular design space. Another major challenge is to generate only physically synthesizable molecules. This is a non-trivial task for neural network-based generative models since the relevant chemical knowledge can only be extracted and generalized from the limited training data. In this work, we propose a data-efficient generative model that can be learned from datasets with orders of magnitude smaller sizes than common benchmarks. At the heart of this method is a learnable graph grammar that generates molecules from a sequence of production rules. Without any human assistance, these production rules are automatically constructed from training data. Furthermore, additional chemical knowledge can be incorporated in the model by further grammar optimization. Our learned graph grammar yields state-of-the-art results on generating high-quality molecules for three monomer datasets that contain only ∼20 samples each. Our approach also achieves remarkable performance in a challenging polymer generation task with only 117 training samples and is competitive against existing methods using 81k data points. Code is available at https://github.com/gmh14/data_efficient_grammar",
    "volume": "main",
    "checked": true,
    "id": "55703c37b77eac03a4dceb991cd03281f826d98e",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=WH6u2SvlLp4": {
    "title": "Learning Prototype-oriented Set Representations for Meta-Learning",
    "abstract": "Learning from set-structured data is a fundamental problem that has recently attracted increasing attention, where a series of summary networks are introduced to deal with the set input. In fact, many meta-learning problems can be treated as set-input tasks. Most existing summary networks aim to design different architectures for the input set in order to enforce permutation invariance. However, scant attention has been paid to the common cases where different sets in a meta-distribution are closely related and share certain statistical properties. Viewing each set as a distribution over a set of global prototypes, this paper provides a novel optimal transport (OT) based way to improve existing summary networks. To learn the distribution over the global prototypes, we minimize its OT distance to the set empirical distribution over data points, providing a natural unsupervised way to improve the summary network. Since our plug-and-play framework can be applied to many meta-learning problems, we further instantiate it to the cases of few-shot classification and implicit meta generative modeling. Extensive experiments demonstrate that our framework significantly improves the existing summary networks on learning more powerful summary statistics from sets and can be successfully integrated into metric-based few-shot classification and generative modeling applications, providing a promising tool for addressing set-input and meta-learning problems. ∗guodandan@cuhk.edu.cn †tianlong_xidian@163.com ‡mzhang388@gatech.edu §mingyuan.zhou@mccombs.utexas.edu ¶Correspoding author: zhahy@cuhksz.edu.cn 1 ar X iv :2 11 0. 09 14 0v 1 [ cs .L G ] 1 8 O ct 2 02 1",
    "volume": "main",
    "checked": true,
    "id": "e2c60526703de22079706112f8509be07c5f9d27",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=Opmqtk_GvYL": {
    "title": "MetaMorph: Learning Universal Controllers with Transformers",
    "abstract": "Multiple domains like vision, natural language, and audio are witnessing tremendous progress by leveraging Transformers for large scale pre-training followed by task specific fine tuning. In contrast, in robotics we primarily train a single robot for a single task. However, modular robot systems now allow for the flexible combination of general-purpose building blocks into task optimized morphologies. However, given the exponentially large number of possible robot morphologies, training a controller for each new design is impractical. In this work, we propose MetaMorph, a Transformer based approach to learn a universal controller over a modular robot design space. MetaMorph is based on the insight that robot morphology is just another modality on which we can condition the output of a Transformer. Through extensive experiments we demonstrate that large scale pretraining on a variety of robot morphologies results in policies with combinatorial generalization capabilities, including zero shot generalization to unseen robot morphologies. We further demonstrate that our pre-trained policy can be used for sample-efficient transfer to completely new robot morphologies and tasks",
    "volume": "main",
    "checked": true,
    "id": "e97bcb1695b586fdd7b76893bcd57d493f339ca6",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=WqoBaaPHS-": {
    "title": "Top-label calibration and multiclass-to-binary reductions",
    "abstract": "A multiclass classiﬁer is said to be top-label calibrated if the reported probability for the predicted class—the top-label—is calibrated, conditioned on the top-label. This conditioning on the top-label is ab-sent in the closely related and popular notion of conﬁdence calibration, which we argue makes conﬁdence calibration diﬃcult to interpret for decision-making. We propose top-label calibration as a rectiﬁcation of conﬁdence calibration. Further, we outline a multiclass-to-binary (M2B) reduction framework that uniﬁes conﬁdence, top-label, and class-wise calibration, among others. As its name suggests, M2B works by reducing multiclass calibration to numerous binary calibration problems, each of which can be solved using simple binary calibration routines. We instantiate the M2B framework with the well-studied histogram binning (HB) binary calibrator, and prove that the overall procedure is multiclass calibrated without making any assumptions on the underlying data distribution. In an empirical evaluation with four deep net architectures on CIFAR-10 and CIFAR-100, we ﬁnd that the M2B + HB procedure achieves lower top-label and class-wise calibration error than other approaches such as temperature scaling. Code for this work is available at https://github.com/aigen/df-posthoc-calibration ",
    "volume": "main",
    "checked": true,
    "id": "2e6dbff05337265390ee33f8d684c8d44a228073",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=d2TT6gK9qZn": {
    "title": "Non-Linear Operator Approximations for Initial Value Problems",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "6d7f1e6f1313e91eacda627b968dbc959343b54e",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=iPHLcmtietq": {
    "title": "Phase Collapse in Neural Networks",
    "abstract": "Deep convolutional image classifiers progressively transform the spatial variability into a smaller number of channels, which linearly separates all classes. A fundamental challenge is to understand the role of rectifiers together with convolutional filters in this transformation. Rectifiers with biases are often interpreted as thresholding operators which improve sparsity and discrimination. This paper demonstrates that it is a different phase collapse mechanism which explains the ability to progressively eliminate spatial variability, while improving linear class separation. This is explained and shown numerically by defining a simplified complex-valued convolutional network architecture. It implements spatial convolutions with wavelet filters and uses a complex modulus to collapse phase variables. This phase collapse network reaches the classification accuracy of ResNets of similar depths, whereas its performance is considerably degraded when replacing the phase collapse with thresholding operators. This is justified by explaining how iterated phase collapses progressively improve separation of class means, as opposed to thresholding non-linearities",
    "volume": "main",
    "checked": true,
    "id": "85df0f58599676a3fe663f7601bdcc2eb79e49cc",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=Z8FzvVU6_Kj": {
    "title": "SUMNAS: Supernet with Unbiased Meta-Features for Neural Architecture Search",
    "abstract": "One-shot Neural Architecture Search (NAS) usually constructs an overparameterized network, which we call a supernet, and typically adopts sharing parameters among the sub-models to improve computational efficiency. Oneshot NAS often repeatedly samples sub-models from the supernet and trains them to optimize the shared parameters. However, this training strategy suffers from multi-model forgetting. Training a sampled sub-model overrides the previous knowledge learned by the other sub-models, resulting in an unfair performance evaluation between the sub-models. We propose Supernet with Unbiased MetaFeatures for Neural Architecture Search (SUMNAS), a supernet learning strategy based on meta-learning to tackle the knowledge forgetting issue. During the training phase, we explicitly address the multi-model forgetting problem and help the supernet learn unbiased meta-features, independent from the sampled submodels. Once training is over, sub-models can be instantly compared to get the overall ranking or the best sub-model. Our evaluation on the NAS-Bench-201 and MobileNet-based search space demonstrate that SUMNAS shows improved ranking ability and finds architectures whose performance is on par with existing state-of-the-art NAS algorithms",
    "volume": "main",
    "checked": true,
    "id": "4f92a16b5e7786e1c4b89699d71094a37d9dc9bc",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=To-R742x7se": {
    "title": "Learning Distributionally Robust Models at Scale via Composite Optimization",
    "abstract": "To train machine learning models that are robust to distribution shifts in the data, distributionally robust optimization (DRO) has been proven very effective. However, the existing approaches to learning a distributionally robust model either require solving complex optimization problems such as semidefinite programming or a first-order method whose convergence scales linearly with the number of data samples– which hinders their scalability to large datasets. In this paper, we show how different variants of DRO are simply instances of a finite-sum composite optimization for which we provide scalable methods. We also provide empirical results that demonstrate the effectiveness of our proposed algorithm with respect to the prior art in order to learn robust models from very large datasets",
    "volume": "main",
    "checked": true,
    "id": "3b35d09d5153edb814bfd82d63666dfb178902d0",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=XJiajt89Omg": {
    "title": "Space-Time Graph Neural Networks",
    "abstract": "We introduce space-time graph neural network (ST-GNN), a novel GNN architecture, tailored to jointly process the underlying space-time topology of time-varying network data. The cornerstone of our proposed architecture is the composition of time and graph convolutional filters followed by pointwise nonlinear activation functions. We introduce a generic definition of convolution operators that mimic the diffusion process of signals over its underlying support. On top of this definition, we propose space-time graph convolutions that are built upon a composition of time and graph shift operators. We prove that ST-GNNs with multivariate integral Lipschitz filters are stable to small perturbations in the underlying graphs as well as small perturbations in the time domain caused by time warping. Our analysis shows that small variations in the network topology and time evolution of a system does not significantly affect the performance of ST-GNNs. Numerical experiments with decentralized control systems showcase the effectiveness and stability of the proposed ST-GNNs",
    "volume": "main",
    "checked": true,
    "id": "b059345775719f19fd970bd05d07914e623f08e6",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=1W0z96MFEoH": {
    "title": "Benchmarking the Spectrum of Agent Capabilities",
    "abstract": "Evaluating the general abilities of intelligent agents requires complex simulation environments. Existing benchmarks typically evaluate only one narrow task per environment, requiring researchers to perform expensive training runs on many different environments. We introduce Crafter, an open world survival game with visual inputs that evaluates a wide range of general abilities within a single environment. Agents either learn from the provided reward signal or through intrinsic objectives and are evaluated by semantically meaningful achievements that can be unlocked during each episode, such as discovering resources and crafting tools. Consistently unlocking all achievements requires strong generalization, deep exploration, and long-term reasoning. We experimentally verify that Crafter is of appropriate difficulty to drive future research and provide baselines scores of reward agents and unsupervised agents. Furthermore, we observe sophisticated behaviors emerging from maximizing the reward signal, such as building tunnel systems, bridges, houses, and plantations. We hope that Crafter will accelerate research progress by quickly evaluating a wide spectrum of abilities",
    "volume": "main",
    "checked": true,
    "id": "8e128a1b2efb0ddf688902ade4405d22d5b61eec",
    "citation_count": 22
  },
  "https://openreview.net/forum?id=04pGUg0-pdZ": {
    "title": "Finite-Time Convergence and Sample Complexity of Multi-Agent Actor-Critic Reinforcement Learning with Average Reward",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "43b3b2148b8593c407fb3d5dd9578efc5212203f",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=xKZ4K0lTj_": {
    "title": "Hierarchical Few-Shot Imitation with Skill Transition Models",
    "abstract": "A desirable property of autonomous agents is the ability to both solve long-horizon problems and generalize to unseen tasks. Recent advances in data-driven skill learning have shown that extracting behavioral priors from offline data can enable agents to solve challenging long-horizon tasks with reinforcement learning. However, generalization to tasks unseen during behavioral prior training remains an outstanding challenge. To this end, we present Few-shot Imitation with Skill Transition Models (FIST), an algorithm that extracts skills from offline data and utilizes them to generalize to unseen tasks given a few downstream demonstrations. FIST learns an inverse skill dynamics model, a distance function, and utilizes a semi-parametric approach for imitation. We show that FIST is capable of generalizing to new tasks and substantially outperforms prior baselines in navigation experiments requiring traversing unseen parts of a large maze and 7-DoF robotic arm experiments requiring manipulating previously unseen objects in a kitchen",
    "volume": "main",
    "checked": true,
    "id": "259b4f5ed43fda5dd3510821b40fac13021e7605",
    "citation_count": 12
  },
  "https://openreview.net/forum?id=TqNsv1TuCX9": {
    "title": "Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning",
    "abstract": "Visual search, recommendation, and contrastive similarity learning power technologies that impact billions of users worldwide. Modern model architectures can be complex and difficult to interpret, and there are several competing techniques one can use to explain a search engine's behavior. We show that the theory of fair credit assignment provides a unique axiomatic solution that generalizes several existing recommendationand metric-explainability techniques in the literature. Using this formalism, we show when existing approaches violate \"fairness\" and derive methods that sidestep these shortcomings and naturally handle counterfactual information. More specifically, we show existing approaches implicitly approximate second-order Shapley-Taylor indices and extend CAM, GradCAM, LIME, SHAP, SBSM, and other methods to search engines. These extensions can extract pairwise correspondences between images from trained opaque-box models. We also introduce a fast kernel-based method for estimating Shapley-Taylor indices that require orders of magnitude fewer function evaluations to converge. Finally, we show that these game-theoretic measures yield more consistent explanations for image similarity architectures",
    "volume": "main",
    "checked": true,
    "id": "4922e89201273b4040cfa5c90a5ab2906d725146",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=SaKO6z6Hl0c": {
    "title": "Unsupervised Semantic Segmentation by Distilling Feature Correspondences",
    "abstract": "Unsupervised semantic segmentation aims to discover and localize semantically meaningful categories within image corpora without any form of annotation. To solve this task, algorithms must produce features for every pixel that are both semantically meaningful and compact enough to form distinct clusters. Unlike previous works which achieve this with a single end-to-end framework, we propose to separate feature learning from cluster compactification. Empirically, we show that current unsupervised feature learning frameworks already generate dense features whose correlations are semantically consistent. This observation motivates us to design STEGO (Self-supervised Transformer with Energy-based Graph Optimization), a novel framework that distills unsupervised features into highquality discrete semantic labels. At the core of STEGO is a novel contrastive loss function that encourages features to form compact clusters while preserving their relationships across the corpora. STEGO yields a significant improvement over the prior state of the art, on both the CocoStuff (+14 mIoU) and Cityscapes (+9 mIoU) semantic segmentation challenges",
    "volume": "main",
    "checked": true,
    "id": "dc6aaacef65638ecc76baa241929cfb0013460f1",
    "citation_count": 22
  },
  "https://openreview.net/forum?id=hniLRD_XCA": {
    "title": "DeSKO: Stability-Assured Robust Control with a Deep Stochastic Koopman Operator",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "6d3b65097096bb0ec52896e9d8a8ad194f18d290",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=L3_SsSNMmy": {
    "title": "On the Connection between Local Attention and Dynamic Depth-wise Convolution",
    "abstract": "Vision Transformer (ViT) attains state-of-the-art performance in visual recognition, and the variant, Local Vision Transformer, makes further improvements. The major component in Local Vision Transformer, local attention, performs the attention separately over small local windows. We rephrase local attention as a channel-wise locally-connected layer and analyze it from two network regularization manners, sparse connectivity and weight sharing, as well as dynamic weight computation. We point out that local attention resembles depth-wise convolution and its dynamic variants in sparse connectivity: there is no connection across channels, and each position is connected to the positions within a small local window. The main differences lie in (i) weight sharing - depth-wise convolution shares connection weights (kernel weights) across spatial positions and attention shares the connection weights across channels, and (ii) dynamic weight computation manners - local attention is based on dot-products between pairwise positions in the local window, and dynamic convolution is based on linear projections conducted on the center representation or the globally pooled representation. The connection between local attention and dynamic depth-wise convolution is empirically veriﬁed by the ablation study about weight sharing and dynamic weight computation in Local Vision Transformer and (dynamic) depth-wise convolution based network, namely (dynamic) DWNet. We empirically observe that the depth-wise convolution based DWNet and its dynamic variants with lower computation complexity perform on-par with or slightly better than Swin Transformer, an instance of Local Vision Transformer, for ImageNet classiﬁcation, COCO object detection and ADE semantic segmentation. Code is available at https://github.com/Atten4Vis/DemystifyLocalViT",
    "volume": "main",
    "checked": true,
    "id": "6b6ffb94626e672caffafc77097491d9ee7a8682",
    "citation_count": 14
  },
  "https://openreview.net/forum?id=BB4e8Atc1eR": {
    "title": "Scalable Sampling for Nonsymmetric Determinantal Point Processes",
    "abstract": "A determinantal point process (DPP) on a collection of M items is a model, parameterized by a symmetric kernel matrix, that assigns a probability to every subset of those items. Recent work shows that removing the kernel symmetry constraint, yielding nonsymmetric DPPs (NDPPs), can lead to signiﬁcant predictive performance gains for machine learning applications. However, existing work leaves open the question of scalable NDPP sampling. There is only one known DPP sampling algorithm, based on Cholesky decomposition, that can directly apply to NDPPs as well. Unfortunately, its runtime is cubic in M , and thus does not scale to large item collections. In this work, we ﬁrst note that this algorithm can be transformed into a linear-time one for kernels with low-rank structure. Furthermore, we develop a scalable sublinear-time rejection sampling algorithm by constructing a novel proposal distribution. Additionally, we show that imposing certain structural constraints on the NDPP kernel enables us to bound the rejection rate in a way that depends only on the kernel rank. In our experiments we compare the speed of all of these samplers for a variety of real-world tasks",
    "volume": "main",
    "checked": true,
    "id": "1b79e26abffcb2cd4df418d780ad44c162851ba4",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=XctLdNfCmP": {
    "title": "Predicting Physics in Mesh-reduced Space with Temporal Attention",
    "abstract": "in modeling complex high-dimensional physical systems on irregular meshes. How-ever, to their short temporal attention span, error accumulation and drift. In we propose a new method that captures long-term dependencies through a transformer-style temporal attention model. We introduce an encoder-decoder structure to summarize features and create a com-pact mesh representation of the system state, to allow the temporal model to operate on a low-dimensional mesh representations in a memory efﬁcient manner. Our method outperforms a competitive GNN baseline on several complex ﬂuid dynamics prediction tasks, from sonic shocks to vascular ﬂow. We demonstrate stable rollouts without the need for training noise and show perfectly phase-stable predictions even for very long sequences. More broadly, we believe our approach paves the way to bringing the beneﬁts of attention-based sequence models to solving high-dimensional complex physics tasks",
    "volume": "main",
    "checked": true,
    "id": "4ed73bfb7fd4178013fabecac638f9d188d3c423",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=pjqqxepwoMy": {
    "title": "Variational oracle guiding for reinforcement learning",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "18ea0cc03be311dc5d24e082d96aed021419b2b2",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=w1UbdvWH_R3": {
    "title": "Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path",
    "abstract": "The recently discovered Neural Collapse (NC) phenomenon occurs pervasively in today's deep net training paradigm of driving cross-entropy (CE) loss towards zero. During NC, last-layer features collapse to their class-means, both classiﬁers and class-means collapse to the same Simplex Equiangular Tight Frame, and classiﬁer behavior collapses to the nearest-class-mean decision rule. Recent works demonstrated that deep nets trained with mean squared error (MSE) loss perform comparably to those trained with CE. As a preliminary, we empirically establish that NC emerges in such MSE-trained deep nets as well through experiments on three canonical networks and ﬁve benchmark datasets. We provide, in a Google Colab notebook, PyTorch code for reproducing MSE-NC and CE-NC: here. The analytically-tractable MSE loss offers more mathematical opportunities than the hard-to-analyze CE loss, inspiring us to leverage MSE loss towards the theoretical investigation of NC. We develop three main contributions: (I) We show a new decomposition of the MSE loss into (A) terms directly interpretable through the lens of NC and which assume the last-layer classiﬁer is exactly the least-squares classiﬁer; and (B) a term capturing the deviation from this least-squares classiﬁer. (II) We exhibit experiments on canonical datasets and networks demonstrating that term-(B) is negligible during training. This motivates us to introduce a new theoretical construct: the central path , where the linear classiﬁer stays MSE-optimal for feature activations throughout the dynamics. (III) By studying renormalized gradient ﬂow along the central path, we derive",
    "volume": "main",
    "checked": true,
    "id": "79eff96fd96b688ff5368f7edb506598bc53d354",
    "citation_count": 32
  },
  "https://openreview.net/forum?id=rpxJc9j04U": {
    "title": "Proof Artifact Co-Training for Theorem Proving with Language Models",
    "abstract": "Labeled data for imitation learning of theorem proving in large libraries of formalized mathematics is scarce as such libraries require years of concentrated effort by human specialists to be built. This is particularly challenging when applying large Transformer language models to tactic prediction, because the scaling of performance with respect to model size is quickly disrupted in the data-scarce, easily-overﬁtted regime. We propose PACT ( P roof A rtifact C o- T raining), a general methodology for extracting abundant self-supervised data from kernel-level proof terms for co-training alongside the usual tactic prediction objective. We apply this methodology to Lean, an interactive proof assistant which hosts some of the most sophisticated formalized mathematics to date. We instrument Lean with a neural theorem prover driven by a Transformer language model and show that PACT improves theorem proving success rate on a held-out suite of test theorems from 32% to 48%",
    "volume": "main",
    "checked": true,
    "id": "065112180cd381ffc018780cf8fc0a14ae2580b1",
    "citation_count": 24
  },
  "https://openreview.net/forum?id=bCrdi4iVvv": {
    "title": "Learning Features with Parameter-Free Layers",
    "abstract": "Trainable layers such as convolutional building blocks are the standard network design choices by learning parameters to capture the global context through successive spatial operations. When designing an efficient network, trainable layers such as the depthwise convolution is the source of efficiency in the number of parameters and FLOPs, but there was little improvement to the model speed in practice. This paper argues that simple built-in parameter-free operations can be a favorable alternative to the efficient trainable layers replacing spatial operations in a network architecture. We aim to break the stereotype of organizing the spatial operations of building blocks into trainable layers. Extensive experimental analyses based on layer-level studies with fully-trained models and neural architecture searches are provided to investigate whether parameter-free operations such as the max-pool are functional. The studies eventually give us a simple yet effective idea for redesigning network architectures, where the parameter-free operations are heavily used as the main building block without sacrificing the model accuracy as much. Experimental results on the ImageNet dataset demonstrate that the network architectures with parameter-free operations could enjoy the advantages of further efficiency in terms of model speed, the number of the parameters, and FLOPs. Code and ImageNet pretrained models are available at https://github.com/naver-ai/PfLayer",
    "volume": "main",
    "checked": true,
    "id": "237776a7902f275c34b1ac389cfc38e974916fea",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=ci7LBzDn2Q": {
    "title": "Deep ReLU Networks Preserve Expected Length",
    "abstract": "Assessing the complexity of functions computed by a neural network helps us understand how the network will learn and generalize. One natural measure of complexity is how the network distorts length – if the network takes a unit-length curve as input, what is the length of the resulting curve of outputs? It has been widely believed that this length grows exponentially in network depth. We prove that in fact this is not the case: the expected length distortion does not grow with depth, and indeed shrinks slightly, for ReLU networks with standard random initialization. We also generalize this result by proving upper bounds both for higher moments of the length distortion and for the distortion of higher-dimensional volumes. These theoretical results are corroborated by our experiments",
    "volume": "main",
    "checked": true,
    "id": "454343bad831481f154a2e4506e44e3159cc0b38",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=Oy9WeuZD51": {
    "title": "A Statistical Framework for Efficient Out of Distribution Detection in Deep Neural Networks",
    "abstract": "paired with a set of trained on CIFAR-10, and SVHN appropriate training dataset is referred to as the in-distribution while results are reported on the full validation split",
    "volume": "main",
    "checked": true,
    "id": "9e52bc970e53b350d393c7c3880ddab074867367",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=yeP_zx9vqNm": {
    "title": "Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks",
    "abstract": "Recent work suggests that feature constraints in the training datasets of deep neu- 1 ral networks (DNNs) drive robustness to adversarial noise (Ilyas et al., 2019). 2 The representations learned by such adversarially robust networks have also been 3 shown to be more human perceptually-aligned than non-robust networks via image 4 manipulations (Santurkar et al., 2019; Engstrom et al., 2019). Despite appearing 5 closer to human visual perception, it is unclear if the constraints in robust DNN 6 representations match biological constraints found in human vision. Human vision 7 seems to rely on texture-based/summary statistic representations in the periphery, 8 which have been shown to explain phenomena such as crowding (Balas et al., 2009) 9 and performance on visual search tasks (Rosenholtz et al., 2012). To understand 10 how adversarially robust optimizations/representations compare to human vision, 11 we performed a psychophysics experiment using a metamer task similar to Freeman 12 & Simoncelli (2011); Wallis et al. (2019); Deza et al. (2017) where we evaluated 13 how well human observers could distinguish between images synthesized to match 14 adversarially robust representations compared to non-robust representations and a 15 texture synthesis model of peripheral vision (Texforms (Long et al., 2018)). We 16 found that the discriminability of robust representation and texture model images 17 decreased to near chance performance as stimuli were presented farther in the 18 periphery. Moreover, performance on robust and texture-model images showed 19 similar trends within participants, while performance on non-robust representa- 20 tions changed minimally across the visual ﬁeld. These results together suggest 21 that (1) adversarially robust representations capture peripheral computation better 22 than non-robust representations and (2) robust representations capture peripheral 23 computation similar to current state-of-the-art texture peripheral vision models. 24 More broadly, our ﬁndings support the idea that localized texture summary statis- 25 tic representations may drive human invariance to adversarial",
    "volume": "main",
    "checked": true,
    "id": "c875d546411c598df075ae555fbb3108fea02910",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=vcUmUvQCloe": {
    "title": "Joint Shapley values: a measure of joint feature importance",
    "abstract": "The Shapley value is one of the most widely used model-agnostic measures of feature importance in explainable AI: it has clear axiomatic foundations, is guaranteed to uniquely exist, and has a clear interpretation as a feature's average effect on a model's prediction. We introduce joint Shapley values, which directly extend the Shapley axioms. This preserves the classic Shapley value's intuitions: joint Shapley values measure a set of features' average effect on a model's prediction. We prove the uniqueness of joint Shapley values, for any order of explanation. Results for games show that joint Shapley values present different insights from existing interaction indices, which assess the effect of a feature within a set of features. Deriving joint Shapley values in ML attribution problems thus gives us the first measure of the joint effect of sets of features on model predictions. In a dataset with binary features, we present a presence-adjusted method for calculating global values that retains the efficiency property",
    "volume": "main",
    "checked": true,
    "id": "ec73937dddc1d5573072d39cc5131178c796dc03",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=DnG75_KyHjX": {
    "title": "MoReL: Multi-omics Relational Learning",
    "abstract": "Multi-omics data analysis has the potential to discover hidden molecular interactions, revealing potential regulatory and/or signal transduction pathways for cellular processes of interest when studying life and disease systems. One of critical challenges when dealing with real-world multi-omics data is that they may manifest heterogeneous structures and data quality as often existing data may be collected from different subjects under different conditions for each type of omics data. We propose a novel deep Bayesian generative model to efﬁciently infer a multi-partite graph that encodes molecular interactions across such heterogeneous views, using a fused Gromov-Wasserstein (FGW) regularization between latent representations of corresponding views for integrative analysis. With such an optimal transport regularization in the deep Bayesian generative model, it not only allows incorporating view-speciﬁc side information, either with graph-structured or unstructured data in different views, but also increases the model ﬂexibility with the distribution-based regularization. This allows efﬁcient alignment of heterogeneous latent variable distributions to derive reliable interaction predictions compared to the existing point-based graph embedding methods. Our experiments on several real-world datasets demonstrate the enhanced performance of MoReL in inferring meaningful interactions compared to existing baselines",
    "volume": "main",
    "checked": true,
    "id": "7e6de103891ff995967de5f8e40a4e948a566d41",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=tBIQEvApZK5": {
    "title": "Feature Kernel Distillation",
    "abstract": "Trained Neural Networks (NNs) can be viewed as data-dependent kernel machines, with predictions determined by the inner product of last-layer representations across inputs, referred to as the feature kernel . We explore the relevance of the feature kernel for Knowledge Distillation (KD), using a mechanistic understanding of an NN's optimisation process. We extend the theoretical analysis of Allen-Zhu & Li (2020) to show that a trained NN's feature kernel is highly dependent on its parameter initialisation, which biases different initialisations of the same architecture to learn different data attributes in a multi-view data setting. This enables us to prove that KD using only pairwise feature kernel compar-isons can improve NN test accuracy in such settings, with both single & ensemble teacher models, whereas standard training without KD fails to generalise. We further use our theory to motivate practical considerations for improving student generalisation when using distillation with feature kernels, which allows us to propose a novel approach: Feature Kernel Distillation (FKD). Finally, we experimentally corroborate our theory in the image classiﬁcation setting, showing that FKD is amenable to ensemble distillation, can transfer knowledge across datasets, and outperforms both vanilla KD & other feature kernel based KD baselines across a range of standard architectures & datasets",
    "volume": "main",
    "checked": true,
    "id": "1c5ac60e4af26048812bf01e45a4e71df07d3a87",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=0RDcd5Axok": {
    "title": "Towards a Unified View of Parameter-Efficient Transfer Learning",
    "abstract": "Fine-tuning large pretrained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches finetune all the parameters of the pretrained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them. Specifically, we re-frame them as modifications to specific hidden states in pretrained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods. Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks.1",
    "volume": "main",
    "checked": true,
    "id": "43a87867fe6bf4eb920f97fc753be4b727308923",
    "citation_count": 101
  },
  "https://openreview.net/forum?id=3YqeuCVwy1d": {
    "title": "GDA-AM: On the Effectiveness of Solving Min-Imax Optimization via Anderson Mixing",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "fb011c5468856f46ee8def942b3d22b51e358660",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=zXM0b4hi5_B": {
    "title": "On the relation between statistical learning and perceptual distances",
    "abstract": "we to the non-trivial relationships between the probability distribution of the perceptual distances, and unsupervised machine learning. To this end, we show that perceptual sensitivity is correlated with the probability of an image in its close neighborhood. We also explore the relation between distances induced by autoencoders and the probability distribution of the training data, as well as how these induced distances are correlated with human perception. Finally, we perceptual distances do not always lead to noticeable gains in performance over Euclidean distance in common image processing tasks, except when data is scarce and the perceptual distance provides regularization. We propose this may be due to a double-counting effect of the image statistics, once in the perceptual distance and once in the training procedure",
    "volume": "main",
    "checked": true,
    "id": "600b3057346e0fa1e1afb59a179acbcb4a679bb6",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=NudBMY-tzDr": {
    "title": "Natural Language Descriptions of Deep Visual Features",
    "abstract": "Some neurons in deep networks specialize in recognizing highly speciﬁc perceptual, structural, or semantic features of inputs. In computer vision, techniques exist for identifying neurons that respond to individual concept categories like colors, textures, and object classes. But these techniques are limited in scope, labeling only a small subset of neurons and behaviors in any network. Is a richer characterization of neuron-level computation possible? We introduce a procedure (called MILAN , for m utual- i nformation-guided l inguistic a nnotation of n eurons) that automatically labels neurons with open-ended, compositional, natural language descriptions. Given a neuron, MILAN generates a description by searching for a natural language string that maximizes pointwise mutual information with the image regions in which the neuron is active. MILAN produces ﬁne-grained descriptions that capture categorical, relational, and logical structure in learned features. These descriptions obtain high agreement with human-generated feature descriptions across a diverse set of model architectures and tasks, and can aid in understanding and controlling learned models. We highlight three applications of natural language neuron descriptions. First, we use MILAN for analysis , characterizing the distribution and importance of neurons selective for attribute, category, and relational information in vision models. Second, we use MILAN for auditing , surfacing neurons sensitive to human faces in datasets designed to obscure them. Finally, we use MILAN for editing , improving robustness in an image classiﬁer by deleting neurons sensitive to text features spuriously correlated with class labels. 1",
    "volume": "main",
    "checked": true,
    "id": "2e7be2911be7c6c8b8016aa30953d479649f1ffe",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=xCVJMsPv3RT": {
    "title": "Dropout Q-Functions for Doubly Efficient Reinforcement Learning",
    "abstract": "Randomized ensembled double Q-learning (REDQ) (Chen et al., 2021b) has recently achieved state-of-the-art sample efficiency on continuous-action reinforcement learning benchmarks. This superior sample efficiency is made possible by using a large Q-function ensemble. However, REDQ is much less computationally efficient than non-ensemble counterparts such as Soft Actor-Critic (SAC) (Haarnoja et al., 2018a). To make REDQ more computationally efficient, we propose a method of improving computational efficiency called DroQ, which is a variant of REDQ that uses a small ensemble of dropout Q-functions. Our dropout Q-functions are simple Q-functions equipped with dropout connection and layer normalization. Despite its simplicity of implementation, our experimental results indicate that DroQ is doubly (sample and computationally) efficient. It achieved comparable sample efficiency with REDQ, much better computational efficiency than REDQ, and comparable computational efficiency with that of SAC",
    "volume": "main",
    "checked": true,
    "id": "b2d931da61559c528c5d4eadcb939425a2531652",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=OXRZeMmOI7a": {
    "title": "Topological Experience Replay",
    "abstract": "State-of-the-art deep Q-learning methods update Q-values using state transition tuples sampled from the experience replay buffer. This strategy often uniformly and randomly samples or prioritizes data sampling based on measures such as the temporal difference (TD) error. Such sampling strategies can be inefficient at learning Q-function because a state's Q-value depends on the Q-value of successor states. If the data sampling strategy ignores the precision of Q-value estimate of the next state, it can lead to useless and often incorrect updates to the Q-values. To mitigate this issue, we organize the agent's experience into a graph that explicitly tracks the dependency between Q-values of states. Each edge in the graph represents a transition between two states by executing a single action. We perform value backups via a breadth-first search starting from the set of terminal states and successively moving backwards. We empirically show that our method is substantially more data-efficient than several baselines on a diverse range of goal-reaching tasks. Notably, the proposed method also outperforms baselines that consume more batches of training experience and operates from high-dimensional observational data such as images",
    "volume": "main",
    "checked": true,
    "id": "a2ed2715d0175e93939e0d9f46f2580968c98020",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=OtEDS2NWhqa": {
    "title": "Using Graph Representation Learning with Schema Encoders to Measure the Severity of Depressive Symptoms",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "6080c4cd6847cbd1c756928639bab086f57415d9",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=_QLmakITKg": {
    "title": "Efficient Split-Mix Federated Learning for On-Demand and In-Situ Customization",
    "abstract": "Federated learning (FL) provides a distributed learning framework for multiple participants to collaborate learning without sharing raw data. In many practical FL scenarios, participants have heterogeneous resources due to disparities in hardware and inference dynamics that require quickly loading models of different sizes and levels of robustness. The heterogeneity and dynamics together impose significant challenges to existing FL approaches and thus greatly limit FL's applicability. In this paper, we propose a novel Split-Mix FL strategy for heterogeneous participants that, once training is done, provides in-situ customization of model sizes and robustness. Specifically, we achieve customization by learning a set of base sub-networks of different sizes and robustness levels, which are later aggregated on-demand according to inference requirements. This split-mix strategy achieves customization with high efficiency in communication, storage, and inference. Extensive experiments demonstrate that our method provides better in-situ customization than the existing heterogeneous-architecture FL methods. Codes and pre-trained models are available: https://github.com/illidanlab/SplitMix",
    "volume": "main",
    "checked": true,
    "id": "c91d39810492e585809658bb9eec065979b9214b",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=LedObtLmCjS": {
    "title": "Bi-linear Value Networks for Multi-goal Reinforcement Learning",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "eae2cf052f034a6cbbb35f56cb3a86329cf1edff",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=fy_XRVHqly": {
    "title": "Structure-Aware Transformer Policy for Inhomogeneous Multi-Task Reinforcement Learning",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "0cabfee41564ec6eefdfcccc938502d4a7813e53",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=Lm8T39vLDTE": {
    "title": "Autoregressive Diffusion Models",
    "abstract": "We introduce Autoregressive Diffusion Models (ARDMs), a model class encom-passing and generalizing order-agnostic autoregressive models (Uria et and absorbing discrete diffusion (Austin et al., which we show are special cases of ARDMs under mild assumptions. ARDMs are simple to implement and easy to train. Unlike standard ARMs, they do not require causal masking of model representations, and can be trained using an efﬁcient objective similar to modern probabilistic diffusion models that scales favourably to highly-dimensional data. At test time, ARDMs support parallel generation which can be adapted to ﬁt any given generation budget. We ﬁnd that ARDMs require signiﬁcantly fewer steps than discrete diffusion models to attain the same performance. Finally, we apply ARDMs to lossless compression, and show that they are uniquely suited to this task. Contrary to existing approaches based on bits-back coding, ARDMs obtain compelling results not only on complete datasets, but also on compressing single data points. Moreover, this can be done using a modest number of network calls for (de)compression due to the model's adaptable parallel generation",
    "volume": "main",
    "checked": true,
    "id": "599bc7cfe98c2b57ddbe111412203a636da57be0",
    "citation_count": 21
  },
  "https://openreview.net/forum?id=oxxUMeFwEHd": {
    "title": "Topological Graph Neural Networks",
    "abstract": "Graph neural networks (GNNs) are a powerful architecture for tackling graph learning tasks, yet have been shown to be oblivious to eminent substructures, such as cycles. We present TOGL, a novel layer that incorporates global topological information of a graph using persistent homology. TOGL can be easily integrated into any type of GNN and is strictly more expressive in terms of the Weisfeiler–Lehman test of isomorphism. Augmenting GNNs with our layer leads to beneficial predictive performance, both on synthetic data sets, which can be trivially classified by humans but not by ordinary GNNs, and on real-world data",
    "volume": "main",
    "checked": true,
    "id": "2db550467862c3bea38901cef84d86efed31b8d2",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=mHu2vIds_-b": {
    "title": "Boosting Randomized Smoothing with Variance Reduced Classifiers",
    "abstract": "Randomized Smoothing (RS) is a promising method for obtaining robustness certiﬁcates by evaluating a base model under noise. In this work, we: (i) theoretically motivate why ensembles are a particularly suitable choice as base models for RS, and (ii) empirically conﬁrm this choice, obtaining state-of-the-art results in multiple settings. The key insight of our work is that the reduced variance of ensembles over the perturbations introduced in RS leads to signiﬁcantly more consistent classiﬁ-cations for a given input. This, in turn, leads to substantially increased certiﬁable radii for samples close to the decision boundary. Additionally, we introduce key op-timizations which enable an up to 55-fold decrease in sample complexity of RS for predetermined radii, thus drastically reducing its computational overhead. Experimentally, we show that ensembles of only 3 to 10 classiﬁers consistently improve on their strongest constituting model with respect to their average certiﬁed radius (ACR) by 5% to 21% on both CIFAR10 and ImageNet, achieving a new state-of-the-art ACR of 0 . 86 and 1 . 11 , respectively. We release all code and models required to reproduce our results at https://github.com/eth-sri/smoothing-ensembles ",
    "volume": "main",
    "checked": true,
    "id": "37613cdd48d6e32d995bbd2dc2e8e3902892dd76",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=nO5caZwFwYu": {
    "title": "Efficient Active Search for Combinatorial Optimization Problems",
    "abstract": "Recently, numerous machine learning based methods for combinatorial optimization problems have been proposed that learn to construct solutions in a sequential decision process via reinforcement learning. While these methods can be easily combined with search strategies like sampling and beam search, it is not straightforward to integrate them into a high-level search procedure offering strong search guidance. Bello et al. (2016) propose active search, which adjusts the weights of a (trained) model with respect to a single instance at test time using reinforcement learning. While active search is simple to implement, it is not competitive with state-of-the-art methods because adjusting all model weights for each test instance is very time and memory intensive. Instead of updating all model weights, we propose and evaluate three efficient active search strategies that only update a subset of parameters during the search. The proposed methods offer a simple way to significantly improve the search performance of a given model and outperform state-of-the-art machine learning based methods on combinatorial problems, even surpassing the well-known heuristic solver LKH3 on the capacitated vehicle routing problem. Finally, we show that (efficient) active search enables learned models to effectively solve instances that are much larger than those seen during training",
    "volume": "main",
    "checked": true,
    "id": "e3f453ee3d2e084cb0f24769ead763844dbc0661",
    "citation_count": 14
  },
  "https://openreview.net/forum?id=ZaVVVlcdaN": {
    "title": "FedChain: Chained Algorithms for Near-optimal Communication Cost in Federated Learning",
    "abstract": "Federated learning (FL) aims to minimize the communication complexity of training a model over heterogeneous data distributed across many clients. A common approach is local update methods, where clients take multiple optimization steps over local data before communicating with the server (e.g., FedAvg). Local update methods can exploit similarity between clients' data. However, in existing analyses, this comes at the cost of slow convergence in terms of the dependence on the number of communication rounds R. On the other hand, global update methods, where clients simply return a gradient vector in each round (e.g., SGD), converge faster in terms of R but fail to exploit the similarity between clients even when clients are homogeneous. We propose FedChain, an algorithmic framework that combines the strengths of local update methods and global update methods to achieve fast convergence in terms of R while leveraging the similarity between clients. Using FedChain, we instantiate algorithms that improve upon previously known rates in the general convex and PL settings, and are near-optimal (via an algorithm-independent lower bound that we show) for problems that satisfy strong convexity. Empirical results support this theoretical gain over existing methods",
    "volume": "main",
    "checked": true,
    "id": "521e6cdabe3f4fa62813559ccb36ee0523fb7904",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=uPv9Y3gmAI5": {
    "title": "Language model compression with weighted low-rank factorization",
    "abstract": "Factorizing a large matrix into small matrices is a popular strategy for model compression. Singular value decomposition (SVD) plays a vital role in this compression strategy, approximating a learned matrix with fewer parameters. However, SVD minimizes the squared error toward reconstructing the original matrix without gauging the importance of the parameters, potentially giving a larger reconstruction error for those who affect the task accuracy more. In other words, the optimization objective of SVD is not aligned with the trained model's task accuracy. We analyze this previously unexplored problem, make observations, and address it by introducing Fisher information to weigh the importance of parameters affecting the model prediction. This idea leads to our method: Fisher-Weighted SVD (FWSVD). Although the factorized matrices from our approach do not result in smaller reconstruction errors, we ﬁnd that our resulting task accuracy is much closer to the original model's performance. We perform analysis with the transformer-based language models, showing our weighted SVD largely allevi-ates the mismatched optimization objectives and can maintain model performance with a higher compression rate. Our method can directly compress a task-speciﬁc model while achieving better performance than other compact model strategies requiring expensive model pre-training. Moreover, the evaluation of compressing an already compact model shows our method can further reduce 9% to 30% parameters with an insigniﬁcant impact on task accuracy",
    "volume": "main",
    "checked": true,
    "id": "d151ced8700d84a2efe411a234a4cb2c595e8ca9",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=RJkAHKp7kNZ": {
    "title": "Vision-Based Manipulators Need to Also See from Their Hands",
    "abstract": "We study how the choice of visual perspective affects learning and generalization in the context of physical manipulation from raw sensor observations. Compared with the more commonly used global third-person perspective, a hand-centric (eye-in-hand) perspective affords reduced observability, but we ﬁnd that it consistently improves training efﬁciency and out-of-distribution generalization. These beneﬁts hold across a variety of learning algorithms, experimental settings, and distribution shifts, and for both simulated and real robot apparatuses. However, this is only the case when hand-centric observability is sufﬁcient; otherwise, including a third-person perspective is necessary for learning, but also harms out-of-distribution generalization. To mitigate this, we propose to regularize the third-person information stream via a variational information bottleneck. On six representative manipulation tasks with varying hand-centric observability adapted from the Meta-World benchmark, this results in a state-of-the-art reinforcement learning agent operating from both perspectives improving its out-of-distribution generalization on every task. While some practitioners have long put cameras in the hands of robots, our work systematically analyzes the beneﬁts of doing so and provides simple and broadly applicable insights for improving end-to-end learned vision-based robotic manipulation. 1",
    "volume": "main",
    "checked": true,
    "id": "bc912d8c1a7165d89483d96d64b2ef20703ed2cb",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=o0ehFykKVtr": {
    "title": "Know Thyself: Transferable Visual Control Policies Through Robot-Awareness",
    "abstract": "Training visual control policies from scratch on a new robot typically requires generating large amounts of robot-specific data. How might we leverage data previously collected on another robot to reduce or even completely remove this need for robot-specific data? We propose a \"robot-aware control\" paradigm that achieves this by exploiting readily available knowledge about the robot. We then instantiate this in a robot-aware model-based RL policy by training modular dynamics models that couple a transferable, robot-aware world dynamics module with a robot-specific, potentially analytical, robot dynamics module. This also enables us to set up visual planning costs that separately consider the robot agent and the world. Our experiments on tabletop manipulation tasks with simulated and real robots demonstrate that these plug-in improvements dramatically boost the transferability of visual model-based RL policies, even permitting zero-shot transfer of visual manipulation skills onto new robots. Project website: https: //www.seas.upenn.edu/~hued/rac",
    "volume": "main",
    "checked": true,
    "id": "93882706a1c92316c9c930e268d09d8a40e3e6e2",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=vEZyTBRPP6o": {
    "title": "Actor-critic is implicitly biased towards high entropy optimal policies",
    "abstract": "We show that the simplest actor-critic method — a linear softmax policy updated with TD through interaction with a linear MDP, but featuring no explicit regularization or exploration — does not merely find an optimal policy, but moreover prefers high entropy optimal policies. To demonstrate the strength of this bias, the algorithm not only has no regularization, no projections, and no exploration like ǫ-greedy, but is moreover trained on a single trajectory with no resets. The key consequence of the high entropy bias is that uniform mixing assumptions on the MDP, which exist in some form in all prior work, can be dropped: the implicit regularization of the high entropy bias is enough to ensure that all chains mix and an optimal policy is reached with high probability. As auxiliary contributions, this work decouples concerns between the actor and critic by writing the actor update as an explicit mirror descent, provides tools to uniformly bound mixing times within KL balls of policy space, and provides a projection-free TD analysis with its own implicit bias which can be run from an unmixed starting distribution",
    "volume": "main",
    "checked": true,
    "id": "2c7bdcd62f7ab957589931b4bf818d92fb3498bb",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=6yVvwR9H9Oj": {
    "title": "On Non-Random Missing Labels in Semi-Supervised Learning",
    "abstract": "Semi-Supervised Learning (SSL) is fundamentally a missing label problem, in which the label Missing Not At Random (MNAR) problem is more realistic and challenging, compared to the widely-adopted yet naı̈ve Missing Completely At Random assumption where both labeled and unlabeled data share the same class distribution. Different from existing SSL solutions that overlook the role of \"class\" in causing the non-randomness, e.g., users are more likely to label popular classes, we explicitly incorporate \"class\" into SSL. Our method is three-fold: 1) We propose Class-Aware Propensity (CAP) score that exploits the unlabeled data to train an improved classifier using the biased labeled data. 2) To encourage rare class training, whose model is low-recall but high-precision that discards too many pseudo-labeled data, we propose Class-Aware Imputation (CAI) that dynamically decreases (or increases) the pseudo-label assignment threshold for rare (or frequent) classes. 3) Overall, we integrate CAP and CAI into a Class-Aware Doubly Robust (CADR) estimator for training an unbiased SSL model. Under various MNAR settings and ablations, our method not only significantly outperforms existing baselines, but also surpasses other label bias removal SSL methods",
    "volume": "main",
    "checked": true,
    "id": "410b8ab687f6f333e254512b49b8703908021fb2",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=nZeVKeeFYf9": {
    "title": "LoRA: Low-Rank Adaptation of Large Language Models",
    "abstract": "The dominant paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, conventional fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example, deploying many independent instances of fine-tuned models, each with 175B parameters, is extremely expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. For GPT-3, LoRA can reduce the number of trainable parameters by 10,000 times and the computation hardware requirement by 3 times compared to full fine-tuning. LoRA performs on-par or better than fine-tuning in model quality on both GPT-3 and GPT-2, despite having fewer trainable parameters, a higher training throughput, and no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptations, which sheds light on the efficacy of LoRA. We release our implementation in GPT-2 at https://github.com/microsoft/LoRA",
    "volume": "main",
    "checked": true,
    "id": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
    "citation_count": 145
  },
  "https://openreview.net/forum?id=H-iABMvzIc": {
    "title": "Switch to Generalize: Domain-Switch Learning for Cross-Domain Few-Shot Classification",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "a01ccf2e3ca3a694e75a2fedc5f9d9d83db06db6",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=_jMtny3sMKU": {
    "title": "Generalizing Few-Shot NAS with Gradient Matching",
    "abstract": "Efficient performance estimation of architectures drawn from large search spaces is essential to Neural Architecture Search. One-Shot methods tackle this challenge by training one supernet to approximate the performance of every architecture in the search space via weight-sharing, thereby drastically reducing the search cost. However, due to coupled optimization between child architectures caused by weight-sharing, One-Shot supernet's performance estimation could be inaccurate, leading to degraded search outcomes. To address this issue, Few-Shot NAS reduces the level of weight-sharing by splitting the One-Shot supernet into multiple separated sub-supernets via edge-wise (layer-wise) exhaustive partitioning. Since each partition of the supernet is not equally important, it necessitates the design of a more effective splitting criterion. In this work, we propose a gradient matching score (GM) that leverages gradient information at the shared weight for making informed splitting decisions. Intuitively, gradients from different child models can be used to identify whether they agree on how to update the shared modules, and subsequently to decide if they should share the same weight. Compared with exhaustive partitioning, the proposed criterion significantly reduces the branching factor per edge. This allows us to split more edges (layers) for a given budget, resulting in substantially improved performance as NAS search spaces usually include dozens of edges (layers). Extensive empirical evaluations of the proposed method on a wide range of search spaces (NASBench-201, DARTS, MobileNet Space), datasets (cifar10, cifar100, ImageNet) and search algorithms (DARTS, SNAS, RSPS, ProxylessNAS, OFA) demonstrate that it significantly outperforms its Few-Shot counterparts while surpassing previous comparable methods in terms of the accuracy of derived architectures. Our code is available at https://github.com/skhu101/GM-NAS",
    "volume": "main",
    "checked": true,
    "id": "74de5cdd6a9134db812026492fe5b6ecebd83703",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=EwqEx5ipbOu": {
    "title": "How Well Does Self-Supervised Pre-Training Perform with Streaming Data?",
    "abstract": "Prior works on self-supervised pre-training focus on the joint training scenario, where massive unlabeled data are assumed to be given as input all at once, and only then is a learner trained. Unfortunately, such a problem setting is often impractical if not infeasible since many real-world tasks rely on sequential learning, e.g., data are decentralized or collected in a streaming fashion. In this paper, we conduct the first thorough and dedicated investigation on self-supervised pretraining with streaming data, aiming to shed light on the model behavior under this overlooked setup. Specifically, we pre-train over 500 models on four categories of pre-training streaming data from ImageNet and DomainNet and evaluate them on three types of downstream tasks and 12 different downstream datasets. Our studies show that, somehow beyond our expectation, with simple data replay or parameter regularization, sequential self-supervised pre-training turns out to be an efficient alternative for joint pre-training, as the performances of the former are mostly on par with those of the latter. Moreover, catastrophic forgetting, a common issue in sequential supervised learning, is much alleviated in sequential self-supervised learning (SSL), which is well justified through our comprehensive empirical analysis on representations and the sharpness of minima in the loss landscape. Our findings, therefore, suggest that, in practice, for SSL, the cumbersome joint training can be replaced mainly by sequential learning, which in turn enables a much broader spectrum of potential application scenarios",
    "volume": "main",
    "checked": true,
    "id": "2824c3ca577822d8201d48127585e0f86284f01b",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=NyJ2KIN8P17": {
    "title": "Neural Program Synthesis with Query",
    "abstract": "Aiming to find a program satisfying the user intent given input-output examples, program synthesis has attracted increasing interest in the area of machine learning. Despite the promising performance of existing methods, most of their success comes from the privileged information of well-designed input-output examples. However, providing such input-output examples is unrealistic because it requires the users to have the ability to describe the underlying program with a few inputoutput examples under the training distribution. In this work, we propose a querybased framework that trains a query neural network to generate informative inputoutput examples automatically and interactively from a large query space. The quality of the query depends on the amount of the mutual information between the query and the corresponding program, which can guide the optimization of the query framework. To estimate the mutual information more accurately, we introduce the functional space (F-space) which models the relevance between the input-output examples and the programs in a differentiable way. We evaluate the effectiveness and generalization of the proposed query-based framework on the Karel task and the list processing task. Experimental results show that the querybased framework can generate informative input-output examples which achieve and even outperform well-designed input-output examples",
    "volume": "main",
    "checked": true,
    "id": "e32f6c3997fe4945e84effb84b1fb79fb59bfdcb",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=5MLb3cLCJY": {
    "title": "Adaptive Wavelet Transformer Network for 3D Shape Representation Learning",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "0dd29e0907722e9957681cd5f46ceb4fcd578536",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=TySnJ-0RdKI": {
    "title": "Backdoor Defense via Decoupling the Training Process",
    "abstract": "Recent studies have revealed that deep neural networks (DNNs) are vulnerable to backdoor attacks, where attackers embed hidden backdoors in the DNN model by poisoning a few training samples. The attacked model behaves normally on benign samples, whereas its prediction will be maliciously changed when the backdoor is activated. We reveal that poisoned samples tend to cluster together in the feature space of the attacked DNN model, which is mostly due to the endto-end supervised training paradigm. Inspired by this observation, we propose a novel backdoor defense via decoupling the original end-to-end training process into three stages. Specifically, we first learn the backbone of a DNN model via self-supervised learning based on training samples without their labels. The learned backbone will map samples with the same ground-truth label to similar locations in the feature space. Then, we freeze the parameters of the learned backbone and train the remaining fully connected layers via standard training with all (labeled) training samples. Lastly, to further alleviate side-effects of poisoned samples in the second stage, we remove labels of some ‘low-credible' samples determined based on the learned model and conduct a semi-supervised fine-tuning of the whole model. Extensive experiments on multiple benchmark datasets and DNN models verify that the proposed defense is effective in reducing backdoor threats while preserving high accuracy in predicting benign samples. Our code is available at https://github.com/SCLBD/DBD",
    "volume": "main",
    "checked": true,
    "id": "6c20a12376619a3119e53202692b091635ff03c5",
    "citation_count": 19
  },
  "https://openreview.net/forum?id=VqzXzA9hjaX": {
    "title": "Optimizer Amalgamation",
    "abstract": "Selecting an appropriate optimizer for a given problem is of major interest for researchers and practitioners. Many analytical optimizers have been proposed using a variety of theoretical and empirical approaches; however, none can offer a universal advantage over other competitive optimizers. We are thus motivated to study a new problem named Optimizer Amalgamation: how can we best combine a pool of \"teacher\" optimizers into a single \"student\" optimizer that can have stronger problem-specific performance? In this paper, we draw inspiration from the field of \"learning to optimize\" to use a learnable amalgamation target. First, we define three differentiable amalgamation mechanisms to amalgamate a pool of analytical optimizers by gradient descent. Then, in order to reduce variance of the amalgamation process, we also explore methods to stabilize the amalgamation process by perturbing the amalgamation target. Finally, we present experiments showing the superiority of our amalgamated optimizer compared to its amalgamated components and learning to optimize baselines, and the efficacy of our variance reducing perturbations. Our code and pre-trained models are publicly available at http://github.com/VITA-Group/OptimizerAmalgamation",
    "volume": "main",
    "checked": true,
    "id": "cb669ae378861bae9ca1ca64df361c29c6f5f576",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=ccWaPGl9Hq": {
    "title": "Towards Deployment-Efficient Reinforcement Learning: Lower Bound and Optimality",
    "abstract": "Deployment efficiency is an important criterion for many real-world applications of reinforcement learning (RL). Despite the community's increasing interest, there lacks a formal theoretical formulation for the problem. In this paper, we propose such a formulation for deployment-efficient RL (DE-RL) from an \"optimization with constraints\" perspective: we are interested in exploring an MDP and obtaining a near-optimal policy within minimal deployment complexity, whereas in each deployment the policy can sample a large batch of data. Using finite-horizon linear MDPs as a concrete structural model, we reveal the fundamental limit in achieving deployment efficiency by establishing information-theoretic lower bounds, and provide algorithms that achieve the optimal deployment efficiency. Moreover, our formulation for DE-RL is flexible and can serve as a building block for other practically relevant settings; we give \"Safe DE-RL\" and \"Sample-Efficient DE-RL\" as two examples, which may be worth future investigation",
    "volume": "main",
    "checked": true,
    "id": "71a3d26326e73489b8756bc73e93dd312eeb4d35",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=8H5bpVwvt5": {
    "title": "AdaRL: What, Where, and How to Adapt in Transfer Reinforcement Learning",
    "abstract": "Most approaches in reinforcement learning (RL) are data-hungry and specific to fixed environments. In this paper, we propose a principled framework for adaptive RL, called AdaRL, that adapts reliably to changes across domains. Specifically, we construct a generative environment model for the structural relationships among variables in the system and embed the changes in a compact way, which provides a clear and interpretable picture for locating what and where the changes are and how to adapt. Based on the environment model, we characterize a minimal set of representations, including both domain-specific factors and domain-shared state representations, that suffice for reliable and low-cost transfer. Moreover, we show that by explicitly leveraging a compact representation to encode changes, we can adapt the policy with only a few samples without further policy optimization in the target domain. We illustrate the efficacy of AdaRL through a series of experiments that allow for changes in different components of Cartpole and Atari games",
    "volume": "main",
    "checked": true,
    "id": "924656f720c47d7bc215b522c6837a708f54fc45",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=ZU-zFnTum1N": {
    "title": "Bregman Gradient Policy Optimization",
    "abstract": "In this paper, we design a novel Bregman gradient policy optimization framework for reinforcement learning based on Bregman divergences and momentum techniques. Specifically, we propose a Bregman gradient policy optimization (BGPO) algorithm based on the basic momentum technique and mirror descent iteration. At the same time, we present an accelerated Bregman gradient policy optimization (VR-BGPO) algorithm based on a momentum variance-reduced technique. Moreover, we introduce a convergence analysis framework for our Bregman gradient policy optimization under the nonconvex setting. Specifically, we prove that BGPO achieves the sample complexity of Õ( −4) for finding -stationary point only requiring one trajectory at each iteration, and VR-BGPO reaches the best known sample complexity of Õ( −3) for finding an -stationary point, which also only requires one trajectory at each iteration. In particular, by using different Bregman divergences, our methods unify many existing policy optimization algorithms and their new variants such as the existing (variance-reduced) policy gradient algorithms and (variance-reduced) natural policy gradient algorithms. Extensive experimental results on multiple reinforcement learning tasks demonstrate the efficiency of our new algorithms",
    "volume": "main",
    "checked": true,
    "id": "8cca25bf5259f86dcb8c7999c3ab743558fdac0f",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=DesNW4-5ai9": {
    "title": "Transferable Adversarial Attack based on Integrated Gradients",
    "abstract": "The vulnerability of deep neural networks to adversarial examples has drawn tremendous attention from the community. Three approaches, optimizing standard objective functions, exploiting attention maps, and smoothing decision sur-faces, are commonly used to craft adversarial examples. By tightly integrating the three approaches, we propose a new and simple algorithm named Transferable Attack based on Integrated Gradients (TAIG) in this paper, which can ﬁnd highly transferable adversarial examples for black-box attacks. Unlike previous methods using multiple computational terms or combining with other methods, TAIG integrates the three approaches into one single term. Two versions of TAIG that compute their integrated gradients on a straight-line path and a random piecewise linear path are studied. Both versions offer strong transferability and can seam-lessly work together with the previous methods. Experimental results demonstrate that TAIG outperforms the state-of-the-art methods. 1",
    "volume": "main",
    "checked": true,
    "id": "848db0c9842cc66379ec7975ca535adc8d90c92d",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=mdUYT5QV0O": {
    "title": "Training Structured Neural Networks Through Manifold Identification and Variance Reduction",
    "abstract": "This paper proposes an algorithm, RMDA, for training neural networks (NNs) with a regularization term for promoting desired structures. RMDA does not incur computation additional to proximal SGD with momentum, and achieves variance reduction without requiring the objective function to be of the finite-sum form. Through the tool of manifold identification from nonlinear optimization, we prove that after a finite number of iterations, all iterates of RMDA possess a desired structure identical to that induced by the regularizer at the stationary point of asymptotic convergence, even in the presence of engineering tricks like data augmentation that complicate the training process. Experiments on training NNs with structured sparsity confirm that variance reduction is necessary for such an identification, and show that RMDA thus significantly outperforms existing methods for this task. For unstructured sparsity, RMDA also outperforms a state-of-the-art pruning method, validating the benefits of training structured NNs through regularization. Implementation of RMDA is available at https://www.github.com/zihsyuan1214/rmda",
    "volume": "main",
    "checked": true,
    "id": "458a5aaf98a159e8c0d1280b0e034e3864000375",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=tT9t_ZctZRL": {
    "title": "Towards Deepening Graph Neural Networks: A GNTK-based Optimization Perspective",
    "abstract": "Graph convolutional networks (GCNs) and their variants have achieved great suc-cess in dealing with graph-structured data. Nevertheless, it is well known that deep GCNs suffer from the over-smoothing problem, where node representations tend to be indistinguishable as more layers are stacked up. The theoretical research to date on deep GCNs has focused primarily on expressive power rather than trainability, an optimization perspective. Compared to expressivity, trainability attempts to address a more fundamental question: Given a sufﬁciently expressive space of models, can we successfully ﬁnd a good solution via gradient descent-based optimizers? This work ﬁlls this gap by exploiting the Graph Neural Tangent Kernel (GNTK), which governs the optimization trajectory under gradient descent for wide GCNs. We formulate the asymptotic behaviors of GNTK in the large depth, which enables us to reveal the dropping trainability of wide and deep GCNs at an exponential rate in the optimization process. Additionally, we extend our theoretical framework to analyze residual connection-based techniques, which are found to be merely able to mitigate the exponential decay of trainability mildly. Inspired by our theoretical insights on trainability, we propose Critical DropEdge, a connectivity-aware and graph-adaptive sampling method, to alleviate the exponential decay problem more fundamentally. Experimental eval-uation in ﬁnitely-wide and Our experiments show using proposed outperform in large depth. can slow down the rate, but are unable to solve the exponential decay problem in essence. To overcome the trainability loss problem, we further propose Critical DropEdge illuminated by our theoretical framework. The experimental results conﬁrm that our method can mitigate the trainability problem of deep GCNs. Future research directions include designing a critical node-centric method so as to make better use of node information",
    "volume": "main",
    "checked": true,
    "id": "c1cfc08ea4daff9f2d989b7021162c59b45fe07f",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=sA4qIu3zv6v": {
    "title": "Towards General Function Approximation in Zero-Sum Markov Games",
    "abstract": "This paper considers two-player zero-sum finite-horizon Markov games with simultaneous moves. The study focuses on the challenging settings where the value function or the model is parameterized by general function classes. Provably efficient algorithms for both decoupled and coordinated settings are developed. In the decoupled setting where the agent controls a single player and plays against an arbitrary opponent, we propose a new model-free algorithm. The sample complexity is governed by the Minimax Eluder dimension—a new dimension of the function class in Markov games. As a special case, this method improves the state-of-the-art algorithm by a √ d factor in the regret when the reward function and transition kernel are parameterized with d-dimensional linear features. In the coordinated setting where both players are controlled by the agent, we propose a model-based algorithm and a modelfree algorithm. In the model-based algorithm, we prove that sample complexity can be bounded by a generalization of Witness rank to Markov games. The model-free algorithm enjoys a √ K-regret upper bound where K is the number of episodes",
    "volume": "main",
    "checked": true,
    "id": "eb46163d767f7de1d3de725909b73d04a1eefd68",
    "citation_count": 16
  },
  "https://openreview.net/forum?id=I2Hw58KHp8O": {
    "title": "Improving Non-Autoregressive Translation Models Without Distillation",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "c207e78d44d14597e007272800ef9cc70988a446",
    "citation_count": 12
  },
  "https://openreview.net/forum?id=-ngwPqanCEZ": {
    "title": "Representation-Agnostic Shape Fields",
    "abstract": "3D shape analysis has been widely explored in the era of deep learning. Numerous models have been developed for various 3D data representation formats, e.g., MeshCNN for meshes, PointNet for point clouds and VoxNet for voxels. In this study, we present Representation-Agnostic Shape Fields (RASF), a generalizable and computation-efficient shape embedding module for 3D deep learning. RASF is implemented with a learnable 3D grid with multiple channels to store local geometry. Based on RASF, shape embeddings for various 3D shape representations (point clouds, meshes and voxels) are retrieved by coordinate indexing. While there are multiple ways to optimize the learnable parameters of RASF, we provide two effective schemes among all in this paper for RASF pre-training: shape reconstruction and normal estimation. Once trained, RASF becomes a plug-and-play performance booster with negligible cost. Extensive experiments on diverse 3D representation formats, networks and applications, validate the universal effectiveness of the proposed RASF. Code and pre-trained models are publicly available1",
    "volume": "main",
    "checked": true,
    "id": "27910a4e2da98c11f639259e6d8d61f122513756",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=izj68lUcBpt": {
    "title": "TAda! Temporally-Adaptive Convolutions for Video Understanding",
    "abstract": "Spatial convolutions1 are widely used in numerous deep video models. It fundamentally assumes spatio-temporal invariance, i.e., using shared weights for every location in different frames. This work presents Temporally-Adaptive Convolutions (TAdaConv) for video understanding2, which shows that adaptive weight calibration along the temporal dimension is an efficient way to facilitate modelling complex temporal dynamics in videos. Specifically, TAdaConv empowers the spatial convolutions with temporal modelling abilities by calibrating the convolution weights for each frame according to its local and global temporal context. Compared to previous temporal modelling operations, TAdaConv is more efficient as it operates over the convolution kernels instead of the features, whose dimension is an order of magnitude smaller than the spatial resolutions. Further, the kernel calibration brings an increased model capacity. We construct TAda2D networks by replacing the 2D convolutions in ResNet with TAdaConv, which leads to at least on par or better performance compared to state-of-the-art approaches on multiple video action recognition and localization benchmarks. We also demonstrate that as a readily plug-in operation with negligible computation overhead, TAdaConv can effectively improve many existing video models with a convincing margin",
    "volume": "main",
    "checked": true,
    "id": "2054d2831aba2418220b119d1480a2a5c6abcb50",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=TBpg4PnXhYH": {
    "title": "SPIRAL: Self-supervised Perturbation-Invariant Representation Learning for Speech Pre-Training",
    "abstract": "We introduce a new approach for speech pre-training named SPIRAL which works by learning denoising representation of perturbed data in a teacher-student framework. Specifically, given a speech utterance, we first feed the utterance to a teacher network to obtain corresponding representation. Then the same utterance is perturbed and fed to a student network. The student network is trained to output representation resembling that of the teacher. At the same time, the teacher network is updated as moving average of student's weights over training steps. In order to prevent representation collapse, we apply an in-utterance contrastive loss as pre-training objective and impose position randomization on the input to the teacher. SPIRAL achieves competitive or better results compared to state-of-theart speech pre-training method wav2vec 2.0, with significant reduction of training cost (80% for BASE model, 65% for LARGE model). Furthermore, we address the problem of noise-robustness that is critical to real-world speech applications. We propose multi-condition pre-training by perturbing the student's input with various types of additive noise. We demonstrate that multi-condition pre-trained SPIRAL models are more robust to noisy speech (9.0% 13.3% relative word error rate reduction on real noisy test data), compared to applying multi-condition training solely in the fine-tuning stage. Source code is available 1",
    "volume": "main",
    "checked": true,
    "id": "0228d04512e04306ed5971117a4e07d11df458b8",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=ar92oEosBIg": {
    "title": "Graph Neural Network Guided Local Search for the Traveling Salesperson Problem",
    "abstract": "Solutions to the Traveling Salesperson Problem (TSP) have practical applications to processes in transportation, logistics, and automation, yet must be computed with minimal delay to satisfy the real-time nature of the underlying tasks. How-ever, solving large TSP instances quickly without sacriﬁcing solution quality remains challenging for current approximate algorithms. To close this gap, we present a hybrid data-driven approach for solving the TSP based on Graph Neural Networks (GNNs) and Guided Local Search (GLS). Our model predicts the regret of including each edge of the problem graph in the solution; GLS uses these predictions in conjunction with the original problem graph to ﬁnd solutions. Our experiments demonstrate that this approach converges to optimal solutions at a faster rate than three recent learning based approaches for the TSP. Notably, we reduce the mean optimality gap on the 100-node problem set from 1.534% to 0.705%, a 2 × improvement. When generalizing from 20-node instances to the 100-node problem set, we reduce the optimality gap from 18.845% to 2.622%, a 7 × improvement",
    "volume": "main",
    "checked": true,
    "id": "7e827e96b96e4c359fd3636b12c925c0ee6b9c4e",
    "citation_count": 12
  },
  "https://openreview.net/forum?id=r5qumLiYwf9": {
    "title": "MaGNET: Uniform Sampling from Deep Generative Network Manifolds Without Retraining",
    "abstract": "Deep Generative Networks (DGNs) are extensively employed in Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and their variants to approximate the data manifold, and data distribution on that manifold. However, training samples are often obtained based on preferences, costs, or convenience producing artifacts in the empirical data distribution e.g., the large fraction of smiling faces in the CelebA dataset or the large fraction of dark-haired individuals in FFHQ. These inconsistencies will be reproduced when sampling from the trained DGN, which has far-reaching potential implications for fairness, data augmentation, anomaly detection, domain adaptation, and beyond. In response, we develop a differential geometry based sampler -coined MaGNETthat, given any trained DGN, produces samples that are uniformly distributed on the learned manifold. We prove theoretically and empirically that our technique produces a uniform distribution on the manifold regardless of the training set distribution. We perform a range of experiments on various datasets and DGNs. One of them considers the state-of-the-art StyleGAN2 trained on FFHQ dataset, where uniform sampling via MaGNET increases distribution precision and recall by 4.1% & 3.0% and decreases gender bias by 41.2%, without requiring labels or retraining. Figure 1: Random batches of StyleGAN2 samples with 1024 × 1024 resolution, generated using standard sampling (left), uniform sampling via MaGNET on the learned pixel-space manifold (middle), and uniform sampling on the style-space manifold (right) of the same model. MaGNET sampling yields a higher number of young faces, better gender balance, and greater background/accessory variation, without the need for labels or retraining. Images are sorted by gender-age and color coded red-green (female-male) according to Microsoft Cognitive API predictions. Larger batches of images and attribute distributions are furnished in Appendix E. 1 ar X iv :2 11 0. 08 00 9v 1 [ cs .L G ] 1 5 O ct 2 02 1 Under review as a conference paper at ICLR 2022",
    "volume": "main",
    "checked": true,
    "id": "62f80fffbdc8b58870a2743cc24f931346406b0c",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=fPhKeld3Okz": {
    "title": "Gradient Step Denoiser for convergent Plug-and-Play",
    "abstract": "Plug-and-Play (PnP) methods constitute a class of iterative algorithms for imaging problems where regularization is performed by an off-the-shelf denoiser. Although PnP methods can lead to tremendous visual performance for various image problems, the few existing convergence guarantees are based on unrealistic (or suboptimal) hypotheses on the denoiser, or limited to strongly convex data-fidelity terms. We propose a new type of PnP method, based on half-quadratic splitting, for which the denoiser is realized as a gradient descent step on a functional parameterized by a deep neural network. Exploiting convergence results for proximal gradient descent algorithms in the nonconvex setting, we show that the proposed PnP algorithm is a convergent iterative scheme that targets stationary points of an explicit global functional. Besides, experiments show that it is possible to learn such a deep denoiser while not compromising the performance in comparison to other state-of-the-art deep denoisers used in PnP schemes. We apply our proximal gradient algorithm to various ill-posed inverse problems, e.g. deblurring, superresolution and inpainting. For all these applications, numerical results empirically confirm the convergence results. Experiments also show that this new algorithm reaches state-of-the-art performance, both quantitatively and qualitatively",
    "volume": "main",
    "checked": true,
    "id": "854926f19cd2426fe6b0bef5ed834c83fd56f97b",
    "citation_count": 13
  },
  "https://openreview.net/forum?id=MMAeCXIa89": {
    "title": "$\\pi$BO: Augmenting Acquisition Functions with User Beliefs for Bayesian Optimization",
    "abstract": "Bayesian optimization (BO) has become an established framework and popular tool for hyperparameter optimization (HPO) of machine learning (ML) algorithms. While known for its sample-efficiency, vanilla BO can not utilize readily available prior beliefs the practitioner has on the potential location of the optimum. Thus, BO disregards a valuable source of information, reducing its appeal to ML practitioners. To address this issue, we propose πBO, an acquisition function generalization which incorporates prior beliefs about the location of the optimum in the form of a probability distribution, provided by the user. In contrast to previous approaches, πBO is conceptually simple and can easily be integrated with existing libraries and many acquisition functions. We provide regret bounds when πBO is applied to the common Expected Improvement acquisition function and prove convergence at regular rates independently of the prior. Further, our experiments show that πBO outperforms competing approaches across a wide suite of benchmarks and prior characteristics. We also demonstrate that πBO improves on the state-of-theart performance for a popular deep learning task, with a 12.5× time-to-accuracy speedup over prominent BO approaches",
    "volume": "main",
    "checked": true,
    "id": "5abbeaf5edb11e5d3026c143da94650b72b57e7b",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=2-mkiUs9Jx7": {
    "title": "Stein Latent Optimization for Generative Adversarial Networks",
    "abstract": "Generative adversarial networks (GANs) with clustered latent spaces can perform conditional generation in a completely unsupervised manner. In the real world, the salient attributes of unlabeled data can be imbalanced. However, most of existing unsupervised conditional GANs cannot cluster attributes of these data in their latent spaces properly because they assume uniform distributions of the attributes. To address this problem, we theoretically derive Stein latent optimization that provides reparameterizable gradient estimations of the latent distribution parameters assuming a Gaussian mixture prior in a continuous latent space. Structurally, we introduce an encoder network and novel unsupervised conditional contrastive loss to ensure that data generated from a single mixture component represent a single attribute. We confirm that the proposed method, named Stein Latent Optimization for GANs (SLOGAN), successfully learns balanced or imbalanced attributes and achieves state-of-the-art unsupervised conditional generation performance even in the absence of attribute information (e.g., the imbalance ratio). Moreover, we demonstrate that the attributes to be learned can be manipulated using a small amount of probe data",
    "volume": "main",
    "checked": true,
    "id": "264472cf2a687c2b7d247a69a891be22a096c7ee",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=d71n4ftoCBy": {
    "title": "FedPara: Low-rank Hadamard Product for Communication-Efficient Federated Learning",
    "abstract": "In this work, we propose a communication-efficient parameterization, FedPara, for federated learning (FL) to overcome the burdens on frequent model uploads and downloads. Our method re-parameterizes weight parameters of layers using low-rank weights followed by the Hadamard product. Compared to the conventional low-rank parameterization, our FedPara method is not restricted to lowrank constraints, and thereby it has a far larger capacity. This property enables to achieve comparable performance while requiring 3 to 10 times lower communication costs than the model with the original layers, which is not achievable by the traditional low-rank methods. The efficiency of our method can be further improved by combining with other efficient FL optimizers. In addition, we extend our method to a personalized FL application, pFedPara, which separates parameters into global and local ones. We show that pFedPara outperforms competing personalized FL methods with more than three times fewer parameters",
    "volume": "main",
    "checked": true,
    "id": "95e960583e96e78873397a502f47f02e948819be",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=J7b4BCtDm4": {
    "title": "How to deal with missing data in supervised deep learning?",
    "abstract": "The issue of missing data in supervised learning has been largely overlooked, especially in the deep learning community. We investigate strategies to adapt neural architectures to handle missing values. Here, we focus on regression and classiﬁcation problems where the features are assumed to be missing at random. Of particular interest are schemes that allow to reuse as-is a neural discriminative architecture. One scheme involves imputing the missing values with learnable constants. We propose a second novel approach that leverages recent advances in deep generative modelling. More precisely, a deep latent variable model can be learned jointly with the discriminative model, using importance-weighted variational inference in an end-to-end way. This hybrid approach, which mimics multiple imputation, also allows to impute the data, by relying on both the discriminative and the generative model. We also discuss ways of using a pre-trained generative model to train the discriminative one. In domains where powerful deep generative models are available, the hybrid approach leads to large performance gains",
    "volume": "main",
    "checked": true,
    "id": "acae90bc6bcde932e32a5fedda90b1f5b750e436",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=j3krplz_4w6": {
    "title": "Fooling Explanations in Text Classifiers",
    "abstract": "State-of-the-art text classiﬁcation models are becoming increasingly reliant on deep neural networks (DNNs). Due to their black-box nature, faithful and robust explanation methods need to accompany classiﬁers for deployment in real-life scenarios. However, it has been shown in vision applications that explanation methods are susceptible to local, imperceptible perturbations that can signiﬁcantly alter the explanations without changing the predicted classes. We show here that the existence of such perturbations extends to text classiﬁers as well. Speciﬁ-cally, we introduce T EXT E XPLANATION F OOLER (TEF), a novel explanation attack algorithm that alters text input samples imperceptibly so that the outcome of widely-used explanation methods changes considerably while leaving classiﬁer predictions unchanged. We evaluate the performance of the attribution robustness estimation performance in TEF on ﬁve sequence classiﬁcation datasets, utilizing three DNN architectures and three transformer architectures for each dataset. TEF can signiﬁcantly decrease the correlation between unchanged and perturbed input attributions, which shows that all models and explanation methods are susceptible to TEF perturbations. Moreover, we evaluate how the perturbations transfer to other model architectures and attribution methods, and show that TEF",
    "volume": "main",
    "checked": true,
    "id": "43ad62d1e64f2301eb2294ab79b3979a470e7ed4",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=fILj7WpI-g": {
    "title": "Perceiver IO: A General Architecture for Structured Inputs &amp; Outputs",
    "abstract": "extensively tune our models for efﬁciency on image classiﬁcation – the primary focus of this work is generality, rather than speed on images – Perceiver IO uses comparable FLOPs to attention-based image classiﬁcation models, especially for the more compact conﬁguration B pretrained on JFT. The positional encoding does not signiﬁcantly change model FLOPs",
    "volume": "main",
    "checked": true,
    "id": "9933a5af7895354087baf6c96b64dc8a8973eaed",
    "citation_count": 129
  },
  "https://openreview.net/forum?id=HCelXXcSEuH": {
    "title": "Doubly Adaptive Scaled Algorithm for Machine Learning Using Second-Order Information",
    "abstract": "We present a novel adaptive optimization algorithm for large-scale machine learning problems. Equipped with a low-cost estimate of local curvature and Lipschitz smoothness, our method dynamically adapts the search direction and step-size. The search direction contains gradient information preconditioned by a well-scaled diagonal preconditioning matrix that captures the local curvature information. Our methodology does not require the tedious task of learning rate tuning, as the learning rate is updated automatically without adding an extra hyperparameter. We provide convergence guarantees on a comprehensive collection of optimization problems, including convex, strongly convex, and nonconvex problems, in both deterministic and stochastic regimes. We also conduct an extensive empirical evaluation on standard machine learning problems, justifying our algorithm's versatility and demonstrating its strong performance compared to other start-of-theart first-order and second-order methods",
    "volume": "main",
    "checked": true,
    "id": "31f79d616785e4ce096953d6524b1032031cc82f",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=MljXVdp4A3N": {
    "title": "Know Your Action Set: Learning Action Relations for Reinforcement Learning",
    "abstract": "Intelligent agents can solve tasks in various ways depending on their available set of actions. However, conventional reinforcement learning (RL) assumes a ﬁxed action set. This work asserts that tasks with varying action sets require reasoning of the relations between the available actions. For instance, taking a nail-action in a repair task is meaningful only if a hammer-action is also available. To learn and utilize such action relations, we propose a novel policy architecture consisting of a graph attention network over the available actions. We show that our model makes informed action decisions by correctly attending to other related actions in both value-based and policy-based RL. Consequently, it outperforms non-relational architectures on applications where the action space often varies, such as recommender systems and physical reasoning with tools and skills. 1",
    "volume": "main",
    "checked": true,
    "id": "abeb9aa8b4ec2fd1a8a8908a4b0c18ee7f1fa117",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=Te5ytkqsnl": {
    "title": "Missingness Bias in Model Debugging",
    "abstract": "Missingness, or the absence of features from an input, is a concept fundamental to many model debugging tools. However, in computer vision, pixels cannot simply be removed from an image. One thus tends to resort to heuristics such as blacking out pixels, which may in turn introduce bias into the debugging process. We study such biases and, in particular, show how transformer-based architectures can enable a more natural implementation of missingness, which side-steps these issues and improves the reliability of model debugging in practice.1",
    "volume": "main",
    "checked": true,
    "id": "39471af34bff3d18b6bffe395502f0493693455a",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=rrWeE9ZDw_": {
    "title": "Autonomous Learning of Object-Centric Abstractions for High-Level Planning",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "c5c4c1b71132f8ac34bb9a6dad1dfe979e69902d",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=qaxhBG1UUaS": {
    "title": "GPT-Critic: Offline Reinforcement Learning for End-to-End Task-Oriented Dialogue Systems",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "49f8a31e13998ae431dde8092973e6bd0f8385be",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=vfsRB5MImo9": {
    "title": "Towards Continual Knowledge Learning of Language Models",
    "abstract": "Large Language Models (LMs) are known to encode world knowledge in their parameters as they pretrain on a vast amount of web corpus, which is often utilized for performing knowledge-dependent downstream tasks such as question answering, fact-checking, and open dialogue. In real-world scenarios, the world knowledge stored in the LMs can quickly become outdated as the world changes, but it is non-trivial to avoid catastrophic forgetting and reliably acquire new knowledge while preserving invariant knowledge. To push the community towards better maintenance of ever-changing LMs, we formulate a new continual learning (CL) problem called Continual Knowledge Learning (CKL). We construct a new benchmark and metric to quantify the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. We adopt applicable recent methods from literature to create several strong baselines. Through extensive experiments, we find that CKL exhibits unique challenges that are not addressed in previous CL setups, where parameter expansion is necessary to reliably retain and learn knowledge simultaneously. By highlighting the critical causes of knowledge forgetting, we show that CKL is a challenging and important problem that helps us better understand and train ever-changing LMs.1",
    "volume": "main",
    "checked": true,
    "id": "ce828f9986b196308a3e40b1de58af1e8e68d728",
    "citation_count": 19
  },
  "https://openreview.net/forum?id=1L0C5ROtFp": {
    "title": "Filtered-CoPhy: Unsupervised Learning of Counterfactual Physics in Pixel Space",
    "abstract": "Learning causal relationships in high-dimensional data (images, videos) is a hard task, as they are often deﬁned on low-dimensional manifolds and must be extracted from complex signals dominated by appearance, lighting, textures and also spurious correlations in the data. We present a method for learning counterfactual reasoning of physical processes in pixel space, which requires the prediction of the impact of interventions on initial conditions. Going beyond the identiﬁcation of structural relationships, we deal with the challenging problem of forecasting raw video over long horizons. Our method does not require the knowledge or supervision of any ground truth positions or other object or scene properties. Our model learns and acts on a suitable hybrid latent representation based on a combination of dense features, sets of 2D keypoints and an additional latent vector per keypoint. We show that this better captures the dynamics of physical processes than purely dense or sparse representations. We introduce a new challenging and carefully designed counterfactual benchmark for predictions in pixel space and outperform strong baselines in physics-inspired ML and video prediction",
    "volume": "main",
    "checked": true,
    "id": "b0b51bf9267baf50c5b415bf9e206779228759db",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=T8wHz4rnuGL": {
    "title": "RotoGrad: Gradient Homogenization in Multitask Learning",
    "abstract": "Multitask learning is being increasingly adopted in applications domains like computer vision and reinforcement learning. However, optimally exploiting its advantages remains a major challenge due to the effect of negative transfer. Previous works have tracked down this issue to the disparities in gradient magnitudes and directions across tasks when optimizing the shared network parameters. While recent work has acknowledged that negative transfer is a two-fold problem, existing approaches fall short. These methods only focus on either homogenizing the gradient magnitude across tasks; or greedily change the gradient directions, overlooking future conflicts. In this work, we introduce RotoGrad, an algorithm that tackles negative transfer as a whole: it jointly homogenizes gradient magnitudes and directions, while ensuring training convergence. We show that RotoGrad outperforms competing methods in complex problems, including multi-label classification in CelebA and computer vision tasks in the NYUv2 dataset. A Pytorch implementation can be found in https://github.com/adrianjav/rotograd",
    "volume": "main",
    "checked": true,
    "id": "a8f97a65f1cb6106b83d3c21a1dad5fa005aee43",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=iMH1e5k7n3L": {
    "title": "Spike-inspired rank coding for fast and accurate recurrent neural networks",
    "abstract": "Biological spiking neural networks (SNNs) can temporally encode information in their outputs, e.g. in the rank order in which neurons fire, whereas artificial neural networks (ANNs) conventionally do not. As a result, models of SNNs for neuromorphic computing are regarded as potentially more rapid and efficient than ANNs when dealing with temporal input. On the other hand, ANNs are simpler to train, and usually achieve superior performance. Here we show that temporal coding such as rank coding (RC) inspired by SNNs can also be applied to conventional ANNs such as LSTMs, and leads to computational savings and speedups. In our RC for ANNs, we apply backpropagation through time using the standard real-valued activations, but only from a strategically early time step of each sequential input example, decided by a threshold-crossing event. Learning then incorporates naturally also when to produce an output, without other changes to the model or the algorithm. Both the forward and the backward training pass can be significantly shortened by skipping the remaining input sequence after that first event. RC-training also significantly reduces time-to-insight during inference, with a minimal decrease in accuracy. The desired speed-accuracy trade-off is tunable by varying the threshold or a regularization parameter that rewards output entropy. We demonstrate these in two toy problems of sequence classification, and in a temporally-encoded MNIST dataset where our RC model achieves 99.19% accuracy after the first input time-step, outperforming the state of the art in temporal coding with SNNs, as well as in spoken-word classification of Google Speech Commands, outperforming non-RC-trained early inference with LSTMs",
    "volume": "main",
    "checked": true,
    "id": "2034cda7a93644b43808f4dbebe72ed02835ad67",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=Ix_mh42xq5w": {
    "title": "PSA-GAN: Progressive Self Attention GANs for Synthetic Time Series",
    "abstract": "Realistic synthetic time series data of sufﬁcient length enables practical applications in time series modeling tasks, such as forecasting, but remains a challenge. In this paper, we present PSA-GAN , a generative adversarial network (GAN) that generates long time series samples of high quality using progressive growing of GANs and self-attention. We show that PSA-GAN can be used to reduce the error in several downstream forecasting tasks over baselines that only use real data. We also introduce a Frechet Inception distance-like score for time series, Context-FID, assessing the quality of synthetic time series samples. We ﬁnd that Context-FID is indicative for downstream performance. Therefore, Context-FID could be a useful tool to develop time series GAN models",
    "volume": "main",
    "checked": true,
    "id": "68043cccd2b620d44d6ffd4983b55060966acb23",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=N9W24a4zU": {
    "title": "Steerable Partial Differential Operators for Equivariant Neural Networks",
    "abstract": "Recent work in equivariant deep learning bears strong similarities to physics. Fields over a base space are fundamental entities in both subjects, as are equivariant maps between these fields. In deep learning, however, these maps are usually defined by convolutions with a kernel, whereas they are partial differential operators (PDOs) in physics. Developing the theory of equivariant PDOs in the context of deep learning could bring these subjects even closer together and lead to a stronger flow of ideas. In this work, we derive a G-steerability constraint that completely characterizes when a PDO between feature vector fields is equivariant, for arbitrary symmetry groups G. We then fully solve this constraint for several important groups. We use our solutions as equivariant drop-in replacements for convolutional layers and benchmark them in that role. Finally, we develop a framework for equivariant maps based on Schwartz distributions that unifies classical convolutions and differential operators and gives insight about the relation between the two. Figure 1: A vector field (left) can be mapped to a scalar field (right) by applying certain partial differential operators (PDOs), such as the Laplacian of the divergence and the 2D curl. Such a PDO from a 2D vector to a scalar field can be represented as a 2× 1 matrix, where each of the two entries is a one-dimensional PDO that acts on one of the two components of the vector field. Similarly, matrices of PDOs with different dimensions map between other types of fields. Our goal is to find all PDOs for which this map becomes equivariant, for arbitrary types of fields. For the implementation, we will later discretize PDOs as stencils (middle)",
    "volume": "main",
    "checked": true,
    "id": "8d1118549b20c451abd0f412eb48a925922d98f8",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=lyLVzukXi08": {
    "title": "Neural Variational Dropout Processes",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "1fe05706b9ac70805929b1fac1311d8f1b3d1e9f",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=Zq2G_VTV53T": {
    "title": "FastSHAP: Real-Time Shapley Value Estimation",
    "abstract": "Shapley values are widely used to explain black-box models, but they are costly to calculate because they require many model evaluations. We introduce FastSHAP, a method for estimating Shapley values in a single forward pass using a learned explainer model. FastSHAP amortizes the cost of explaining many inputs via a learning approach inspired by the Shapley value's weighted least squares characterization, and it can be trained using standard stochastic gradient optimization. We compare FastSHAP to existing estimation approaches, revealing that it generates high-quality explanations with orders of magnitude speedup",
    "volume": "main",
    "checked": true,
    "id": "cce242dfa75492b02952ad2692b1eed4dc43c8c1",
    "citation_count": 14
  },
  "https://openreview.net/forum?id=BZnnMbt0pW": {
    "title": "Promoting Saliency From Depth: Deep Unsupervised RGB-D Saliency Detection",
    "abstract": "Growing interests in RGB-D salient object detection (RGB-D SOD) have been witnessed in recent years, owing partly to the popularity of depth sensors and the rapid progress of deep learning techniques. Unfortunately, existing RGB-D SOD methods typically demand large quantity of training images being thoroughly annotated at pixel-level. The laborious and time-consuming manual annotation has become a real bottleneck in various practical scenarios. On the other hand, current unsupervised RGB-D SOD methods still heavily rely on handcrafted feature representations. This inspires us to propose in this paper a deep unsupervised RGB-D saliency detection approach, which requires no manual pixel-level annotation during training. It is realized by two key ingredients in our training pipeline. First, a depth-disentangled saliency update (DSU) framework is designed to automatically produce pseudo-labels with iterative follow-up refinements, which provides more trustworthy supervision signals for training the saliency network. Second, an attentive training strategy is introduced to tackle the issue of noisy pseudo-labels, by properly re-weighting to highlight the more reliable pseudo-labels. Extensive experiments demonstrate the superior efficiency and effectiveness of our approach in tackling the challenging unsupervised RGB-D SOD scenarios. Moreover, our approach can also be adapted to work in fully-supervised situation. Empirical studies show the incorporation of our approach gives rise to notably performance improvement in existing supervised RGB-D SOD models",
    "volume": "main",
    "checked": true,
    "id": "a0d19f43c75f231fd631babbf5d9125b4dff311c",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=WZ3yjh8coDg": {
    "title": "An Unconstrained Layer-Peeled Perspective on Neural Collapse",
    "abstract": "Neural collapse is a highly symmetric geometry of neural networks that emerges during the terminal phase of training, with profound implications on the generalization performance and robustness of the trained networks. To understand how the last-layer features and classiﬁers exhibit this recently discovered implicit bias, in this paper, we introduce a surrogate model called the unconstrained layer-peeled model (ULPM). We prove that gradient ﬂow on this model converges to critical points of a minimum-norm separation problem exhibiting neural collapse in its global minimizer. Moreover, we show that the ULPM with the cross-entropy loss has a benign global landscape for its loss function, which allows us to prove that all the critical points are strict saddle points except the global minimizers that exhibit the neural collapse phenomenon. Empirically, we show that our results also hold during the training of neural networks in real-world tasks when explicit regularization or weight decay is not used",
    "volume": "main",
    "checked": true,
    "id": "571faa33a05d0de7f634b49b72e63c9e47b5fe5f",
    "citation_count": 19
  },
  "https://openreview.net/forum?id=OUz_9TiTv9j": {
    "title": "A Zest of LIME: Towards Architecture-Independent Model Distances",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "74843350d925d60f68aaaa7ac109dccec2fd9ae7",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=gJLEXy3ySpu": {
    "title": "Almost Tight L0-norm Certified Robustness of Top-k Predictions against Adversarial Perturbations",
    "abstract": "Top-$k$ predictions are used in many real-world applications such as machine learning as a service, recommender systems, and web searches. $\\ell_0$-norm adversarial perturbation characterizes an attack that arbitrarily modifies some features of an input such that a classifier makes an incorrect prediction for the perturbed input. $\\ell_0$-norm adversarial perturbation is easy to interpret and can be implemented in the physical world. Therefore, certifying robustness of top-$k$ predictions against $\\ell_0$-norm adversarial perturbation is important. However, existing studies either focused on certifying $\\ell_0$-norm robustness of top-$1$ predictions or $\\ell_2$-norm robustness of top-$k$ predictions. In this work, we aim to bridge the gap. Our approach is based on randomized smoothing, which builds a provably robust classifier from an arbitrary classifier via randomizing an input. Our major theoretical contribution is an almost tight $\\ell_0$-norm certified robustness guarantee for top-$k$ predictions. We empirically evaluate our method on CIFAR10 and ImageNet. For instance, our method can build a classifier that achieves a certified top-3 accuracy of 69.2\\% on ImageNet when an attacker can arbitrarily perturb 5 pixels of a testing image",
    "volume": "main",
    "checked": true,
    "id": "0808bdbb02e60fd3ac3c13f454346ea47067b987",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=7inCJ3MhXt3": {
    "title": "Learning Neural Contextual Bandits through Perturbed Rewards",
    "abstract": "Thanks to the power of representation learning, neural contextual bandit algorithms demonstrate remarkable performance improvement against their classical counterparts. But because their exploration has to be performed in the entire neural network parameter space to obtain nearly optimal regret, the resulting computational cost is prohibitively high. We perturb the rewards when updating the neural network to eliminate the need of explicit exploration and the corresponding computational overhead. We prove that a Õ(d̃ √ T ) regret upper bound is still achievable under standard regularity conditions, where T is the number of rounds of interactions and d̃ is the effective dimension of a neural tangent kernel matrix. Extensive comparisons with several benchmark contextual bandit algorithms, including two recent neural contextual bandit models, demonstrate the effectiveness and computational efficiency of our proposed neural bandit algorithm",
    "volume": "main",
    "checked": true,
    "id": "efd619218ab764f78acf070a9c39adf376f0abd6",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=VNqaB1g9393": {
    "title": "Decoupled Adaptation for Cross-Domain Object Detection",
    "abstract": "Cross-domain object detection is more challenging than object classification since multiple objects exist in an image and the location of each object is unknown in the unlabeled target domain. As a result, when we adapt features of different objects to enhance the transferability of the detector, the features of the foreground and the background are easy to be confused, which may hurt the discriminability of the detector. Besides, previous methods focused on category adaptation but ignored another important part for object detection, i.e., the adaptation on bounding box regression. To this end, we propose D-adapt, namely Decoupled Adaptation, to decouple the adversarial adaptation and the training of the detector. Besides, we introduce a bounding box adaptor to improve the localization performance. Experiments show that D-adapt achieves state-of-the-art results on four crossdomain object detection tasks and yields 17% and 21% relative improvement on benchmark datasets Clipart1k and Comic2k in particular",
    "volume": "main",
    "checked": true,
    "id": "6f54d34b472c0f88fc062787d377a32244d9651e",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=YigKlMJwjye": {
    "title": "Generalized Demographic Parity for Group Fairness",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "71629202ad8ad4550520726148afe7305b7387b8",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=DSQHjibtgKR": {
    "title": "Online Facility Location with Predictions",
    "abstract": "We provide nearly optimal algorithms for online facility location (OFL) with predictions. In OFL, n demand points arrive in order and the algorithm must irrevocably assign each demand point to an open facility upon its arrival. The objective is to minimize the total connection costs from demand points to assigned facilities plus the facility opening cost. We further assume the algorithm is additionally given for each demand point xi a natural prediction f xi , which is supposed to be the facility f xi that serves xi in the offline optimal solution. Our main result is an O(min{log nη∞ OPT , log n})-competitive algorithm where η∞ is the maximum prediction error (i.e., the distance between f xi and f opt xi ). Our algorithm overcomes the fundamental Ω( logn log logn ) lower bound of OFL (without predictions) when η∞ is small, and it still maintains O(log n) ratio even when η∞ is unbounded. Furthermore, our theoretical analysis is supported by empirical evaluations for the tradeoffs between η∞ and the competitive ratio on various real datasets of different types",
    "volume": "main",
    "checked": true,
    "id": "a54d78ea8852836225d52e6bfb8156e772bcc32a",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=HbtFCX2PLq0": {
    "title": "Churn Reduction via Distillation",
    "abstract": "In real-world systems, models are frequently updated as more data becomes available, and in addition to achieving high accuracy, the goal is to also maintain a low difference in predictions compared to the base model (i.e. predictive \"churn\"). If model retraining results in vastly different behavior, then it could cause negative effects in downstream systems, especially if this churn can be avoided with limited impact on model accuracy. In this paper, we show an equivalence between training with distillation using the base model as the teacher and training with an explicit constraint on the predictive churn. We then show that distillation performs strongly for low churn training against a number of recent baselines on a wide range of datasets and model architectures, including fully-connected networks, convolutional networks, and transformers",
    "volume": "main",
    "checked": true,
    "id": "a695e233173b89d9e400d8de0934a068af8c1f4b",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=WvOGCEAQhxl": {
    "title": "Assessing Generalization of SGD via Disagreement",
    "abstract": "We empirically show that the test error of deep networks can be estimated by training the same architecture on the same training set but with two different runs of Stochastic Gradient Descent (SGD), and then measuring the disagreement rate between the two networks on unlabeled test data . This builds on — and is a stronger version of — the observation in Nakkiran & Bansal (2020), which requires the runs to be on separate training sets. We further theoretically show that this peculiar phenomenon arises from the well-calibrated nature of ensembles of SGD-trained models. This ﬁnding not only provides a simple empirical measure to directly predict the test error using unlabeled test data, but also establishes a new conceptual connection between generalization and calibration",
    "volume": "main",
    "checked": true,
    "id": "7f8b0cd4ef0ee1fb9970d83c71b765fc67f35887",
    "citation_count": 26
  },
  "https://openreview.net/forum?id=6y2KBh-0Fd9": {
    "title": "Revisiting flow generative models for Out-of-distribution detection",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "31278cec03fa87b45a02f43d275dd92b678fbc5b",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=cBu4ElJfneV": {
    "title": "GiraffeDet: A Heavy-Neck Paradigm for Object Detection",
    "abstract": "In conventional object detection frameworks, a backbone body inherited from image recognition models extracts deep latent features and then a neck module fuses these latent features to capture information at different scales. As the resolution in object detection is much larger than in image recognition, the computational cost of the backbone often dominates the total inference cost. This heavy-backbone design paradigm is mostly due to the historical legacy when transferring image recognition models to object detection rather than an end-to-end optimized design for object detection. In this work, we show that such paradigm indeed leads to sub-optimal object detection models. To this end, we propose a novel heavy-neck paradigm, GiraffeDet, a giraffe-like network for efficient object detection. The GiraffeDet uses an extremely lightweight backbone and a very deep and large neck module which encourages dense information exchange among different spatial scales as well as different levels of latent semantics simultaneously. This design paradigm allows detectors to process the high-level semantic information and lowlevel spatial information at the same priority even in the early stage of the network, making it more effective in detection tasks. Numerical evaluations on multiple popular object detection benchmarks show that GiraffeDet consistently outperforms previous SOTA models across a wide spectrum of resource constraints. The source code is available at https://github.com/jyqi/GiraffeDet",
    "volume": "main",
    "checked": true,
    "id": "2a8b79d45f39dbeb1a3af2f8ae78e169def8a2fa",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=ecH2FKaARUp": {
    "title": "An Information Fusion Approach to Learning with Instance-Dependent Label Noise",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "7810007373538a00d1f928670d8b0d00b31a5ec2",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=9-Rfew334N": {
    "title": "Givens Coordinate Descent Methods for Rotation Matrix Learning in Trainable Embedding Indexes",
    "abstract": "Product quantization (PQ) coupled with a space rotation, is widely used in modern approximate nearest neighbor (ANN) search systems to significantly compress the disk storage for embeddings and speed up the inner product computation. Existing rotation learning methods, however, minimize quantization distortion for fixed embeddings, which are not applicable to an end-to-end training scenario where embeddings are updated constantly. In this paper, based on geometric intuitions from Lie group theory, in particular the special orthogonal group SO(n), we propose a family of block Givens coordinate descent algorithms to learn rotation matrix that are provably convergent on any convex objectives. Compared to the state-of-the-art SVD method, the Givens algorithms are much more parallelizable, reducing runtime by orders of magnitude on modern GPUs, and converge more stably according to experimental studies. They further improve upon vanilla product quantization significantly in an end-to-end training scenario",
    "volume": "main",
    "checked": true,
    "id": "48e3b328bc8ba8ce78f2a0dc3be2adddf05b76fb",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=KeI9E-gsoB": {
    "title": "Learning Curves for Gaussian Process Regression with Power-Law Priors and Targets",
    "abstract": "We study the power-law asymptotics of learning curves for Gaussian process regression (GPR). When the eigenspectrum of the prior decays with rate α and the eigenexpansion coefficients of the target function decay with rate β, we show that the generalization error behaves as Õ(nmax{ 1 α−1, 1−2β α }) with high probability over the draw of n input samples. Under similar assumptions, we show that the generalization error of kernel ridge regression (KRR) has the same asymptotics. Infinitely wide neural networks can be related to KRR with respect to the neural tangent kernel (NTK), which in several cases is known to have a power-law spectrum. Hence our methods can be applied to study the generalization error of infinitely wide neural networks. We present toy experiments demonstrating the theory",
    "volume": "main",
    "checked": true,
    "id": "90fa397436474f9b8937c185b85e83b8982b7038",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=rFbR4Fv-D6-": {
    "title": "Automated Self-Supervised Learning for Graphs",
    "abstract": "Graph self-supervised learning has gained increasing attention due to its capacity to learn expressive node representations. Many pretext tasks, or loss functions have been designed from distinct perspectives. However, we observe that different pretext tasks affect downstream tasks differently across datasets, which suggests that searching over pretext tasks is crucial for graph self-supervised learning. Different from existing works focusing on designing single pretext tasks, this work aims to investigate how to automatically leverage multiple pretext tasks effectively. Nevertheless, evaluating representations derived from multiple pretext tasks without direct access to ground truth labels makes this problem challenging. To address this obstacle, we make use of a key principle of many real-world graphs, i.e., homophily, or the principle that \"like attracts like,\" as the guidance to effectively search various self-supervised pretext tasks. We provide theoretical understanding and empirical evidence to justify the flexibility of homophily in this search task. Then we propose the AUTOSSL framework which can automatically search over combinations of various self-supervised tasks. By evaluating the framework on 7 real-world datasets, our experimental results show that AUTOSSL can significantly boost the performance on downstream tasks including node clustering and node classification compared with training under individual tasks. Code will be released at https://github.com/ChandlerBang/AutoSSL",
    "volume": "main",
    "checked": true,
    "id": "940f3234bb388adcb7e77ff887c0e5f444e70e79",
    "citation_count": 15
  },
  "https://openreview.net/forum?id=_CfpJazzXT2": {
    "title": "F8Net: Fixed-Point 8-bit Only Multiplication for Network Quantization",
    "abstract": "Neural network quantization is a promising compression technique to reduce memory footprint and save energy consumption, potentially leading to real-time inference. However, there is a performance gap between quantized and fullprecision models. To reduce it, existing quantization approaches require highprecision INT32 or full-precision multiplication during inference for scaling or dequantization. This introduces a noticeable cost in terms of memory, speed, and required energy. To tackle these issues, we present F8Net, a novel quantization framework consisting of only fixed-point 8-bit multiplication. To derive our method, we first discuss the advantages of fixed-point multiplication with different formats of fixed-point numbers and study the statistical behavior of the associated fixedpoint numbers. Second, based on the statistical and algorithmic analysis, we apply different fixed-point formats for weights and activations of different layers. We introduce a novel algorithm to automatically determine the right format for each layer during training. Third, we analyze a previous quantization algorithm— parameterized clipping activation (PACT)—and reformulate it using fixed-point arithmetic. Finally, we unify the recently proposed method for quantization finetuning and our fixed-point approach to show the potential of our method. We verify F8Net on ImageNet for MobileNet V1/V2 and ResNet18/50. Our approach achieves comparable and better performance, when compared not only to existing quantization techniques with INT32 multiplication or floating-point arithmetic, but also to the full-precision counterparts, achieving state-of-the-art performance",
    "volume": "main",
    "checked": true,
    "id": "d31a2a1b1d2378030aed23f6888bce02897e20e7",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=HndgQudNb91": {
    "title": "Learning to Downsample for Segmentation of Ultra-High Resolution Images",
    "abstract": "to adapt the sampling budget to the difﬁculty of segmented pixels/regions hence achieving optimal cost-performance trade-off. We empirically illustrate the SOTA method (Marin et al., 2019) been limited by ﬁxing downsampling locations to manual designed sampling objective, and hence motivate our end-to-end adaptive downsampling method. We illustrate simply extending the learn to downsample method from image classiﬁcation (Recasens et al., 2018) to segmentation does not work, and propose to avoid learning trivial downsampling solutions by incorporating an edge-loss to regularise the training. Although our edge-loss , despite a simpler approximation, share the same spirit with Marin et al. (2019) we demonstrate our jointly trained system generalises sampling more robustly especially when object boundaries are less informative, hence consistently leading to a better cost-performance trade-off. Our method is light weighted and can be ﬂexibly combined with the existing segmentation method without modifying the architecture",
    "volume": "main",
    "checked": true,
    "id": "7136650a9433f30ae9323ef3717ab4d7642d5662",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=LI2bhrE_2A": {
    "title": "Iterative Refinement Graph Neural Network for Antibody Sequence-Structure Co-design",
    "abstract": "Antibodies are versatile proteins that bind to pathogens like viruses and stimulate the adaptive immune system. The specificity of antibody binding is determined by complementarity-determining regions (CDRs) at the tips of these Y-shaped proteins. In this paper, we propose a generative model to automatically design the CDRs of antibodies with enhanced binding specificity or neutralization capabilities. Previous generative approaches formulate protein design as a structure-conditioned sequence generation task, assuming the desired 3D structure is given a priori. In contrast, we propose to co-design the sequence and 3D structure of CDRs as graphs. Our model unravels a sequence autoregressively while iteratively refining its predicted global structure. The inferred structure in turn guides subsequent residue choices. For efficiency, we model the conditional dependence between residues inside and outside of a CDR in a coarse-grained manner. Our method achieves superior log-likelihood on the test set and outperforms previous baselines in designing antibodies capable of neutralizing the SARS-CoV-2 virus",
    "volume": "main",
    "checked": true,
    "id": "01bacd0a631f11fd090b5cc68596fccc7d58b4e0",
    "citation_count": 32
  },
  "https://openreview.net/forum?id=PTRo58zPt3P": {
    "title": "Inductive Relation Prediction Using Analogy Subgraph Embeddings",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "271f40f4fad4d112e436565e668b79ede690d755",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=g5tANwND04i": {
    "title": "On the Convergence of mSGD and AdaGrad for Stochastic Optimization",
    "abstract": "As one of the most fundamental stochastic optimization algorithms, stochastic gradient descent (SGD) has been intensively developed and extensively applied in machine learning in the past decade. There have been some modified SGD-type algorithms, which outperform the SGD in many competitions and applications in terms of convergence rate and accuracy, such as momentum-based SGD (mSGD) and adaptive gradient algorithm (AdaGrad). Despite these empirical successes, the theoretical properties of these algorithms have not been well established due to technical difficulties. With this motivation, we focus on convergence analysis of mSGD and AdaGrad for any smooth (possibly non-convex) loss functions in stochastic optimization. First, we prove that the iterates of mSGD are asymptotically convergent to a connected set of stationary points with probability one, which is more general than existing works on subsequence convergence or convergence of time averages. Moreover, we prove that the loss function of mSGD decays at a certain rate faster than that of SGD. In addition, we prove the iterates of AdaGrad are asymptotically convergent to a connected set of stationary points with probability one. Also, this result extends the results from the literature on subsequence convergence and the convergence of time averages. Despite the generality of the above convergence results, we have relaxed some assumptions of gradient noises, convexity of loss functions, as well as boundedness of iterates",
    "volume": "main",
    "checked": true,
    "id": "b8a0ded7907faf2e513116e7a0af44577e0c9dab",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=izvwgBic9q": {
    "title": "Unsupervised Learning of Full-Waveform Inversion: Connecting CNN and Partial Differential Equation in a Loop",
    "abstract": "This paper investigates unsupervised learning of Full-Waveform Inversion (FWI), which has been widely used in geophysics to estimate subsurface velocity maps from seismic data. This problem is mathematically formulated by a second order partial differential equation (PDE), but is hard to solve. Moreover, acquiring velocity map is extremely expensive, making it impractical to scale up a supervised approach to train the mapping from seismic data to velocity maps with convolutional neural networks (CNN).We address these difficulties by integrating PDE and CNN in a loop, thus shifting the paradigm to unsupervised learning that only requires seismic data. In particular, we use finite difference to approximate the forward modeling of PDE as a differentiable operator (from velocity map to seismic data) and model its inversion by CNN (from seismic data to velocity map). Hence, we transform the supervised inversion task into an unsupervised seismic data reconstruction task. We also introduce a new large-scale dataset OpenFWI, to establish a more challenging benchmark for the community. Experiment results show that our model (using seismic data alone) yields comparable accuracy to the supervised counterpart (using both seismic data and velocity map). Furthermore, it outperforms the supervised model when involving more seismic data. Figure 1: Schematic illustration of our proposed method, which comprises a CNN to learn an inverse mapping and a differentiable operator to approximate the forward modeling of PDE",
    "volume": "main",
    "checked": true,
    "id": "da7698b4387e8de6138c7cf279f34521a841226d",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=WLEx3Jo4QaB": {
    "title": "Graph Condensation for Graph Neural Networks",
    "abstract": "Given the prevalence of large-scale graphs in real-world applications, the storage and time for training neural models have raised increasing concerns. To alleviate the concerns, we propose and study the problem of graph condensation for graph neural networks (GNNs). Specifically, we aim to condense the large, original graph into a small, synthetic and highly-informative graph, such that GNNs trained on the small graph and large graph have comparable performance. We approach the condensation problem by imitating the GNN training trajectory on the original graph through the optimization of a gradient matching loss and design a strategy to condense node futures and structural information simultaneously. Extensive experiments have demonstrated the effectiveness of the proposed framework in condensing different graph datasets into informative smaller graphs. In particular, we are able to approximate the original test accuracy by 95.3% on Reddit, 99.8% on Flickr and 99.0% on Citeseer, while reducing their graph size by more than 99.9%, and the condensed graphs can be used to train various GNN architectures",
    "volume": "main",
    "checked": true,
    "id": "3dad9ca15d28f4cec7d17eadca42b748510e3b16",
    "citation_count": 13
  },
  "https://openreview.net/forum?id=YevsQ05DEN7": {
    "title": "Understanding Dimensional Collapse in Contrastive Self-supervised Learning",
    "abstract": "Self-supervised visual representation learning aims to learn useful representations without relying on human annotations. Joint embedding approach bases on max-imizing the agreement between embedding vectors from different views of the same image. Various methods have been proposed to solve the collapsing problem where all embedding vectors collapse to a trivial constant solution. Among these methods, contrastive learning prevents collapse via negative sample pairs. It has been shown that non-contrastive methods suffer from a lesser collapse problem of a different nature: dimensional collapse, whereby the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available embedding space. Here, we show that dimensional collapse also happens in contrastive learning. In this paper, we shed light on the dynamics at play in contrastive learning that leads to dimensional collapse. Inspired by our theory, we propose a novel contrastive learning method, called DirectCLR , which directly optimizes the representation space without relying on an explicit trainable projector. Experiments show that DirectCLR outperforms SimCLR with a trainable linear projector on ImageNet",
    "volume": "main",
    "checked": true,
    "id": "28c17db217f2d7af12482a087d197851f0a97db0",
    "citation_count": 64
  },
  "https://openreview.net/forum?id=OY1A8ejQgEX": {
    "title": "Mention Memory: incorporating textual knowledge into Transformers through entity mention attention",
    "abstract": "Natural language understanding tasks such as open-domain question answering often require retrieving and assimilating factual information from multiple sources. We propose to address this problem by integrating a semi-parametric representation of a large text corpus into a Transformer model as a source of factual knowledge. Speciﬁcally, our method represents knowledge with \"mention memory\", a table of dense vector representations of every entity mention in a corpus. The proposed model - TOME - is a Transformer that accesses the information through internal memory layers in which each entity mention in the input passage attends to the mention memory. This approach enables synthesis of and reasoning over many disparate sources of information within a single Transformer model. In experiments using a memory of 150 million Wikipedia mentions, TOME achieves strong performance on several open-domain knowledge-intensive tasks, including the claim veriﬁcation benchmarks HoVer and FEVER and several entity-based QA benchmarks. We also show that the model learns to attend to informative mentions without any direct supervision. Finally we demonstrate that the model can generalize to new unseen entities by updating the memory without retraining",
    "volume": "main",
    "checked": true,
    "id": "7b7416c90e8d3fc9ad5c9fb3923a638f69294ed7",
    "citation_count": 12
  },
  "https://openreview.net/forum?id=1xXvPrAshao": {
    "title": "Learning Multimodal VAEs through Mutual Supervision",
    "abstract": "Multimodal variational autoencoders (VAEs) seek to model the joint distribution over heterogeneous data (e.g. vision, language), whilst also capturing a shared representation across such modalities. Prior work has typically combined information from the modalities by reconciling idiosyncratic representations directly in the recognition model through explicit products, mixtures, or other such factorisations. Here we introduce a novel alternative, the Mutually supErvised Multimodal VAE (MEME), that avoids such explicit combinations by repurposing semi-supervised VAEs to combine information between modalities implicitly through mutual supervision. This formulation naturally allows learning from partially-observed data where some modalities can be entirely missing—something that most existing approaches either cannot handle, or do so to a limited extent. We demonstrate that MEME outperforms baselines on standard metrics across both partial and complete observation schemes on the MNIST-SVHN (image–image) and CUB (image–text) datasets. We also contrast the quality of the representations learnt by mutual supervision against standard approaches and observe interesting trends in its ability to capture relatedness between data",
    "volume": "main",
    "checked": true,
    "id": "9e75405a9737dbbcccef3f2fa6ed89a1aeb59ed9",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=L01Nn_VJ9i": {
    "title": "Back2Future: Leveraging Backfill Dynamics for Improving Real-time Predictions in Future",
    "abstract": "In real-time forecasting in public health, data collection is a non-trivial and demanding task. Often after initially released, it undergoes several revisions later (maybe due to human or technical constraints) - as a result, it may take weeks until the data reaches to a stable value. This so-called 'backfill' phenomenon and its effect on model performance has been barely studied in the prior literature. In this paper, we introduce the multi-variate backfill problem using COVID-19 as the motivating example. We construct a detailed dataset composed of relevant signals over the past year of the pandemic. We then systematically characterize several patterns in backfill dynamics and leverage our observations for formulating a novel problem and neural framework Back2Future that aims to refines a given model's predictions in real-time. Our extensive experiments demonstrate that our method refines the performance of top models for COVID-19 forecasting, in contrast to non-trivial baselines, yielding 18% improvement over baselines, enabling us obtain a new SOTA performance. In addition, we show that our model improves model evaluation too;hence policy-makers can better understand the true accuracy of forecasting models in real-time",
    "volume": "main",
    "checked": true,
    "id": "035c271232ab6951016ef24eb2046f9d2c458145",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=25kzAhUB1lz": {
    "title": "Direct then Diffuse: Incremental Unsupervised Skill Discovery for State Covering and Goal Reaching",
    "abstract": "Learning meaningful behaviors in the absence of reward is a difﬁcult problem in reinforcement learning. A desirable and challenging unsupervised objective is to learn a set of diverse skills that provide a thorough coverage of the state space while being directed, i.e., reliably reaching distinct regions of the environment. In this paper, we build on the mutual information framework for skill discovery and introduce UPSIDE , which addresses the coverage-directedness trade-off in the following ways: 1) We design policies with a decoupled structure of a directed skill, trained to reach a speciﬁc region, followed by a diffusing part that induces a local coverage. 2) We optimize policies by maximizing their number under the constraint that each of them reaches distinct regions of the environment (i.e., they are sufﬁciently discriminable) and prove that this serves as a lower bound to the original mutual information objective. 3) Finally, we compose the learned directed skills into a growing tree that adaptively covers the environment. We illustrate in several navigation and control environments how the skills learned by UPSIDE solve sparse-reward downstream tasks better than existing baselines",
    "volume": "main",
    "checked": true,
    "id": "a01fae01cd9a3067fa6b8a777e70efe86bdc4699",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=84NMXTHYe-": {
    "title": "Evidential Turing Processes",
    "abstract": "A probabilistic classifier with reliable predictive uncertainties i) fits successfully to the target domain data, ii) provides calibrated class probabilities in difficult regions of the target domain (e.g. class overlap), and iii) accurately identifies queries coming out of the target domain and reject them. We introduce an original combination of evidential deep learning, neural processes, and neural Turing machines capable of providing all three essential properties mentioned above for total uncertainty quantification. We observe our method on three image classification benchmarks and two neural net architectures to consistently give competitive or superior scores with respect to multiple uncertainty quantification metrics against state-of-the-art methods explicitly tailored to one or a few of them. Our unified solution delivers an implementation-friendly and computationally efficient recipe for safety clearance and provides intellectual economy to an investigation of algorithmic roots of epistemic awareness in deep neural nets",
    "volume": "main",
    "checked": true,
    "id": "81314b15e4dd3a5c12f5a186beedd381a037d272",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=vUH85MOXO7h": {
    "title": "A Neural Tangent Kernel Perspective of Infinite Tree Ensembles",
    "abstract": "In practical situations, the tree ensemble is one of the most popular models along with neural networks. A soft tree is a variant of a decision tree. Instead of using a greedy method for searching splitting rules, the soft tree is trained using a gradient method in which the entire splitting operation is formulated in a differentiable form. Although ensembles of such soft trees have been used increasingly in recent years, little theoretical work has been done to understand their behavior. By considering an ensemble of infinite soft trees, this paper introduces and studies the Tree Neural Tangent Kernel (TNTK), which provides new insights into the behavior of the infinite ensemble of soft trees. Using the TNTK, we theoretically identify several non-trivial properties, such as global convergence of the training, the equivalence of the oblivious tree structure, and the degeneracy of the TNTK induced by the deepening of the trees",
    "volume": "main",
    "checked": true,
    "id": "05f9306ad29607699d033a6fe97e4c9a1565151f",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=LDAwu17QaJz": {
    "title": "MAML is a Noisy Contrastive Learner in Classification",
    "abstract": "Model-agnostic meta-learning (MAML) is one of the most popular and widely adopted meta-learning algorithms, achieving remarkable success in various learning problems. Yet, with the unique design of nested inner-loop and outer-loop updates, which govern the task-specific and meta-model-centric learning, respectively, the underlying learning objective of MAML remains implicit and thus impedes a more straightforward understanding of it. In this paper, we provide a new perspective of the working mechanism of MAML. We discover that MAML is analogous to a meta-learner using a supervised contrastive objective. The query features are pulled towards the support features of the same class and against those of different classes. Such contrastiveness is experimentally verified via an analysis based on the cosine similarity. Moreover, we reveal that vanilla MAML has an undesirable interference term originating from the random initialization and the cross-task interaction. We thus propose a simple but effective technique, zeroing trick, to alleviate the interference. Extensive experiments are conducted on both mini-ImageNet and Omniglot datasets to demonstrate the consistent improvement brought by our proposed method, validating its effectiveness. 1",
    "volume": "main",
    "checked": true,
    "id": "73e1b848711c304dbf16ed6f390d0d765ca1c7e1",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=tFgdrQbbaa": {
    "title": "Learning curves for continual learning in neural networks: Self-knowledge transfer and forgetting",
    "abstract": "Sequential training from task to task is becoming one of the major objects in deep learning applications such as continual learning and transfer learning. Nevertheless, it remains unclear under what conditions the trained model's performance improves or deteriorates. To deepen our understanding of sequential training, this study provides a theoretical analysis of generalization performance in a solvable case of continual learning. We consider neural networks in the neural tangent kernel (NTK) regime that continually learn target functions from task to task, and investigate the generalization by using an established statistical mechanical analysis of kernel ridge-less regression. We ﬁrst show characteristic transitions from positive to negative transfer. More similar targets above a speciﬁc critical value can achieve positive knowledge transfer for the subsequent task while catastrophic forgetting occurs even with very similar targets. Next, we investigate a variant of continual learning which supposes the same target function in multiple tasks. Even for the same target, the trained model shows some transfer and forgetting depending on the sample size of each task. We can guarantee that the generalization error monotonically decreases from task to task for equal sample sizes while unbalanced sample sizes deteriorate the generalization. We respectively refer to these improvement and deterioration as self -knowledge transfer and forgetting, and empirically conﬁrm them in realistic training of deep neural networks as well",
    "volume": "main",
    "checked": true,
    "id": "a46a42d3a713a8e3843203764654c6d0219422a5",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=zuqcmNVK4c2": {
    "title": "Self-Joint Supervised Learning",
    "abstract": "Supervised learning is a fundamental framework used to train machine learning systems. A supervised learning problem is often formulated using an i.i.d. assumption that restricts model attention to a single relevant signal at a time when predicting. This contrasts with the human ability to actively use related samples as reference when making decisions. We hypothesize that the restriction to a single signal for each prediction in the standard i.i.d. framework contributes to well-known drawbacks of supervised learning: making overconfident predictions and vulnerability to overfitting, adversarial attacks, and out-of-distribution data. To address these limitations, we propose a new supervised learning paradigm called self-joint learning that generalizes the standard approach by modeling the joint conditional distribution of two observed samples, where each sample is an image and its label. Rather than assuming samples are independent, our models explicitly learn the sample-to-sample relation of conditional independence. Our framework can naturally incorporate auxiliary unlabeled data to further improve the performance. Experiments on benchmark image datasets show our method offers significant improvement over standard supervised learning in terms of accuracy, robustness against adversarial attacks, out-of-distribution detection, and overconfidence mitigation. Code: github.com/ndkn/Self-joint-Learning",
    "volume": "main",
    "checked": true,
    "id": "0b8313c5be24f248a42433b7a49db7d94fc98508",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=jXKKDEi5vJt": {
    "title": "Byzantine-Robust Learning on Heterogeneous Datasets via Bucketing",
    "abstract": "In Byzantine robust distributed or federated learning, a central server wants to train a machine learning model over data distributed across multiple workers. However, a fraction of these workers may deviate from the prescribed algorithm and send arbitrary messages. While this problem has received signiﬁcant attention recently, most current defenses assume that the workers have identical data. For realistic cases when the data across workers are heterogeneous (non-iid), we design new attacks which circumvent current defenses, leading to signiﬁcant loss of performance. We then propose a simple bucketing scheme that adapts existing robust algorithms to heterogeneous datasets at a negligible computational cost. We also theoretically and experimentally validate our approach, showing that combining bucketing with existing robust algorithms is effective against challenging attacks. Our work is the ﬁrst to establish guaranteed convergence for the non-iid Byzantine robust problem under realistic assumptions",
    "volume": "main",
    "checked": true,
    "id": "a9c13ada98fa493dd3552c794fdb3af9e9fbd523",
    "citation_count": 17
  },
  "https://openreview.net/forum?id=7twQI5VnC8": {
    "title": "Learning Causal Models from Conditional Moment Restrictions by Importance Weighting",
    "abstract": "We consider learning causal relationships under conditional moment restrictions. Unlike causal inference under unconditional moment restrictions, conditional moment restrictions pose serious challenges for causal inference, especially in highdimensional settings. To address this issue, we propose a method that transforms conditional moment restrictions to unconditional moment restrictions through importance weighting, using a conditional density ratio estimator. Using this transformation, we successfully estimate nonparametric functions defined under conditional moment restrictions. Our proposed framework is general and can be applied to a wide range of methods, including neural networks. We analyze the estimation error, providing theoretical support for our proposed method. In experiments, we confirm the soundness of our proposed method",
    "volume": "main",
    "checked": true,
    "id": "face5fecc4c3ccd9874c41e5704ed27bf8539f7e",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=dSw0QtRMJkO": {
    "title": "High Probability Bounds for a Class of Nonconvex Algorithms with AdaGrad Stepsize",
    "abstract": "In this paper, we propose a new, simpliﬁed high probability analysis of AdaGrad for smooth, non-convex problems. More speciﬁcally, we focus on a particular accelerated gradient (AGD) template (Lan, 2020), through which we recover the original AdaGrad and its variant with averaging, and prove a convergence rate of O (1 / √ T ) with high probability without the knowledge of smoothness and variance. We use a particular version of Freedman's concentration bound for martingale difference sequences (Kakade & Tewari, 2008) which enables us to achieve the best-known dependence of log(1 /δ ) on the probability margin δ . We present our analysis in a modular way and obtain a complementary O (1 /T ) convergence rate in the deterministic setting. To the best of our knowledge, this is the ﬁrst high probability result for AdaGrad with a truly adaptive scheme, i.e., completely oblivious to the knowledge of smoothness and uniform variance bound, which si-multaneously has best-known dependence of log(1 /δ ) . We further prove noise adaptation property of AdaGrad under additional noise assumptions",
    "volume": "main",
    "checked": true,
    "id": "f1d2a203a8f4e3f596bee84bae87d5f415eff81e",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=nhN-fqxmNGx": {
    "title": "A Comparison of Hamming Errors of Representative Variable Selection Methods",
    "abstract": "Lasso is a celebrated method for variable selection in linear models, but it faces challenges when the variables are moderately or strongly correlated. This motivates alternative approaches such as using a non-convex penalty, adding a ridge regularization, or conducting a post-Lasso thresholding. In this paper, we compare Lasso with 5 other methods: Elastic net, SCAD, forward selection, thresholded Lasso, and forward backward selection. We measure their performances theoretically by the expected Hamming error, assuming that the regression coefficients are iid drawn from a two-point mixture and that the Gram matrix is block-wise diagonal. By deriving the rates of convergence of Hamming errors and the phase diagrams, we obtain useful conclusions about the pros and cons of different methods",
    "volume": "main",
    "checked": true,
    "id": "42386ac380737c8879f7d9cb9e413114672e47e7",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=TpJMvo0_pu-": {
    "title": "Curriculum learning as a tool to uncover learning principles in the brain",
    "abstract": "BSTRACT We present a novel approach to use curricula to identify principles by which a system learns. Previous work in curriculum learning has focused on how curricula can be designed to improve learning of a model on particular tasks. We consider the inverse problem: what can a curriculum tell us about how a learning system acquired a task? Using recurrent neural networks (RNNs) and models of common experimental neuroscience tasks, we demonstrate that curricula can be used to differentiate learning principles using target-based and a representation-based loss functions as use cases. In particular, we compare the performance of RNNs using a target-based learning principle versus those using a representational learning principle on three different curricula in the context of two tasks. We show that the learned state-space trajectories of RNNs trained by these two learning principles under all curricula tested are indistinguishable. However, by comparing learning times during different curricula, we can disambiguate the learning principles and challenge traditional approaches of interrogating learning systems. Although all animals in neuroscience lab settings are trained by curriculum-based procedures called shaping, almost no behavioral or neural data are collected or published on the relative successes or training times under different curricula. Our results motivate the systematic collection and curation of data during shaping by demonstrating curriculum learning in RNNs as a tool to probe and differentiate learning principles used by biological systems, over statistical analyses of learned state spaces. trained state. Our results emphasize the importance of studying animals during shaping and the value of curriculum learning in RNNs as a hypothesis test-bed for probing learning principles in the biological brain",
    "volume": "main",
    "checked": true,
    "id": "a29d959673a16ccd8d2d402b4109f8ac11718cee",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=oj2yn1Q4Ett": {
    "title": "Decentralized Learning for Overparameterized Problems: A Multi-Agent Kernel Approximation Approach",
    "abstract": "This work develops a novel framework for communication-efficient distributed learning where the models to be learnt are overparameterized. We focus on a class of kernel learning problems (which includes the popular neural tangent kernel (NTK) learning as a special case) and propose a novel multi-agent kernel approximation technique that allows the agents to distributedly estimate the full kernel function, and subsequently perform distributed learning, without directly exchanging any local data or parameters. The proposed framework is a significant departure from the classical consensus-based approaches, because the agents do not exchange problem parameters, and consensus is not required. We analyze the optimization and the generalization performance of the proposed framework for the `2 loss. We show that with M agents and N total samples, when certain generalized inner-product (GIP) kernels (resp. the random features (RF) kernel) are used, each agent needs to communicate O N 2 /M bits (resp. O N p N/M real values) to achieve minimax optimal generalization performance. Further, we show that the proposed algorithms can significantly reduce the communication complexity compared with state-of-the-art algorithms, for distributedly training models to fit UCI benchmarking datasets. Moreover, each agent needs to share about 200N/M bits to closely match the performance of the centralized algorithms, and these numbers are independent of parameter and feature dimension",
    "volume": "main",
    "checked": true,
    "id": "61531184e5d9e5602ef8f8c87d17c19c6300cf49",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=MXdFBmHT4C": {
    "title": "Differentiable Expectation-Maximization for Set Representation Learning",
    "abstract": "We tackle the set2vec problem, the task of extracting a vector representation from an input set comprised of a variable number of feature vectors. Although recent approaches based on self attention such as (Set)Transformers were very successful due to the capability of capturing complex interaction between set elements, the computational overhead is the well-known downside. The inducing-point attention and the latest optimal transport kernel embedding (OTKE) are promising remedies that attain comparable or better performance with reduced computational cost, by incorporating a fixed number of learnable queries in attention. In this paper we approach the set2vec problem from a completely different perspective. The elements of an input set are considered as i.i.d. samples from a mixture distribution, and we define our set embedding feed-forward network as the maximum-a-posterior (MAP) estimate of the mixture which is approximately attained by a few Expectation-Maximization (EM) steps. The whole MAP-EM steps are differentiable operations with a fixed number of mixture parameters, allowing efficient auto-diff back-propagation for any given downstream task. Furthermore, the proposed mixture set data fitting framework allows unsupervised set representation learning naturally via marginal likelihood maximization aka the empirical Bayes. Interestingly, we also find that OTKE can be seen as a special case of our framework, specifically a single-step EM with extra balanced assignment constraints on the E-step. Compared to OTKE, our approach provides more flexible set embedding as well as prior-induced model regularization. We evaluate our approach on various tasks demonstrating improved performance over the state-of-the-arts",
    "volume": "main",
    "checked": true,
    "id": "854141a1ba72fec0f531dc45c21bed64e7ef1f7a",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=9otKVlgrpZG": {
    "title": "Multi-Task Processes",
    "abstract": "Neural Processes (NPs) consider a task as a function realized from a stochastic process and flexibly adapt to unseen tasks through inference on functions. However, naive NPs can model data from only a single stochastic process and are designed to infer each task independently. Since many real-world data represent a set of correlated tasks from multiple sources (e.g., multiple attributes and multi-sensor data), it is beneficial to infer them jointly and exploit the underlying correlation to improve the predictive performance. To this end, we propose Multi-Task Processes (MTPs), an extension of NPs designed to jointly infer tasks realized from multiple stochastic processes. We build our MTPs in a hierarchical manner such that intertask correlation is considered by conditioning all per-task latent variables on a single global latent variable. In addition, we further design our MTPs so that they can address multi-task settings with incomplete data (i.e., not all tasks share the same set of input points), which has high practical demands in various applications. Experiments demonstrate that MTPs can successfully model multiple tasks jointly by discovering and exploiting their correlations in various real-world data such as time series of weather attributes and pixel-aligned visual modalities",
    "volume": "main",
    "checked": true,
    "id": "5d23a0bdac52ec017a58291368f127fb1e765cde",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=JPkQwEdYn8": {
    "title": "Neural Processes with Stochastic Attention: Paying more attention to the context dataset",
    "abstract": "Neural processes (NPs) aim to stochastically complete unseen data points based on a given context dataset. NPs essentially leverage a given dataset as a context representation to derive a suitable identiﬁer for a novel task. To improve the prediction accuracy, many variants of NPs have investigated context embedding approaches that generally design novel network architectures and aggregation functions satisfying permutation invariant. In this work, we propose a stochastic attention mechanism for NPs to capture appropriate context information. From the perspective of information theory, we demonstrate that the proposed method encourages context embedding to be differentiated from a target dataset, allowing NPs to consider features in a target dataset and context embedding independently. We observe that the proposed method can appropriately capture context embedding even under noisy data sets and restricted task distributions, where typical NPs suffer from a lack of context embeddings. We empirically show that our approach substantially outperforms conventional NPs in various domains through 1D regression, predator-prey model, and image completion. Moreover, the proposed method is also validated by MovieLens-10k dataset, a real-world problem",
    "volume": "main",
    "checked": true,
    "id": "8759b8dccdb5a807b29f9d91b300138cf4dc9b8b",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=XWODe7ZLn8f": {
    "title": "Contrastive Fine-grained Class Clustering via Generative Adversarial Networks",
    "abstract": "Unsupervised fine-grained class clustering is a practical yet challenging task due to the difficulty of feature representations learning of subtle object details. We introduce C3-GAN, a method that leverages the categorical inference power of InfoGAN with contrastive learning. We aim to learn feature representations that encourage a dataset to form distinct cluster boundaries in the embedding space, while also maximizing the mutual information between the latent code and its image observation. Our approach is to train a discriminator, which is also used for inferring clusters, to optimize the contrastive loss, where image-latent pairs that maximize the mutual information are considered as positive pairs and the rest as negative pairs. Specifically, we map the input of a generator, which was sampled from the categorical distribution, to the embedding space of the discriminator and let them act as a cluster centroid. In this way, C3-GAN succeeded in learning a clustering-friendly embedding space where each cluster is distinctively separable. Experimental results show that C3-GAN achieved the state-of-the-art clustering performance on four fine-grained image datasets, while also alleviating the mode collapse phenomenon. Code is available at https://github.com/naver-ai/c3-gan",
    "volume": "main",
    "checked": true,
    "id": "3d79e7bc943c1e0bed999fbb249e4529a2fb1de9",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=Ucx3DQbC9GH": {
    "title": "What Makes Better Augmentation Strategies? Augment Difficult but Not too Different",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "e8b90fd68d8ebb398fd8527170724554ae00fe4b",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=cGDAkQo1C0p": {
    "title": "Reversible Instance Normalization for Accurate Time-Series Forecasting against Distribution Shift",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "0e95008a8c49a4c2538aed62ff61977ff7b47ca5",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=BrPdX1bDZkQ": {
    "title": "DemoDICE: Offline Imitation Learning with Supplementary Imperfect Demonstrations",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "8c16bd5d73372a48aec00c23312d66b0dc2044fa",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=aD7uesX1GF_": {
    "title": "Conditional Object-Centric Learning from Video",
    "abstract": "Object-centric representations are a promising path toward more systematic generalization by providing flexible abstractions upon which compositional world models can be built. Recent work on simple 2D and 3D datasets has shown that models with object-centric inductive biases can learn to segment and represent meaningful objects from the statistical structure of the data alone without the need for any supervision. However, such fully-unsupervised methods still fail to scale to diverse realistic data, despite the use of increasingly complex inductive biases such as priors for the size of objects or the 3D geometry of the scene. In this paper, we instead take a weakly-supervised approach and focus on how 1) using the temporal dynamics of video data in the form of optical flow and 2) conditioning the model on simple object location cues can be used to enable segmenting and tracking objects in significantly more realistic synthetic data. We introduce a sequential extension to Slot Attention which we train to predict optical flow for realistic looking synthetic scenes and show that conditioning the initial state of this model on a small set of hints, such as center of mass of objects in the first frame, is sufficient to significantly improve instance segmentation. These benefits generalize beyond the training distribution to novel objects, novel backgrounds, and to longer video sequences. We also find that such initial-state-conditioning can be used during inference as a flexible interface to query the model for specific objects or parts of objects, which could pave the way for a range of weakly-supervised approaches and allow more effective interaction with trained models. Project page: https://slot-attention-video.github.io/",
    "volume": "main",
    "checked": true,
    "id": "95805eb7ab0edcd05128cf0256feaea8e2497de9",
    "citation_count": 42
  },
  "https://openreview.net/forum?id=sPfB2PI87BZ": {
    "title": "Mapping conditional distributions for domain adaptation under generalized target shift",
    "abstract": "We consider the problem of unsupervised domain adaptation (UDA) between a source and a target domain under conditional and label shift a.k.a Generalized Target Shift (GeTarS). Unlike simpler UDA settings, few works have addressed this challenging problem. Recent approaches learn domain-invariant representations, yet they have practical limitations and rely on strong assumptions that may not hold in practice. In this paper, we explore a novel and general approach to align pretrained representations, which circumvents existing drawbacks. Instead of constraining representation invariance, it learns an optimal transport map, implemented as a NN, which maps source representations onto target ones. Our approach is flexible and scalable, it preserves the problem's structure and it has strong theoretical guarantees under mild assumptions. In particular, our solution is unique, matches conditional distributions across domains, recovers target proportions and explicitly controls the target generalization risk. Through an exhaustive comparison on several datasets, we challenge the state-of-the-art in GeTarS",
    "volume": "main",
    "checked": true,
    "id": "7b9af84120079445410210626e2909a03b828211",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=hcMvApxGSzZ": {
    "title": "Fixed Neural Network Steganography: Train the images, not the network",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "042bfdccaebd07b47085408bf6baa3566112e538",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=anbBFlX1tJ1": {
    "title": "Boosted Curriculum Reinforcement Learning",
    "abstract": "Curriculum value-based reinforcement learning (RL) solves a complex target task by reusing action-values across a tailored sequence of related tasks of increasing difficulty. However, finding an exact way of reusing action-values in this setting is still a poorly understood problem. In this paper, we introduce the concept of boosting to curriculum value-based RL, by approximating the action-value function as a sum of residuals trained on each task. This approach, which we refer to as boosted curriculum reinforcement learning (BCRL), has the benefit of naturally increasing the representativeness of the functional space by adding a new residual each time a new task is presented. This procedure allows reusing previous action-values while promoting expressiveness of the action-value function. We theoretically study BCRL as an approximate value iteration algorithm, discussing advantages over regular curriculum RL in terms of approximation accuracy and convergence to the optimal action-value function. Finally, we provide detailed empirical evidence of the benefits of BCRL in problems requiring curricula for accurate action-value estimation and targeted exploration",
    "volume": "main",
    "checked": true,
    "id": "1909317ccf9d01052bbe9eafbbe06380848effb8",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=y_op4lLLaWL": {
    "title": "Variational autoencoders in the presence of low-dimensional data: landscape and implicit bias",
    "abstract": "Variational Autoencoders (VAEs) are one of the most commonly used generative models, particularly for image data. A prominent diﬃculty in training VAEs is data that is supported on a lower dimensional manifold. Recent work by Dai and Wipf (2020) proposes a two-stage training algorithm for VAEs, based on a conjecture that in standard VAE training the generator will converge to a solution with 0 variance which is correctly supported on the ground truth manifold. They gave partial support for this conjecture by showing that some optima of the VAE loss do satisfy this property, but did not analyze the training dynamics. In this paper, we show that for linear encoders/decoders, the conjecture is true—that is the VAE training does recover a generator with support equal to the ground truth manifold—and does so due to an implicit bias of gradient descent rather than merely the VAE loss itself. In the nonlinear case, we show that VAE training frequently learns a higher-dimensional manifold which is a superset of the ground truth manifold",
    "volume": "main",
    "checked": true,
    "id": "431d2847ab714367b10b5f429217064f4b274432",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=nrGGfMbY_qK": {
    "title": "Online Continual Learning on Class Incremental Blurry Task Configuration with Anytime Inference",
    "abstract": "Despite rapid advances in continual learning, a large body of research is devoted to improving performance in the existing setups. While a handful of work do propose new continual learning setups, they still lack practicality in certain aspects. For better practicality, we first propose a novel continual learning setup that is online, task-free, class-incremental, of blurry task boundaries and subject to inference queries at any moment. We additionally propose a new metric to better measure the performance of the continual learning methods subject to inference queries at any moment. To address the challenging setup and evaluation protocol, we propose an effective method that employs a new memory management scheme and novel learning techniques. Our empirical validation demonstrates that the proposed method outperforms prior arts by large margins. Code and data splits are available at https://github.com/naver-ai/i-Blurry",
    "volume": "main",
    "checked": true,
    "id": "272e9e9fc81b4620f2226e77a59ad899fdcadb5b",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=giBFoa-uS12": {
    "title": "Iterated Reasoning with Mutual Information in Cooperative and Byzantine Decentralized Teaming",
    "abstract": "Information sharing is key in building team cognition and enables coordination and cooperation. High-performing human teams also beneﬁt from acting strategically with hierarchical levels of iterated communication and rationalizability, meaning a human agent can reason about the actions of their teammates in their decision-making. Yet, the majority of prior work in Multi-Agent Reinforcement Learning (MARL) does not support iterated rationalizability and only encourage inter-agent communication, resulting in a suboptimal equilibrium cooperation strategy. In this work, we show that reformulating an agent's policy to be conditional on the policies of its neighboring teammates inherently maximizes Mutual Information (MI) lower-bound when optimizing under Policy Gradient (PG). Building on the idea of decision-making under bounded rationality and cognitive hierarchy theory, we show that our modiﬁed PG approach not only maximizes local agent rewards but also implicitly reasons about MI between agents without the need for any explicit ad-hoc regularization terms. Our approach, InfoPG, outperforms baselines in learning emergent collaborative behaviors and sets the state-of-the-art in decentralized cooperative MARL tasks. Our experiments validate the utility of InfoPG by achieving higher sample efﬁciency and signiﬁcantly larger cumulative reward in several complex cooperative multi-agent domains",
    "volume": "main",
    "checked": true,
    "id": "bb110105320dce94791af083a3ecad85989b6e20",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=EskfH0bwNVn": {
    "title": "Resolving Training Biases via Influence-based Data Relabeling",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "d9a1efee3c3c71f7586c67ee80367330f9a0e11f",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=e0jtGTfPihs": {
    "title": "Signing the Supermask: Keep, Hide, Invert",
    "abstract": "The exponential growth in numbers of parameters of neural networks over the past years has been accompanied by an increase in performance across several fields. However, due to their sheer size, the networks not only became difficult to interpret but also problematic to train and use in real-world applications, since hardware requirements increased accordingly. Tackling both issues, we present a novel approach that either drops a neural network's initial weights or inverts their respective sign. Put simply, a network is trained by weight selection and inversion without changing their absolute values. Our contribution extends previous work on masking by additionally sign-inverting the initial weights and follows the findings of the Lottery Ticket Hypothesis. Through this extension and adaptations of initialization methods, we achieve a pruning rate of up to 99%, while still matching or exceeding the performance of various baseline and previous models. Our approach has two main advantages. First, and most notable, signed Supermask models drastically simplify a model's structure, while still performing well on given tasks. Second, by reducing the neural network to its very foundation, we gain insights into which weights matter for performance",
    "volume": "main",
    "checked": true,
    "id": "53b4feab1858f39744220b61717b7f848ff36822",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=68n2s9ZJWF8": {
    "title": "Offline Reinforcement Learning with Implicit Q-Learning",
    "abstract": "Offline reinforcement learning requires reconciling two conflicting aims: learning a policy that improves over the behavior policy that collected the dataset, while at the same time minimizing the deviation from the behavior policy so as to avoid errors due to distributional shift. This trade-off is critical, because most current offline reinforcement learning methods need to query the value of unseen actions during training to improve the policy, and therefore need to either constrain these actions to be in-distribution, or else regularize their values. We propose a new offline RL method that never needs to evaluate actions outside of the dataset, but still enables the learned policy to improve substantially over the best behavior in the data through generalization. The main insight in our work is that, instead of evaluating unseen actions from the latest policy, we can approximate the policy improvement step implicitly by treating the state value function as a random variable, with randomness determined by the action (while still integrating over the dynamics to avoid excessive optimism), and then taking a state conditional upper expectile of this random variable to estimate the value of the best actions in that state. This leverages the generalization capacity of the function approximator to estimate the value of the best available action at a given state without ever directly querying a Q-function with this unseen action. Our algorithm alternates between fitting this upper expectile value function and backing it up into a Q-function, without any explicit policy. Then, we extract the policy via advantage-weighted behavioral cloning, which also avoids querying out-of-sample actions. We dub our method implicit Q-learning (IQL). IQL is easy to implement, computationally efficient, and only requires fitting an additional critic with an asymmetric L2 loss. IQL demonstrates the state-of-the-art performance on D4RL, a standard benchmark for offline reinforcement learning. We also demonstrate that IQL achieves strong performance fine-tuning using online interaction after offline initialization",
    "volume": "main",
    "checked": true,
    "id": "348a855fe01f3f4273bf0ecf851ca688686dbfcc",
    "citation_count": 86
  },
  "https://openreview.net/forum?id=JGO8CvG5S9": {
    "title": "Universal Approximation Under Constraints is Possible with Transformers",
    "abstract": "Many practical problems need the output of a machine learning model to satisfy a set of constraints, K. There are, however, no known guarantees that classical neural networks can exactly encode constraints while simultaneously achieving universality. We provide a quantitative constrained universal approximation theorem which guarantees that for any convex or non-convex compact set K and any continuous function f : R → K, there is a probabilistic transformer F̂ whose randomized outputs all lie in K and whose expected output uniformly approximates f . Our second main result is a \"deep neural version\" of Berge (1963)'s Maximum Theorem. The result guarantees that given an objective function L, a constraint set K, and a family of soft constraint sets, there is a probabilistic transformer F̂ that approximately minimizes L and whose outputs belong to K; moreover, F̂ approximately satisfies the soft constraints. Our results imply the first universal approximation theorem for classical transformers with exact convex constraint satisfaction, and a chart-free universal approximation theorem for Riemannian manifold-valued functions subject to geodesically-convex constraints",
    "volume": "main",
    "checked": true,
    "id": "e1bccf9e3fa257b550e1e1bc477c63ab82d18457",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=EcGGFkNTxdJ": {
    "title": "Trust Region Policy Optimisation in Multi-Agent Reinforcement Learning",
    "abstract": "Trust region methods rigorously enabled reinforcement learning (RL) agents to learn monotonically improving policies, leading to superior performance on a variety of tasks. Unfortunately, when it comes to multi-agent reinforcement learning (MARL), the property of monotonic improvement may not simply apply; this is because agents, even in cooperative games, could have conflicting directions of policy updates. As a result, achieving a guaranteed improvement on the joint policy where each agent acts individually remains an open challenge. In this paper, we extend the theory of trust region learning to cooperative MARL. Central to our findings are the multi-agent advantage decomposition lemma and the sequential policy update scheme. Based on these, we develop Heterogeneous-Agent Trust Region Policy Optimisation (HATPRO) and Heterogeneous-Agent Proximal Policy Optimisation (HAPPO) algorithms. Unlike many existing MARL algorithms, HATRPO/HAPPO do not need agents to share parameters, nor do they need any restrictive assumptions on decomposibility of the joint value function. Most importantly, we justify in theory the monotonic improvement property of HATRPO/HAPPO. We evaluate the proposed methods on a series of Multi-Agent MuJoCo and StarCraftII tasks. Results show that HATRPO and HAPPO significantly outperform strong baselines such as IPPO, MAPPO and MADDPG on all tested tasks, thereby establishing a new state of the art",
    "volume": "main",
    "checked": true,
    "id": "27302766f8d0eb6c052eb400e234c5be0e7a767e",
    "citation_count": 23
  },
  "https://openreview.net/forum?id=mwdfai8NBrJ": {
    "title": "Policy Smoothing for Provably Robust Reinforcement Learning",
    "abstract": "The study of provable adversarial robustness for deep neural networks (DNNs) has mainly focused on static supervised learning tasks such as image classiﬁcation. However, DNNs have been used extensively in real-world adaptive tasks such as reinforcement learning (RL), making such systems vulnerable to adversarial attacks as well. Prior works in provable robustness in RL seek to certify the behaviour of the victim policy at every time-step against a non-adaptive adversary using methods developed for the static setting. But in the real world, an RL adversary can infer the defense strategy used by the victim agent by observ-ing the states, actions, etc. from previous time-steps and adapt itself to produce stronger attacks in future steps (e.g., by focusing more on states critical to the agent's performance). We present an efﬁcient procedure, designed speciﬁcally to defend against an adaptive RL adversary, that can directly certify the total reward without requiring the policy to be robust at each time-step. Focusing on randomized smoothing based defenses, our main theoretical contribution is to prove an adaptive version of the Neyman-Pearson Lemma – a key lemma for smoothing-based certiﬁcates – where the adversarial perturbation at a particular time can be a stochastic function of current and previous observations and states as well as previous actions. Building on this result, we propose policy smoothing where the agent adds a Gaussian noise to its observation at each time-step before passing it through the policy function. Our robustness certiﬁcates guarantee that the ﬁnal total reward obtained by policy smoothing remains above a certain threshold, even though the actions at intermediate time-steps may change under the attack. We show that our certiﬁcates are tight by constructing a worst-case scenario that achieves the bounds derived in our analysis. Our experiments on various environments like Cartpole, Pong, Freeway and Mountain Car show that our method can yield meaningful robustness guarantees in practice",
    "volume": "main",
    "checked": true,
    "id": "7f2f8042750df1be7562023760148a391d247904",
    "citation_count": 13
  },
  "https://openreview.net/forum?id=POvMvLi91f": {
    "title": "DR3: Value-Based Deep Reinforcement Learning Requires Explicit Regularization",
    "abstract": "Despite overparameterization, deep networks trained via supervised learning are easy to optimize and exhibit excellent generalization. One hypothesis to explain this is that overparameterized deep networks enjoy the beneﬁts of implicit regularization induced by stochastic gradient descent, which favors parsimonious solutions that generalize well on test inputs. It is reasonable to surmise that deep reinforcement learning (RL) methods could also beneﬁt from this effect. In this paper, we discuss how the implicit regularization effect of SGD seen in supervised learning could in fact be harmful in the ofﬂine deep RL setting, leading to poor generalization and degenerate feature representations. Our theoretical analysis shows that when existing models of implicit regularization are applied to temporal difference learning, the resulting derived regularizer favors degenerate solutions with excessive \"aliasing\", in stark contrast to the supervised learning case. We back up these ﬁndings empirically, showing that feature representations learned by a deep network value function trained via bootstrapping can indeed become degenerate, aliasing the representations for state-action pairs that appear on either side of the Bellman backup. To address this issue, we derive the form of this implicit regularizer and, inspired by this derivation, propose a simple and effective explicit regularizer, called DR3, that counteracts the undesirable effects of this implicit regularizer. When combined with existing ofﬂine RL methods, DR3 substantially improves performance and stability, alleviating unlearning in Atari 2600 games, D4RL domains and robotic manipulation from images. statistical uncertainties. we computed the average probability of improvement [3] of CQL + DR3 over CQL on the antmaze and kitchen domains, and we ﬁnd that DR3 does signiﬁcantly and meaningfully improve over CQL on both the Kitchen and AntMaze domains. Before presenting the results, let us ﬁrst describe the metric we compute",
    "volume": "main",
    "checked": true,
    "id": "c271b4d25bc184bc94622cef6c9aba80e8e2cce3",
    "citation_count": 19
  },
  "https://openreview.net/forum?id=AP1MKT37rJ": {
    "title": "Should I Run Offline Reinforcement Learning or Behavioral Cloning?",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "a8c409791404828a3276ffb0e4a71d3263b0526e",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=UYneFzXSJWh": {
    "title": "Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution",
    "abstract": "When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer—the \"head\"). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR → STL, CIFAR10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2% higher accuracy ID but 7% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head—this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1% better ID, 10% better OOD than full fine-tuning)",
    "volume": "main",
    "checked": true,
    "id": "29b77089a0a40f46372ce2dca9c3bb2dd5d46b1d",
    "citation_count": 71
  },
  "https://openreview.net/forum?id=GsH-K1VIyy": {
    "title": "Data-Driven Offline Optimization for Architecting Hardware Accelerators",
    "abstract": "Industry has gradually moved towards application-specific hardware accelerators in order to attain higher efficiency. While such a paradigm shift is already starting to show promising results, designers need to spend considerable manual effort and perform a large number of time-consuming simulations to find accelerators that can accelerate multiple target applications while obeying design constraints. Moreover, such a \"simulation-driven\" approach must be re-run from scratch every time the set of target applications or design constraints change. An alternative paradigm is to use a \"data-driven\", offline approach that utilizes logged simulation data, to architect hardware accelerators, without needing any form of simulations. Such an approach not only alleviates the need to run time-consuming simulation, but also enables data reuse and applies even when set of target applications changes. In this paper, we develop such a data-driven offline optimization method for designing hardware accelerators, dubbed PRIME, that enjoys all of these properties. Our approach learns a conservative, robust estimate of the desired cost function, utilizes infeasible points and optimizes the design against this estimate without any additional simulator queries during optimization. PRIME architects accelerators—tailored towards both single and multiple applications—improving performance upon state-of-theart simulation-driven methods by about 1.54× and 1.20×, while considerably reducing the required total simulation time by 93% and 99%, respectively. In addition, PRIME also architects effective accelerators for unseen applications in a zero-shot setting, outperforming simulation-based methods by 1.26×",
    "volume": "main",
    "checked": true,
    "id": "d352df83b48a221867b2bcd42e72f6491b6445d9",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=wfZGut6e09": {
    "title": "Pareto Policy Adaptation",
    "abstract": "We present a policy gradient method for Multi-Objective Reinforcement Learning under unknown, linear preferences. By enforcing Pareto stationarity, a first-order condition for Pareto optimality, we are able to design a simple policy gradient algorithm that approximates the Pareto front and infers the unknown preferences. Our method relies on a projected gradient descent solver that identifies common ascent directions for all objectives. Leveraging the solution of that solver, we introduce Pareto Policy Adaptation ( PPA ), a loss function that adapts the policy to be optimal with respect to any distribution over preferences. PPA uses implicit differentiation to back-propagate the loss gradient bypassing the operations of the projected gradient descent solver. Our approach is straightforward, easy to implement and can be used with all existing policy gradient and actor-critic methods. We evaluate our method in a series of reinforcement learning tasks",
    "volume": "main",
    "checked": true,
    "id": "f883ad9d52b3925274ed7690ec5d60effece2272",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=_l_QjPGN5ye": {
    "title": "The Boltzmann Policy Distribution: Accounting for Systematic Suboptimality in Human Models",
    "abstract": "Models of human behavior for prediction and collaboration tend to fall into two categories: ones that learn from large amounts of data via imitation learning, and ones that assume human behavior to be noisily-optimal for some reward function. The former are very useful, but only when it is possible to gather a lot of human data in the target environment and distribution. The advantage of the latter type, which includes Boltzmann rationality, is the ability to make accurate predictions in new environments without extensive data when humans are actually close to optimal. However, these models fail when humans exhibit systematic suboptimality, i.e. when their deviations from optimal behavior are not independent, but instead consistent over time. Our key insight is that systematic suboptimality can be modeled by predicting policies , which couple action choices over time, instead of trajectories . We introduce the Boltzmann policy distribution (BPD), which serves as a prior over human policies and adapts via Bayesian inference to capture systematic deviations by observing human actions during a single episode. The BPD is difﬁcult to compute and represent because policies lie in a high-dimensional continuous space, but we leverage tools from generative and sequence models to enable efﬁcient sampling and inference. We show that the BPD enables prediction of human behavior and human-AI collaboration equally as well as imitation learning-based human models while using far less data",
    "volume": "main",
    "checked": true,
    "id": "1405b02da0fbd87b181b8ba7fcef558327596a23",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=HFPTzdwN39": {
    "title": "Measuring the Interpretability of Unsupervised Representations via Quantized Reversed Probing",
    "abstract": "Self-supervised visual representation learning recently signiﬁcant research interest. While a common way to evaluate self-supervised representations is through transfer to various downstream tasks, we instead investigate the problem of measuring their interpretability, i e . understanding the semantics encoded in raw representations. We formulate the latter as estimating the mutual information between the representation and a space of manually labelled concepts. To quantify this we introduce a decoding bottleneck: information must be captured by simple predictors, mapping concepts to clusters in representation space. This approach, which we call reverse linear probing provides a single number sensitive to the semanticity of the representation. This measure is also able to detect when the representation contains combinations of concepts ( e g ., \"red apple\") instead of just individual attributes (\"red\" and \"apple\" independently). Finally, we propose to use supervised classiﬁers to automatically label large datasets in order to enrich the space of concepts used for probing. We use our method to evaluate a large number of self-supervised representations, ranking them by interpretability, highlight the differences that emerge compared to the standard evaluation with linear probes and discuss several qualitative insights",
    "volume": "main",
    "checked": true,
    "id": "b53c7d7ddcc1535bf199b0c8a571955bace5ba83",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=L7wzpQttNO": {
    "title": "BDDM: Bilateral Denoising Diffusion Models for Fast and High-Quality Speech Synthesis",
    "abstract": "Diffusion probabilistic models (DPMs) and their extensions have emerged as competitive generative models yet confront challenges of efﬁcient sampling. We propose a new bilateral denoising diffusion model (BDDM) that parameterizes both the forward and reverse processes with a schedule network and a score network, which can train with a novel bilateral modeling objective. We show that the new surrogate objective can achieve a lower bound of the log marginal likelihood tighter than a conventional surrogate. We also ﬁnd that BDDM al-lows inheriting pre-trained score network parameters from any DPMs and con-sequently enables speedy and stable learning of the schedule network and optimization of a noise schedule for sampling. Our experiments demonstrate that BDDMs can generate high-ﬁdelity audio samples with as few as three sampling steps. Moreover, compared to other state-of-the-art diffusion-based neural vocoders, BDDMs produce comparable or higher quality samples indistinguishable from human speech, notably with only seven sampling steps (143x faster than WaveGrad and 28.6x faster than DiffWave). We release our code at https://github.com/tencent-ailab/bddm ",
    "volume": "main",
    "checked": true,
    "id": "2f67789df52a9747a64cdcd545640efd19ff199f",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=ChMLTGRjFcU": {
    "title": "How many degrees of freedom do we need to train deep networks: a loss landscape perspective",
    "abstract": "A variety of recent works, spanning pruning, lottery tickets, and training within random subspaces, have shown that deep neural networks can be trained using far fewer degrees of freedom than the total number of parameters. We analyze this phenomenon for random subspaces by ﬁrst examining the success probability of hitting a training loss sublevel set when training within a random subspace of a given training dimensionality. We ﬁnd a sharp phase transition in the success probability from 0 to 1 as the training dimension surpasses a threshold. This threshold training dimension increases as the desired ﬁnal loss decreases, but decreases as the initial loss decreases. We then theoretically explain the origin of this phase transition, and its dependence on initialization and ﬁnal desired loss, in terms of properties of the high dimensional geometry of the loss landscape. In particular, we show via Gordon's escape theorem, that the training dimension plus the Gaussian width of the desired loss sub-level set, projected onto a unit sphere surrounding the initialization, must exceed the total number of parameters for the success probability to be large. In several architectures and datasets, we measure the threshold training dimension as a function of initialization and demonstrate that it is a small fraction of the total parameters, implying by our theory that successful training with so few dimensions is possible precisely because the Gaussian width of low loss sub-level sets is very large. Moreover, we compare this threshold training dimension to more sophisticated ways of reducing training degrees of freedom, including lottery tickets as well as a new, analogous method: lottery subspaces",
    "volume": "main",
    "checked": true,
    "id": "93cdcc6f246422d5ac08930a8ee80ac2213ee65a",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=bTteFbU99ye": {
    "title": "Evaluating Distributional Distortion in Neural Language Modeling",
    "abstract": "A fundamental characteristic of natural language is the high rate at which speakers produce novel expressions. Because of this novelty, a heavy-tail of rare events accounts for a significant amount of the total probability mass of distributions in language (Baayen, 2001). Standard language modeling metrics such as perplexity quantify the performance of language models (LM) in aggregate. As a result, we have relatively little understanding of whether neural LMs accurately estimate the probability of sequences in this heavy-tail of rare events. To address this gap, we develop a controlled evaluation scheme which uses generative models trained on natural data as artificial languages from which we can exactly compute sequence probabilities. Training LMs on generations from these artificial languages, we compare the sequence-level probability estimates given by LMs to the true probabilities in the target language. Our experiments reveal that LSTM and Transformer language models (i) systematically underestimate the probability of sequences drawn from the target language, and (ii) do so more severely for lessprobable sequences. Investigating where this probability mass went, (iii) we find that LMs tend to overestimate the probability of ill-formed (perturbed) sequences. In addition, we find that this underestimation behaviour (iv) is weakened, but not eliminated by greater amounts of training data, and (v) is exacerbated for target distributions with lower entropy",
    "volume": "main",
    "checked": true,
    "id": "d4550863c9b4102472a2326ab994aafdb13de7b9",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=u2GZOiUTbt": {
    "title": "Task Affinity with Maximum Bipartite Matching in Few-Shot Learning",
    "abstract": "We propose an asymmetric affinity score for representing the complexity of utilizing the knowledge of one task for learning another one. Our method is based on the maximum bipartite matching algorithm and utilizes the Fisher Information matrix. We provide theoretical analyses demonstrating that the proposed score is mathematically well-defined, and subsequently use the affinity score to propose a novel algorithm for the few-shot learning problem. In particular, using this score, we find relevant training data labels to the test data and leverage the discovered relevant data for episodically fine-tuning a few-shot model. Results on various few-shot benchmark datasets demonstrate the efficacy of the proposed approach by improving the classification accuracy over the state-of-the-art methods even when using smaller models",
    "volume": "main",
    "checked": true,
    "id": "87397e5d3eaede08709f5d1d8b83da9823e6c1f8",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=XEW8CQgArno": {
    "title": "Training invariances and the low-rank phenomenon: beyond linear networks",
    "abstract": "The implicit bias induced by the training of neural networks has become a topic of rigorous study. In the limit of gradient ﬂow and gradient descent with appropriate step size, it has been shown that when one trains a deep linear network with logistic or exponential loss on linearly separable data, the weights converge to rank- 1 matrices. In this paper, we extend this theoretical result to the last few linear layers of the much wider class of nonlinear ReLU-activated feedforward networks containing fully-connected layers and skip connections. Similar to the linear case, the proof relies on speciﬁc local training invariances, sometimes referred to as alignment, which we show to hold for submatrices where neurons are stably-activated in all training examples, and it reﬂects empirical results in the literature. We also show this is not true in general for the full matrix of ReLU fully-connected layers. Our proof relies on a speciﬁc decomposition of the network into a multilinear function and another ReLU network whose weights are constant under a certain parameter directional convergence",
    "volume": "main",
    "checked": true,
    "id": "ee14966a8aea312b1af6569108f09440922fb704",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=8la28hZOwug": {
    "title": "Prototypical Contrastive Predictive Coding",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "da4deb6a7594782d3aa507f9121c3375b5f9f2a2",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=dwg5rXg1WS_": {
    "title": "ViTGAN: Training GANs with Vision Transformers",
    "abstract": "Recently, Vision Transformers (ViTs) have shown competitive performance on image recognition while requiring less vision-specific inductive biases. In this paper, we investigate if such observation can be extended to image generation. To this end, we integrate the ViT architecture into generative adversarial networks (GANs). We observe that existing regularization methods for GANs interact poorly with self-attention, causing serious instability during training. To resolve this issue, we introduce novel regularization techniques for training GANs with ViTs. Empirically, our approach, named ViTGAN, achieves comparable performance to state-of-the-art CNN-based StyleGAN2 on CIFAR-10, CelebA, and LSUN bedroom datasets",
    "volume": "main",
    "checked": true,
    "id": "bd163f27b409a4d903632009d38df77cfd70a437",
    "citation_count": 44
  },
  "https://openreview.net/forum?id=oDFvtxzPOx": {
    "title": "Self-Supervision Enhanced Feature Selection with Correlated Gates",
    "abstract": "Discovering relevant input features for predicting a target variable is a key scientific question. However, in many domains, such as medicine and biology, feature selection is confounded by a scarcity of labeled samples coupled with significant correlations among features. In this paper, we propose a novel deep learning approach to feature selection that addresses both challenges simultaneously. First, we pre-train the network using unlabeled samples within a self-supervised learning framework by solving pretext tasks that require the network to learn informative representations from partial feature sets. Then, we fine-tune the pre-trained network to discover relevant features using labeled samples. During both training phases, we explicitly account for the correlation structure of the input features by generating correlated gate vectors from a multivariate Bernoulli distribution. Experiments on multiple real-world datasets including clinical and omics demonstrate that our model discovers relevant features that provide superior prediction performance compared to the state-of-the-art benchmarks in practical scenarios where there is often limited labeled data and high correlations among features",
    "volume": "main",
    "checked": true,
    "id": "3f5692b1f79fdebc1a92e8868504d0d08c9ee056",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=wwDg3bbYBIq": {
    "title": "Learning to Remember Patterns: Pattern Matching Memory Networks for Traffic Forecasting",
    "abstract": "Traffic forecasting is a challenging problem due to complex road networks and sudden speed changes caused by various events on roads. A number of models have been proposed to solve this challenging problem with a focus on learning spatio-temporal dependencies of roads. In this work, we propose a new perspective of converting the forecasting problem into a pattern matching task, assuming that large data can be represented by a set of patterns. To evaluate the validness of the new perspective, we design a novel traffic forecasting model, called PatternMatching Memory Networks (PM-MemNet), which learns to match input data to the representative patterns with a key-value memory structure. We first extract and cluster representative traffic patterns, which serve as keys in the memory. Then via matching the extracted keys and inputs, PM-MemNet acquires necessary information of existing traffic patterns from the memory and uses it for forecasting. To model spatio-temporal correlation of traffic, we proposed novel memory architecture GCMem, which integrates attention and graph convolution for memory enhancement. The experiment results indicate that PM-MemNet is more accurate than state-of-the-art models, such as Graph WaveNet with higher responsiveness. We also present a qualitative analysis result, describing how PM-MemNet works and achieves its higher accuracy when road speed rapidly changes",
    "volume": "main",
    "checked": true,
    "id": "b680663cef887d75a827ea0151dca2b9fe2885c6",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=_BNiN4IjC5": {
    "title": "PriorGrad: Improving Conditional Denoising Diffusion Models with Data-Dependent Adaptive Prior",
    "abstract": "Denoising diffusion probabilistic models have been recently proposed to generate high-quality samples by estimating the gradient of the data density. The framework defines the prior noise as a standard Gaussian distribution, whereas the corresponding data distribution may be more complicated than the standard Gaussian distribution, which potentially introduces inefficiency in denoising the prior noise into the data sample because of the discrepancy between the data and the prior. In this paper, we propose PriorGrad to improve the efficiency of the conditional diffusion model for speech synthesis (for example, a vocoder using a mel-spectrogram as the condition) by applying an adaptive prior derived from the data statistics based on the conditional information. We formulate the training and sampling procedures of PriorGrad and demonstrate the advantages of an adaptive prior through a theoretical analysis. Focusing on the speech synthesis domain, we consider the recently proposed diffusion-based speech generative models based on both the spectral and time domains and show that PriorGrad achieves faster convergence and inference with superior performance, leading to an improved perceptual quality and robustness to a smaller network capacity, and thereby demonstrating the efficiency of a data-dependent adaptive prior",
    "volume": "main",
    "checked": true,
    "id": "69614f326557928d9d142ca0de2e5f572d813f04",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=ivQruZvXxtz": {
    "title": "Sequential Reptile: Inter-Task Gradient Alignment for Multilingual Learning",
    "abstract": "Multilingual models jointly pretrained on multiple languages have achieved remarkable performance on various multilingual downstream tasks. Moreover, models finetuned on a single monolingual downstream task have shown to generalize to unseen languages. In this paper, we first show that it is crucial for those tasks to align gradients between them in order to maximize knowledge transfer while minimizing negative transfer. Despite its importance, the existing methods for gradient alignment either have a completely different purpose, ignore inter-task alignment, or aim to solve continual learning problems in rather inefficient ways. As a result of the misaligned gradients between tasks, the model suffers from severe negative transfer in the form of catastrophic forgetting of the knowledge acquired from the pretraining. To overcome the limitations, we propose a simple yet effective method that can efficiently align gradients between tasks. Specifically, we perform each inner-optimization by sequentially sampling batches from all the tasks, followed by a Reptile outer update. Thanks to the gradients aligned between tasks by our method, the model becomes less vulnerable to negative transfer and catastrophic forgetting. We extensively validate our method on various multi-task learning and zero-shot cross-lingual transfer tasks, where our method largely outperforms all the relevant baselines we consider",
    "volume": "main",
    "checked": true,
    "id": "ce40dde9c42b782995c7070bc280c2a2ec308a4b",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=01AMRlen9wJ": {
    "title": "Online Hyperparameter Meta-Learning with Hypergradient Distillation",
    "abstract": "Many gradient-based meta-learning methods assume a set of parameters that do not participate in inner-optimization, which can be considered as hyperparameters. Although such hyperparameters can be optimized using the existing gradientbased hyperparameter optimization (HO) methods, they suffer from the following issues. Unrolled differentiation methods do not scale well to high-dimensional hyperparameters or horizon length , Implicit Function Theorem (IFT) based methods are restrictive for online optimization, and short horizon approximations suffer from short horizon bias. In this work, we propose a novel HO method that can overcome these limitations, by approximating the second-order term with knowledge distillation. Specifically, we parameterize a single Jacobian-vector product (JVP) for each HO step and minimize the distance from the true second-order term. Our method allows online optimization and also is scalable to the hyperparameter dimension and the horizon length. We demonstrate the effectiveness of our method on two different meta-learning methods and three benchmark datasets",
    "volume": "main",
    "checked": true,
    "id": "d896f9f96457f709346eb454db502e00ceac94bf",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=3HJOA-1hb0e": {
    "title": "Toward Efficient Low-Precision Training: Data Format Optimization and Hysteresis Quantization",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "83132bd8b07bf383e15654e693115d41eaa21c6c",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=mQxt8l7JL04": {
    "title": "Regularized Autoencoders for Isometric Representation Learning",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "6ccf2df209c28fd6b1599131221ea28037be6306",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=YVPBh4k78iZ": {
    "title": "Scale Mixtures of Neural Network Gaussian Processes",
    "abstract": "Recent works have revealed that infinitely-wide feed-forward or recurrent neural networks of any architecture correspond to Gaussian processes referred to as Neural Network Gaussian Processes (NNGPs). While these works have extended the class of neural networks converging to Gaussian processes significantly, however, there has been little focus on broadening the class of stochastic processes that such neural networks converge to. In this work, inspired by the scale mixture of Gaussian random variables, we propose the scale mixture of NNGPs for which we introduce a prior distribution on the scale of the last-layer parameters. We show that simply introducing a scale prior on the last-layer parameters can turn infinitely-wide neural networks of any architecture into a richer class of stochastic processes. Especially, with certain scale priors, we obtain heavy-tailed stochastic processes, and we recover Student's t processes in the case of inverse gamma priors. We further analyze the distributions of the neural networks initialized with our prior setting and trained with gradient descents and obtain similar results as for NNGPs. We present a practical posterior-inference algorithm for the scale mixture of NNGPs and empirically demonstrate its usefulness on regression and classification tasks",
    "volume": "main",
    "checked": true,
    "id": "162f332bf7c0f51cc1cab62ce72f138a45b25029",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=xNO7OEIcJc6": {
    "title": "Language-biased image classification: evaluation based on semantic representations",
    "abstract": "Humans show language-biased image recognition for a word-embedded image, known as picture-word interference. Such interference depends on hierarchical semantic categories and reflects that human language processing highly interacts with visual processing. Similar to humans, recent artificial models jointly trained on texts and images, e.g., OpenAI CLIP, show language-biased image classification. Exploring whether the bias leads to interference similar to those observed in humans can contribute to understanding how much the model acquires hierarchical semantic representations from joint learning of language and vision. The present study introduces methodological tools from the cognitive science literature to assess the biases of artificial models. Specifically, we introduce a benchmark task to test whether words superimposed on images can distort the image classification across different category levels and, if it can, whether the perturbation is due to the shared semantic representation between language and vision. Our dataset is a set of word-embedded images and consists of a mixture of natural image datasets and hierarchical word labels with superordinate/basic category levels. Using this benchmark test, we evaluate the CLIP model. We show that presenting words distorts the image classification by the model across different category levels, but the effect does not depend on the semantic relationship between images and embedded words. This suggests that the semantic word representation in the CLIP visual processing is not shared with the image representation, although the word representation strongly dominates for word-embedded images",
    "volume": "main",
    "checked": true,
    "id": "9f984b48d57a7e0704c4cb46264e9f697cf60e42",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=gSdSJoenupI": {
    "title": "PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions",
    "abstract": "Cross-entropy loss and focal loss are the most common choices when training deep neural networks for classiﬁcation problems. Generally speaking, however, a good loss function can take on much more ﬂexible forms, and should be tailored for different tasks and datasets. Motivated by how functions can be approximated via Taylor expansion, we propose a simple framework, named PolyLoss to view and design loss functions as a linear combination of polynomial functions. Our PolyLoss allows the importance of different polynomial bases to be easily ad-justed depending on the targeting tasks and datasets, while naturally subsuming the aforementioned cross-entropy loss and focal loss as special cases. Extensive experimental results show that the optimal choice within the PolyLoss is indeed dependent on the task and dataset. Simply by introducing one extra hyperparameter and adding one line of code, our Poly-1 formulation outperforms the cross-entropy loss and focal loss on 2D image classiﬁcation, instance segmentation, object detection, and 3D object detection tasks, sometimes by a large margin. : a novel framework for understanding and designing loss functions. Our key insight is to decompose commonly used classiﬁcation loss functions, such as cross-entropy loss and focal loss, into a series of weighted polynomial bases. They are decomposed in the form of (cid:80) ∞ j =1 α j (1 − P t ) j , where α j ∈ R + is the polynomial coefﬁcient and P t is the prediction probability of the target class label. Each polynomial base (1 − P t ) j is weighted by a corresponding polynomial coefﬁcient α j , which enables us to easily adjust the importance of different bases for different applications. When α j = 1 /j for all j , our PolyLoss becomes equivalent to the commonly used cross-entropy loss, but this coefﬁcient assignment may not be optimal",
    "volume": "main",
    "checked": true,
    "id": "30c6126b7bb567d06c5c62ad811175fd400a38f8",
    "citation_count": 18
  },
  "https://openreview.net/forum?id=rUwm9wCjURV": {
    "title": "In a Nutshell, the Human Asked for This: Latent Goals for Following Temporal Specifications",
    "abstract": "We address the problem of building agents whose goal is to learn to execute out-of distribution (OOD) multi-task instructions expressed in temporal logic (TL) by using deep reinforcement learning (DRL). Recent works provided evidence that the agent's neural architecture is a key feature when DRL agents are learning to solve OOD tasks in TL. Yet, the studies on this topic are still in their infancy. In this work, we propose a new deep learning configuration with inductive biases that lead agents to generate latent representations of their current goal, yielding a stronger generalization performance. We use these latent-goal networks within a neuro-symbolic framework that executes multi-task formally-defined instructions and contrast the performance of the proposed neural networks against employing different state-of-the-art (SOTA) architectures when generalizing to unseen instructions in OOD environments",
    "volume": "main",
    "checked": true,
    "id": "5b7d0f5fde3f8c8ee97a0d8b13dd4d225c48a987",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=gfwON7rAm4": {
    "title": "Global Convergence of Multi-Agent Policy Gradient in Markov Potential Games",
    "abstract": "Potential games are arguably one of the most important and widely studied classes of normal form games. They define the archetypal setting of multi-agent coordination as all agent utilities are perfectly aligned with each other via a common potential function. Can this intuitive framework be transplanted in the setting of Markov Games? What are the similarities and differences between multi-agent coordination with and without state dependence? We present a novel definition of Markov Potential Games (MPG) that generalizes prior attempts at capturing complex stateful multiagent coordination. Counter-intuitively, insights from normal-form potential games do not carry over as MPGs can consist of settings where stategames can be zero-sum games. In the opposite direction, Markov games where every state-game is a potential game are not necessarily MPGs. Nevertheless, MPGs showcase standard desirable properties such as the existence of deterministic Nash policies. In our main technical result, we prove (polynomially fast in the approximation error) convergence of independent policy gradient to Nash policies by adapting recent gradient dominance property arguments developed for single agent MDPs to multi-agent learning settings. 1. Extended Abstract Reinforcement learning (RL) has been a fundamental driver of numerous recent advances in Artificial Intelligence (AI) applications that range from super-human performance in competitive game-playing (Silver et al., 2016; 2018; Brown and Sandholm, 2019) and strategic decision-making in multiple tasks (Mnih et al., 2015; OpenAI, 2018; Vinyals et al., 2019) to robotics, autonomous-driving and cyber-physical systems (Busoniu et al., 2008; Zhang et al., 2019). A core ingredient for the success of single-agent RL systems, which are typically modelled as Markov Decision Processes (MDPs), is the guarantee of existence of stationary deterministic optimal policies (Bertsekas, 2000; Sutton and Barto, 2018). This allows for the design of efficient algorithms that provably converge towards the optimal policy (Agarwal et al., 2020). However, a majority of the above systems involve multi-agent interactions and despite the notable empirical advancements, there is a lack of understanding about the theoretical convergence guarantees of the existing multiagent reinforcement learning (MARL) algorithms. The main challenge when transitioning from single to multiagent RL settings is the computation of Nash policies. A Nash policy for n > 1 agents is defined to be a profile of policies (π∗ 1 , ..., π ∗ n) so that by fixing the stationary policies of all agents but i, π∗ i is an optimal policy for the resulting single-agent MDP and this is true for all 1 ≤ i ≤ n 1 (see Definition 1). Note that in multi-agent settings, Nash policies may not be unique in principle. A common approach for computing Nash policies in MDPs is the use of policy gradient methods. There has been significant progress in the analysis of policy gradient methods during the last couple of years, notably including the works of (Agarwal et al., 2020) (and references therein), but it has mainly concerned the single-agent case: the convergence properties of policy gradient in MARL remain poorly understood. Existing steps towards a theory for multi-agent settings involve the papers of (Daskalakis et al., 2020) who show convergence of independent policy gradient to the optimal policy, for two-agent zero-sum stochastic games, of (Wei et al., 2021) who improve the result of (Daskalakis et al., 2020) using optimistic policy gradient and of (Zhao et al., 2021) who study extensions of Natural Policy Gradient using function approximation. It is worth noting that the positive results of (Daskalakis et al., 2020; Wei et al., 2021) and (Zhao et al., 2021) depend on the fact that two-agent stochastic zero-sum games satisfy the \"min-max equals max-min\" property (Shapley, 1953) (even though the value-function landscape may not be convex-concave, which implies that Von Neumann's celebrated minimax theorem may not be applicable). Model and Informal Statement of Results. While the previous works enhance our understanding in competitive Analogue of Nash equilibrium notion. 1 055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 100 101 102 103 104 105 106 107 108 109 interactions, i.e., interactions in which gains can only come at the expense of others, MARL in cooperative settings remains largely under-explored and constitutes one of the current frontiers in AI research (Dafoe et al., 2020; Dafoe et al., 2021). Based on the above, our work is motivated by the following natural question: Can we get (provably) fast convergence guarantees for multi-agent RL settings in which cooperation is desirable? To address this question, we define and study a class of n-agent MDPs that naturally generalize normal form potential games (Monderer and Shapley, 1996), called Markov Potential Games (MPGs). In words, a multi-agent MDP is a MPG as long as there exists a (state-dependent) real-valued potential function Φ so that if an agent i changes their policy (and the rest of the agents keep their policy unchanged), the difference in agent i's value/utility, V , is captured by the difference in the value of Φ (see Definition 2). Weighted and ordinal MPGs are defined similar to the normal form counterparts (see Remark 1). Under our definition, we answer the above motivating question in the affirmative. In particular, we show that if every agent i independently runs (with simultaneous updates) policy gradient on his utility/value V , after O(1/ ) iterations, the system will reach an -approximate Nash policy (see informal Theorem 1.1 and formal Theorem 4.5). Moreover, we show the finite sample analogue, that is if every agent i independently runs (with simultaneous updates) stochastic policy gradient, then with high probability, the system will reach an -approximate Nash policy after O(1/ ) iterations. Along the way, we prove several properties about the structure of MPGs and their Nash policies (see Theorem 1.2 and Section 3). Our results can be summarized in the following two Theorems. Theorem 1.1 (Convergence of Policy Gradient (Informal)). Consider a MPG with n agents and let > 0. Suppose that each agent i runs independent policy gradient using direct parameterization on his policy and that the updates are simultaneous. Then, the learning dynamics reach an -Nash policy after O(1/ ) iterations. Moreover, suppose that each agent i runs stochastic policy gradient using greedy parameterization (see (4)) on his policy and that the updates are simultaneous. Then the learning dynamics reach an Nash policy after O(1/ ) iterations. This result holds trivially for weighted MPGs and asymptotically also for ordinal MPGs, see Remark 4. Theorem 1.2 (Structural Properties of MPGs). The following facts are true for MPGs with n-agents: a. There always exists a Nash policy profile (π∗ 1 , . . . , π ∗ n) s0 ( 0 1 0 2, 0 2, 0 1 2, 0 2, 0 ) s1 ( 0 1 0 0, 2 0, 2 1 0, 2 0, 2 ) aA ⊕ aB = 0 otherwise otherwise aA ⊕ aB = 0 Figure 1. A MDP which is potential at every state but which not a MPG due to conflicting preferences over states. The agents' instantaneous rewards, (RA(s,a), RB(s,a)), are shown in matrix form below each state s = 0, 1",
    "volume": "main",
    "checked": true,
    "id": "766f25e1e6adb4d76403e2a59733257b77d5db12",
    "citation_count": 33
  },
  "https://openreview.net/forum?id=lnEaqbTJIRz": {
    "title": "The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design",
    "abstract": "Pretraining Neural Language Models (NLMs) over a large corpus involves chunking the text into training examples, which are contiguous text segments of sizes processable by the neural architecture. We highlight a bias introduced by this common practice: we prove that the pretrained NLM can model much stronger dependencies between text segments that appeared in the same training example, than it can between text segments that appeared in different training examples. This intuitive result has a twofold role. First, it formalizes the motivation behind a broad line of recent successful NLM training heuristics, proposed for the pretraining and fine-tuning stages, which do not necessarily appear related at first glance. Second, our result clearly indicates further improvements to be made in NLM pretraining for the benefit of Natural Language Understanding tasks. As an example, we propose \"kNN-Pretraining\": we show that including semantically related non-neighboring sentences in the same pretraining example yields improved sentence representations and open domain question answering abilities. This theoretically motivated degree of freedom for pretraining example design indicates new training schemes for self-improving representations",
    "volume": "main",
    "checked": true,
    "id": "a2412fdebd53bd25476f834ae2b8aa8cb44cb1e1",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=nBU_u6DLvoK": {
    "title": "UniFormer: Unified Transformer for Efficient Spatial-Temporal Representation Learning",
    "abstract": "It is a challenging task to learn rich and multi-scale spatiotemporal semantics from high-dimensional videos, due to large local redundancy and complex global dependency between video frames. The recent advances in this research have been mainly driven by 3D convolutional neural networks and vision transformers. Although 3D convolution can efficiently aggregate local context to suppress local redundancy from a small 3D neighborhood, it lacks the capability to capture global dependency because of the limited receptive field. Alternatively, vision transformers can effectively capture long-range dependency by self-attention mechanism, while having the limitation on reducing local redundancy with blind similarity comparison among all the tokens in each layer. Based on these observations, we propose a novel Unified transFormer (UniFormer) which seamlessly integrates merits of 3D convolution and spatiotemporal self-attention in a concise transformer format, and achieves a preferable balance between computation and accuracy. Different from traditional transformers, our relation aggregator can tackle both spatiotemporal redundancy and dependency, by learning local and global token affinity respectively in shallow and deep layers. We conduct extensive experiments on the popular video benchmarks, e.g., Kinetics-400, Kinetics600, and Something-Something V1&V2. With only ImageNet-1K pretraining, our UniFormer achieves 82.9%/84.8% top-1 accuracy on Kinetics-400/Kinetics600, while requiring 10× fewer GFLOPs than other state-of-the-art methods. For Something-Something V1 and V2, our UniFormer achieves new state-of-the-art performances of 60.9% and 71.2% top-1 accuracy respectively. Code is available at https://github.com/Sense-X/UniFormer",
    "volume": "main",
    "checked": true,
    "id": "c8831d0629f0eaf7f723317d71bbd60b8eb3c39f",
    "citation_count": 26
  },
  "https://openreview.net/forum?id=gI7feJ9yXPz": {
    "title": "High Probability Generalization Bounds with Fast Rates for Minimax Problems",
    "abstract": "Minimax problems are receiving an increasing amount of attention in a wide range of applications in machine learning (ML), for instance, reinforcement learning, robust optimization, adversarial learning, and distributed computing, to mention but a few. Current studies focus on the fundamental understanding of general minimax problems with an emphasis on convergence behavior. As a comparison, there is far less work to study the generalization performance. Additionally, existing generalization bounds are almost all derived in expectation, and the high probability bounds are all presented in the slow order O(1/ √ n), where n is the sample size. In this paper, we provide improved generalization analyses and obtain sharper high probability generalization bounds for most existing generalization measures of minimax problems. We then use the improved learning bounds to establish high probability generalization bounds with fast rates for classical empirical saddle point (ESP) solution and several popular gradient-based optimization algorithms, including gradient descent ascent (GDA), stochastic gradient descent ascent (SGDA), proximal point method (PPM), extra-gradient (EG), and optimistic gradient descent ascent (OGDA). In summary, we provide a systematical analysis of sharper generalization bounds of minimax problems",
    "volume": "main",
    "checked": true,
    "id": "aaf43fc215965b0b7818e77a3b30cd21fc65bba8",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=rzvOQrnclO0": {
    "title": "Gradient Information Matters in Policy Optimization by Back-propagating through Model",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "1930877e3bd43a684154f28fa4a226d030e1624b",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=nbC8iTTXIrk": {
    "title": "Optimization inspired Multi-Branch Equilibrium Models",
    "abstract": "Works have shown the strong connections between some implicit models and optimization problems. However, explorations on such relationships are limited. Most works pay attention to some common mathematical properties, such as sparsity. In this work, we propose a new type of implicit model inspired by the designing of the systems' hidden objective functions, called the Multi-branch Optimization induced Equilibrium networks (MOptEqs). The model architecture is designed based on modelling the hidden objective function for the multi-resolution recognition task. Furthermore, we also propose a new strategy inspired by our understandings of the hidden objective function. In this manner, the proposed model can better utilize the hierarchical patterns for recognition tasks and retain the abilities for interpreting the whole structure as trying to obtain the minima of the problem's goal. Comparing with the state-of-the-art models, our MOptEqs not only enjoys better explainability but are also superior to MDEQ with less parameter consumption and better performance on practical tasks. Furthermore, we also implement various experiments to demonstrate the eﬀectiveness of our new methods and explore the applicability of the model's hidden objective function. we also propose a new strategy inspired by our understandings of the hidden optimization problem. The empirical results show the advantages of our proposed methods. The success of our HH & D module and PE strategy demonstrates the deep link between the optimization problem and neural architecture and may motivate further explorations",
    "volume": "main",
    "checked": true,
    "id": "3a0ef15417a40192870834a4344a30b9f8d0d1e3",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=MQ2sAGunyBP": {
    "title": "R4D: Utilizing Reference Objects for Long-Range Distance Estimation",
    "abstract": "Estimating the distance of objects is a safety-critical task for autonomous driving. Focusing on short-range objects, existing methods and datasets neglect the equally important long-range objects. In this paper, we introduce a challenging and underexplored task, which we refer to as Long-Range Distance Estimation, as well as two datasets to validate new methods developed for this task. We then propose R4D, the first framework to accurately estimate the distance of long-range objects by using references with known distances in the scene. Drawing inspiration from human perception, R4D builds a graph by connecting a target object to all references. An edge in the graph encodes the relative distance information between a pair of target and reference objects. An attention module is then used to weigh the importance of reference objects and combine them into one target object distance prediction. Experiments on the two proposed datasets demonstrate the effectiveness and robustness of R4D by showing significant improvements compared to existing baselines. We're looking to make the proposed dataset, Waymo Open Dataset Long-Range Labels, available publicly, at waymo.com/open/download",
    "volume": "main",
    "checked": true,
    "id": "c104e14a13846680c675c1782a97a3be46b8064a",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=ibqTBNfJmi": {
    "title": "Frequency-aware SGD for Efficient Embedding Learning with Provable Benefits",
    "abstract": "Embedding learning has found widespread applications in recommendation systems and natural language modeling, among other domains. To learn quality embeddings efficiently, adaptive learning rate algorithms have demonstrated superior empirical performance over SGD, largely accredited to their token-dependent learning rate. However, the underlying mechanism for the efficiency of token-dependent learning rate remains underexplored. We show that incorporating frequency information of tokens in the embedding learning problems leads to provably efficient algorithms, and demonstrate that common adaptive algorithms implicitly exploit the frequency information to a large extent. Specifically, we propose (Counterbased) Frequency-aware Stochastic Gradient Descent, which applies a frequency-dependent learning rate for each token, and exhibits provable speed-up compared to SGD when the token distribution is imbalanced. Empirically, we show the proposed algorithms are able to improve or match adaptive algorithms on benchmark recommendation tasks and a large-scale industrial recommendation system, closing the performance gap between SGD and adaptive algorithms, while using significantly lower memory. Our results are the first to show token-dependent learning rate provably improves convergence for non-convex embedding learning problems",
    "volume": "main",
    "checked": true,
    "id": "9075e67a8711ba29bc67c6d75118b6031ecfc31a",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=6HN7LHyzGgC": {
    "title": "Uncertainty Modeling for Out-of-Distribution Generalization",
    "abstract": "Though remarkable progress has been achieved in various vision tasks, deep neural networks still suffer obvious performance degradation when tested in out-ofdistribution scenarios. We argue that the feature statistics (mean and standard deviation), which carry the domain characteristics of the training data, can be properly manipulated to improve the generalization ability of deep learning models. Common methods often consider the feature statistics as deterministic values measured from the learned features and do not explicitly consider the uncertain statistics discrepancy caused by potential domain shifts during testing. In this paper, we improve the network generalization ability by modeling the uncertainty of domain shifts with synthesized feature statistics during training. Specifically, we hypothesize that the feature statistic, after considering the potential uncertainties, follows a multivariate Gaussian distribution. Hence, each feature statistic is no longer a deterministic value, but a probabilistic point with diverse distribution possibilities. With the uncertain feature statistics, the models can be trained to alleviate the domain perturbations and achieve better robustness against potential domain shifts. Our method can be readily integrated into networks without additional parameters. Extensive experiments demonstrate that our proposed method consistently improves the network generalization ability on multiple vision tasks, including image classification, semantic segmentation, and instance retrieval. The code can be available at https://github.com/lixiaotong97/DSU",
    "volume": "main",
    "checked": true,
    "id": "3485aa52bf7a8310e80e139907d8c2e649e3ab66",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=P07dq7iSAGr": {
    "title": "Explaining Point Processes by Learning Interpretable Temporal Logic Rules",
    "abstract": "We propose a principled method to learn a set of human-readable logic rules to explain temporal point processes. We assume that the generative mechanisms underlying the temporal point processes are governed by a set of first-order temporal logic rules, as a compact representation of domain knowledge. Our method formulates the rule discovery process from noisy event data as a maximum likelihood problem, and designs an efficient and tractable branch-and-price algorithm to progressively search for new rules and expand existing rules. The proposed algorithm alternates between the rule generation stage and the rule evaluation stage, and uncovers the most important collection of logic rules within a fixed time limit for both synthetic and real event data. In a real healthcare application, we also had human experts (i.e., doctors) verify the learned temporal logic rules and provide further improvements. These expert-revised interpretable rules lead to a point process model which outperforms previous state-of-the-arts for symptom prediction, both in their occurrence times and types. 1",
    "volume": "main",
    "checked": true,
    "id": "07938ad12ced90fe381803b9ad754ea51fb77120",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=mmUA7_O9mjY": {
    "title": "Contact Points Discovery for Soft-Body Manipulations with Differentiable Physics",
    "abstract": "Differentiable physics has recently been shown as a powerful tool for solving soft-body manipulation tasks. However, the differentiable physics solver often gets stuck when the initial contact points of the end effectors are sub-optimal or when performing multi-stage tasks that require contact point switching, which often leads to local minima. To address this challenge, we propose a contact point discovery approach (CPDeform) that guides the stand-alone differentiable physics solver to deform various soft-body plasticines. The key idea of our approach is to integrate optimal transport-based contact points discovery into the differentiable physics solver to overcome the local minima from initial contact points or contact switching. On single-stage tasks, our method can automatically ﬁnd suitable initial contact points based on transport priorities. On complex multi-stage tasks, we can iteratively switch the contact points of end-effectors based on transport priorities. To evaluate the effectiveness of our method, we introduce PlasticineLab-M that extends the existing differentiable physics benchmark PlasticineLab to seven new challenging multi-stage soft-body manipulation tasks. Extensive experimental results suggest that: 1) on multi-stage tasks that are infeasible for the vanilla differentiable physics solver, our approach discovers contact points that efﬁciently guide the solver to completion; 2) on tasks where the vanilla solver performs sub-optimally or near-optimally, our contact point discovery method performs better than or on par with the process to optimize the contact plan. We employ a that takes a contact as its and outputs the loss achieved by the differentiable physics solver with that contact We then perform Bayesian optimization for 15 iterations to optimize the contact for the",
    "volume": "main",
    "checked": true,
    "id": "e3dd531e8aaec2966cfdc9f7e00703157658d1c1",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=92tYQiil17": {
    "title": "Learning Transferable Reward for Query Object Localization with Policy Adaptation",
    "abstract": "We propose a reinforcement learning based approach to query object localization, for which an agent is trained to localize objects of interest specified by a small exemplary set. We learn a transferable reward signal formulated using the exemplary set by ordinal metric learning. Our proposed method enables test-time policy adaptation to new environments where the reward signals are not readily available, and outperforms fine-tuning approaches that are limited to annotated images. In addition, the transferable reward allows repurposing the trained agent from one specific class to another class. Experiments on corrupted MNIST, CU-Birds, and COCO datasets demonstrate the effectiveness of our approach 1",
    "volume": "main",
    "checked": true,
    "id": "2017b16ada7c0744c215bc00b1732e758ca2b3cd",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=xDIvIqQ3DXD": {
    "title": "On the approximation properties of recurrent encoder-decoder architectures",
    "abstract": "Encoder-decoder architectures have recently gained popularity in sequence to sequence modelling, featuring in state-of-the-art models such as transformers. However, a mathematical understanding of their working princi-ples still remains limited. In this paper, we study the approximation properties of recurrent encoder-decoder architectures. Prior work established theoretical results for RNNs in the linear setting, where approximation capabilities can be related to smoothness and memory of target temporal relationships. Here, we uncover that the encoder and decoder together form a particular \"temporal product structure\" which determines the approximation eﬃciency. Moreover, the encoder-decoder architecture generalises RNNs with the capability to learn time-inhomogeneous relationships. Our results provide the theoretical understanding of approximation properties of the recurrent encoder-decoder architecture, which precisely characterises, in the considered setting, the types of temporal relationships that can be eﬃciently learned",
    "volume": "main",
    "checked": true,
    "id": "765a88e682672dc1478737a4df8503c9230d2b1d",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=CI-xXX9dg9l": {
    "title": "On Distributed Adaptive Optimization with Gradient Compression",
    "abstract": "1 We study C OMP -AMS, a distributed optimization framework based on gradient averaging and adaptive AMSGrad algorithm. Gradient compression with error feedback is applied to reduce the communication cost in the gradient transmission process. Our convergence analysis of C OMP -AMS shows that such compressed gradient averaging strategy yields same convergence rate as standard AMSGrad, and also exhibits the linear speedup effect w.r.t. the number of local workers. Compared with recently proposed protocols on distributed adaptive methods, C OMP -AMS is simple and convenient. Numerical experiments are conducted to justify the theoretical ﬁndings, and demonstrate that the proposed method can achieve same test accuracy as the full-gradient AMSGrad with substantial communication savings. With its simplicity and efﬁciency, C OMP -AMS can serve as a useful distributed training framework for adaptive gradient methods",
    "volume": "main",
    "checked": true,
    "id": "bf64a012b7d8fc831da367fc56419e5bf9c968c7",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=KTPuIsx4pmo": {
    "title": "Meta-Imitation Learning by Watching Video Demonstrations",
    "abstract": "Meta-Imitation Learning is a promising technique for the robot to learn a new task from observing one or a few human demonstrations. However, it usually requires a signiﬁcant number of demonstrations both from humans and robots during the meta-training phase, which is a laborious and hard work for data collection, especially in recording the actions and specifying the correspondence between human and robot. In this work, we present an approach of meta-imitation learning by watching video demonstrations from humans. In comparison to prior works, our approach is able to translate human videos into practical robot demonstrations and train the meta-policy with adaptive loss based on the quality of the translated data. Our approach relies only on human videos and does not require robot demonstration, which facilitates data collection and is more in line with human imitation behavior. Experiments reveal that our method achieves the comparable performance to the baseline on fast learning a set of vision-based tasks through watching a single video demonstration",
    "volume": "main",
    "checked": true,
    "id": "e258ff722e473ee9c6800369d89ef9528e7db839",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=FEBFJ98FKx": {
    "title": "TPU-GAN: Learning temporal coherence from dynamic point cloud sequences",
    "abstract": "Point cloud sequence is an important data representation that provides ﬂexible shape and motion information. Prior work demonstrates that incorporating scene ﬂow information into loss can make model learn temporally coherent feature spaces. However, it is prohibitively expensive to acquire point correspondence information across frames in real-world environments. In this work, we propose a super-resolution generative adversarial network (GAN) for dynamic point cloud sequences without requiring point correspondence annotation. Our model, Temporal Point cloud Upsampling GAN (TPU-GAN), can implicitly learn the underlying temporal coherence from point cloud sequence, which in turn guides the generator to produce temporally coherent output. In addition, we propose a learnable masking module to adapt upsampling ratio according to the point distribution. We conduct extensive experiments on point cloud sequences from two different domains: particles in the ﬂuid dynamical system and human action scanned data. The quantitative and qualitative evaluation demonstrates the effectiveness of our method on upsampling task as well as learning temporal coherence from irregular point cloud sequences",
    "volume": "main",
    "checked": true,
    "id": "186a83af4f769833bf37138408e56c9be8a85a86",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=NH29920YEmj": {
    "title": "Who Is Your Right Mixup Partner in Positive and Unlabeled Learning",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "bd30aa7245e49c49e3cd8f1bd62a8a0cadfe8aeb",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=lY0-7bj0Vfz": {
    "title": "Prototype memory and attention mechanisms for few shot image generation",
    "abstract": "Recent discoveries indicate that the neural codes in the superﬁcial layers of the primary visual cortex (V1) of macaque monkeys are complex, diverse and super-sparse. This leads us to ponder the computational advantages and functional role of these \"grandmother cells.\" Here, we propose that such cells can serve as prototype memory priors that bias and shape the distributed feature processing during the image generation process in the brain. These memory prototypes are learned by momentum online clustering and are utilized through a memory-based attention operation. Integrating this mechanism, we propose Memory Concept Attention ( MoCA ) to improve few shot image generation quality. We show that having a prototype memory with attention mechanisms can improve image synthesis quality, learn interpretable visual concept clusters, and improve the robustness of the model. Our results demonstrate the feasibility of the idea that these super-sparse complex feature detectors can serve as prototype memory priors for modulating the image synthesis processes in the visual system",
    "volume": "main",
    "checked": true,
    "id": "4df285d03d9945fbd3720337e4b1f2b3f0d68fe0",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=zq1iJkNk3uN": {
    "title": "Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm",
    "abstract": "Recently, large-scale Contrastive Language-Image Pre-training (CLIP) (Radford et al., 2021) has attracted unprecedented attention for its impressive zero-shot recognition ability and excellent transferability to downstream tasks. However, CLIP is quite data-hungry and requires 400M image-text pairs for pre-training, thereby restricting its adoption. This work proposes a novel training paradigm, Data efficient CLIP (DeCLIP), to alleviate this limitation. We demonstrate that by carefully utilizing the widespread supervision among the image-text pairs, our DeCLIP can learn generic visual features more efficiently. Instead of using the single image-text contrastive supervision, we fully exploit data potential through the use of (1) self-supervision within each modality; (2) multi-view supervision across modalities; (3) nearest-neighbor supervision from other similar pairs. Benefiting from these intrinsic supervision, our DeCLIP-ResNet50 can achieve 60.4% zeroshot top1 accuracy on ImageNet, which is 0.8% above the CLIP-ResNet50 while using 7.1× fewer data. Our DeCLIP-ResNet50 outperforms its counterpart in 8 out of 11 visual datasets when transferred to downstream tasks. Moreover, Scaling up the model and computing also works well in our framework. Our code, dataset and models are released at: https://github.com/Sense-GVT/DeCLIP",
    "volume": "main",
    "checked": true,
    "id": "767923635f2fd4467d848dba9655866e4f9b55c8",
    "citation_count": 65
  },
  "https://openreview.net/forum?id=X0nrKAXu7g-": {
    "title": "HyperDQN: A Randomized Exploration Method for Deep Reinforcement Learning",
    "abstract": "Randomized least-square value iteration (RLSVI) is a provably efficient exploration method. However, it is limited to the case where (1) a good feature is known in advance and (2) this feature is fixed during the training. If otherwise, RLSVI suffers an unbearable computational burden to obtain the posterior samples. In this work, we present a practical algorithm named HyperDQN to address the above issues under deep RL. In addition to a non-linear neural network (i.e., base model) that predicts Q-values, our method employs a probabilistic hypermodel (i.e., meta model), which outputs the parameter of the base model. When both models are jointly optimized under a specifically designed objective, three purposes can be achieved. First, the hypermodel can generate approximate posterior samples regarding the parameter of the Q-value function. As a result, diverse Q-value functions are sampled to select exploratory action sequences. This retains the punchline of RLSVI for efficient exploration. Second, a good feature is learned to approximate Q-value functions. This addresses limitation (1). Third, the posterior samples of the Q-value function can be obtained in a more efficient way than the existing methods, and the changing feature does not affect the efficiency. This deals with limitation (2). On the Atari suite, HyperDQN with 20M frames outperforms DQN with 200M frames in terms of the maximum human-normalized score. For SuperMarioBros, HyperDQN outperforms several exploration bonus and randomized exploration methods on 5 out of 9 games",
    "volume": "main",
    "checked": true,
    "id": "4c4d41a977e6d8d82b6cc19ac28e50de3c814fd8",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=3ILxkQ7yElm": {
    "title": "Learning Continuous Environment Fields via Implicit Functions",
    "abstract": "Figure 1: Implicit environment fields in 2D and 3D space. With implicit functions, we learn a continuous environment field that encodes the reaching distance between any pair of points in 2D and 3D space. We present utilization of the environment field for 2D maze navigation in (d)-(e) and human scene interaction modeling in 3D scenes in (f). Note that we flip the environment field in (e) upside down for visualization purposes",
    "volume": "main",
    "checked": true,
    "id": "2cba4246d75a950fecf4c3ee5ea812a2f6e608ff",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=OT3mLgR8Wg8": {
    "title": "IFR-Explore: Learning Inter-object Functional Relationships in 3D Indoor Scenes",
    "abstract": "single-object or agent-object functionality to study a new kind of visual relationship that is also important to perceive and model – inter-object functional relationships ( e.g. , a switch on the wall turns on or off the light, a remote control operates the TV). Humans often spend little or no effort to infer these relationships, even when entering a new room, by using our strong prior knowledge ( e.g. , we know that buttons control electrical devices) or using only a few exploratory interactions in cases of uncertainty ( e.g. , multiple switches and lights in the same room). In this paper, we take the ﬁrst step in building AI system learning inter-object functional relationships in 3D indoor environments with key technical contributions of modeling prior knowledge by training over large-scale scenes and designing interactive policies for effectively exploring the training scenes and quickly adapting to novel test scenes. We create a new benchmark based on the AI2Thor and PartNet datasets and perform extensive experiments that prove the effectiveness of our proposed method. Results show that our model successfully learns priors and fast-interactive-adaptation strategies for exploring inter-object functional relationships in complex 3D scenes. Several ablation studies further the usefulness of each proposed",
    "volume": "main",
    "checked": true,
    "id": "6551c77fd3a99b3ea00bdc54075357c8cc5a2150",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=0cgU-BZp2ky": {
    "title": "Efficient Learning of Safe Driving Policy via Human-AI Copilot Optimization",
    "abstract": "Human intervention is an effective way to inject human knowledge into the loop of reinforcement learning, bringing fast learning and training safety. But given the very limited budget of human intervention, it is challenging to design when and how human expert interacts with the learning agent in the training. In this work, we develop a novel human-in-the-loop learning method called Human-AI Copilot Optimization (HACO). To allow the agent's sufficient exploration in the risky environments while ensuring the training safety, the human expert can take over the control and demonstrate to the agent how to avoid probably dangerous situations or trivial behaviors. The proposed HACO then effectively utilizes the data collected both from the trial-and-error exploration and human's partial demonstration to train a high-performing agent. HACO extracts proxy state-action values from partial human demonstration and optimizes the agent to improve the proxy values while reducing the human interventions. No environmental reward is required in HACO. The experiments show that HACO achieves a substantially high sample efficiency in the safe driving benchmark. It can train agents to drive in unseen traffic scenes with a handful of human intervention budget and achieve high safety and generalizability, outperforming both reinforcement learning and imitation learning baselines with a large margin. Code and demo videos are available at: https://decisionforce.github.io/HACO/",
    "volume": "main",
    "checked": true,
    "id": "f3e8c42b56bf8406726eaaccc68398df2eaccc61",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=cOtBRgsf2fO": {
    "title": "Label Leakage and Protection in Two-party Split Learning",
    "abstract": "Two-party split learning is a popular technique for learning a model across feature-partitioned data. In this work, we explore whether it is possible for one party to steal the private label information from the other party during split training, and whether there are methods that can protect against such attacks. Speciﬁcally, we ﬁrst formulate a realistic threat model and propose a privacy loss metric to quantify label leakage in split learning. We then show that there exist two simple yet eﬀective methods within the threat model that can allow one party to accurately recover private ground-truth labels owned by the other party. To combat these attacks, we propose several random perturbation techniques, including Marvell , an approach that strategically ﬁnds the structure of the noise perturbation by minimizing the amount of label leakage (measured through our quantiﬁcation metric) of a worst-case adversary. We empirically demonstrate the eﬀectiveness of our protection techniques against the identiﬁed attacks, and show that Marvell in particular has improved privacy-utility tradeoﬀs relative to baseline approaches",
    "volume": "main",
    "checked": true,
    "id": "42cbf5de56f5bc4992b5ab181e9a7f0705ecd5df",
    "citation_count": 32
  },
  "https://openreview.net/forum?id=bVuP3ltATMz": {
    "title": "Large Language Models Can Be Strong Differentially Private Learners",
    "abstract": "Differentially Private (DP) learning has seen limited success for building large deep learning models of text, and straightforward attempts at applying Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have resulted in large performance drops and high computational overhead. We show that this performance drop can be mitigated with (1) the use of large pretrained language models; (2) non-standard hyperparameters that suit DP optimization; and (3) ﬁne-tuning objectives which are aligned with the pretraining procedure. With the above, we obtain NLP models that outperform state-of-the-art DP-trained models under the same privacy budget and strong non-private baselines—by directly ﬁne-tuning pretrained models with DP optimization on moderately-sized corpora. To address the computational challenge of running DP-SGD with large Transformers, we propose a memory saving technique that allows clipping in DP-SGD to run without instantiating per-example gradients for any linear layer in the model. The technique enables privately training Transformers with almost the same memory cost as non-private training at a modest run-time overhead. Contrary to conventional wisdom that DP optimization fails at learning high-dimensional models (due to noise that scales with dimension) empirical results reveal that private learning with pretrained language models tends to not suffer from dimension-dependent performance degradation. Code to reproduce results can be found at https: . on par with or outperforms others methods that execute gradient update in low dimensional spaces. Results are on E2E from ﬁne-tuning GPT-2",
    "volume": "main",
    "checked": true,
    "id": "6a758ada5c48a2ae48d1392d12ce4f4e1977e0dd",
    "citation_count": 54
  },
  "https://openreview.net/forum?id=64trBbOhdGU": {
    "title": "HyAR: Addressing Discrete-Continuous Action Reinforcement Learning via Hybrid Action Representation",
    "abstract": "Discrete-continuous hybrid action space is a natural setting in many practical problems, such as robot control and game AI. However, most previous Reinforcement Learning (RL) works only demonstrate the success in controlling with either discrete or continuous action space, while seldom take into account the hybrid action space. One naive way to address hybrid action RL is to convert the hybrid action space into a unified homogeneous action space by discretization or continualization, so that conventional RL algorithms can be applied. However, this ignores the underlying structure of hybrid action space and also induces the scalability issue and additional approximation difficulties, thus leading to degenerated results. In this paper, we propose Hybrid Action Representation (HyAR) to learn a compact and decodable latent representation space for the original hybrid action space. HyAR constructs the latent space and embeds the dependence between discrete action and continuous parameter via an embedding table and conditional Variantional Auto-Encoder (VAE). To further improve the effectiveness, the action representation is trained to be semantically smooth through unsupervised environmental dynamics prediction. Finally, the agent then learns its policy with conventional DRL algorithms in the learned representation space and interacts with the environment by decoding the hybrid action embeddings to the original action space. We evaluate HyAR in a variety of environments with discrete-continuous action space. The results demonstrate the superiority of HyAR when compared with previous baselines, especially for high-dimensional action spaces",
    "volume": "main",
    "checked": true,
    "id": "c73157e860991103ea56a336ed241c74b5ac4a6f",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=XHUxf5aRB3s": {
    "title": "Dealing with Non-Stationarity in MARL via Trust-Region Decomposition",
    "abstract": "Non-stationarity is one thorny issue in cooperative multi-agent reinforcement learning (MARL). One of the reasons is the policy changes of agents during the learning process. Some existing works have discussed various consequences caused by non-stationarity with several kinds of measurement indicators. This makes the objectives or goals of existing algorithms are inevitably inconsistent and disparate. In this paper, we introduce a novel notion, the δ-stationarity measurement, to explicitly measure the non-stationarity of a policy sequence, which can be further proved to be bounded by the KL-divergence of consecutive joint policies. A straightforward but highly non-trivial way is to control the joint policies' divergence, which is difficult to estimate accurately by imposing the trust-region constraint on the joint policy. Although it has lower computational complexity to decompose the joint policy and impose trust-region constraints on the factorized policies, simple policy factorization like mean-field approximation will lead to more considerable policy divergence, which can be considered as the trust-region decomposition dilemma. We model the joint policy as a pairwise Markov random field and propose a trust-region decomposition network (TRD-Net) based on message passing to estimate the joint policy divergence more accurately. The Multi-Agent Mirror descent policy algorithm with Trust region decomposition, called MAMT, is established by adjusting the trust-region of the local policies adaptively in an end-to-end manner. MAMT can approximately constrain the consecutive joint policies' divergence to satisfy δ-stationarity and alleviate the non-stationarity problem. Our method can bring noticeable and stable performance improvement compared with baselines in cooperative tasks of different complexity",
    "volume": "main",
    "checked": true,
    "id": "79eadadbb5251f74b017de36346833733a6896ce",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=RriDjddCLN": {
    "title": "Language-driven Semantic Segmentation",
    "abstract": "We present LSeg, a novel model for language-driven semantic image segmentation. LSeg uses a text encoder to compute embeddings of descriptive input labels (e.g., \"grass\" or \"building\") together with a transformer-based image encoder that computes dense per-pixel embeddings of the input image. The image encoder is trained with a contrastive objective to align pixel embeddings to the text embedding of the corresponding semantic class. The text embeddings provide a ﬂexible label representation in which semantically similar labels map to similar regions in the embedding space (e.g., \"cat\" and \"furry\"). This allows LSeg to generalize to previously unseen categories at test time, without retraining or even requiring a single additional training sample. We demonstrate that our approach achieves highly competitive zero-shot performance compared to existing zero- and few-shot semantic segmentation methods, and even matches the accuracy of traditional segmentation algorithms when a ﬁxed label set is provided. Code and demo are available at https://github.com/isl-org/lang-seg",
    "volume": "main",
    "checked": true,
    "id": "cc9826c222ac1e81b4b374dd9e0df130f298b1e8",
    "citation_count": 26
  },
  "https://openreview.net/forum?id=fVu3o-YUGQK": {
    "title": "Efficient Self-supervised Vision Transformers for Representation Learning",
    "abstract": "This paper investigates two techniques for developing efﬁcient self-supervised vision transformers (EsViT) for visual representation learning. First, we show through a comprehensive empirical study that multi-stage architectures with sparse self-attentions can signiﬁcantly reduce modeling complexity but with a cost of losing the ability to capture ﬁne-grained correspondences between image regions. Second, we propose a new pre-training task of region matching which allows the model to capture ﬁne-grained region dependencies and as a result signiﬁcantly improves the quality of the learned vision representations. Our results show that combining the two techniques, EsViT achieves 81.3% top-1 accuracy on the ImageNet linear probe evaluation, outperforming prior arts with around an order magnitude of higher throughput. When transferring to downstream linear classiﬁcation tasks, EsViT outperforms its supervised counterpart on 17 out of 18 datasets. The code and pre-trained models are released at: https://github. com/microsoft/esvit 2020). When adding region-matching tasks to BYOL and pre-training 50 epochs, Softmax and MSE yield k -NN accuracy of 37.2% and 34.9%, while the baseline BYOL yields 33.1%. We also replace the region-matching metric in EsViT as MSE , yielding k -NN accuracy 72.6%, which lower than the view-level task only (74.2%). These results show that Softmax is essential in L R . ( ii ) Optimal Transport (OT) vs Simple Argmax. To avoid heavy computational overhead, a simple feature-level argmax solution is considered in Eq. (2) to pair two local regions. To study the impact of high region-matching quality, we consider OT. Empirically, we observe OT yields slightly higher k -NN accuracy at the early stage, but the gain is diminished in the end. Considering the extra computational cost of solving OT with an inner loop in sinkhorn algorithm (Cuturi, 2013), we opt for simple argmax in our experiments",
    "volume": "main",
    "checked": true,
    "id": "b70bb1855e217edffb5dfa0632e8216860821870",
    "citation_count": 70
  },
  "https://openreview.net/forum?id=5-2mX9_U5i": {
    "title": "Sqrt(d) Dimension Dependence of Langevin Monte Carlo",
    "abstract": "This article considers the popular MCMC method of unadjusted Langevin Monte Carlo (LMC) and provides a non-asymptotic analysis of its sampling error in 2Wasserstein distance. The proof is based on a refinement of mean-square analysis in Li et al. (2019), and this refined framework automates the analysis of a large class of sampling algorithms based on discretizations of contractive SDEs. Using this framework, we establish an Õ (√ d/ ) mixing time bound for LMC, without warm start, under the common log-smooth and log-strongly-convex conditions, plus a growth condition on the 3rd-order derivative of the potential of target measures. This bound improves the best previously known Õ ( d/ ) result and is optimal (in terms of order) in both dimension d and accuracy tolerance for target measures satisfying the aforementioned assumptions. Our theoretical analysis is further validated by numerical experiments",
    "volume": "main",
    "checked": true,
    "id": "f71004edb4c3c26874837b55108e82e1b1a92818",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=sNuFKTMktcY": {
    "title": "Active Hierarchical Exploration with Stable Subgoal Representation Learning",
    "abstract": "Goal-conditioned hierarchical reinforcement learning (GCHRL) provides a promising approach to solving long-horizon tasks. Recently, its success has been extended to more general settings by concurrently learning hierarchical policies and subgoal representations. Although GCHRL possesses superior exploration ability by decomposing tasks via subgoals, existing GCHRL methods struggle in temporally extended tasks with sparse external rewards, since the high-level policy learning relies on external rewards. As the high-level policy selects subgoals in an online learned representation space, the dynamic change of the subgoal space severely hinders effective high-level exploration. In this paper, we propose a novel regularization that contributes to both stable and efficient subgoal representation learning. Building upon the stable representation, we design measures of novelty and potential for subgoals, and develop an active hierarchical exploration strategy that seeks out new promising subgoals and states without intrinsic rewards. Experimental results show that our approach significantly outperforms state-of-the-art baselines in continuous control tasks with sparse rewards",
    "volume": "main",
    "checked": true,
    "id": "167b5d14442024f3af48ae4e79c6fb41ac87b42b",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=DmpCfq6Mg39": {
    "title": "Omni-Dimensional Dynamic Convolution",
    "abstract": "Learning a single static convolutional kernel 1 in each convolutional layer is the common training paradigm of modern Convolutional Neural Networks (CNNs). Instead, recent research in dynamic convolution shows that learning a linear combination of n convolutional kernels weighted with their input-dependent attentions can significantly improve the accuracy of light-weight CNNs, while maintaining efficient inference. However, we observe that existing works endow convolutional kernels with the dynamic property through one dimension (regarding the convolutional kernel number) of the kernel space, but the other three dimensions (regarding the spatial size, the input channel number and the output channel number for each convolutional kernel) are overlooked. Inspired by this, we present Omni-dimensional Dynamic Convolution (ODConv), a more generalized yet elegant dynamic convolution design, to advance this line of research. ODConv leverages a novel multi-dimensional attention mechanism with a parallel strategy to learn complementary attentions for convolutional kernels along all four dimensions of the kernel space at any convolutional layer. As a drop-in replacement of regular convolutions, ODConv can be plugged into many CNN architectures. Extensive experiments on the ImageNet and MS-COCO datasets show that ODConv brings solid accuracy boosts for various prevailing CNN backbones including both light-weight and large ones, e.g., 3.77%∼5.71%|1.86%∼3.72% absolute top-1 improvements to MobivleNetV2|ResNet family on the ImageNet dataset. Intriguingly, thanks to its improved feature learning ability, ODConv with even one single kernel can compete with or outperform existing dynamic convolution counterparts with multiple kernels, substantially reducing extra parameters. Furthermore, ODConv is also superior to other attention modules for modulating the output features or the convolutional weights. Code and models will be available at https://github.com/OSVAI/ODConv",
    "volume": "main",
    "checked": true,
    "id": "77e8c2d78638056cd7a347d7c6e36406ca176dbd",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=fvLLcIYmXb": {
    "title": "AS-MLP: An Axial Shifted MLP Architecture for Vision",
    "abstract": "An Axial Shifted MLP architecture (AS-MLP) is proposed in this paper. Different from MLP-Mixer, where the global spatial feature is encoded for information flow through matrix transposition and one token-mixing MLP, we pay more attention to the local features interaction. By axially shifting channels of the feature map, ASMLP is able to obtain the information flow from different axial directions, which captures the local dependencies. Such an operation enables us to utilize a pure MLP architecture to achieve the same local receptive field as CNN-like architecture. We can also design the receptive field size and dilation of blocks of AS-MLP, etc, in the same spirit of convolutional neural networks. With the proposed ASMLP architecture, our model obtains 83.3% Top-1 accuracy with 88M parameters and 15.2 GFLOPs on the ImageNet-1K dataset. Such a simple yet effective architecture outperforms all MLP-based architectures and achieves competitive performance compared to the transformer-based architectures (e.g., Swin Transformer) even with slightly lower FLOPs. In addition, AS-MLP is also the first MLP-based architecture to be applied to the downstream tasks (e.g., object detection and semantic segmentation). The experimental results are also impressive. Our proposed AS-MLP obtains 51.5 mAP on the COCO validation set and 49.5 MS mIoU on the ADE20K dataset, which is competitive compared to the transformer-based architectures. Our AS-MLP establishes a strong baseline of MLP-based architecture. Code is available at https://github.com/svip-lab/AS-MLP",
    "volume": "main",
    "checked": true,
    "id": "71363797140647ebb3f540584de0a8758d2f7aa2",
    "citation_count": 52
  },
  "https://openreview.net/forum?id=MTex8qKavoS": {
    "title": "MetaShift: A Dataset of Datasets for Evaluating Contextual Distribution Shifts and Training Conflicts",
    "abstract": "Understanding the performance of machine learning models across diverse data distributions is critically important for reliable applications. Mo-tivated by this, there is a growing focus on curat-ing benchmark datasets that capture distribution shifts. In this work, we present MetaShift—a collection of 12,868 sets of natural images across 410 classes—to address this challenge. We leverage the natural heterogeneity of Visual Genome and its annotations to construct MetaShift. The key construction idea is to cluster images using its metadata, which provides context for each image (e.g. cats with cars or cats in bathroom ) that represent distinct data distributions. MetaShift has two important benefits: first, it contains or-ders of magnitude more natural data shifts than previously available. Second, it provides explicit explanations of what is unique about each of its data sets and a distance score that measures the amount of distribution shift between any two of its data sets. Importantly, MetaShift can be readily used to evaluate any ImageNet pre-trained vision model, as we have matched MetaShift with ImageNet hierarchy. The matched version covers 867 out of 1,000 classes in ImageNet-1k. Each class in the ImageNet-matched MetaShift contains 2301.6 images on average, and 19.3 subsets capturing images in different contexts. We also propose methods to construct either binary or multiclass classification tasks, providing access to evaluate the model's robustness across diverse distribution shifts",
    "volume": "main",
    "checked": true,
    "id": "081964f4aea6bb4950b86b2603d3956df876af9b",
    "citation_count": 15
  },
  "https://openreview.net/forum?id=BjyvwnXXVn_": {
    "title": "EViT: Expediting Vision Transformers via Token Reorganizations",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "293535c2b0ef674e1ed9a7ba227e37cca35e5e4b",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=uVXEKeqJbNa": {
    "title": "Stiffness-aware neural network for learning Hamiltonian systems",
    "abstract": "We propose stiffness-aware neural network (SANN), a new method for learning Hamiltonian dynamical systems from data. SANN identiﬁes and splits the training data into stiff and nonstiff portions based on a stiffness-aware index, a simple, yet effective metric we introduce to quantify the stiffness of the dynamical system. This classiﬁcation along with a resampling technique allows us to apply different time integration strategies such as step size adaptation to better capture the dynamical characteristics of the Hamiltonian vector ﬁelds. We evaluate SANN on complex physical systems including a three-body problem and billiard model. We show that SANN is more stable and can better preserve energy when compared with the state-of-the-art methods, leading to signiﬁcant improvement in accuracy",
    "volume": "main",
    "checked": true,
    "id": "df0dc45dbfdc6525cb210b16d83b7a4ef873b1ca",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=cuvga_CiVND": {
    "title": "No Parameters Left Behind: Sensitivity Guided Adaptive Learning Rate for Training Large Transformer Models",
    "abstract": "Recent research has shown the existence of significant redundancy in large Transformer models. One can prune the redundant parameters without significantly sacrificing the generalization performance. However, we question whether the redundant parameters could have contributed more if they were properly trained. To answer this question, we propose a novel training strategy that encourages all parameters to be trained sufficiently. Specifically, we adaptively adjust the learning rate for each parameter according to its sensitivity, a robust gradient-based measure reflecting this parameter's contribution to the model performance. A parameter with low sensitivity is redundant, and we improve its fitting by increasing its learning rate. In contrast, a parameter with high sensitivity is well-trained, and we regularize it by decreasing its learning rate to prevent further overfitting. We conduct extensive experiments on natural language understanding, neural machine translation, and image classification to demonstrate the effectiveness of the proposed schedule. Analysis shows that the proposed schedule indeed reduces the redundancy and improves generalization performance",
    "volume": "main",
    "checked": true,
    "id": "52e8102e070dbed745c39fd518f4f6aa3daffb3c",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=OWZVD-l-ZrC": {
    "title": "Reward Uncertainty for Exploration in Preference-based Reinforcement Learning",
    "abstract": "Conveying complex objectives to reinforcement learning (RL) agents often requires meticulous reward engineering. Preference-based RL methods are able to learn a more ﬂexible reward model based on human preferences by actively incorporating human feedback, i.e. teacher's preferences between two clips of behaviors. However, poor feedback-efﬁciency still remains a problem in current preference-based RL algorithms, as tailored human feedback is very expensive. To handle this issue, previous methods have mainly focused on improving query selection and policy initialization. At the same time, recent exploration methods have proven to be a recipe for improving sample-efﬁciency in RL. We present an exploration method speciﬁcally for preference-based RL algorithms. Our main idea is to design an intrinsic reward by measuring the novelty based on learned reward. Speciﬁcally, we utilize disagreement across ensemble of learned reward models. Our intuition is that disagreement in learned reward model reﬂects uncertainty in tailored human feedback and could be useful for exploration. Our experiments show that exploration bonus from uncertainty in learned reward improves both feedback- and sample-efﬁciency of preference-based RL algorithms on complex robot manipulation tasks from MetaWorld benchmarks, compared with other existing exploration methods that measure the novelty of state visitation",
    "volume": "main",
    "checked": true,
    "id": "83a9eabd8b5130aad44cc68117842ceed91b4f08",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=oAy7yPmdNz": {
    "title": "CoordX: Accelerating Implicit Neural Representation with a Split MLP Architecture",
    "abstract": "Implicit neural representations with multi-layer perceptrons (MLPs) have recently gained prominence for a wide variety of tasks such as novel view synthesis and 3D object representation and rendering. However, a significant challenge with these representations is that both training and inference with an MLP over a large number of input coordinates to learn and represent an image, video, or 3D object, require large amounts of computation and incur long processing times. In this work, we aim to accelerate inference and training of coordinate-based MLPs for implicit neural representations by proposing a new split MLP architecture, CoordX. With CoordX, the initial layers are split to learn each dimension of the input coordinates separately. The intermediate features are then fused by the last layers to generate the learned signal at the corresponding coordinate point. This significantly reduces the amount of computation required and leads to large speedups in training and inference, while achieving similar accuracy as the baseline MLP. This approach thus aims at first learning functions that are a decomposition of the original signal and then fusing them to generate the learned signal. Our proposed architecture can be generally used for many implicit neural representation tasks with no additional memory overheads. We demonstrate a speedup of up to 2.92x compared to the baseline model for image, video, and 3D shape representation and rendering tasks",
    "volume": "main",
    "checked": true,
    "id": "a6ba988fded4d8c6d66efa8744fccc1ad23733e7",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=HFmAukZ-k-2": {
    "title": "Learning the Dynamics of Physical Systems from Sparse Observations with Finite Element Networks",
    "abstract": "We propose a new method for spatio-temporal forecasting on arbitrarily distributed points. Assuming that the observed system follows an unknown partial differential equation, we derive a continuous-time model for the dynamics of the data via the finite element method. The resulting graph neural network estimates the instantaneous effects of the unknown dynamics on each cell in a meshing of the spatial domain. Our model can incorporate prior knowledge via assumptions on the form of the unknown PDE, which induce a structural bias towards learning specific processes. Through this mechanism, we derive a transport variant of our model from the convection equation and show that it improves the transfer performance to higher-resolution meshes on sea surface temperature and gas flow forecasting against baseline models representing a selection of spatio-temporal forecasting methods. A qualitative analysis shows that our model disentangles the data dynamics into their constituent parts, which makes it uniquely interpretable",
    "volume": "main",
    "checked": true,
    "id": "162b00580f87c9b0a2ac1c17658b316f4b038b05",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=M6M8BEmd6dq": {
    "title": "PEARL: Data Synthesis via Private Embeddings and Adversarial Reconstruction Learning",
    "abstract": "We propose a new framework of synthesizing data using deep generative models in a differentially private manner. Within our framework, sensitive data are sanitized with rigorous privacy guarantees in a one-shot fashion, such that training deep generative models is possible without re-using the original data. Hence, no extra privacy costs or model constraints are incurred, in contrast to popular approaches such as Differentially Private Stochastic Gradient Descent (DP-SGD), which, among other issues, causes degradation in privacy guarantees as the training iteration increases. We demonstrate a realization of our framework by making use of the characteristic function and an adversarial re-weighting objective, which are of independent interest as well. Our proposal has theoretical guarantees of performance, and empirical evaluations on multiple datasets show that our approach outperforms other methods at reasonable levels of privacy",
    "volume": "main",
    "checked": true,
    "id": "ab118ecc89641bb30beefa2a7b916aec63671694",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=vJb4I2ANmy": {
    "title": "Noisy Feature Mixup",
    "abstract": "We introduce Noisy Feature Mixup (NFM), an inexpensive yet effective method for data augmentation that combines the best of interpolation based training and noise injection schemes. Rather than training with convex combinations of pairs of examples and their labels, we use noise-perturbed convex combinations of pairs of data points in both input and feature space. This method includes mixup and manifold mixup as special cases, but it has additional advantages, including better smoothing of decision boundaries and enabling improved model robustness. We provide theory to understand this as well as the implicit regularization effects of NFM. Our theory is supported by empirical results, demonstrating the advantage of NFM, as compared to mixup and manifold mixup. We show that residual networks and vision transformers trained with NFM have favorable trade-offs between predictive accuracy on clean data and robustness with respect to various types of data perturbation across a range of computer vision benchmark datasets",
    "volume": "main",
    "checked": true,
    "id": "cb4c989386da6c2381c358bf6f37740785b20e4d",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=Kef8cKdHWpP": {
    "title": "DiffSkill: Skill Abstraction from Differentiable Physics for Deformable Object Manipulations with Tools",
    "abstract": "We consider the problem of sequential robotic manipulation of deformable objects using tools. Previous works have shown that differentiable physics simulators provide gradients to the environment state and help trajectory optimization to converge orders of magnitude faster than model-free reinforcement learning algorithms for deformable object manipulation. However, such gradient-based trajectory optimization typically requires access to the full simulator states and can only solve short-horizon, single-skill tasks due to local optima. In this work, we propose a novel framework, named DiffSkill, that uses a differentiable physics simulator for skill abstraction to solve long-horizon deformable object manipulation tasks from sensory observations. In particular, we ﬁrst obtain short-horizon skills using individual tools from a gradient-based optimizer, using the full state information in a differentiable simulator; we then learn a neural skill abstractor from the demonstration trajectories which takes RGBD images as input. Finally, we plan over the skills by ﬁnding the intermediate goals and then solve long-horizon tasks. We show the advantages of our method in a new set of sequential deformable object manipulation tasks compared to previous reinforcement learning algorithms and compared to the trajectory optimizer. Videos are available at our project page 1",
    "volume": "main",
    "checked": true,
    "id": "807e519409d9b51ea0dbf8b8c7f0c1d2d1c74bbb",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=ufGMqIM0a4b": {
    "title": "InfinityGAN: Towards Infinite-Pixel Image Synthesis",
    "abstract": "We present a novel framework, InfinityGAN, for arbitrary-sized image generation. The task is associated with several key challenges. First, scaling existing models to an arbitrarily large image size is resource-constrained, in terms of both computation and availability of large-field-of-view training data. InfinityGAN trains and infers in a seamless patch-by-patch manner with low computational resources. Second, large images should be locally and globally consistent, avoid repetitive patterns, and look realistic. To address these, InfinityGAN disentangles global appearances, local structures, and textures. With this formulation, we can generate images with spatial size and level of details not attainable before. Experimental evaluation validates that InfinityGAN generates images with superior realism compared to baselines and features parallelizable inference. Finally, we show several applications unlocked by our approach, such as spatial style fusion, multimodal outpainting, and image inbetweening. All applications can be operated with arbitrary input and output sizes",
    "volume": "main",
    "checked": true,
    "id": "91e396fc0d0def65026df8d9a0f0d7b98df81f49",
    "citation_count": 13
  },
  "https://openreview.net/forum?id=SsPCtEY6yCl": {
    "title": "On the Uncomputability of Partition Functions in Energy-Based Sequence Models",
    "abstract": "In this paper, we argue that energy-based sequence models backed by expressive parametric families can result in uncomputable and inapproximable partition functions. Among other things, this makes model selection — and therefore learning model parameters — not only diﬃcult, but generally undecidable . The reason is that there are no good deterministic or randomized estimators of partition functions. Speciﬁcally, we exhibit a pathological example where under common assumptions, no useful importance sampling estimators of the partition function can guarantee to have variance bounded below a rational number. As alternatives, we consider sequence model families whose partition functions are computable (if they exist), but at the cost of reduced expressiveness. Our theoretical results suggest that statistical procedures with asymptotic guarantees and sheer (but ﬁnite) amounts of compute are not the only things that make sequence modeling work; computability concerns must not be neglected as we consider more expressive model parametrizations",
    "volume": "main",
    "checked": true,
    "id": "dc04b92fc0ceec383dd4d262ff759a3cbd4cd3f6",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=EBn0uInJZWh": {
    "title": "Model-Based Offline Meta-Reinforcement Learning with Regularization",
    "abstract": "Existing ofﬂine reinforcement learning (RL) methods face a few major challenges, particularly the distributional shift between the learned policy and the behavior policy. Ofﬂine Meta-RL is emerging as a promising approach to address these challenges, aiming to learn an informative meta-policy from a collection of tasks. Nevertheless, as shown in our empirical studies, ofﬂine Meta-RL could be outperformed by ofﬂine single-task RL methods on tasks with good quality of datasets, indicating that a right balance has to be delicately calibrated between \"exploring\" the out-of-distribution state-actions by following the meta-policy and \"exploiting\" the ofﬂine dataset by staying close to the behavior policy. Motivated by such empirical analysis, we propose model-based ofﬂine Meta-RL with regularized Policy Optimization (MerPO), which learns a meta-model for efﬁcient task structure inference and an informative meta-policy for safe exploration of out-of-distribution state-actions. In particular, we devise a new meta-Regularized model-based Actor-Critic (RAC) method for within-task policy optimization, as a key building block of MerPO, using both conservative policy evaluation and regularized policy improvement; and the intrinsic tradeoff therein is achieved via striking the right balance between two regularizers, one based on the behavior policy and the other on the meta-policy. We theoretically show that the learnt policy offers guaranteed improvement over both the behavior policy and the meta-policy, thus ensuring the performance improvement on new tasks via ofﬂine Meta-RL. Experiments the state-of-the-art performance; (2) (Li et 2020a), an ofﬂine multi-task RL approach with metric (3) Batch PEARL, which modiﬁes PEARL (Rakelly et 2019) to train and test from ofﬂine datasets without exploration; (4) Contextual BCQ (CBCQ), which is a task-augmented variant of the ofﬂine RL algorithm BCQ (Fujimoto et al., 2019) by integrating a task latent variable into the state information. We train on a set of ofﬂine RL tasks, and evaluate the performance of the learnt meta-policy during the training process on a set of unseen testing ofﬂine RL tasks",
    "volume": "main",
    "checked": true,
    "id": "02ad21eea9ec32783ba529487e74a76e85499a53",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=iEvAf8i6JjO": {
    "title": "TRGP: Trust Region Gradient Projection for Continual Learning",
    "abstract": "Catastrophic forgetting is one of the major challenges in continual learning. To address this issue, some existing methods put restrictive constraints on the optimization space of the new task for minimizing the interference to old tasks. However, this may lead to unsatisfactory performance for the new task, especially when the new task is strongly correlated with old tasks. To tackle this challenge, we propose Trust Region Gradient Projection (TRGP) for continual learning to facilitate the forward knowledge transfer based on an efﬁcient characterization of task correlation. Particularly, we introduce a notion of ‘trust region' to select the most related old tasks for the new task in a layer-wise and single-shot manner, using the norm of gradient projection onto the subspace spanned by task inputs. Then, a scaled weight projection is proposed to cleverly reuse the frozen weights of the selected old tasks in the trust region through a layer-wise scaling matrix. By jointly opti-mizing the scaling matrices and the model, where the model is updated along the directions orthogonal to the subspaces of old tasks, TRGP can effectively prompt knowledge transfer without forgetting. Extensive experiments show that our approach achieves signiﬁcant improvement over related state-of-the-art methods",
    "volume": "main",
    "checked": true,
    "id": "b895a5c79406c8db2807e78b00fdeadc80e5a774",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=KntaNRo6R48": {
    "title": "L0-Sparse Canonical Correlation Analysis",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "ee8e867420d7e3603dc5e8aab50555a386e23a13",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=rhOiUS8KQM9": {
    "title": "Enabling Arbitrary Translation Objectives with Adaptive Tree Search",
    "abstract": "We introduce an adaptive tree search algorithm, that can find high-scoring outputs under translation models that make no assumptions about the form or structure of the search objective. This algorithm – a deterministic variant of Monte Carlo tree search – enables the exploration of new kinds of models that are unencumbered by constraints imposed to make decoding tractable, such as autoregressivity or conditional independence assumptions. When applied to autoregressive models, our algorithm has different biases than beam search has, which enables a new analysis of the role of decoding bias in autoregressive models. Empirically, we show that our adaptive tree search algorithm finds outputs with substantially better model scores compared to beam search in autoregressive models, and compared to reranking techniques in models whose scores do not decompose additively with respect to the words in the output. We also characterise the correlation of several translation model objectives with respect to BLEU. We find that while some standard models are poorly calibrated and benefit from the beam search bias, other often more robust models (autoregressive models tuned to maximize expected automatic metric scores, the noisy channel model and a newly proposed objective) benefit from increasing amounts of search using our proposed decoder, whereas the beam search bias limits the improvements obtained from such objectives. Thus, we argue that as models improve, the improvements may be masked by over-reliance on beam search or reranking based methods",
    "volume": "main",
    "checked": true,
    "id": "4bd990e865aea7de457e917d3b0f8412567bdb5d",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=eYciPrLuUhG": {
    "title": "Efficient Neural Causal Discovery without Acyclicity Constraints",
    "abstract": "Learning the structure of a causal graphical model using both observational and interventional data is a fundamental problem in many scientific fields. A promising direction is continuous optimization for score-based methods, which efficiently learn the causal graph in a data-driven manner. However, to date, those methods require constrained optimization to enforce acyclicity or lack convergence guarantees. In this paper, we present ENCO, an efficient structure learning method for directed, acyclic causal graphs leveraging observational and interventional data. ENCO formulates the graph search as an optimization of independent edge likelihoods, with the edge orientation being modeled as a separate parameter. Consequently, we can provide convergence guarantees of ENCO under mild conditions without constraining the score function with respect to acyclicity. In experiments, we show that ENCO can efficiently recover graphs with hundreds of nodes, an order of magnitude larger than what was previously possible, while handling deterministic variables and latent confounders",
    "volume": "main",
    "checked": true,
    "id": "7275c0f95c1597c394874aeb6dc730a14e428779",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=J_PHjw4gvXJ": {
    "title": "Improving the Accuracy of Learning Example Weights for Imbalance Classification",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "1c90744253b2737712da17230d1c7f12a17f26c8",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=PlKWVd2yBkY": {
    "title": "Pseudo Numerical Methods for Diffusion Models on Manifolds",
    "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) can generate high-quality samples such as image and audio samples. However, DDPMs require hundreds to thousands of iterations to produce final samples. Several prior works have successfully accelerated DDPMs through adjusting the variance schedule (e.g., Improved Denoising Diffusion Probabilistic Models) or the denoising equation (e.g., Denoising Diffusion Implicit Models (DDIMs)). However, these acceleration methods cannot maintain the quality of samples and even introduce new noise at a high speedup rate, which limit their practicability. To accelerate the inference process while keeping the sample quality, we provide a fresh perspective that DDPMs should be treated as solving differential equations on manifolds. Under such a perspective, we propose pseudo numerical methods for diffusion models (PNDMs). Specifically, we figure out how to solve differential equations on manifolds and show that DDIMs are simple cases of pseudo numerical methods. We change several classical numerical methods to corresponding pseudo numerical methods and find that the pseudo linear multi-step method is the best in most situations. According to our experiments, by directly using pre-trained models on Cifar10, CelebA and LSUN, PNDMs can generate higher quality synthetic images with only 50 steps compared with 1000-step DDIMs (20x speedup), significantly outperform DDIMs with 250 steps (by around 0.4 in FID) and have good generalization on different variance schedules.1",
    "volume": "main",
    "checked": true,
    "id": "82482585e94192b4e9913727e461f89cd08e9725",
    "citation_count": 31
  },
  "https://openreview.net/forum?id=Az-7gJc6lpr": {
    "title": "Relational Learning with Variational Bayes",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "71ad3fdcb38268ea092e1ace8344864b8c532688",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=B6EIcyp-Rb7": {
    "title": "Learning Object-Oriented Dynamics for Planning from Text",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "07cf239db20908b18227648df28cefda640806c5",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=RLtqs6pzj1-": {
    "title": "Deep Ensembling with No Overhead for either Training or Testing: The All-Round Blessings of Dynamic Sparsity",
    "abstract": "The success of deep ensembles on improving predictive performance, uncertainty estimation, and out-of-distribution robustness has been extensively studied in the machine learning literature. Albeit the promising results, naively training multiple deep neural networks and combining their predictions at inference leads to prohibitive computational costs and memory requirements. Recently proposed efficient ensemble approaches reach the performance of the traditional deep ensembles with significantly lower costs. However, the training resources required by these approaches are still at least the same as training a single dense model. In this work, we draw a unique connection between sparse neural network training and deep ensembles, yielding a novel efficient ensemble learning framework called FreeT ickets. Instead of training multiple dense networks and averaging them, we directly train sparse subnetworks from scratch and extract diverse yet accurate subnetworks during this efficient, sparse-to-sparse training. Our framework, FreeT ickets, is defined as the ensemble of these relatively cheap sparse subnetworks. Despite being an ensemble method, FreeT ickets has even fewer parameters and training FLOPs than a single dense model. This seemingly counterintuitive outcome is due to the ultra training/inference efficiency of dynamic sparse training. FreeT ickets surpasses the dense baseline in all the following criteria: prediction accuracy, uncertainty estimation, out-of-distribution (OoD) robustness, as well as efficiency for both training and inference. Impressively, FreeT ickets outperforms the naive deep ensemble with ResNet50 on ImageNet using around only 1/5 of the training FLOPs required by the latter. We have released our source code at https://github.com/VITA-Group/FreeTickets",
    "volume": "main",
    "checked": true,
    "id": "439d158e3ab3910d836535dd1aec693f5c0420cf",
    "citation_count": 13
  },
  "https://openreview.net/forum?id=VBZJ_3tz-t": {
    "title": "The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training",
    "abstract": "Random pruning is arguably the most naive way to attain sparsity in neural networks, but has been deemed uncompetitive by either post-training pruning or sparse training. In this paper, we focus on sparse training and highlight a perhaps counter-intuitive finding, that random pruning at initialization can be quite powerful for the sparse training of modern neural networks. Without any delicate pruning criteria or carefully pursued sparsity structures, we empirically demonstrate that sparsely training a randomly pruned network from scratch can match the performance of its dense equivalent. There are two key factors that contribute to this revival: (i) the network sizes matter: as the original dense networks grow wider and deeper, the performance of training a randomly pruned sparse network will quickly grow to matching that of its dense equivalent, even at high sparsity ratios; (ii) appropriate layer-wise sparsity ratios can be pre-chosen for sparse training, which shows to be another important performance booster. Simple as it looks, a randomly pruned subnetwork of Wide ResNet-50 can be sparsely trained to outperforming a dense Wide ResNet-50, on ImageNet. We also observed such randomly pruned networks outperform dense counterparts in other favorable aspects, such as out-of-distribution detection, uncertainty estimation, and adversarial robustness. Overall, our results strongly suggest there is larger-thanexpected room for sparse training at scale, and the benefits of sparsity might be more universal beyond carefully designed pruning. Our source code can be found at https://github.com/VITA-Group/Random_Pruning",
    "volume": "main",
    "checked": true,
    "id": "821b08d595b6482e3d1f5bab6835b72d67ebd894",
    "citation_count": 12
  },
  "https://openreview.net/forum?id=rw1mZl_ss3L": {
    "title": "Concurrent Adversarial Learning for Large-Batch Training",
    "abstract": "Large-batch training has become a commonly used technique when training neural networks with a large number of GPU/TPU processors. As batch size increases, stochastic optimizers tend to converge to sharp local minima, leading to degraded test performance. Current methods usually use extensive data augmentation to increase the batch size, but we found the performance gain with data augmentation decreases as batch size increases, and data augmentation will become insufficient after certain point. In this paper, we propose to use adversarial learning to increase the batch size in large-batch training. Despite being a natural choice for smoothing the decision surface and biasing towards a flat region, adversarial learning has not been successfully applied in large-batch training since it requires at least two sequential gradient computations at each step, which will at least double the running time compared with vanilla training even with a large number of processors. To overcome this issue, we propose a novel Concurrent Adversarial Learning (ConAdv) method that decouple the sequential gradient computations in adversarial learning by utilizing staled parameters. Experimental results demonstrate that ConAdv can successfully increase the batch size on both ResNet-50 and EfficientNet training on ImageNet while maintaining high accuracy. In particular, we show ConAdv along can achieve 75.3% top-1 accuracy on ImageNet ResNet-50 training with 96K batch size, and the accuracy can be further improved to 76.2% when combining ConAdv with data augmentation. This is the first work successfully scales ResNet-50 training batch size to 96K",
    "volume": "main",
    "checked": true,
    "id": "dece6a5bd2246a09777f5201e7f530b00c4b5ba8",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=O50443AsCP": {
    "title": "TAPEX: Table Pre-training via Learning a Neural SQL Executor",
    "abstract": "Recent years pre-trained language models hit a success on modeling natural language sentences and (semi-)structured tables. However, existing table pre-training techniques always suffer from low data quality and low pre-training efficiency. In this paper, we show that table pre-training can be realized by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. By pre-training on the synthetic corpus, our approach TAPEX dramatically improves the performance on downstream tasks, boosting existing language models by at most 19.5%. Meanwhile, TAPEX has remarkably high pretraining efficiency and yields strong results when using a small pre-trained corpus. Experimental results demonstrate that TAPEX outperforms previous table pre-training approaches by a large margin, and our model achieves new state-of-the-art results on four well-known datasets, including improving the WIKISQL denotation accuracy to 89.6% (+4.9%), the WIKITABLEQUESTIONS denotation accuracy to 57.5% (+4.8%), the SQA denotation accuracy to 74.5% (+3.5%), and the TABFACT accuracy to 84.6% (+3.6%). Our work opens the way to reason over structured data by pre-training on synthetic executable programs. The project homepage is at https: //table-pretraining.github.io/",
    "volume": "main",
    "checked": true,
    "id": "2406cf39805c70264c4226b7325a09b506c70921",
    "citation_count": 37
  },
  "https://openreview.net/forum?id=ULfq0qR25dY": {
    "title": "Maximum n-times Coverage for Vaccine Design",
    "abstract": "We introduce the maximum $n$-times coverage problem that selects $k$ overlays to maximize the summed coverage of weighted elements, where each element must be covered at least $n$ times. We also define the min-cost $n$-times coverage problem where the objective is to select the minimum set of overlays such that the sum of the weights of elements that are covered at least $n$ times is at least $τ$. Maximum $n$-times coverage is a generalization of the multi-set multi-cover problem, is NP-complete, and is not submodular. We introduce two new practical solutions for $n$-times coverage based on integer linear programming and sequential greedy optimization. We show that maximum $n$-times coverage is a natural way to frame peptide vaccine design, and find that it produces a pan-strain COVID-19 vaccine design that is superior to 29 other published designs in predicted population coverage and the expected number of peptides displayed by each individual's HLA molecules",
    "volume": "main",
    "checked": true,
    "id": "32b2818ecf36bd8ad9a5b0e864c7f1326717920d",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=-TSe5o7STVR": {
    "title": "Non-Parallel Text Style Transfer with Self-Parallel Supervision",
    "abstract": "The performance of existing text style transfer models is severely limited by the non-parallel datasets on which the models are trained. In non-parallel datasets, no direct mapping exists between sentences of the source and target style; the style transfer models thus only receive weak supervision of the target sentences during training, which often leads the model to discard too much style-independent information, or utterly fail to transfer the style. In this work, we propose LaMer, a novel text style transfer framework based on large-scale language models. LaMer first mines the roughly parallel expressions in the non-parallel datasets with scene graphs, and then employs MLE training, followed by imitation learning refinement, to leverage the intrinsic parallelism within the data. On two benchmark tasks (sentiment & formality transfer) and a newly proposed challenging task (political stance transfer), our model achieves qualitative advances in transfer accuracy, content preservation, and fluency. Further empirical and human evaluations demonstrate that our model not only makes training more efficient, but also generates more readable and diverse expressions than previous models",
    "volume": "main",
    "checked": true,
    "id": "85e5ed73c769141c5eb2f51410c8979818169cde",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=4AZz9osqrar": {
    "title": "Self-supervised Learning is More Robust to Dataset Imbalance",
    "abstract": "Self-supervised learning (SSL) is a scalable way to learn general visual representations since it learns without labels. However, large-scale unlabeled datasets in the wild often have long-tailed label distributions, where we know little about the behavior of SSL. In this work, we systematically investigate self-supervised learning under dataset imbalance. First, we ﬁnd out via extensive experiments that oﬀ-the-shelf self-supervised representations are already more robust to class imbalance than supervised representations. The performance gap between balanced and imbalanced pre-training with SSL is signiﬁcantly smaller than the gap with supervised learning, across sample sizes, for both in-domain and, especially, out-of-domain evaluation. Second, towards understanding the robustness of SSL, we hypothesize that SSL learns richer features from frequent data: it may learn label-irrelevant-but-transferable features that help classify the rare classes and downstream tasks. In contrast, supervised learning has no incentive to learn features irrelevant to the labels from frequent examples. We validate this hypothesis with semi-synthetic experiments and theoretical analyses on a simpliﬁed setting. Third, inspired by the theoretical insights, we devise a re-weighted regularization technique that consistently improves the SSL representation quality on imbalanced datasets with several evaluation criteria, closing the small gap between balanced and imbalanced datasets with the same number of examples",
    "volume": "main",
    "checked": true,
    "id": "a01ac66f5f66a2b23152f631b920972e4407275c",
    "citation_count": 35
  },
  "https://openreview.net/forum?id=7YDLgf9_zgm": {
    "title": "Continual Learning with Recursive Gradient Optimization",
    "abstract": "Learning multiple tasks sequentially without forgetting previous knowledge, called Continual Learning (CL), remains a long-standing challenge for neural networks. Most existing methods rely on additional network capacity or data replay. In contrast, we introduce a novel approach which we refer to as Recursive Gradient Optimization (RGO). RGO is composed of an iteratively updated optimizer that modiﬁes the gradient to minimize forgetting without data replay and a virtual Feature Encoding Layer (FEL) that represents different network structures with only task descriptors. Experiments demonstrate that RGO has signiﬁcantly better performance on popular continual classiﬁcation benchmarks when compared to the baselines and achieves new state-of-the-art performance on 20-split-CIFAR100 (82.22%) and 20-split-miniImageNet (72.63%). With higher average accuracy than Single-Task Learning (STL), this method is ﬂexible and reliable to provide continual learning capabilities for learning models that rely on gradient descent",
    "volume": "main",
    "checked": true,
    "id": "088be49337c7a91eafe0eae85592e0f8a80f7e43",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=pgir5f7ekAL": {
    "title": "Generative Principal Component Analysis",
    "abstract": "In this paper, we study the problem of principal component analysis with generative modeling assumptions, adopting a general model for the observed matrix that encompasses notable special cases, including spiked matrix recovery and phase retrieval. The key assumption is that the underlying signal lies near the range of an L -Lipschitz continuous generative model with bounded k -dimensional inputs. We propose a quadratic estimator, and show that it enjoys a statistical rate of order (cid:113) k log L m , where m is the number of samples. We also provide a near-matching algorithm-independent lower bound. Moreover, we provide a variant of the classic power method, which projects the calculated data onto the range of the generative model during each iteration. We show that under suitable conditions, this method converges exponentially fast to a point achieving the above-mentioned statistical rate. We perform experiments on various image datasets for spiked matrix and phase retrieval models, and illustrate performance gains of our method to the classic power method and the truncated power method devised for sparse principal component analysis",
    "volume": "main",
    "checked": true,
    "id": "48111a61b287b4bf938c9053b273afae9f2e6160",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=uorVGbWV5sw": {
    "title": "Strength of Minibatch Noise in SGD",
    "abstract": "The noise in stochastic gradient descent (SGD), caused by minibatch sampling, is poorly understood despite its practical importance in deep learning. This work presents the first systematic study of the SGD noise and fluctuations close to a local minimum. We first analyze the SGD noise in linear regression in detail and then derive a general formula for approximating SGD noise in different types of minima. For application, our results (1) provide insight into the stability of training a neural network, (2) suggest that a large learning rate can help generalization by introducing an implicit regularization, (3) explain why the linear learning ratebatchsize scaling law fails at a large learning rate or at a small batchsize and (4) can provide an understanding of how discrete-time nature of SGD affects the recently discovered power-law phenomenon of SGD",
    "volume": "main",
    "checked": true,
    "id": "6cd064b3fb736cef8404ab24227add67c7c23779",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=9XhPLAjjRB": {
    "title": "SGD Can Converge to Local Maxima",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "5ec766770e4cd2bdb927a384ef1e0dbc63b31b24",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=oMI9PjOb9Jl": {
    "title": "DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR",
    "abstract": "We present in this paper a novel query formulation using dynamic anchor boxes for DETR (DEtection TRansformer) and offer a deeper understanding of the role of queries in DETR. This new formulation directly uses box coordinates as queries in Transformer decoders and dynamically updates them layer-by-layer. Using box coordinates not only helps using explicit positional priors to improve the queryto-feature similarity and eliminate the slow training convergence issue in DETR, but also allows us to modulate the positional attention map using the box width and height information. Such a design makes it clear that queries in DETR can be implemented as performing soft ROI pooling layer-by-layer in a cascade manner. As a result, it leads to the best performance on MS-COCO benchmark among the DETR-like detection models under the same setting, e.g., AP 45.7% using ResNet50-DC5 as backbone trained in 50 epochs. We also conducted extensive experiments to confirm our analysis and verify the effectiveness of our methods. Code is available at https://github.com/SlongLiu/DAB-DETR",
    "volume": "main",
    "checked": true,
    "id": "004f1d2b1b7d7dcecafdd94daee9c1b0aa3e65cf",
    "citation_count": 43
  },
  "https://openreview.net/forum?id=X_hByk2-5je": {
    "title": "Lossless Compression with Probabilistic Circuits",
    "abstract": "Despite extensive progress on image generation, common deep generative model architectures are not easily applied to lossless compression. For example, VAEs suffer from a compression cost overhead due to their latent variables. This overhead can only be partially eliminated with elaborate schemes such as bits-back coding, often resulting in poor single-sample compression rates. To overcome such problems, we establish a new class of tractable lossless compression models that permit efficient encoding and decoding: Probabilistic Circuits (PCs). These are a class of neural networks involving |p| computational units that support efficient marginalization over arbitrary subsets of the D feature dimensions, enabling efficient arithmetic coding. We derive efficient encoding and decoding schemes that both have time complexity O(log(D) · |p|), where a naive scheme would have linear costs in D and |p|, making the approach highly scalable. Empirically, our PC-based (de)compression algorithm runs 5-40 times faster than neural compression algorithms that achieve similar bitrates. By scaling up the traditional PC structure learning pipeline, we achieve state-of-the-art results on image datasets such as MNIST. Furthermore, PCs can be naturally integrated with existing neural compression algorithms to improve the performance of these base models on natural image datasets. Our results highlight the potential impact that non-standard learning architectures may have on neural data compression",
    "volume": "main",
    "checked": true,
    "id": "43157e4f2e4c7f85cdfa9e4a92eff7b3bcecb2c7",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=MIX3fJkl_1": {
    "title": "NeuPL: Neural Population Learning",
    "abstract": "Learning in strategy games (e.g. StarCraft, poker) requires the discovery of diverse policies. This is often achieved by iteratively training new policies against existing ones, growing a policy population that is robust to exploit. This iterative approach suffers from two issues in real-world games: a) under finite budget, approximate best-response operators at each iteration needs truncating, resulting in under-trained good-responses populating the population; b) repeated learning of basic skills at each iteration is wasteful and becomes intractable in the presence of increasingly strong opponents. In this work, we propose Neural Population Learning (NeuPL) as a solution to both issues. NeuPL offers convergence guarantees to a population of best-responses under mild assumptions. By representing a population of policies within a single conditional model, NeuPL enables transfer learning across policies. Empirically, we show the generality, improved performance and efficiency of NeuPL across several test domains1. Most interestingly, we show that novel strategies become more accessible, not less, as the neural population expands. The need for learning not one, but a population of strategies is rooted in classical game theory. Consider the purely cyclical game of rock-paper-scissors, the performance of individual strategies is meaningless as improving against one entails losing to another. By contrast, performance can be meaningfully examined between populations. A population consisting of pure strategies {rock, paper} does well against a singleton population of {scissors} because in the meta-game where both populations are revealed, a player picking strategies from the former can always beat a player choosing from the latter2. This observation underpins the unifying population learning framework of Policy Space Response Oracle (PSRO) where a new policy is trained to best-respond to a mixture over previous policies at each iteration, following a meta-strategy solver (Lanctot et al., 2017). Most impressively, Vinyals et al. (2019) explored the strategy game of StarCraft with a league of policies, using a practical variation of PSRO. The league counted close to a thousand sophisticated deep RL agents as the population collectively became robust to exploits. Unfortunately, such empirical successes often come at considerable costs. Population learning algorithms with theoretical guarantees are traditionally studied in normal-form games (Brown, 1951; McMahan et al., 2003) where best-responses can be solved exactly. This is in stark contrast to real-world Game-of-Skills (Czarnecki et al., 2020) — such games are often temporal in nature, where best-responses can only be approximated with computationally intensive methods (e.g. deep RL). This has two implications. First, for a given opponent, one cannot efficiently tell apart good-responses that temporarily plateaued at local optima from globally optimal best-responses. As a result, approximate best-response operators are often truncated prematurely, according to hand-crafted schedules (Lanctot et al., 2017; Mcaleer et al., 2020). Second, real-world games often afford strategy-agnostic transitive ∗Currently at Reality Labs, work carried out while at DeepMind. †Work carried out while at DeepMind. See https://neupl.github.io/demo/ for supplementary illustrations. This is formally quantified by Relative Population Performance, see Definition A.1 (Balduzzi et al., 2019)",
    "volume": "main",
    "checked": true,
    "id": "080072f28bd52a35c01ee4caecf94887eb3d54c9",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=eBS-3YiaIL-": {
    "title": "Analyzing and Improving the Optimization Landscape of Noise-Contrastive Estimation",
    "abstract": "Noise-contrastive estimation (NCE) is a statistically consistent method for learning unnormalized probabilistic models. It has been empirically observed that the choice of the noise distribution is crucial for NCE's performance. However, such observations have never been made formal or quantitative. In fact, it is not even clear whether the difficulties arising from a poorly chosen noise distribution are statistical or algorithmic in nature. In this work, we formally pinpoint reasons for NCE's poor performance when an inappropriate noise distribution is used. Namely, we prove these challenges arise due to an ill-behaved (more precisely, flat) loss landscape. To address this, we introduce a variant of NCE called eNCE which uses an exponential loss and for which normalized gradient descent addresses the landscape issues provably when the target and noise distributions are in a given exponential family",
    "volume": "main",
    "checked": true,
    "id": "08bfc2eb24636087f2be571bcb39e99ba3930e57",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=V0A5g83gdQ_": {
    "title": "Tuformer: Data-driven Design of Transformers for Improved Generalization or Efficiency",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "338d0501947b5fc7d92d09eed9a3e299f7b48ec1",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=xQUe1pOKPam": {
    "title": "Pre-training Molecular Graph Representation with 3D Geometry",
    "abstract": "Molecular graph representation learning is a fundamental problem in modern drug and material discovery. Molecular graphs are typically modeled by their 2D topological structures, but it has been recently discovered that 3D geometric information plays a more vital role in predicting molecular functionalities. However, the lack of 3D information in real-world scenarios has signiﬁcantly impeded the learning of geometric graph representation. To cope with this challenge, we propose the Graph Multi-View Pre-training (GraphMVP) framework where self-supervised learning (SSL) is performed by leveraging the correspondence and consistency between 2D topological structures and 3D geometric views. GraphMVP effectively learns a 2D molecular graph encoder that is enhanced by richer and more discriminative 3D geometry. We further provide theoretical insights to justify the effectiveness of GraphMVP. Finally, comprehensive experiments show that GraphMVP can consistently outperform existing graph SSL methods. Code is available on GitHub",
    "volume": "main",
    "checked": true,
    "id": "b8f816e23ff40d6afabccca2ee4770087ef0ef57",
    "citation_count": 36
  },
  "https://openreview.net/forum?id=0EXmFzUn5I": {
    "title": "Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "30dcc0e191a376fea0e7a46f94c53872c029efc9",
    "citation_count": 14
  },
  "https://openreview.net/forum?id=NoB8YgRuoFU": {
    "title": "PI3NN: Out-of-distribution-aware Prediction Intervals from Three Neural Networks",
    "abstract": "We propose a novel prediction interval (PI) method for uncertainty quantification, which addresses three major issues with the state-of-the-art PI methods. First, existing PI methods require retraining of neural networks (NNs) for every given confidence level and suffer from the crossing issue in calculating multiple PIs. Second, they usually rely on customized loss functions with extra sensitive hyperparameters for which fine tuning is required to achieve a well-calibrated PI. Third, they usually underestimate uncertainties of out-of-distribution (OOD) samples leading to over-confident PIs. Our PI3NN method calculates PIs from linear combinations of three NNs, each of which is independently trained using the standard mean squared error loss. The coefficients of the linear combinations are computed using root-finding algorithms to ensure tight PIs for a given confidence level. We theoretically prove that PI3NN can calculate PIs for a series of confidence levels without retraining NNs and it completely avoids the crossing issue. Additionally, PI3NN does not introduce any unusual hyperparameters resulting in a stable performance. Furthermore, we address OOD identification challenge by introducing an initialization scheme which provides reasonably larger PIs of the OOD samples than those of the in-distribution samples. Benchmark and real-world experiments show that our method outperforms several state-of-the-art approaches with respect to predictive uncertainty quality, robustness, and OOD samples identification",
    "volume": "main",
    "checked": true,
    "id": "9b799db46342be286610f79a924079f5757976c3",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=BRFWxcZfAdC": {
    "title": "Lossy Compression with Distribution Shift as Entropy Constrained Optimal Transport",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "226425817d69aa51f6df1b7b8eca65e29a4e84fb",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=upnDJ7itech": {
    "title": "Knowledge Infused Decoding",
    "abstract": "Pre-trained language models (LMs) have been shown to memorize a substantial amount of knowledge from the pre-training corpora; however, they are still limited in recalling factually correct knowledge given a certain context. Hence, they tend to suffer from counterfactual or hallucinatory generation when used in knowledge-intensive natural language generation (NLG) tasks. Recent remedies to this problem focus on modifying either the pre-training or task fine-tuning objectives to incorporate knowledge, which normally require additional costly training or architecture modification of LMs for practical applications. We present Knowledge Infused Decoding (KID)—a novel decoding algorithm for generative LMs, which dynamically infuses external knowledge into each step of the LM decoding. Specifically, we maintain a local knowledge memory based on the current context, interacting with a dynamically created external knowledge trie, and continuously update the local memory as a knowledge-aware constraint to guide decoding via reinforcement learning. On six diverse knowledgeintensive NLG tasks, task-agnostic LMs (e.g., GPT-2 and BART) armed with KID outperform many task-optimized state-of-the-art models, and show particularly strong performance in few-shot scenarios over seven related knowledge-infusion techniques. Human evaluation confirms KID's ability to generate more relevant and factual language for the input context when compared with multiple baselines. Finally, KID also alleviates exposure bias and provides stable generation quality when generating longer sequences. Code for KID is available at https:// github.com/microsoft/KID",
    "volume": "main",
    "checked": true,
    "id": "706c6b3781374b0b11f98f204a4ddd05b26ed009",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=6u6N8WWwYSM": {
    "title": "Bootstrapping Semantic Segmentation with Regional Contrast",
    "abstract": "We present ReCo, a contrastive learning framework designed at a regional level to assist learning in semantic segmentation. ReCo performs semi-supervised or supervised pixel-level contrastive learning on a sparse set of hard negative pixels, with minimal additional memory footprint. ReCo is easy to implement, being built on top of offthe-shelf segmentation networks, and consistently improves performance in both semisupervised and supervised semantic segmentation methods, achieving smoother segmentation boundaries and faster convergence. The strongest effect is in semi-supervised learning with very few labels. With ReCo, we achieve high quality semantic segmentation models, requiring only 5 examples of each semantic class. Code is available at https://github.com/lorenmt/reco",
    "volume": "main",
    "checked": true,
    "id": "419da16abc452464aa4f13587eff2c544ae28f17",
    "citation_count": 28
  },
  "https://openreview.net/forum?id=U4uFaLyg7PV": {
    "title": "T-WaveNet: A Tree-Structured Wavelet Neural Network for Time Series Signal Analysis",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "8332f1ea4f84a0ca19df1842e9f3c4705aff2f47",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=9SDQB3b68K": {
    "title": "DARA: Dynamics-Aware Reward Augmentation in Offline Reinforcement Learning",
    "abstract": "Offline reinforcement learning algorithms promise to be applicable in settings where a fixed dataset is available and no new experience can be acquired. However, such formulation is inevitably offline-data-hungry and, in practice, collecting a large offline dataset for one specific task over one specific environment is also costly and laborious. In this paper, we thus 1) formulate the offline dynamics adaptation by using (source) offline data collected from another dynamics to relax the requirement for the extensive (target) offline data, 2) characterize the dynamics shift problem in which prior offline methods do not scale well, and 3) derive a simple dynamics-aware reward augmentation (DARA) framework from both modelfree and model-based offline settings. Specifically, DARA emphasizes learning from those source transition pairs that are adaptive for the target environment and mitigates the offline dynamics shift by characterizing state-action-next-state pairs instead of the typical state-action distribution sketched by prior offline RL methods. The experimental evaluation demonstrates that DARA, by augmenting rewards in the source offline dataset, can acquire an adaptive policy for the target environment and yet significantly reduce the requirement of target offline data. With only modest amounts of target offline data, our performance consistently outperforms the prior offline RL methods in both simulated and real-world tasks",
    "volume": "main",
    "checked": true,
    "id": "33fae337de4cbf73dcf55acac1a2605fb727eadb",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=vkaMaq95_rX": {
    "title": "EXACT: Scalable Graph Neural Networks Training via Extreme Activation Compression",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "72567f3953c5479e09aacf48dfd888e38000b699",
    "citation_count": 16
  },
  "https://openreview.net/forum?id=BK-4qbGgIE3": {
    "title": "No One Representation to Rule Them All: Overlapping Features of Training Methods",
    "abstract": "Despite being able to capture a range of features of the data, high accuracy models trained with supervision tend to make similar predictions. This seemingly implies that high-performing models share similar biases regardless of training methodology, which would limit ensembling benefits and render low-accuracy models as having little practical use. Against this backdrop, recent work has developed quite different training techniques, such as large-scale contrastive learning, yielding competitively high accuracy on generalization and robustness benchmarks. This motivates us to revisit the assumption that models necessarily learn similar functions. We conduct a large-scale empirical study of models across hyper-parameters, architectures, frameworks, and datasets. We find that model pairs that diverge more in training methodology display categorically different generalization behavior, producing increasingly uncorrelated errors. We show these models specialize in subdomains of the data, leading to higher ensemble performance: with just 2 models (each with ImageNet accuracy 7̃6.5%), we can create ensembles with 83.4% (+7% boost). Surprisingly, we find that even significantly low-accuracy models can be used to improve high-accuracy models. Finally, we show diverging training methodology yield representations that capture overlapping (but not supersetting) feature sets which, when combined, lead to increased downstream performance",
    "volume": "main",
    "checked": true,
    "id": "5bcc379da187b69d705a81e93bf5ddbb90cda1b1",
    "citation_count": 17
  },
  "https://openreview.net/forum?id=Zf4ZdI4OQPV": {
    "title": "Attacking deep networks with surrogate-based adversarial black-box methods is easy",
    "abstract": "A recent line of work on black-box adversarial attacks has revived the use of transfer from surrogate models by integrating it into query-based search. However, we find that existing approaches of this type underperform their potential, and can be overly complicated besides. Here, we provide a short and simple algorithm which achieves state-of-the-art results through a search which uses the surrogate network's classscore gradients, with no need for other priors or heuristics. The guiding assumption of the algorithm is that the studied networks are in a fundamental sense learning similar functions, and that a transfer attack from one to the other should thus be fairly \"easy\". This assumption is validated by the extremely low query counts and failure rates achieved: e.g. an untargeted attack on a VGG-16 ImageNet network using a ResNet-152 as the surrogate yields a median query count of 6 at a success rate of 99.9%. Code is available at https://github.com/fiveai/GFCS",
    "volume": "main",
    "checked": true,
    "id": "7d3675344a3ae1b639cce4208779f914c3a9a68d",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=kF9DZQQrU0w": {
    "title": "Information Bottleneck: Exact Analysis of (Quantized) Neural Networks",
    "abstract": "The information bottleneck (IB) principle has been suggested as a way to analyze deep neural networks. The learning dynamics are studied by inspecting the mutual information (MI) between the hidden layers and the input and output. Notably, separate fitting and compression phases during training have been reported. This led to some controversy including claims that the observations are not reproducible and strongly dependent on the type of activation function used as well as on the way the MI is estimated. Our study confirms that different ways of binning when computing the MI lead to qualitatively different results, either supporting or refusing IB conjectures. To resolve the controversy, we study the IB principle in settings where MI is non-trivial and can be computed exactly. We monitor the dynamics of quantized neural networks, that is, we discretize the whole deep learning system so that no approximation is required when computing the MI. This allows us to quantify the information flow without measurement errors. In this setting, we observed a fitting phase for all layers and a compression phase for the output layer in all experiments; the compression in the hidden layers was dependent on the type of activation function. Our study shows that the initial IB results were not artifacts of binning when computing the MI. However, the critical claim that the compression phase may not be observed for some networks also holds true",
    "volume": "main",
    "checked": true,
    "id": "0436bade813530e3f9f1441d8881a2d14c27ef9e",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=KhLK0sHMgXK": {
    "title": "NASPY: Automated Extraction of Automated Machine Learning Models",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "76b2638a986099d9f5092f81e45974319f3daeea",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=GWQWAeE9EpB": {
    "title": "DictFormer: Tiny Transformer with Shared Dictionary",
    "abstract": "et al., 2020; Lou et al., 2020b), pruning (Behnke & Heaﬁeld, 2020), low-rank factorization (Ma et al., 2019), and knowledge distillation (Wang et al., 2020). These two directions of research can be combined with our Dictformer. The third line of research is efﬁcient architecture design (So et al., 2019; Wu et al., 2020; Mehta et al., 2021) by improving the expressiveness of transformers. The forth line of research is weights sharing (Xia et al., 2019; Ma et al., 2019; Dehghani et al., 2019; Reid et al., 2021; Takase & Kiyono, 2021) by reusing parameters across transformer blocks. Weights sharing cannot reduce computations. Our Dictformer falls into the third and forth category. We show that our Dictformer with Dictionary sharing can reduce both model size and computations. on 36M training sentence pairs from WMT'14, validating on newstest2012 and 2013, and testing on newstest2014. The vocabulary has a size of 40K and is based on a joint source and target BPE factorization. The evaluation setting used in Vaswani et al. is utilized with a beam size of 4 and a length penalty of 0.6. We replicate the same BLEU calculations in Wu et al. (2020) with case-sensitive tokenization. The last 10 model checkpoints are averaged for testing and the lowest-perplexity model is picked for validation",
    "volume": "main",
    "checked": true,
    "id": "bd55dc24da4110ca58286f1ef59c88b0290c9cd9",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=dpXL6lz4mOQ": {
    "title": "Learning Guarantees for Graph Convolutional Networks on the Stochastic Block Model",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "0e7f3ee94083170392b27a7198a47d39ce311b1c",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=zz9hXVhf40": {
    "title": "Revisiting Design Choices in Offline Model Based Reinforcement Learning",
    "abstract": "Offline reinforcement learning enables agents to leverage large pre-collected datasets of environment transitions to learn control policies, circumventing the need for potentially expensive or unsafe online data collection. Significant progress has been made recently in offline model-based reinforcement learning, approaches which leverage a learned dynamics model. This typically involves constructing a probabilistic model, and using the model uncertainty to penalize rewards where there is insufficient data, solving for a pessimistic MDP that lower bounds the true MDP. Existing methods, however, exhibit a breakdown between theory and practice, whereby pessimistic return ought to be bounded by the total variation distance of the model from the true dynamics, but is instead implemented through a penalty based on estimated model uncertainty. This has spawned a variety of uncertainty heuristics, with little to no comparison between differing approaches. In this paper, we compare these heuristics, and design novel protocols to investigate their interaction with other hyperparameters, such as the number of models, or imaginary rollout horizon. Using these insights, we show that selecting these key hyperparameters using Bayesian Optimization produces superior configurations that are vastly different to those currently used in existing hand-tuned state-of-the-art methods, and result in drastically stronger performance",
    "volume": "main",
    "checked": true,
    "id": "877e8140cb7f26042f6c5f1eefcf68a2748721f0",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=2eXhNpHeW6E": {
    "title": "R5: Rule Discovery with Reinforced and Recurrent Relational Reasoning",
    "abstract": "spaces based on pairs of relations. Besides, R5 performs rule induction with a dynamic rule memory module, which is updated during the training of an agent. We further incorporate hidden (invented) relations in our state and action spaces to help accelerate model training as well as improve rule mining. R5 exhibits high accuracy for relation prediction and a high recall rate for rule discovery, as has been demonstrated by extensive experimental results. We also show that R5 has a strong ability of systematicity and is robust to data noise",
    "volume": "main",
    "checked": true,
    "id": "477e4f98e03a88628b86605e10c9d2f9fbc73a0e",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=7gWSJrP3opB": {
    "title": "A General Analysis of Example-Selection for Stochastic Gradient Descent",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "c0b07ce929c821ab6f7d84ce424d3850a516555e",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=-e4EXDWXnSn": {
    "title": "Invariant Causal Representation Learning for Out-of-Distribution Generalization",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "164b72b345a4cb03f62547abf62a033dcbd784ae",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=WHA8009laxu": {
    "title": "Federated Learning from Only Unlabeled Data with Class-conditional-sharing Clients",
    "abstract": "Supervised federated learning (FL) enables multiple clients to share the trained model without sharing their labeled data. However, potential clients might even be reluctant to label their own data, which could limit the applicability of FL in practice. In this paper, we show the possibility of unsupervised FL whose model is still a classifier for predicting class labels, if the class-prior probabilities are shifted while the class-conditional distributions are shared among the unlabeled data owned by the clients. We propose federation of unsupervised learning (FedUL), where the unlabeled data are transformed into surrogate labeled data for each of the clients, a modified model is trained by supervised FL, and the wanted model is recovered from the modified model. FedUL is a very general solution to unsupervised FL: it is compatible with many supervised FL methods, and the recovery of the wanted model can be theoretically guaranteed as if the data have been labeled. Experiments on benchmark and real-world datasets demonstrate the effectiveness of FedUL. Code is available at https://github.com/lunanbit/FedUL",
    "volume": "main",
    "checked": true,
    "id": "4b821bde79e97eed4cb1d1fa8705f68e3ae137cd",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=C03Ajc-NS5W": {
    "title": "An Autoregressive Flow Model for 3D Molecular Geometry Generation from Scratch",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "0756fa74ec4bae4d1cf2b0f40b0d7fc843ee0fcc",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=C8Ltz08PtBp": {
    "title": "Distributional Reinforcement Learning with Monotonic Splines",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "95ab0295c77a7064ea8aa50e98d9ca617c70d254",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=l8It-0lE5e7": {
    "title": "Implicit Bias of Adversarial Training for Deep Neural Networks",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "c33ce165a65ce4372cfc64d1e6cbd6e9b6b1bf0d",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=ZkC8wKoLbQ7": {
    "title": "Understanding and Preventing Capacity Loss in Reinforcement Learning",
    "abstract": "The reinforcement learning (RL) problem is rife with sources of non-stationarity, making it a notoriously difﬁcult problem domain for the application of neural networks. We identify a mechanism by which non-stationary prediction targets can prevent learning progress in deep RL agents: capacity loss , whereby networks trained on a sequence of target values lose their ability to quickly update their predictions over time. We demonstrate that capacity loss occurs in a range of RL agents and environments, and is particularly damaging to performance in sparse-reward tasks. We then present a simple regularizer, Initial Feature Regularization (InFeR), that mitigates this phenomenon by regressing a subspace of features towards its value at initialization, leading to signiﬁcant performance improvements in sparse-reward environments such as Montezuma's Revenge. We conclude that preventing capacity loss is crucial to enable agents to maximally beneﬁt from the learning signals they obtain throughout the entire training trajectory. rank of agent representations over the course of training on all 57 games in the Atari benchmark. We compare Rainbow against Rainbow+InFeR. Rainbow+InFeR does not uniformly prevent decreases in feature rank across all games, but on average it has a beneﬁcial effect on preserving representation dimension",
    "volume": "main",
    "checked": true,
    "id": "0a818d426c813b7a4758c959d38ece5f2cc11476",
    "citation_count": 12
  },
  "https://openreview.net/forum?id=5FUq05QRc5b": {
    "title": "Understanding Latent Correlation-Based Multiview Learning and Self-Supervision: An Identifiability Perspective",
    "abstract": "Multiple views of data, both naturally acquired (e.g., image and audio) and arti-ﬁcially produced (e.g., via adding different noise to data samples), have proven useful in enhancing representation learning. Natural views are often handled by multiview analysis tools, e.g., (deep) canonical correlation analysis [(D)CCA], while the artiﬁcial ones are frequently used in self-supervised learning (SSL) paradigms, e.g., BYOL and Barlow Twins . Both types of approaches often involve learning neural feature extractors such that the embeddings of data exhibit high cross-view correlations. Although intuitive, the effectiveness of correlation-based neural embedding is mostly empirically validated. This work aims to understand latent correlation maximization-based deep multiview learning from a latent component identiﬁcation viewpoint. An intuitive generative model of multiview data is adopted, where the views are different nonlinear mixtures of shared and private components. Since the shared components are view/distortion-invariant, representing the data using such components is believed to reveal the identity of the samples effectively and robustly. Under this model, latent correlation maximization is shown to guarantee the extraction of the shared components across views (up to certain ambiguities). In addition, it is further shown that the private information in each view can be provably disentangled from the shared using proper regularization design. A ﬁnite sample analysis, which has been rare in nonlinear mixture identiﬁability study, is also presented. The theoretical results and newly designed regularization are tested on a series of tasks",
    "volume": "main",
    "checked": true,
    "id": "72311c24962cd27b0f7820ece1895f4d416d26ac",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=wqD6TfbYkrn": {
    "title": "A Conditional Point Diffusion-Refinement Paradigm for 3D Point Cloud Completion",
    "abstract": "3D point cloud is an important 3D representation for capturing real world 3D objects. However, real-scanned 3D point clouds are often incomplete, and it is important to recover complete point clouds for downstream applications. Most existing point cloud completion methods use Chamfer Distance (CD) loss for training. The CD loss estimates correspondences between two point clouds by searching nearest neighbors, which does not capture the overall point density distribution on the generated shape, and therefore likely leads to non-uniform point cloud generation. To tackle this problem, we propose a novel Point Diffusion-Refinement (PDR) paradigm for point cloud completion. PDR consists of a Conditional Generation Network (CGNet) and a ReFinement Network (RFNet). The CGNet uses a conditional generative model called the denoising diffusion probabilistic model (DDPM) to generate a coarse completion conditioned on the partial observation. DDPM establishes a one-to-one pointwise mapping between the generated point cloud and the uniform ground truth, and then optimizes the mean squared error loss to realize uniform generation. The RFNet refines the coarse output of the CGNet and further improves quality of the completed point cloud. Furthermore, we develop a novel dual-path architecture for both networks. The architecture can (1) effectively and efficiently extract multi-level features from partially observed point clouds to guide completion, and (2) accurately manipulate spatial locations of 3D points to obtain smooth surfaces and sharp details. Extensive experimental results on various benchmark datasets show that our PDR paradigm outperforms previous state-of-the-art methods for point cloud completion. Remarkably, with the help of the RFNet, we can accelerate the iterative generation process of the DDPM by up to 50 times without much performance drop",
    "volume": "main",
    "checked": true,
    "id": "c940509c5b1ee8db9e4ce70254726719b8d56c54",
    "citation_count": 13
  },
  "https://openreview.net/forum?id=6kCiVaoQdx9": {
    "title": "Few-shot Learning via Dirichlet Tessellation Ensemble",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "ab3ad1b6d55a3ca0ca9183e9a1a96735fe1c500d",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=TZeArecH2Nf": {
    "title": "Bridging Recommendation and Marketing via Recurrent Intensity Modeling",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "a4cca89cc692d83442c436d24464343cc4341f01",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=afoV8W3-IYp": {
    "title": "RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning",
    "abstract": "Reasoning about visual relationships is central to how humans interpret the visual world. This task remains challenging for current deep learning algorithms since it requires addressing three key technical problems jointly: 1) identifying object entities and their properties, 2) inferring semantic relations between pairs of entities, and 3) generalizing to novel object-relation combinations, i.e. systematic generalization. In this work, we use vision transformers (ViTs) as our base model for visual reasoning and make better use of concepts defined as object entities and their relations to improve the reasoning ability of ViTs. Specifically, we introduce a novel concept-feature dictionary to allow flexible image feature retrieval at training time with concept keys. This dictionary enables two new conceptguided auxiliary tasks: 1) a global task for promoting relational reasoning, and 2) a local task for facilitating semantic object-centric correspondence learning. To examine the systematic generalization of visual reasoning models, we introduce systematic splits for the standard HICO and GQA benchmarks. We show the resulting model, Concept-guided Vision Transformer (or RelViT for short) significantly outperforms prior approaches on HICO and GQA by 16% and 13% in the original split, and by 43% and 18% in the systematic split. Our ablation analyses also reveal our model's compatibility with multiple ViT variants and robustness to hyper-parameters",
    "volume": "main",
    "checked": true,
    "id": "1c062b3575c9cb9be5dd60da0175f33bc084b780",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=xa6otUDdP2W": {
    "title": "Effective Model Sparsification by Scheduled Grow-and-Prune Methods",
    "abstract": "Deep neural networks (DNNs) are effective in solving many real-world problems. Larger DNN models usually exhibit better quality (e.g., accuracy) but their excessive computation results in long training and inference time. Model sparsification can reduce the computation and memory cost while maintaining model quality. Most existing sparsification algorithms unidirectionally remove weights, while others randomly or greedily explore a small subset of weights in each layer for pruning. The limitations of these algorithms reduce the level of sparsity. In addition, many algorithms still require pre-trained dense models and thus suffer from large memory footprint. In this paper, we propose a novel scheduled grow-and-prune (GaP) methodology without having to pre-train a dense model. It addresses the shortcomings of the previous work by repeatedly growing a subset of layers to dense and then pruning them back to sparse after some training. Experiments have shown that such models can match or beat the quality of highly optimized dense models at 80% sparsity on a variety of tasks, such as image classification, objective detection, 3D object part segmentation, and translation. They also outperform other state-of-the-art (SOTA) methods for model sparsification. As an example, a 90% sparse ResNet-50 obtained via GaP achieves 77.9% top-1 accuracy on ImageNet, improving the SOTA results of sparsification algorithms by 1.5%",
    "volume": "main",
    "checked": true,
    "id": "279b5affd1a3aec43e1f9d9c21ae69b0d1aa7928",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=3Pbra-_u76D": {
    "title": "Rethinking Network Design and Local Geometry in Point Cloud: A Simple Residual MLP Framework",
    "abstract": "Point cloud analysis is challenging due to irregularity and unordered data structure. To capture the 3D geometries, prior works mainly rely on exploring sophisticated local geometric extractors, using convolution, graph, or attention mechanisms. These methods, however, incur unfavorable latency during inference and the performance saturates over the past few years. In this paper, we present a novel perspective on this task. We find detailed local geometrical information probably is not the key to point cloud analysis – we introduce a pure residual MLP network, called PointMLP, which integrates no local geometrical extractors but still performs very competitively. Equipped with a proposed lightweight geometric affine module to stabilize the training, PointMLP delivers the new stateof-the-art on multiple datasets. On the real-world ScanObjectNN dataset, our method even surpasses the prior best method by 3.3% accuracy. We emphasize PointMLP achieves this strong performance without any sophisticated operations, hence leading to a prominent inference speed. Compared to most recent CurveNet, PointMLP trains 2× faster, tests 7× faster, and is more accurate on ModelNet40 benchmark. We hope our PointMLP may help the community towards a better understanding of point cloud analysis. The code is available at https://anonymous.4open.science/r/pointMLP-pytorch/",
    "volume": "main",
    "checked": true,
    "id": "8c121704ae00e61ad169b8e293b94e51e8e5a973",
    "citation_count": 47
  },
  "https://openreview.net/forum?id=RCZqv9NXlZ": {
    "title": "Offline Reinforcement Learning with Value-based Episodic Memory",
    "abstract": "Offline reinforcement learning (RL) shows promise of applying RL to real-world problems by effectively utilizing previously collected data. Most existing offline RL algorithms use regularization or constraints to suppress extrapolation error for actions outside the dataset. In this paper, we adopt a different framework, which learns the V -function instead of the Q-function to naturally keep the learning procedure within the support of an offline dataset. To enable effective generalization while maintaining proper conservatism in offline learning, we propose Expectile V -Learning (EVL), which smoothly interpolates between the optimal value learning and behavior cloning. Further, we introduce implicit planning along offline trajectories to enhance learned V -values and accelerate convergence. Together, we present a new offline method called Value-based Episodic Memory (VEM). We provide theoretical analysis for the convergence properties of our proposed VEM method, and empirical results in the D4RL benchmark show that our method achieves superior performance in most tasks, particularly in sparse-reward tasks",
    "volume": "main",
    "checked": true,
    "id": "b7ba291af983b5e31f3adfa9a0ceb7a7b3114c7f",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=9Hrka5PA7LW": {
    "title": "Representational Continuity for Unsupervised Continual Learning",
    "abstract": "Continual learning (CL) aims to learn a sequence of tasks without forgetting the previously acquired knowledge. However, recent CL advances are restricted to supervised continual learning (SCL) scenarios. Consequently, they are not scalable to real-world applications where the data distribution is often biased and unannotated. In this work, we focus on unsupervised continual learning (UCL), where we learn the feature representations on an unlabelled sequence of tasks and show that reliance on annotated data is not necessary for continual learning. We conduct a systematic study analyzing the learned feature representations and show that unsupervised visual representations are surprisingly more robust to catastrophic forgetting, consistently achieve better performance, and generalize better to out-ofdistribution tasks than SCL. Furthermore, we find that UCL achieves a smoother loss landscape through qualitative analysis of the learned representations and learns meaningful feature representations. Additionally, we propose Lifelong Unsupervised Mixup (LUMP), a simple yet effective technique that interpolates between the current task and previous tasks' instances to alleviate catastrophic forgetting for unsupervised representations. We release our code online",
    "volume": "main",
    "checked": true,
    "id": "771e0af5535a0122004c265d8d17931c710677b6",
    "citation_count": 13
  },
  "https://openreview.net/forum?id=v8OlxjGn23S": {
    "title": "Low-Budget Active Learning via Wasserstein Distance: An Integer Programming Approach",
    "abstract": "Given restrictions on the availability of data, active learning is the process of training a model with limited labeled data by selecting a core subset of an unlabeled data pool to label. Although selecting the most useful points for training is an optimization problem, the scale of deep learning data sets forces most selection strategies to employ efficient heuristics. Instead, we propose a new integer optimization problem for selecting a core set that minimizes the discrete Wasserstein distance from the unlabeled pool. We demonstrate that this problem can be tractably solved with a Generalized Benders Decomposition algorithm. Our strategy requires highquality latent features which we obtain by unsupervised learning on the unlabeled pool. Numerical results on several data sets show that our optimization approach is competitive with baselines and particularly outperforms them in the low budget regime where less than one percent of the data set is labeled",
    "volume": "main",
    "checked": true,
    "id": "72f17c5757e5e44dfea5a9af7a97319371a3eac4",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=fCSq8yrDkc": {
    "title": "A fast and accurate splitting method for optimal transport: analysis and implementation",
    "abstract": "We develop a fast and reliable method for solving large-scale optimal transport (OT) problems at an unprecedented combination of speed and accuracy. Built on the celebrated Douglas-Rachford splitting technique, our method tackles the original OT problem directly instead of solving an approximate regularized problem, as many state-of-the-art techniques do. This allows us to provide sparse transport plans and avoid numerical issues of methods that use entropic regularization. The algorithm has the same cost per iteration as the popular Sinkhorn method, and each iteration can be executed efficiently, in parallel. The proposed method enjoys an iteration complexity O(1/ ) compared to the best-known O(1/ ) of the Sinkhorn method. In addition, we establish a linear convergence rate for our formulation of the OT problem. We detail an efficient GPU implementation of the proposed method that maintains a primal-dual stopping criterion at no extra cost. Substantial experiments demonstrate the effectiveness of our method, both in terms of computation times and robustness",
    "volume": "main",
    "checked": true,
    "id": "2bacba88a59ac9f307fc450d388bf03921e0466d",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=vrW3tvDfOJQ": {
    "title": "Sample Efficient Deep Reinforcement Learning via Uncertainty Estimation",
    "abstract": "In model-free deep reinforcement learning (RL) algorithms, using noisy value estimates to supervise policy evaluation and optimization is detrimental to the sample efﬁciency. As this noise is heteroscedastic, its effects can be mitigated using uncertainty-based weights in the optimization process. Previous methods rely on sampled ensembles, which do not capture all aspects of uncertainty. We provide a systematic analysis of the sources of uncertainty in the noisy supervision that occurs in RL, and introduce inverse-variance RL, a Bayesian framework which combines probabilistic ensembles and Batch Inverse Variance weighting. We propose a method whereby two complementary uncertainty estimation methods ac-count for both the Q-value and the environment stochasticity to better mitigate the negative impacts of noisy supervision. Our results show signiﬁcant improvement in terms of sample efﬁciency on discrete and continuous control tasks. et al., 2017) to estimate the uncertainty of the target due to the prediction of Q ( s (cid:48) , a (cid:48) ) during the temporal-difference update. We merge the predicted variances of several variance networks through a mixture of Gaussians, which has been shown to be a reliable method to capture predictive uncertainty (Ovadia et al., 2019). We then use Batch Inverse-Variance (BIV) (Mai et al., 2021), which has been shown to signiﬁcantly improve the performance of supervised learning with neural networks in the case of heteroscedastic regression. BIV is normalized, which makes it ideal to cope with different and time-varying scales of variance. We show analytically that these two different variance predictions for the target are complementary and their combination leads to consistent and signiﬁcant improvements in the sample efﬁciency and overall performance of the learning process",
    "volume": "main",
    "checked": true,
    "id": "6f987340adf4f0b90cf900d3e219341cf46248d5",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=POxF-LEqnF": {
    "title": "You Mostly Walk Alone: Analyzing Feature Attribution in Trajectory Prediction",
    "abstract": "Predicting the future trajectory of a moving agent can be easy when the past trajectory continues smoothly but is challenging when complex interactions with other agents are involved. Recent deep learning approaches for trajectory prediction show promising performance and partially attribute this to successful reasoning about agent-agent interactions. However, it remains unclear which features such black-box models actually learn to use for making predictions. This paper pro-poses a procedure that quantiﬁes the contributions of different cues to model performance based on a variant of Shapley values. Applying this procedure to state-of-the-art trajectory prediction methods on standard benchmark datasets shows that they are, in fact, unable to reason about interactions. Instead, the past trajectory of the target is the only feature used for predicting its future. For a task with richer social interaction patterns, on the other hand, the tested models do pick up such interactions to a certain extent, as quantiﬁed by our feature attribution method. We discuss the limits of the proposed method and its links to causality",
    "volume": "main",
    "checked": true,
    "id": "781e22916f75c1340cfae83caaba8aff5fa219a2",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=rI0LYgGeYaw": {
    "title": "Understanding approximate and unrolled dictionary learning for pattern recovery",
    "abstract": "Dictionary learning consists of finding a sparse representation from noisy data and is a common way to encode data-driven prior knowledge on signals. Alternating minimization (AM) is standard for the underlying optimization, where gradient descent steps alternate with sparse coding procedures. The major drawback of this method is its prohibitive computational cost, making it unpractical on large real-world data sets. This work studies an approximate formulation of dictionary learning based on unrolling and compares it to alternating minimization to find the best trade-off between speed and precision. We analyze the asymptotic behavior and convergence rate of gradients estimates in both methods. We show that unrolling performs better on the support of the inner problem solution and during the first iterations. Finally, we apply unrolling on pattern learning in magnetoencephalography (MEG) with the help of a stochastic algorithm and compare the performance to a state-of-the-art method",
    "volume": "main",
    "checked": true,
    "id": "1909e2176bbc8b2c3f88c7d78b37a5e00b697d11",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=RQ428ZptQfU": {
    "title": "A Deep Variational Approach to Clustering Survival Data",
    "abstract": "In this work, we study the problem of clustering survival data — a challenging and so far under-explored task. We introduce a novel semi-supervised probabilistic approach to cluster survival data by leveraging recent advances in stochastic gradient variational inference. In contrast to previous work, our proposed method employs a deep generative model to uncover the underlying distribution of both the explanatory variables and censored survival times. We compare our model to the related work on clustering and mixture models for survival data in comprehensive experiments on a wide range of synthetic, semi-synthetic, and real-world datasets, including medical imaging data. Our method performs better at identifying clusters and is competitive at predicting survival times. Relying on novel generative assumptions, the proposed model offers a holistic perspective on clustering survival data and holds a promise of discovering subpopulations whose survival is regulated by different generative mechanisms",
    "volume": "main",
    "checked": true,
    "id": "9af28c385cd8765c8d243c4ff1b31ca98db43549",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=8hWs60AZcWk": {
    "title": "Discrete Representations Strengthen Vision Transformer Robustness",
    "abstract": "Vision Transformer (ViT) is emerging as the state-of-the-art architecture for image recognition. While recent studies suggest that ViTs are more robust than their convolutional counterparts, our experiments find that ViTs trained on ImageNet are overly reliant on local textures and fail to make adequate use of shape information. ViTs thus have difficulties generalizing to out-of-distribution, real-world data. To address this deficiency, we present a simple and effective architecture modification to ViT's input layer by adding discrete tokens produced by a vectorquantized encoder. Different from the standard continuous pixel tokens, discrete tokens are invariant under small perturbations and contain less information individually, which promote ViTs to learn global information that is invariant. Experimental results demonstrate that adding discrete representation on four architecture variants strengthens ViT robustness by up to 12% across seven ImageNet robustness benchmarks while maintaining the performance on ImageNet",
    "volume": "main",
    "checked": true,
    "id": "601ab36b6f077ff57472f4a0cf2e061dd05b9b85",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=3pugbNqOh5m": {
    "title": "Practical Conditional Neural Process Via Tractable Dependent Predictions",
    "abstract": "to In a class of models that correlated exact maximum training is and scalable. We the models by using output transformations, to capture output distributions. Our models be used in estimation for dependencies, our models improved predictive on of with functions. The resulting GNP models, account for output correlations, but unlike existing methods, they can be applied to data with higher-dimensional inputs while maintaining an analytically tractable log-likelihood, which makes them especially easy to train. GNPs can be extended to multi-output regression, and also combined with invertible marginal transformations to model non-Gaussian data. We demonstrate that modelling correlations improves predictive performance over mean-ﬁeld models on Gaussian and non-Gaussian synthetic data, including a downstream estimation task that mean-ﬁeld models cannot solve. Our models also show improved performance over their mean-ﬁeld and latent counterparts on real EEG and climate tasks. In statistical temperature downscaling, our models outperform a standard ensemble of widely used methods in statistical downscaling, while providing spatially coherent temperature samples. This renders our models suitable for application to climate impact studies",
    "volume": "main",
    "checked": true,
    "id": "3a8b89dcfc947bc052326cf4df37594cef7672da",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=45Mr7LeKR9": {
    "title": "Explanations of Black-Box Models based on Directional Feature Interactions",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "a4ab2885279de2a34e7554aff01ce4c2a8512cf3",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=p98WJxUC3Ca": {
    "title": "Discrepancy-Based Active Learning for Domain Adaptation",
    "abstract": "The goal of the paper is to design active learning strategies which lead to domain adaptation under an assumption of Lipschitz functions. Building on previous work by Mansour et al. (2009) we adapt the concept of discrepancy distance between source and target distributions to restrict the maximization over the hypothesis class to a localized class of functions which are performing accurate labeling on the source domain. We derive generalization error bounds for such active learning strategies in terms of Rademacher average and localized discrepancy for general loss functions which satisfy a regularity condition. A practical Kmedoids algorithm that can address the case of large data set is inferred from the theoretical bounds. Our numerical experiments show that the proposed algorithm is competitive against other state-of-the-art active learning techniques in the context of domain adaptation, in particular on large data sets of around one hundred thousand images",
    "volume": "main",
    "checked": true,
    "id": "6d49abd4a9d563e6cf86ae5e2a808ecd4d67c3fa",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=ZTsoE8G3GG": {
    "title": "Learning to Extend Molecular Scaffolds with Structural Motifs",
    "abstract": "Recent advancements in deep learning-based modeling of molecules promise to accelerate in silico drug discovery. A plethora of generative models is available, building molecules either atom-by-atom and bond-by-bond or fragment-by-fragment. However, many drug discovery projects require a ﬁxed scaffold to be present in the generated molecule, and incorporating that constraint has only recently been explored. Here, we propose MoLeR, a graph-based model that naturally supports scaffolds as initial seed of the generative procedure, which is possible because it is not conditioned on the generation history. Our experiments show that MoLeR performs comparably to state-of-the-art methods on unconstrained molecular optimization tasks, and outperforms them on scaffold-based tasks, while being an order of magnitude faster to train and sample from than existing approaches. Furthermore, we show the inﬂuence of a number of seemingly minor design choices on the overall performance. model does not depend on history, it can complete arbitrary scaffolds, while still outperforming state-of-the-art graph-based generative models in unconstrained generation. Our quantitative and qualitative results show that MoLeR retains desirable properties of generative models - such as smooth interpolation - while respecting the scaffold constraint. Finally, we show that it exhibits good performance in unconstrained optimization, while excelling in scaffold-constrained optimization",
    "volume": "main",
    "checked": true,
    "id": "df76e78d56180704250917273f2f3eed92f0b954",
    "citation_count": 14
  },
  "https://openreview.net/forum?id=XOh5x-vxsrV": {
    "title": "Cross-Trajectory Representation Learning for Zero-Shot Generalization in RL",
    "abstract": "A highly desirable property of a reinforcement learning (RL) agent – and a major difficulty for deep RL approaches – is the ability to generalize policies learned on a few tasks over a high-dimensional observation space to similar tasks not seen during training. Many promising approaches to this challenge consider RL as a process of training two functions simultaneously: a complex nonlinear encoder that maps high-dimensional observations to a latent representation space, and a simple linear policy over this space. We posit that a superior encoder for zero-shot generalization in RL can be trained by using solely an auxiliary SSL objective if the training process encourages the encoder to map behaviorally similar observations to similar representations, as reward-based signal can cause overfitting in the encoder [32]. We propose Cross Trajectory Representation Learning (CTRL), a method that runs within an RL agent and conditions its encoder to recognize behavioral similarity in observations by applying a novel SSL objective to pairs of trajectories from the agent's policies. CTRL can be viewed as having the same effect as inducing a pseudo-bisimulation metric but, crucially, avoids the use of rewards and associated overfitting risks. Our experiments ablate various components of CTRL and demonstrate that in combination with PPO it achieves better generalization performance on the challenging Procgen benchmark suite [12]",
    "volume": "main",
    "checked": true,
    "id": "f496a4934ae20d7da7869b4c7c5c47b61fccb299",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=bjy5Zb2fo2": {
    "title": "Scattering Networks on the Sphere for Scalable and Rotationally Equivariant Spherical CNNs",
    "abstract": "Convolutional neural networks (CNNs) constructed natively on the sphere have been developed recently and shown to be highly effective for the analysis of spherical data. While an efficient framework has been formulated, spherical CNNs are nevertheless highly computationally demanding; typically they cannot scale beyond spherical signals of thousands of pixels. We develop scattering networks constructed natively on the sphere that provide a powerful representational space for spherical data. Spherical scattering networks are computationally scalable and exhibit rotational equivariance, while their representational space is invariant to isometries and provides efficient and stable signal representations. By integrating scattering networks as an additional type of layer in the generalized spherical CNN framework, we show how they can be leveraged to scale spherical CNNs to the high resolution data typical of many practical applications, with spherical signals of many tens of megapixels and beyond",
    "volume": "main",
    "checked": true,
    "id": "28193843c562cbb90c719ead9969ce1d394bd933",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=NRX9QZ6yqt": {
    "title": "Memory Augmented Optimizers for Deep Learning",
    "abstract": "Popular approaches for minimizing loss in data-driven learning often involve an abstraction or an explicit retention of the history of gradients for efficient parameter updates. The aggregated history of gradients nudges the parameter updates in the right direction even when the gradients at any given step are not informative. Although the history of gradients summarized in meta-parameters or explicitly stored in memory has been shown effective in theory and practice, the question of whether all or only a subset of the gradients in the history are sufficient in deciding the parameter updates remains unanswered. In this paper, we propose a framework of memory-augmented gradient descent optimizers that retain a limited view of their gradient history in their internal memory. Such optimizers scale well to large real-life datasets, and our experiments show that the memory augmented extensions of standard optimizers enjoy accelerated convergence and improved performance on a majority of computer vision and language tasks that we considered. Additionally, we prove that the proposed class of optimizers with fixed-size memory converge under assumptions of strong convexity, regardless of which gradients are selected or how they are linearly combined to form the update step",
    "volume": "main",
    "checked": true,
    "id": "c4025c12d802491a56d1801b7ca32a58ea1b14bb",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=C_vsGwEIjAr": {
    "title": "Trivial or Impossible --- dichotomous data difficulty masks model differences (on ImageNet and beyond)",
    "abstract": "\"The power of a generalization system follows directly from its biases\" (Mitchell Today, CNNs are incredibly powerful generalisation systems—but to what degree have we understood how their inductive bias inﬂuences model decisions? We here attempt to disentangle the various aspects that determine how a model decides. In particular, we ask: what makes one model decide differently from another? In a meticulously controlled setting, we ﬁnd that (1.) irrespective of the network architecture or objective (e.g. self-supervised, semi-supervised, vision transformers, recurrent models) all models end up making similar decisions. (2.) To understand these ﬁndings, we analysed model decisions on the ImageNet validation set from epoch to epoch and image by image. We ﬁnd that the ImageNet validation set, among others, suffers from dichotomous data difﬁculty (DDD): For the range of investigated models and their accuracies, it is dominated by 46.0% \"trivial\" and 11.5% \"impossible\" images (beyond label errors). Only 42.5% of the images could possibly be responsible for the differences between two models' decision boundaries. (3.) Only removing the \"impossible\" and \"trivial\" images allows us to see pronounced differences between models. (4.) Humans are highly accurate at predicting which images are \"trivial\" and \"impossible\" for CNNs (81.4%). This implies that in future comparisons of brains, machines and behaviour, much may be gained from investigating the decisive role of images and the distribution of their difﬁculties",
    "volume": "main",
    "checked": true,
    "id": "68aca52dc55878a345420f9c32eaaec77794481b",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=5i2f-aR6B8H": {
    "title": "Privacy Implications of Shuffling",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "5eb4891433333237b06fbc60d4c2f3b28e73be61",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=-w2oomO6qgc": {
    "title": "GeneDisco: A Benchmark for Experimental Design in Drug Discovery",
    "abstract": "In vitro cellular experimentation with genetic interventions, using for example CRISPR technologies, is an essential step in early-stage drug discovery and target validation that serves to assess initial hypotheses about causal associations between biological mechanisms and disease pathologies. With billions of potential hypotheses to test, the experimental design space for in vitro genetic experiments is extremely vast, and the available experimental capacity even at the largest research institutions in the world pales in relation to the size of this biological hypothesis space. Machine learning methods, such as active and reinforcement learning, could aid in optimally exploring the vast biological space by integrating prior knowledge from various information sources as well as extrapolating to yet unexplored areas of the experimental design space based on available data. However, there exist no standardised benchmarks and data sets for this challenging task and little research has been conducted in this area to date. Here, we introduce GeneDisco, a benchmark suite for evaluating active learning algorithms for experimental design in drug discovery. GeneDisco contains a curated set of multiple publicly available experimental data sets as well as open-source implementations of state-of-the-art active learning policies for experimental design and exploration",
    "volume": "main",
    "checked": true,
    "id": "9afd22d6a636082dc977d05e0772b3c5749451af",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=0no8Motr-zO": {
    "title": "An Experimental Design Perspective on Model-Based Reinforcement Learning",
    "abstract": "In many practical applications of RL, it is expensive to observe state transitions from the environment. For example, in the problem of plasma control for nuclear fusion, computing the next state for a given state-action pair requires querying an expensive transition function which can lead to many hours of computer simulation or dollars of scientific research. Such expensive data collection prohibits application of standard RL algorithms which usually require a large number of observations to learn. In this work, we address the problem of efficiently learning a policy while making a minimal number of state-action queries to the transition function. In particular, we leverage ideas from Bayesian optimal experimental design to guide the selection of state-action queries for efficient learning. We propose an acquisition function that quantifies how much information a state-action pair would provide about the optimal solution to a Markov decision process. At each iteration, our algorithm maximizes this acquisition function, to choose the most informative state-action pair to be queried, thus yielding a data-efficient RL approach. We experiment with a variety of simulated continuous control problems and show that our approach learns an optimal policy with up to 5 – 1, 000× less data than modelbased RL baselines and 10 – 105× less data than model-free RL baselines. We also provide several ablated comparisons which point to substantial improvements arising from the principled method of obtaining data",
    "volume": "main",
    "checked": true,
    "id": "2bd2080efe63afd1fd0c17e04b80b79166ec5aa7",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=vh-0sUt8HlG": {
    "title": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer",
    "abstract": "Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision transformers (ViTs) have been adopted. Unlike CNNs, ViTs are heavyweight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers. Our results show that MobileViT significantly outperforms CNNand ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: https://github.com/apple/ml-cvnets",
    "volume": "main",
    "checked": true,
    "id": "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e",
    "citation_count": 104
  },
  "https://openreview.net/forum?id=0DLwqQLmqV": {
    "title": "NAS-Bench-Suite: NAS Evaluation is (Now) Surprisingly Easy",
    "abstract": "The release of tabular benchmarks, such as NAS-Bench-101 and NAS-Bench-201, has significantly lowered the computational overhead for conducting scientific research in neural architecture search (NAS). Although they have been widely adopted and used to tune real-world NAS algorithms, these benchmarks are limited to small search spaces and focus solely on image classification. Recently, several new NAS benchmarks have been introduced that cover significantly larger search spaces over a wide range of tasks, including object detection, speech recognition, and natural language processing. However, substantial differences among these NAS benchmarks have so far prevented their widespread adoption, limiting researchers to using just a few benchmarks. In this work, we present an in-depth analysis of popular NAS algorithms and performance prediction methods across 25 different combinations of search spaces and datasets, finding that many conclusions drawn from a few NAS benchmarks do not generalize to other benchmarks. To help remedy this problem, we introduce NAS-Bench-Suite, a comprehensive and extensible collection of NAS benchmarks, accessible through a unified interface, created with the aim to facilitate reproducible, generalizable, and rapid NAS research. Our code is available at https://github.com/automl/naslib",
    "volume": "main",
    "checked": true,
    "id": "4857d4de584ed6f15e7ef12a96823ec8b4a17ec1",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=hcoswsDHNAW": {
    "title": "Fast AdvProp",
    "abstract": "Adversarial Propagation (AdvProp) is an effective way to improve recognition models, leveraging adversarial examples. Nonetheless, AdvProp suffers from the extremely slow training speed, mainly because: a) extra forward and backward passes are required for generating adversarial examples; b) both original samples and their adversarial counterparts are used for training (i.e., 2× data). In this paper, we introduce Fast AdvProp, which aggressively revamps AdvProp's costly training components, rendering the method nearly as cheap as the vanilla training. Specifically, our modifications in Fast AdvProp are guided by the hypothesis that disentangled learning with adversarial examples is the key for performance improvements, while other training recipes (e.g., paired clean and adversarial training samples, multi-step adversarial attackers) could be largely simplified. Our empirical results show that, compared to the vanilla training baseline, Fast AdvProp is able to further model performance on a spectrum of visual benchmarks, without incurring extra training cost. Additionally, our ablations find Fast AdvProp scales better if larger models are used, is compatible with existing data augmentation methods (i.e., Mixup and CutMix), and can be easily adapted to other recognition tasks like object detection. The code is available here: https://github.com/meijieru/fast_advprop",
    "volume": "main",
    "checked": true,
    "id": "bbc8df27808cf1831087a080be5c76e29657c0b2",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=htWIlvDcY8": {
    "title": "FALCON: Fast Visual Concept Learning by Integrating Images, Linguistic descriptions, and Conceptual Relations",
    "abstract": "We present a meta-learning framework for learning new visual concepts quickly, from just one or a few examples, guided by multiple naturally occurring data streams: simultaneously looking at images, reading sentences that describe the objects in the scene, and interpreting supplemental sentences that relate the novel concept with other concepts. The learned concepts support downstream applications, such as answering questions by reasoning about unseen images. Our model, namely FALCON, represents individual visual concepts, such as colors and shapes, as axis-aligned boxes in a high-dimensional space (the \"box embedding space\"). Given an input image and its paired sentence, our model first resolves the referential expression in the sentence and associates the novel concept with particular objects in the scene. Next, our model interprets supplemental sentences to relate the novel concept with other known concepts, such as \"X has property Y\" or \"X is a kind of Y\". Finally, it infers an optimal box embedding for the novel concept that jointly 1) maximizes the likelihood of the observed instances in the image, and 2) satisfies the relationships between the novel concepts and the known ones. We demonstrate the effectiveness of our model on both synthetic and real-world datasets",
    "volume": "main",
    "checked": true,
    "id": "ae9f2d6a29daec0eaf5700d327df0f4f30558b74",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=Rty5g9imm7H": {
    "title": "Transformer Embeddings of Irregularly Spaced Events and Their Participants",
    "abstract": "The neural Hawkes process (Mei & Eisner, 2017) is a generative model of irregularly spaced sequences of discrete events. To handle complex domains with many event types, Mei et al. further consider a setting in which each event in the sequence updates a deductive database of facts (via domain-speciﬁc pattern-matching rules); future events are then conditioned on the database contents. They show how to convert such a symbolic system into a neuro-symbolic continuous-time generative model, in which each database fact and possible event has a time-varying embedding that is derived from its symbolic provenance",
    "volume": "main",
    "checked": true,
    "id": "1393715f85730cdd5b948a7819af226a2a465c26",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=JfaWawZ8BmX": {
    "title": "Anisotropic Random Feature Regression in High Dimensions",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "36bebabf39a981cabf3c38de3c3c3fc6b345bb8f",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=Ug-bgjgSlKV": {
    "title": "Finding an Unsupervised Image Segmenter in each of your Deep Generative Models",
    "abstract": "Recent research has shown that numerous humaninterpretable directions exist in the latent space of GANs. In this paper, we develop an automatic procedure for finding directions that lead to foreground-background image separation, and we use these directions to train an image segmentation model without human supervision. Our method is generator-agnostic, producing strong segmentation results with a wide range of different GAN architectures. Furthermore, by leveraging GANs pretrained on large datasets such as ImageNet, we are able to segment images from a range of domains without further training or finetuning. Evaluating our method on image segmentation benchmarks, we compare favorably to prior work while using neither human supervision nor access to the training data. Broadly, our results demonstrate that automatically extracting foregroundbackground structure from pretrained deep generative models can serve as a remarkably effective substitute for human supervision",
    "volume": "main",
    "checked": true,
    "id": "6470d56e2b96542d191067a258261f92aa8ed82c",
    "citation_count": 17
  },
  "https://openreview.net/forum?id=5XmLzdslFNN": {
    "title": "Modular Lifelong Reinforcement Learning via Neural Composition",
    "abstract": "Humans commonly solve complex problems by decomposing them into easier subproblems and then combining the subproblem solutions. This type of compositional reasoning permits reuse of the subproblem solutions when tackling future tasks that share part of the underlying compositional structure. In a continual or lifelong reinforcement learning (RL) setting, this ability to decompose knowledge into reusable components would enable agents to quickly learn new RL tasks by leveraging accumulated compositional structures. We explore a particular form of composition based on neural modules and present a set of RL problems that intuitively admit compositional solutions. Empirically, we demonstrate that neural composition indeed captures the underlying structure of this space of problems. We further propose a compositional lifelong RL method that leverages accumulated neural components to accelerate the learning of future tasks while retaining performance on previous tasks via off-line RL over replayed experiences",
    "volume": "main",
    "checked": true,
    "id": "0b3ca2a700085a877c560e20558566536f18d5a2",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=7l1IjZVddDW": {
    "title": "Improving Federated Learning Face Recognition via Privacy-Agnostic Clusters",
    "abstract": "The growing public concerns on data privacy in face recognition can be greatly addressed by the federated learning (FL) paradigm. However, conventional FL methods perform poorly due to the uniqueness of the task: broadcasting class centers among clients is crucial for recognition performances but leads to privacy leakage. To resolve the privacy-utility paradox, this work proposes PrivacyFace, a framework largely improves the federated learning face recognition via communicating auxiliary and privacy-agnostic information among clients. PrivacyFace mainly consists of two components: First, a practical Differentially Private Local Clustering (DPLC) mechanism is proposed to distill sanitized clusters from local class centers. Second, a consensus-aware recognition loss subsequently encourages global consensuses among clients, which ergo results in more discriminative features. The proposed framework is mathematically proved to be differentially private, introducing a lightweight overhead as well as yielding prominent performance boosts (e.g., +9.63% and +10.26% for TAR@FAR=1e-4 on IJB-B and IJB-C respectively). Extensive experiments and ablation studies on a large-scale dataset have demonstrated the efficacy and practicability of our method",
    "volume": "main",
    "checked": true,
    "id": "eaccb8a31f7d7b7b442747bf2ffe1d0b4c6570b3",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=aBsCjcPu_tE": {
    "title": "SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations",
    "abstract": "Guided image synthesis enables everyday users to create and edit photo-realistic images with minimum effort. The key challenge is balancing faithfulness to the user inputs (e.g., hand-drawn colored strokes) and realism of the synthesized images. Existing GAN-based methods attempt to achieve such balance using either conditional GANs or GAN inversions, which are challenging and often require additional training data or loss functions for individual applications. To address these issues, we introduce a new image synthesis and editing method, Stochastic Differential Editing (SDEdit), based on a diffusion model generative prior, which synthesizes realistic images by iteratively denoising through a stochastic differential equation (SDE). Given an input image with user guide in a form of manipulating RGB pixels, SDEdit first adds noise to the input, then subsequently denoises the resulting image through the SDE prior to increase its realism. SDEdit does not require task-specific training or inversions and can naturally achieve the balance between realism and faithfulness. SDEdit outperforms state-of-the-art GAN-based methods by up to 98.09% on realism and 91.72% on overall satisfaction scores, according to a human perception study, on multiple tasks, including stroke-based image synthesis and editing as well as image compositing. Stroke Painting to Image Input (guide) Output Image Compositing Source Output Stroke-based Editing Source Input (guide) Output Source Output Input (guide) Input (guide) Figure 1: Stochastic Differential Editing (SDEdit) is a unified image synthesis and editing framework based on stochastic differential equations. SDEdit allows stroke painting to image, image compositing, and stroke-based editing without task-specific model training and loss functions",
    "volume": "main",
    "checked": true,
    "id": "f671a09e3e5922e6d38cb77dda8d76d5ceac2a27",
    "citation_count": 40
  },
  "https://openreview.net/forum?id=BS49l-B5Bql": {
    "title": "GNN-LM: Language Modeling based on Global Contexts via GNN",
    "abstract": "Inspired by the notion that \" to copy is easier than to memorize \", in this work, we introduce GNN-LM, which extends vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM. 1 between tokens. Graph neural networks are then leveraged to aggregate information from the retrieved contexts to decode the next token. Experimental results show that our proposed method outperforms strong baselines in standard benchmark datasets, and by combining with k NN LM, we are able to achieve state-of-the-art results on WikiText-103. In future work, we will consider improving efﬁciency for building the graph and retrieving nearest neighbors",
    "volume": "main",
    "checked": true,
    "id": "7d1e859fefee1eaac430c38d01cd35003604288b",
    "citation_count": 14
  },
  "https://openreview.net/forum?id=gNp54NxHUPJ": {
    "title": "Fast Regression for Structured Inputs",
    "abstract": "We study the `p regression problem, which requires finding x ∈ R that minimizes ‖Ax−b‖p for a matrix A ∈ Rn×d and response vector b ∈ R. There has been recent interest in developing subsampling methods for this problem that can outperform standard techniques when n is very large. However, all known subsampling approaches have run time that depends exponentially on p, typically, dO(p), which can be prohibitively expensive. We improve on this work by showing that for a large class of common structured matrices, such as combinations of low-rank matrices, sparse matrices, and Vandermonde matrices, there are subsampling based methods for `p regression that depend polynomially on p. For example, we give an algorithm for `p regression on Vandermonde matrices that runs in time O(n log n+(dp) ·polylogn), where ω is the exponent of matrix multiplication. The polynomial dependence on p crucially allows our algorithms to extend naturally to efficient algorithms for `∞ regression, via approximation of `∞ by `O(logn). Of practical interest, we also develop a new subsampling algorithm for `p regression for arbitrary matrices, which is simpler than previous approaches for p ≥ 4",
    "volume": "main",
    "checked": true,
    "id": "4baaa1e4f87c337914576e1147a975332cb337a6",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=CpTuR2ECuW": {
    "title": "LIGS: Learnable Intrinsic-Reward Generation Selection for Multi-Agent Learning",
    "abstract": "Efficient exploration is important for reinforcement learners to achieve high rewards. In multi-agent systems, coordinated exploration and behaviour is critical for agents to jointly achieve optimal outcomes. In this paper, we introduce a new general framework for improving coordination and performance of multi-agent reinforcement learners (MARL). Our framework, named Learnable Intrinsic-Reward Generation Selection algorithm (LIGS) introduces an adaptive learner, Generator that observes the agents and learns to construct intrinsic rewards online that coordinate the agents' joint exploration and joint behaviour. Using a novel combination of MARL and switching controls, LIGS determines the best states to learn to add intrinsic rewards which leads to a highly efficient learning process. LIGS can subdivide complex tasks making them easier to solve and enables systems of MARL agents to quickly solve environments with sparse rewards. LIGS can seamlessly adopt existing MARL algorithms and, our theory shows that it ensures convergence to policies that deliver higher system performance. We demonstrate its superior performance in challenging tasks in Foraging and StarCraft II",
    "volume": "main",
    "checked": true,
    "id": "f23a418ddb2f40fb6bd2abd79ffd8c198cb63a79",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=CJzi3dRlJE-": {
    "title": "Connectome-constrained Latent Variable Model of Whole-Brain Neural Activity",
    "abstract": "The availability of both anatomical connectivity and brain-wide neural activity measurements in C. elegans make the worm a promising system for learning detailed, mechanistic models of an entire nervous system in a data-driven way. However, one faces several challenges when constructing such a model. We often do not have direct experimental access to important modeling details such as single-neuron dynamics and the signs and strengths of the synaptic connectivity. Further, neural activity can only be measured in a subset of neurons, often indirectly via calcium imaging, and signiﬁcant trial-to-trial variability has been observed. To address these challenges, we introduce a connectome-constrained latent variable model (CC-LVM) of the unobserved voltage dynamics of the entire C. elegans nervous system and the observed calcium signals. We used the framework of variational autoencoders to ﬁt parameters of the mechanistic simulation constituting the generative model of the LVM to calcium imaging observations. A variational approximate posterior distribution over latent voltage traces for all neurons is efﬁciently inferred using an inference network, and constrained by a prior distribution given by the biophysical simulation of neural dynamics. We applied this model to an experimental whole-brain dataset, and found that connectomic constraints enable our LVM to predict the activity of neurons whose activity were withheld signiﬁcantly better than models unconstrained by a connectome. We explored models with different degrees of biophysical detail, and found that models with realistic conductance-based synapses provide markedly better some",
    "volume": "main",
    "checked": true,
    "id": "18ab60e2cd524d4d8eca056dad5a279a9697246c",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=O1DEtITim__": {
    "title": "Learning Pruning-Friendly Networks via Frank-Wolfe: One-Shot, Any-Sparsity, And No Retraining",
    "abstract": "We present a novel framework to train a large deep neural network (DNN) for only once , which can then be pruned to any sparsity ratio to preserve competitive accuracy without any re-training . Conventional methods often require (iterative) pruning followed by re-training, which not only incurs large overhead beyond the original DNN training but also can be sensitive to retraining hyperparameters. Our core idea is to re-cast the DNN training as an explicit pruning-aware process: that is formulated with an auxiliary K -sparse polytope constraint, to encourage network weights to lie in a convex hull spanned by K -sparse vectors, potentially resulting in more sparse weight matrices. We then leverage a stochastic Frank-Wolfe (SFW) algorithm to solve this new constrained optimization, which naturally leads to sparse weight updates each time. We further note an overlooked fact that ex-isting DNN initializations were derived to enhance SGD training (e.g., avoid gradient explosion or collapse), but was unaligned with the challenges of training with SFW. We hence also present the ﬁrst learning-based initialization scheme speciﬁcally for boosting SFW-based DNN training. Experiments on CIFAR-10 and Tiny-ImageNet datasets demonstrate that our new framework named SFW-pruning consistently achieves the state-of-the-art performance on various benchmark DNNs over a wide range of pruning update weights. Our proposed initialization scheme is organically designed for SFW algorithm and is formulated as a meta learning problem, drawing mo-tivations from Zhu et al. (2021). It learns the layer-wise initialization scaling factors that lead to the largest loss reduction in the ﬁrst SFW training step. We demonstrate that with the new initialization scheme, our proposed one-shot pruning algorithm can consistently achieve better test performance under different pruning ratios without retraining. Now we summarize our main contributions",
    "volume": "main",
    "checked": true,
    "id": "55c235224f2c15f076184c3c65337d36aafa460f",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=nzvbBD_3J-g": {
    "title": "On Incorporating Inductive Biases into VAEs",
    "abstract": "We explain why directly changing the prior can be a surprisingly ineffective mechanism for incorporating inductive biases into variational auto-encoders (VAEs), and introduce a simple and effective alternative approach: Intermediary Latent Space VAEs (InteL-VAEs). InteL-VAEs use an intermediary set of latent variables to control the stochasticity of the encoding process, before mapping these in turn to the latent representation using a parametric function that encapsulates our desired inductive bias(es). This allows us to impose properties like sparsity or clustering on learned representations, and incorporate human knowledge into the generative model. Whereas changing the prior only indirectly encourages behavior through regularizing the encoder, InteL-VAEs are able to directly enforce desired characteristics. Moreover, they bypass the computation and encoder design issues caused by non-Gaussian priors, while allowing for additional flexibility through training of the parametric mapping function. We show that these advantages, in turn, lead to both better generative models and better representations being learned",
    "volume": "main",
    "checked": true,
    "id": "fc26d1bff24719f82ca268b096760c4f05564b94",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=metRpM4Zrcb": {
    "title": "Continual Learning with Filter Atom Swapping",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "453118d8a9284a104a111666c6a1f7c331432092",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=a34GrNaYEcS": {
    "title": "Distributionally Robust Models with Parametric Likelihood Ratios",
    "abstract": "we three simple ideas – mini-batch level normalization, a KL penalty and simultaneous gradient updates – allow us to train models with DRO using a broader class of parametric likelihood ratios. In a series of experiments on both image and classiﬁcation benchmarks, we the are consistently more to subpopulation shifts when to other and the performs with",
    "volume": "main",
    "checked": true,
    "id": "51836dfa1542277ed982612caa90ecf31ead4ba8",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=aOX3a9q3RVV": {
    "title": "Divisive Feature Normalization Improves Image Recognition Performance in AlexNet",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "04e96e36267501a8a4ec1038d129445f8116241a",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=qI4542Y2s1D": {
    "title": "FILM: Following Instructions in Language with Modular Methods",
    "abstract": "Recent methods for embodied instruction following are typically trained end-toend using imitation learning. This often requires the use of expert trajectories and low-level language instructions. Such approaches assume that neural states will integrate multimodal semantics to perform state tracking, building spatial memory, exploration, and long-term planning. In contrast, we propose a modular method with structured representations that (1) builds a semantic map of the scene and (2) performs exploration with a semantic search policy, to achieve the natural language goal. Our modular method achieves SOTA performance (24.46%) with a substantial (8.17 % absolute) gap from previous work while using less data by eschewing both expert trajectories and low-level instructions. Leveraging low-level language, however, can further increase our performance (26.49%).1 Our findings suggest that an explicit spatial memory and a semantic search policy can provide a stronger and more general representation for state-tracking and guidance, even in the absence of expert trajectories or low-level instructions.2",
    "volume": "main",
    "checked": true,
    "id": "bef63d4f7656393b7bceb2ec704e86577c286166",
    "citation_count": 28
  },
  "https://openreview.net/forum?id=pFyXqxChZc": {
    "title": "IntSGD: Adaptive Floatless Compression of Stochastic Gradients",
    "abstract": "We propose a family of adaptive integer compression operators for distributed Stochastic Gradient Descent (SGD) that do not communicate a single float. This is achieved by multiplying floating-point vectors with a number known to every device and then rounding to integers. In contrast to the prior work on integer compression for SwitchML by Sapio et al. (2021), our IntSGD method is provably convergent and computationally cheaper as it estimates the scaling of vectors adaptively. Our theory shows that the iteration complexity of IntSGD matches that of SGD up to constant factors for both convex and non-convex, smooth and nonsmooth functions, with and without overparameterization. Moreover, our algorithm can also be tailored for the popular all-reduce primitive and shows promising empirical performance",
    "volume": "main",
    "checked": true,
    "id": "1836c0c53bc8c77aa56d45834c3a3ff910089bc2",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=oiZJwC_fyS": {
    "title": "Neural Network Approximation based on Hausdorff distance of Tropical Zonotopes",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "d42ebfcac832ccf54fd40ddc8a84050a6a0e609f",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=0DcZxeWfOPt": {
    "title": "Fast Model Editing at Scale",
    "abstract": "While large pre-trained models have enabled impressive results on a variety of downstream tasks, the largest existing models still make errors, and even accurate predictions may become outdated over time. Because detecting all such failures at training time is impossible, enabling both developers and end users of such models to correct inaccurate outputs while leaving the model otherwise intact is desirable. However, the distributed, black-box nature of the representations learned by large neural networks makes producing such targeted edits difficult. If presented with only a single problematic input and new desired output, fine-tuning approaches tend to overfit; other editing algorithms are either computationally infeasible or simply ineffective when applied to very large models. To enable easy post-hoc editing at scale, we propose Model Editor Networks with Gradient Decomposition (MEND), a collection of small auxiliary editing networks that use a single desired input-output pair to make fast, local edits to a pre-trained model's behavior. MEND learns to transform the gradient obtained by standard fine-tuning, using a low-rank decomposition of the gradient to make the parameterization of this transformation tractable. MEND can be trained on a single GPU in less than a day even for 10 billion+ parameter models; once trained MEND enables rapid application of new edits to the pre-trained model. Our experiments with T5, GPT, BERT, and BART models show that MEND is the only approach to model editing that effectively edits the behavior of models with more than 10 billion parameters. Code and data available at https://sites.google.com/view/mend-editing",
    "volume": "main",
    "checked": true,
    "id": "76beeb2ece0abd7fc586d4006435e696d02c6757",
    "citation_count": 35
  },
  "https://openreview.net/forum?id=IwJPj2MBcIa": {
    "title": "Compositional Attention: Disentangling Search and Retrieval",
    "abstract": "Multi-head, key-value attention is the backbone of the widely successful Transformer model and its variants. This attention mechanism uses multiple parallel key-value attention blocks (called heads), each performing two fundamental computations: (1) search – selection of a relevant entity from a set via query-key interactions, and (2) retrieval – extraction of relevant features from the selected entity via a value matrix. Importantly, standard attention heads learn a rigid mapping between search and retrieval. In this work, we first highlight how this static nature of the pairing can potentially: (a) lead to learning of redundant parameters in certain tasks, and (b) hinder generalization. To alleviate this problem, we propose a novel attention mechanism, called Compositional Attention, that replaces the standard head structure. The proposed mechanism disentangles search and retrieval and composes them in a dynamic, flexible and context-dependent manner through an additional soft competition stage between the query-key combination and value pairing. Through a series of numerical experiments, we show that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings. Through our qualitative analysis, we demonstrate that Compositional Attention leads to dynamic specialization based on the type of retrieval needed. Our proposed mechanism generalizes multi-head attention, allows independent scaling of search and retrieval, and can easily be implemented in lieu of standard attention heads in any network architecture.1",
    "volume": "main",
    "checked": true,
    "id": "b8b813111c411ae61881ab9cd25707d9de6444ec",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=qnQN4yr6FJz": {
    "title": "Variational Inference for Discriminative Learning with Generative Modeling of Feature Incompletion",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "82df2081fadcc832d24d01dfcec8e96912702bfc",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=bYGSzbCM_i": {
    "title": "Online Adversarial Attacks",
    "abstract": "Adversarial expose important of deep learning we formalize the online adversarial attack problem, emphasizing two key elements found in real-world use-cases: attackers must operate under partial knowledge of the target model, and the decisions made by the attacker are irrevocable since they operate on a transient data stream. We ﬁrst rigorously analyze a deterministic variant of the online threat model by drawing parallels to the well-studied k secretary problem in theoretical computer science and propose V IRTUAL +, a simple yet practical online algorithm. Our main theoretical result shows V IRTUAL + yields provably the best competitive ratio over all single-threshold algorithms for k < 5 —extending the previous analysis of the k -secretary problem. We also introduce the stochastic k -secretary —effectively reducing online blackbox transfer attacks to a k -secretary problem under noise—and prove theoretical bounds on the performance of V IRTUAL + adapted to this setting. Finally, we complement our theoretical results by conducting experiments on MNIST, CIFAR-10, and Imagenet classiﬁers, revealing the necessity of online algorithms in achieving near-optimal performance and also the rich interplay between attack strategies and online attack selection, enabling simple strategies like FGSM to outperform stronger adversaries",
    "volume": "main",
    "checked": true,
    "id": "c02ade047261fb3a199219ae1274d70529d0a26c",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=bsycpMi00R1": {
    "title": "Generalized Natural Gradient Flows in Hidden Convex-Concave Games and GANs",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "4fe997e21a68b433f217007a12f9bd797cf2f1e2",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=N1WI0vJLER": {
    "title": "Parallel Training of GRU Networks with a Multi-Grid Solver for Long Sequences",
    "abstract": "Parallelizing Gated Recurrent Unit (GRU) networks is a challenging task, as the training procedure of GRU is inherently sequential. Prior efforts to parallelize GRU have largely focused on conventional parallelization strategies such as dataparallel and model-parallel training algorithms. However, when the given sequences are very long, existing approaches are still inevitably performance limited in terms of training time. In this paper, we present a novel parallel training scheme (called parallel-in-time) for GRU based on a multigrid reduction in time (MGRIT) solver. MGRIT partitions a sequence into multiple shorter sub-sequences and trains the sub-sequences on different processors in parallel. The key to achieving speedup is a hierarchical correction of the hidden state to accelerate end-to-end communication in both the forward and backward propagation phases of gradient descent. Experimental results on the HMDB51 dataset, where each video is an image sequence, demonstrate that the new parallel training scheme achieves up to 6.5× speedup over a serial approach. As efficiency of our new parallelization strategy is associated with the sequence length, our parallel GRU algorithm achieves significant performance improvement as the sequence length increases",
    "volume": "main",
    "checked": true,
    "id": "0c7fa4d8935a05afadebe0a2daf21ef1d91daf49",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=CS4463zx6Hi": {
    "title": "Geometric Transformers for Protein Interface Contact Prediction",
    "abstract": "Computational methods for predicting the interface contacts between proteins come highly sought after for drug discovery as they can significantly advance the accuracy of alternative approaches, such as protein-protein docking, protein function analysis tools, and other computational methods for protein bioinformatics. In this work, we present the Geometric Transformer, a novel geometryevolving graph transformer for rotation and translation-invariant protein interface contact prediction, packaged within DeepInteract, an end-to-end prediction pipeline. DeepInteract predicts partner-specific protein interface contacts (i.e., inter-protein residue-residue contacts) given the 3D tertiary structures of two proteins as input. In rigorous benchmarks, DeepInteract, on challenging protein complex targets from the new Enhanced Database of Interacting Protein Structures (DIPS-Plus) and the 13th and 14th CASP-CAPRI experiments, achieves 17% and 13% top L/5 precision (L: length of a protein unit in a complex), respectively. In doing so, DeepInteract, with the Geometric Transformer as its graph-based backbone, outperforms existing methods for interface contact prediction in addition to other graph-based neural network backbones compatible with DeepInteract, thereby validating the effectiveness of the Geometric Transformer for learning rich relational-geometric features for downstream tasks on 3D protein structures.1",
    "volume": "main",
    "checked": true,
    "id": "0ab0e3b97c2f24565af38bbfba0ba60b5a11d8b3",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=FZoZ7a31GCW": {
    "title": "Ancestral protein sequence reconstruction using a tree-structured Ornstein-Uhlenbeck variational autoencoder",
    "abstract": "We introduce a deep generative model for representation learning of biological sequences that, unlike existing models, explicitly represents the evolutionary process. The model makes use of a tree-structured Ornstein-Uhlenbeck process, obtained from a given phylogenetic tree, as an informative prior for a variational autoencoder. We show the model performs well on the task of ancestral sequence reconstruction of single protein families. Our results and ablation studies indicate that the explicit representation of evolution using a suitable tree-structured prior has the potential to improve representation learning of biological sequences considerably. Finally, we briefly discuss extensions of the model to genomicscale data sets and the case of a latent phylogenetic tree",
    "volume": "main",
    "checked": true,
    "id": "8461c8b7dbbf71c0f3aea3e6a5852bcc21620401",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=6NePxZwfae": {
    "title": "Goal-Directed Planning via Hindsight Experience Replay",
    "abstract": "We consider the problem of goal-directed planning under a deterministic transition model. Monte Carlo Tree Search has shown remarkable performance in solving deterministic control problems. By using function approximators to bias the search of the tree, MCTS has been extended to complex continuous domains, resulting in the AlphaZero family of algorithms. Nonetheless, these algorithms still struggle with control problems with sparse rewards such as goal-directed domains, where a positive reward is awarded only when reaching a goal state. In this work, we extend AlphaZero with Hindsight Experience Replay to tackle complex goal-directed planning tasks. We demonstrate the effectiveness of the proposed approach through an extensive empirical evaluation in several simulated domains, including a novel application to a quantum compiling domain",
    "volume": "main",
    "checked": true,
    "id": "f2c954712914d75c752cb0950ef32043793d58b9",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=v3aeIsY_vVX": {
    "title": "Chunked Autoregressive GAN for Conditional Waveform Synthesis",
    "abstract": "Conditional waveform synthesis models learn a distribution of audio waveforms given conditioning such as text, mel-spectrograms, or MIDI. These systems employ deep generative models that model the waveform via either sequential (autoregressive) or parallel (non-autoregressive) sampling. Generative adversarial networks (GANs) have become a common choice for non-autoregressive waveform synthesis. However, state-of-the-art GAN-based models produce artifacts when performing mel-spectrogram inversion. In this paper, we demonstrate that these artifacts correspond with an inability for the generator to learn accurate pitch and periodicity. We show that simple pitch and periodicity conditioning is insufficient for reducing this error relative to using autoregression. We discuss the inductive bias that autoregression provides for learning the relationship between instantaneous frequency and phase, and show that this inductive bias holds even when autoregressively sampling large chunks of the waveform during each forward pass. Relative to prior state-of-the-art GAN-based models, our proposed model, Chunked Autoregressive GAN (CARGAN) reduces pitch error by 40-60%, reduces training time by 58%, maintains a fast generation speed suitable for realtime or interactive applications, and maintains or improves subjective quality",
    "volume": "main",
    "checked": true,
    "id": "01fbbc3a5c317478aed073bd90ef7bd3693e7bbb",
    "citation_count": 12
  },
  "https://openreview.net/forum?id=JBAZe2yN6Ub": {
    "title": "A First-Occupancy Representation for Reinforcement Learning",
    "abstract": "Both animals and artificial agents benefit from state representations that support rapid transfer of learning across tasks and which enable them to efficiently traverse their environments to reach rewarding states. The successor representation (SR), which measures the expected cumulative, discounted state occupancy under a fixed policy, enables efficient transfer to different reward structures in an otherwise constant Markovian environment and has been hypothesized to underlie aspects of biological behavior and neural activity. However, in the real world, rewards may move or only be available for consumption once, may shift location, or agents may simply aim to reach goal states as rapidly as possible without the constraint of artificially imposed task horizons. In such cases, the most behaviorally-relevant representation would carry information about when the agent was likely to first reach states of interest, rather than how often it should expect to visit them over a potentially infinite time span. To reflect such demands, we introduce the first-occupancy representation (FR), which measures the expected temporal discount to the first time a state is accessed. We demonstrate that the FR facilitates the selection of efficient paths to desired states, allows the agent, under certain conditions, to plan provably optimal trajectories defined by a sequence of subgoals, and induces similar behavior to animals avoiding threatening stimuli",
    "volume": "main",
    "checked": true,
    "id": "5d0c5b55db27d44e80406a825fb86ae3e1325a8b",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=avgclFZ221l": {
    "title": "Asymmetry Learning for Counterfactually-invariant Classification in OOD Tasks",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "61d0b903c290047ae67f30092e1dd95b39210a13",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=UtGtoS4CYU": {
    "title": "Measuring CLEVRness: Black-box Testing of Visual Reasoning Models",
    "abstract": "How can we measure the reasoning capabilities of intelligence systems? Visual question answering provides a convenient framework for testing the model's abilities by interrogating the model through questions about the scene. However, despite scores of various visual QA datasets and architectures, which sometimes yield even a super-human performance, the question of whether those architectures can actually reason remains open to debate. To answer this, we extend the visual question answering framework and propose the following behavioral test in the form of a two-player game. We consider black-box neural models of CLEVR. These models are trained on a diagnostic dataset benchmarking reasoning. Next, we train an adversarial player that re-configures the scene to fool the CLEVR model. We show that CLEVR models, which otherwise could perform at a \"human level\", can easily be fooled by our agent. Our results put in doubt whether data-driven approaches can do reasoning without exploiting the numerous biases that are often present in those datasets. Finally, we also propose a controlled experiment measuring the efficiency of such models to learn and perform reasoning",
    "volume": "main",
    "checked": true,
    "id": "6847e2ba6796b8a786b3b6d8d8a2d922a6c7c31d",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=A05I5IvrdL-": {
    "title": "The Geometry of Memoryless Stochastic Policy Optimization in Infinite-Horizon POMDPs",
    "abstract": "We consider the problem of finding the best memoryless stochastic policy for an infinite-horizon partially observable Markov decision process (POMDP) with finite state and action spaces with respect to either the discounted or mean reward criterion. We show that the (discounted) state-action frequencies and the expected cumulative reward are rational functions of the policy, whereby the degree is determined by the degree of partial observability. We then describe the optimization problem as a linear optimization problem in the space of feasible state-action frequencies subject to polynomial constraints that we characterize explicitly. This allows us to address the combinatorial and geometric complexity of the optimization problem using recent tools from polynomial optimization. In particular, we estimate the number of critical points and use the polynomial programming description of reward maximization to solve a navigation problem in a grid world. This is work with Johannes Müller",
    "volume": "main",
    "checked": true,
    "id": "992fac6a20eb8ef5e921213d8518a82fe76d36fa",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=rHMaBYbkkRJ": {
    "title": "CLEVA-Compass: A Continual Learning Evaluation Assessment Compass to Promote Research Transparency and Comparability",
    "abstract": "What is the state of the art in continual machine learning? Although a natural question for predominant static benchmarks, the notion to train systems in a lifelong manner entails a plethora of additional challenges with respect to set-up and evaluation. The latter have recently sparked a growing amount of critiques on prominent algorithm-centric perspectives and evaluation protocols being too narrow, resulting in several attempts at constructing guidelines in favor of specific desiderata or arguing against the validity of prevalent assumptions. In this work, we depart from this mindset and argue that the goal of a precise formulation of desiderata is an ill-posed one, as diverse applications may always warrant distinct scenarios. Instead, we introduce the Continual Learning EValuation Assessment Compass: the CLEVA-Compass. The compass provides the visual means to both identify how approaches are practically reported and how works can simultaneously be contextualized in the broader literature landscape. In addition to promoting compact specification in the spirit of recent replication trends, it thus provides an intuitive chart to understand the priorities of individual systems, where they resemble each other, and what elements are missing towards a fair comparison",
    "volume": "main",
    "checked": true,
    "id": "0f51b10baca455068919915976b6b0e1ab62f2c3",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=SIKV0_MrZlr": {
    "title": "Auto-Transfer: Learning to Route Transferable Representations",
    "abstract": "Knowledge transfer between heterogeneous source and target networks and tasks has received a lot of attention in recent times as large amounts of quality labelled data can be difficult to obtain in many applications. Existing approaches typically constrain the target deep neural network (DNN) feature representations to be close to the source DNNs feature representations, which can be limiting. We, in this paper, propose a novel adversarial multi-armed bandit approach which automatically learns to route source representations to appropriate target representations following which they are combined in meaningful ways to produce accurate target models. We see upwards of 5% accuracy improvements compared with the stateof-the-art knowledge transfer methods on four benchmark (target) image datasets CUB200, Stanford Dogs, MIT67 and Stanford40 where the source dataset is ImageNet. We qualitatively analyze the goodness of our transfer scheme by showing individual examples of the important features focused on by our target network at different layers compared with the (closest) competitors. We also observe that our improvement over other methods is higher for smaller target datasets making it an effective tool for small data applications that may benefit from transfer learning.1",
    "volume": "main",
    "checked": true,
    "id": "15e437c7be0f9eca8a74c92bafeeb9de892d02b4",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=rJvY_5OzoI": {
    "title": "Multi-Critic Actor Learning: Teaching RL Policies to Act with Style",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "753d10c000e0df1c25ba20833d48b0d60d148467",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=YX0lrvdPQc": {
    "title": "A Johnson-Lindenstrauss Framework for Randomly Initialized CNNs",
    "abstract": "How does the geometric representation of a dataset change after the application of each randomly initialized layer of a neural network? The celebrated Johnson– Lindenstrauss lemma answers this question for linear fully-connected neural networks (FNNs), stating that the geometry is essentially preserved. For FNNs with the ReLU activation, the angle between two inputs contracts according to a known mapping. The question for non-linear convolutional neural networks (CNNs) becomes much more intricate. To answer this question, we introduce a geometric framework. For linear CNNs, we show that the Johnson–Lindenstrauss lemma continues to hold, namely, that the angle between two inputs is preserved. For CNNs with ReLU activation, on the other hand, the behavior is richer: The angle between the outputs contracts, where the level of contraction depends on the nature of the inputs. In particular, after one layer, the geometry of natural images is essentially preserved, whereas for Gaussian correlated inputs, CNNs exhibit the same contracting behavior as FNNs with ReLU activation",
    "volume": "main",
    "checked": true,
    "id": "4fdf66503dab74cab51193b6e68f98bfe6461cf0",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=kezNJydWvE": {
    "title": "Clean Images are Hard to Reblur: Exploiting the Ill-Posed Inverse Task for Dynamic Scene Deblurring",
    "abstract": "The goal of dynamic scene deblurring is to remove the motion blur in a given image. Typical learning-based approaches implement their solutions by minimizing the L1 or L2 distance between the output and the reference sharp image. Recent attempts adopt visual recognition features in training to improve the perceptual quality. However, those features are primarily designed to capture high-level con-texts rather than low-level structures such as blurriness. Instead, we propose a more direct way to make images sharper by exploiting the inverse task of deblurring, namely, reblurring. Reblurring ampliﬁes the remaining blur to rebuild the original blur, however, a well-deblurred clean image with zero-magnitude blur is hard to reblur. Thus, we design two types of reblurring loss functions for better deblurring. The supervised reblurring loss at training stage compares the ampliﬁed blur between the deblurred and the sharp images. The self-supervised reblurring loss at inference stage inspects if there noticeable blur remains in the deblurred. Our experimental results on large-scale benchmarks and real images demonstrate the effectiveness of the reblurring losses in improving the perceptual quality of the deblurred images in terms of NIQE and LPIPS scores as well as visual sharpness",
    "volume": "main",
    "checked": true,
    "id": "9a531d3f6eee43b93349bf1f1e7f8c2c3b6def61",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=_F9xpOrqyX9": {
    "title": "Spread Spurious Attribute: Improving Worst-group Accuracy with Spurious Attribute Estimation",
    "abstract": "The paradigm of worst-group loss minimization has shown its promise in avoiding to learn spurious correlations, but requires costly additional supervision on spurious attributes. To resolve this, recent works focus on developing weaker forms of supervision—e.g., hyperparameters discovered with a small number of group-labeled samples with spurious attribute annotation—but none of the methods retain comparable performance to methods using full supervision on the spurious attribute. In this paper, instead of searching for weaker supervisions, we ask: Given access to a ﬁxed number of group-labeled samples, what is the best achievable worst-group loss if we \"fully exploit\" them? To this end, we propose a pseudo-attribute-based algorithm, coined Spread Spurious Attribute (SSA), for improving the worst-group accuracy. In particular, we leverage samples both with and without spurious attribute annotations to train a model predicting the spurious attribute, then use the pseudo-attribute predicted by the trained model as a supervision on the spurious attribute to train a new robust model having minimal worst-group loss. Our experiments on various benchmark datasets show that our algorithm consistently outperforms the baseline methods using the same number of group-labeled samples. We also demonstrate that the proposed SSA can achieve comparable performances to methods using full (100%) spurious attribute supervision, by using a much smaller number of group-labeled samples—from 0.6% and up to 1.5%, depending on the dataset",
    "volume": "main",
    "checked": true,
    "id": "3d112021bb96568c830c03c779f232da3788067c",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=jeLW-Fh9bV": {
    "title": "Skill-based Meta-Reinforcement Learning",
    "abstract": "While deep reinforcement learning methods have shown impressive results in robot learning, their sample inefficiency makes the learning of complex, longhorizon behaviors with real robot systems infeasible. To mitigate this issue, metareinforcement learning methods aim to enable fast learning on novel tasks by learning how to learn. Yet, the application has been limited to short-horizon tasks with dense rewards. To enable learning long-horizon behaviors, recent works have explored leveraging prior experience in the form of offline datasets without reward or task annotations. While these approaches yield improved sample efficiency, millions of interactions with environments are still required to solve complex tasks. In this work, we devise a method that enables meta-learning on long-horizon, sparse-reward tasks, allowing us to solve unseen target tasks with orders of magnitude fewer environment interactions. Our core idea is to leverage prior experience extracted from offline datasets during meta-learning. Specifically, we propose to (1) extract reusable skills and a skill prior from offline datasets, (2) meta-train a high-level policy that learns to efficiently compose learned skills into long-horizon behaviors, and (3) rapidly adapt the meta-trained policy to solve an unseen target task. Experimental results on continuous control tasks in navigation and manipulation demonstrate that the proposed method can efficiently solve longhorizon novel target tasks by combining the strengths of meta-learning and the usage of offline datasets, while prior approaches in RL, meta-RL, and multi-task RL require substantially more environment interactions to solve the tasks",
    "volume": "main",
    "checked": true,
    "id": "23bac2542b145bf2fcd17d7fa0a02ae03d0a45f7",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=ibrUkC-pbis": {
    "title": "Neural Models for Output-Space Invariance in Combinatorial Problems",
    "abstract": "Recently many neural models have been proposed to solve combinatorial puzzles by implicitly learning underlying constraints using their solved instances, such as sudoku or graph coloring (GCP). One drawback of the proposed architectures, which are often based on Graph Neural Networks (GNN) (Zhou et al., 2020), is that they cannot generalize across the size of the output space from which variables are assigned a value, for example, set of colors in a GCP, or board-size in sudoku. We call the output space for the variables as ‘value-set'. While many works have demonstrated generalization of GNNs across graph size, there has been no study on how to design a GNN for achieving value-set invariance for problems that come from the same domain. For example, learning to solve 16× 16 sudoku after being trained on only 9× 9 sudokus, or coloring a 7 colorable graph after training on 4 colorable graphs. In this work, we propose novel methods to extend GNN based architectures to achieve value-set invariance. Specifically, our model builds on recently proposed Recurrent Relational Networks (RRN) (Palm et al., 2018). Our first approach exploits the graph-size invariance of GNNs by converting a multi-class node classification problem into a binary node classification problem. Our second approach works directly with multiple classes by adding multiple nodes corresponding to the values in the value-set, and then connecting variable nodes to value nodes depending on the problem initialization. Our experimental evaluation on three different combinatorial problems demonstrates that both our models perform well on our novel problem, compared to a generic neural reasoner. Between two of our models, we observe an inherent trade-off: while the binarized model gives better performance when trained on smaller value-sets, multi-valued model is much more memory efficient, resulting in improved performance when trained on larger value-sets, where binarized model fails to train",
    "volume": "main",
    "checked": true,
    "id": "6294b2966523a264e99cf90dffaf31e40874c6cb",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=5kq11Tl1z4": {
    "title": "IGLU: Efficient GCN Training via Lazy Updates",
    "abstract": "a novel technique to train GCN architectures on large graphs that outperforms the art techniques in terms accuracy convergence The paper does not explore experiments",
    "volume": "main",
    "checked": true,
    "id": "348a65fd16a8502dc75d9960b27c8261f9bc3c27",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=D6nH3719vZy": {
    "title": "On Improving Adversarial Transferability of Vision Transformers",
    "abstract": "Vision transformers (ViTs) process input images as sequences of patches via self-attention; a radically different architecture than convolutional neural networks (CNNs). This makes it interesting to study the adversarial feature space of ViT models and their transferability. In particular, we observe that adversarial patterns found via conventional adversarial attacks show very low black-box transferability even for large ViT models. We show that this phenomenon is only due to the sub-optimal attack procedures that do not leverage the true representation potential of ViTs. A deep ViT is composed of multiple blocks, with a consistent architecture comprising of self-attention and feed-forward layers, where each block is capable of independently producing a class token. Formulating an attack using only the last class token (conventional approach) does not directly leverage the discriminative information stored in the earlier tokens, leading to poor adversarial transferability of ViTs. Using the compositional nature of ViT models, we enhance transferability of existing attacks by introducing two novel strategies speciﬁc to the architecture of ViT models. (i) Self-Ensemble: We propose a method to ﬁnd multiple discriminative pathways by dissecting a single ViT model into an ensemble of networks. This allows explicitly utilizing class-speciﬁc information at each ViT block. (ii) Token Reﬁnement: We then propose to reﬁne the tokens to further enhance the discriminative capacity at each block of ViT. Our token reﬁnement systematically combines the class tokens with structural information preserved within the patch tokens. An adversarial attack when applied to such reﬁned tokens within the ensemble (ViTs) and show the potential for much strong attack mechanisms that would exploit the architectural characteristics of ViTs. Our proposed novel approach involving multiple discriminative pathways and token reﬁnement is able to ﬁll in these gaps, achieving signiﬁcant performance boosts when applied over a range of state-of-the-art attack methods. set distributed across the entire dataset. We extracted class tokens with and without reﬁnement from the intermediate blocks (5,6,7,8) of Deit-T, Deit-S, and Deit-B. Our reﬁned tokens have lower intra-class variations i.e., feature representations of samples from the same class are clustered together. Furthermore, the reﬁned tokens have better inter-class separation than the original tokens. This indicates that reﬁnement minimizes the misalignment between the ﬁnal classiﬁer and intermediate class tokens, which leads to more disentangled representations. Attacking such disentangled representations across the self-ensemble allows us to ﬁnd better adversarial direction that leads to more powerful attacks",
    "volume": "main",
    "checked": true,
    "id": "0918125daacb6c2b3a2d3f155ad095d5ae8fb9b9",
    "citation_count": 30
  },
  "https://openreview.net/forum?id=NdOoQnYPj_": {
    "title": "BAM: Bayes with Adaptive Memory",
    "abstract": "Online learning via Bayes' theorem allows new data to be continuously integrated into an agent's current beliefs. However, a naive application of Bayesian methods in non-stationary environments leads to slow adaptation and results in state estimates that may converge confidently to the wrong parameter value. A common solution when learning in changing environments is to discard/downweight past data; however, this simple mechanism of \"forgetting\" fails to account for the fact that many real-world environments involve revisiting similar states. We propose a new framework, Bayes with Adaptive Memory (BAM), that takes advantage of past experience by allowing the agent to choose which past observations to remember and which to forget. We demonstrate that BAM generalizes many popular Bayesian update rules for non-stationary environments. Through a variety of experiments, we demonstrate the ability of BAM to continuously adapt in an ever-changing world",
    "volume": "main",
    "checked": true,
    "id": "27fcffc50eba1ccf8fa3dbc181b9fde0cc24b652",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=DhzIU48OcZh": {
    "title": "P-Adapters: Robustly Extracting Factual Information from Language Models with Diverse Prompts",
    "abstract": "Recent work (e.g. LAMA (Petroni et al., 2019)) has found that the quality of the factual information extracted from Large Language Models (LLMs) depends on the prompts used to query them. This inconsistency is problematic because different users will query LLMs for the same information using different wording, but should receive the same, accurate responses regardless. In this work we aim to address this shortcoming by introducing P-Adapters: lightweight models that sit between the embedding layer and first attention layer of LLMs. They take LLM embeddings as input and output continuous prompts that are used to query the LLM. Additionally, we investigate Mixture of Experts (MoE) models that learn a set of continuous prompts (\"experts\") and select one to query the LLM. They require a separate classifier trained on human-annotated data to map natural language prompts to the continuous ones. P-Adapters perform comparably to the more complex MoE models in extracting factual information from BERT and RoBERTa while eliminating the need for additional annotations. PAdapters show between 12-26% absolute improvement in precision and 36-50% absolute improvement in consistency over a baseline of only using natural language queries. Finally, we investigate what makes a P-Adapter successful and conclude that access to the LLM's embeddings of the original natural language prompt, particularly the subject of the entity pair being asked about, is a significant factor",
    "volume": "main",
    "checked": true,
    "id": "58947177663d73b4d7809e74482b54aadaee6444",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=Wm3EA5OlHsG": {
    "title": "Scene Transformer: A unified architecture for predicting future trajectories of multiple agents",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "a4bc6aee56e91c8881c421fa3472e2d211e0bdb2",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=sPIFuucA3F": {
    "title": "Offline Neural Contextual Bandits: Pessimism, Optimization and Generalization",
    "abstract": "Offline policy learning (OPL) leverages existing data collected a priori for policy optimization without any active exploration. Despite the prevalence and recent interest in this problem, its theoretical and algorithmic foundations in function approximation settings remain under-developed. In this paper, we consider this problem on the axes of distributional shift, optimization, and generalization in offline contextual bandits with neural networks. In particular, we propose a provably efficient offline contextual bandit with neural network function approximation that does not require any functional assumption on the reward. We show that our method provably generalizes over unseen contexts under a milder condition for distributional shift than the existing OPL works. Notably, unlike any other OPL method, our method learns from the offline data in an online manner using stochastic gradient descent, allowing us to leverage the benefits of online learning into an offline setting. Moreover, we show that our method is more computationally efficient and has a better dependence on the effective dimension of the neural network than an online counterpart. Finally, we demonstrate the empirical effectiveness of our method in a range of synthetic and real-world OPL problems",
    "volume": "main",
    "checked": true,
    "id": "0c78bf086ae589d0a5a68b8e8326d2048b87018f",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=pN1JOdrSY9": {
    "title": "Contrastive Clustering to Mine Pseudo Parallel Data for Unsupervised Translation",
    "abstract": "Modern unsupervised machine translation systems mostly train their models by generating synthetic parallel training data from large unlabeled monolingual corpora of different languages through various means, such as iterative back-translation. However, there may exist small amount of actual parallel data hidden in the sea of unlabeled data, which has not been exploited. We develop a new fine-tuning objective, called Language-Agnostic Constraint for SwAV loss, or LAgSwAV, which enables a pre-trained model to extract such pseudo-parallel data from the monolingual corpora in a fully unsupervised manner. We then propose an effective strategy to utilize the obtained synthetic data to augment unsupervised machine translation. Our method achieves the state of the art in the WMT'14 English-French, WMT'16 German-English and English-Romanian bilingual unsupervised translation tasks, with 40.2, 36.8, 37.0 BLEU, respectively. We also achieve substantial improvements in the FLoRes low-resource English-Nepali and English-Sinhala unsupervised tasks with 5.3 and 5.4 BLEU, respectively. One-sentence Summary: We propose a fine-tuning loss that enables pre-trained model's ability to mine pseudo-parallel data for fully unsupervised machine translation",
    "volume": "main",
    "checked": true,
    "id": "18977cf1e4a3c200529f8c01d89a63fceeff026f",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=0JzqUlIVVDd": {
    "title": "KL Guided Domain Adaptation",
    "abstract": "Domain adaptation is an important problem and often needed for real-world applications. In this problem, instead of i.i.d. datapoints, we assume that the source (training) data and the target (testing) data have different distributions. With that setting, the empirical risk minimization training procedure often does not perform well, since it does not account for the change in the distribution. A common approach in the domain adaptation literature is to learn a representation of the input that has the same distributions over the source and the target domain. However, these approaches often require additional networks and/or optimizing an adversarial (minimax) objective, which can be very expensive or unstable in practice. To tackle this problem, we first derive a generalization bound for the target loss based on the training loss and the reverse Kullback–Leibler (KL) divergence between the source and the target representation distributions. Based on this bound, we derive an algorithm that minimizes the KL term to obtain a better generalization to the target domain. We show that with a probabilistic representation network, the KL term can be estimated efficiently via minibatch samples without any additional network or a minimax objective. This leads to a theoretically sound alignment method which is also very efficient and stable in practice. Experimental results also suggest that our method outperforms other representation-alignment approaches",
    "volume": "main",
    "checked": true,
    "id": "aab9863570372c6847fe40581332579388178665",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=vds4SNooOe": {
    "title": "Superclass-Conditional Gaussian Mixture Model For Learning Fine-Grained Embeddings",
    "abstract": "Learning fine-grained embeddings is essential for extending the generalizability of models pre-trained on \"coarse\" labels (e.g., animals). It is crucial to fields for which fine-grained labeling (e.g., breeds of animals) is expensive, but fine-grained prediction is desirable, such as medicine. The dilemma necessitates adaptation of a \"coarsely\" pre-trained model to new tasks with a few \"finer-grained\" training labels. However, coarsely supervised pre-training tends to suppress intra-class variation, which is vital for cross-granularity adaptation. In this paper, we develop a training framework underlain by a novel superclass-conditional Gaussian mixture model (SCGM). SCGM imitates the generative process of samples from hierarchies of classes through latent variable modeling of the fine-grained subclasses. The framework is agnostic to the encoders and only adds a few distribution related parameters, thus is efficient, and flexible to different domains. The model parameters are learned end-to-end by maximum-likelihood estimation via a principled Expectation-Maximization algorithm. Extensive experiments on benchmark datasets and a real-life medical dataset indicate the effectiveness of our method",
    "volume": "main",
    "checked": true,
    "id": "1ae600e7a24da0e6914efaaa75ff12e9d6e21208",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=gICys3ITSmj": {
    "title": "The Close Relationship Between Contrastive Learning and Meta-Learning",
    "abstract": "the close relationship between contrastive learning and meta-learning under a certain task distribution. We com-plement this observation by showing that established meta-learning methods, such as Prototypical Networks, achieve comparable performance to SimCLR when paired with this task distribution. This relationship can be leveraged by taking established techniques from meta-learning, such as task-based data augmentation, and showing that they benefit contrastive learning as well. These tricks also benefit state-of-the-art self-supervised learners without using negative pairs such as BYOL, which achieves 94.6% accuracy on CIFAR-10 using a self-supervised ResNet-18 feature extractor trained with our meta-learning tricks. We conclude that existing advances designed for contrastive learning or meta-learning can be exploited to benefit the other, and it is better for contrastive learning researchers to take lessons from the meta-learning literature (and vice-versa) than to reinvent the wheel. Our Pytorch implementation can be found",
    "volume": "main",
    "checked": true,
    "id": "5942115e923dcd5b3eae63865a59c05160ad1ad7",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=4C93Qvn-tz": {
    "title": "MCMC Should Mix: Learning Energy-Based Model with Neural Transport Latent Space MCMC",
    "abstract": "Learning energy-based model (EBM) requires MCMC sampling of the learned model as an inner loop of the learning algorithm. However, MCMC sampling of EBMs in high-dimensional data space is generally not mixing, because the energy function, which is usually parametrized by a deep network, is highly multi-modal in the data space. This is a serious handicap for both theory and practice of EBMs. In this paper, we propose to learn an EBM with a flow-based model (or in general a latent variable model) serving as a backbone, so that the EBM is a correction or an exponential tilting of the flow-based model. We show that the model has a particularly simple form in the space of the latent variables of the backbone model, and MCMC sampling of the EBM in the latent space mixes well and traverses modes in the data space. This enables proper sampling and learning of EBMs",
    "volume": "main",
    "checked": true,
    "id": "11319c881ab2c1033c7c840cb1abc425e1377f40",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=t5EmXZ3ZLR": {
    "title": "SOSP: Efficiently Capturing Global Correlations by Second-Order Structured Pruning",
    "abstract": "Pruning neural networks reduces inference time and memory costs. On standard hardware, these beneﬁts will be especially prominent if coarse-grained structures, like feature maps, are pruned. We devise two novel saliency-based methods for second-order structured pruning (SOSP) which include correlations among all structures and layers. Our main method SOSP-H employs an innovative second-order approximation, which enables saliency evaluations by fast Hessian-vector products. SOSP-H thereby scales like a ﬁrst-order method despite taking into account the full Hessian. We validate SOSP-H by comparing it to our second method SOSP-I that uses a well-established Hessian approximation, and to numerous state-of-the-art methods. While SOSP-H performs on par or better in terms of accuracy, it has clear advantages in terms of scalability and efﬁciency. This allowed us to scale SOSP-H to large-scale vision tasks, even though it captures correlations across all layers of the network. To underscore the global nature of our pruning methods, we evaluate their performance not only by removing structures from a pretrained network, but also by detecting architectural bottlenecks. We show that our algorithms allow to systematically reveal architectural bottlenecks, which we then widen to further increase the accuracy of the networks",
    "volume": "main",
    "checked": true,
    "id": "1658db02f2fabd9fb7bb62cb6d1c8774156ef7a5",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=tBtoZYKd9n": {
    "title": "Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions",
    "abstract": "Graph generative models are a highly active branch of machine learning. Given the steady development of new models of ever-increasing complexity, it is necessary to provide a principled way to evaluate and compare them. In this paper, we enumerate the desirable criteria for such a comparison metric and provide an overview of the status quo of graph generative model comparison in use today, which predominantly relies on the maximum mean discrepancy (MMD). We perform a systematic evaluation of MMD in the context of graph generative model comparison, highlighting some of the challenges and pitfalls researchers inadvertently may encounter. After conducting a thorough analysis of the behaviour of MMD on synthetically-generated perturbed graphs as well as on recently-proposed graph generative models, we are able to provide a suitable procedure to mitigate these challenges and pitfalls. We aggregate our findings into a list of practical recommendations for researchers to use when evaluating graph generative models",
    "volume": "main",
    "checked": true,
    "id": "95b7bd36ba9fbfa3f9e0a1cd4b3f1660d6780297",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=HuaYQfggn5u": {
    "title": "FedBABU: Toward Enhanced Representation for Federated Image Classification",
    "abstract": "Federated learning has evolved to improve a single global model under data heterogeneity (as a curse) or to develop multiple personalized models using data heterogeneity (as a blessing). However, little research has considered both directions simultaneously. In this paper, we first investigate the relationship between them by analyzing Federated Averaging (McMahan et al., 2017) at the client level and determine that a better federated global model performance does not constantly improve personalization. To elucidate the cause of this personalization performance degradation problem, we decompose the entire network into the body (extractor), which is related to universality, and the head (classifier), which is related to personalization. We then point out that this problem stems from training the head. Based on this observation, we propose a novel federated learning algorithm, coined FedBABU, which only updates the body of the model during federated training (i.e., the head is randomly initialized and never updated), and the head is fine-tuned for personalization during the evaluation process. Extensive experiments show consistent performance improvements and an efficient personalization of FedBABU. The code is available at https://github.com/jhoon-oh/FedBABU",
    "volume": "main",
    "checked": true,
    "id": "3cd5b67f7bc25a14a9505d27a5b18cfb5592769b",
    "citation_count": 15
  },
  "https://openreview.net/forum?id=WuEiafqdy9H": {
    "title": "Model-augmented Prioritized Experience Replay",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "fa7b186382cfc93454830b5d947a4ac1a9453cff",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=PQQp7AJwz3": {
    "title": "Particle Stochastic Dual Coordinate Ascent: Exponential convergent algorithm for mean field neural network optimization",
    "abstract": "We introduce Particle-SDCA, a gradient-based optimization algorithm for two-layer neural networks in the mean field regime that achieves exponential convergence rate in regularized empirical risk minimization. The proposed algorithm can be regarded as an infinite dimensional extension of Stochastic Dual Coordinate Ascent (SDCA) in the probability space: we exploit the convexity of the dual problem, for which the coordinate-wise proximal gradient method can be applied. Our proposed method inherits advantages of the original SDCA, including (i) exponential convergence (with respect to the outer iteration steps), and (ii) better dependency on the sample size and condition number than the full-batch gradient method. One technical challenge in implementing the SDCA update is the intractable integral over the entire parameter space at every step. To overcome this limitation, we propose a tractable particle method that approximately solves the dual problem, and an importance re-weighting technique to reduce the computational cost. The convergence rate of our method is verified by numerical experiments",
    "volume": "main",
    "checked": true,
    "id": "8815d9fd43661ffdc6f9240dbca0a020c4f80d98",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=dgxFTxuJ50e": {
    "title": "Learnability of convolutional neural networks for infinite dimensional input via mixed and anisotropic smoothness",
    "abstract": "Among a wide range of success of deep learning, convolutional neural networks have been extensively utilized in several tasks such as speech recognition, image processing, and natural language processing, which require inputs with large dimensions. Several studies have investigated function estimation capability of deep learning, but most of them have assumed that the dimensionality of the input is much smaller than the sample size. However, for typical data in applications such as those handled by the convolutional neural networks described above, the dimensionality of inputs is relatively high or even infinite. In this paper, we investigate the approximation and estimation errors of the (dilated) convolutional neural networks when the input is infinite dimensional. Although the approximation and estimation errors of neural networks are affected by the curse of dimensionality in the existing analyses for typical function spaces such as the Hölder and Besov spaces, we show that, by considering anisotropic smoothness, they can alleviate exponential dependency on the dimensionality but they only depend on the smoothness of the target functions. Our theoretical analysis supports the great practical success of convolutional networks. Furthermore, we show that the dilated convolution is advantageous when the smoothness of the target function has a sparse structure",
    "volume": "main",
    "checked": true,
    "id": "34b6284beb50b0b60afe20632dea33db23ada5e0",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=s03AQxehtd_": {
    "title": "ProtoRes: Proto-Residual Network for Pose Authoring via Learned Inverse Kinematics",
    "abstract": "Our work focuses on the development of a learnable neural representation of human pose for advanced AI assisted animation tooling. Speciﬁcally, we tackle the problem of constructing a full static human pose based on sparse and variable user inputs ( e.g. locations and/or orientations of a subset of body joints). To solve this problem, we propose a novel neural architecture that combines residual connections with prototype encoding of a partially speciﬁed pose to create a new complete pose from the learned latent space. We show that our architecture outperforms a baseline based on Transformer, both in terms of accuracy and computational efﬁciency. Additionally, we develop a user interface to integrate our neural model in Unity, a real-time 3D development platform. Furthermore, we introduce two new datasets representing the static human pose modeling problem, based on high-quality human motion capture data. Our code is publically available here: https://github.com/boreshkinai/protores . deforming a 3D mesh on top of the skeleton, to avoid unrealistic twisting. This is viable via skeleton representations based on Euler angles (Han et al., 2017), rotation matrices (Zhang and quaternions (Pavllo et al., 2018). In this work, we use the two-row 6D rotation matrix representation that addresses the continuity issues reminiscent of the other representations et al., 2019). number of heterogeneous user inputs (position, angle, direction) to reconstruct a full-body pose. We compare ProtoRes against two strong ML baselines, Masked-FCR and Transformer, showing superior results for ProtoRes, both in terms of accuracy and computational efﬁciency. We also show that ML models reconstruct full-body poses from sparse user inputs more accurately than existing non-learnable inverse kinematics models. We develop a suite of UI tools for the integration of our model in Unity and provide demos showing how our model can be used effectively to solve the discrete pose authoring problem by the end user. Our results have a few implications. First, our ML based tools will have positive impacts on the simpliﬁcation and democratization of the game development process by helping a wide audience materialize their creative animation ideas. Second, our novel approach to neural pose representation could be applied in a variety of tasks where efﬁcient and accurate reconstruction of full-body poses from noisy intermittent measurements is important",
    "volume": "main",
    "checked": true,
    "id": "3642b974f794e4dc6cc9327a09cc8c28b6ae6bd3",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=AJsI-ymaKn_": {
    "title": "POETREE: Interpretable Policy Learning with Adaptive Decision Trees",
    "abstract": "Building models of human decision-making from observed behaviour is critical to better understand, diagnose and support real-world policies such as clinical care. As established policy learning approaches remain focused on imitation performance, they fall short of explaining the demonstrated decision-making process. Policy Extraction through decision Trees (POETREE) is a novel framework for interpretable policy learning, compatible with fully-offline and partially-observable clinical decision environments – and builds probabilistic tree policies determining physician actions based on patients' observations and medical history. Fullydifferentiable tree architectures are grown incrementally during optimization to adapt their complexity to the modelling task, and learn a representation of patient history through recurrence, resulting in decision tree policies that adapt over time with patient information. This policy learning method outperforms the stateof-the-art on real and synthetic medical datasets, both in terms of understanding, quantifying and evaluating observed behaviour as well as in accurately replicating it – with potential to improve future decision support systems",
    "volume": "main",
    "checked": true,
    "id": "3986a3070cde3354e9d80faff70d72c328434032",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=7fFO4cMBx_9": {
    "title": "Variational Neural Cellular Automata",
    "abstract": "In nature, the process of cellular growth and differentiation has lead to an amazing diversity of organisms — algae, starfish, giant sequoia, tardigrades, and orcas are all created by the same generative process. Inspired by the incredible diversity of this biological generative process, we propose a generative model, the Variational Neural Cellular Automata (VNCA), which is loosely inspired by the biological processes of cellular growth and differentiation. Unlike previous related works, the VNCA is a proper probabilistic generative model, and we evaluate it according to best practices. We find that the VNCA learns to reconstruct samples well and that despite its relatively few parameters and simple local-only communication, the VNCA can learn to generate a large variety of output from information encoded in a common vector format. While there is a significant gap to the current state-of-the-art in terms of generative modeling performance, we show that the VNCA can learn a purely self-organizing generative process of data. Additionally, we show that the VNCA can learn a distribution of stable attractors that can recover from significant damage",
    "volume": "main",
    "checked": true,
    "id": "9e2fcbf361e3d89b10f1ae2e3d9c49876db42976",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=JYtwGwIL7ye": {
    "title": "The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models",
    "abstract": "Reward hacking—where RL agents exploit gaps in misspecified reward functions—has been widely observed, but not yet systematically studied. To understand how reward hacking arises, we construct four RL environments with misspecified rewards. We investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, observation space noise, and training time. More capable agents often exploit reward misspecifications, achieving higher proxy reward and lower true reward than less capable agents. Moreover, we find instances of phase transitions: capability thresholds at which the agent's behavior qualitatively shifts, leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To address this, we propose an anomaly detection task for aberrant policies and offer several baseline detectors",
    "volume": "main",
    "checked": true,
    "id": "10b9ca173c665e3f2c322c2d5ce9b9d433fe4629",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=CCu6RcUMwK0": {
    "title": "Neural Link Prediction with Walk Pooling",
    "abstract": "Graph neural networks achieve high accuracy in link prediction by jointly leveraging graph topology and node attributes. Topology, however, is represented indirectly; state-of-the-art methods based on subgraph classification label nodes with distance to the target link, so that, although topological information is present, it is tempered by pooling. This makes it challenging to leverage features like loops and motifs associated with network formation mechanisms. We propose a link prediction algorithm based on a new pooling scheme called WalkPool. WalkPool combines the expressivity of topological heuristics with the feature-learning ability of neural networks. It summarizes a putative link by random walk probabilities of adjacent paths. Instead of extracting transition probabilities from the original graph, it computes the transition matrix of a \"predictive\" latent graph by applying attention to learned features; this may be interpreted as feature-sensitive topology fingerprinting. WalkPool can leverage unsupervised node features or be combined with GNNs and trained end-to-end. It outperforms state-of-the-art methods on all common link prediction benchmarks, both homophilic and heterophilic, with and without node attributes. Applying WalkPool to a set of unsupervised GNNs significantly improves prediction accuracy, suggesting that it may be used as a general-purpose graph pooling scheme",
    "volume": "main",
    "checked": true,
    "id": "610621a8ac941ae6c6781250a3cdb705616c983c",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=rTAclwH46Tb": {
    "title": "Eigencurve: Optimal Learning Rate Schedule for SGD on Quadratic Objectives with Skewed Hessian Spectrums",
    "abstract": "Learning rate schedulers have been widely adopted in training deep neural networks. Despite their practical importance, there is a discrepancy between its practice and its theoretical analysis. For instance, it is not known what schedules of SGD achieve best convergence, even for simple problems such as optimizing quadratic objectives. In this paper, we propose Eigencurve, the ﬁrst family of learning rate schedules that can achieve minimax optimal convergence rates (up to a constant) for SGD on quadratic objectives when the eigenvalue distribution of the underlying Hessian matrix is skewed. The condition is quite common in practice. Experimental results show that Eigencurve can signiﬁcantly outperform step decay in image classiﬁcation tasks on CIFAR-10, especially when the number of epochs is small. Moreover, the theory inspires two simple learning rate schedulers for practical applications that can approximate eigencurve. For some problems, the optimal shape of the proposed schedulers resembles that of cosine decay, which sheds light to the success of cosine decay for such situations. For other situations, the proposed schedulers are superior to cosine decay",
    "volume": "main",
    "checked": true,
    "id": "ae8210e2443572ad9a05e7e66058ea0919e6db9e",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=lbauk6wK2-y": {
    "title": "Object Pursuit: Building a Space of Objects via Discriminative Weight Generation",
    "abstract": "We propose a framework to continuously learn object-centric representations for visual learning and understanding. Existing object-centric representations either rely on supervisions that individualize objects in the scene, or perform unsupervised disentanglement that can hardly deal with complex scenes in the real world. To mitigate the annotation burden and relax the constraints on the statistical com-plexity of the data, our method leverages interactions to effectively sample diverse variations of an object and the corresponding training signals while learning the object-centric representations. Throughout learning, objects are streamed one by one in random order with unknown identities, and are associated with latent codes that can synthesize discriminative weights for each object through a convolutional hypernetwork. Moreover, re-identiﬁcation of learned objects and forgetting prevention are employed to make the learning process efﬁcient and robust. We perform an extensive study of the key features of the proposed framework and analyze the characteristics of the learned representations. Furthermore, we demonstrate the capability of the proposed framework in learning representations that can improve label efﬁciency in downstream tasks. Our code and trained models are made publicly available at: https://github.com/pptrick/ Object-Pursuit ",
    "volume": "main",
    "checked": true,
    "id": "a3f828416f5c8400e268ef07b71b5b6fd9e73f96",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=jbrgwbv8nD": {
    "title": "Constraining Linear-chain CRFs to Regular Languages",
    "abstract": "A major challenge in structured prediction is to represent the interdependencies within output structures. When outputs are structured as sequences, linear-chain conditional random fields (CRFs) are a widely used model class which can learn local dependencies in the output. However, the CRF's Markov assumption makes it impossible for CRFs to represent distributions with nonlocal dependencies, and standard CRFs are unable to respect nonlocal constraints of the data (such as global arity constraints on output labels). We present a generalization of CRFs that can enforce a broad class of constraints, including nonlocal ones, by specifying the space of possible output structures as a regular language L. The resulting regular-constrained CRF (RegCCRF) has the same formal properties as a standard CRF, but assigns zero probability to all label sequences not in L. Notably, RegCCRFs can incorporate their constraints during training, while related models only enforce constraints during decoding. We prove that constrained training is never worse than constrained decoding, and show empirically that it can be substantially better in practice. Finally, we demonstrate a practical benefit on downstream tasks by incorporating a RegCCRF into a deep neural model for semantic role labeling, exceeding state-of-the-art results on a standard dataset",
    "volume": "main",
    "checked": true,
    "id": "81ccc813ece5ea08a0b2f7e0e5455ddf53e5f89f",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=-70L8lpp9DF": {
    "title": "Hyperparameter Tuning with Renyi Differential Privacy",
    "abstract": "For many differentially private algorithms, such as the prominent noisy stochastic gradient descent (DP-SGD), the analysis needed to bound the privacy leakage of a single training run is well understood. However, few studies have reasoned about the privacy leakage resulting from the multiple training runs needed to fine tune the value of the training algorithm's hyperparameters. In this work, we first illustrate how simply setting hyperparameters based on non-private training runs can leak private information. Motivated by this observation, we then provide privacy guarantees for hyperparameter search procedures within the framework of Renyi Differential Privacy. Our results improve and extend the work of Liu and Talwar (STOC 2019). Our analysis supports our previous observation that tuning hyperparameters does indeed leak private information, but we prove that, under certain assumptions, this leakage is modest, as long as each candidate training run needed to select hyperparameters is itself differentially private",
    "volume": "main",
    "checked": true,
    "id": "12d8a96760e1752bb7fd78f6507ec91ec7581f79",
    "citation_count": 23
  },
  "https://openreview.net/forum?id=Vr_BTpw3wz": {
    "title": "Hindsight: Posterior-guided training of retrievers for improved open-ended generation",
    "abstract": "Many text generation systems benefit from using a retriever to retrieve passages from a textual knowledge corpus (e.g., Wikipedia) and providing these passages as additional context to the generator. For open-ended generation tasks (like generating informative utterances in conversations) many varied passages may be equally relevant and we find that existing methods that jointly train the retriever and generator underperform: the retriever may not find relevant passages even amongst the top-10 and the generator may hence not learn a preference to ground its generated output in them. We propose using an additional guide retriever that is allowed to use the target output and \"in hindsight\" retrieve relevant passages during training. We model the guide retriever after the posterior distribution Q of passages given the input and the target output and train it jointly with the standard retriever and the generator by maximizing the evidence lower bound (ELBo) in expectation over Q. For informative conversations from the Wizard of Wikipedia dataset, with posterior-guided training, the retriever finds passages with higher relevance in the top-10 (23% relative improvement), the generator's responses are more grounded in the retrieved passage (19% relative improvement) and the end-to-end system produces better overall output (6.4% relative improvement)",
    "volume": "main",
    "checked": true,
    "id": "753fd6952c9f06f3bbd46e37129acc3f7a984896",
    "citation_count": 15
  },
  "https://openreview.net/forum?id=7_JR7WpwKV1": {
    "title": "The Effects of Invertibility on the Representational Complexity of Encoders in Variational Autoencoders",
    "abstract": "Training and using modern neural-network based latent-variable generative models (like Variational Autoencoders) often require simultaneously training a generative direction along with an inferential (encoding) direction, which approximates the posterior distribution over the latent variables. Thus, the question arises: how complex does the inferential model need to be, in order to be able to accurately model the posterior distribution of a given generative model? In this paper, we identify an important property of the generative map impacting the required size of the encoder. We show that if the generative map is \"strongly invertible\" (in a sense we suitably formalize), the inferential model need not be much more complex. Conversely, we prove that there exist noninvertible generative maps, for which the encoding direction needs to be exponentially larger (under standard assumptions in computational complexity). Importantly, we do not require the generative model to be layerwise invertible, which a lot of the related literature assumes and isn't satisfied by many architectures used in practice (e.g. convolution and pooling based networks). Thus, we provide theoretical support for the empirical wisdom that learning deep generative models is harder when data lies on a low-dimensional manifold",
    "volume": "main",
    "checked": true,
    "id": "7bb65e9167e5d21f04ebaacdd7bc59f7c4972bb7",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=BGvt0ghNgA": {
    "title": "Lipschitz-constrained Unsupervised Skill Discovery",
    "abstract": "We study the problem of unsupervised skill discovery, whose goal is to learn a set of diverse and useful skills with no external reward. There have been a number of skill discovery methods based on maximizing the mutual information (MI) between skills and states. However, we point out that their MI objectives usually prefer static skills to dynamic ones, which may hinder the application for downstream tasks. To address this issue, we propose Lipschitz-constrained Skill Discovery ( LSD ), which encourages the agent to discover more diverse, dynamic, and far-reaching skills. Another beneﬁt of LSD is that its learned representation function can be utilized for solving goal-following downstream tasks even in a zero-shot manner — i.e ., without further training or complex planning. Through experiments on various MuJoCo robotic locomotion and manipulation environments, we demonstrate that LSD outperforms previous approaches in terms of skill diversity, state space coverage, and performance on seven downstream tasks including the challenging task of following multiple goals on Humanoid. Our code and videos are available at https://shpark.me/projects/lsd/ ",
    "volume": "main",
    "checked": true,
    "id": "bf5d2d302f046cda8f75ceb09c842109e09c5862",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=ItkxLQU01lD": {
    "title": "Convergent Graph Solvers",
    "abstract": "We propose the convergent graph solver (CGS)1, a deep learning method that learns iterative mappings to predict the properties of a graph system at its stationary state (fixed point) with guaranteed convergence. CGS systematically computes the fixed points of a target graph system and decodes them to estimate the stationary properties of the system without the prior knowledge of existing solvers or intermediate solutions. The forward propagation of CGS proceeds in three steps: (1) constructing the input dependent linear contracting iterative maps, (2) computing the fixed-points of the linear maps, and (3) decoding the fixed-points to estimate the properties. The contractivity of the constructed linear maps guarantees the existence and uniqueness of the fixed points following the Banach fixed point theorem. To train CGS efficiently, we also derive a tractable analytical expression for its gradient by leveraging the implicit function theorem. We evaluate the performance of CGS by applying it to various network-analytic and graph benchmark problems. The results indicate that CGS has competitive capabilities for predicting the stationary properties of graph systems, irrespective of whether the target systems are linear or non-linear. CGS also shows high performance for graph classification problems where the existence or the meaning of a fixed point is hard to be clearly defined, which highlights the potential of CGS as a general graph neural network architecture",
    "volume": "main",
    "checked": true,
    "id": "7a35a6061ac036dad9ae03c3b1a27435630611bf",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=DhP9L8vIyLc": {
    "title": "PAC Prediction Sets Under Covariate Shift",
    "abstract": "An important challenge facing modern machine learning is how to rigorously quantify the uncertainty of model predictions. Conveying uncertainty is especially important when there are changes to the underlying data distribution that might invalidate the predictive model. Yet, most existing uncertainty quantification algorithms break down in the presence of such shifts. We propose a novel approach that addresses this challenge by constructing probably approximately correct (PAC) prediction sets in the presence of covariate shift. Our approach focuses on the setting where there is a covariate shift from the source distribution (where we have labeled training examples) to the target distribution (for which we want to quantify uncertainty). Our algorithm assumes given importance weights that encode how the probabilities of the training examples change under the covariate shift. In practice, importance weights typically need to be estimated; thus, we extend our algorithm to the setting where we are given confidence intervals for the importance weights. We demonstrate the effectiveness of our approach on covariate shifts based on DomainNet and ImageNet. Our algorithm satisfies the PAC constraint, and gives prediction sets with the smallest average normalized size among approaches that always satisfy the PAC constraint",
    "volume": "main",
    "checked": true,
    "id": "1a7cc437c4fb4ae26919f8f5926670b5cba5605c",
    "citation_count": 15
  },
  "https://openreview.net/forum?id=D78Go4hVcxO": {
    "title": "How Do Vision Transformers Work?",
    "abstract": "The success of multi-head self-attentions (MSAs) for computer vision is now indisputable. However, little is known about how MSAs work. We present fundamental explanations to help better understand the nature of MSAs. In particular, we demonstrate the following properties of MSAs and Vision Transformers (ViTs): 1 MSAs improve not only accuracy but also generalization by flattening the loss landscapes. Such improvement is primarily attributable to their data specificity, not long-range dependency. On the other hand, ViTs suffer from non-convex losses. Large datasets and loss landscape smoothing methods alleviate this problem; 2 MSAs and Convs exhibit opposite behaviors. For example, MSAs are low-pass filters, but Convs are high-pass filters. Therefore, MSAs and Convs are complementary; 3 Multi-stage neural networks behave like a series connection of small individual models. In addition, MSAs at the end of a stage play a key role in prediction. Based on these insights, we propose AlterNet, a model in which Conv blocks at the end of a stage are replaced with MSA blocks. AlterNet outperforms CNNs not only in large data regimes but also in small data regimes",
    "volume": "main",
    "checked": true,
    "id": "430bab3890e1e52c4c1f74900b0e408e47a1cb8f",
    "citation_count": 68
  },
  "https://openreview.net/forum?id=Vs5NK44aP9P": {
    "title": "Encoding Weights of Irregular Sparsity for Fixed-to-Fixed Model Compression",
    "abstract": "Even though fine-grained pruning techniques achieve a high compression ratio, conventional sparsity representations (such as CSR) associated with irregular sparsity degrade parallelism significantly. Practical pruning methods, thus, usually lower pruning rates (by structured pruning) to improve parallelism. In this paper, we study fixed-to-fixed (lossless) encoding architecture/algorithm to support finegrained pruning methods such that sparse neural networks can be stored in a highly regular structure. We first estimate the maximum compression ratio of encodingbased compression using entropy. Then, as an effort to push the compression ratio to the theoretical maximum (by entropy), we propose a sequential fixed-to-fixed encoding scheme. We demonstrate that our proposed compression scheme achieves almost the maximum compression ratio for the Transformer and ResNet-50 pruned by various fine-grained pruning methods",
    "volume": "main",
    "checked": true,
    "id": "cfcc55816a7b6f952e3010d998240cbff7fdfdb7",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=7DI6op61AY": {
    "title": "Neural Markov Controlled SDE: Stochastic Optimization for Continuous-Time Data",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "425bd431cd48ae4dbe15d83bd433b11b4ba84c79",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=TfhfZLQ2EJO": {
    "title": "SURF: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning",
    "abstract": "Preference-based reinforcement learning (RL) has shown potential for teaching agents to perform the target tasks without a costly, pre-defined reward function by learning the reward with a supervisor's preference between the two agent behaviors. However, preference-based learning often requires a large amount of human feedback, making it difficult to apply this approach to various applications. This data-efficiency problem, on the other hand, has been typically addressed by using unlabeled samples or data augmentation techniques in the context of supervised learning. Motivated by the recent success of these approaches, we present SURF, a semi-supervised reward learning framework that utilizes a large amount of unlabeled samples with data augmentation. In order to leverage unlabeled samples for reward learning, we infer pseudo-labels of the unlabeled samples based on the confidence of the preference predictor. To further improve the label-efficiency of reward learning, we introduce a new data augmentation that temporally crops consecutive subsequences from the original behaviors. Our experiments demonstrate that our approach significantly improves the feedback-efficiency of the state-ofthe-art preference-based method on a variety of locomotion and robotic manipulation tasks",
    "volume": "main",
    "checked": true,
    "id": "f2a2401a35b6b892d43642b31700e83e88b2ebb8",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=MXEl7i-iru": {
    "title": "GraphENS: Neighbor-Aware Ego Network Synthesis for Class-Imbalanced Node Classification",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "4d0f0212ac509445983dab3032af8cbd14a7c3e3",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=WQc075jmBmf": {
    "title": "CodeTrek: Flexible Modeling of Code using an Extensible Relational Representation",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "7bbfe2586d10d56081915a9edc44be2d29bbf8dc",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=tyTH9kOxcvh": {
    "title": "Modeling Label Space Interactions in Multi-label Classification using Box Embeddings",
    "abstract": "Multi-label classiﬁcation is a challenging structured prediction task in which a set of output class labels are predicted for each input. Real-world datasets often have natural or latent taxonomic relationships between labels, making it desirable for models to employ label representations capable of capturing such taxonomies. Most existing multi-label classiﬁcation methods do not do so, resulting in label predictions that are inconsistent with the taxonomic constraints, thus failing to accu-rately represent the fundamentals of problem setting. In this work we introduce the multi-label box model (MBM), a multi-label classiﬁcation method that combines the encoding power of neural networks with the inductive bias and probabilistic semantics of box embeddings (Vilnis, et al 2018). Box embeddings can be under-stood as trainable Venn-diagrams based on hyper-rectangles. Representing labels by boxes rather than vectors, MBM is able to capture taxonomic relations among labels. Furthermore, since box embeddings allow these relations to be learned by stochastic gradient descent from data, and to be read as calibrated conditional probabilities, our model is endowed with a high degree of interpretability. This interpretability also facilitates the injection of partial information about label-label relationships into model training, to further improve its consistency. We provide theoretical grounding for our method and show experimentally the model's ability to learn the true latent taxonomic structure from data. Through extensive empirical evaluations on both small and large-scale multi-label classiﬁcation datasets, we show that BBM can signiﬁcantly improve taxonomic consistency while preserving or surpassing state-of-the-art predictive performance",
    "volume": "main",
    "checked": true,
    "id": "325d64c3583a52cef76a33f174028493a850f960",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=gJcEM8sxHK": {
    "title": "Mapping Language Models to Grounded Conceptual Spaces",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "57db9833549247241decf442fcc30f6eb414981b",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=6VpeS27viTq": {
    "title": "Learnability Lock: Authorized Learnability Control Through Adversarial Invertible Transformations",
    "abstract": "Owing much to the revolution of information technology, the recent progress of deep learning benefits incredibly from the vastly enhanced access to data available in various digital formats. However, in certain scenarios, people may not want their data being used for training commercial models and thus studied how to attack the learnability of deep learning models. Previous works on learnability attack only consider the goal of preventing unauthorized exploitation on the specific dataset but not the process of restoring the learnability for authorized cases. To tackle this issue, this paper introduces and investigates a new concept called \"learnability lock\" for controlling the model's learnability on a specific dataset with a special key. In particular, we propose adversarial invertible transformation, that can be viewed as a mapping from image to image, to slightly modify data samples so that they become \"unlearnable\" by machine learning models with negligible loss of visual features. Meanwhile, one can unlock the learnability of the dataset and train models normally using the corresponding key. The proposed learnability lock leverages class-wise perturbation that applies a universal transformation function on data samples of the same label. This ensures that the learnability can be easily restored with a simple inverse transformation while remaining difficult to be detected or reverse-engineered. We empirically demonstrate the success and practicability of our method on visual classification tasks",
    "volume": "main",
    "checked": true,
    "id": "3f8af18256f5b158510ec248974568db25495e53",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=t98k9ePQQpn": {
    "title": "Optimal Transport for Long-Tailed Recognition with Learnable Cost Matrix",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "832c8e12e821db5fea16e486d40fd437d8302c6a",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=ahi2XSHpAUZ": {
    "title": "WeakM3D: Towards Weakly Supervised Monocular 3D Object Detection",
    "abstract": "Monocular 3D object detection is one of the most challenging tasks in 3D scene understanding. Due to the ill-posed nature of monocular imagery, existing monocular 3D detection methods highly rely on training with the manually annotated 3D box labels on the LiDAR point clouds. This annotation process is very laborious and expensive. To dispense with the reliance on 3D box labels, in this paper we explore the weakly supervised monocular 3D detection. Specifically, we first detect 2D boxes on the image. Then, we adopt the generated 2D boxes to select corresponding RoI LiDAR points as the weak supervision. Eventually, we adopt a network to predict 3D boxes which can tightly align with associated RoI LiDAR points. This network is learned by minimizing our newly-proposed 3D alignment loss between the 3D box estimates and the corresponding RoI LiDAR points. We will illustrate the potential challenges of the above learning problem and resolve these challenges by introducing several effective designs into our method. Codes will be available at https://github.com/SPengLiang/WeakM3D",
    "volume": "main",
    "checked": true,
    "id": "cfec17d737cd9b6573c54a21cfec025fe7be3bb2",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=6tmjoym9LR6": {
    "title": "Stability Regularization for Discrete Representation Learning",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "35906070dbcbd947d467d2ba54c591d6b27df510",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=IcUWShptD7d": {
    "title": "Monotonic Differentiable Sorting Networks",
    "abstract": "Differentiable sorting algorithms allow training with sorting and ranking supervision, where only the ordering or ranking of samples is known. Various methods have been proposed to address this challenge, ranging from optimal transport-based differentiable Sinkhorn sorting algorithms to making classic sorting networks differentiable. One problem of current differentiable sorting methods is that they are non-monotonic. To address this issue, we propose a novel relaxation of conditional swap operations that guarantees monotonicity in differentiable sorting networks. We introduce a family of sigmoid functions and prove that they produce differentiable sorting networks that are monotonic. Monotonicity ensures that the gradients always have the correct sign, which is an advantage in gradient-based optimization. We demonstrate that monotonic differentiable sorting networks improve upon previous differentiable sorting methods",
    "volume": "main",
    "checked": true,
    "id": "09a4e48ecfcf01a11dd78bef525255d683226345",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=2_vhkAMARk": {
    "title": "Escaping limit cycles: Global convergence for constrained nonconvex-nonconcave minimax problems",
    "abstract": "This paper introduces a new extragradient-type algorithm for a class of nonconvex-nonconcave minimax problems. It is well-known that ﬁnding a local solution for general minimax problems is computationally intractable. This ob-servation has recently motivated the study of structures su ﬃ cient for convergence of ﬁrst order methods in the more general setting of variational inequalities when the so-called weak Minty variational inequality (MVI) holds. This problem class captures non-trivial structures as we demonstrate with examples, for which a large family of existing algorithms provably converge to limit cycles. Our results require a less restrictive parameter range in the weak MVI compared to what is previously known, thus extending the applicability of our scheme. The proposed algorithm is applicable to constrained and regularized problems, and involves an adaptive stepsize allowing for potentially larger stepsizes. Our scheme also converges globally even in settings where the underlying operator exhibits limit cycles",
    "volume": "main",
    "checked": true,
    "id": "d7c46f51692defa6821e8e50da33498d2a7c4bce",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=vwLLQ-HwqhZ": {
    "title": "Continual Normalization: Rethinking Batch Normalization for Online Continual Learning",
    "abstract": "Existing continual learning methods use Batch Normalization (BN) to facilitate training and improve generalization across tasks. However, the non-i.i.d and nonstationary nature of continual learning data, especially in the online setting, amplify the discrepancy between training and testing in BN and hinder the performance of older tasks. In this work, we study the cross-task normalization effect of BN in online continual learning where BN normalizes the testing data using moments biased towards the current task, resulting in higher catastrophic forgetting. This limitation motivates us to propose a simple yet effective method that we call Continual Normalization (CN) to facilitate training similar to BN while mitigating its negative effect. Extensive experiments on different continual learning algorithms and online scenarios show that CN is a direct replacement for BN and can provide substantial performance improvements. Our implementation is available at https://github.com/phquang/Continual-Normalization",
    "volume": "main",
    "checked": true,
    "id": "f0373087349d3393e42cd15c6aa01e6f320d4341",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=Harn4_EZBw": {
    "title": "Generative Pseudo-Inverse Memory",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "5bbc8681584b307eff08ebcaf1bbef4cc91bdd16",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=irARV_2VFs4": {
    "title": "Focus on the Common Good: Group Distributional Robustness Follows",
    "abstract": "generalization performance when appropriate, they are marked in the third row. All the results are averaged over multiple runs; FMoW numbers are averaged over three seeds, CivilComments over ﬁve seeds, Camelyon17 over ten seeds, and PovertyMap over the ﬁve data folds. Conﬁrming with the WILDS standard, we report worst group accuracy for FMoW, CivilComments, worst region Pearson correlation for PovertyMap, average out-of-domain accuracy for Camelyon17. We make the following observations from the results. ERM is surprisingly strong on all the tasks except CivilComments. Strikingly, Group-DRO is worse than ERM on four of the ﬁve tasks shown in the table, including the in-domain (sub-population shift) evaluation on PovertyMap task. CGD is the only algorithm that performs consistently well across all the tasks. The results suggest CGD is signiﬁcantly robust to sub-population shift, and performs no worse than ERM on domain shifts. Further, we study Colored-MNIST dataset under varying ratio of majority to minority group sizes in Appendix G and demonstrate that CGD is robust to sub-population shifts even under extreme training population disparity",
    "volume": "main",
    "checked": true,
    "id": "1e57462f93d78279549a8508e691dc4920151b35",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=dEwfxt14bca": {
    "title": "When should agents explore?",
    "abstract": "Exploration remains a central challenge for reinforcement learning (RL). Virtu-ally all existing methods share the feature of a monolithic behaviour policy that changes only gradually (at best). In contrast, the exploratory behaviours of animals and humans exhibit a rich diversity, namely including forms of switching between modes. This paper presents an initial study of mode-switching, non-monolithic exploration for RL. We investigate different modes to switch between, at what timescales it makes sense to switch, and what signals make for good switching triggers. We also propose practical algorithmic components that make the switching mechanism adaptive and robust, which enables ﬂexibility without an accompanying hyper-parameter-tuning burden. Finally, we report a promis-ing and detailed analysis on Atari, using two-mode exploration and switching at sub-episodic time-scales. intra-episodic variants and their resulting behaviours",
    "volume": "main",
    "checked": true,
    "id": "cfabb20df17109036a82934cc12b5c8e92223f6c",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=srtIXtySfT4": {
    "title": "Neural Parameter Allocation Search",
    "abstract": "Training neural networks requires increasing amounts of memory. Parameter sharing can reduce memory and communication costs, but existing methods assume networks have many identical layers and utilize hand-crafted sharing strategies that fail to generalize. We introduce Neural Parameter Allocation Search (NPAS), a novel task where the goal is to train a neural network given an arbitrary, fixed parameter budget. NPAS covers both low-budget regimes, which produce compact networks, as well as a novel high-budget regime, where additional capacity can be added to boost performance without increasing inference FLOPs. To address NPAS, we introduce Shapeshifter Networks (SSNs), which automatically learn where and how to share parameters in a network to support any parameter budget without requiring any changes to the architecture or loss function. NPAS and SSNs provide a complete framework for addressing generalized parameter sharing, and can also be combined with prior work for additional performance gains. We demonstrate the effectiveness of our approach using nine network architectures across four diverse tasks, including ImageNet classification and transformers",
    "volume": "main",
    "checked": true,
    "id": "235de6164e754400f82b77fb33be84fc43c79078",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=Ro_zAjZppv": {
    "title": "Tracking the risk of a deployed model and detecting harmful distribution shifts",
    "abstract": "When deployed in the real world, machine learning models inevitably encounter changes in the data distribution, and certain—but not all—distribution shifts could result in signiﬁcant performance degradation. In practice, it may make sense to ignore benign shifts, under which the performance of a deployed model does not degrade substantially, making interventions by a human expert (or model retraining) unnecessary. While several works have developed tests for distribution shifts, these typically either use non-sequential methods, or detect arbitrary shifts (benign or harmful), or both. We argue that a sensible method for ﬁring oﬀ a warning has to both (a) detect harmful shifts while ignoring benign ones, and (b) allow continuous monitoring of model performance without increasing the false alarm rate. In this work, we design simple sequential tools for testing if the diﬀerence between source (training) and target (test) distributions leads to a signiﬁcant increase in a risk function of interest, like accuracy or calibration. Recent advances in constructing time-uniform conﬁdence sequences allow eﬃcient aggregation of statistical evidence accumulated during the tracking process. The designed framework is applicable in settings where (some) true labels are revealed after the prediction is performed, or when batches of labels become available in a delayed fashion. We demonstrate the eﬃcacy of the proposed framework through an extensive empirical study on a collection of simulated and real datasets",
    "volume": "main",
    "checked": true,
    "id": "2e51e475ca448e9234c9a4e49c5d9a6605591084",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=KmtVD97J43e": {
    "title": "Synchromesh: Reliable Code Generation from Pre-trained Language Models",
    "abstract": "Large pre-trained language models have been used to generate code, providing a ﬂexible interface for synthesizing programs from natural language speciﬁcations. However, they often violate syntactic and semantic rules of their output language, limiting their practical usability. In this paper, we propose S YNCHROMESH : a framework for substantially improving the reliability of pre-trained models for code generation. S YNCHROMESH comprises two components. First, it retrieves few-shot examples from a training bank using Target Similarity Tuning (TST), a novel method for semantic example selection. TST learns to recognize utterances that describe similar target programs despite differences in surface natural language features. Then, S YNCHROMESH feeds the examples to a pre-trained language model and samples programs using Constrained Semantic Decoding (CSD): a general framework for constraining the output to a set of valid programs in the target language. CSD leverages constraints on partial outputs to sample complete correct programs, and needs neither re-training nor ﬁne-tuning of the language model. We evaluate our methods by synthesizing code from natural language descriptions using GPT-3 and Codex in three real-world languages: SQL queries, Vega-Lite visualizations and SMCalFlow programs. These domains showcase rich constraints that CSD is able to enforce, including syntax, scope, typing rules, and contextual logic. We observe substantial complementary gains from CSD and TST in prediction accuracy and in effectively preventing run-time errors",
    "volume": "main",
    "checked": true,
    "id": "b62d63580b81a2cbb20c3c1593dd62d118e4cb07",
    "citation_count": 18
  },
  "https://openreview.net/forum?id=HTVch9AMPa": {
    "title": "Delaunay Component Analysis for Evaluation of Data Representations",
    "abstract": "Advanced representation learning techniques require reliable and general evaluation methods. Recently, several algorithms based on the common idea of geometric and topological analysis of a manifold approximated from the learned data representations have been proposed. In this work, we introduce Delaunay Component Analysis (DCA) – an evaluation algorithm which approximates the data manifold using a more suitable neighbourhood graph called Delaunay graph. This provides a reliable manifold estimation even for challenging geometric arrangements of representations such as clusters with varying shape and density as well as outliers, which is where existing methods often fail. Furthermore, we exploit the nature of Delaunay graphs and introduce a framework for assessing the quality of individual novel data representations. We experimentally validate the proposed DCA method on representations obtained from neural networks trained with contrastive objective, supervised and generative models, and demonstrate various use cases of our extended single point evaluation framework",
    "volume": "main",
    "checked": true,
    "id": "b24d290cea1cb2c34f83e374c28ec1cb84ef7c2e",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=H7HDG--DJF0": {
    "title": "Multi-Agent MDP Homomorphic Networks",
    "abstract": "This paper introduces Multi-Agent MDP Homomorphic Networks, a class of networks that allows distributed execution using only local information, yet is able to share experience between global symmetries in the joint state-action space of cooperative multi-agent systems. In cooperative multi-agent systems, complex symmetries arise between different conﬁgurations of the agents and their local observations. For example, consider a group of agents navigating: rotating the state globally results in a permutation of the optimal joint policy. Existing work on symmetries in single agent reinforcement learning can only be generalized to the fully centralized setting, because such approaches rely on the global symmetry in the full state-action spaces, and these can result in correspondences across agents. To encode such symmetries while still allowing distributed execution we propose a factorization that decomposes global symmetries into local transformations. Our proposed factorization allows for distributing the computation that enforces global symmetries over local agents and local interactions. We introduce a multi-agent equivariant policy network based on this factorization. We show empirically on symmetric multi-agent problems that globally symmetric distributable policies improve data efﬁciency compared to non-equivariant baselines",
    "volume": "main",
    "checked": true,
    "id": "9a03d9abfd504bc94686acb920103f802b0878da",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=8c50f-DoWAu": {
    "title": "Diffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling Scheme",
    "abstract": "Voice conversion is a common speech synthesis task which can be solved in different ways depending on a particular real-world scenario. The most challenging one often referred to as one-shot many-to-many voice conversion consists in copying target voice from only one reference utterance in the most general case when both source and target speakers do not belong to the training dataset. We present a scalable high-quality solution based on diffusion probabilistic modeling and demonstrate its superior quality compared to state-of-the-art one-shot voice conversion approaches. Moreover, focusing on real-time applications, we investigate general principles which can make diffusion models faster while keeping synthesis quality at a high level. As a result, we develop a novel Stochastic Differential Equations solver suitable for various diffusion model types and generative tasks as shown through empirical studies and justify it by theoretical analysis. The code is publicly available at https://github.com/huawei-noah/ Speech-Backbones/tree/main/DiffVC",
    "volume": "main",
    "checked": true,
    "id": "d49a230b7718bd82fd7816d9d78e3ebd49118d2a",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=RxplU3vmBx": {
    "title": "Looking Back on Learned Experiences For Class/task Incremental Learning",
    "abstract": "Classical deep neural networks are limited in their ability to learn from emerging streams of training data. When trained sequentially on new or evolving tasks, their performance degrades sharply, making them inappropriate in real-world use cases. Existing methods tackle it by either storing old data samples or only updating a parameter set of deep neural networks, which, however, demands a large memory budget or spoils the flexibility of models to learn the incremented task distribution. In this paper, we shed light on an on-call transfer set to provide past experiences whenever a new task arises in the data stream. In particular, we propose a CostFree Incremental Learning (CF-IL) not only to replay past experiences the model has learned but also to perform this in a cost free manner. Towards this end, we introduced a memory recovery paradigm in which we query the network to synthesize past exemplars whenever a new task emerges. Thus, our method needs no extra memory for data buffering or network growing, besides calls the proposed memory recovery paradigm to provide past exemplars, named a transfer set in order to mitigate catastrophically forgetting the former tasks in the Incremental Learning (IL) setup. Moreover, in contrast with recently proposed methods, the suggested paradigm does not desire a parallel architecture since it only relies on the learner network. Compared to the state-of-the-art data techniques without buffering past data samples, CF-IL demonstrates significantly better performance on the well-known datasets whether a task oracle is available in test time (Task-IL) or not (Class-IL)1",
    "volume": "main",
    "checked": true,
    "id": "95794ba6bc47f35077ab5b1bf3df0c86fd8a7e11",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=DfMqlB0PXjM": {
    "title": "Interpretable Unsupervised Diversity Denoising and Artefact Removal",
    "abstract": "Image denoising and artefact removal are complex inverse problems admitting multiple valid solutions. Unsupervised diversity restoration, that is, obtaining a diverse set of possible restorations given a corrupted image, is important for ambiguity removal in many applications such as microscopy where paired data for supervised training are often unobtainable. In real world applications, imaging noise and artefacts are typically hard to model, leading to unsatisfactory performance of existing unsupervised approaches. This work presents an interpretable approach for unsupervised and diverse image restoration. To this end, we introduce a capable architecture called HIERARCHICAL DIVNOISING (HDN) based on hierarchical Variational Autoencoder. We show that HDN learns an interpretable multi-scale representation of artefacts and we leverage this interpretability to remove imaging artefacts commonly occurring in microscopy data. Our method achieves stateof-the-art results on twelve benchmark image denoising datasets while providing access to a whole distribution of sensibly restored solutions. Additionally, we demonstrate on three real microscopy datasets that HDN removes artefacts without supervision, being the first method capable of doing so while generating multiple plausible restorations all consistent with the given corrupted image",
    "volume": "main",
    "checked": true,
    "id": "b28841064bd35ae71b7bf4fc117e87f0bd734aaa",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=R8sQPpGCv0": {
    "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
    "abstract": "Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.1",
    "volume": "main",
    "checked": true,
    "id": "9ca329408813d209b1dcb36936f7f9cba82506bd",
    "citation_count": 45
  },
  "https://openreview.net/forum?id=USIgIY6TNDe": {
    "title": "Graph-based Nearest Neighbor Search in Hyperbolic Spaces",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "ebc55b11825ce9b017946f0b503bcfd975ee9a93",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=12RoR2o32T": {
    "title": "Out-of-distribution Generalization in the Presence of Nuisance-Induced Spurious Correlations",
    "abstract": "In many prediction problems, spurious correlations are induced by a changing relationship between the label and a nuisance variable that is also correlated with the covariates. For example, in classifying animals in natural images, the background, which is a nuisance, can predict the type of animal. This nuisance-label relationship does not always hold, and the performance of a model trained under one such relationship may be poor on data with a different nuisance-label relationship. To build predictive models that perform well regardless of the nuisance-label relationship, we develop Nuisance-Randomized Distillation (NURD). We introduce the nuisance-randomized distribution, a distribution where the nuisance and the label are independent. Under this distribution, we define the set of representations such that conditioning on any member, the nuisance and the label remain independent. We prove that the representations in this set always perform better than chance, while representations outside of this set may not. NURD finds a representation from this set that is most informative of the label under the nuisance-randomized distribution, and we prove that this representation achieves the highest performance regardless of the nuisance-label relationship. We evaluate NURD on several tasks including chest X-ray classification where, using non-lung patches as the nuisance, NURD produces models that predict pneumonia under strong spurious correlations",
    "volume": "main",
    "checked": true,
    "id": "440c098ce8c0ff2042543d3e4188ebb95acdb75a",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=zIUyj55nXR": {
    "title": "Frame Averaging for Invariant and Equivariant Network Design",
    "abstract": "Many machine learning tasks involve learning functions that are known to be invariant or equivariant to certain symmetries of the input data. However, it is often challenging to design neural network architectures that respect these symmetries while being expressive and computationally efficient. For example, Euclidean motion invariant/equivariant graph or point cloud neural networks. We introduce Frame Averaging (FA), a general purpose and systematic framework for adapting known (backbone) architectures to become invariant or equivariant to new symmetry types. Our framework builds on the well known group averaging operator that guarantees invariance or equivariance but is intractable. In contrast, we observe that for many important classes of symmetries, this operator can be replaced with an averaging operator over a small subset of the group elements, called a frame. We show that averaging over a frame guarantees exact invariance or equivariance while often being much simpler to compute than averaging over the entire group. Furthermore, we prove that FA-based models have maximal expressive power in a broad setting and in general preserve the expressive power of their backbone architectures. Using frame averaging, we propose a new class of universal Graph Neural Networks (GNNs), universal Euclidean motion invariant point cloud networks, and Euclidean motion invariant Message Passing (MP) GNNs. We demonstrate the practical effectiveness of FA on several applications including point cloud normal estimation, beyond 2-WL graph separation, and n-body dynamics prediction, achieving state-of-the-art results in all of these benchmarks",
    "volume": "main",
    "checked": true,
    "id": "3513f49b1d476ca5f86cf51e3d640e1afc5cb13b",
    "citation_count": 16
  },
  "https://openreview.net/forum?id=wENMvIsxNN": {
    "title": "D-CODE: Discovering Closed-form ODEs from Observed Trajectories",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "fd05d3d6dbed753b0f4b081a986ba5d63d958833",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=VrjOFfcnSV8": {
    "title": "Entroformer: A Transformer-based Entropy Model for Learned Image Compression",
    "abstract": "One critical component in lossy deep image compression is the entropy model, which predicts the probability distribution of the quantized latent representation in the encoding and decoding modules. Previous works build entropy models upon convolutional neural networks which are inefficient in capturing global dependencies. In this work, we propose a novel transformer-based entropy model, termed Entroformer, to capture long-range dependencies in probability distribution estimation effectively and efficiently. Different from vision transformers in image classification, the Entroformer is highly optimized for image compression, including a top-k self-attention and a diamond relative position encoding. Meanwhile, we further expand this architecture with a parallel bidirectional context model to speed up the decoding process. The experiments show that the Entroformer achieves state-of-the-art performance on image compression while being time-efficient. Code is available at https://github.com/mx54039q/entroformer",
    "volume": "main",
    "checked": true,
    "id": "686e6558008c127849cb2f5197036db1591495df",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=5xEgrl_5FAJ": {
    "title": "BiBERT: Accurate Fully Binarized BERT",
    "abstract": "The large pre-trained BERT has achieved remarkable performance on Natural Language Processing (NLP) tasks but is also computation and memory expensive. As one of the powerful compression approaches, binarization extremely reduces the computation and memory consumption by utilizing 1-bit parameters and bitwise operations. Unfortunately, the full binarization of BERT (i.e., 1-bit weight, embedding, and activation) usually suffer a significant performance drop, and there is rare study addressing this problem. In this paper, with the theoretical justification and empirical analysis, we identify that the severe performance drop can be mainly attributed to the information degradation and optimization direction mismatch respectively in the forward and backward propagation, and propose BiBERT, an accurate fully binarized BERT, to eliminate the performance bottlenecks. Specifically, BiBERT introduces an efficient Bi-Attention structure for maximizing representation information statistically and a DirectionMatching Distillation (DMD) scheme to optimize the full binarized BERT accurately. Extensive experiments show that BiBERT outperforms both the straightforward baseline and existing state-of-the-art quantized BERTs with ultra-low bit activations by convincing margins on the NLP benchmark. As the first fully binarized BERT, our method yields impressive 56.3× and 31.2× saving on FLOPs and model size, demonstrating the vast advantages and potential of the fully binarized BERT model in real-world resource-constrained scenarios",
    "volume": "main",
    "checked": true,
    "id": "fb2307f7ce7c6868429ee3ee15d6eaf311ecba5c",
    "citation_count": 12
  },
  "https://openreview.net/forum?id=HCRVf71PMF": {
    "title": "LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5",
    "abstract": "Existing approaches to lifelong language learning rely on plenty of labeled data for learning a new task, which is hard to obtain in most real scenarios. Considering that humans can continually learn new tasks from a handful of examples, we expect the models also to be able to generalize well on new few-shot tasks without forgetting the previous ones. In this work, we deﬁne this more challenging yet practical problem as Lifelong Few-shot Language Learning (LFLL) and propose a uniﬁed framework for it based on prompt tuning (PT) of T5. Our framework called LFPT5 takes full advantage of PT's strong few-shot learning ability, and simultaneously trains the model as a task solver and a data generator. Before learning a new domain of the same task type, LFPT5 generates pseudo (labeled) samples of previously learned domains, and later gets trained on those samples to alleviate forgetting of previous knowledge as it learns the new domain. In addi-tion, a KL divergence loss is minimized to achieve label consistency between the previous and the current model. While adapting to a new task type, LFPT5 includes and tunes additional prompt embeddings for the new task. With extensive experiments, we demonstrate that LFPT5 can be applied to various different types of tasks and signiﬁcantly outperform previous methods in different LFLL settings",
    "volume": "main",
    "checked": true,
    "id": "e1c1349614377f77bb3cff93322e2f4a1ac513f4",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=Bl8CQrx2Up4": {
    "title": "cosFormer: Rethinking Softmax In Attention",
    "abstract": "Transformer has shown great successes in natural language processing, computer vision, and audio processing. As one of its core components, the softmax attention helps to capture long-range dependencies yet prohibits its scale-up due to the quadratic space and time complexity to the sequence length. Kernel methods are often adopted to reduce the complexity by approximating the softmax operator. Nevertheless, due to the approximation errors, their performances vary in different tasks/corpus and suffer crucial performance drops when compared with the vanilla softmax attention. In this paper, we propose a linear transformer called COSFORMER that can achieve comparable or better accuracy to the vanilla transformer in both casual and cross attentions. COSFORMER is based on two key properties of softmax attention: i). non-negativeness of the attention matrix; ii). a non-linear re-weighting scheme that can concentrate the distribution of the attention matrix. As its linear substitute, COSFORMER fulfills these properties with a linear operator and a cosine-based distance re-weighting mechanism. Extensive experiments on language modeling and text understanding tasks demonstrate the effectiveness of our method. We further examine our method on long sequences and achieve state-of-the-art performance on the Long-Range Arena benchmark. The source code is available at COSFORMER ",
    "volume": "main",
    "checked": true,
    "id": "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
    "citation_count": 36
  },
  "https://openreview.net/forum?id=2sDQwC_hmnM": {
    "title": "ZeroFL: Efficient On-Device Training for Federated Learning with Local Sparsity",
    "abstract": "When the available hardware cannot meet the memory and compute requirements to efficiently train high performing machine learning models, a compromise in either the training quality or the model complexity is needed. In Federated Learning (FL), nodes are orders of magnitude more constrained than traditional servergrade hardware and are often battery powered, severely limiting the sophistication of models that can be trained under this paradigm. While most research has focused on designing better aggregation strategies to improve convergence rates and in alleviating the communication costs of FL, fewer efforts have been devoted to accelerating on-device training. Such stage, which repeats hundreds of times (i.e. every round) and can involve thousands of devices, accounts for the majority of the time required to train federated models and, the totality of the energy consumption at the client side. In this work, we present the first study on the unique aspects that arise when introducing sparsity at training time in FL workloads. We then propose ZeroFL, a framework that relies on highly sparse operations to accelerate on-device training. Models trained with ZeroFL and 95% sparsity achieve up to 2.3% higher accuracy compared to competitive baselines obtained from adapting a state-of-the-art sparse training framework to the FL setting",
    "volume": "main",
    "checked": true,
    "id": "c9ac807a8760ae2e35dff0967ad8f24440fadb7b",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=6Tk2noBdvxt": {
    "title": "Programmatic Reinforcement Learning without Oracles",
    "abstract": "Deep reinforcement learning (RL) has led to encouraging successes in many challenging control tasks. However, a deep RL model lacks interpretability due to the difficulty of identifying how the model's control logic relates to its network structure. Programmatic policies structured in more interpretable representations emerge as a promising solution. Yet two shortcomings remain: First, synthesizing programmatic policies requires optimizing over the discrete and non-differentiable search space of program architectures. Previous works are suboptimal because they only enumerate program architectures greedily guided by a pretrained RL oracle. Second, these works do not exploit compositionality, an important programming concept, to reuse and compose primitive functions to form a complex function for new tasks. Our first contribution is a programmatically interpretable RL framework that conducts program architecture search on top of a continuous relaxation of the architecture space defined by programming language grammar rules. Our algorithm allows policy architectures to be learned with policy parameters via bilevel optimization using efficient policy-gradient methods, and thus does not require a pretrained oracle. Our second contribution is improving programmatic policies to support compositionality by integrating primitive functions learned to grasp task-agnostic skills as a composite program to solve novel RL problems. Experiment results demonstrate that our algorithm excels in discovering optimal programmatic policies that are highly interpretable",
    "volume": "main",
    "checked": true,
    "id": "77b2b707bc416293053d4244d57a035b78444e80",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=YWNAX0caEjI": {
    "title": "Neural Structured Prediction for Inductive Node Classification",
    "abstract": "This paper studies node classiﬁcation in the inductive setting, i.e., aiming to learn a model on labeled training graphs and generalize it to infer node labels on unlabeled test graphs. This problem has been extensively studied with graph neural networks (GNNs) by learning effective node representations, as well as traditional structured prediction methods for modeling the structured output of node labels, e.g., conditional random ﬁelds (CRFs). In this paper, we present a new approach called the Structured Proxy Network (SPN), which combines the advantages of both worlds. SPN deﬁnes ﬂexible potential functions of CRFs with GNNs. However, learning such a model is nontrivial as it involves optimizing a maximin game with high-cost inference. Inspired by the underlying connection between joint and marginal distributions deﬁned by Markov networks, we propose to solve an approximate version of the optimization problem as a proxy, which yields a near-optimal solution, making learning more efﬁcient. Extensive experiments on two settings show that our approach outperforms many competitive baselines 1 ",
    "volume": "main",
    "checked": true,
    "id": "8ccf555540a8593ed6c0760586c91b7624346d02",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=Azh9QBQ4tR7": {
    "title": "Reducing Excessive Margin to Achieve a Better Accuracy vs. Robustness Trade-off",
    "abstract": "While adversarial training has become the de facto approach for training robust classifiers, it leads to a drop in accuracy. This has led to prior works postulating that accuracy is inherently at odds with robustness. Yet, the phenomenon remains inexplicable. In this paper, we closely examine the changes induced in the decision boundary of a deep network during adversarial training. We find that adversarial training leads to unwarranted increase in the margin along certain adversarial directions, thereby hurting accuracy. Motivated by this observation, we present a novel algorithm, called Helper-based Adversarial Training (HAT), to reduce this effect by incorporating additional wrongly labelled examples during training. Our proposed method provides a notable improvement in accuracy without compromising robustness. It achieves a better trade-off between accuracy and robustness in comparison to existing defenses",
    "volume": "main",
    "checked": true,
    "id": "ca1a8b92898c4e57aebcbd6c6f57e1cb9a4d0804",
    "citation_count": 29
  },
  "https://openreview.net/forum?id=B5XahNLmna": {
    "title": "Data Poisoning Won't Save You From Facial Recognition",
    "abstract": "Data poisoning has been proposed as a compelling defense against facial recognition models trained on Web-scraped pictures. By perturbing the images they post online, users can fool models into misclassifying future (unperturbed) pictures. We demonstrate that this strategy provides a false sense of security, as it ignores an inherent asymmetry between the parties: users' pictures are perturbed once and for all before being published and scraped, and must thereafter fool all future models—including models trained adaptively against the users' past attacks, or models that use technologies discovered after the attack. We evaluate two poisoning attacks against largescale facial recognition, Fawkes (500,000+ downloads) and LowKey. We demonstrate how an \"oblivious\" model trainer can simply wait for future developments in computer vision to nullify the protection of pictures collected in the past. We further show that an adversary with black-box access to the attack can train a robust model that resists the perturbations of collected pictures. We caution that facial recognition poisoning will not admit an \"arms race\" between attackers and defenders. Once perturbed pictures are scraped, the attack cannot be changed so any future defense irrevocably undermines users' privacy",
    "volume": "main",
    "checked": true,
    "id": "9d8a948634204fedef929f1e0a24eb0cfc3685eb",
    "citation_count": 13
  },
  "https://openreview.net/forum?id=zzk231Ms1Ih": {
    "title": "A Theory of Tournament Representations",
    "abstract": "Real world tournaments are almost always intransitive. Recent works have noted that parametric models which assume d dimensional node representations can effectively model intransitive tournaments. However, nothing is known about the structure of the class of tournaments that arise out of any fixed d dimensional representations. In this work, we develop a novel theory for understanding parametric tournament representations. Our first contribution is to structurally characterize the class of tournaments that arise out of d dimensional representations. We do this by showing that these tournament classes have forbidden configurations which must necessarily be union of flip classes, a novel way to partition the set of all tournaments. We further characterize rank 2 tournaments completely by showing that the associated forbidden flip class contains just 2 tournaments. Specifically, we show that the rank 2 tournaments are equivalent to locally-transitive tournaments. This insight allows us to show that the minimum feedback arc set problem on this tournament class can be solved using the standard Quicksort procedure. For a general rank d tournament class, we show that the flip class associated with a coned-doubly regular tournament of size O( √ d) must be a forbidden configuration. To answer a dual question, using a celebrated result of [10], we show a lower bound of O( √ n) on the minimum dimension needed to represent all tournaments on n nodes. For any given tournament, we show a novel upper bound on the smallest representation dimension that depends on the least size of the number of unique nodes in any feedback arc set of the flip class associated with a tournament. We show how our results also shed light on upper bound of sign-rank of matrices",
    "volume": "main",
    "checked": true,
    "id": "c8008a051073285a8ee8463c1eaf69c5f2d801a3",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=YiBa9HKTyXE": {
    "title": "Permutation-Based SGD: Is Random Optimal?",
    "abstract": "A recent line of ground-breaking results for permutation-based SGD has corroborated a widely observed phenomenon: random permutations offer faster convergence than with-replacement sampling. However, is random optimal? We show that this depends heavily on what functions we are optimizing, and the convergence gap between optimal and random permutations can vary from exponential to nonexistent. We first show that for 1-dimensional strongly convex functions, with smooth second derivatives, there exist optimal permutations that offer exponentially faster convergence compared to random. However, for general strongly convex functions, random permutations are optimal. Finally, we show that for quadratic, strongly-convex functions, there are easy-to-construct permutations that lead to accelerated convergence compared to random. Our results suggest that a general convergence characterization of optimal permutations cannot capture the nuances of individual function classes, and can mistakenly indicate that one cannot do much better than random",
    "volume": "main",
    "checked": true,
    "id": "1b7af5e0ef5ce854c5d56e9953f026bbdc371327",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=DTkEfj0Ygb8": {
    "title": "Learning meta-features for AutoML",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "f20b74b6246c0edbd865e1ebeb328fea91f6361d",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=DBiQQYWykyy": {
    "title": "Environment Predictive Coding for Visual Navigation",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "905043837109bc9b6437aef683d72de07e169fae",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=GhVS8_yPeEa": {
    "title": "Effect of scale on catastrophic forgetting in neural networks",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "b584309dccafe64a7d6b96f064554330cbe1bbd3",
    "citation_count": 17
  },
  "https://openreview.net/forum?id=WfvgGBcgbE7": {
    "title": "Model Zoo: A Growing Brain That Learns Continually",
    "abstract": "This paper argues that continual learning methods can beneﬁt by splitting the capacity of the learner across multiple models. We use statistical learning theory and experimental analysis to show how multiple tasks can interact with each other in a non-trivial fashion when a single model is trained on them. The generalization error on a particular task can improve when it is trained with synergistic tasks, but can also deteriorate when trained with competing tasks. This theory motivates our method named Model Zoo which, inspired from the boosting literature, grows an ensemble of small models, each of which is trained during one episode of continual learning. We demonstrate that Model Zoo obtains large gains in accuracy on a variety of continual learning benchmark problems. Code is available at https://github.com/grasp-lyrl/modelzoo_continual",
    "volume": "main",
    "checked": true,
    "id": "ff3d64942ee32c12816cd0d60d338d858edbc99f",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=kR1hC6j48Tp": {
    "title": "GATSBI: Generative Adversarial Training for Simulation-Based Inference",
    "abstract": "Simulation-based inference (SBI) refers to statistical inference on stochastic models for which we can generate samples, but not compute likelihoods. Like SBI algorithms, generative adversarial networks (GANs) do not require explicit likelihoods. We study the relationship between SBI and GANs, and introduce GATSBI, an adversarial approach to SBI. GATSBI reformulates the variational objective in an adversarial setting to learn implicit posterior distributions. Inference with GATSBI is amortised across observations, works in high-dimensional posterior spaces and supports implicit priors. We evaluate GATSBI on two SBI benchmark problems and on two high-dimensional simulators. On a model for wave propagation on the surface of a shallow water body, we show that GATSBI can return well-calibrated posterior estimates even in high dimensions. On a model of camera optics, it infers a high-dimensional posterior given an implicit prior, and performs better than a state-of-the-art SBI approach. We also show how GATSBI can be extended to perform sequential posterior estimation to focus on individual observations. Overall, GATSBI opens up opportunities for leveraging advances in GANs to perform Bayesian inference on high-dimensional simulation-based models",
    "volume": "main",
    "checked": true,
    "id": "9557af7426fde227e1cbd623dd4d221122265863",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=FndDxSz3LxQ": {
    "title": "Learn Locally, Correct Globally: A Distributed Algorithm for Training Graph Neural Networks",
    "abstract": "Despite the recent success of Graph Neural Networks (GNNs), training GNNs on large graphs remains challenging. The limited resource capacities of the existing servers, the dependency between nodes in a graph, and the privacy concern due to the centralized storage and model learning have spurred the need to design an effective distributed algorithm for GNN training. However, existing distributed GNN training methods impose either excessive communication costs or large memory overheads that hinders their scalability. To overcome these issues, we propose a communication-efficient distributed GNN training technique named Learn Locally, Correct Globally (LLCG). To reduce the communication and memory overhead, each local machine in LLCG first trains a GNN on its local data by ignoring the dependency between nodes among different machines, then sends the locally trained model to the server for periodic model averaging. However, ignoring node dependency could result in significant performance degradation. To solve the performance degradation, we propose to apply Global Server Corrections on the server to refine the locally learned models. We rigorously analyze the convergence of distributed methods with periodic model averaging for training GNNs and show that naively applying periodic model averaging but ignoring the dependency between nodes will suffer from an irreducible residual error. However, this residual error can be eliminated by utilizing the proposed global corrections to entail fast convergence rate. Extensive experiments on real-world datasets show that LLCG can significantly improve the efficiency without hurting the performance",
    "volume": "main",
    "checked": true,
    "id": "e38b78b4a71451a143e10a252d3fd0de70ef47c3",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=t5s-hd1bqLk": {
    "title": "Conditioning Sequence-to-sequence Networks with Learned Activations",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "36c2b7f37554bc508b7d0a0bebb07b240c56169b",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=-ApAkox5mp": {
    "title": "SHINE: SHaring the INverse Estimate from the forward pass for bi-level optimization and implicit models",
    "abstract": "In recent years, implicit deep learning has emerged as a method to increase the depth of deep neural networks. While their training is memory-efficient, they are still significantly slower to train than their explicit counterparts. In Deep Equilibrium Models (DEQs), the training is performed as a bi-level problem, and its computational complexity is partially driven by the iterative inversion of a huge Jacobian matrix. In this paper, we propose a novel strategy to tackle this computational bottleneck from which many bi-level problems suffer. The main idea is to use the quasi-Newton matrices from the forward pass to efficiently approximate the inverse Jacobian matrix in the direction needed for the gradient computation. We provide a theorem that motivates using our method with the original forward algorithms. In addition, by modifying these forward algorithms, we further provide theoretical guarantees that our method asymptotically estimates the true implicit gradient. We empirically study this approach in many settings, ranging from hyperparameter optimization to large Multiscale DEQs applied to CIFAR and ImageNet. We show that it reduces the computational cost of the backward pass by up to two orders of magnitude. All this is achieved while retaining the excellent performance of the original models in hyperparameter optimization and on CIFAR, and giving encouraging and competitive results on ImageNet",
    "volume": "main",
    "checked": true,
    "id": "c6f171ad41178c5d0ab3b23261e4cd7f0c63a62e",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=Vog_3GXsgmb": {
    "title": "Discovering Nonlinear PDEs from Scarce Data with Physics-encoded Learning",
    "abstract": "There have been growing interests in leveraging experimental measurements to discover the underlying partial differential equations (PDEs) that govern complex physical phenomena. Although past research attempts have achieved great success in data-driven PDE discovery, the robustness of the existing methods cannot be guaranteed when dealing with low-quality measurement data. To overcome this challenge, we propose a novel physics-encoded discrete learning framework for discovering spatiotemporal PDEs from scarce and noisy data. The general idea is to (1) firstly introduce a novel deep convolutional-recurrent network, which can encode prior physics knowledge (e.g., known PDE terms, assumed PDE structure, initial/boundary conditions, etc.) while remaining flexible on representation capability, to accurately reconstruct high-fidelity data, and (2) perform sparse regression with the reconstructed data to identify the explicit form of the governing PDEs. We validate our method on three nonlinear PDE systems. The effectiveness and superiority of the proposed method over baseline models are demonstrated",
    "volume": "main",
    "checked": true,
    "id": "ef06ed8154aba813f6b7520aec10085511999032",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=qTHBE7E9iej": {
    "title": "Learning transferable motor skills with hierarchical latent mixture policies",
    "abstract": "For robots operating in the real world, it is desirable to learn reusable behaviours that can effectively be transferred and adapted to numerous tasks and scenarios. We propose an approach to learn abstract motor skills from data using a hierarchical mixture latent variable model. In contrast to existing work, our method exploits a three-level hierarchy of both discrete and continuous latent variables, to capture a set of high-level behaviours while allowing for variance in how they are executed. We demonstrate in manipulation domains that the method can effectively cluster ofﬂine data into distinct, executable behaviours, while retaining the ﬂexibility of a continuous latent variable model. The resulting skills can be transferred and ﬁne-tuned on new tasks, unseen objects, and from state to vision-based policies, yielding better sample efﬁciency and asymptotic performance compared to existing skill- and imitation-based methods. We further analyse how and when the skills are most beneﬁcial: they encourage directed exploration to cover large regions of the state space relevant to the task, making them most effective in challenging sparse-reward settings. We an approach to learn a three-level skill hierarchy from an ofﬂine dataset, capturing both discrete and continuous variations at multiple levels of behavioural abstraction. The model comprises a low-level latent-conditioned controller that can learn motor primitives, a set of continuous latent mid-level skills, and a discrete high-level controller that can compose and select among these abstract mid-level behaviours. Since the mid- and high-level form a mixture, we call our method Hierarchical Latent Mixtures of Skills (HeLMS). We demonstrate on challenging object manipulation tasks that our method can decompose a dataset into distinct, intuitive, and reusable behaviours. We show that these skills lead to improved sample efﬁciency and performance in numerous transfer scenarios: reusing skills for new tasks, generalising across unseen objects, and transferring from state to vision-based policies. Further analysis and ablations reveal that both continuous and discrete components are beneﬁcial, and that the learned hierarchical skills are most useful in sparse-reward settings, as they encourage directed exploration of task-relevant parts of the state space",
    "volume": "main",
    "checked": true,
    "id": "c85662dcd17eed4452019b640a30a323970472ef",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=pWBNOgdeURp": {
    "title": "An Operator Theoretic View On Pruning Deep Neural Networks",
    "abstract": "The discovery of sparse subnetworks that are able to perform as well as full models has found broad applied and theoretical interest. While many pruning methods have been developed to this end, the naı̈ve approach of removing parameters based on their magnitude has been found to be as robust as more complex, state-of-theart algorithms. The lack of theory behind magnitude pruning's success, especially pre-convergence, and its relation to other pruning methods, such as gradient based pruning, are outstanding open questions in the field that are in need of being addressed. We make use of recent advances in dynamical systems theory, namely Koopman operator theory, to define a new class of theoretically motivated pruning algorithms. We show that these algorithms can be equivalent to magnitude and gradient based pruning, unifying these seemingly disparate methods, and find that they can be used to shed light on magnitude pruning's performance during the early part of training",
    "volume": "main",
    "checked": true,
    "id": "6eb0518b1743487d351854825edd5b21c9ed11a3",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=nxcABL7jbQh": {
    "title": "Zero Pixel Directional Boundary by Vector Transform",
    "abstract": "Boundaries are among the primary visual cues used by human and computer vision systems. One of the key problems in boundary detection is the label representation, which typically leads to class imbalance and, as a consequence, to thick boundaries that require non-differential post-processing steps to be thinned. In this paper, we re-interpret boundaries as 1-D surfaces and formulate a one-to-one vector transform function that allows for training of boundary prediction completely avoiding the class imbalance issue. Speciﬁcally, we deﬁne the boundary representation at any point as the unit vector pointing to the closest boundary surface. Our problem formulation leads to the estimation of direction as well as richer contex-tual information of the boundary, and, if desired, the availability of zero-pixel thin boundaries also at training time. Our method uses no hyper-parameter in the training loss and a ﬁxed stable hyper-parameter at inference. We provide theoretical justiﬁcation/discussions of the vector transform representation. We evaluate the proposed loss method using a standard architecture and show the excellent performance over other losses and representations on several datasets. Code is available at https://github.com/edomel/BoundaryVT",
    "volume": "main",
    "checked": true,
    "id": "40b6dc974aa5b4b1a02e2fbb1b6dc681e0890ae6",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=lpkGn3k2YdD": {
    "title": "Learning Long-Term Reward Redistribution via Randomized Return Decomposition",
    "abstract": "randomized return decomposition (RRD), which sets up a surrogate optimization problem of the least-squares-based return decomposition. The proposed surrogate objective allows us to conduct return decomposition on short subsequences of agent trajectories, which is scalable in long-horizon tasks. We provide analyses to characterize the algorithmic property of our surrogate objective function and discuss connections to existing methods",
    "volume": "main",
    "checked": true,
    "id": "80110db20124242316a13795e964d09cd15a3802",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=Iog0djAdbHj": {
    "title": "Better Supervisory Signals by Observing Learning Paths",
    "abstract": "Better-supervised models might have better performance. In this paper, we first clarify what makes for good supervision for a classification problem, and then explain two existing label refining methods, label smoothing and knowledge distillation, in terms of our proposed criterion. To further answer why and how better supervision emerges, we observe the learning path, i.e., the trajectory of the model's predictions during training, for each training sample. We find that the model can spontaneously refine \"bad\" labels through a \"zig-zag\" learning path, which occurs on both toy and real datasets. Observing the learning path not only provides a new perspective for understanding knowledge distillation, overfitting, and learning dynamics, but also reveals that the supervisory signal of a teacher network can be very unstable near the best points in training on real tasks. Inspired by this, we propose a new knowledge distillation scheme, Filter-KD, which improves downstream classification performance in various settings",
    "volume": "main",
    "checked": true,
    "id": "91db2f1dae977f2cf139ce194173487133b1ad67",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=CmsfC7u054S": {
    "title": "Reinforcement Learning in Presence of Discrete Markovian Context Evolution",
    "abstract": "We consider a context-dependent Reinforcement Learning (RL) setting, which is characterized by: a) an unknown finite number of not directly observable contexts; b) abrupt (discontinuous) context changes occurring during an episode; and c) Markovian context evolution. We argue that this challenging case is often met in applications and we tackle it using a Bayesian approach and variational inference. We adapt a sticky Hierarchical Dirichlet Process (HDP) prior for model learning, which is arguably best-suited for Markov process modeling. We then derive a context distillation procedure, which identifies and removes spurious contexts in an unsupervised fashion. We argue that the combination of these two components allows to infer the number of contexts from data thus dealing with the context cardinality assumption. We then find the representation of the optimal policy enabling efficient policy learning using off-the-shelf RL algorithms. Finally, we demonstrate empirically (using gym environments cart-pole swing-up, drone, intersection) that our approach succeeds where state-of-the-art methods of other frameworks fail and elaborate on the reasons for such failures",
    "volume": "main",
    "checked": true,
    "id": "69d042aaaf2fd7b15888aaf98009e184b54c32f8",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=j-63FSNcO5a": {
    "title": "Learning Disentangled Representation by Exploiting Pretrained Generative Models: A Contrastive Learning View",
    "abstract": "From the intuitive notion of disentanglement, the image variations corresponding to different factors should be distinct from each other, and the disentangled representation should reflect those variations with separate dimensions. To discover the factors and learn disentangled representation, previous methods typically leverage an extra regularization term when learning to generate realistic images. However, the term usually results in a trade-off between disentanglement and generation quality. For the generative models pretrained without any disentanglement term, the generated images show semantically meaningful variations when traversing along different directions in the latent space. Based on this observation, we argue that it is possible to mitigate the trade-off by (i) leveraging the pretrained generative models with high generation quality, (ii) focusing on discovering the traversal directions as factors for disentangled representation learning. To achieve this, we propose Disentaglement via Contrast (DisCo) as a framework to model the variations based on the target disentangled representations, and contrast the variations to jointly discover disentangled directions and learn disentangled representations. DisCo achieves the state-of-the-art disentangled representation learning and distinct direction discovering, given pretrained nondisentangled generative models including GAN, VAE, and Flow. Source code is at https://github.com/xrenaa/DisCo",
    "volume": "main",
    "checked": true,
    "id": "77064160e80b260d044c91b44134df4de08e4c38",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=YJ1WzgMVsMt": {
    "title": "Reinforcement Learning with Sparse Rewards using Guidance from Offline Demonstration",
    "abstract": "A major challenge in real-world reinforcement learning (RL) is the sparsity of reward feedback. Often, what is available is an intuitive but sparse reward function that only indicates whether the task is completed partially or fully. However, the lack of carefully designed, fine grain feedback implies that most existing RL algorithms fail to learn an acceptable policy in a reasonable time frame. This is because of the large number of exploration actions that the policy has to perform before it gets any useful feedback that it can learn from. In this work, we address this challenging problem by developing an algorithm that exploits the offline demonstration data generated by a sub-optimal behavior policy for faster and efficient online RL in such sparse reward settings. The proposed algorithm, which we call the Learning Online with Guidance Offline (LOGO) algorithm, merges a policy improvement step with an additional policy guidance step by using the offline demonstration data. The key idea is that by obtaining guidance from not imitating the offline data, LOGO orients its policy in the manner of the sub-optimal policy, while yet being able to learn beyond and approach optimality. We provide a theoretical analysis of our algorithm, and provide a lower bound on the performance improvement in each learning episode. We also extend our algorithm to the even more challenging incomplete observation setting, where the demonstration data contains only a censored version of the true state observation. We demonstrate the superior performance of our algorithm over state-of-the-art approaches on a number of benchmark environments with sparse rewards and censored state. Further, we demonstrate the value of our approach via implementing LOGO on a mobile robot for trajectory tracking and obstacle avoidance, where it shows excellent performance",
    "volume": "main",
    "checked": true,
    "id": "00438218d81c2d50fc96592e16c07ae720440bb6",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=M752z9FKJP": {
    "title": "Learning Strides in Convolutional Neural Networks",
    "abstract": "Convolutional neural networks typically contain several downsampling operators, such as strided convolutions or pooling layers, that progressively reduce the resolution of intermediate representations. This provides some shift-invariance while reducing the computational complexity of the whole architecture. A critical hyperparameter of such layers is their stride: the integer factor of downsampling. As strides are not differentiable, finding the best configuration either requires crossvalidation or discrete optimization (e.g. architecture search), which rapidly become prohibitive as the search space grows exponentially with the number of downsampling layers. Hence, exploring this search space by gradient descent would allow finding better configurations at a lower computational cost. This work introduces DiffStride, the first downsampling layer with learnable strides. Our layer learns the size of a cropping mask in the Fourier domain, that effectively performs resizing in a differentiable way. Experiments on audio and image classification show the generality and effectiveness of our solution: we use DiffStride as a drop-in replacement to standard downsampling layers and outperform them. In particular, we show that introducing our layer into a ResNet-18 architecture allows keeping consistent high performance on CIFAR10, CIFAR100 and ImageNet even when training starts from poor random stride configurations. Moreover, formulating strides as learnable variables allows us to introduce a regularization term that controls the computational complexity of the architecture. We show how this regularization allows trading off accuracy for efficiency on ImageNet",
    "volume": "main",
    "checked": true,
    "id": "6de8a22a3c3a6ad3727aaa02c2e7816c7f716207",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=kAa9eDS0RdO": {
    "title": "Attention-based Interpretability with Concept Transformers",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "2f47a4c37c01d3ad4e6c4b074ff61468f1e976b8",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=5Qkd7-bZfI": {
    "title": "On the role of population heterogeneity in emergent communication",
    "abstract": "we We explore After we challenge the assumption We then investigate how structure through the analysis a potential diversity factor: learning speed. From then, we leverage this observation to control heterogeneity introducing factors. We show that",
    "volume": "main",
    "checked": true,
    "id": "61cbee9df90ede1fae2c7fb9af30cb855a50c1c6",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=p0rCmDEN_-": {
    "title": "Visual hyperacuity with moving sensor and recurrent neural computations",
    "abstract": "Dynamical phenomena, such as recurrent neuronal activity and perpetual motion of the eye, are typically overlooked in models of bottom-up visual perception. Recent experiments suggest that a tiny inter-saccadic eye motion (\"ﬁxational drift\") enhances visual acuity beyond the limit imposed by the density of retinal photoreceptors. Here we hypothesize that such an enhancement is enabled by recurrent neuronal computations in early visual areas. Speciﬁcally, we explore a setting involving a low-resolution dynamical sensor that moves with respect to a static scene, with drift-like tiny steps. This setting mimics a dynamical eye, viewing objects in perceptually-challenging conditions. The dynamical sensory input is classiﬁed by a convolutional neural network with recurrent connectivity added to its lower layers, in analogy to recurrent connectivity in early visual areas. Applying our system to CiFAR-10 and CiFAR-100 datasets down-sampled via 8x8 sensor, we found that (i) classiﬁcation accuracy, which is drastically reduced by this down-sampling, is mostly restored to its 32x32 baseline level when using a moving sensor and recurrent connectivity, (ii) in this setting, neurons in the early layers exhibit a wide repertoire of selectivity patterns, spanning the spatio-temporal selectivity space, with neurons preferring different combinations of spatial and temporal patterning, and (iii) curved sensor's trajectories improve visual acuity compared to straight trajectories, echoing recent experimental ﬁndings involving eye-tracking in challenging conditions. Our work sheds light on the possible role of recurrent connectivity in early vision as",
    "volume": "main",
    "checked": true,
    "id": "1bf45f35f6f32822d20c22715b1b9543d74b2c3f",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=oapKSVM2bcj": {
    "title": "Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "8a85ef6a7ebcd8735b868bf9c4a77e6a3c195caa",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=RRGVCN8kjim": {
    "title": "Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity",
    "abstract": "DETR is the first end-to-end object detector using a transformer encoder-decoder architecture and demonstrates competitive performance but low computational efficiency on high resolution feature maps. The subsequent work, Deformable DETR, enhances the efficiency of DETR by replacing dense attention with deformable attention, which achieves 10× faster convergence and improved performance. Deformable DETR uses the multiscale feature to ameliorate performance, however, the number of encoder tokens increases by 20× compared to DETR, and the computation cost of the encoder attention remains a bottleneck. In our preliminary experiment, we observe that the detection performance hardly deteriorates even if only a part of the encoder token is updated. Inspired by this observation, we propose Sparse DETR that selectively updates only the tokens expected to be referenced by the decoder, thus help the model effectively detect objects. In addition, we show that applying an auxiliary detection loss on the selected tokens in the encoder improves the performance while minimizing computational overhead. We validate that Sparse DETR achieves better performance than Deformable DETR even with only 10% encoder tokens on the COCO dataset. Albeit only the encoder tokens are sparsified, the total computation cost decreases by 38% and the frames per second (FPS) increases by 42% compared to Deformable DETR. Code is available at https://github.com/kakaobrain/sparse-detr",
    "volume": "main",
    "checked": true,
    "id": "e0d272e01929024f28f0f7cacf26177cd60b3ee7",
    "citation_count": 14
  },
  "https://openreview.net/forum?id=3jooF27-0Wy": {
    "title": "FlexConv: Continuous Kernel Convolutions With Differentiable Kernel Sizes",
    "abstract": "When designing Convolutional Neural Networks (CNNs), one must select the size of the convolutional kernels before training. Recent works show CNNs benefit from different kernel sizes at different layers, but exploring all possible combinations is unfeasible in practice. A more efficient approach is to learn the kernel size during training. However, existing works that learn the kernel size have a limited bandwidth. These approaches scale kernels by dilation, and thus the detail they can describe is limited. In this work, we propose FlexConv, a novel convolutional operation with which high bandwidth convolutional kernels of learnable kernel size can be learned at a fixed parameter cost. FlexNets model long-term dependencies without the use of pooling, achieve state-of-the-art performance on several sequential datasets, outperform recent works with learned kernel sizes, and are competitive with much deeper ResNets on image benchmark datasets. Additionally, FlexNets can be deployed at higher resolutions than those seen during training. To avoid aliasing, we propose a novel kernel parameterization with which the frequency of the kernels can be analytically controlled. Our novel kernel parameterization shows higher descriptive power and faster convergence speed than existing parameterizations. This leads to important improvements in classification accuracy",
    "volume": "main",
    "checked": true,
    "id": "41a4ec2c2ac022d2d7791e717927e406c5bba4b4",
    "citation_count": 18
  },
  "https://openreview.net/forum?id=8FhxBtXSl0": {
    "title": "CKConv: Continuous Kernel Convolution For Sequential Data",
    "abstract": "Conventional neural architectures for sequential data present important limitations. Recurrent networks suffer from exploding and vanishing gradients, small effective memory horizons, and must be trained sequentially. Convolutional networks are unable to handle sequences of unknown size and their memory horizon must be defined a priori. In this work, we show that all these problems can be solved by formulating convolutional kernels in CNNs as continuous functions. The resulting Continuous Kernel Convolution (CKConv) allows us to model arbitrarily long sequences in a parallel manner, within a single operation, and without relying on any form of recurrence. We show that Continuous Kernel Convolutional Networks (CKCNNs) obtain state-of-the-art results in multiple datasets, e.g., permuted MNIST, and, thanks to their continuous nature, are able to handle non-uniformly sampled datasets and irregularlysampled data natively. CKCNNs match or perform better than neural ODEs designed for these purposes in a much faster and simpler manner",
    "volume": "main",
    "checked": true,
    "id": "1d5c8c6e5a774d2fef8d92bd28670a6345a97f7a",
    "citation_count": 24
  },
  "https://openreview.net/forum?id=6IYp-35L-xJ": {
    "title": "CADDA: Class-wise Automatic Differentiable Data Augmentation for EEG Signals",
    "abstract": "Data augmentation is a key element of deep learning pipelines, as it informs the network during training about transformations of the input data that keep the label unchanged. Manually finding adequate augmentation methods and parameters for a given pipeline is however rapidly cumbersome. In particular, while intuition can guide this decision for images, the design and choice of augmentation policies remains unclear for more complex types of data, such as neuroscience signals. Moreover, label independent strategies might not be suitable for such structured data and class-dependent augmentations might be necessary. This idea has been surprisingly unexplored in the literature, while it is quite intuitive: changing the color of a car image does not change the object class to be predicted, but doing the same to the picture of an orange does. This paper aims to increase the generalization power added through class-wise data augmentation. Yet, as seeking transformations depending on the class largely increases the complexity of the task, using gradient-free optimization techniques as done by most existing automatic approaches becomes intractable for real-world datasets. For this reason we propose to use differentiable data augmentation amenable to gradient-based learning. EEG signals are a perfect example of data for which good augmentation policies are mostly unknown. In this work, we demonstrate the relevance of our approach on the clinically relevant sleep staging classification task, for which we also propose differentiable transformations",
    "volume": "main",
    "checked": true,
    "id": "80ad37dc3147702839a4415ca79855215350b1ac",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=neqU3HWDgE": {
    "title": "Unsupervised Disentanglement with Tensor Product Representations on the Torus",
    "abstract": "The current methods for learning representations with auto-encoders almost exclusively employ vectors as the latent representations. In this work, we propose to employ a tensor product structure for this purpose. This way, the obtained representations are naturally disentangled. In contrast to the conventional variations methods, which are targeted toward normally distributed features, the latent space in our representation is distributed uniformly over a set of unit circles. We argue that the torus structure of the latent space captures the generative factors effectively. We employ recent tools for measuring unsupervised disentanglement, and in an extensive set of experiments demonstrate the advantage of our method in terms of disentanglement, completeness, and informativeness. The code for our proposed method is available at https://github.com/rotmanmi/ Unsupervised-Disentanglement-Torus",
    "volume": "main",
    "checked": true,
    "id": "b3171abaf8ddca2cfa7a3d9e99a09bf8e255697f",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=CgIEctmcXx1": {
    "title": "ADAVI: Automatic Dual Amortized Variational Inference Applied To Pyramidal Bayesian Models",
    "abstract": "Frequently, population studies feature pyramidally-organized data represented using Hierarchical Bayesian Models (HBM) enriched with plates. These models can become prohibitively large in settings such as neuroimaging, where a sample is composed of a functional MRI signal measured on 300 brain locations, across 4 measurement sessions, and 30 subjects, resulting in around 1 million latent parameters. Such high dimensionality hampers the usage of modern, expressive flowbased techniques. To infer parameter posterior distributions in this challenging class of problems, we designed a novel methodology that automatically produces a variational family dual to a target HBM. This variational family, represented as a neural network, consists in the combination of an attention-based hierarchical encoder feeding summary statistics to a set of normalizing flows. Our automaticallyderived neural network exploits exchangeability in the plate-enriched HBM and factorizes its parameter space. The resulting architecture reduces by orders of magnitude its parameterization with respect to that of a typical flow-based representation, while maintaining expressivity. Our method performs inference on the specified HBM in an amortized setup: once trained, it can readily be applied to a new data sample to compute the parameters' full posterior. We demonstrate the capability and scalability of our method on simulated data, as well as a challenging high-dimensional brain parcellation experiment. We also open up several questions that lie at the intersection between normalizing flows, SBI, structured Variational Inference, and inference amortization",
    "volume": "main",
    "checked": true,
    "id": "168996ed9f03045c5dfc5d669041e0a2e7622b12",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=5JdLZg346Lw": {
    "title": "Generative Modeling with Optimal Transport Maps",
    "abstract": "With the discovery of Wasserstein GANs, Optimal Transport (OT) has become a powerful tool for large-scale generative modeling tasks. In these tasks, OT cost is typically used as the loss for training GANs. In contrast to this approach, we show that the OT map itself can be used as a generative model, providing comparable performance. Previous analogous approaches consider OT maps as generative models only in the latent spaces due to their poor performance in the original high-dimensional ambient space. In contrast, we apply OT maps directly in the ambient space, e.g., a space of high-dimensional images. First, we derive a minmax optimization algorithm to efficiently compute OT maps for the quadratic cost (Wasserstein-2 distance). Next, we extend the approach to the case when the input and output distributions are located in the spaces of different dimensions and derive error bounds for the computed OT map. We evaluate the algorithm on image generation and unpaired image restoration tasks. In particular, we consider denoising, colorization, and inpainting, where the optimality of the restoration map is a desired attribute, since the output (restored) image is expected to be close to the input (degraded) one",
    "volume": "main",
    "checked": true,
    "id": "987a879358bdc2150965d50396c0eb0159ffdf86",
    "citation_count": 14
  },
  "https://openreview.net/forum?id=cmt-6KtR4c4": {
    "title": "Leveraging Automated Unit Tests for Unsupervised Code Translation",
    "abstract": "With little to no parallel data available for programming languages, unsupervised methods are well-suited to source code translation. However, the majority of unsupervised machine translation approaches rely on back-translation, a method developed in the context of natural language translation and one that inherently involves training on noisy inputs. Unfortunately, source code is highly sensitive to small changes; a single token can result in compilation failures or erroneous programs, unlike natural languages where small inaccuracies may not change the meaning of a sentence. To address this issue, we propose to leverage an automated unit-testing system to filter out invalid translations, thereby creating a fully tested parallel corpus. We found that fine-tuning an unsupervised model with this filtered data set significantly reduces the noise in the translations so-generated, comfortably outperforming the state-of-the-art for all language pairs studied. In particular, for Java→ Python and Python→ C++ we outperform the best previous methods by more than 16% and 24% respectively, reducing the error rate by more than 35%",
    "volume": "main",
    "checked": true,
    "id": "1aed58bd07026492194672adec494dc37c894a28",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=Rf58LPCwJj0": {
    "title": "Optimal Representations for Covariate Shift",
    "abstract": "Machine learning systems often experience a distribution shift between training and testing. In this paper, we introduce a simple variational objective whose optima are exactly the set of all representations on which risk minimizers are guaranteed to be robust to any distribution shift that preserves the Bayes predictor, e.g., covariate shifts. Our objective has two components. First, a representation must remain discriminative for the task, i.e., some predictor must be able to simultaneously minimize the source and target risk. Second, the representation's marginal support needs to be the same across source and target. We make this practical by designing self-supervised objectives that only use unlabelled data and augmentations to train robust representations. Our objectives give insights into the robustness of CLIP, and further improve CLIP's representations to achieve SOTA results on DomainBed",
    "volume": "main",
    "checked": true,
    "id": "5382d9bc17aabfd47b7c7d9873d2b64fdde48305",
    "citation_count": 13
  },
  "https://openreview.net/forum?id=VPjw9KPWRSK": {
    "title": "Self-Supervised Inference in State-Space Models",
    "abstract": "We perform approximate inference in state-space models with nonlinear state transitions. Without parameterizing a generative model, we apply Bayesian update formulas using a local linearity approximation parameterized by neural networks. This comes accompanied by a maximum likelihood objective that requires no supervision via uncorrupt observations or ground truth latent states. The optimization backpropagates through a recursion similar to the classical Kalman filter and smoother. Additionally, using an approximate conditional independence, we can perform smoothing without having to parameterize a separate model. In scientific applications, domain knowledge can give a linear approximation of the latent transition maps, which we can easily incorporate into our model. Usage of such domain knowledge is reflected in excellent results (despite our model's simplicity) on the chaotic Lorenz system compared to fully supervised and variational inference methods. Finally, we show competitive results on an audio denoising experiment",
    "volume": "main",
    "checked": true,
    "id": "2e7716c31962c629e0064a7c387d2cfd9792e467",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=vwj6aUeocyf": {
    "title": "Long Expressive Memory for Sequence Modeling",
    "abstract": "We propose a novel method called Long Expressive Memory (LEM) for learning long-term sequential dependencies. LEM is gradient-based, it can efficiently process sequential tasks with very long-term dependencies, and it is sufficiently expressive to be able to learn complicated input-output maps. To derive LEM, we consider a system of multiscale ordinary differential equations, as well as a suitable time-discretization of this system. For LEM, we derive rigorous bounds to show the mitigation of the exploding and vanishing gradients problem, a wellknown challenge for gradient-based recurrent sequential learning methods. We also prove that LEM can approximate a large class of dynamical systems to high accuracy. Our empirical results, ranging from image and time-series classification through dynamical systems prediction to keyword spotting and language modeling, demonstrate that LEM outperforms state-of-the-art recurrent neural networks, gated recurrent units, and long short-term memory models",
    "volume": "main",
    "checked": true,
    "id": "2889f162fbcbc325e8080ff213063b2fc57ae4f2",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=sTNHCrIKDQc": {
    "title": "Graphon based Clustering and Testing of Networks: Algorithms and Theory",
    "abstract": "Network-valued data are encountered in a wide range of applications, and pose challenges in learning due to their complex structure and absence of vertex correspondence. Typical examples of such problems include classification or grouping of protein structures and social networks. Various methods, ranging from graph kernels to graph neural networks, have been proposed that achieve some success in graph classification problems. However, most methods have limited theoretical justification, and their applicability beyond classification remains unexplored. In this work, we propose methods for clustering multiple graphs, without vertex correspondence, that are inspired by the recent literature on estimating graphons— symmetric functions corresponding to infinite vertex limit of graphs. We propose a novel graph distance based on sorting-and-smoothing graphon estimators. Using the proposed graph distance, we present two clustering algorithms and show that they achieve state-of-the-art results. We prove the statistical consistency of both algorithms under Lipschitz assumptions on the graph degrees. We further study the applicability of the proposed distance for graph two-sample testing problems",
    "volume": "main",
    "checked": true,
    "id": "1435272aeb0dfb8692b6d74fcb473b5e529a89c6",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=z7p2V6KROOV": {
    "title": "Extending the WILDS Benchmark for Unsupervised Adaptation",
    "abstract": "Machine learning systems deployed in the wild are often trained on a source distribution but deployed on a diﬀerent target distribution. Unlabeled data can be a powerful point of leverage for mitigating these distribution shifts, as it is frequently much more available than labeled data and can often be obtained from distributions beyond the source distribution as well. However, existing distribution shift benchmarks with unlabeled data do not reﬂect the breadth of scenarios that arise in real-world applications. In this work, we present the Wilds 2.0 update, which extends 8 of the 10 datasets in the Wilds benchmark of distribution shifts to include curated unlabeled data that would be realistically obtainable in deployment. These datasets span a wide range of applications (from histology to wildlife conservation), tasks (classiﬁcation, regression, and detection), and modalities (photos, satellite images, microscope slides, text, molecular graphs). The update maintains consistency with the original Wilds benchmark by using identical labeled training, validation, and test sets, as well as the evaluation metrics. On these datasets, we systematically benchmark state-of-the-art methods that leverage unlabeled data, including domain-invariant, self-training, and self-supervised methods, and show that their success on Wilds is limited. To facilitate method development and evaluation, we provide an open-source package that automates data loading and contains all of the model architectures and methods used in this paper. Code and leaderboards are available at https://wilds.stanford.edu ",
    "volume": "main",
    "checked": true,
    "id": "ab2a8ca21309859ed027928dc38e6915be0e6776",
    "citation_count": 24
  },
  "https://openreview.net/forum?id=e2Lle5cij9D": {
    "title": "Hidden Convexity of Wasserstein GANs: Interpretable Generative Models with Closed-Form Solutions",
    "abstract": "Generative Adversarial Networks (GANs) are commonly used for modeling complex distributions of data. Both the generators and discriminators of GANs are often modeled by neural networks, posing a non-transparent optimization problem which is non-convex and non-concave over the generator and discriminator, respectively. Such networks are often heuristically optimized with gradient descent-ascent (GDA), but it is unclear whether the optimization problem contains any saddle points, or whether heuristic methods can find them in practice. In this work, we analyze the training of Wasserstein GANs with two-layer neural network discriminators through the lens of convex duality, and for a variety of generators expose the conditions under which Wasserstein GANs can be solved exactly with convex optimization approaches, or can be represented as convex-concave games. Using this convex duality interpretation, we further demonstrate the impact of different activation functions of the discriminator. Our observations are verified with numerical results demonstrating the power of the convex interpretation, with applications in progressive training of convex architectures corresponding to linear generators and quadratic-activation discriminators for CelebA image generation. The code for our experiments is available at https://github.com/ardasahiner/ProCoGAN",
    "volume": "main",
    "checked": true,
    "id": "d6206e3cc9fdefcaa901cd9634a892216b6a98d4",
    "citation_count": 12
  },
  "https://openreview.net/forum?id=TIdIXIpzhoI": {
    "title": "Progressive Distillation for Fast Sampling of Diffusion Models",
    "abstract": "Diffusion models have recently shown great promise for generative modeling, outperforming GANs on perceptual quality and autoregressive models at density estimation. A remaining downside is their slow sampling time: generating high quality samples takes many hundreds or thousands of model evaluations. Here we make two contributions to help eliminate this downside: First, we present new parameterizations of diffusion models that provide increased stability when using few sampling steps. Second, we present a method to distill a trained deterministic diffusion sampler, using many steps, into a new diffusion model that takes half as many sampling steps. We then keep progressively applying this distillation procedure to our model, halving the number of required sampling steps each time. On standard image generation benchmarks like CIFAR-10, ImageNet, and LSUN, we start out with state-of-the-art samplers taking as many as 8192 steps, and are able to distill down to models taking as few as 4 steps without losing much perceptual quality; achieving, for example, a FID of 3.0 on CIFAR-10 in 4 steps. Finally, we show that the full progressive distillation procedure does not take more time than it takes to train the original model, thus representing an efﬁcient solution for generative modeling using diffusion at both train and test time",
    "volume": "main",
    "checked": true,
    "id": "640e6bb94372d15c26832839645d02cb67f04384",
    "citation_count": 67
  },
  "https://openreview.net/forum?id=nRj0NcmSuxb": {
    "title": "FairCal: Fairness Calibration for Face Verification",
    "abstract": "Despite being widely used, face recognition models suffer from bias: the probability of a false positive (incorrect face match) strongly depends on sensitive attributes such as the ethnicity of the face. As a result, these models can disproportionately and negatively impact minority groups, particularly when used by law enforcement. The majority of bias reduction methods have several drawbacks: they use an end-to-end retraining approach, may not be feasible due to privacy issues, and often reduce accuracy. An alternative approach is post-processing methods that build fairer decision classifiers using the features of pre-trained models. However, they still have drawbacks: they reduce accuracy (AGENDA, FTC), or require retuning for different false positive rates (FSN). In this work, we introduce the Fairness Calibration (FairCal) method, a post-training approach that: (i) increases model accuracy (improving the state-of-the-art), (ii) produces fairly-calibrated probabilities, (iii) significantly reduces the gap in the false positive rates, (iv) does not require knowledge of the sensitive attribute, and (v) does not require retraining, training an additional model, or retuning. We apply it to the task of Face Verification, and obtain state-of-the-art results with all the above advantages",
    "volume": "main",
    "checked": true,
    "id": "3cf69ac8cae3d365db9139faee93d1631f42b5ed",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=9Vrb9D0WI4": {
    "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
    "abstract": "Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All trained models are available at https://github.com/bigscience-workshop/t-zero and all prompts are available at https://github.com/bigscience-workshop/promptsource",
    "volume": "main",
    "checked": true,
    "id": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca",
    "citation_count": 241
  },
  "https://openreview.net/forum?id=QRX0nCX_gk": {
    "title": "Multimeasurement Generative Models",
    "abstract": "empirical measure with a kernel. In that methodology, the kernel bandwidth ( σ , for Gaussian kernels)",
    "volume": "main",
    "checked": true,
    "id": "54a876282b9eb2b98ad174e944eb2e6b8c8518e1",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=T0GpzBQ1Fg6": {
    "title": "Step-unrolled Denoising Autoencoders for Text Generation",
    "abstract": "In this paper we propose a new generative model of text, Step-unrolled Denoising Autoencoder (SUNDAE), that does not rely on autoregressive models. Similarly to denoising diffusion techniques, SUNDAE is repeatedly applied on a sequence of tokens, starting from random inputs and improving them each time until convergence. We present a simple new improvement operator that converges in fewer iterations than diffusion methods, while qualitatively producing better samples on natural language datasets. SUNDAE achieves state-of-the-art results (among non-autoregressive methods) on the WMT'14 English-to-German translation task and good qualitative results on unconditional language modeling on the Colossal Cleaned Common Crawl dataset and a dataset of Python code from GitHub. The non-autoregressive nature of SUNDAE opens up possibilities beyond left-to-right prompted generation, by ﬁlling in arbitrary blank patterns in a template",
    "volume": "main",
    "checked": true,
    "id": "2da2a44f78e1bd9735d94fee3bd944d47d45742b",
    "citation_count": 13
  },
  "https://openreview.net/forum?id=wRODLDHaAiW": {
    "title": "iLQR-VAE : control-based learning of input-driven dynamics with applications to neural data",
    "abstract": "Understanding how neural dynamics give rise to behaviour is one of the most fundamental questions in systems neuroscience. To achieve this, a common approach is to record neural populations in behaving animals, and model these data as emanating from a latent dynamical system whose state trajectories can then be related back to behavioural observations via some form of decoding. As recordings are typically performed in localized circuits that form only a part of the wider implicated network, it is important to simultaneously learn the local dynamics and infer any unobserved external input that might drive them. Here, we introduce iLQR-VAE, a control-based approach to variational inference in nonlinear dynamical systems, capable of learning both latent dynamics, initial conditions, and ongoing external inputs. As in recent deep learning approaches, our method is based on an input-driven sequential variational autoencoder (VAE). The main novelty lies in the use of the powerful iterative linear quadratic regulator algorithm (iLQR) in the recognition model. Optimization of the standard evidence lower-bound requires differentiating through iLQR solutions, which is made possible by recent advances in differentiable control. Importantly, the recognition model is naturally tied to the generative model, greatly reducing the number of free parameters and ensuring high-quality inference throughout the course of learning. Moreover, iLQR can be used to perform inference flexibly on heterogeneous trials of varying lengths. This allows for instance to evaluate the model on a single long trial after training on smaller chunks. We demonstrate the effectiveness of iLQR-VAE on a range of synthetic systems, with autonomous as well as input-driven dynamics. We further apply it to neural and behavioural recordings in non-human primates performing two different reaching tasks, and show that iLQR-VAE yields high-quality kinematic reconstructions from the neural data",
    "volume": "main",
    "checked": true,
    "id": "6aac809cc3c9c39b3c156538494fb5742e0549cf",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=EZNOb_uNpJk": {
    "title": "ClimateGAN: Raising Climate Change Awareness by Generating Images of Floods",
    "abstract": "Climate change is a major threat to humanity, and the actions required to prevent its catastrophic consequences include changes in both policy-making and individual behaviour. However, taking action requires understanding the effects of climate change, even though they may seem abstract and distant. Projecting the potential consequences of extreme climate events such as flooding in familiar places can help make the abstract impacts of climate change more concrete and encourage action. As part of a larger initiative to build a website that projects extreme climate events onto user-chosen photos, we present our solution to simulate photo-realistic floods on authentic images. To address this complex task in the absence of suitable training data, we propose ClimateGAN, a model that leverages both simulated and real data for unsupervised domain adaptation and conditional image generation. In this paper, we describe the details of our framework, thoroughly evaluate components of our architecture and demonstrate that our model is capable of robustly generating photo-realistic flooding",
    "volume": "main",
    "checked": true,
    "id": "05e4b8d6db3dac820734482ccf67ca2a0bd29263",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=HTx7vrlLBEj": {
    "title": "Half-Inverse Gradients for Physical Deep Learning",
    "abstract": "Recent works in deep learning have shown that integrating differentiable physics simulators into the training process can greatly improve the quality of results. Although this combination represents a more complex optimization task than supervised neural network training, the same gradient-based optimizers are typically employed to minimize the loss function. However, the integrated physics solvers have a profound effect on the gradient flow as manipulating scales in magnitude and direction is an inherent property of many physical processes. Consequently, the gradient flow is often highly unbalanced and creates an environment in which existing gradient-based optimizers perform poorly. In this work, we analyze the characteristics of both physical and neural network optimizations to derive a new method that does not suffer from this phenomenon. Our method is based on a halfinversion of the Jacobian and combines principles of both classical network and physics optimizers to solve the combined optimization task. Compared to state-ofthe-art neural network optimizers, our method converges more quickly and yields better solutions, which we demonstrate on three complex learning problems involving nonlinear oscillators, the Schrödinger equation and the Poisson problem",
    "volume": "main",
    "checked": true,
    "id": "49228c2db9ba4d622f9351c83d67a3a86b22f3ec",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=9RUHPlladgh": {
    "title": "Visual Representation Learning Does Not Generalize Strongly Within the Same Domain",
    "abstract": "An important component for generalization in machine learning is to uncover underlying latent factors of variation as well as the mechanism through which each factor acts in the world. In this paper, we test whether 17 unsupervised, weakly supervised, and fully supervised representation learning approaches correctly infer the generative factors of variation in simple datasets (dSprites, Shapes3D, MPI3D). In contrast to prior robustness work that introduces novel factors of variation during test time, such as blur or other (un)structured noise, we here recompose, interpolate, or extrapolate only existing factors of variation from the training data set (e.g., small and medium-sized objects during training and large objects during testing). Models that learn the correct mechanism should be able to generalize to this benchmark. In total, we train and test 2000+ models and observe that all of them struggle to learn the underlying mechanism regardless of supervision signal and architectural bias. Moreover, the generalization capabilities of all tested models drop significantly as we move from artificial datasets towards more realistic real-world datasets. Despite their inability to identify the correct mechanism, the models are quite modular as their ability to infer other in-distribution factors remains fairly stable, providing only a single factor is out-of-distribution. These results point to an important yet understudied problem of learning mechanistic models of observations that can facilitate generalization",
    "volume": "main",
    "checked": true,
    "id": "61b1171efcf0242eb011816de1aa415f4262c55a",
    "citation_count": 15
  },
  "https://openreview.net/forum?id=3wNcr5nq56": {
    "title": "The Uncanny Similarity of Recurrence and Depth",
    "abstract": "It is widely believed that deep neural networks contain layer specialization, wherein neural networks extract hierarchical features representing edges and patterns in shallow layers and complete objects in deeper layers. Unlike common feed-forward models that have distinct filters at each layer, recurrent networks reuse the same parameters at various depths. In this work, we observe that recurrent models exhibit the same hierarchical behaviors and the same performance benefits with depth as feed-forward networks despite reusing the same filters at every recurrence. By training models of various feed-forward and recurrent architectures on several datasets for image classification as well as maze solving, we show that recurrent networks have the ability to closely emulate the behavior of non-recurrent deep models, often doing so with far fewer parameters",
    "volume": "main",
    "checked": true,
    "id": "0c9ea8f25c4f29a28e1c04c0c7121b22b6daa3bf",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=qRDQi3ocgR3": {
    "title": "Which Shortcut Cues Will DNNs Choose? A Study from the Parameter-Space Perspective",
    "abstract": "Deep neural networks (DNNs) often rely on easy–to–learn discriminatory features, or cues, that are not necessarily essential to the problem at hand. For example, ducks in an image may be recognized based on their typical background scenery, such as lakes or streams. This phenomenon, also known as shortcut learning, is emerging as a key limitation of the current generation of machine learning models. In this work, we introduce a set of experiments to deepen our understanding of shortcut learning and its implications. We design a training setup with several shortcut cues, named WCST-ML, where each cue is equally conducive to the visual recognition problem at hand. Even under equal opportunities, we observe that (1) certain cues are preferred to others, (2) solutions biased to the easy–to–learn cues tend to converge to relatively flat minima on the loss surface, and (3) the solutions focusing on those preferred cues are far more abundant in the parameter space. We explain the abundance of certain cues via their Kolmogorov (descriptional) complexity: solutions corresponding to Kolmogorov-simple cues are abundant in the parameter space and are thus preferred by DNNs. Our studies are based on the synthetic dataset DSprites and the face dataset UTKFace. In our WCST-ML, we observe that the bias of models leans toward simple cues, such as color and ethnicity. Our findings emphasize the importance of active human intervention to remove the model biases that may cause negative societal impacts",
    "volume": "main",
    "checked": true,
    "id": "535131d7218e26360c09cd8f9a2e7198ec0e3e6f",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=Ow1C7s3UcY": {
    "title": "Vitruvion: A Generative Model of Parametric CAD Sketches",
    "abstract": "Parametric computer-aided design (CAD) tools are the predominant way that engineers specify physical structures, from bicycle pedals to airplanes to printed circuit boards. The key characteristic of parametric CAD is that design intent is encoded not only via geometric primitives, but also by parameterized constraints between the elements. This relational speciﬁcation can be viewed as the construction of a constraint program, allowing edits to coherently propagate to other parts of the design. Machine learning offers the intriguing possibility of accelerating the design process via generative modeling of these structures, enabling new tools such as autocompletion, constraint inference, and conditional synthesis. In this work, we present such an approach to generative modeling of parametric CAD sketches, which constitute the basic computational building blocks of modern mechanical design. Our model, trained on real-world designs from the SketchGraphs dataset, autoregressively synthesizes sketches as sequences of primitives, with initial coordinates, and constraints that reference back to the sampled primitives. As samples from the model match the constraint graph representation used in standard CAD software, they may be directly imported, solved, and edited according to downstream design tasks. In addition, we condition the model on various contexts, including partial sketches (primers) and images of hand-drawn sketches. Evaluation of the proposed approach demonstrates its ability to synthesize realistic CAD sketches and its potential to aid the mechanical design workﬂow",
    "volume": "main",
    "checked": true,
    "id": "b0285407d3704914edb7a2ddfcb50bc377d367c4",
    "citation_count": 12
  },
  "https://openreview.net/forum?id=WVX0NNVBBkV": {
    "title": "Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?",
    "abstract": "While additional training data improves the robustness of deep neural networks against adversarial examples, it presents the challenge of curating a large number of specific real-world samples. We circumvent this challenge by using additional data from proxy distributions learned by state-of-the-art generative models. We first seek to formally understand the transfer of robustness from classifiers trained on proxy distributions to the real data distribution. We prove that the difference between the robustness of a classifier on the two distributions is upper bounded by the conditional Wasserstein distance between them. Motivated by our result, we next ask how to empirically select an appropriate generative model? We find that existing distance metrics, such as FID, fail to correctly determine the robustness transfer from proxy distributions. We propose a robust discrimination approach, which measures the distinguishability of synthetic and real samples under adversarial perturbations. Our approach accurately predicts the robustness transfer from different proxy distributions. After choosing a proxy distribution, the next question is which samples are most beneficial? We successfully optimize this selection by estimating the importance of each sample in robustness transfer. Finally, using our selection criterion for proxy distribution and individual samples, we curate a set of ten million most beneficial synthetic samples for robust training on the CIFAR-10 dataset. Using this set we improve robust accuracy by up to 7.5% and 6.7% in `∞ and `2 threat model, and certified robust accuracy by 7.6% in `2 threat model over baselines not using proxy distributions on the CIFAR-10 dataset",
    "volume": "main",
    "checked": true,
    "id": "8bcb5534227214b83255f5b9dedbc0d46a44794a",
    "citation_count": 19
  },
  "https://openreview.net/forum?id=aPOpXlnV1T": {
    "title": "On the Pitfalls of Heteroscedastic Uncertainty Estimation with Probabilistic Neural Networks",
    "abstract": "Capturing aleatoric uncertainty is a critical part of many machine learning systems. In deep learning, a common approach to this end is to train a neural network to estimate the parameters of a heteroscedastic Gaussian distribution by maximizing the logarithm of the likelihood function under the observed data. In this work, we examine this approach and identify potential hazards associated with the use of log-likelihood in conjunction with gradient-based optimizers. First, we present a synthetic example illustrating how this approach can lead to very poor but stable parameter estimates. Second, we identify the culprit to be the log-likelihood loss, along with certain conditions that exacerbate the issue. Third, we present an alternative formulation, termed β − NLL , in which each data point's contribution to the loss is weighted by the β -exponentiated variance estimate. We show that using an appropriate β largely mitigates the issue in our illustrative example. Fourth, we evaluate this approach on a range of domains and tasks and show that it achieves considerable improvements and performs more robustly concerning hyperparameters, both in predictive RMSE and log-likelihood criteria",
    "volume": "main",
    "checked": true,
    "id": "2b65a87983774bd86a362b73c6e21df1bb7cbfc7",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=K0E_F0gFDgA": {
    "title": "The MultiBERTs: BERT Reproductions for Robustness Analysis",
    "abstract": "Experiments with pretrained models such as BERT are often based on a single checkpoint. While the conclusions drawn apply to the artifact (i.e., the particular instance of the model), it is not always clear whether they hold for the more general procedure (which includes the model architecture, training data, initialization scheme, and loss function). Recent work has shown that re-running pretraining can lead to substantially different conclusions about performance, suggesting that alternative evaluations are needed to make principled statements about procedures. To address this question, we introduce MultiBERTs: a set of 25 BERTbase checkpoints, trained with similar hyperparameters as the original BERT model but differing in random initialization and data shuffling. The aim is to enable researchers to draw robust and statistically justified conclusions about pretraining procedures. The full release includes 25 fully trained checkpoints, as well as statistical guidelines and a code library implementing our recommended hypothesis testing methods. Finally, for five of these models we release a set of 28 intermediate checkpoints in order to support research on learning dynamics",
    "volume": "main",
    "checked": true,
    "id": "5b540745f4b51f95bf90fb3420e51edb037fc51a",
    "citation_count": 28
  },
  "https://openreview.net/forum?id=dg79moSRqIo": {
    "title": "One After Another: Learning Incremental Skills for a Changing World",
    "abstract": "Reward-free, unsupervised discovery of skills is an attractive alternative to the bottleneck of hand-designing rewards in environments where task supervision is scarce or expensive. However, current skill pre-training methods, like many RL techniques, make a fundamental assumption – stationary environments during training. Traditional methods learn all their skills simultaneously, which makes it difficult for them to both quickly adapt to changes in the environment, and to not forget earlier skills after such adaptation. On the other hand, in an evolving or expanding environment, skill learning must be able to adapt fast to new environment situations while not forgetting previously learned skills. These two conditions make it difficult for classic skill discovery to do well in an evolving environment. In this work, we propose a new framework for skill discovery, where skills are learned one after another in an incremental fashion. This framework allows newly learned skills to adapt to new environment or agent dynamics, while the fixed old skills ensure the agent doesn't forget a learned skill. We demonstrate experimentally that in both evolving and static environments, incremental skills significantly outperform current state-of-the-art skill discovery methods on both skill quality and the ability to solve downstream tasks. Videos for learned skills and code are made public on: https://notmahi.github.io/disk",
    "volume": "main",
    "checked": true,
    "id": "77d3d69f1c4c160e3765c416bc13aed863176197",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=8WawVDdKqlL": {
    "title": "Label Encoding for Regression Networks",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "aba33952946e1669fa7ed4e1ca112fc97962737e",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=vgqS1vkkCbE": {
    "title": "Value Function Spaces: Skill-Centric State Abstractions for Long-Horizon Reasoning",
    "abstract": "Reinforcement learning can train policies that effectively perform complex tasks. However for long-horizon tasks, the performance of these methods degrades with horizon, often necessitating reasoning over and chaining lower-level skills. Hierarchical reinforcement learning aims to enable this by providing a bank of low-level skills as action abstractions. Hierarchies can further improve on this by ab-stracting the space states as well. We posit that a suitable state abstraction should depend on the capabilities of the available lower-level policies. We propose Value Function Spaces: a simple approach that produces such a representation by using the value functions corresponding to each lower-level skill. These value functions capture the affordances of the scene, thus forming a representation that compactly abstracts task relevant information and robustly ignores distractors. Empirical evaluations for maze-solving and robotic manipulation tasks demonstrate that our approach improves long-horizon performance and enables better zero-shot generalization than alternative model-free and model-based methods",
    "volume": "main",
    "checked": true,
    "id": "0213fa01c7b8aa7668b11fd9edf283fe10d5719e",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=7TZeCsNOUB_": {
    "title": "Collapse by Conditioning: Training Class-conditional GANs with Limited Data",
    "abstract": "Class-conditioning offers a direct means of controlling a Generative Adversarial Network (GAN) based on a discrete input variable. While necessary in many applications, the additional information provided by the class labels could even be expected to benefit the training of the GAN itself. Contrary to this belief, we observe that class-conditioning causes mode collapse in limited data settings, where unconditional learning leads to satisfactory generative ability. Motivated by this observation, we propose a training strategy for conditional GANs (cGANs) that effectively prevents the observed mode-collapse by leveraging unconditional learning. Our training strategy starts with an unconditional GAN and gradually injects conditional information into the generator and the objective function. The proposed method for training cGANs with limited data results not only in stable training but also in generating high-quality images, thanks to the early-stage exploitation of the shared information across classes. We analyze the aforementioned mode collapse problem in comprehensive experiments on four datasets. Our approach demonstrates outstanding results compared with state-of-the-art methods and established baselines. The code is available at https://github.com/mshahbazi72/transitional-cGAN",
    "volume": "main",
    "checked": true,
    "id": "80d5305deec523715dd2b047fde7b929cac6499c",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=ds8yZOUsea": {
    "title": "Hidden Parameter Recurrent State Space Models For Changing Dynamics Scenarios",
    "abstract": "We We show and competing models on",
    "volume": "main",
    "checked": true,
    "id": "b66f6ef8c4ff9b5fce38396d43c7891ad4e6969f",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=f9MHpAGUyMn": {
    "title": "Dynamic Token Normalization improves Vision Transformers",
    "abstract": "(b) and (a & c), we ﬁnd that both intra-token statistics from LN and positional attention matrix P h can improve IN by a large margin, showing the reasonableness of our design in Eqn.(5). It also implies that both intra-token and inter-token statistics are useful in normalization. By comparing (e) and (c & d), we see that a learnable λ h can better trade off inter-token and intra-token statistics for each head, which further improves the performance of DTN",
    "volume": "main",
    "checked": true,
    "id": "7c8c6286a62a023f5d0d71fb315f9a0d4b9a2058",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=1NUsBU-7HAL": {
    "title": "Map Induction: Compositional spatial submap learning for efficient exploration in novel environments",
    "abstract": "Humans are expert explorers and foragers. Understanding the computational cognitive mechanisms that support this capability can advance the study of the human mind and enable more efﬁcient exploration algorithms. We hypothesize that humans explore new environments by inferring the structure of unobserved spaces through re-use of spatial information collected from previously explored spaces. Taking inspiration from the neuroscience of repeating map fragments and ideas about program induction, we present a novel \"Map Induction\" framework, which involves the generation of novel map proposals for unseen environments based on compositions of already-seen spaces in a Hierarchical Bayesian framework. The model thus explicitly reasons about unseen spaces through a distribution of strong spatial priors. We introduce a new behavioral Map Induction Task (MIT) that involves foraging for rewards to compare human performance with state-of-the-art existing models and Map Induction. We show that Map Induction better predicts human behavior than the non-inductive baselines. We also show that Map Induction, when used to augment state-of-the-art approximate planning algorithms, improves their performance. show that map induction can improve the performance of a Partially Observable Markov Decision Process (POMDP) planner during foraging. We begin by reviewing the literature on human spatial cognition involved in exploration and parallel developments of exploration algorithms, followed by introducing our computational modeling and experimental results",
    "volume": "main",
    "checked": true,
    "id": "75e55822158a651c8121992918ad2d319532b9a6",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=nkaba3ND7B5": {
    "title": "Autonomous Reinforcement Learning: Formalism and Benchmarking",
    "abstract": "Reinforcement learning (RL) provides a naturalistic framing for learning through trial and error, which is appealing both because of its simplicity and effectiveness and because of its resemblance to how humans and animals acquire skills through experience. However, real-world embodied learning, such as that performed by humans and animals, is situated in a continual, non-episodic world, whereas common benchmark tasks in RL are episodic, with the environment resetting between trials to provide the agent with multiple attempts. This discrepancy presents a major challenge when attempting to take RL algorithms developed for episodic simulated environments and run them on real-world platforms, such as robots. In this paper, we aim to address this discrepancy by laying out a framework for Autonomous Reinforcement Learning (ARL): reinforcement learning where the agent not only learns through its own experience, but also contends with lack of human supervision to reset between trials. We introduce a simulated benchmark EARL1 around this framework, containing a set of diverse and challenging simulated tasks reflective of the hurdles introduced to learning when only a minimal reliance on extrinsic intervention can be assumed. We show that standard approaches to episodic RL and existing approaches struggle as interventions are minimized, underscoring the need for developing new algorithms for reinforcement learning with a greater focus on autonomy",
    "volume": "main",
    "checked": true,
    "id": "5dd82eee3efefb96aeaaae8b817b6be2e204dc2f",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=bp-LJ4y_XC": {
    "title": "Sequence Approximation using Feedforward Spiking Neural Network for Spatiotemporal Learning: Theory and Optimization Methods",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "d4bc30ab26b2c8bfc519c9d81f9aa167ab460965",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=hjd-kcpDpf2": {
    "title": "Maximizing Ensemble Diversity in Deep Reinforcement Learning",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "b2960eeaab35c5a02d8a8dfd7636402ec0d2b7cd",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=OIs3SxU5Ynl": {
    "title": "VAE Approximation Error: ELBO and Exponential Families",
    "abstract": "The importance of Variational Autoencoders reaches far beyond standalone generative models — the approach is also used for learning latent representations and can be generalized to semi-supervised learning. This requires a thorough analysis of their commonly known shortcomings: posterior collapse and approximation errors. This paper analyzes VAE approximation errors caused by the combination of the ELBO objective and encoder models from conditional exponential families, including, but not limited to, commonly used conditionally independent discrete and continuous models. We characterize subclasses of generative models consistent with these encoder families. We show that the ELBO optimizer is pulled away from the likelihood optimizer towards the consistent subset and study this effect experimentally. Importantly, this subset can not be enlarged, and the respective error cannot be decreased, by considering deeper encoder/decoder networks",
    "volume": "main",
    "checked": true,
    "id": "8bca798d8eb54a2e6d692eb253166697703d9c9c",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=Xo0lbDt975": {
    "title": "An Agnostic Approach to Federated Learning with Class Imbalance",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "e6982936a30c8c3c4ec8161a864dcd4edaf08b3b",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=zf_Ll3HZWgy": {
    "title": "How Much Can CLIP Benefit Vision-and-Language Tasks?",
    "abstract": "Most existing Vision-and-Language (V&L) models rely on pre-trained visual encoders, using a relatively small set of manuallyannotated data (as compared to web-crawled data), to perceive the visual world. However, it has been observed that large-scale pretraining usually can result in better generalization performance, e.g., CLIP (Contrastive Language-Image Pre-training), trained on a massive amount of image-caption pairs, has shown a strong zero-shot capability on various vision tasks. To further study the advantage brought by CLIP, we propose to use CLIP as the visual encoder in various V&L models in two typical scenarios: 1) plugging CLIP into task-specific fine-tuning; 2) combining CLIP with V&L pre-training and transferring to downstream tasks. We show that CLIP significantly outperforms widely-used visual encoders trained with in-domain annotated data, such as BottomUp-TopDown. We achieve competitive or better results on diverse V&L tasks, while establishing new state-of-the-art results on Visual Question Answering, Visual Entailment, and V&L Navigation tasks",
    "volume": "main",
    "checked": true,
    "id": "8f167ec1149921fac63b1ea855443de109bb013a",
    "citation_count": 109
  },
  "https://openreview.net/forum?id=_hszZbt46bT": {
    "title": "Anomaly Detection for Tabular Data with Internal Contrastive Learning",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "ab4d553afd613373f56520da52c83239be32422e",
    "citation_count": 13
  },
  "https://openreview.net/forum?id=lzupY5zjaU9": {
    "title": "Distribution Compression in Near-Linear Time",
    "abstract": "In distribution compression, one aims to accurately summarize a probability distribution P using a small number of representative points. Near-optimal thinning procedures achieve this goal by sampling n points from a Markov chain and identifying √ n points with (cid:101) O (1 / √ n ) discrepancy to P . Unfortunately, these algorithms suffer from quadratic or super-quadratic runtime in the sample size n . To address this deficiency, we introduce Compress++, a simple meta-procedure for speeding up any thinning algorithm while suffering at most a factor of 4 in error. When combined with the quadratic-time kernel halving and kernel thinning algorithms of Dwivedi and Mackey (2021), Compress++ delivers √ n points with better-than-Monte-Carlo maximum mean discrepancy in O ( n log 3 n ) time and O ( √ n log 2 n ) space. Moreover, Compress++ enjoys the same near-linear runtime given any quadratic-time input and reduces the runtime of super-quadratic algorithms by a square-root factor. In our benchmarks with high-dimensional Monte Carlo samples and Markov chains targeting challenging differential equation posteriors, Compress++ matches or nearly matches the accuracy of its input algorithm in orders of magnitude less time",
    "volume": "main",
    "checked": true,
    "id": "9ede06642621f187412d1ded62e2d672ac8c1149",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=qhC8mr2LEKq": {
    "title": "CrossBeam: Learning to Search in Bottom-Up Program Synthesis",
    "abstract": "Many approaches to program synthesis perform a search within an enormous space of programs to find one that satisfies a given specification. Prior works have used neural models to guide combinatorial search algorithms, but such approaches still explore a huge portion of the search space and quickly become intractable as the size of the desired program increases. To tame the search space blowup, we propose training a neural model to learn a hands-on search policy for bottom-up synthesis, instead of relying on a combinatorial search algorithm. Our approach, called CROSSBEAM, uses the neural model to choose how to combine previouslyexplored programs into new programs, taking into account the search history and partial program executions. Motivated by work in structured prediction on learning to search, CROSSBEAM is trained on-policy using data extracted from its own bottom-up searches on training tasks. We evaluate CROSSBEAM in two very different domains, string manipulation and logic programming. We observe that CROSSBEAM learns to search efficiently, exploring much smaller portions of the program space compared to the state-of-the-art",
    "volume": "main",
    "checked": true,
    "id": "c347093e2dca530ce347526380b0b7aedf03a6b2",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=dUV91uaXm3": {
    "title": "Revisiting Over-smoothing in BERT from the Perspective of Graph",
    "abstract": "Recently over-smoothing phenomenon of Transformer-based models is observed in both vision and language fields. However, no existing work has delved deeper to further investigate the main cause of this phenomenon. In this work, we make the attempt to analyze the over-smoothing problem from the perspective of graph, where such problem was first discovered and explored. Intuitively, the self-attention matrix can be seen as a normalized adjacent matrix of a corresponding graph. Based on the above connection, we provide some theoretical analysis and find that layer normalization plays a key role in the over-smoothing issue of Transformer-based models. Specifically, if the standard deviation of layer normalization is sufficiently large, the output of Transformer stacks will converge to a specific low-rank subspace and result in over-smoothing. To alleviate the over-smoothing problem, we consider hierarchical fusion strategies, which combine the representations from different layers adaptively to make the output more diverse. Extensive experiment results on various data sets illustrate the effect of our fusion method",
    "volume": "main",
    "checked": true,
    "id": "383116b2685d0c7ae9669ac929b628b9af0af5f3",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=Z1Qlm11uOM": {
    "title": "Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction",
    "abstract": "Video recordings of speech contain correlated audio and visual information, providing a strong signal for speech representation learning from the speaker's lip movements and the produced sound. We introduce Audio-Visual Hidden Unit BERT (AV-HuBERT), a self-supervised representation learning framework for audio-visual speech, which masks multi-stream video input and predicts automatically discovered and iteratively refined multimodal hidden units. AV-HuBERT learns powerful audio-visual speech representation benefiting both lip-reading and automatic speech recognition. On the largest public lip-reading benchmark LRS3 (433 hours), AV-HuBERT achieves 32.5% WER with only 30 hours of labeled data, outperforming the former state-of-the-art approach (33.6%) trained with a thousand times more transcribed video data (31K hours) (Makino et al., 2019). The lip-reading WER is further reduced to 26.9% when using all 433 hours of labeled data from LRS3 and combined with self-training. Using our audio-visual representation on the same benchmark for audio-only speech recognition leads to a 40% relative WER reduction over the state-of-the-art performance (1.3% vs 2.3%). Our code and models are available at https://github.com/ facebookresearch/av_hubert",
    "volume": "main",
    "checked": true,
    "id": "dc9a76b7cb690e6a95f0f07bb3d4fabb7181b68d",
    "citation_count": 22
  },
  "https://openreview.net/forum?id=eMudnJsb1T5": {
    "title": "Sampling with Mirrored Stein Operators",
    "abstract": "We introduce a new family of particle evolution samplers suitable for constrained domains and non-Euclidean geometries. Stein Variational Mirror Descent and Mirrored Stein Variational Gradient Descent minimize the Kullback-Leibler (KL) divergence to constrained target distributions by evolving particles in a dual space deﬁned by a mirror map. Stein Variational Natural Gradient exploits non-Euclidean geometry to more efﬁciently minimize the KL divergence to unconstrained targets. We derive these samplers from a new class of mirrored Stein operators and adaptive kernels developed in this work. We demonstrate that these new samplers yield accurate approximations to distributions on the simplex, deliver valid conﬁdence intervals in post-selection inference, and converge more rapidly than prior methods in large-scale unconstrained posterior inference. Finally, we establish the convergence of our new procedures under veriﬁable conditions on the target distribution",
    "volume": "main",
    "checked": true,
    "id": "535536f05afaebaffa460d8e2c6ff3ad5a973431",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=vDwBW49HmO": {
    "title": "Gradient Matching for Domain Generalization",
    "abstract": "Machine learning systems typically assume that the distributions of training and test sets match closely. However, a critical requirement of such systems in the real world is their ability to generalize to unseen domains. Here, we propose an inter-domain gradient matching objective that targets domain generalization by maximizing the inner product between gradients from different domains. Since direct optimization of the gradient inner product can be computationally prohibitive – requires computation of second-order derivatives – we derive a simpler first-order algorithm named Fish that approximates its optimisation. We perform experiments on both the WILDS benchmark, which captures distribution shift in the real world, as well as datasets in DOMAINBED benchmark that focuses more on synthetic-to-real transfer. Our method produces competitive results on both benchmarks, demonstrating its effectiveness across a wide range of domain generalization tasks",
    "volume": "main",
    "checked": true,
    "id": "525dd120c0b5808ddcbbf703677b46346fb0729b",
    "citation_count": 70
  },
  "https://openreview.net/forum?id=wMpS-Z_AI_E": {
    "title": "A Theoretical Analysis on Feature Learning in Neural Networks: Emergence from Inputs and Advantage over Fixed Features",
    "abstract": "An important characteristic of neural networks is their ability to learn representations of the input data with effective features for prediction, which is believed to be a key factor to their superior empirical performance. To better understand the source and benefit of feature learning in neural networks, we consider learning problems motivated by practical data, where the labels are determined by a set of class relevant patterns and the inputs are generated from these along with some background patterns. We prove that neural networks trained by gradient descent can succeed on these problems. The success relies on the emergence and improvement of effective features, which are learned among exponentially many candidates efficiently by exploiting the data (in particular, the structure of the input distribution). In contrast, no linear models on data-independent features of polynomial sizes can learn to as good errors. Furthermore, if the specific input structure is removed, then no polynomial algorithm in the Statistical Query model can learn even weakly. These results provide theoretical evidence showing that feature learning in neural networks depends strongly on the input structure and leads to the superior performance. Our preliminary experimental results on synthetic and real data also provide positive support",
    "volume": "main",
    "checked": true,
    "id": "2c3238a5ac2d18dab25d620c4933b497bce22c8a",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=AvcfxqRy4Y": {
    "title": "Understanding the Role of Self Attention for Efficient Speech Recognition",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "9da271bd20d17ec9d22b4ac5f18e0f8fb6bd0b92",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=YpPiNigTzMT": {
    "title": "Universalizing Weak Supervision",
    "abstract": "Weak supervision (WS) frameworks are a popular way to bypass hand-labeling large datasets for training data-hungry models. These approaches synthesize multiple noisy but cheaply-acquired estimates of labels into a set of high-quality pseudolabels for downstream training. However, the synthesis technique is specific to a particular kind of label, such as binary labels or sequences, and each new label type requires manually designing a new synthesis algorithm. Instead, we propose a universal technique that enables weak supervision over any label type while still offering desirable properties, including practical flexibility, computational efficiency, and theoretical guarantees. We apply this technique to important problems previously not tackled by WS frameworks including learning to rank, regression, and learning in hyperbolic space. Theoretically, our synthesis approach produces a consistent estimators for learning some challenging but important generalizations of the exponential family model. Experimentally, we validate our framework and show improvement over baselines in diverse settings including real-world learning-to-rank and regression problems along with learning on hyperbolic manifolds",
    "volume": "main",
    "checked": true,
    "id": "f1a1cbdac308f9122d415e35827126e345e22b73",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=gjNcH0hj0LM": {
    "title": "Coherence-based Label Propagation over Time Series for Accelerated Active Learning",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "b9c2697a3222651061c468f44eb4cb8b4f0e193e",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=v-v1cpNNK_v": {
    "title": "NASI: Label- and Data-agnostic Neural Architecture Search at Initialization",
    "abstract": "Recent years have witnessed a surging interest in Neural Architecture Search (NAS). Various algorithms have been proposed to improve the search efficiency and effectiveness of NAS, i.e., to reduce the search cost and improve the generalization performance of the selected architectures, respectively. However, the search efficiency of these algorithms is severely limited by the need for model training during the search process. To overcome this limitation, we propose a novel NAS algorithm called NAS at Initialization (NASI) that exploits the capability of a Neural Tangent Kernel in being able to characterize the converged performance of candidate architectures at initialization, hence allowing model training to be completely avoided to boost the search efficiency. Besides the improved search efficiency, NASI also achieves competitive search effectiveness on various datasets like CIFAR-10/100 and ImageNet. Further, NASI is shown to be labeland data-agnostic under mild conditions, which guarantees the transferability of architectures selected by our NASI over different datasets",
    "volume": "main",
    "checked": true,
    "id": "e3f732c15c0932e08cd51cab282d7cc7a1739c75",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=Az7opqbQE-3": {
    "title": "Heteroscedastic Temporal Variational Autoencoder For Irregularly Sampled Time Series",
    "abstract": "Irregularly sampled time series commonly occur in several domains where they present a significant challenge to standard deep learning models. In this paper, we propose a new deep learning framework for probabilistic interpolation of irregularly sampled time series that we call the Heteroscedastic Temporal Variational Autoencoder (HeTVAE). HeTVAE includes a novel input layer to encode information about input observation sparsity, a temporal VAE architecture to propagate uncertainty due to input sparsity, and a heteroscedastic output layer to enable variable uncertainty in output interpolations. Our results show that the proposed architecture is better able to reflect variable uncertainty through time due to sparse and irregular sampling than a range of baseline and traditional models, as well as recently proposed deep latent variable models that use homoscedastic output layers.1",
    "volume": "main",
    "checked": true,
    "id": "02ddd2ac73c072d25ed8676b6a425b22f4963bb3",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=z1-I6rOKv1S": {
    "title": "Autoregressive Quantile Flows for Predictive Uncertainty Estimation",
    "abstract": "Numerous applications of machine learning involve representing probability distributions over high-dimensional data. We propose autoregressive quantile ﬂows, a ﬂexible class of normalizing ﬂow models trained using a novel objective based on proper scoring rules. Our objective does not require calculating computationally expensive determinants of Jacobians during training and supports new types of neural architectures, such as neural autoregressive ﬂows from which it is easy to sample. We leverage these models in quantile ﬂow regression, an approach that parameterizes predictive conditional distributions with ﬂows, resulting in improved probabilistic predictions on tasks such as time series forecasting and object detection. Our novel objective functions and neural ﬂow parameterizations also yield improvements on popular generation and density estimation tasks, and represent a step beyond maximum likelihood learning of ﬂows",
    "volume": "main",
    "checked": true,
    "id": "4272e3c08657897850bc20ac828aa00fa3bec8a0",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=5i7lJLuhTm": {
    "title": "Learning by Directional Gradient Descent",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "afd60ef58e012023903d2815273f21ce4e49a506",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=9pEJSVfDbba": {
    "title": "Embedded-model flows: Combining the inductive biases of model-free deep learning and explicit probabilistic modeling",
    "abstract": "Normalizing flows have shown great success as general-purpose density estima-tors. However, many real world applications require the use of domain-specific knowledge, which normalizing flows cannot readily incorporate. We propose embedded-model flows (EMF), which alternate general-purpose transformations with structured layers that embed domain-specific inductive biases. These layers are automatically constructed by converting user-specified differentiable probabilistic models into equivalent bijective transformations. We also introduce gated structured layers , which allow bypassing the parts of the models that fail to capture the statistics of the data. We demonstrate that EMFs can be used to induce desirable properties such as multimodality and continuity. Furthermore, we show that EMFs enable a high performance form of variational inference where the structure of the prior model is embedded in the variational architecture. In our experiments, we show that this approach outperforms a large number of alternative methods in common structured inference problems",
    "volume": "main",
    "checked": true,
    "id": "21a75213c0d3dad6c6d891960099196496e03a14",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=h0OYV0We3oh": {
    "title": "Illiterate DALL-E Learns to Compose",
    "abstract": "Although DALL·E has shown an impressive ability of composition-based systematic generalization in image generation, it requires the dataset of text-image pairs and the compositionality is provided by the text. In contrast, object-centric representation models like the Slot Attention model learn composable representations without the text prompt. However, unlike DALL·E its ability to systematically generalize for zero-shot generation is significantly limited. In this paper, we propose a simple but novel slot-based autoencoding architecture, called SLATE1, for combining the best of both worlds: learning object-centric representations that allows systematic generalization in zero-shot image generation without text. As such, this model can also be seen as an illiterate DALL·E model. Unlike the pixel-mixture decoders of existing object-centric representation models, we propose to use the Image GPT decoder conditioned on the slots for capturing complex interactions among the slots and pixels. In experiments, we show that this simple and easy-to-implement architecture not requiring a text prompt achieves significant improvement in in-distribution and out-of-distribution (zero-shot) image generation and qualitatively comparable or better slot-attention structure than the models based on mixture decoders. https://sites.google.com/view/slate-autoencoder",
    "volume": "main",
    "checked": true,
    "id": "daa5ca0a39ecec8ab5c534196eca526bafe41051",
    "citation_count": 12
  },
  "https://openreview.net/forum?id=lTqGXfn9Tv": {
    "title": "Phenomenology of Double Descent in Finite-Width Neural Networks",
    "abstract": "‘Double descent' delineates the generalization behaviour of models depending on the regime they belong to: underor over-parameterized. The current theoretical understanding behind the occurrence of this phenomenon is primarily based on linear and kernel regression models — with informal parallels to neural networks via the Neural Tangent Kernel. Therefore such analyses do not adequately capture the mechanisms behind double descent in finite-width neural networks, as well as, disregard crucial components — such as the choice of the loss function. We address these shortcomings by leveraging influence functions in order to derive suitable expressions of the population loss and its lower bound, while imposing minimal assumptions on the form of the parametric model. Our derived bounds bear an intimate connection with the spectrum of the Hessian at the optimum, and importantly, exhibit a double descent behaviour at the interpolation threshold. Building on our analysis, we further investigate how the loss function affects double descent — and thus uncover interesting properties of neural networks and their Hessian spectra near the interpolation threshold",
    "volume": "main",
    "checked": true,
    "id": "aa436bea266bea7cfb64c221a69249657b3c67b1",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=v6s3HVjPerv": {
    "title": "Do Users Benefit From Interpretable Vision? A User Study, Baseline, And Dataset",
    "abstract": "A variety of methods exist to explain image classiﬁcation models. However, it remains unclear whether they provide any beneﬁt to users over simply comparing various inputs and the model's respective predictions. We conducted a user study (N=240) to test how such a baseline explanation technique performs against concept-based and counterfactual explanations. To this end, we contribute a synthetic dataset generator capable of biasing individual attributes and quantifying their relevance to the model. In a study, we assess if participants can identify the relevant set of attributes compared to the ground-truth. Our results show that the baseline outperformed concept-based explanations. Counterfactual explanations from an invertible neural network performed similarly as the baseline. Still, they allowed users to identify some attributes more accurately. Our results highlight the importance of measuring how well users can reason about biases of a model, rather than solely relying on technical evaluations or proxy tasks. We open-source our study and dataset so it can serve as a blue-print for future studies",
    "volume": "main",
    "checked": true,
    "id": "be1f40bebd38b3e66638da680aad9b3ce21ac5ec",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=ckZY7DGa7FQ": {
    "title": "A Fine-Tuning Approach to Belief State Modeling",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "fffb1db211354c7838e24b069c423ac7c064bfbd",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=6MmiS0HUJHR": {
    "title": "When Can We Learn General-Sum Markov Games with a Large Number of Players Sample-Efficiently?",
    "abstract": "Multi-agent reinforcement learning has made substantial empirical progresses in solving games with a large number of players. However, theoretically, the best known sample complexity for finding a Nash equilibrium in general-sum games scales exponentially in the number of players due to the size of the joint action space, and there is a matching exponential lower bound. This paper investigates what learning goals admit better sample complexities in the setting of m-player general-sum Markov games with H steps, S states, and Ai actions per player. First, we design algorithms for learning an ε-Coarse Correlated Equilibrium (CCE) in Õ(HSmaxi≤mAi/ε) episodes, and an ε-Correlated Equilibrium (CE) in Õ(HSmaxi≤mAi /ε) episodes. This is the first line of results for learning CCE and CE with sample complexities polynomial in maxi≤mAi. Our algorithm for learning CE integrates an adversarial bandit subroutine which minimizes a weighted swap regret, along with several novel designs in the outer loop. Second, we consider the important special case of Markov Potential Games, and design an algorithm that learns an ε-approximate Nash equilibrium within Õ(S ∑ i≤mAi/ε ) episodes (when only highlighting the dependence on S, Ai, and ε), which only depends linearly in ∑ i≤mAi and significantly improves over existing efficient algorithms in the ε dependence. Overall, our results shed light on what equilibria or structural assumptions on the game may enable sample-efficient learning with many players",
    "volume": "main",
    "checked": true,
    "id": "0136a33c07bc7003066a1f0ae8e0ab26f84c0cbe",
    "citation_count": 27
  },
  "https://openreview.net/forum?id=-AOEi-5VTU8": {
    "title": "Fast Differentiable Matrix Square Root",
    "abstract": "Computing the matrix square root or its inverse in a differentiable manner is important in a variety of computer vision tasks. Previous methods either adopt the Singular Value Decomposition (SVD) to explicitly factorize the matrix or use the Newton-Schulz iteration (NS iteration) to derive the approximate solution. However, both methods are not computationally efficient enough in either the forward pass or in the backward pass. In this paper, we propose two more efficient variants to compute the differentiable matrix square root. For the forward propagation, one method is to use Matrix Taylor Polynomial (MTP), and the other method is to use Matrix Padé Approximants (MPA). The backward gradient is computed by iteratively solving the continuous-time Lyapunov equation using the matrix sign function. Both methods yield considerable speed-up compared with the SVD or the Newton-Schulz iteration. Experimental results on the de-correlated batch normalization and second-order vision transformer demonstrate that our methods can also achieve competitive and even slightly better performances. The code is available at https://github.com/KingJamesSong/FastDifferentiableMatSqrt",
    "volume": "main",
    "checked": true,
    "id": "e10f14defd9613cf5ab7a953cd834cb6f8fb2b63",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=w4cXZDDib1H": {
    "title": "ViDT: An Efficient and Effective Fully Transformer-based Object Detector",
    "abstract": "Transformers are transforming the landscape of computer vision, especially for recognition tasks. Detection transformers are the ﬁrst fully end-to-end learning systems for object detection, while vision transformers are the ﬁrst fully transformer-based architecture for image classiﬁcation. In this paper, we integrate Vision and Detection Transformers (ViDT) to build an effective and efﬁcient object detector. ViDT introduces a reconﬁgured attention module to extend the recent Swin Transformer to be a standalone object detector, followed by a computationally efﬁcient transformer decoder that exploits multi-scale features and auxiliary techniques essential to boost the detection performance without much increase in computational load. Extensive evaluation results on the Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP and latency trade-off among existing fully transformer-based object detectors, and achieves 49.2 AP ow-ing to its high scalability for large models. We release the code and trained models at https://github.com/naver-ai/vidt",
    "volume": "main",
    "checked": true,
    "id": "38c493ed7121ec136516b9a93f76e4c680999a93",
    "citation_count": 13
  },
  "https://openreview.net/forum?id=5HvpvYd68b": {
    "title": "switch-GLAT: Multilingual Parallel Machine Translation Via Code-Switch Decoder",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "5962ff4e4adbdb52e45955cce6a76f56493cf70a",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=0kPL3xO4R5": {
    "title": "Fast topological clustering with Wasserstein distance",
    "abstract": "The topological patterns exhibited by many real-world networks motivate the development of topology-based methods for assessing the similarity of networks. However, extracting topological structure is difficult, especially for large and dense networks whose node degrees range over multiple orders of magnitude. In this paper, we propose a novel and computationally practical topological clustering method that clusters complex networks with intricate topology using principled theory from persistent homology and optimal transport. Such networks are aggregated into clusters through a centroid-based clustering strategy based on both their topological and geometric structure, preserving correspondence between nodes in different networks. The notions of topological proximity and centroid are characterized using a novel and efficient approach to computation of the Wasserstein distance and barycenter for persistence barcodes associated with connected components and cycles. The proposed method is demonstrated to be effective using both simulated networks and measured functional brain networks",
    "volume": "main",
    "checked": true,
    "id": "35c8dc360c4981a6926d606619a94466f4f1292f",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=o-1v9hdSult": {
    "title": "Bridging the Gap: Providing Post-Hoc Symbolic Explanations for Sequential Decision-Making Problems with Inscrutable Representations",
    "abstract": "As increasingly complex AI systems are introduced into our daily lives, it becomes important for such systems to be capable of explaining the rationale for their decisions and allowing users to contest these decisions. A significant hurdle to allowing for such explanatory dialogue could be the vocabulary mismatch between the user and the AI system. This paper introduces methods for providing contrastive explanations in terms of user-specified concepts for sequential decisionmaking settings where the system's model of the task may be best represented as a blackbox simulator. We do this by building partial symbolic models of the task that can be leveraged to answer the user queries. We empirically test these methods on a popular Atari game (Montezuma's Revenge) and modified versions of Sokoban (a well known planning benchmark) and report the results of user studies to evaluate whether people find explanations generated in this form useful",
    "volume": "main",
    "checked": true,
    "id": "9a2bbb65a072a69a28c0b3cb2143f0e62c505f50",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=0jP2n0YFmKG": {
    "title": "Towards Training Billion Parameter Graph Neural Networks for Atomic Simulations",
    "abstract": "Recent progress in Graph Neural Networks (GNNs) for modeling atomic simulations has the potential to revolutionize catalyst discovery, which is a key step in making progress towards the energy breakthroughs needed to combat climate change. However, the GNNs that have proven most effective for this task are memory intensive as they model higher-order interactions in the graphs such as those between triplets or quadruplets of atoms, making it challenging to scale these models. In this paper, we introduce Graph Parallelism, a method to distribute input graphs across multiple GPUs, enabling us to train very large GNNs with hundreds of millions or billions of parameters. We empirically evaluate our method by scaling up the number of parameters of the recently proposed DimeNet++ and GemNet models by over an order of magnitude. On the large-scale Open Catalyst 2020 (OC20) dataset, these graph-parallelized models lead to relative improvements of 1) 15% on the force MAE metric for the S2EF task and 2) 21% on the AFbT metric for the IS2RS task, establishing new state-of-the-art results",
    "volume": "main",
    "checked": true,
    "id": "6a62ca8c617656caef26a31b13e1cadab6e996e3",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=msRBojTz-Nh": {
    "title": "Learned Simulators for Turbulence",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "8049cb71b76be847c1c4886a79a9cbd4d9b1d821",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=cU8rknuhxc": {
    "title": "Learning more skills through optimistic exploration",
    "abstract": "Unsupervised skill learning objectives (Eysenbach et al., 2019; Gregor et al., 2016) allow agents to learn rich repertoires of behavior in the absence of extrinsic rewards. They work by simultaneously training a policy to produce distinguishable latent-conditioned trajectories, and a discriminator to evaluate distinguishability by trying to infer latents from trajectories. The hope is for the agent to explore and master the environment by encouraging each skill (latent) to reliably reach different states. However, an inherent exploration problem lingers: when a novel state is actually encountered, the discriminator will necessarily not have seen enough training data to produce accurate and confident skill classifications, leading to low intrinsic reward for the agent and effective penalization of the sort of exploration needed to actually maximize the objective. To combat this inherent pessimism towards exploration, we derive an information gain auxiliary objective that involves training an ensemble of discriminators and rewarding the policy for their disagreement. Our objective directly estimates the epistemic uncertainty that comes from the discriminator not having seen enough training examples, thus providing an intrinsic reward more tailored to the true objective compared to pseudocount-based methods (Burda et al., 2019). We call this exploration bonus discriminator disagreement intrinsic reward, or DISDAIN. We demonstrate empirically that DISDAIN improves skill learning both in a tabular grid world (Four Rooms) and the 57 games of the Atari Suite (from pixels). Thus, we encourage researchers to treat pessimism with DISDAIN",
    "volume": "main",
    "checked": true,
    "id": "3c91a4534d51d21ae67e4a9f9287bb2a14dc5e3b",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=t8O-4LKFVx": {
    "title": "Learning Optimal Conformal Classifiers",
    "abstract": "Algorithm B Python Pseudo-Code for ConfTr: We present code based on our Python and Jax implementation of ConfTr. In particular, we include smooth calibration and prediction steps for T HR as well as the classiﬁcation loss L class and the size loss Ω . Instead of including a full training loop, compute_loss_and_error shows how to compute the loss which can then be called using jax.value_and_grad(compute_loss_and_error, has_aux= True ) and used for training using Optax. Hyper-parameters, including alpha , dispersion , size_weight , temperature , loss_matrix , size_weights and weight_decay , are not deﬁned explicitly for brevity. smooth_quantile is assumed to be a provided differentiable quantile com-putation method. Finally, model can be any Jax/Haiku model",
    "volume": "main",
    "checked": true,
    "id": "7157c8eb81d972094230d28d0817004e3acf4148",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=F5Em8ASCosV": {
    "title": "Causal Contextual Bandits with Targeted Interventions",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "313e6a4b0ae9c8fb701a56451f6f1f5952138357",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=bZJbzaj_IlP": {
    "title": "A Non-Parametric Regression Viewpoint : Generalization of Overparametrized Deep RELU Network Under Noisy Observations",
    "abstract": "We study the generalization properties of the overparameterized deep neural network (DNN) with Rectiﬁed Linear Unit (ReLU) activations. Under the nonparametric regression framework, it is assumed that the ground-truth function is from a reproducing kernel Hilbert space (RKHS) induced by a neural tangent kernel (NTK) of ReLU DNN, and a dataset is given with the noises. Without a delicate adoption of early stopping, we prove that the overparametrized DNN trained by vanilla gradient descent does not recover the ground-truth function. It turns out that the estimated DNN's L 2 prediction error is bounded away from 0 . As a complement of the above result, we show that the (cid:96) 2 -regularized gradient descent enables the overparametrized DNN to achieve the minimax optimal convergence rate of the L 2 prediction error, without early stopping. Notably, the rate we obtained is faster than O ( n − 1 / 2 ) known in the literature",
    "volume": "main",
    "checked": true,
    "id": "cb63525f1a63ddae95d449c7ad3f70645d425d53",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=IpctgL7khPp": {
    "title": "Information-theoretic Online Memory Selection for Continual Learning",
    "abstract": "A challenging problem in task-free continual learning is the online selection of a representative replay memory from data streams. In this work, we investigate the online memory selection problem from an information-theoretic perspective. To gather the most information, we propose the surprise and the learnability criteria to pick informative points and to avoid outliers. We present a Bayesian model to compute the criteria efﬁciently by exploiting rank-one matrix structures. We demonstrate that these criteria encourage selecting informative points in a greedy algorithm for online memory selection. Furthermore, by identifying the importance of the timing to update the memory , we introduce a stochastic information-theoretic reservoir sampler (InfoRS), which conducts sampling among selective points with high information. Compared to reservoir sampling, InfoRS demonstrates improved robustness against data imbalance. Finally, empirical performances over continual learning benchmarks manifest its efﬁciency and efﬁcacy. involves 20 tasks, and transforms the images by task-dependent permutations. The agent attempts to classify the digits without task identities. The other benchmarks split the dataset into disjoint tasks based on labels. For Split MNIST and Split CIFAR10, 10 classes are split into 5 tasks evenly; for Split MiniImageNet, 100 classes are split into 10 tasks evenly. We use a ﬁxed split across all random seeds. The agent attempts to classify the images without task identities either. For the sake of space, we present the results for Permuted MNIST in the Appendix",
    "volume": "main",
    "checked": true,
    "id": "0d8d554d2d7f5b88d2a512036a6838d9e31265f0",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=JSR-YDImK95": {
    "title": "Path Auxiliary Proposal for MCMC in Discrete Space",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "dfd10d51c4e7dd93e93f1cb2459e839e0ac1eb51",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=VGnOJhd5Q1q": {
    "title": "Sparse Attention with Learning to Hash",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "c49c292e1fb1d215c88828a52134b7ccfa52be44",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=JM2kFbJvvI": {
    "title": "Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep RL",
    "abstract": "Evaluating the worst-case performance of a reinforcement learning (RL) agent under the strongest/optimal adversarial perturbations on state observations (within some constraints) is crucial for understanding the robustness of RL agents. However, finding the optimal adversary is challenging, in terms of both whether we can find the optimal attack and how efficiently we can find it. Existing works on adversarial RL either use heuristics-based methods that may not find the strongest adversary, or directly train an RL-based adversary by treating the agent as a part of the environment, which can find the optimal adversary but may become intractable in a large state space. In this paper, we propose a novel attacking algorithm which has an RL-based \"director\" searching for the optimal policy perturbation, and an \"actor\" crafting state perturbations following the directions from the director (i.e. the actor executes targeted attacks). Our proposed algorithm, PA-AD, is theoretically optimal against an RL agent and significantly improves the efficiency compared with prior RL-based works in environments with large or pixel state spaces. Empirical results show that our proposed PA-AD universally outperforms state-of-the-art attacking methods in a wide range of environments. Our method can be easily applied to any RL algorithms to evaluate and improve their robustness",
    "volume": "main",
    "checked": true,
    "id": "4754ad07af3dce5262382ae47e496f694b61f589",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=7KdAoOsI81C": {
    "title": "Transfer RL across Observation Feature Spaces via Model-Based Regularization",
    "abstract": "In many reinforcement learning (RL) applications, the observation space is specified by human developers and restricted by physical realizations, and may thus be subject to dramatic changes over time (e.g. increased number of observable features). However, when the observation space changes, the previous policy will likely fail due to the mismatch of input features, and another policy must be trained from scratch, which is inefficient in terms of computation and sample complexity. Following theoretical insights, we propose a novel algorithm which extracts the latent-space dynamics in the source task, and transfers the dynamics model to the target task to use as a model-based regularizer. Our algorithm works for drastic changes of observation space (e.g. from vector-based observation to image-based observation), without any inter-task mapping or any prior knowledge of the target task. Empirical results show that our algorithm significantly improves the efficiency and stability of learning in the target task",
    "volume": "main",
    "checked": true,
    "id": "38f0c0e8b9218a505ebb5435d1b859b28b9687f0",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=GugZ5DzzAu": {
    "title": "Permutation Compressors for Provably Faster Distributed Nonconvex Optimization",
    "abstract": "We study the MARINA method of Gorbunov et al. (2021) – the current state-of-theart distributed non-convex optimization method in terms of theoretical communication complexity. Theoretical superiority of this method can be largely attributed to two sources: the use of a carefully engineered biased stochastic gradient estimator, which leads to a reduction in the number of communication rounds, and the reliance on independent stochastic communication compression operators, which leads to a reduction in the number of transmitted bits within each communication round. In this paper we i) extend the theory of MARINA to support a much wider class of potentially correlated compressors, extending the reach of the method beyond the classical independent compressors setting, ii) show that a new quantity, for which we coin the name Hessian variance, allows us to significantly refine the original analysis of MARINA without any additional assumptions, and iii) identify a special class of correlated compressors based on the idea of random permutations, for which we coin the term PermK , the use of which leads to O( √ n) (resp. O(1+ d/ √ n)) improvement in the theoretical communication complexity of MARINA in the low Hessian variance regime when d ≥ n (resp. d ≤ n), where n is the number of workers and d is the number of parameters describing the model we are learning. We corroborate our theoretical results with carefully engineered synthetic experiments with minimizing the average of nonconvex quadratics, and on autoencoder training with the MNIST dataset",
    "volume": "main",
    "checked": true,
    "id": "d09e15839a228241c655ad22bab872576cc1b5ca",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=hl9ePdHO4_s": {
    "title": "Do We Need Anisotropic Graph Neural Networks?",
    "abstract": "Graph Benchmark (Hu et al., 2020). These datasets cover a wide range of domains, cover both transductive and inductive tasks, and are larger than datasets which are typically used in GNN works. We use evaluation metrics and splits speciﬁed by these papers. Baseline architectures chosen reﬂect popular general-purpose choices (Kipf & Welling, 2017; Xu et al., 2019; Hamilton et al., 2017; Veli ˇ ckovi ´ c et al., 2018; Gilmer et al., 2017), along with the state-of-the-art PNA (Corso et al., 2020) and GATv2 (Brody et al., 2022) architectures",
    "volume": "main",
    "checked": true,
    "id": "6d0096bc12d61c6f7bca4999bc0c4ce83f44bc72",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=9jInD9JjicF": {
    "title": "PoNet: Pooling Network for Efficient Token Mixing in Long Sequences",
    "abstract": "Transformer-based models have achieved great success in various NLP, vision, and speech tasks. However, the core of Transformer, the self-attention mechanism, has a quadratic time and memory complexity with respect to the sequence length, which hinders applications of Transformer-based models to long sequences. Many approaches have been proposed to mitigate this problem, such as sparse attention mechanisms, low-rank matrix approximations and scalable kernels, and token mixing alternatives to self-attention. We propose a novel Pooling Network (PoNet) for token mixing in long sequences with linear complexity. We design multi-granularity pooling and pooling fusion to capture different levels of contextual information and combine their interactions with tokens. On the Long Range Arena benchmark, PoNet significantly outperforms Transformer and achieves competitive accuracy, while being only slightly slower than the fastest model, FNet, across all sequence lengths measured on GPUs. We also conduct systematic studies on the transfer learning capability of PoNet and observe that PoNet achieves 95.7% of the accuracy of BERT on the GLUE benchmark, outperforming FNet by 4.5% relative. Comprehensive ablation analysis demonstrates effectiveness of the designed multi-granularity pooling and pooling fusion for token mixing in long sequences and efficacy of the designed pre-training tasks for PoNet to learn transferable contextualized language representations. Our implementation is available at https://github.com/lxchtan/PoNet",
    "volume": "main",
    "checked": true,
    "id": "bc1bb29b560a11c883ad8c92a8ff876e4d53e2a0",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=fExcSKdDo_": {
    "title": "Learning to Dequantise with Truncated Flows",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "552e1aa0e0c842ae84b33d67cba297471d4b1048",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=ATUh28lnSuW": {
    "title": "Graph Auto-Encoder via Neighborhood Wasserstein Reconstruction",
    "abstract": "Graph neural networks (GNNs) have drawn significant research attention recently, mostly under the setting of semi-supervised learning. When task-agnostic representations are preferred or supervision is simply unavailable, the auto-encoder framework comes in handy with a natural graph reconstruction objective for unsupervised GNN training. However, existing graph auto-encoders are designed to reconstruct the direct links, so GNNs trained in this way are only optimized towards proximity-oriented graph mining tasks, and will fall short when the topological structures matter. In this work, we revisit the graph encoding process of GNNs which essentially learns to encode the neighborhood information of each node into an embedding vector, and propose a novel graph decoder to reconstruct the entire neighborhood information regarding both proximity and structure via Neighborhood Wasserstein Reconstruction (NWR). Specifically, from the GNN embedding of each node, NWR jointly predicts its node degree and neighbor feature distribution, where the distribution prediction adopts an optimal-transport loss based on the Wasserstein distance. Extensive experiments on both synthetic and real-world network datasets show that the unsupervised node representations learned with NWR have much more advantageous in structure-oriented graph mining tasks, while also achieving competitive performance in proximity-oriented ones.1",
    "volume": "main",
    "checked": true,
    "id": "2ab0630d0a5e209708e03dc6e21a477c3099f282",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=k9bx1EfHI_-": {
    "title": "Self-Supervised Graph Neural Networks for Improved Electroencephalographic Seizure Analysis",
    "abstract": "Automated seizure detection and classification from electroencephalography (EEG) can greatly improve seizure diagnosis and treatment. However, several modeling challenges remain unaddressed in prior automated seizure detection and classification studies: (1) representing non-Euclidean data structure in EEGs, (2) accurately classifying rare seizure types, and (3) lacking a quantitative interpretability approach to measure model ability to localize seizures. In this study, we address these challenges by (1) representing the spatiotemporal dependencies in EEGs using a graph neural network (GNN) and proposing two EEG graph structures that capture the electrode geometry or dynamic brain connectivity, (2) proposing a self-supervised pre-training method that predicts preprocessed signals for the next time period to further improve model performance, particularly on rare seizure types, and (3) proposing a quantitative model interpretability approach to assess a model's ability to localize seizures within EEGs. When evaluating our approach on seizure detection and classification on a large public dataset (5,499 EEGs), we find that our GNN with self-supervised pre-training achieves 0.875 Area Under the Receiver Operating Characteristic Curve on seizure detection and 0.749 weighted F1-score on seizure classification, outperforming previous methods for both seizure detection and classification. Moreover, our self-supervised pre-training strategy significantly improves classification of rare seizure types (e.g. 47 points increase in combined tonic seizure accuracy over baselines). Furthermore, quantitative interpretability analysis shows that our GNN with self-supervised pre-training precisely localizes 25.4% focal seizures, a 21.9 point improvement over existing CNNs. Finally, by superimposing the identified seizure locations on both raw EEG signals and EEG graphs, our approach could provide clinicians with an intuitive visualization of localized seizure regions",
    "volume": "main",
    "checked": true,
    "id": "0a4d5fdfaba62390b25c725badffee524bbcf0a6",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=PDYs7Z2XFGv": {
    "title": "Omni-Scale CNNs: a simple and effective kernel size configuration for time series classification",
    "abstract": "The Receptive Field (RF) size has been one of the most important factors for One Dimensional Convolutional Neural Networks (1D-CNNs) on time series classification tasks. Large efforts have been taken to choose the appropriate size because it has a huge influence on the performance and differs significantly for each dataset. In this paper, we propose an Omni-Scale block (OS-block) for 1D-CNNs, where the kernel sizes are decided by a simple and universal rule. Particularly, it is a set of kernel sizes that can efficiently cover the best RF size across different datasets via consisting of multiple prime numbers according to the length of the time series. The experiment result shows that models with the OS-block can achieve a similar performance as models with the searched optimal RF size and due to the strong optimal RF size capture ability, simple 1D-CNN models with OS-block achieves the state-of-the-art performance on four time series benchmarks, including both univariate and multivariate data from multiple domains. Comprehensive analysis and discussions shed light on why the OS-block can capture optimal RF sizes across different datasets. Code available here 1",
    "volume": "main",
    "checked": true,
    "id": "fe9d361fd7a024507c9d0016f129766f3a3cb19b",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=oJGDYQFKL3i": {
    "title": "Object Dynamics Distillation for Scene Decomposition and Representation",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "88022cc82844753ad95841ed23d18f0995ad198c",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=fR-EnKWL_Zb": {
    "title": "Quadtree Attention for Vision Transformers",
    "abstract": "Transformers have been successful in many vision tasks, thanks to their capability of capturing long-range dependency. However, their quadratic computational complexity poses a major obstacle for applying them to vision tasks requiring dense predictions, such as object detection, feature matching, stereo, etc. We introduce QuadTree Attention, which reduces the computational complexity from quadratic to linear. Our quadtree transformer builds token pyramids and computes attention in a coarse-to-fine manner. At each level, the top K patches with the highest attention scores are selected, such that at the next level, attention is only evaluated within the relevant regions corresponding to these top K patches. We demonstrate that quadtree attention achieves state-of-theart performance in various vision tasks, e.g. with 4.0% improvement in feature matching on ScanNet, about 50% flops reduction in stereo matching, 0.4-1.5% improvement in top-1 accuracy on ImageNet classification, 1.2-1.8% improvement on COCO object detection, and 0.7-2.4% improvement on semantic segmentation over previous state-of-the-art transformers. The codes are available at https://github.com/Tangshitao/QuadtreeAttention",
    "volume": "main",
    "checked": true,
    "id": "0d9b8ccb1135b8e380dd8015b080158c6aae3ae5",
    "citation_count": 17
  },
  "https://openreview.net/forum?id=ZOcX-eybqoL": {
    "title": "Generalisation in Lifelong Reinforcement Learning through Logical Composition",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "ef3a4da272c35fe6dbd71e27385f0a253eacc038",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=f2OYVDyfIB": {
    "title": "Scale Efficiently: Insights from Pretraining and Finetuning Transformers",
    "abstract": "There remain many open questions pertaining to the scaling behaviour of Transformer architectures. These scaling decisions and findings can be critical, as training runs often come with an associated computational cost which have both financial and/or environmental impact. The goal of this paper is to present scaling insights from pretraining and finetuning Transformers. While Kaplan et al. (2020) presents a comprehensive study of the scaling behaviour of Transformer language models, the scope is only on the upstream (pretraining) loss. Therefore, it is still unclear if these set of findings transfer to downstream task within the context of the pretrain-finetune paradigm. The key findings of this paper are as follows: (1) we show that aside from only the model size, model shape matters for downstream fine-tuning, (2) scaling protocols operate differently at different compute regions, (3) widely adopted T5-base and T5-large sizes are Pareto-inefficient. To this end, we present improved scaling protocols whereby our redesigned models achieve similar downstream fine-tuning quality while having 50% fewer parameters and training 40% faster compared to the widely adopted T5-base model. We publicly release over 100 pretrained checkpoints of different T5 configurations to facilitate future research and analysis",
    "volume": "main",
    "checked": true,
    "id": "2d4f66046bb436864cd6bf589e3a931c405f9f44",
    "citation_count": 31
  },
  "https://openreview.net/forum?id=JtBRnrlOEFN": {
    "title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization",
    "abstract": "State-of-the-art models in natural language processing rely on separate rigid subword tokenization algorithms, which limit their generalization ability and adap-tation to new settings. In this paper, we propose a new model inductive bias that learns a subword tokenization end-to-end as part of the model. To this end, we introduce a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion. Concretely, GBST enumerates candidate subword blocks and learns to score them in a position-wise fashion using a block scoring network. We additionally introduce C HARFORMER , a deep Transformer model that integrates GBST and operates on the byte level. Via extensive experiments on English GLUE, multilingual, and noisy text datasets, we show that C HARFORMER outperforms a series of competitive byte-level baselines while generally performing on par and sometimes outperforming subword-based models. Additionally, C HARFORMER is fast, improving the speed of both vanilla byte-level and subword-level Transformers by 28-100% while maintaining competitive quality. We believe this work paves the way for highly performant token-free models that are trained completely end-to-end",
    "volume": "main",
    "checked": true,
    "id": "e79d1206292bc5e67ba19737d87d4b2ea4a37105",
    "citation_count": 39
  },
  "https://openreview.net/forum?id=rS9-7AuPKWK": {
    "title": "Towards Understanding Generalization via Decomposing Excess Risk Dynamics",
    "abstract": "Generalization is one of the critical issues in machine learning. However, traditional methods like uniform convergence are not powerful enough to fully explain generalization because they may yield vacuous bounds even in overparameterized linear regression regimes. An alternative solution is to analyze the generalization dynamics to derive algorithm-dependent bounds, e.g., stability. Unfortunately, the stability-based bound is still far from explaining the remarkable generalization ability of neural networks due to the coarse-grained analysis of the signal and noise. Inspired by the observation that neural networks show a slow convergence rate when fitting noise, we propose decomposing the excess risk dynamics and applying stability-based bound only on the variance part (which measures how the model performs on pure noise). We provide two applications for the framework, including a linear case (overparameterized linear regression with gradient descent) and a non-linear case (matrix recovery with gradient flow). Under the decomposition framework, the new bound accords better with the theoretical and empirical evidence compared to the stability-based bound and uniform convergence bound",
    "volume": "main",
    "checked": true,
    "id": "83add0326380fe4a3324d0eb838693ea4f3213b7",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=w01vBAcewNX": {
    "title": "On Covariate Shift of Latent Confounders in Imitation and Reinforcement Learning",
    "abstract": "We consider the problem of using expert data with unobserved confounders for imitation and reinforcement learning. We begin by defining the problem of learning from confounded expert data in a contextual MDP setup. We analyze the limitations of learning from such data with and without external reward, and propose an adjustment of standard imitation learning algorithms to fit this setup. We then discuss the problem of distribution shift between the expert data and the online environment when the data is only partially observable. We prove possibility and impossibility results for imitation learning under arbitrary distribution shift of the missing covariates. When additional external reward is provided, we propose a sampling procedure that addresses the unknown shift and prove convergence to an optimal solution. Finally, we validate our claims empirically on challenging assistive healthcare and recommender system simulation tasks",
    "volume": "main",
    "checked": true,
    "id": "c3d9aeed8d61a69bfc077a498436c180b529ef13",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=0UXT6PpRpW": {
    "title": "Large-Scale Representation Learning on Graphs via Bootstrapping",
    "abstract": "Self-supervised learning provides a promising path towards eliminating the need for costly label information in representation learning on graphs. However, to achieve state-of-the-art performance, methods often need large numbers of negative examples and rely on complex augmentations. This can be prohibitively expensive, especially for large graphs. To address these challenges, we introduce Bootstrapped Graph Latents (BGRL) a graph representation learning method that learns by predicting alternative augmentations of the input. BGRL uses only simple augmentations and alleviates the need for contrasting with negative examples, and is thus scalable by design. BGRL outperforms or matches prior methods on several established benchmarks, while achieving a 2-10x reduction in memory costs. Furthermore, we show that BGRL can be scaled up to extremely large graphs with hundreds of millions of nodes in the semi-supervised regime achieving state-ofthe-art performance and improving over supervised baselines where representations are shaped only through label information. In particular, our solution centered on BGRL constituted one of the winning entries to the Open Graph Benchmark Large Scale Challenge at KDD Cup 2021, on a graph orders of magnitudes larger than all previously available benchmarks, thus demonstrating the scalability and effectiveness of our approach",
    "volume": "main",
    "checked": true,
    "id": "9141fb25380237bb3db9b9a9fac2ddc50def5ee4",
    "citation_count": 32
  },
  "https://openreview.net/forum?id=zNHzqZ9wrRB": {
    "title": "Equivariant Transformers for Neural Network based Molecular Potentials",
    "abstract": "The prediction of quantum mechanical properties is historically plagued by a trade-off between accuracy and speed. Machine learning potentials have previously shown great success in this domain, reaching increasingly better accuracy while maintaining computational efficiency comparable with classical force fields.In this work we propose TorchMD-NET, a novel equivariant Transformer (ET) architecture, outperforming state-of-the-art on MD17, ANI-1, and many QM9 tar-gets in both accuracy and computational efficiency. Through an extensive attention weight analysis, we gain valuable insights into the black box predictor and show differences in the learned representation of conformers versus conformations sampled from molecular dynamics or normal modes. Furthermore, we highlight the importance of datasets including off-equilibrium conformations for the evaluation of molecular potentials",
    "volume": "main",
    "checked": true,
    "id": "383eb0c99826381a9623b58670b5c85d79fd6a87",
    "citation_count": 19
  },
  "https://openreview.net/forum?id=EnwCZixjSh": {
    "title": "On Evaluation Metrics for Graph Generative Models",
    "abstract": "In image generation, generative models can be evaluated naturally by visually in-specting model outputs. However, this is not always the case for graph generative models (GGMs), making their evaluation challenging. Currently, the standard process for evaluating GGMs suffers from three critical limitations: i) it does not pro-duce a single score which makes model selection challenging, ii) in many cases it fails to consider underlying edge and node features, and iii) it is prohibitively slow to perform. In this work, we mitigate these issues by searching for scalar, domain-agnostic, and scalable metrics for evaluating and ranking GGMs. To this end, we study existing GGM metrics and neural-network-based metrics emerging from generative models of images that use embeddings extracted from a task-speciﬁc network. Motivated by the power of Graph Neural Networks (GNNs) to extract meaningful graph representations without any training , we introduce several metrics based on the features extracted by an untrained random GNN. We design experiments to thoroughly test and objectively score metrics on their ability to measure the diversity and ﬁdelity of generated graphs, as well as their sample and computational efﬁciency. Depending on the quantity of samples, we recommend one of two metrics from our collection of random-GNN-based metrics. We show these two metrics to be more expressive than pre-existing and alternative random-GNN-based metrics using our objective scoring. While we focus on applying these metrics",
    "volume": "main",
    "checked": true,
    "id": "410f0a0a2311c50e6dd2338f2708286ea8c87f23",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=EMxu-dzvJk": {
    "title": "GRAND++: Graph Neural Diffusion with A Source Term",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "5635a74cf4ee75e8d7899232cd7c93d1549589e5",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=pETy-HVvGtt": {
    "title": "Disentanglement Analysis with Partial Information Decomposition",
    "abstract": "We propose a framework to analyze how multivariate representations disentangle ground-truth generative factors. A quantitative analysis of disentanglement has been based on metrics designed to compare how one variable explains each generative factor. Current metrics, however, may fail to detect entanglement that involves more than two variables, e.g., representations that duplicate and rotate generative factors in high dimensional spaces. In this work, we establish a framework to analyze information sharing in a multivariate representation with Partial Information Decomposition and propose a new disentanglement metric. This framework enables us to understand disentanglement in terms of uniqueness, redundancy, and synergy. We develop an experimental protocol to assess how increasingly entangled representations are evaluated with each metric and confirm that the proposed metric correctly responds to entanglement. Through experiments on variational autoencoders, we find that models with similar disentanglement scores have a variety of characteristics in entanglement, for each of which a distinct strategy may be required to obtain a disentangled representation",
    "volume": "main",
    "checked": true,
    "id": "9d31879efaea84a237bd1abec678d3315818a2ef",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=aBO5SvgSt1": {
    "title": "Mirror Descent Policy Optimization",
    "abstract": "We propose deep Reinforcement Learning (RL) algorithms inspired by mirror descent, a well-known first-order trust region optimization method for solving constrained convex problems. Our approach, which we call as Mirror Descent Policy Optimization (MDPO), is based on the idea of iteratively solving a `trust-region' problem that minimizes a sum of two terms: a linearization of the objective function and a proximity term that restricts two consecutive updates to be close to each other. Following this approach we derive on-policy and off-policy variants of the MDPO algorithm and analyze their performance while emphasizing important implementation details, motivated by the existing theoretical framework. We highlight the connections between on-policy MDPO and two popular trust region RL algorithms: TRPO and PPO, and conduct a comprehensive empirical comparison of these algorithms. We then derive off-policy MDPO and compare its performance to existing approaches. Importantly, we show that the theoretical framework of MDPO can be scaled to deep RL while achieving good performance on popular benchmarks",
    "volume": "main",
    "checked": true,
    "id": "08eb40da621640e14ffac36c5e2595d7c0250541",
    "citation_count": 35
  },
  "https://openreview.net/forum?id=26gKg6x-ie": {
    "title": "Adversarial Support Alignment",
    "abstract": "We study the problem of aligning the supports of distributions. Compared to the existing work on distribution alignment, support alignment does not require the densities to be matched. We propose symmetric support difference as a divergence measure to quantify the mismatch between supports. We show that select discrimi-nators (e.g. discriminator trained for Jensen–Shannon divergence) are able to map support differences as support differences in their one-dimensional output space. Following this result, our method aligns supports by minimizing a symmetrized relaxed optimal transport cost in the discriminator 1D space via an adversarial process. Furthermore, we show that our approach can be viewed as a limit of existing notions of alignment by increasing transportation assignment tolerance. We quantitatively evaluate the method across domain adaptation tasks with shifts in label distributions. Our experiments 1 show that the proposed method is more robust against these shifts than other alignment-based baselines",
    "volume": "main",
    "checked": true,
    "id": "cce3a7980d6afee9ce7156df8aa76aabd0d9b918",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=7UmjRGzp-A": {
    "title": "Understanding over-squashing and bottlenecks on graphs via curvature",
    "abstract": "Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from distant nodes as a factor limiting the efficiency of message passing for tasks relying on long-distance interactions. This phenomenon, referred to as ‘over-squashing', has been heuristically attributed to graph bottlenecks where the number of k-hop neighbors grows rapidly with k. We provide a precise description of the over-squashing phenomenon in GNNs and analyze how it arises from bottlenecks in the graph. For this purpose, we introduce a new edge-based combinatorial curvature and prove that negatively curved edges are responsible for the over-squashing issue. We also propose and experimentally test a curvature-based graph rewiring method to alleviate the over-squashing",
    "volume": "main",
    "checked": true,
    "id": "1a08231d539e70db109c4a5e06821687f00a5377",
    "citation_count": 55
  },
  "https://openreview.net/forum?id=8eb12UQYxrG": {
    "title": "The Role of Pretrained Representations for the OOD Generalization of RL Agents",
    "abstract": "Building sample-efficient agents that generalize out-of-distribution (OOD) in real-world settings remains a fundamental unsolved problem on the path towards achieving higher-level cognition. One particularly promising approach is to begin with low-dimensional, pretrained representations of our world, which should facilitate efficient downstream learning and generalization. By training 240 representations and over 10,000 reinforcement learning policies on a simulated robotic setup, we evaluate to what extent different properties of pretrained VAE-based representations affect the OOD generalization of downstream agents. We observe that many agents are surprisingly robust to realistic distribution shifts, including the challenging sim-to-real case. In addition, we find that the generalization performance of a simple downstream proxy task reliably predicts the generalization performance of our reinforcement learning control tasks under a wide range of practically relevant OOD settings. Such proxy tasks can thus be used to select pretrained representations that will lead to agents that generalize out-of-distribution",
    "volume": "main",
    "checked": true,
    "id": "1cb4c822ba7710159c62911e30bddb7d3662fbb4",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=MSwEFaztwkE": {
    "title": "Learning Weakly-supervised Contrastive Representations",
    "abstract": "We argue that a form of the valuable information provided by the auxiliary information is its implied data clustering information. For instance, considering hashtags as auxiliary information, we can hypothesize that an Instagram image will be semantically more similar with the same hashtags. With this intuition, we present a two-stage weakly-supervised contrastive learning approach. The first stage is to cluster data according to its auxiliary information. The second stage is to learn similar representations within the same cluster and dissimilar representations for data from different clusters. Our empirical experiments suggest the following three contributions. First, compared to conventional self-supervised representations, the auxiliary-information-infused representations bring the performance closer to the supervised representations, which use direct downstream labels as supervision signals. Second, our approach performs the best in most cases, when comparing our approach with other baseline representation learning methods that also leverage auxiliary data information. Third, we show that our approach also works well with unsupervised constructed clusters (e.g., no auxiliary information), resulting in a strong unsupervised representation learning approach",
    "volume": "main",
    "checked": true,
    "id": "f581629c86f1769f6e4d65c5497c5ea7a97408e5",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=AAJLBoGt0XM": {
    "title": "Conditional Contrastive Learning with Kernel",
    "abstract": "Conditional contrastive learning frameworks consider the conditional sampling procedure that constructs positive or negative data pairs conditioned on specific variables. Fair contrastive learning constructs negative pairs, for example, from the same gender (conditioning on sensitive information), which in turn reduces undesirable information from the learned representations; weakly supervised contrastive learning constructs positive pairs with similar annotative attributes (conditioning on auxiliary information), which in turn are incorporated into the representations. Although conditional contrastive learning enables many applications, the conditional sampling procedure can be challenging if we cannot obtain sufficient data pairs for some values of the conditioning variable. This paper presents Conditional Contrastive Learning with Kernel (CCL-K) that converts existing conditional contrastive objectives into alternative forms that mitigate the insufficient data problem. Instead of sampling data according to the value of the conditioning variable, CCLK uses the Kernel Conditional Embedding Operator that samples data from all available data and assigns weights to each sampled data given the kernel similarity between the values of the conditioning variable. We conduct experiments using weakly supervised, fair, and hard negatives contrastive learning, showing CCL-K outperforms state-of-the-art baselines",
    "volume": "main",
    "checked": true,
    "id": "9e9dc4b54b20882f871cf3b1438df33162bb5414",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=q4HaTeMO--y": {
    "title": "Declarative nets that are equilibrium models",
    "abstract": "Implicit layers are computational modules that output the solution to some problem depending on the input and the layer parameters. Deep equilibrium models (DEQs) output a solution to a fixed point equation. Deep declarative networks (DDNs) solve an optimisation problem in their forward pass, an arguably more intuitive, interpretable problem than finding a fixed point. We show that solving a kernelised regularised maximum likelihood estimate as an inner problem in a DDN yields a large class of DEQ architectures. Our proof uses the exponential family in canonical form, and provides a closed-form expression for the DEQ parameters in terms of the kernel. The activation functions have interpretations in terms of the derivative of the log partition function. Building on existing literature, we interpret DEQs as fine-tuned, unrolled classical algorithms, giving an intuitive justification for why DEQ models are sensible. We use our theoretical result to devise an initialisation scheme for DEQs that allows them to solve kGLMs in their forward pass at initialisation. We empirically show that this initialisation scheme improves training stability and performance over random initialisation",
    "volume": "main",
    "checked": true,
    "id": "120d537965ef4c0eb560c1bce866ae9815065cd5",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=qwBK94cP1y": {
    "title": "Optimal Transport for Causal Discovery",
    "abstract": "To determine causal relationships between two variables, approaches based on Functional Causal Models (FCMs) have been proposed by properly restricting model classes; however, the performance is sensitive to the model assumptions, which makes it difficult to use. In this paper, we provide a novel dynamical-system view of FCMs and propose a new framework for identifying causal direction in the bivariate case. We first show the connection between FCMs and optimal transport, and then study optimal transport under the constraints of FCMs. Furthermore, by exploiting the dynamical interpretation of optimal transport under the FCM constraints, we determine the corresponding underlying dynamical process of the static cause-effect pair data. It provides a new dimension for describing static causal discovery tasks while enjoying more freedom for modeling the quantitative causal influences. In particular, we show that Additive Noise Models (ANMs) correspond to volume-preserving pressureless flows. Consequently, based on their velocity field divergence, we introduce a criterion for determining causal direction. With this criterion, we propose a novel optimal transport-based algorithm for ANMs which is robust to the choice of models and extend it to post-nonlinear models. Our method demonstrated state-of-the-art results on both synthetic and causal discovery benchmark datasets",
    "volume": "main",
    "checked": true,
    "id": "ee2ee79708fdc3f37924a68993c0f29d4d247fbb",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=Ek7PSN7Y77z": {
    "title": "Multi-Stage Episodic Control for Strategic Exploration in Text Games",
    "abstract": "Text adventure games present unique challenges to reinforcement learning methods due to their combinatorially large action spaces and sparse rewards. The interplay of these two factors is particularly demanding because large action spaces require extensive exploration, while sparse rewards provide limited feedback. This work proposes to tackle the explore-vs-exploit dilemma using a multi-stage approach that explicitly disentangles these two strategies within each episode. Our algorithm, called eXploit-Then-eXplore (XTX), begins each episode using an exploitation policy that imitates a set of promising trajectories from the past, and then switches over to an exploration policy aimed at discovering novel actions that lead to unseen state spaces. This policy decomposition allows us to combine global decisions about which parts of the game space to return to with curiosity-based local exploration in that space, motivated by how a human may approach these games. Our method significantly outperforms prior approaches by 27% and 11% average normalized score over 12 games from the Jericho benchmark (Hausknecht et al., 2020) in both deterministic and stochastic settings, respectively. On the game of Zork1, in particular, XTX obtains a score of 103, more than a 2x improvement over prior methods, and pushes past several known bottlenecks in the game that have plagued previous state-of-the-art methods",
    "volume": "main",
    "checked": true,
    "id": "3b61bc41dff751edbead03aab5e4a1da1aafcc06",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=tyrJsbKAe6": {
    "title": "Pessimistic Model-based Offline Reinforcement Learning under Partial Coverage",
    "abstract": "We study model-based offline Reinforcement Learning with general function approximation without a full coverage assumption on the offline data distribution. We present an algorithm named Constrained Pessimistic Policy Optimization (CPPO) which leverages a general function class and uses a constraint over the model class to encode pessimism. Under the assumption that the ground truth model belongs to our function class (i.e., realizability in the function class), CPPO has a PAC guarantee with offline data only providing partial coverage, i.e., it can learn a policy that competes against any policy that is covered by the offline data. We then demonstrate that this algorithmic framework can be applied to many specialized Markov Decision Processes where additional structural assumptions can further refine the concept of partial coverage. Two notable examples are: (1) low-rank MDP with representation learning where the partial coverage condition is defined using a relative condition number measured by the unknown ground truth feature representation; (2) factored MDP where the partial coverage condition is defined using density ratio based concentrability coefficients associated with individual factors",
    "volume": "main",
    "checked": true,
    "id": "01ddfec6421f93da58f21f7e52043b9885b5da1e",
    "citation_count": 27
  },
  "https://openreview.net/forum?id=J4iSIR9fhY0": {
    "title": "Representation Learning for Online and Offline RL in Low-rank MDPs",
    "abstract": "This work studies the question of Representation Learning in RL: how can we learn a compact low-dimensional representation such that on top of the representation we can perform RL procedures such as exploration and exploitation, in a sample efficient manner. We focus on the low-rank Markov Decision Processes (MDPs) where the transition dynamics correspond to a low-rank transition matrix. Unlike prior works that assume the representation is known (e.g., linear MDPs), here we need to learn the representation for the low-rank MDP. We study both the online RL and offline RL settings. For the online setting, operating with the same computational oracles used in FLAMBE(Agarwal et al., 2020b)—-the state-of-art algorithm for learning representations in low-rank MDPs, we propose an algorithm REP-UCB—Upper Confidence Bound driven REPresentation learning for RL, which significantly improves the sample complexity from Õ(Ad/( (1 − γ))) for FLAMBE to Õ(Ad/( (1 − γ))) with d being the rank of the transition matrix (or dimension of the ground truth representation), A being the number of actions, and γ being the discount factor. Notably, REP-UCB is simpler than FLAMBE, as it directly balances the interplay between representation learning, exploration, and exploitation, while FLAMBE is an explore-then-commit style approach and has to perform reward-free exploration step-by-step forward in time. For the offline RL setting, we develop an algorithm that leverages pessimism to learn under a partial coverage condition: our algorithm is able to compete against any policy (including non-Markovian history dependent ones) as long as it is covered by the offline data distribution",
    "volume": "main",
    "checked": true,
    "id": "8d16ffb11c7b62181146db43296852424426a3cd",
    "citation_count": 34
  },
  "https://openreview.net/forum?id=P1QUVhOtEFP": {
    "title": "Topologically Regularized Data Embeddings",
    "abstract": "Unsupervised feature learning often finds low-dimensional embeddings that capture the structure of complex data. For tasks for which prior expert topological knowledge is available, incorporating this into the learned representation may lead to higher quality embeddings. For example, this may help one to embed the data into a given number of clusters, or to accommodate for noise that prevents one from deriving the distribution of the data over the model directly, which can then be learned more effectively. However, a general tool for integrating different prior topological knowledge into embeddings is lacking. Although differentiable topology layers have been recently developed that can (re)shape embeddings into prespecified topological models, they have two important limitations for representation learning, which we address in this paper. First, the currently suggested topological losses fail to represent simple models such as clusters and flares in a natural manner. Second, these losses neglect all original structural (such as neighborhood) information in the data that is useful for learning. We overcome these limitations by introducing a new set of topological losses, and proposing their usage as a way for topologically regularizing data embeddings to naturally represent a prespecified model. We include thorough experiments on synthetic and real data that highlight the usefulness and versatility of this approach, with applications ranging from modeling high-dimensional single-cell data, to graph embedding",
    "volume": "main",
    "checked": true,
    "id": "7cf25d2b572acd3734c0439eb4e12e1502075b41",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=MkTPtnjeYTV": {
    "title": "On the Optimal Memorization Power of ReLU Neural Networks",
    "abstract": "We study the memorization power of feedforward ReLU neural networks. We show that such networks can memorize any N points that satisfy a mild separability assumption using Õ (√ N ) parameters. Known VC-dimension upper bounds imply that memorizing N samples requires Ω( √ N) parameters, and hence our construction is optimal up to logarithmic factors. We also give a generalized construction for networks with depth bounded by 1 ≤ L ≤ √ N , for memorizing N samples using Õ(N/L) parameters. This bound is also optimal up to logarithmic factors. Our construction uses weights with large bit complexity. We prove that having such a large bit complexity is both necessary and sufficient for memorization with a sub-linear number of parameters",
    "volume": "main",
    "checked": true,
    "id": "ded2a06120007faacd9bdd4ff39fec65b5756b44",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=5hLP5JY9S2d": {
    "title": "Open-Set Recognition: A Good Closed-Set Classifier is All You Need",
    "abstract": "The ability to identify whether or not a test sample belongs to one of the semantic classes in a classiﬁer's training set is critical to practical deployment of the model. This task is termed open-set recognition (OSR) and has received signiﬁcant attention in recent years. In this paper, we ﬁrst demonstrate that the ability of a classiﬁer to make the ‘none-of-above' decision is highly correlated with its accuracy on the closed-set classes. We ﬁnd that this relationship holds across loss objectives and architectures, and further demonstrate the trend both on the standard OSR benchmarks as well as on a large-scale ImageNet evaluation. Second, we use this correlation to boost the performance of a maximum logit score OSR ‘baseline' by improving its closed-set accuracy, and with this strong baseline achieve state-of-the-art on a number of OSR benchmarks. Similarly, we boost the performance of the existing state-of-the-art method by improving its closed-set accuracy, but the resulting discrepancy with the strong baseline is marginal. Our third contribution is to present the ‘Semantic Shift Benchmark' (SSB), which better respects the task of detecting semantic novelty, in contrast to other forms of distribution shift also considered in related sub-ﬁelds, such as out-of-distribution detection. On this new evaluation, we again demonstrate that there is negligible difference between the strong baseline and the existing state-of-the-art. Project",
    "volume": "main",
    "checked": true,
    "id": "8d54db5ec8f2719fb37d98cb1d47ff48838c0370",
    "citation_count": 44
  },
  "https://openreview.net/forum?id=ZKy2X3dgPA": {
    "title": "It Takes Two to Tango: Mixup for Deep Metric Learning",
    "abstract": "Metric learning involves learning a discriminative representation such that embeddings of similar classes are encouraged to be close, while embeddings of dissimilar classes are pushed far apart. State-of-the-art methods focus mostly on sophisticated loss functions or mining strategies. On the one hand, metric learning losses consider two or more examples at a time. On the other hand, modern data augmentation methods for classification consider two or more examples at a time. The combination of the two ideas is under-studied. In this work, we aim to bridge this gap and improve representations using mixup, which is a powerful data augmentation approach interpolating two or more examples and corresponding target labels at a time. This task is challenging because unlike classification, the loss functions used in metric learning are not additive over examples, so the idea of interpolating target labels is not straightforward. To the best of our knowledge, we are the first to investigate mixing examples and target labels for deep metric learning. We develop a generalized formulation that encompasses existing metric learning loss functions and modify it to accommodate for mixup, introducing Metric Mix, or Metrix. We show that mixing inputs, intermediate representations or embeddings along with target labels significantly improves representations and outperforms state-of-the-art metric learning methods on four benchmark datasets",
    "volume": "main",
    "checked": true,
    "id": "a7721bf626f4996117dbb88b385be2e12462e7e6",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=EHaUTlm2eHg": {
    "title": "Policy Gradients Incorporating the Future",
    "abstract": "Reasoning about the future – understanding how decisions in the present time affect outcomes in the future – is one of the central challenges for reinforcement learning (RL), especially in highly-stochastic or partially observable environments. While predicting the future directly is hard, in this work we introduce a method that allows an agent to \"look into the future\" without explicitly predicting it. Namely, we propose to allow an agent, during its training on past experience, to observe what actually happened in the future at that time, while enforcing an information bottleneck to avoid the agent overly relying on this privileged information. This gives our agent the opportunity to utilize rich and useful information about the future trajectory dynamics in addition to the present. Our method, Policy Gradients Incorporating the Future (PGIF), is easy to implement and versatile, being applicable to virtually any policy gradient algorithm. We apply our proposed method to a number of off-the-shelf RL algorithms and show that PGIF is able to achieve higher reward faster in a variety of online and offline RL domains, as well as sparse-reward and partially observable environments",
    "volume": "main",
    "checked": true,
    "id": "32bcee7bafa179cf468b3ec0bd66a2c91104573c",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=-Gk_IPJWvk": {
    "title": "Top-N: Equivariant Set and Graph Generation without Exchangeability",
    "abstract": "This work addresses one-shot set and graph generation, and, more speciﬁcally, the parametrization of probabilistic decoders that map a vector-shaped prior to a distribution over sets or graphs. Sets and graphs are most commonly generated by ﬁrst sampling points i.i.d. from a normal distribution, and then processing these points along with the prior vector using Transformer layers or Graph Neural Networks. This architecture is designed to generate exchangeable distributions, i.e., all permutations of the generated outputs are equally likely. We however show that it only optimizes a proxy to the evidence lower bound, which makes it hard to train. We then study equivariance in generative settings and show that non-exchangeable methods can still achieve permutation equivariance. Using this result, we introduce Top-n creation , a differentiable generation mechanism that uses the latent vector to select the most relevant points from a trainable reference set. Top-n can replace i.i.d. generation in any Variational Autoencoder or Generative Adversarial Network. Experimentally, our method outperforms i.i.d. generation by 15% at SetMNIST reconstruction, by 33% at object detection on CLEVR, generates sets that are 74% closer to the true distribution on a synthetic molecule-like dataset, and generates more valid molecules on QM9",
    "volume": "main",
    "checked": true,
    "id": "07e503916bf4881c2b65336ca1f8f5a81d7f3b9c",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=RShaMexjc-x": {
    "title": "Semi-relaxed Gromov-Wasserstein divergence and applications on graphs",
    "abstract": "Comparing structured objects such as graphs is a fundamental operation involved in many learning tasks. To this end, the Gromov-Wasserstein (GW) distance, based on Optimal Transport (OT), has proven to be successful in handling the specific nature of the associated objects. More specifically, through the nodes connectivity relations, GW operates on graphs, seen as probability measures over specific spaces. At the core of OT is the idea of conservation of mass, which imposes a coupling between all the nodes from the two considered graphs. We argue in this paper that this property can be detrimental for tasks such as graph dictionary or partition learning, and we relax it by proposing a new semi-relaxed Gromov-Wasserstein divergence. Aside from immediate computational benefits, we discuss its properties, and show that it can lead to an efficient graph dictionary learning algorithm. We empirically demonstrate its relevance for complex tasks on graphs such as partitioning, clustering and completion",
    "volume": "main",
    "checked": true,
    "id": "85569e22a03af50af7d320019653f459f0da9b83",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=Fl3Mg_MZR-": {
    "title": "On Lottery Tickets and Minimal Task Representations in Deep Reinforcement Learning",
    "abstract": "The lottery ticket hypothesis questions the role of overparameterization in supervised deep learning. But how is the performance of winning lottery tickets af-fected by the distributional shift inherent to reinforcement learning problems? In this work, we address this question by comparing sparse agents who have to address the non-stationarity of the exploration-exploitation problem with supervised agents trained to imitate an expert. We show that feed-forward networks trained with behavioural cloning compared to reinforcement learning can be pruned to higher levels of sparsity without performance degradation. This suggests that in order to solve the RL problem agents require more degrees of freedom. Using a set of carefully designed baseline conditions, we ﬁnd that the majority of the lottery ticket effect in both learning paradigms can be attributed to the identiﬁed mask rather than the weight initialization. The input layer mask selectively prunes entire input dimensions that turn out to be irrelevant for the task at hand. At a moderate level of sparsity the mask identiﬁed by iterative magnitude pruning yields minimal task-relevant representations, i.e., an interpretable inductive bias. Finally, we propose a simple initialization rescaling which promotes the robust identiﬁcation of sparse task representations in low-dimensional control tasks",
    "volume": "main",
    "checked": true,
    "id": "054756e00b52512b432c1aae9b847f84aa6b64c9",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=73MEhZ0anV": {
    "title": "Query Efficient Decision Based Sparse Attacks Against Black-Box Deep Learning Models",
    "abstract": "Despite efforts, deep learning models remain highly vulnerable to even tiny adversarial perturbations applied to the inputs. The ability to extract information from solely the output of a machine learning model to craft adversarial perturbations to black-box models is a practical threat against real-world systems, such as Machine Learning as a Service (MLaaS), particularly sparse attacks . The realisation of sparse attacks in black-box settings demonstrates that machine learning models are more vulnerable than we believe. Because, these attacks aim to minimize a number of perturbed pixels —measured by l 0 norm—required to mislead a model by solely observing the decision ( the predicted label ) returned to a model query; the so-called decision-based setting . But, such an attack leads to an NP-hard optimization problem. We develop an evolution-based algorithm— SparseEvo —for the problem and evaluate it against both convolutional deep neural networks and vision transformers . Notably, vision transformers are yet to be investigated under a decision-based setting. SparseEvo requires signiﬁcantly fewer queries than the state-of-the-art sparse attack Pointwise for both untargeted and targeted attacks. The attack algorithm, although conceptually simple, is compet-itive with only a limited query budget against the state-of-the-art gradient-based whitebox attacks in standard computer vision tasks such as ImageNet . Impor-tantly, the query efﬁcient SparseEvo, along with decision-based attacks, in general, raises new questions regarding the safety of deployed systems and poses to and understand the of machine learning models",
    "volume": "main",
    "checked": true,
    "id": "ad4e93187347c51ef25dea361a3f205262b684c3",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=4-D6CZkRXxI": {
    "title": "Value Gradient weighted Model-Based Reinforcement Learning",
    "abstract": "Model-based reinforcement learning (MBRL) is a sample efﬁcient technique to obtain control policies, yet unavoidable modeling errors often lead performance deterioration. The model in MBRL is often solely ﬁtted to reconstruct dynamics, state observations in particular, while the impact of model error on the policy is not captured by the training objective. This leads to a mismatch between the intended goal of MBRL, enabling good policy and value learning, and the target of the loss function employed in practice, future state prediction. Naive intuition suggests that value-aware model learning would ﬁx this problem and, indeed, several solutions to this objective mismatch problem have been proposed based on theoretical analysis. However, they tend to be inferior in practice to commonly used maximum likelihood (MLE) based approaches. In this paper we propose the Value-Gradient weighted Model loss (VaGraM), a novel method for value-aware model learning which improves the performance of MBRL in challenging settings, such as small model capacity and the presence of distracting state dimensions. We analyze both MLE and value-aware approaches and demonstrate how they fail to account for exploration and the behavior of function approximation when learning value-aware models and highlight the additional goals that must be met to stabilize optimization in the deep learning setting. To achieve this, we leverage the gradient of the empirical value function as a measure of the sensitivity of the RL algorithm to model errors. We verify our analysis by showing that our loss function is able to achieve high returns on the Mujoco benchmark suite while being more robust than maximum likelihood based approaches",
    "volume": "main",
    "checked": true,
    "id": "5fc31b0b091789d0d5449808993714320a8d2957",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=9NVd-DMtThY": {
    "title": "Distributionally Robust Fair Principal Components via Geodesic Descents",
    "abstract": "Principal component analysis is a simple yet useful dimensionality reduction technique in modern machine learning pipelines. In consequential domains such as college admission, healthcare and credit approval, it is imperative to take into account emerging criteria such as the fairness and the robustness of the learned projection. In this paper, we propose a distributionally robust optimization problem for principal component analysis which internalizes a fairness criterion in the objective function. The learned projection thus balances the trade-off between the total reconstruction error and the reconstruction error gap between subgroups, taken in the min-max sense over all distributions in a moment-based ambiguity set. The resulting optimization problem over the Stiefel manifold can be efficiently solved by a Riemannian subgradient descent algorithm with a sub-linear convergence rate. Our experimental results on real-world datasets show the merits of our proposed method over state-of-the-art baselines",
    "volume": "main",
    "checked": true,
    "id": "bbf64d3561c3dd89c254483bf10facdfc322907f",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=P7OVkHEoHOZ": {
    "title": "Hindsight Foresight Relabeling for Meta-Reinforcement Learning",
    "abstract": "Meta-reinforcement learning (meta-RL) algorithms allow for agents to learn new behaviors from small amounts of experience, mitigating the sample inefﬁciency problem in RL. However, while meta-RL agents can adapt quickly to new tasks at test time after experiencing only a few trajectories, the meta-training process is still sample-inefﬁcient. Prior works have found that in the multi-task RL setting, relabeling past transitions and thus sharing experience among tasks can improve sample efﬁciency and asymptotic performance. We apply this idea to the meta-RL setting and devise a new relabeling method called Hindsight Foresight Relabeling (HFR). We construct a relabeling distribution using the combination of hindsight , which is used to relabel trajectories using reward functions from the training task distribution, and foresight , which takes the relabeled trajectories and computes the utility of each trajectory for each task. HFR is easy to implement and readily com-patible with existing meta-RL algorithms. We ﬁnd that HFR improves performance when compared to other relabeling methods on a variety of meta-RL tasks 1 ",
    "volume": "main",
    "checked": true,
    "id": "4ab9a90bc1ce86359b69d9cf8d005e7de89985f7",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=-llS6TiOew": {
    "title": "Fairness in Representation for Multilingual NLP: Insights from Controlled Experiments on Conditional Language Modeling",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "7a670e9c4cf6655cfbcacf169565d4d645c0d475",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=N0n_QyQ5lBF": {
    "title": "Unsupervised Vision-Language Grammar Induction with Shared Structure Modeling",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "89a5b94a0b5c8dbb5633ae21ff9f956372f7a53b",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=kSwqMH0zn1F": {
    "title": "PipeGCN: Efficient Full-Graph Training of Graph Convolutional Networks with Pipelined Feature Communication",
    "abstract": "Graph Convolutional Networks (GCNs) is the state-of-the-art method for learning graph-structured data, and training large-scale GCNs requires distributed training across multiple accelerators such that each accelerator is able to hold a partitioned subgraph. However, distributed GCN training incurs prohibitive overhead of communicating node features and feature gradients among partitions for every GCN layer during each training iteration, limiting the achievable training efficiency and model scalability. To this end, we propose PipeGCN, a simple yet effective scheme that hides the communication overhead by pipelining inter-partition communication with intra-partition computation. It is non-trivial to pipeline for efficient GCN training, as communicated node features/gradients will become stale and thus can harm the convergence, negating the pipeline benefit. Notably, little is known regarding the convergence rate of GCN training with both stale features and stale feature gradients. This work not only provides a theoretical convergence analysis but also finds the convergence rate of PipeGCN to be close to that of the vanilla distributed GCN training without any staleness. Furthermore, we develop a smoothing method to further improve PipeGCN's convergence. Extensive experiments show that PipeGCN can largely boost the training throughput (1.7x~28.5x) while achieving the same accuracy as its vanilla counterpart and existing full-graph training methods. The code is available at https://github.com/RICE-EIC/PipeGCN",
    "volume": "main",
    "checked": true,
    "id": "286d371febea99ec19044e69e163e8bd53137a7f",
    "citation_count": 12
  },
  "https://openreview.net/forum?id=rFJWoYoxrDB": {
    "title": "On Redundancy and Diversity in Cell-based Neural Architecture Search",
    "abstract": "Searching for the architecture cells is a dominant paradigm in NAS. However, little attention has been devoted to the analysis of the cell-based search spaces even though it is highly important for the continual development of NAS. In this work, we conduct an empirical post-hoc analysis of architectures from the popular cellbased search spaces and find that the existing search spaces contain a high degree of redundancy: the architecture performance is minimally sensitive to changes at large parts of the cells, and universally adopted designs, like the explicit search for a reduction cell, significantly increase the complexities but have very limited impact on the performance. Across architectures found by a diverse set of search strategies, we consistently find that the parts of the cells that do matter for architecture performance often follow similar and simple patterns. By explicitly constraining cells to include these patterns, randomly sampled architectures can match or even outperform the state of the art. These findings cast doubts into our ability to discover truly novel architectures in the existing cell-based search spaces, and inspire our suggestions for improvement to guide future NAS research. Code is available at https: //github.com/xingchenwan/cell-based-NAS-analysis",
    "volume": "main",
    "checked": true,
    "id": "820a2446977e410b34d17dd73b3fe40d928db97d",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=QjOQkpzKbNk": {
    "title": "Distilling GANs with Style-Mixed Triplets for X2I Translation with Limited Data",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "2e1f585dedc7eef1e47c31d07c7bb7d767d75fb4",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=oU3aTsmeRQV": {
    "title": "Self-ensemble Adversarial Training for Improved Robustness",
    "abstract": "Due to numerous breakthroughs in real-world applications brought by machine intelligence, deep neural networks (DNNs) are widely employed in critical applications. However, predictions of DNNs are easily manipulated with imperceptible adversarial perturbations, which impedes the further deployment of DNNs and may result in profound security and privacy implications. By incorporating adversarial samples into the training data pool, adversarial training is the strongest principled strategy against various adversarial attacks among all sorts of defense methods. Recent works mainly focus on developing new loss functions or regularizers, attempting to find the unique optimal point in the weight space. But none of them taps the potentials of classifiers obtained from standard adversarial training, especially states on the searching trajectory of training. In this work, we are dedicated to the weight states of models through the training process and devise a simple but powerful Self-Ensemble Adversarial Training (SEAT) method for yielding a robust classifier by averaging weights of history models. This considerably improves the robustness of the target model against several well known adversarial attacks, even merely utilizing the naive cross-entropy loss to supervise. We also discuss the relationship between the ensemble of predictions from different adversarially trained models and the prediction of weight-ensembled models, as well as provide theoretical and empirical evidence that the proposed self-ensemble method provides a smoother loss landscape and better robustness than both individual models and the ensemble of predictions from different classifiers. We further analyze a subtle but fatal issue in the general settings for the self-ensemble model, which causes the deterioration of the weight-ensembled method in the late phases",
    "volume": "main",
    "checked": true,
    "id": "a35bd2ddff96d876a0462acfbab0c3714dd906d0",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=6Q52pZ-Th7N": {
    "title": "Pseudo-Labeled Auto-Curriculum Learning for Semi-Supervised Keypoint Localization",
    "abstract": "Localizing keypoints of an object is a basic visual problem. However, supervised learning of a keypoint localization network often requires a large amount of data, which is expensive and time-consuming to obtain. To remedy this, there is an ever-growing interest in semi-supervised learning (SSL), which leverages a small set of labeled data along with a large set of unlabeled data. Among these SSL approaches, pseudo-labeling (PL) is one of the most popular. PL approaches apply pseudo-labels to unlabeled data, and then train the model with a combination of the labeled and pseudo-labeled data iteratively. The key to the success of PL is the selection of high-quality pseudo-labeled samples. Previous works mostly select training samples by manually setting a single confidence threshold. We propose to automatically select reliable pseudo-labeled samples with a series of dynamic thresholds, which constitutes a learning curriculum. Extensive experiments on six keypoint localization benchmark datasets demonstrate that the proposed approach significantly outperforms the previous state-of-the-art SSL approaches",
    "volume": "main",
    "checked": true,
    "id": "ada6d6057e232ba86d050e7c5b8ff2611ce1cd4c",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=2ggNjUisGyr": {
    "title": "Partial Wasserstein Adversarial Network for Non-rigid Point Set Registration",
    "abstract": "Given two point sets, the problem of registration is to recover a transformation that matches one set to the other. This task is challenging due to the presence of the large number of outliers, the unknown non-rigid deformations and the large sizes of point sets. To obtain strong robustness against outliers, we formulate the registration problem as a partial distribution matching (PDM) problem, where the goal is to partially match the distributions represented by point sets in a metric space. To handle large point sets, we propose a scalable PDM algorithm by utilizing the efficient partial Wasserstein-1 (PW) discrepancy. Specifically, we derive the Kantorovich-Rubinstein duality for the PW discrepancy, and show its gradient can be explicitly computed. Based on these results, we propose a partial Wasserstein adversarial network (PWAN), which is able to approximate the PW discrepancy by a neural network, and minimize it by gradient descent. In addition, it also incorporates an efficient coherence regularizer for non-rigid transformations to avoid unrealistic deformations. We evaluate PWAN on practical point set registration tasks, and show that the proposed PWAN is robust, scalable and performs more favorably than the state-of-the-art methods",
    "volume": "main",
    "checked": true,
    "id": "a353c993af7fc69b9598d204bd3b5756a300641e",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=uqBOne3LUKy": {
    "title": "Is Importance Weighting Incompatible with Interpolating Classifiers?",
    "abstract": "Importance weighting is a classic technique to handle distribution shifts. However, prior work has presented strong empirical and theoretical evidence demonstrating that importance weights can have little to no effect on overparameterized neural networks. Is importance weighting truly incompatible with the training of overparameterized neural networks? Our paper answers this in the negative. We show that importance weighting fails not because of the overparameterization, but instead, as a result of using exponentially-tailed losses like the logistic or cross-entropy loss. As a remedy, we show that polynomially-tailed losses restore the effects of importance reweighting in correcting distribution shift in overparameterized models. We characterize the behavior of gradient descent on importance weighted polynomially-tailed losses with overparameterized linear models, and theoretically demonstrate the advantage of using polynomially-tailed losses in a label shift setting. Surprisingly, our theory shows that using weights that are obtained by exponentiating the classical unbiased importance weights can improve performance. Finally, we demonstrate the practical value of our analysis with neural network experiments on a subpopulation shift and a label shift dataset. When reweighted, our loss function can outperform reweighted cross-entropy by as much as 9% in test accuracy. Our loss function also gives test accuracies comparable to, or even exceeding, well-tuned state-of-the-art methods for correcting distribution shifts",
    "volume": "main",
    "checked": true,
    "id": "a4e3a467804781a3c5a427f4032bf180ea8bf585",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=P3Bh01hBYTH": {
    "title": "X-model: Improving Data Efficiency in Deep Learning with A Minimax Model",
    "abstract": "To mitigate the burden of data labeling, we aim at improving data efficiency for both classification and regression setups in deep learning. However, the current focus is on classification problems while rare attention has been paid to deep regression, which usually requires more human effort to labeling. Further, due to the intrinsic difference between categorical and continuous label space, the common intuitions for classification, e.g. cluster assumptions or pseudo labeling strategies, cannot be naturally adapted into deep regression. To this end, we first delved into the existing data-efficient methods in deep learning and found that they either encourage invariance to data stochasticity (e.g., consistency regularization under different augmentations) or model stochasticity (e.g., difference penalty for predictions of models with different dropout). To take the power of both worlds, we propose a novel χ-model by simultaneously encouraging the invariance to data stochasticity and model stochasticity. Further, the χ-model plays a minimax game between the feature extractor and task-specific heads to further enhance the invariance to model stochasticity. Extensive experiments verify the superiority of the χ-model among various tasks, from a single-value prediction task of age estimation to a dense-value prediction task of keypoint localization, a 2D synthetic and a 3D realistic dataset, as well as a multi-category object recognition task",
    "volume": "main",
    "checked": true,
    "id": "2a5ee2b57b3c3dd60880e9bf9405f9fd87c02784",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=3tbDrs77LJ5": {
    "title": "Large Learning Rate Tames Homogeneity: Convergence and Balancing Effect",
    "abstract": "Recent empirical advances show that training deep models with large learning rate often improves generalization performance. However, theoretical justifications on the benefits of large learning rate are highly limited, due to challenges in analysis. In this paper, we consider using Gradient Descent (GD) with a large learning rate on a homogeneous matrix factorization problem, i.e., minX,Y ‖A−XY ‖F. We prove a convergence theory for constant large learning rates well beyond 2/L, where L is the largest eigenvalue of Hessian at the initialization. Moreover, we rigorously establish an implicit bias of GD induced by such a large learning rate, termed ‘balancing', meaning that magnitudes of X and Y at the limit of GD iterations will be close even if their initialization is significantly unbalanced. Numerical experiments are provided to support our theory",
    "volume": "main",
    "checked": true,
    "id": "0b4108af924be64650e205d77f10e57aacbcbd5f",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=yjMQuLLcGWK": {
    "title": "FP-DETR: Detection Transformer Advanced by Fully Pre-training",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "64cb695fef260e36c1d3d8830b197923f1e865ea",
    "citation_count": 14
  },
  "https://openreview.net/forum?id=pMQwKL1yctf": {
    "title": "Language modeling via stochastic processes",
    "abstract": "Modern language models can generate high-quality short texts. However, they often meander or are incoherent when generating longer texts. These issues arise from the next-token-only language modeling objective. To address these issues, we introduce Time Control (TC), a language model that implicitly plans via a latent stochastic process. TC does this by learning a representation which maps the dynamics of how text changes in a document to the dynamics of a stochastic process of interest. Using this representation, the language model can generate text by first implicitly generating a document plan via a stochastic process, and then generating text that is consistent with this latent plan. Compared to domain-specific methods and fine-tuning GPT2 across a variety of text domains, TC improves performance on text infilling and discourse coherence. On long text generation settings, TC preserves the text structure both in terms of ordering (up to +40% better) and text length consistency (up to +17% better). Human evaluators also prefer TC's output 28.6% more than the baselines.1",
    "volume": "main",
    "checked": true,
    "id": "da563938dd9688e16dc6ef4b0f864db3a79f6ed6",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=VTNjxbFRKly": {
    "title": "Why Propagate Alone? Parallel Use of Labels and Features on Graphs",
    "abstract": "Graph neural networks (GNNs) and label propagation represent two interrelated modeling strategies designed to exploit graph structure in tasks such as node property prediction. The former is typically based on stacked message-passing layers that share neighborhood information to transform node features into predictive embeddings. In contrast, the latter involves spreading label information to unlabeled nodes via a parameter-free diffusion process, but operates independently of the node features. Given then that the material difference is merely whether features or labels are smoothed across the graph, it is natural to consider combinations of the two for improving performance. In this regard, it has recently been proposed to use a randomly-selected portion of the training labels as GNN inputs, concatenated with the original node features for making predictions on the remaining labels. This so-called label trick accommodates the parallel use of features and labels, and is foundational to many of the top-ranking submissions on the Open Graph Benchmark (OGB) leaderboard. And yet despite its wide-spread adoption, thus far there has been little attempt to carefully unpack exactly what statistical properties the label trick introduces into the training pipeline, intended or otherwise. To this end, we prove that under certain simplifying assumptions, the stochastic label trick can be reduced to an interpretable, deterministic training objective composed of two factors. The first is a data-fitting term that naturally resolves potential label leakage issues, while the second serves as a regularization factor conditioned on graph structure that adapts to graph size and connectivity. Later, we leverage this perspective to motivate a broader range of label trick use cases, and provide experiments to verify the efficacy of these extensions",
    "volume": "main",
    "checked": true,
    "id": "2f64df4e61994b127e36f8114da3a070535941f4",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=MvO2t0vbs4-": {
    "title": "Wisdom of Committees: An Overlooked Approach To Faster and More Accurate Models",
    "abstract": "Committee-based models (ensembles or cascades) construct models by combining existing pre-trained ones. While ensembles and cascades are well-known techniques that were proposed before deep learning, they are not considered a core building block of deep model architectures and are rarely compared to in recent literature on developing efficient models. In this work, we go back to basics and conduct a comprehensive analysis of the efficiency of committee-based models. We find that even the most simplistic method for building committees from existing, independently pre-trained models can match or exceed the accuracy of stateof-the-art models while being drastically more efficient. These simple committeebased models also outperform sophisticated neural architecture search methods (e.g., BigNAS). These findings hold true for several tasks, including image classification, video classification, and semantic segmentation, and various architecture families, such as ViT, EfficientNet, ResNet, MobileNetV2, and X3D. Our results show that an EfficientNet cascade can achieve a 5.4x speedup over B7 and a ViT cascade can achieve a 2.3x speedup over ViT-L-384 while being equally accurate",
    "volume": "main",
    "checked": true,
    "id": "f61855fa6acf7bd28b20187ad1cf219e96429534",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=6sh3pIzKS-": {
    "title": "Chemical-Reaction-Aware Molecule Representation Learning",
    "abstract": "Molecule representation learning (MRL) methods aim to embed molecules into a real vector space. However, existing SMILES-based (Simplified Molecular-Input Line-Entry System) or GNN-based (Graph Neural Networks) MRL methods either take SMILES strings as input that have difficulty in encoding molecule structure information, or over-emphasize the importance of GNN architectures but neglect their generalization ability. Here we propose using chemical reactions to assist learning molecule representation. The key idea of our approach is to preserve the equivalence of molecules with respect to chemical reactions in the embedding space, i.e., forcing the sum of reactant embeddings and the sum of product embeddings to be equal for each chemical equation. This constraint is proven effective to 1) keep the embedding space well-organized and 2) improve the generalization ability of molecule embeddings. Moreover, our model can use any GNN as the molecule encoder and is thus agnostic to GNN architectures. Experimental results demonstrate that our method achieves state-of-the-art performance in a variety of downstream tasks, e.g., 17.4% absolute Hit@1 gain in chemical reaction prediction, 2.3% absolute AUC gain in molecule property prediction, and 18.5% relative RMSE gain in graph-edit-distance prediction, respectively, over the best baseline method. The code is available at https://github.com/hwwang55/MolR",
    "volume": "main",
    "checked": true,
    "id": "309049d5003f0876a759c983fce4edf510f1b006",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=Z7Lk2cQEG8a": {
    "title": "The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks: an Exact Characterization of Optimal Solutions",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "fbed65c0ba64deade97a9defbadce00a6c7c6f38",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=oWZsQ8o5EA": {
    "title": "On the Generalization of Models Trained with SGD: Information-Theoretic Bounds and Implications",
    "abstract": "This paper follows up on a recent work of Neu et al. (2021) and presents some new information-theoretic upper bounds for the generalization error of machine learning models, such as neural networks, trained with SGD. We apply these bounds to analyzing the generalization behaviour of linear and two-layer ReLU networks. Experimental study of these bounds provide some insights on the SGD training of neural networks. They also point to a new and simple regularization scheme which we show performs comparably to the current state of the art",
    "volume": "main",
    "checked": true,
    "id": "55f77fd6a6ec1869c08c74cdb45640f275e65788",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=B3Nde6lvab": {
    "title": "Eliminating Sharp Minima from SGD with Truncated Heavy-tailed Noise",
    "abstract": "The empirical success of deep learning is often attributed to SGD's mysterious ability to avoid sharp local minima in the loss landscape, as sharp minima are known to lead to poor generalization. Recently, empirical evidence of heavy-tailed gradient noise was reported in many deep learning tasks, and it was shown in \\c{S}im\\c{s}ekli (2019a,b) that SGD can escape sharp local minima under the presence of such heavy-tailed gradient noise, providing a partial solution to the mystery. In this work, we analyze a popular variant of SGD where gradients are truncated above a fixed threshold. We show that it achieves a stronger notion of avoiding sharp minima: it can effectively eliminate sharp local minima entirely from its training trajectory. We characterize the dynamics of truncated SGD driven by heavy-tailed noises. First, we show that the truncation threshold and width of the attraction field dictate the order of the first exit time from the associated local minimum. Moreover, when the objective function satisfies appropriate structural conditions, we prove that as the learning rate decreases, the dynamics of heavy-tailed truncated SGD closely resemble those of a continuous-time Markov chain that never visits any sharp minima. Real data experiments on deep learning confirm our theoretical prediction that heavy-tailed SGD with gradient clipping finds a\"flatter\"local minima and achieves better generalization",
    "volume": "main",
    "checked": true,
    "id": "68e193fe7536b5298986376e3e721d97d1688de1",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=5QhUE1qiVC6": {
    "title": "The Convex Geometry of Backpropagation: Neural Network Gradient Flows Converge to Extreme Points of the Dual Convex Program",
    "abstract": "We study non-convex subgradient flows for training two-layer ReLU neural networks from a convex geometry and duality perspective. We characterize the implicit bias of unregularized non-convex gradient flow as convex regularization of an equivalent convex model. We then show that the limit points of non-convex subgradient flows can be identified via primal-dual correspondence in this convex optimization problem. Moreover, we derive a sufficient condition on the dual variables which ensures that the stationary points of the non-convex objective are the KKT points of the convex objective, thus proving convergence of non-convex gradient flows to the global optimum. For a class of regular training data distributions such as orthogonal separable data, we show that this sufficient condition holds. Therefore, non-convex gradient flows in fact converge to optimal solutions of a convex optimization problem. We present numerical results verifying the predictions of our theory for non-convex subgradient descent",
    "volume": "main",
    "checked": true,
    "id": "de54fe9d50f0ecccd2aa1468ca3b353f1865eed7",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=7I8LPkcx8V": {
    "title": "Differentially Private Fractional Frequency Moments Estimation with Polylogarithmic Space",
    "abstract": "We prove that Fp sketch, a well-celebrated streaming algorithm for frequency moments estimation, is differentially private as is when p ∈ (0, 1]. Fp sketch uses only polylogarithmic space, exponentially better than existing DP baselines and only worse than the optimal non-private baseline by a logarithmic factor. The evaluation shows that Fp sketch can achieve reasonable accuracy with strong privacy guarantees",
    "volume": "main",
    "checked": true,
    "id": "115c3aa15ca5b07b067ce020f6cd0582d4f8e7f2",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=RftryyYyjiG": {
    "title": "Exploring extreme parameter compression for pre-trained language models",
    "abstract": "Recent work explored the potential of large-scale Transformer-based pre-trained models, especially Pre-trained Language Models (PLMs) in natural language processing. This raises many concerns from various perspectives, e.g., ﬁnancial costs and carbon emissions. Compressing PLMs like BERT with negligible performance loss for faster inference and cheaper deployment has attracted much attention. In this work, we aim to explore larger compression ratios for PLMs, among which tensor decomposition is a potential but under-investigated one. Two decomposition and reconstruction protocols are further proposed to improve the effectiveness and efﬁciency during compression. Our compressed BERT 1 with 1 / 7 parameters in Transformer layers performs on-par with, sometimes slightly better than the original BERT in GLUE benchmark. A tiny version achieves 96.7% performance of BERT-base with 1 / 48 encoder parameters (i.e., less than 2M parameters excluding the embedding layer) and 2 . 7 × faster on inference. To show that the proposed method is orthogonal to existing compression methods like knowledge distillation, we also explore the beneﬁt of the proposed method on a distilled BERT",
    "volume": "main",
    "checked": true,
    "id": "05d70085d1b580b2369942410ae77c48d1eeacca",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=yhCp5RcZD7": {
    "title": "Geometry-Consistent Neural Shape Representation with Implicit Displacement Fields",
    "abstract": "We present implicit displacement fields, a novel representation for detailed 3D geometry. Inspired by a classic surface deformation technique, displacement mapping, our method represents a complex surface as a smooth base surface plus a displacement along the base's normal directions, resulting in a frequency-based shape decomposition, where the high-frequency signal is constrained geometrically by the low-frequency signal. Importantly, this disentanglement is unsupervised thanks to a tailored architectural design that has an innate frequency hierarchy by construction. We explore implicit displacement field surface reconstruction and detail transfer and demonstrate superior representational power, training stability, and generalizability. Code and data available at: https: //github.com/yifita/idf",
    "volume": "main",
    "checked": true,
    "id": "719af7de83ef30bb46815de20e5cee604ed7b9b8",
    "citation_count": 20
  },
  "https://openreview.net/forum?id=7udZAsEzd60": {
    "title": "VC dimension of partially quantized neural networks in the overparametrized regime",
    "abstract": "Vapnik-Chervonenkis (VC) theory has so far been unable to explain the small generalization error of overparametrized neural networks. Indeed, existing applications of VC theory to large networks obtain upper bounds on VC dimension that are proportional to the number of weights, and for a large class of networks, these upper bound are known to be tight. In this work, we focus on a subclass of partially quantized networks that we refer to as hyperplane arrangement neural networks (HANNs). Using a sample compression analysis, we show that HANNs can have VC dimension signiﬁcantly smaller than the number of weights, while being highly expressive. In particular, empirical risk minimization over HANNs in the overparametrized regime achieves the minimax rate for classiﬁcation with Lipschitz posterior class probability. We further demonstrate the expressivity of HANNs empirically. On a panel of 121 UCI datasets, overparametrized HANNs match the performance of state-of-the-art full-precision models",
    "volume": "main",
    "checked": true,
    "id": "7e52442932a8c316a87028964bd43a1e7442eba3",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=YeShU5mLfLt": {
    "title": "On the Convergence of Certified Robust Training with Interval Bound Propagation",
    "abstract": "Interval Bound Propagation (IBP) is so far the base of state-of-the-art methods for training neural networks with certifiable robustness guarantees when potential adversarial perturbations present, while the convergence of IBP training remains unknown in existing literature. In this paper, we present a theoretical analysis on the convergence of IBP training. With an overparameterized assumption, we analyze the convergence of IBP robust training. We show that when using IBP training to train a randomly initialized two-layer ReLU neural network with logistic loss, gradient descent can linearly converge to zero robust training error with a high probability if we have sufficiently small perturbation radius and large network width",
    "volume": "main",
    "checked": true,
    "id": "7cd5d4942a5cd6f1299841a5fec96eb65cdb677b",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=OJm3HZuj4r7": {
    "title": "Convergent and Efficient Deep Q Learning Algorithm",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "9a69433192e15ac673339632c8e3d05d37b90254",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=7F9cOhdvfk_": {
    "title": "$\\mathrm{SO}(2)$-Equivariant Reinforcement Learning",
    "abstract": "Equivariant neural networks enforce symmetry within the structure of their convolutional layers, resulting in a substantial improvement in sample efficiency when learning an equivariant or invariant function. Such models are applicable to robotic manipulation learning which can often be formulated as a rotationally symmetric problem. This paper studies equivariant model architectures in the context ofQ-learning and actor-critic reinforcement learning. We identify equivariant and invariant characteristics of the optimal Q-function and the optimal policy and propose equivariant DQN and SAC algorithms that leverage this structure. We present experiments that demonstrate that our equivariant versions of DQN and SAC can be significantly more sample efficient than competing algorithms on an important class of robotic manipulation problems",
    "volume": "main",
    "checked": true,
    "id": "347131d36cda1c921158bb6928a488ab13bd26e5",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=EhYjZy6e1gJ": {
    "title": "PiCO: Contrastive Label Disambiguation for Partial Label Learning",
    "abstract": "Partial label learning (PLL) is an important problem that allows each training example to be labeled with a coarse candidate set, which well suits many real-world data annotation scenarios with label ambiguity. Despite the promise, the performance of PLL often lags behind the supervised counterpart. In this work, we bridge the gap by addressing two key research challenges in PLL—representation learning and label disambiguation—in one coherent framework. Specifically, our proposed framework PiCO consists of a contrastive learning module along with a novel class prototype-based label disambiguation algorithm. PiCO produces closely aligned representations for examples from the same classes and facilitates label disambiguation. Theoretically, we show that these two components are mutually beneficial, and can be rigorously justified from an expectationmaximization (EM) algorithm perspective. Extensive experiments demonstrate that PiCO significantly outperforms the current state-of-the-art approaches in PLL and even achieves comparable results to fully supervised learning. Code and data available: https://github.com/hbzju/PiCO",
    "volume": "main",
    "checked": true,
    "id": "5562b37612ab56173f028c587c4e02ac684f66bc",
    "citation_count": 16
  },
  "https://openreview.net/forum?id=74x5BXs4bWD": {
    "title": "Evolutionary Diversity Optimization with Clustering-based Selection for Reinforcement Learning",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "fed0701afdfa6896057f7d04bd30ab1328eff110",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=tYRrOdSnVUy": {
    "title": "Non-Transferable Learning: A New Approach for Model Ownership Verification and Applicability Authorization",
    "abstract": "As Artificial Intelligence as a Service gains popularity, protecting well-trained models as intellectual property is becoming increasingly important. There are two common types of protection methods: ownership verification and usage authorization. In this paper, we propose Non-Transferable Learning (NTL), a novel approach that captures the exclusive data representation in the learned model and restricts the model generalization ability to certain domains. This approach provides effective solutions to both model verification and authorization. Specifically: 1) For ownership verification, watermarking techniques are commonly used but are often vulnerable to sophisticated watermark removal methods. By comparison, our NTL-based ownership verification provides robust resistance to stateof-the-art watermark removal methods, as shown in extensive experiments with 6 removal approaches over the digits, CIFAR10 & STL10, and VisDA datasets. 2) For usage authorization, prior solutions focus on authorizing specific users to access the model, but authorized users can still apply the model to any data without restriction. Our NTL-based authorization approach instead provides data-centric protection, which we call applicability authorization, by significantly degrading the performance of the model on unauthorized data. Its effectiveness is also shown through experiments on aforementioned datasets",
    "volume": "main",
    "checked": true,
    "id": "a343ed52ae6075168272f0c70ee989ef81c46a83",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=7r6kDq0mK_": {
    "title": "Latent Image Animator: Learning to Animate Images via Latent Space Navigation",
    "abstract": "Due to the remarkable progress of deep generative models, animating images has become increasingly efficient, whereas associated results have become increasingly realistic. Current animation-approaches commonly exploit structure representation extracted from driving videos. Such structure representation is instrumental in transferring motion from driving videos to still images. However, such approaches fail in case the source image and driving video encompass large appearance variation. Moreover, the extraction of structure information requires additional modules that endow the animation-model with increased complexity. Deviating from such models, we here introduce the Latent Image Animator (LIA), a self-supervised autoencoder that evades need for structure representation. LIA is streamlined to animate images by linear navigation in the latent space. Specifically, motion in generated video is constructed by linear displacement of codes in the latent space. Towards this, we learn a set of orthogonal motion directions simultaneously, and use their linear combination, in order to represent any displacement in the latent space. Extensive quantitative and qualitative analysis suggests that our model systematically and significantly outperforms state-of-art methods on VoxCeleb, Taichi and TED-talk datasets w.r.t. generated quality. Source code and pre-trained models are publicly available1. Figure 1: LIA animation examples. The two images of Marilyn Monroe and Emmanuel Macron are animated by LIA, which transfers motion of a driving video (smaller images on the top) from VoxCeleb dataset (Chung et al., 2018) onto the still images. LIA is able to successfully animate these two images without relying on any explicit structure representations, such as landmarks and region representations. https://wyhsirius.github.io/LIA-project/ 1 ar X iv :2 20 3. 09 04 3v 1 [ cs .C V ] 1 7 M ar 2 02 2 Published as a conference paper at ICLR 2022",
    "volume": "main",
    "checked": true,
    "id": "4ec12c985a530ff7b7009cfbbd277dbb4703338b",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=hSktDu-h94": {
    "title": "Automatic Loss Function Search for Predict-Then-Optimize Problems with Strong Ranking Property",
    "abstract": "Combinatorial optimization problems with parameters to be predicted from side information are commonly seen in a variety of problems during the paradigm shifts from reactive decision making to proactive decision making. Due to the misalignment between the continuous prediction results and the discrete decisions in optimization problems, it is hard to achieve a satisfactory prediction result with the ordinary l2 loss in the prediction phase. To properly connect the prediction loss with the optimization goal, in this paper we propose a total group preorder (TGP) loss and its differential version called approximate total group preorder (ATGP) loss for predict-then-optimize (PTO) problems with strong ranking property. These new losses are provably more robust than the usual l2 loss in a linear regression setting and have great potential to extend to other settings. We also propose an automatic searching algorithm that adapts the ATGP loss to PTO problems with different combinatorial structures. Extensive experiments on the ranking problem, the knapsack problem, and the shortest path problem have demonstrated that our proposed method can achieve a significantly better performance compared to the other methods designed for PTO problems",
    "volume": "main",
    "checked": true,
    "id": "d42b866ad0b57225916a96e1a50b44c00adaf47b",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=JzNB0eA2-M4": {
    "title": "On the Convergence of the Monte Carlo Exploring Starts Algorithm for Reinforcement Learning",
    "abstract": "A simple and natural algorithm for reinforcement learning is Monte Carlo Exploring States (MCES), where the Q-function is estimated by averaging the Monte Carlo returns, and the policy is improved by choosing actions that maximize the current estimate of the Q-function. Exploration is performed by \"exploring starts\", that is, each episode begins with a randomly chosen state and action and then follows the current policy. Establishing convergence for this algorithm has been an open problem for more than 20 years. We make headway with this problem by proving convergence for Optimal Policy Feed-Forward MDPs, which are MDPs whose states are not revisited within any episode for an optimal policy. Such MDPs include all deterministic environments (including Cliff Walking and other gridworld examples) and a large class of stochastic environments (including Blackjack). The convergence results presented here make progress for this long-standing open problem in reinforcement learning",
    "volume": "main",
    "checked": true,
    "id": "6c731580134b0fdf6e312d23dbc38b7faa2ee3a5",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=GUrhfTuf_3": {
    "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision",
    "abstract": "(b)). These results suggest zero-shot cross-modality transfer emerges with the scaling of weakly labeled data",
    "volume": "main",
    "checked": true,
    "id": "5e00596fa946670d894b1bdaeff5a98e3867ef13",
    "citation_count": 154
  },
  "https://openreview.net/forum?id=e95i1IHcWj": {
    "title": "Equivariant and Stable Positional Encoding for More Powerful Graph Neural Networks",
    "abstract": "and 2021). Over the ﬁrst 6 graphs, we utilize 85%, 5%, 10% to partition the link set that gives positive examples for training, validation and testing and pair them with the same numbers of negative examples (missing edges). For the two OGB graphs, we adopt the dataset splits in (Hu et al., 2020). The links for validation and test are removed during the training stage and the links for validation are also not used in the test stage. All the models are trained till the loss converges and the models with the best validation performance is used to test",
    "volume": "main",
    "checked": true,
    "id": "c7bfa52795a8d1f44f6426f6d818a9a626ab5368",
    "citation_count": 13
  },
  "https://openreview.net/forum?id=XLxhEjKNbXj": {
    "title": "GLASS: GNN with Labeling Tricks for Subgraph Representation Learning",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "90e76bbf2ff66de0e88fddefc0bcac176f86a3b5",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=O476oWmiNNp": {
    "title": "Anti-Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis: From Theory to Practice",
    "abstract": "Vision Transformer (ViT) has recently demonstrated promise in computer vision problems. However, unlike Convolutional Neural Networks (CNN), it is known that the performance of ViT saturates quickly with depth increasing, due to the observed attention collapse or patch uniformity. Despite a couple of empirical solutions, a rigorous framework studying on this scalability issue remains elusive. In this paper, we first establish a rigorous theory framework to analyze ViT features from the Fourier spectrum domain. We show that the self-attention mechanism inherently amounts to a low-pass filter, which indicates when ViT scales up its depth, excessive low-pass filtering will cause feature maps to only preserve their Direct-Current (DC) component. We then propose two straightforward yet effective techniques to mitigate the undesirable low-pass limitation. The first technique, termed AttnScale, decomposes a self-attention block into low-pass and high-pass components, then rescales and combines these two filters to produce an all-pass self-attention matrix. The second technique, termed FeatScale, re-weights feature maps on separate frequency bands to amplify the high-frequency signals. Both techniques are efficient and hyperparameter-free, while effectively overcoming relevant ViT training artifacts such as attention collapse and patch uniformity. By seamlessly plugging in our techniques to multiple ViT variants, we demonstrate that they consistently help ViTs benefit from deeper architectures, bringing up to 1.1% performance gains \"for free\" (e.g., with little parameter overhead). We publicly release our codes and pre-trained models at https://github.com/VITA-Group/ViT-Anti-Oversmoothing",
    "volume": "main",
    "checked": true,
    "id": "b9225c672a5078409d890393780a5eb90f2ec3ca",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=9kpuB2bgnim": {
    "title": "Huber Additive Models for Non-stationary Time Series Analysis",
    "abstract": "Sparse additive models have shown promising flexibility and interpretability in processing time series data. However, existing methods usually assume the time series data to be stationary and the innovation is sampled from a Gaussian distribution. Both assumptions are too stringent for heavy-tailed and non-stationary time series data that frequently arise in practice, such as finance and medical fields. To address these problems, we propose an adaptive sparse Huber additive model for robust forecasting in both non-Gaussian data and (non)stationary data. In theory, the generalization bounds of our estimator are established for both stationary and nonstationary time series data, which are independent of the widely used mixing conditions in learning theory of dependent observations. Moreover, the error bound for non-stationary time series contains a discrepancy measure for the shifts of the data distributions over time. Such a discrepancy measure can be estimated empirically and used as a penalty in our method. Experimental results on both synthetic and real-world benchmark datasets validate the effectiveness of the proposed method. The code is available at https://github.com/xianruizhong/SpHAM",
    "volume": "main",
    "checked": true,
    "id": "3259ccb8a56723bbfa5bc30f40dd4125082c7023",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=Oh1r2wApbPv": {
    "title": "Contextualized Scene Imagination for Generative Commonsense Reasoning",
    "abstract": "Humans use natural language to compose common concepts from their environment into plausible, day-to-day scene descriptions. However, such generative commonsense reasoning (GCSR) skills are lacking in state-of-the-art text generation methods. Descriptive sentences about arbitrary concepts generated by neural text generation models (e.g., pre-trained text-to-text Transformers) are often grammatically fluent but may not correspond to human common sense, largely due to their lack of mechanisms to capture concept relations, to identify implicit concepts, and to perform generalizable reasoning about unseen concept compositions. In this paper, we propose an Imagine-and-Verbalize (I&V) method, which learns to imagine a relational scene knowledge graph (SKG) with relations between the input concepts, and leverage the SKG as a constraint when generating a plausible scene description. We collect and harmonize a set of knowledge resources from different domains and modalities, providing a rich auxiliary supervision signal for I&V. The experiments demonstrate the effectiveness of I&V in improving language models on both concept-to-sentence and concept-to-story generation tasks, while enabling the model to learn well from fewer task examples and generate SKGs that make common sense to human annotators 1",
    "volume": "main",
    "checked": true,
    "id": "cc37f5821b2ea4c4528b5ba3b96f302db7ec3870",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=2t7CkQXNpuq": {
    "title": "ToM2C: Target-oriented Multi-agent Communication and Cooperation with Theory of Mind",
    "abstract": "Being able to predict the mental states of others is a key factor to effective social interaction. It is also crucial for distributed multi-agent systems, where agents are required to communicate and cooperate. In this paper, we introduce such an important social-cognitive skill, i.e. Theory of Mind (ToM), to build socially intelligent agents who are able to communicate and cooperate ef-fectively to accomplish challenging tasks. With ToM, each agent is capable of inferring the mental states and intentions of others according to its (local) observation. Based on the inferred states, the agents decide \"when\" and with \"whom\" to share their intentions. With the information observed, inferred, and received, the agents decide their sub-goals and reach a consensus among the team. In the end, the low-level executors inde-pendently take primitive actions to accomplish the sub-goals. We demonstrate the idea in the multi-sensor target coverage problem, a typical target-oriented multi-agent task. The experiments show that the proposed model not only outperforms the state-of-the-art methods on target coverage rate and communication efﬁciency, but also shows good generalization across different scales of the environment",
    "volume": "main",
    "checked": true,
    "id": "52a027e80c24ecf9bcc468609dbb5be72478ec7a",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=hzmQ4wOnSb": {
    "title": "GNN is a Counter? Revisiting GNN for Question Answering",
    "abstract": "Question Answering (QA) has been a long-standing research topic in AI and NLP fields, and a wealth of studies have been conducted to attempt to equip QA systems with human-level reasoning capability. To approximate the complicated human reasoning process, state-of-the-art QA systems commonly use pre-trained language models (LMs) to access knowledge encoded in LMs together with elaborately designed modules based on Graph Neural Networks (GNNs) to perform reasoning over knowledge graphs (KGs). However, many problems remain open regarding the reasoning functionality of these GNN-based modules. Can these GNN-based modules really perform a complex reasoning process? Are they underor overcomplicated for QA? To open the black box of GNN and investigate these problems, we dissect state-of-the-art GNN modules for QA and analyze their reasoning capability. We discover that even a very simple graph neural counter can outperform all the existing GNN modules on CommonsenseQA and OpenBookQA, two popular QA benchmark datasets which heavily rely on knowledge-aware reasoning. Our work reveals that existing knowledge-aware GNN modules may only carry out some simple reasoning such as counting. It remains a challenging open problem to build comprehensive reasoning modules for knowledge-powered QA",
    "volume": "main",
    "checked": true,
    "id": "ed02d4a08a50d23709b7ce5df9878a8bc34ac12c",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=a7H7OucbWaU": {
    "title": "Memory Replay with Data Compression for Continual Learning",
    "abstract": "Continual learning needs to overcome catastrophic forgetting of the past. Memory replay of representative old training samples has been shown as an effective solution, and achieves the state-of-the-art (SOTA) performance. However, existing work is mainly built on a small memory buffer containing a few original data, which cannot fully characterize the old data distribution. In this work, we propose memory replay with data compression (MRDC) to reduce the storage cost of old training samples and thus increase their amount that can be stored in the memory buffer. Observing that the trade-off between the quality and quantity of compressed data is highly nontrivial for the efficacy of memory replay, we propose a novel method based on determinantal point processes (DPPs) to efficiently determine an appropriate compression quality for currently-arrived training samples. In this way, using a naive data compression algorithm with a properly selected quality can largely boost recent strong baselines by saving more compressed data in a limited storage space. We extensively validate this across several benchmarks of class-incremental learning and in a realistic scenario of object detection for autonomous driving",
    "volume": "main",
    "checked": true,
    "id": "45d49272a3ec51b61ff88af04b38dbce29b05dfc",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=QJWVP4CTmW4": {
    "title": "Ada-NETS: Face Clustering via Adaptive Neighbour Discovery in the Structure Space",
    "abstract": "Face clustering has attracted rising research interest recently to take advantage of massive amounts of face images on the web. State-of-the-art performance has been achieved by Graph Convolutional Networks (GCN) due to their powerful representation capacity. However, existing GCN-based methods build face graphs mainly according to kNN relations in the feature space, which may lead to a lot of noise edges connecting two faces of different classes. The face features will be polluted when messages pass along these noise edges, thus degrading the performance of GCNs. In this paper, a novel algorithm named Ada-NETS is proposed to cluster faces by constructing clean graphs for GCNs. In Ada-NETS, each face is transformed to a new structure space, obtaining robust features by considering face features of the neighbour images. Then, an adaptive neighbour discovery strategy is proposed to determine a proper number of edges connecting to each face image. It significantly reduces the noise edges while maintaining the good ones to build a graph with clean yet rich edges for GCNs to cluster faces. Experiments on multiple public clustering datasets show that Ada-NETS significantly outperforms current state-of-the-art methods, proving its superiority and generalization. Code is available at https://github.com/damo-cv/Ada-NETS",
    "volume": "main",
    "checked": true,
    "id": "6490835ea97ddd98e7dadf246892c26d53eaa328",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=VFBjuF8HEp": {
    "title": "Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality",
    "abstract": "Diffusion models have emerged as an expressive family of generative models ri-valing GANs in sample quality and autoregressive models in likelihood scores. Standard diffusion models typically require hundreds of forward passes through the model to generate a single high-ﬁdelity sample. We introduce Differentiable Diffusion Sampler Search (DDSS): a method that optimizes fast samplers for any pre-trained diffusion model by differentiating through sample quality scores. We present Generalized Gaussian Diffusion Models (GGDM), a family of ﬂexible non-Markovian samplers for diffusion models. We show that optimizing the degrees of freedom of GGDM samplers by maximizing sample quality scores via gradient descent leads to improved sample quality. Our optimization procedure backpropagates through the sampling process using the reparametrization trick and gradient rematerialization. DDSS achieves strong results on unconditional image generation across various datasets ( e.g., FID scores on LSUN church 128x128 of 11.6 with only 10 inference steps, and 4.82 with 20 steps, compared to 51.1 and 14.9 with strongest DDPM/DDIM baselines). Our method is compatible with any pre-trained diffusion model without ﬁne-tuning or re-training required",
    "volume": "main",
    "checked": true,
    "id": "7e839c2667479d91e21e84583c27257dc7dc1a36",
    "citation_count": 20
  },
  "https://openreview.net/forum?id=3eIrli0TwQ": {
    "title": "On the Importance of Difficulty Calibration in Membership Inference Attacks",
    "abstract": "The vulnerability of machine learning models to membership inference attacks has received much attention in recent years. Existing attacks mostly remain impractical due to having high false positive rates, where non-member samples are often erroneously predicted as members. This type of error makes the predicted membership signal unreliable, especially since most samples are non-members in real world applications. In this work, we argue that membership inference attacks can benefit drastically from difficulty calibration, where an attack's predicted membership score is adjusted to the difficulty of correctly classifying the target sample. We show that difficulty calibration can significantly reduce the false positive rate of a variety of existing attacks without a loss in accuracy",
    "volume": "main",
    "checked": true,
    "id": "5d96e9061337fcaedcfba2ee2ceb6daa85151568",
    "citation_count": 13
  },
  "https://openreview.net/forum?id=_X90SIKbHa": {
    "title": "A Class of Short-term Recurrence Anderson Mixing Methods and Their Applications",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "fe3d665472d47df4ea60b953b81aea3ccd502ea4",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=gEZrGCozdqR": {
    "title": "Finetuned Language Models are Zero-Shot Learners",
    "abstract": "A BSTRACT This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning —ﬁnetuning language models on a collection of datasets described via instructions—substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodiﬁed counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of ﬁnetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning. many tasks",
    "volume": "main",
    "checked": true,
    "id": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd",
    "citation_count": 242
  },
  "https://openreview.net/forum?id=ySQH0oDyp7": {
    "title": "QDrop: Randomly Dropping Quantization for Extremely Low-bit Post-Training Quantization",
    "abstract": "Recently, post-training quantization (PTQ) has driven much attention to produce efficient neural networks without long-time retraining. Despite its low cost, current PTQ works tend to fail under the extremely low-bit setting. In this study, we pioneeringly confirm that properly incorporating activation quantization into the PTQ reconstruction benefits the final accuracy. To deeply understand the inherent reason, a theoretical framework is established, indicating that the flatness of the optimized low-bit model on calibration and test data is crucial. Based on the conclusion, a simple yet effective approach dubbed as QDROP is proposed, which randomly drops the quantization of activations during PTQ. Extensive experiments on various tasks including computer vision (image classification, object detection) and natural language processing (text classification and question answering) prove its superiority. With QDROP, the limit of PTQ is pushed to the 2-bit activation for the first time and the accuracy boost can be up to 51.49%. Without bells and whistles, QDROP establishes a new state of the art for PTQ. Our code is available at https://github.com/wimh966/QDrop and has been integrated into MQBench (https://github.com/ModelTC/MQBench)",
    "volume": "main",
    "checked": true,
    "id": "ea75e60f5d1757ef6e3bf8e54d9bacff65df61ca",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=y1PXylgrXZ": {
    "title": "Certified Robustness for Deep Equilibrium Models via Interval Bound Propagation",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "6e9577c4b4518b9976b9d421755ce37f8ea3ed7f",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=TBWA6PLJZQm": {
    "title": "Learning with Noisy Labels Revisited: A Study Using Real-World Human Annotations",
    "abstract": "Existing research on learning with noisy labels mainly focuses on synthetic label noise. Synthetic label noise, though has clean structures which greatly enable statistical analyses, often fails to model the real-world noise patterns. The recent literature has observed several efforts to offer real-world noisy datasets, e.g., Food-101N, WebVision, and Clothing1M. Yet the existing efforts suffer from two caveats: firstly, the lack of ground-truth verification makes it hard to theoretically study the property and treatment of real-world label noise. Secondly, these efforts are often of large scales, which may result in unfair comparisons of robust methods within reasonable and accessible computation power. To better understand real-world label noise, it is important to establish controllable and moderate-sized real-world noisy datasets with both ground-truth and noisy labels. This work presents two new benchmark datasets, which we name as CIFAR-10N, CIFAR-100N, equipping the training datasets of CIFAR-10 and CIFAR-100 with human-annotated real-world noisy labels that we collect from Amazon Mechanical Turk. We quantitatively and qualitatively show that real-world noisy labels follow an instance-dependent pattern rather than the classically assumed and adopted ones (e.g., class-dependent label noise). We then initiate an effort to benchmark a subset of the existing solutions using CIFAR-10N and CIFAR-100N. We further proceed to study the memorization of correct and wrong predictions, which further illustrates the difference between human noise and class-dependent synthetic noise. We show indeed the real-world noise patterns impose new and outstanding challenges as compared to synthetic label noise. These observations require us to rethink the treatment of noisy labels, and we hope the availability of these two datasets would facilitate the development and evaluation of future learning with noisy label solutions. The corresponding datasets and the leaderboard are publicly available at http://noisylabels.com",
    "volume": "main",
    "checked": true,
    "id": "ab8510d2675a94c9d29e49d949820a2eb136df10",
    "citation_count": 29
  },
  "https://openreview.net/forum?id=wogsFPHwftY": {
    "title": "Learning Super-Features for Image Retrieval",
    "abstract": "Methods that combine local and global features have recently shown excellent performance on multiple challenging deep image retrieval benchmarks, but their use of local features raises at least two issues. First, these local features simply boil down to the localized map activations of a neural network, and hence can be extremely redundant. Second, they are typically trained with a global loss that only acts on top of an aggregation of local features; by contrast, testing is based on local feature matching, which creates a discrepancy between training and testing. In this paper, we propose a novel architecture for deep image retrieval, based solely on mid-level features that we call Super-features. These Super-features are constructed by an iterative attention module and constitute an ordered set in which each element focuses on a localized and discriminant image pattern. For training, they require only image labels. A contrastive loss operates directly at the level of Super-features and focuses on those that match across images. A second complementary loss encourages diversity. Experiments on common landmark retrieval benchmarks validate that Super-features substantially outperform state-of-the-art methods when using the same number of features, and only require a significantly smaller memory footprint to match their performance. Code and models are available at: https://github.com/naver/FIRe",
    "volume": "main",
    "checked": true,
    "id": "6c6d5e6baaa702a1dd0a18464a80acba8128ebb9",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=l3SDgUh7qZO": {
    "title": "SphereFace2: Binary Classification is All You Need for Deep Face Recognition",
    "abstract": "State-of-the-art deep face recognition methods are mostly trained with a softmax-based multi-class classification framework. Despite being popular and effective, these methods still have a few shortcomings that limit empirical performance. In this paper, we first identify the discrepancy between training and evaluation in the existing multi-class classification framework and then discuss the potential limitations caused by the \"competitive\" nature of softmax normalization. Motivated by these limitations, we propose a novel binary classification training framework, termed SphereFace2. In contrast to existing methods, SphereFace2 circumvents the softmax normalization, as well as the corresponding closed-set assumption. This effectively bridges the gap between training and evaluation, enabling the representations to be improved individually by each binary classification task. Besides designing a specific well-performing loss function, we summarize a few general principles for this \"one-vs-all\" binary classification framework so that it can outperform current competitive methods. We conduct comprehensive experiments on popular benchmarks to demonstrate that SphereFace2 can consistently outperform current state-of-the-art deep face recognition methods",
    "volume": "main",
    "checked": true,
    "id": "0316532ad3fd10622b36741c970f3cbc43d3c5cc",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=B8DVo9B1YE0": {
    "title": "Relating transformers to models and neural representations of the hippocampal formation",
    "abstract": "Many deep neural network architectures loosely based on brain networks have recently been shown to replicate neural firing patterns observed in the brain. One of the most exciting and promising novel architectures, the Transformer neural network, was developed without the brain in mind. In this work, we show that transformers, when equipped with recurrent position encodings, replicate the precisely tuned spatial representations of the hippocampal formation; most notably place and grid cells. Furthermore, we show that this result is no surprise since it is closely related to current hippocampal models from neuroscience. We additionally show the transformer version offers dramatic performance gains over the neuroscience version. This work continues to bind computations of artificial and brain networks, offers a novel understanding of the hippocampal-cortical interaction, and suggests how wider cortical areas may perform complex tasks beyond current neuroscience models such as language comprehension",
    "volume": "main",
    "checked": true,
    "id": "06e40b7a703079c280f8f0886ac2bd984cd318ce",
    "citation_count": 15
  },
  "https://openreview.net/forum?id=uxgg9o7bI_3": {
    "title": "A New Perspective on \"How Graph Neural Networks Go Beyond Weisfeiler-Lehman?\"",
    "abstract": "We propose a new perspective on designing powerful Graph Neural Networks (GNNs). In a nutshell, this enables a general solution to inject structural properties of graphs into a message-passing aggregation scheme of GNNs. As a theoretical basis, we develop a new hierarchy of local isomorphism on neighborhood subgraphs. Then, we theoretically characterize how message-passing GNNs can be designed to be more expressive than the Weisfeiler Lehman test. To elaborate this characterization, we propose a novel neural model, called GraphSNN, and prove that this model is strictly more expressive than the Weisfeiler Lehman test in distinguishing graph structures. We empirically verify the strength of our model on different graph learning tasks. It is shown that our model consistently improves the state-of-the-art methods on the benchmark tasks without sacrificing computational simplicity and efficiency",
    "volume": "main",
    "checked": true,
    "id": "6a0cbf943183a6751ff438c16ad75434b4cf47da",
    "citation_count": 18
  },
  "https://openreview.net/forum?id=Dl4LetuLdyK": {
    "title": "A Fine-Grained Analysis on Distribution Shift",
    "abstract": "Robustness to distribution shifts is critical for deploying machine learning models in the real world. Despite this necessity, there has been little work in defining the underlying mechanisms that cause these shifts and evaluating the robustness of algorithms across multiple, different distribution shifts. To this end, we introduce a framework that enables fine-grained analysis of various distribution shifts. We provide a holistic analysis of current state-of-the-art methods by evaluating 19 distinct methods grouped into five categories across both synthetic and real-world datasets. Overall, we train more than 85K models. Our experimental framework can be easily extended to include new methods, shifts, and datasets. We find, unlike previous work (Gulrajani & Lopez-Paz, 2021), that progress has been made over a standard ERM baseline; in particular, pretraining and augmentations (learned or heuristic) offer large gains in many cases. However, the best methods are not consistent over different datasets and shifts",
    "volume": "main",
    "checked": true,
    "id": "0e845ef0a3ae71bd32a6954fafe0702d0f0f033f",
    "citation_count": 42
  },
  "https://openreview.net/forum?id=GQd7mXSPua": {
    "title": "Meta Learning Low Rank Covariance Factors for Energy Based Deterministic Uncertainty",
    "abstract": "Numerous recent works utilize bi-Lipschitz regularization of neural network layers to preserve relative distances between data instances in the feature spaces of each layer. This distance sensitivity with respect to the data aids in tasks such as uncertainty calibration and out-of-distribution (OOD) detection. In previous works, features extracted with a distance sensitive model are used to construct feature covariance matrices which are used in deterministic uncertainty estimation or OOD detection. However, in cases where there is a distribution over tasks, these methods result in covariances which are sub-optimal, as they may not leverage all of the meta information which can be shared among tasks. With the use of an attentive set encoder, we propose to meta learn either diagonal or diagonal plus low-rank factors to efficiently construct task specific covariance matrices. Additionally, we propose an inference procedure which utilizes scaled energy to achieve a final predictive distribution which is well calibrated under a distributional dataset shift",
    "volume": "main",
    "checked": true,
    "id": "4483571168f80e4efe83c0c9ccfd1730adacaa7d",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=PilZY3omXV2": {
    "title": "CoST: Contrastive Learning of Disentangled Seasonal-Trend Representations for Time Series Forecasting",
    "abstract": "Deep learning has been actively studied for time series forecasting, and the main-stream paradigm is based on the end-to-end training of neural network architectures, ranging from classical LSTM/RNNs to more recent TCNs and Transformers. Motivated by the recent success of representation learning in computer vi-sion and natural language processing, we argue that a more promising paradigm for time series forecasting, is to ﬁrst learn disentangled feature representations, followed by a simple regression ﬁne-tuning step – we justify such a paradigm from a causal perspective. Following this principle, we propose a new time series representation learning framework for long sequence time series forecasting named CoST, which applies contrastive learning methods to learn disentangled seasonal-trend representations. CoST comprises both time domain and frequency domain contrastive losses to learn discriminative trend and seasonal representations, respectively. Extensive experiments on real-world datasets show that CoST consistently outperforms the state-of-the-art methods by a considerable margin, achieving a 21.3% improvement in MSE on multivariate benchmarks. It is also robust to various choices of backbone encoders, as well as downstream regressors. Code is available at https://github.com/salesforce/CoST ",
    "volume": "main",
    "checked": true,
    "id": "c090ff3dec01e06f46735b7b9ab133a5db8c73c3",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=nZOUYEN6Wvy": {
    "title": "Granger causal inference on DAGs identifies genomic loci regulating transcription",
    "abstract": "When a dynamical system can be modeled as a sequence of observations, Granger causality is a powerful approach for detecting predictive interactions between its variables. However, traditional Granger causal inference has limited utility in domains where the dynamics need to be represented as directed acyclic graphs (DAGs) rather than as a linear sequence, such as with cell differentiation trajectories. Here, we present GrID-Net, a framework based on graph neural networks with lagged message passing for Granger causal inference on DAG-structured systems. Our motivating application is the analysis of single-cell multimodal data to identify genomic loci that mediate the regulation of specific genes. To our knowledge, GrID-Net is the first single-cell analysis tool that accounts for the temporal lag between a genomic locus becoming accessible and its downstream effect on a target gene's expression. We applied GrID-Net on multimodal single-cell assays that profile chromatin accessibility (ATAC-seq) and gene expression (RNA-seq) in the same cell and show that it dramatically outperforms existing methods for inferring regulatory locus–gene links, achieving up to 71% greater agreement with independent population genetics-based estimates. By extending Granger causality to DAG-structured dynamical systems, our work unlocks new domains for causal analyses and, more specifically, opens a path towards elucidating gene regulatory interactions relevant to cellular differentiation and complex human diseases at unprecedented scale and resolution.1",
    "volume": "main",
    "checked": true,
    "id": "3f75b8d436a7240189c9ce7a31c9bb3c09f99670",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=kavTY__jxp": {
    "title": "Spatial Graph Attention and Curiosity-driven Policy for Antiviral Drug Discovery",
    "abstract": "We developed Distilled Graph Attention Policy Network (DGAPN), a reinforcement learning model to generate novel graph-structured chemical representations that optimize user-defined objectives by efficiently navigating a physically constrained domain. The framework is examined on the task of generating molecules that are designed to bind, noncovalently, to functional sites of SARS-CoV-2 proteins. We present a spatial Graph Attention (sGAT) mechanism that leverages self-attention over both node and edge attributes as well as encoding the spatial structure -- this capability is of considerable interest in synthetic biology and drug discovery. An attentional policy network is introduced to learn the decision rules for a dynamic, fragment-based chemical environment, and state-of-the-art policy gradient techniques are employed to train the network with stability. Exploration is driven by the stochasticity of the action space design and the innovation reward bonuses learned and proposed by random network distillation. In experiments, our framework achieved outstanding results compared to state-of-the-art algorithms, while reducing the complexity of paths to chemical synthesis",
    "volume": "main",
    "checked": true,
    "id": "5d7cb59281f7152a0a7c7d4ac80117fb2e77d38b",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=figzpGMrdD": {
    "title": "Pretrained Language Model in Continual Learning: A Comparative Study",
    "abstract": "Continual learning (CL) is a setting in which a model learns from a stream of incoming data while avoiding to forget previously learned knowledge. Pre-trained language models (PLMs) have been successfully employed in continual learning of different natural language problems. With the rapid development of many continual learning methods and PLMs, understanding and disentangling their interactions become essential for continued improvement of continual learning performance. In this paper, we thoroughly compare the continual learning performance over the combination of 5 PLMs and 4 CL approaches on 3 benchmarks in 2 typical incremental settings. Our extensive experimental analyses reveal interesting performance differences across PLMs and across CL methods. Furthermore, our representativeness probing analyses dissect PLMs' performance characteristics in a layer-wise and task-wise manner, uncovering the extent to which their inner layers suffer from forgetting, and the effect of different CL approaches on each layer. Finally, our observations and analyses open up a number of important research questions that will inform and guide the design of effective continual learning techniques",
    "volume": "main",
    "checked": true,
    "id": "8e125d392ea0d8240be654d90a28838711a5bd36",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=G89-1yZLFHk": {
    "title": "Data Efficient Language-Supervised Zero-Shot Recognition with Optimal Transport Distillation",
    "abstract": "Traditional computer vision models are trained to predict a fixed set of predefined categories. Recently, natural language has been shown to be a broader and richer source of supervision that provides finer descriptions to visual concepts than supervised \"gold\" labels. Previous works, such as CLIP, use InfoNCE loss to train a model to predict the pairing between images and text captions. CLIP, however, is data hungry and requires more than 400M image-text pairs for training. The inefficiency can be partially attributed to the fact that the image-text pairs are noisy. To address this, we propose OTTER (Optimal TransporT distillation for Efficient zero-shot Recognition), which uses online entropic optimal transport to find a soft image-text match as labels for contrastive learning. Based on pretrained image and text encoders, models trained with OTTER achieve strong performance with only 3M image text pairs. Compared with InfoNCE loss, label smoothing, and knowledge distillation, OTTER consistently outperforms these baselines in zero-shot evaluation on Google Open Images (19,958 classes) and multi-labeled ImageNet 10K (10032 classes) from Tencent ML-Images. Over 42 evaluations on 7 different dataset/architecture settings x 6 metrics, OTTER outperforms (32) or ties (2) all baselines in 34 of them. Our source code is open sourced at https: //github.com/facebookresearch/OTTER",
    "volume": "main",
    "checked": true,
    "id": "38cc96e5a780716dc7e19e037d96d952b3a37940",
    "citation_count": 7
  },
  "https://openreview.net/forum?id=q7n2RngwOM": {
    "title": "$\\beta$-Intact-VAE: Identifying and Estimating Causal Effects under Limited Overlap",
    "abstract": "As an important problem in causal inference, we discuss the identiﬁcation and estimation of treatment effects (TEs) under limited overlap; that is, when subjects with certain features belong to a single treatment group. We use a latent variable to model a prognostic score which is widely used in biostatistics and sufﬁcient for TEs; i.e., we build a generative prognostic model. We prove that the latent variable recovers a prognostic score, and the model identiﬁes individualized treatment effects. The model is then learned as β -Intact-VAE––a new type of variational au-toencoder (VAE). We derive the TE error bounds that enable representations balanced for treatment groups conditioned on individualized features. The proposed method is compared with recent methods using (semi-)synthetic datasets",
    "volume": "main",
    "checked": true,
    "id": "9beafe78796e475c1ef2990f7873ffd9bc3f2962",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=psh0oeMSBiF": {
    "title": "COPA: Certifying Robust Policies for Offline Reinforcement Learning against Poisoning Attacks",
    "abstract": "As reinforcement learning (RL) has achieved near human-level performance in a variety of tasks, its robustness has raised great attention. While a vast body of research has explored test-time (evasion) attacks in RL and corresponding defenses, its robustness against training-time (poisoning) attacks remains largely unanswered. In this work, we focus on certifying the robustness of offline RL in the presence of poisoning attacks, where a subset of training trajectories could be arbitrarily manipulated. We propose the first certification framework, COPA, to certify the number of poisoning trajectories that can be tolerated regarding different certification criteria. Given the complex structure of RL, we propose two certification criteria: per-state action stability and cumulative reward bound. To further improve the certification, we propose new partition and aggregation protocols to train robust policies. We further prove that some of the proposed certification methods are theoretically tight and some are NP-Complete problems. We leverage COPA to certify three RL environments trained with different algorithms and conclude: (1) The proposed robust aggregation protocols such as temporal aggregation can significantly improve the certifications; (2) Our certifications for both per-state action stability and cumulative reward bound are efficient and tight; (3) The certification for different training algorithms and environments are different, implying their intrinsic robustness properties. All experimental results are available at https://copa-leaderboard.github.io",
    "volume": "main",
    "checked": true,
    "id": "9700b4d0a9e6a64452192ae1c98e1aba34bc8c28",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=4N-17dske79": {
    "title": "Associated Learning: an Alternative to End-to-End Backpropagation that Works on CNN, RNN, and Transformer",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "f11a560c7ce728fdd5f3cf5eca0199323e73b2b5",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=HOjLHrlZhmx": {
    "title": "CROP: Certifying Robust Policies for Reinforcement Learning through Functional Smoothing",
    "abstract": "As reinforcement learning (RL) has achieved great success and been even adopted in safety-critical domains such as autonomous vehicles, a range of empirical studies have been conducted to improve its robustness against adversarial attacks. However, how to certify its robustness with theoretical guarantees still remains challenging. In this paper, we present the first unified framework CROP (Certifying Robust Policies for RL) to provide robustness certification on both action and reward levels. In particular, we propose two robustness certification criteria: robustness of per-state actions and lower bound of cumulative rewards. We then develop a local smoothing algorithm for policies derived from Q-functions to guarantee the robustness of actions taken along the trajectory; we also develop a global smoothing algorithm for certifying the lower bound of a finite-horizon cumulative reward, as well as a novel local smoothing algorithm to perform adaptive search in order to obtain tighter reward certification. Empirically, we apply CROP to evaluate several existing empirically robust RL algorithms, including adversarial training and different robust regularization, in four environments (two representative Atari games, Highway, and CartPole). Furthermore, by evaluating these algorithms against adversarial attacks, we demonstrate that our certifications are often tight. All experiment results are available at website https://crop-leaderboard.github.io",
    "volume": "main",
    "checked": true,
    "id": "5374af7bb076f9eaea7aea3045edd9b6a76d0a3b",
    "citation_count": 11
  },
  "https://openreview.net/forum?id=UseMOjWENv": {
    "title": "MIDI-DDSP: Detailed Control of Musical Performance via Hierarchical Modeling",
    "abstract": "Musical expression requires control of both what notes are played, and how they are performed. Conventional audio synthesizers provide detailed expressive controls, but at the cost of realism. Black-box neural audio synthesis and concatenative samplers can produce realistic audio, but have few mechanisms for control. In this work, we introduce MIDI-DDSP, a hierarchical model of musical instruments that enables both realistic neural audio synthesis and detailed user control. Starting from interpretable Differentiable Digital Signal Processing (DDSP) synthesis parameters, we infer musical notes and high-level properties of their expressive performance (such as timbre, vibrato, dynamics, and articulation). This creates a 3-level hierarchy (notes, performance, synthesis) that affords individuals the option to intervene at each level, or utilize trained priors (performance given notes, synthesis given performance) for creative assistance. Through quantitative experiments and listening tests, we demonstrate that this hierarchy can reconstruct high-fidelity audio, accurately predict performance attributes for a note sequence, independently manipulate the attributes of a given performance, and as a complete system, generate realistic audio from a novel note sequence. By utilizing an interpretable hierarchy, with multiple levels of granularity, MIDI-DDSP opens the door to assistive tools to empower individuals across a diverse range of musical experience",
    "volume": "main",
    "checked": true,
    "id": "95a35473fd1936927dd4a53fe0a5d2d6762d99b3",
    "citation_count": 9
  },
  "https://openreview.net/forum?id=Qg2vi4ZbHM9": {
    "title": "StyleAlign: Analysis and Applications of Aligned StyleGAN Models",
    "abstract": "In this paper, we perform an in-depth study of the properties and applications of aligned generative models . We refer to two models as aligned if they share the same architecture, and one of them (the child ) is obtained from the other (the parent ) via ﬁne-tuning to another domain, a common practice in transfer learning. Several works already utilize some basic properties of aligned StyleGAN models to perform image-to-image translation. Here, we perform the ﬁrst detailed explo-ration of model alignment, also focusing on StyleGAN. First, we empirically an-alyze aligned models and provide answers to important questions regarding their nature. In particular, we ﬁnd that the child model's latent spaces are semantically aligned with those of the parent, inheriting incredibly rich semantics, even for distant data domains such as human faces and churches. Second, equipped with this better understanding, we leverage aligned models to solve a diverse set of tasks. In addition to image translation, we demonstrate fully automatic cross-domain image morphing. We further show that zero-shot vision tasks may be performed in the child domain, while relying exclusively on supervision in the parent domain. We demonstrate qualitatively and quantitatively that our approach yields state-of-the-art results, while requiring only simple ﬁne-tuning and inversion",
    "volume": "main",
    "checked": true,
    "id": "83bb34556b953a172bf5687131b24c17e015f476",
    "citation_count": 16
  },
  "https://openreview.net/forum?id=TrjbxzRcnf-": {
    "title": "Memorizing Transformers",
    "abstract": "Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights. We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. On benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time",
    "volume": "main",
    "checked": true,
    "id": "0e802c0739771acf70e60d59c2df51cd7e8c50c0",
    "citation_count": 23
  },
  "https://openreview.net/forum?id=06Wy2BtxXrz": {
    "title": "Learning Scenario Representation for Solving Two-stage Stochastic Integer Programs",
    "abstract": "Many practical combinatorial optimization problems under uncertainty can be modeled as stochastic integer programs (SIPs), which are extremely challenging to solve due to the high complexity. To solve two-stage SIPs efficiently, we propose a conditional variational autoencoder (CVAE) based method to learn scenario representation for a class of SIP instances. Specifically, we design a graph convolutional network based encoder to embed each scenario with the deterministic part of its instance (i.e. context) into a low-dimensional latent space, from which a decoder reconstructs the scenario from its latent representation conditioned on the context. Such a design effectively captures the dependencies of the scenarios on their corresponding instances. We apply the trained encoder to two tasks in typical SIP solving, i.e. scenario reduction and objective prediction. Experiments on two SIP problems show that the learned latent representation significantly boosts the solving performance to attain high-quality solutions in short computational time, and generalizes fairly well to problems of larger sizes or with more scenarios",
    "volume": "main",
    "checked": true,
    "id": "e5a4ca43024028b1916baeeaa67797a0d3cc7d01",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=nKWjE4QF1hB": {
    "title": "AlphaZero-based Proof Cost Network to Aid Game Solving",
    "abstract": "The AlphaZero algorithm learns and plays games without hand-crafted expert knowledge. However, since its objective is to play well, we hypothesize that a better objective can be deﬁned for the related but separate task of solving games. This paper proposes a novel approach to solving problems by modifying the training target of the AlphaZero algorithm, such that it prioritizes solving the game quickly, rather than winning. We train a Proof Cost Network (PCN), where proof cost is a heuristic that estimates the amount of work required to solve problems. This matches the general concept of the so-called proof number from proof number search, which has been shown to be well-suited for game solving. We propose two speciﬁc training targets. The ﬁrst ﬁnds the shortest path to a solution, while the second estimates the proof cost. We conduct experiments on solving 15x15 Gomoku and 9x9 Killall-Go problems with both MCTS-based and focused depth-ﬁrst proof number search solvers. Comparisons between using AlphaZero networks and PCN as heuristics show that PCN can solve more problems",
    "volume": "main",
    "checked": true,
    "id": "6305bb7712d21a090f65832df5194c62260b330b",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=hGXij5rfiHw": {
    "title": "Discovering Invariant Rationales for Graph Neural Networks",
    "abstract": "Intrinsic interpretability of graph neural networks (GNNs) is to find a small subset of the input graph's features — rationale — which guides the model prediction. Unfortunately, the leading rationalization models often rely on data biases, especially shortcut features, to compose rationales and make predictions without probing the critical and causal patterns. Moreover, such data biases easily change outside the training distribution. As a result, these models suffer from a huge drop in interpretability and predictive performance on out-of-distribution data. In this work, we propose a new strategy of discovering invariant rationale (DIR) to construct intrinsically interpretable GNNs. It conducts interventions on the training distribution to create multiple interventional distributions. Then it approaches the causal rationales that are invariant across different distributions while filtering out the spurious patterns that are unstable. Experiments on both synthetic and realworld datasets validate the superiority of our DIR in terms of interpretability and generalization ability on graph classification over the leading baselines. Code and datasets are available at https://github.com/Wuyxin/DIR-GNN",
    "volume": "main",
    "checked": true,
    "id": "bd15a322c20f891f38e247bd5ed6e9d2f0b637eb",
    "citation_count": 29
  },
  "https://openreview.net/forum?id=iEx3PiooLy": {
    "title": "VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D ARTiculated Objects",
    "abstract": "Perceiving and manipulating 3D articulated objects ( e.g. , cabinets, doors) in human environments is an important yet challenging task for future home-assistant robots. The space of 3D articulated objects is exceptionally rich in their myriad semantic categories, diverse shape geometry, and complicated part functionality. Previous works mostly abstract kinematic structure with estimated joint parameters and part poses as the visual representations for manipulating 3D articulated objects. In this paper, we propose object-centric actionable visual priors as a novel perception-interaction handshaking point that the perception system outputs more actionable guidance than kinematic structure estimation, by predicting dense geometry-aware , interaction-aware , and task-aware visual action affordance and trajectory proposals. We design an interaction-for-perception framework VAT-M ART to learn such actionable visual representations by simultaneously training a curiosity-driven reinforcement learning policy exploring diverse interaction trajectories and a perception module summarizing and generalizing the explored knowledge for pointwise predictions among diverse shapes. Experiments prove the effectiveness of the proposed approach using the large-scale PartNet-Mobility dataset in SAPIEN environment and show promising generalization capabilities to novel test shapes, unseen object categories, and real-world dataset 3D We use 562 to perform our experiments and show that our VAT-M ART . We also observe reasonably good gen-2 eralization capabilities over unseen shapes, novel object categories, and real-world data, thanks to large-scale training over diverse textureless geometry",
    "volume": "main",
    "checked": true,
    "id": "883ac0fcebca7f8df1a8a253c4d27ed83494efae",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=FQOC5u-1egI": {
    "title": "Handling Distribution Shifts on Graphs: An Invariance Perspective",
    "abstract": "There is increasing evidence suggesting neural networks' sensitivity to distribution shifts, so that research on out-of-distribution (OOD) generalization comes into the spotlight. Nonetheless, current endeavors mostly focus on Euclidean data, and its formulation for graph-structured data is not clear and remains under-explored, given two-fold fundamental challenges: 1) the inter-connection among nodes in one graph, which induces non-IID generation of data points even under the same environment, and 2) the structural information in the input graph, which is also informative for prediction. In this paper, we formulate the OOD problem on graphs and develop a new invariant learning approach, Explore-to-Extrapolate Risk Minimization (EERM), that facilitates graph neural networks to leverage invariance principles for prediction. EERM resorts to multiple context explorers (specified as graph structure editers in our case) that are adversarially trained to maximize the variance of risks from multiple virtual environments. Such a design enables the model to extrapolate from a single observed environment which is the common case for node-level prediction. We prove the validity of our method by theoretically showing its guarantee of a valid OOD solution and further demonstrate its power on various real-world datasets for handling distribution shifts from artificial spurious features, cross-domain transfers and dynamic graph evolution1",
    "volume": "main",
    "checked": true,
    "id": "4818381d399636a4caefc24710b5c26a1e6e906c",
    "citation_count": 15
  },
  "https://openreview.net/forum?id=CALFyKVs87": {
    "title": "Dynamics-Aware Comparison of Learned Reward Functions",
    "abstract": "The ability to learn reward functions plays an important role in enabling the deployment of intelligent agents in the real world. However, comparing reward functions, for example as a means of evaluating reward learning methods, presents a challenge. Reward functions are typically compared by considering the behavior of optimized policies, but this approach conflates deficiencies in the reward function with those of the policy search algorithm used to optimize it. To address this challenge, Gleave et al. (2020) propose the Equivalent-Policy Invariant Comparison (EPIC) distance. EPIC avoids policy optimization, but in doing so requires computing reward values at transitions that may be impossible under the system dynamics. This is problematic for learned reward functions because it entails evaluating them outside of their training distribution, resulting in inaccurate reward values that we show can render EPIC ineffective at comparing rewards. To address this problem, we propose the Dynamics-Aware Reward Distance (DARD), a new reward pseudometric. DARD uses an approximate transition model of the environment to transform reward functions into a form that allows for comparisons that are invariant to reward shaping while only evaluating reward functions on transitions close to their training distribution. Experiments in simulated physical domains demonstrate that DARD enables reliable reward comparisons without policy optimization and is significantly more predictive than baseline methods of downstream policy performance when dealing with learned reward functions.†",
    "volume": "main",
    "checked": true,
    "id": "bd45112d6f1745a6dccf10f4c423ad9331102cde",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=xENf4QUL4LW": {
    "title": "Sample Selection with Uncertainty of Losses for Learning with Noisy Labels",
    "abstract": "In learning with noisy labels, the sample selection approach is very popular, which 1 regards small-loss data as correctly labeled during training. However, losses are 2 generated on-the-ﬂy based on the model being trained with noisy labels, and thus 3 large-loss data are likely but not certainly to be incorrect. There are actually 4 two possibilities of a large-loss data point: (a) it is mislabeled, and then its loss 5 decreases slower than other data, since deep neural networks \"learn patterns ﬁrst\"; 6 (b) it belongs to an underrepresented group of data and has not been selected yet . In 7 this paper, we incorporate the uncertainty of losses by adopting interval estimation 8 instead of point estimation of losses, where lower bounds of the conﬁdence intervals 9 of losses derived from distribution-free concentration inequalities , but not losses 10 themselves, are used for sample selection. In this way, we also give large-loss but 11 less selected data a try; then, we can better distinguish between the cases (a) and 12 (b) by seeing if the losses effectively decrease with the uncertainty after the try. As 13 a result, we can better explore underrepresented data that are correctly labeled but 14 seem to be mislabeled at ﬁrst glance . Experiments demonstrate that the proposed 15 method is superior to baselines and robust to a broad range of label noise types. 16",
    "volume": "main",
    "checked": true,
    "id": "b9ea1c5fd417c5af2e3aa44a2a3c66f77bac190e",
    "citation_count": 17
  },
  "https://openreview.net/forum?id=k7-s5HSSPE5": {
    "title": "Cross-Lingual Transfer with Class-Weighted Language-Invariant Representations",
    "abstract": "Recent advances in neural modeling have produced deep multilingual language models capable of extracting cross-lingual knowledge from non-parallel texts and enabling zero-shot downstream transfer. While their success is often attributed to shared representations, quantitative analyses are limited. Towards a better understanding, through empirical analyses, we show that the invariance of feature representations across languages—an effect of shared representations—strongly correlates with transfer performance. We also observe that distributional shifts in class priors between source and target language task data negatively affect performance, a largely overlooked issue that could cause negative transfer with existing unsupervised approaches. Based on these ﬁndings, we propose and evaluate a method for unsupervised transfer, called importance-weighted domain alignment (IWDA), that performs representation alignment with prior shift estimation and correction using unlabeled target language task data. Experiments demonstrate its superiority under large prior shifts, and show further performance gains when combined with existing semi-supervised learning techniques",
    "volume": "main",
    "checked": true,
    "id": "415f924c5c79e300891881af367e4d77602f9f39",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=MSgB8D4Hy51": {
    "title": "Post-Training Detection of Backdoor Attacks for Two-Class and Multi-Attack Scenarios",
    "abstract": "Backdoor attacks (BAs) are an emerging threat to deep neural network classifiers. A victim classifier will predict to an attacker-desired target class whenever a test sample is embedded with the same backdoor pattern (BP) that was used to poison the classifier's training set. Detecting whether a classifier is backdoor attacked is not easy in practice, especially when the defender is, e.g., a downstream user without access to the classifier's training set. This challenge is addressed here by a reverse-engineering defense (RED), which has been shown to yield state-of-the-art performance in several domains. However, existing REDs are not applicable when there are only two classes or when multiple attacks are present. These scenarios are first studied in the current paper, under the practical constraints that the defender neither has access to the classifier's training set nor to supervision from clean reference classifiers trained for the same domain. We propose a detection framework based on BP reverseengineering and a novel expected transferability (ET) statistic. We show that our ET statistic is effective using the same detection threshold, irrespective of the classification domain, the attack configuration, and the BP reverse-engineering algorithm that is used. The excellent performance of our method is demonstrated on six benchmark datasets. Notably, our detection framework is also applicable to multi-class scenarios with multiple attacks. Code is available at https://github.com/zhenxianglance/2ClassBADetection",
    "volume": "main",
    "checked": true,
    "id": "c85f7d1e65be9465ac76ea2e1711dbb57d733285",
    "citation_count": 8
  },
  "https://openreview.net/forum?id=shbAgEsk3qM": {
    "title": "Understanding and Leveraging Overparameterization in Recursive Value Estimation",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "46f100e7f8a8e73b38371d4ddef110fece012031",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=JprM0p-q0Co": {
    "title": "Tackling the Generative Learning Trilemma with Denoising Diffusion GANs",
    "abstract": "A wide variety of deep generative models has been developed in the past decade. Yet, these models often struggle with simultaneously addressing three key requirements including: high sample quality, mode coverage, and fast sampling. We call the challenge imposed by these requirements the generative learning trilemma, as the existing models often trade some of them for others. Particularly, denoising diffusion models have shown impressive sample quality and diversity, but their expensive sampling does not yet allow them to be applied in many real-world applications. In this paper, we argue that slow sampling in these models is fundamentally attributed to the Gaussian assumption in the denoising step which is justified only for small step sizes. To enable denoising with large steps, and hence, to reduce the total number of denoising steps, we propose to model the denoising distribution using a complex multimodal distribution. We introduce denoising diffusion generative adversarial networks (denoising diffusion GANs) that model each denoising step using a multimodal conditional GAN. Through extensive evaluations, we show that denoising diffusion GANs obtain sample quality and diversity competitive with original diffusion models while being 2000× faster on the CIFAR-10 dataset. Compared to traditional GANs, our model exhibits better mode coverage and sample diversity. To the best of our knowledge, denoising diffusion GAN is the first model that reduces sampling cost in diffusion models to an extent that allows them to be applied to real-world applications inexpensively. Project page and code: https://nvlabs.github.io/denoising-diffusion-gan",
    "volume": "main",
    "checked": true,
    "id": "0d4154cbd76c4753ba3cb7a5b89ab29bab53384f",
    "citation_count": 59
  },
  "https://openreview.net/forum?id=CIaQKbTBwtU": {
    "title": "Learning to Generalize across Domains on Single Test Samples",
    "abstract": "We strive to learn a model from a set of source domains that generalizes well to unseen target domains. The main challenge in such a domain generalization scenario is the unavailability of any target domain data during training, resulting in the learned model not being explicitly adapted to the unseen target domains. We propose learning to generalize across domains on single test samples. We leverage a meta-learning paradigm to learn our model to acquire the ability of adaptation with single samples at training time so as to further adapt itself to each single test sample at test time. We formulate the adaptation to the single test sample as a variational Bayesian inference problem, which incorporates the test sample as a conditional into the generation of model parameters. The adaptation to each test sample requires only one feed-forward computation at test time without any fine-tuning or self-supervised training on additional data from the unseen domains. Extensive ablation studies demonstrate that our model learns the ability to adapt models to each single sample by mimicking domain shifts during training. Further, our model achieves at least comparable – and often better – performance than state-of-the-art methods on multiple benchmarks for domain generalization 1",
    "volume": "main",
    "checked": true,
    "id": "e6a619cdb4bdfa973078809f4b4d215c98174e2f",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=03RLpj-tc_": {
    "title": "Crystal Diffusion Variational Autoencoder for Periodic Material Generation",
    "abstract": "Generating the periodic structure of stable materials is a long-standing challenge for the material design community. This task is difficult because stable materials only exist in a low-dimensional subspace of all possible periodic arrangements of atoms: 1) the coordinates must lie in the local energy minimum defined by quantum mechanics, and 2) global stability also requires the structure to follow the complex, yet specific bonding preferences between different atom types. Existing methods fail to incorporate these factors and often lack proper invariances. We propose a Crystal Diffusion Variational Autoencoder (CDVAE) that captures the physical inductive bias of material stability. By learning from the data distribution of stable materials, the decoder generates materials in a diffusion process that moves atomic coordinates towards a lower energy state and updates atom types to satisfy bonding preferences between neighbors. Our model also explicitly encodes interactions across periodic boundaries and respects permutation, translation, rotation, and periodic invariances. We significantly outperform past methods in three tasks: 1) reconstructing the input structure, 2) generating valid, diverse, and realistic materials, and 3) generating materials that optimize a specific property. We also provide several standard datasets and evaluation metrics for the broader machine learning community. 1",
    "volume": "main",
    "checked": true,
    "id": "f50f877b07d64f116de7bf161cf009d2ebad7d15",
    "citation_count": 22
  },
  "https://openreview.net/forum?id=RdJVFCHjUMI": {
    "title": "An Explanation of In-context Learning as Implicit Bayesian Inference",
    "abstract": "Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn . Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning 1 . Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning",
    "volume": "main",
    "checked": true,
    "id": "10bd4160b44803ada6a3d2e366c44b7e2a4ffe90",
    "citation_count": 21
  },
  "https://openreview.net/forum?id=MsHnJPaBUZE": {
    "title": "iFlood: A Stable and Effective Regularizer",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "7e8124222117525876f346122329c13afa81008d",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=31d5RLCUuXC": {
    "title": "A Tale of Two Flows: Cooperative Learning of Langevin Flow and Normalizing Flow Toward Energy-Based Model",
    "abstract": "This paper studies the cooperative learning of two generative ﬂow models, in which the two models are iteratively updated based on the jointly synthesized examples. The ﬁrst ﬂow model is a normalizing ﬂow that transforms an initial simple density into a target density by applying a sequence of invertible transformations. The second ﬂow model is a Langevin ﬂow that runs ﬁnite steps of gradient-based MCMC toward an energy-based model. We start from proposing a generative framework that trains an energy-based model with a normalizing ﬂow as an amortized sampler to initialize the MCMC chains of the energy-based model. In each learning iteration, we generate synthesized examples by using a normalizing ﬂow initialization followed by a short-run Langevin ﬂow revision toward the current energy-based model. Then we treat the synthesized examples as fair samples from the energy-based model and update the model parameters with the maximum likelihood learning gradient, while the normalizing ﬂow directly learns from the synthesized examples by maximizing the tractable likelihood. Under the short-run non-mixing MCMC scenario, the estimation of the energy-based model is shown to follow the perturbation of maximum likelihood, and the short-run Langevin ﬂow and the normalizing ﬂow form a two-ﬂow generator that we call CoopFlow. We provide an understating of the CoopFlow algorithm by information geometry and show that it is a valid generator as it converges to a moment matching estimator. We demonstrate that the trained CoopFlow is capable of synthesizing realistic images, reconstructing images, and interpolating between images. Our paper studies amortized sampling for training a short-run non-mixing Langevin sampler toward an EBM. We propose a novel framework, the CoopFlow, which cooperatively trains a short-run Langevin ﬂow as a valid generator and a normalizing ﬂow as an amortized sampler for image representation. We provide both theoretical and empirical justiﬁcations for the proposed CoopFlow algorithm, which has been implemented in the PaddlePaddle deep learning platform. The following are some closely related work. We will point out the differences between our work and the prior arts to further highlight the contributions and novelty of our paper",
    "volume": "main",
    "checked": true,
    "id": "e598e7a5ca5f37cf2e5fe6e3805f541bd65c5439",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=k7efTb0un9z": {
    "title": "Learning to Schedule Learning rate with Graph Neural Networks",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "a4bea03d7a62917bf46ece2eb1e24f1fcacbc6ed",
    "citation_count": 1
  },
  "https://openreview.net/forum?id=kcwyXtt7yDJ": {
    "title": "Graph-Relational Domain Adaptation",
    "abstract": "Existing domain adaptation methods tend to treat every domain equally and align them all perfectly. Such uniform alignment ignores topological structures among different domains; therefore it may be beneficial for nearby domains, but not necessarily for distant domains. In this work, we relax such uniform alignment by using a domain graph to encode domain adjacency, e.g., a graph of states in the US with each state as a domain and each edge indicating adjacency, thereby allowing domains to align flexibly based on the graph structure. We generalize the existing adversarial learning framework with a novel graph discriminator using encodingconditioned graph embeddings. Theoretical analysis shows that at equilibrium, our method recovers classic domain adaptation when the graph is a clique, and achieves non-trivial alignment for other types of graphs. Empirical results show that our approach successfully generalizes uniform alignment, naturally incorporates domain information represented by graphs, and improves upon existing domain adaptation methods on both synthetic and real-world datasets1",
    "volume": "main",
    "checked": true,
    "id": "e18c5308517280207f1095d876f8ffe109ebf529",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=XGzk5OKWFFc": {
    "title": "CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation",
    "abstract": "Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from a labeled source domain to a different unlabeled target domain. Most existing UDA methods focus on learning domain-invariant feature representation, either from the domain level or category level, using convolution neural networks (CNNs)-based frameworks. One fundamental problem for the category level based UDA is the production of pseudo labels for samples in target domain, which are usually too noisy for accurate domain alignment, inevitably compromising the UDA performance. With the success of Transformer in various tasks, we find that the cross-attention in Transformer is robust to the noisy input pairs for better feature alignment, thus in this paper Transformer is adopted for the challenging UDA task. Specifically, to generate accurate input pairs, we design a two-way centeraware labeling algorithm to produce pseudo labels for target samples. Along with the pseudo labels, a weight-sharing triple-branch transformer framework is proposed to apply self-attention and cross-attention for source/target feature learning and source-target domain alignment, respectively. Such design explicitly enforces the framework to learn discriminative domain-specific and domain-invariant representations simultaneously. The proposed method is dubbed CDTrans (cross-domain transformer), and it provides one of the first attempts to solve UDA tasks with a pure transformer solution. Extensive experiments show that our proposed method achieves the best performance on Office-Home, VisDA-2017, and DomainNet datasets",
    "volume": "main",
    "checked": true,
    "id": "f734996bd0300856f6fabd3dde15ef080967cb02",
    "citation_count": 15
  },
  "https://openreview.net/forum?id=N0uJGWDw21d": {
    "title": "Bag of Instances Aggregation Boosts Self-supervised Distillation",
    "abstract": "Recent advances in self-supervised learning have experienced remarkable progress, especially for contrastive learning based methods, which regard each image as well as its augmentations as an individual class and try to distinguish them from all other images. However, due to the large quantity of exemplars, this kind of pretext task intrinsically suffers from slow convergence and is hard for optimization. This is especially true for small scale models, which we find the performance drops dramatically comparing with its supervised counterpart. In this paper, we propose a simple but effective distillation strategy for unsupervised learning. The highlight is that the relationship among similar samples counts and can be seamlessly transferred to the student to boost the performance. Our method, termed as BINGO, which is short for Bag of InstaNces aGgregatiOn, targets at transferring the relationship learned by the teacher to the student. Here bag of instances indicates a set of similar samples constructed by the teacher and are grouped within a bag, and the goal of distillation is to aggregate compact representations over the student with respect to instances in a bag. Notably, BINGO achieves new state-of-the-art performance on small scale models, i.e., 65.5% and 68.9% top-1 accuracies with linear evaluation on ImageNet, using ResNet-18 and ResNet-34 as backbone, respectively, surpassing baselines (52.5% and 57.4% top-1 accuracies) by a significant margin. The code is available at https://github.com/haohang96/bingo",
    "volume": "main",
    "checked": true,
    "id": "30e864862e59221174e5c9dea7d0cf847463f71c",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=541PxiEKN3F": {
    "title": "Acceleration of Federated Learning with Alleviated Forgetting in Local Training",
    "abstract": "Federated learning (FL) enables distributed optimization of machine learning models while protecting privacy by independently training local models on each client and then aggregating parameters on a central server, thereby producing an effective global model. Although a variety of FL algorithms have been proposed, their training efficiency remains low when the data are not independently and identically distributed (non-i.i.d.) across different clients. We observe that the slow convergence rates of the existing methods are (at least partially) caused by the catastrophic forgetting issue during the local training stage on each individual client, which leads to a large increase in the loss function concerning the previous training data at the other clients. Here, we propose FedReg, an algorithm to accelerate FL with alleviated knowledge forgetting in the local training stage by regularizing locally trained parameters with the loss on generated pseudo data, which encode the knowledge of previous training data learned by the global model. Our comprehensive experiments demonstrate that FedReg not only significantly improves the convergence rate of FL, especially when the neural network architecture is deep and the clients' data are extremely non-i.i.d., but is also able to protect privacy better in classification problems and more robust against gradient inversion attacks. The code is available at: https://github.com/Zoesgithub/FedReg",
    "volume": "main",
    "checked": true,
    "id": "2631cddf793aab58cd38aacc45b0e04786db4e15",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=nnU3IUMJmN": {
    "title": "Capturing Structural Locality in Non-parametric Language Models",
    "abstract": "Structural locality is a ubiquitous feature of real-world datasets, wherein data points are organized into local hierarchies. Some examples include topical clusters in text or project hierarchies in source code repositories. In this paper, we explore utilizing this structural locality within non-parametric language models, which generate sequences that reference retrieved examples from an external source. We propose a simple yet effective approach for adding locality information into such models by adding learned parameters that improve the likelihood of retrieving examples from local neighborhoods. Experiments on two different domains, Java source code and Wikipedia text, demonstrate that locality features improve model efficacy over models without access to these features, with interesting differences. We also perform an analysis of how and where locality features contribute to improved performance and why the traditionally used contextual similarity metrics alone are not enough to grasp the locality structure",
    "volume": "main",
    "checked": true,
    "id": "c6bb04f3d8000b7e800f6359082de39548c7da79",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=DIjCrlsu6Z": {
    "title": "Controlling Directions Orthogonal to a Classifier",
    "abstract": "We propose to identify directions invariant to a given classifier so that these directions can be controlled in tasks such as style transfer. While orthogonal decomposition is directly identifiable when the given classifier is linear, we formally define a notion of orthogonality in the non-linear case. We also provide a surprisingly simple method for constructing the orthogonal classifier (a classifier utilizing directions other than those of the given classifier). Empirically, we present three use cases where controlling orthogonal variation is important: style transfer, domain adaptation, and fairness. The orthogonal classifier enables desired style transfer when domains vary in multiple aspects, improves domain adaptation with label shifts and mitigates the unfairness as a predictor. The code is available at https://github.com/Newbeeer/orthogonal_classifier",
    "volume": "main",
    "checked": true,
    "id": "c2c26d7e6b3679cd0a48ca8eace3ffed24f273a3",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=LzQQ89U1qm_": {
    "title": "Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy",
    "abstract": "Unsupervised detection of anomaly points in time series is a challenging problem, which requires the model to derive a distinguishable criterion. Previous methods tackle the problem mainly through learning pointwise representation or pairwise association, however, neither is sufficient to reason about the intricate dynamics. Recently, Transformers have shown great power in unified modeling of pointwise representation and pairwise association, and we find that the self-attention weight distribution of each time point can embody rich association with the whole series. Our key observation is that due to the rarity of anomalies, it is extremely difficult to build nontrivial associations from abnormal points to the whole series, thereby, the anomalies' associations shall mainly concentrate on their adjacent time points. This adjacent-concentration bias implies an association-based criterion inherently distinguishable between normal and abnormal points, which we highlight through the Association Discrepancy. Technically, we propose the Anomaly Transformer with a new Anomaly-Attention mechanism to compute the association discrepancy. A minimax strategy is devised to amplify the normal-abnormal distinguishability of the association discrepancy. The Anomaly Transformer achieves state-of-theart results on six unsupervised time series anomaly detection benchmarks of three applications: service monitoring, space & earth exploration, and water treatment",
    "volume": "main",
    "checked": true,
    "id": "a46b06a4b8b4deecf96a4e42cd19b4696f999e66",
    "citation_count": 19
  },
  "https://openreview.net/forum?id=PzcvxEMzvQC": {
    "title": "GeoDiff: A Geometric Diffusion Model for Molecular Conformation Generation",
    "abstract": "Predicting molecular conformations from molecular graphs is a fundamental problem in cheminformatics and drug discovery. Recently, significant progress has been achieved with machine learning approaches, especially with deep generative models. Inspired by the diffusion process in classical non-equilibrium thermodynamics where heated particles will diffuse from original states to a noise distribution, in this paper, we propose a novel generative model named GEODIFF for molecular conformation prediction. GEODIFF treats each atom as a particle and learns to directly reverse the diffusion process (i.e., transforming from a noise distribution to stable conformations) as a Markov chain. Modeling such a generation process is however very challenging as the likelihood of conformations should be rototranslational invariant. We theoretically show that Markov chains evolving with equivariant Markov kernels can induce an invariant distribution by design, and further propose building blocks for the Markov kernels to preserve the desirable equivariance property. The whole framework can be efficiently trained in an end-toend fashion by optimizing a weighted variational lower bound to the (conditional) likelihood. Experiments on multiple benchmarks show that GEODIFF is superior or comparable to existing state-of-the-art approaches, especially on large molecules.1",
    "volume": "main",
    "checked": true,
    "id": "c871d2dc802d276608a6734637f8bc9e6da0d837",
    "citation_count": 39
  },
  "https://openreview.net/forum?id=jT1EwXu-4hj": {
    "title": "From Intervention to Domain Transportation: A Novel Perspective to Optimize Recommendation",
    "abstract": "The interventional nature of recommendation has attracted increasing attention in recent years. It particularly motivates researchers to formulate learning and evaluating recommendation as causal inference and data missing-not-at-random problems. However, few take seriously the consequence of violating the critical assumption of overlapping, which we prove can significantly threaten the validity and interpretation of the outcome. We find a critical piece missing in the current understanding of information retrieval (IR) systems: as interventions, recommendation not only affects the already observed data, but it also interferes with the target domain (distribution) of interest. We then rephrase optimizing recommendation as finding an intervention that best transports the patterns it learns from the observed domain to its intervention domain. Towards this end, we use domain transportation to characterize the learning-intervention mechanism of recommendation. We design a principled transportation-constraint risk minimization objective and convert it to a two-player minimax game. We prove the consistency, generalization, and excessive risk bounds for the proposed objective, and elaborate how they compare to the current results. Finally, we carry out extensive real-data and semi-synthetic experiments to demonstrate the advantage of our approach, and launch online testing with a real-world IR system",
    "volume": "main",
    "checked": true,
    "id": "588b41d7a6d97fbf52fc7d4086752e6330951fc9",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=OzyXtIZAzFv": {
    "title": "Task-Induced Representation Learning",
    "abstract": "In this work, we evaluate the effectiveness of representation learning approaches for decision making in visually complex environments. Representation learning is essential for effective reinforcement learning (RL) from high-dimensional inputs. Unsupervised representation learning approaches based on reconstruction, prediction or contrastive learning have shown substantial learning efﬁciency gains. Yet, they have mostly been evaluated in clean laboratory or simulated settings. In contrast, real environments are visually complex and contain substantial amounts of clutter and distractors. Unsupervised representations will learn to model such distractors, potentially impairing the agent's learning efﬁciency. In contrast, an alternative class of approaches, which we call task-induced representation learning , leverages task information such as rewards or demonstrations from prior tasks to focus on task-relevant parts of the scene and ignore distractors. We investigate the effectiveness of unsupervised and task-induced representation learning approaches on four visually complex environments, from Distracting DMControl to the CARLA driving simulator. For both, RL and imitation learning, we ﬁnd that representation learning generally improves sample efﬁciency on unseen tasks even in visually complex scenes and that task-induced representations can double learning efﬁciency compared to unsupervised alternatives. 1 a scene while ignoring distractors. We compare task-induced representations to common unsupervised representation learning approaches across four visually complex environments with substantial distractors and ﬁnd that they lead to improved sample efﬁciency on downstream tasks compared to unsupervised approaches. Future work should investigate approaches for incorporating other sources of task information such as language commands to learn task-induced representation",
    "volume": "main",
    "checked": true,
    "id": "821eda22a55fecd25d1cf6c9d120274cb6ef36ba",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=085y6YPaYjP": {
    "title": "Zero-Shot Self-Supervised Learning for MRI Reconstruction",
    "abstract": "Deep learning (DL) has emerged as a powerful tool for accelerated MRI reconstruction, but these methods often necessitate a database of fully-sampled measurements for training. Recent self-supervised and unsupervised learning approaches enable training without fully-sampled data. However, a database of undersampled measurements may not be available in many scenarios, especially for scans involving contrast or recently developed translational acquisitions. Moreover, database-trained models may not generalize well when the unseen measurements differ in terms of sampling pattern, acceleration rate, SNR, image contrast, and anatomy. Such challenges necessitate a new methodology that can enable scan-speciﬁc DL MRI reconstruction without any external training datasets. In this work, we propose a zero-shot self-supervised learning approach to perform scan-speciﬁc accelerated MRI reconstruction to tackle these issues. The proposed approach splits available measurements for each scan into three disjoint sets. Two of these sets are used to enforce data consistency and deﬁne loss during training, while the last set is used to establish an early stopping criterion. In the presence of models pre-trained on a database with different image characteristics, we show that the proposed approach can be combined with transfer learning to further improve reconstruction quality",
    "volume": "main",
    "checked": true,
    "id": "7c830468dceeb9ab4cc8ca12b78165745e6fdca9",
    "citation_count": 13
  },
  "https://openreview.net/forum?id=vIC-xLFuM6": {
    "title": "Overcoming The Spectral Bias of Neural Value Approximation",
    "abstract": "Value approximation using deep neural networks is at the heart of off-policy deep reinforcement learning, and is often the primary module that provides learning signals to the rest of the algorithm. While multi-layer perceptron networks are universal function approximators, recent works in neural kernel regression suggest the presence of a spectral bias, where fitting high-frequency components of the value function requires exponentially more gradient update steps than the low-frequency ones. In this work, we re-examine off-policy reinforcement learning through the lens of kernel regression and propose to overcome such bias via a composite neural tangent kernel. With just a single line-change, our approach, the Fourier feature networks (FFN) produce state-of-the-art performance on challenging continuous control domains with only a fraction of the compute. Faster convergence and better off-policy stability also make it possible to remove the target network without suffering catastrophic divergences, which further reduces TD(0)'s estimation bias on a few tasks. Code and analysis available at https://geyang.github.io/ffn",
    "volume": "main",
    "checked": true,
    "id": "f57ad283f8d55c3f49d36dcc4673243ec352f6c7",
    "citation_count": 5
  },
  "https://openreview.net/forum?id=OjPmfr9GkVv": {
    "title": "Enhancing Cross-lingual Transfer by Manifold Mixup",
    "abstract": "Based on large-scale pre-trained multilingual representations, recent cross-lingual transfer methods have achieved impressive transfer performances. However, the performance of target languages still lags far behind the source language. In this paper, our analyses indicate such a performance gap is strongly associated with the cross-lingual representation discrepancy. To achieve better cross-lingual transfer performance, we propose the cross-lingual manifold mixup (X-M IXUP ) method, which adaptively calibrates the representation discrepancy and gives compromised representations for target languages. Experiments on the XTREME benchmark show X-M IXUP achieves 1.8% performance gains on multiple text understanding tasks, compared with strong baselines, and reduces the cross-lingual representation discrepancy signiﬁcantly. implementing the cross-lingual manifold mixup on parallel data. Findings on the relationship between the cross-lingual transfer performance and representation discrepancy shed light on a promising way to boost cross-lingual transfer for future research",
    "volume": "main",
    "checked": true,
    "id": "fd708dc43c0ed70ed03b2818a3f50fedda6d7f6e",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=NYBmJN4MyZ": {
    "title": "Safe Neurosymbolic Learning with Differentiable Symbolic Execution",
    "abstract": "We study the problem of learning worst-case-safe parameters for programs that use neural networks as well as symbolic, human-written code. Such neurosymbolic programs arise in many safety-critical domains. However, because they need not be continuous, let alone differentiable, they cannot be learned using existing gradient-based approaches to safe learning. Our method, Differentiable Symbolic Execution (D SE ), learns such programs by sampling code paths using symbolic execution, constructing gradients of a worst-case \"safety loss\" along these paths, and then backpropagating these gradients through program operations using a generalization of the R EINFORCE estimator. We evaluate the method on real-world benchmarks. Our experiments show that D SE signiﬁcantly outperforms the state-of-the-art D IFF A I method on these tasks",
    "volume": "main",
    "checked": true,
    "id": "66c96d6eceaefed1509dbc720ae3f997d3116432",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=zNR43c03lRy": {
    "title": "Learning to Annotate Part Segmentation with Gradient Matching",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "b18072fa828f4d19319ed76d97cb2b356a29f67c",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=kQ2SOflIOVC": {
    "title": "Towards Better Understanding and Better Generalization of Low-shot Classification in Histology Images with Contrastive Learning",
    "abstract": "Few-shot learning is an established topic in natural images for years, but few work is attended to histology images, which is of high clinical value since well-labeled datasets and rare abnormal samples are expensive to collect. Here, we facilitate the study of few-shot learning in histology images by setting up three cross-domain tasks that simulate real clinics problems. To enable label-efficient learning and better generalizability, we propose to incorporate contrastive learning (CL) with latent augmentation (LA) to build a few-shot system. CL learns useful representations without manual labels, while LA transfers semantic variations of the base dataset in an unsupervised way. These two components fully exploit unlabeled training data and can scale gracefully to other label-hungry problems. In experiments, we find i) models learned by CL generalize better than supervised learning for histology images in unseen classes, and ii) LA brings consistent gains over baselines. Prior studies of self-supervised learning mainly focus on ImageNet-like images, which only present a dominant object in their centers. Recent attention has been paid to images with multi-objects and multi-textures (Chen & Li, 2020). Histology images are a natural choice for such a study. We show the superiority of CL over supervised learning in terms of generalization for such data and provide our empirical understanding for this observation. The findings in this work could contribute to understanding how the model generalizes in the context of both representation learning and histological image analysis. Code is available at https://github.com/TencentAILabHealthcare/Few-shot-WSI",
    "volume": "main",
    "checked": true,
    "id": "620611fb99f47be149ff2b4b2f5f73432dda5557",
    "citation_count": 2
  },
  "https://openreview.net/forum?id=OqcZu8JIIzS": {
    "title": "Pareto Policy Pool for Model-based Offline Reinforcement Learning",
    "abstract": "we apply a subset of datasets, including three environments (halfcheetah, hopper, and walker2d) and ﬁve dataset types (random, medium, medium-replay, expert, and medium-expert), to yield a total of 15 benchmark problems, in which random contains 1M samples from a random policy, medium contains 1M samples from a policy trained to approximately 1/3 of the performance of the expert, expert contains 1M samples from a policy trained to the performance of the expert, medium-replay contains the whole replay buffer of a policy trained up to the performance of the medium agent, and medium-expert contains a 50-50 split of medium and expert dataset (2M samples)",
    "volume": "main",
    "checked": true,
    "id": "1e650cb12aaad371a49b0c8c4514e1b988a5178c",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=kj0_45Y4r9i": {
    "title": "Discriminative Similarity for Data Clustering",
    "abstract": "Similarity-based clustering methods separate data into clusters according to the pairwise similarity between the data, and the pairwise similarity is crucial for their performance. In this paper, we propose Clustering by Discriminative Similarity (CDS), a novel method which learns discriminative similarity for data clustering. CDS learns an unsupervised similarity-based classifier from each data partition, and searches for the optimal partition of the data by minimizing the generalization error of the learnt classifiers associated with the data partitions. By generalization analysis via Rademacher complexity, the generalization error bound for the unsupervised similarity-based classifier is expressed as the sum of discriminative similarity between the data from different classes. It is proved that the derived discriminative similarity can also be induced by the integrated squared error bound for kernel density classification. In order to evaluate the performance of the proposed discriminative similarity, we propose a new clustering method using a kernel as the similarity function, CDS via unsupervised kernel classification (CDSK), with its effectiveness demonstrated by experimental results",
    "volume": "main",
    "checked": true,
    "id": "c91abdc3d7400ef8e28c88cfcd3e016d6890e5f8",
    "citation_count": 0
  },
  "https://openreview.net/forum?id=eBCmOocUejf": {
    "title": "On Robust Prefix-Tuning for Text Classification",
    "abstract": "Recently, prefix-tuning has gained increasing attention as a parameter-efficient finetuning method for large-scale pretrained language models. The method keeps the pretrained models fixed and only updates the prefix token parameters for each downstream task. Despite being lightweight and modular, prefix-tuning still lacks robustness to textual adversarial attacks. However, most currently developed defense techniques necessitate auxiliary model update and storage, which inevitably hamper the modularity and low storage of prefix-tuning. In this work, we propose a robust prefix-tuning framework that preserves the efficiency and modularity of prefix-tuning. The core idea of our framework is leveraging the layerwise activations of the language model by correctly-classified training data as the standard for additional prefix finetuning. During the test phase, an extra batch-level prefix is tuned for each batch and added to the original prefix for robustness enhancement. Extensive experiments on three text classification benchmarks show that our framework substantially improves robustness over several strong baselines against five textual attacks of different types while maintaining comparable accuracy on clean texts. We also interpret our robust prefix-tuning framework from the optimal control perspective and pose several directions for future research 1",
    "volume": "main",
    "checked": true,
    "id": "4bcd4f8ef3f269562dce183ed0329f93b24fd4e6",
    "citation_count": 4
  },
  "https://openreview.net/forum?id=KJztlfGPdwW": {
    "title": "Rethinking Goal-Conditioned Supervised Learning and Its Connection to Offline RL",
    "abstract": "Solving goal-conditioned tasks with sparse rewards using self-supervised learning is promising because of its simplicity and stability over current reinforcement learning (RL) algorithms. A recent work, called Goal-Conditioned Supervised Learning (GCSL), provides a new learning framework by iteratively relabeling and imitating self-generated experiences. In this paper, we revisit the theoretical property of GCSL — optimizing a lower bound of the goal reaching objective, and extend GCSL as a novel offline goal-conditioned RL algorithm. The proposed method is named Weighted GCSL (WGCSL), in which we introduce an advanced compound weight consisting of three parts (1) discounted weight for goal relabeling, (2) goal-conditioned exponential advantage weight, and (3) bestadvantage weight. Theoretically, WGCSL is proved to optimize an equivalent lower bound of the goal-conditioned RL objective and generates monotonically improved policies via an iterated scheme. The monotonic property holds for any behavior policies, and therefore WGCSL can be applied to both online and offline settings. To evaluate algorithms in the offline goal-conditioned RL setting, we provide a benchmark including a range of point and simulated robot domains. Experiments in the introduced benchmark demonstrate that WGCSL can consistently outperform GCSL and existing state-of-the-art offline methods in the fully offline goal-conditioned setting",
    "volume": "main",
    "checked": true,
    "id": "7f712d58084e32ddc1b0cd60932f8bc0a0916330",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=6q_2b6u0BnJ": {
    "title": "TRAIL: Near-Optimal Imitation Learning with Suboptimal Data",
    "abstract": "The aim in imitation learning is to learn effective policies by utilizing near-optimal expert demonstrations. However, high-quality demonstrations from human ex-perts can be expensive to obtain in large number. On the other hand, it is often much easier to obtain large quantities of suboptimal or task-agnostic trajectories, which are not useful for direct imitation, but can nevertheless provide insight into the dynamical structure of the environment, showing what could be done in the environment even if not what should be done. We ask the question, is it possible to utilize such suboptimal ofﬂine datasets to facilitate provably improved downstream imitation learning? In this work, we answer this question afﬁrmatively and present training objectives that use ofﬂine datasets to learn a factored transition model whose structure enables the extraction of a latent action space . Our theoretical analysis shows that the learned latent action space can boost the sample-efﬁciency of downstream imitation learning, effectively reducing the need for large near-optimal expert datasets through the use of aux-iliary non-expert data. To learn the latent action space in practice, we propose TRAIL (Transition-Reparametrized Actions for Imitation Learning), an algorithm that learns an energy-based transition model contrastively, and uses the transition model to reparametrize the action space for sample-efﬁcient imitation learning. We evaluate the practicality of our objective through experiments on a set of navigation and locomotion tasks. Our results verify the beneﬁts suggested by our theory and show that TRAIL is able to improve baseline imitation learning by up to 4x in performance",
    "volume": "main",
    "checked": true,
    "id": "d3c6e0b80c36c14f7d1761fb881f20c35165f507",
    "citation_count": 12
  },
  "https://openreview.net/forum?id=tUa4REjGjTf": {
    "title": "On the Certified Robustness for Ensemble Models and Beyond",
    "abstract": "show deep neural networks (DNN) are vulnerable to adversarial which aim to mislead DNNs by adding perturbations with small magnitude. To defend against such attacks, both empirical and theoretical defense approaches have been extensively studied for a single ML model . In this work, we aim to analyze and provide the certiﬁed robustness for ensemble ML models , together with the sufﬁcient and necessary conditions of robustness for different ensemble protocols. Although ensemble models are shown more robust than a single model empirically; surprisingly, we ﬁnd that in terms of the certiﬁed robustness the standard ensemble models only achieve marginal improvement compared to a single model. Thus, to explore the conditions that guarantee to provide certiﬁably robust ensemble ML models, we ﬁrst prove that diversiﬁed gradient and large conﬁdence margin are sufﬁcient and necessary conditions for certiﬁably robust ensemble models under the model-smoothness assumption. We then provide the bounded model-smoothness analysis based on the proposed Ensemble-before-Smoothing strategy. We also prove that an ensemble model can always achieve higher certiﬁed robustness than a single base model under mild conditions. Inspired by the theoretical ﬁndings, we propose the lightweight Diversity Regularized Training (DRT) to train certiﬁably robust ensemble ML models. Extensive experiments show that our DRT enhanced ensembles can consistently achieve higher certiﬁed robustness than existing single and ensemble ML models, demonstrating the state-of-the-art certiﬁed L 2 -robustness on MNIST, CIFAR-10, base justiﬁcation of the regularization-based training approach DRT. Extensive experiments showed that DRT-enhanced ensembles achieve the highest certiﬁed robustness compared with existing baselines",
    "volume": "main",
    "checked": true,
    "id": "d4aa4fd1d0ea6da1905640adb17c67db435f9f12",
    "citation_count": 10
  },
  "https://openreview.net/forum?id=H4PmOqSZDY": {
    "title": "Towards Empirical Sandwich Bounds on the Rate-Distortion Function",
    "abstract": "Rate-distortion (R-D) function, a key quantity in information theory, characterizes the fundamental limit of how much a data source can be compressed subject to a fidelity criterion, by any compression algorithm. As researchers push for everimproving compression performance, establishing the R-D function of a given data source is not only of scientific interest, but also sheds light on the possible room for improving compression algorithms. Previous work on this problem relied on distributional assumptions on the data source (Gibson, 2017) or only applied to discrete data. By contrast, this paper makes the first attempt at an algorithm for sandwiching the R-D function of a general (not necessarily discrete) source requiring only i.i.d. data samples. We estimate R-D sandwich bounds for a variety of artificial and real-world data sources, in settings far beyond the feasibility of any known method, and shed light on the optimality of neural data compression (Ballé et al., 2021; Yang et al., 2022). Our R-D upper bound on natural images indicates theoretical room for improving state-of-the-art image compression methods by at least one dB in PSNR at various bitrates. Our data and code can be found here",
    "volume": "main",
    "checked": true,
    "id": "8f77b6563918f729aea987bee0547fff87116815",
    "citation_count": 3
  },
  "https://openreview.net/forum?id=YgPqNctmyd": {
    "title": "Towards Building A Group-based Unsupervised Representation Disentanglement Framework",
    "abstract": "Disentangled representation learning is one of the major goals of deep learning, and is a key step for achieving explainable and generalizable models. A well-defined theoretical guarantee still lacks for the VAE-based unsupervised methods, which are a set of popular methods to achieve unsupervised disentanglement. The Group Theory based definition of representation disentanglement mathematically connects the data transformations to the representations using the formalism of group. In this paper, built on the group-based definition and inspired by the n-th dihedral group, we first propose a theoretical framework towards achieving unsupervised representation disentanglement. We then propose a model, based on existing VAEbased methods, to tackle the unsupervised learning problem of the framework. In the theoretical framework, we prove three sufficient conditions on model, group structure, and data respectively in an effort to achieve, in an unsupervised way, disentangled representation per group-based definition. With the first two of the conditions satisfied and a necessary condition derived for the third one, we offer additional constraints, from the perspective of the group-based definition, for the existing VAE-based models. Experimentally, we train 1800 models covering the most prominent VAE-based methods on five datasets to verify the effectiveness of our theoretical framework. Compared to the original VAE-based methods, these Groupified VAEs consistently achieve better mean performance with smaller variances",
    "volume": "main",
    "checked": true,
    "id": "bd71fc4689d265c54506ce6ca352363e926bba0b",
    "citation_count": 6
  },
  "https://openreview.net/forum?id=tUMr0Iox8XW": {
    "title": "Efficient Computation of Deep Nonlinear Infinite-Width Neural Networks that Learn Features",
    "abstract": null,
    "volume": "main",
    "checked": true,
    "id": "a2b2d75389ace794a87a7b944f6820d2463936f5",
    "citation_count": 3
  }
}