{
  "https://jmlr.org/papers/v23/18-131.html": {
    "title": "Joint Estimation and Inference for Data Integration Problems based on Multiple Multi-layered Gaussian Graphical Models",
    "abstract": "The rapid development of high-throughput technologies has enabled the generation of data from biological or disease processes that span multiple layers, like genomic, proteomic or metabolomic data, and further pertain to multiple sources, like disease subtypes or experimental conditions. In this work, we propose a general statistical framework based on Gaussian graphical models for horizontal (i.e. across conditions or subtypes) and vertical (i.e. across different layers containing data on molecular compartments) integration of information in such datasets. We start with decomposing the multi-layer problem into a series of two-layer problems. For each two-layer problem, we model the outcomes at a node in the lower layer as dependent on those of other nodes in that layer, as well as all nodes in the upper layer. We use a combination of neighborhood selection and group-penalized regression to obtain sparse estimates of all model parameters. Following this, we develop a debiasing technique and asymptotic distributions of inter-layer directed edge weights that utilize already computed neighborhood selection coefficients for nodes in the upper layer. Subsequently, we establish global and simultaneous testing procedures for these edge weights. Performance of the proposed methodology is evaluated on synthetic and real data",
    "volume": "main",
    "checked": true,
    "id": "a13f15b97d1d35812c4347696fe764eb8631dd70",
    "citation_count": 2
  },
  "https://jmlr.org/papers/v23/18-467.html": {
    "title": "Debiased Distributed Learning for Sparse Partial Linear Models in High Dimensions",
    "abstract": "Although various distributed machine learning schemes have been proposed recently for purely linear models and fully nonparametric models, little attention has been paid to distributed optimization for semi-parametric models with multiple structures (e.g. sparsity, linearity and nonlinearity).  To address these issues, the current paper proposes a new communication-efficient distributed learning algorithm for sparse partially linear models with an increasing number of features. The proposed method is based on the classical divide and conquer strategy for handling big data and the computation on each subsample consists of  a debiased estimation of the doubly regularized least squares approach. With the proposed method, we theoretically prove that our global parametric estimator can achieve the optimal parametric rate in our semi-parametric model given an appropriate partition on the total data. Specifically, the choice of data partition  relies on the underlying smoothness of the nonparametric component, and it is adaptive to the sparsity parameter. Finally, some simulated experiments are carried out to illustrate the empirical performances of our debiased technique under the distributed setting",
    "volume": "main",
    "checked": true,
    "id": "4a805261114aecb1307c4b5285b67de19fbec2b3",
    "citation_count": 4
  },
  "https://jmlr.org/papers/v23/19-1056.html": {
    "title": "Recovering shared structure from multiple networks with unknown edge distributions",
    "abstract": "In increasingly many settings, data sets consist of multiple samples from a population of networks, with vertices aligned across networks; for example, brain connectivity networks in neuroscience. We consider the setting where the observed networks have a shared expectation, but may differ in the noise structure on their edges. Our approach exploits the shared mean structure to denoise edge-level measurements of the observed networks and estimate the underlying population-level parameters. We also explore the extent to which edge-level errors influence estimation and downstream inference. In the process, we establish a finite-sample concentration inequality for the low-rank eigenvalue truncation of a random weighted adjacency matrix, which may be of independent interest. The proposed approach is illustrated on synthetic networks and on data from an fMRI study of schizophrenia",
    "volume": "main",
    "checked": true,
    "id": "b968f851a2f3241eaef77e8eb342eab5f23d73d9",
    "citation_count": 4
  },
  "https://jmlr.org/papers/v23/19-267.html": {
    "title": "Exploiting locality in high-dimensional Factorial hidden Markov models",
    "abstract": "We propose algorithms for approximate filtering and smoothing in high-dimensional Factorial hidden Markov models. The approximation involves discarding, in a principled way, likelihood factors according to a notion of locality in a factor graph associated with the emission distribution. This allows the exponential-in-dimension cost of exact filtering and smoothing to be avoided. We prove that the approximation accuracy, measured in a local total variation norm, is \"dimension-free\" in the sense that as the overall dimension of the model increases the error bounds we derive do not necessarily degrade. A key step in the analysis is to quantify the error introduced by localizing the likelihood function in a Bayes' rule update. The factorial structure of the likelihood function which we exploit arises naturally when data have known spatial or network structure. We demonstrate the new algorithms on synthetic examples and a London Underground passenger flow problem, where the factor graph is effectively given by the train network",
    "volume": "main",
    "checked": true,
    "id": "1f7ef39a026f9dcc983b835f63fc343db3454585",
    "citation_count": 2
  },
  "https://jmlr.org/papers/v23/19-450.html": {
    "title": "Empirical Risk Minimization under Random Censorship",
    "abstract": "We consider the classic supervised learning problem where a continuous non-negative random label $Y$ (e.g. a random duration) is to be predicted based upon observing a random vector $X$ valued in $\\mathbb{R}^d$ with $d\\geq 1$ by means of a regression rule with minimum least square error. In various applications, ranging from industrial quality control to public health through credit risk analysis for instance, training observations can be right censored, meaning that, rather than on independent copies of $(X,Y)$, statistical learning relies on a collection of $n\\geq 1$ independent realizations of the triplet $(X, \\; \\min\\{Y,\\; C\\},\\; \\delta)$, where $C$ is a nonnegative random variable with unknown distribution, modelling censoring and $\\delta=\\mathbb{I}\\{Y\\leq C\\}$ indicates whether the duration is right censored or not. As ignoring censoring in the risk computation may clearly lead to a severe underestimation of the target duration and jeopardize prediction, we consider a plug-in estimate of the true risk based on a Kaplan-Meier estimator of the conditional survival function of the censoring $C$ given $X$, referred to as Beran risk, in order to perform empirical risk minimization. It is established, under mild conditions, that the learning rate of minimizers of this biased/weighted empirical risk functional is of order $O_{\\mathbb{P}}(\\sqrt{\\log(n)/n})$ when ignoring model bias issues inherent to plug-in estimation, as can be attained in absence of censoring. Beyond theoretical results, numerical experiments are presented in order to illustrate the relevance of the approach developed",
    "volume": "main",
    "checked": true,
    "id": "faf7a50b8d717565730367c273aa3abf1bf79d4b",
    "citation_count": 2
  },
  "https://jmlr.org/papers/v23/19-497.html": {
    "title": "XAI Beyond Classification: Interpretable Neural Clustering",
    "abstract": "In this paper, we study two challenging problems in explainable AI (XAI) and data clustering. The first is how to directly design a neural network with inherent interpretability, rather than giving post-hoc explanations of a black-box model. The second is implementing discrete $k$-means with a differentiable neural network that embraces the advantages of parallel computing, online clustering, and clustering-favorable representation learning. To address these two challenges, we design a novel neural network, which is a differentiable reformulation of the vanilla $k$-means, called inTerpretable nEuraL cLustering (TELL). Our contributions are threefold. First, to the best of our knowledge, most existing XAI works focus on supervised learning paradigms. This work is one of the few XAI studies on unsupervised learning, in particular, data clustering. Second, TELL is an interpretable, or the so-called intrinsically explainable and transparent model. In contrast, most existing XAI studies resort to various means for understanding a black-box model with post-hoc explanations. Third, from the view of data clustering, TELL possesses many properties highly desired by $k$-means, including but not limited to online clustering, plug-and-play module, parallel computing, and provable convergence. Extensive experiments show that our method achieves superior performance comparing with 14 clustering approaches on three challenging data sets. The source code could be accessed at www.pengxi.me",
    "volume": "main",
    "checked": true,
    "id": "b5c09079d9432c0318475d61f27efb6323c642b7",
    "citation_count": 27
  },
  "https://jmlr.org/papers/v23/19-882.html": {
    "title": "Bayesian Multinomial Logistic Normal Models through Marginally Latent Matrix-T Processes",
    "abstract": "Bayesian multinomial logistic-normal (MLN) models are popular for the analysis of sequence count data (e.g., microbiome or gene expression data) due to their ability to model multivariate count data with complex covariance structure. However, existing implementations of MLN models are limited to small datasets due to the non-conjugacy of the multinomial and logistic-normal distributions. Motivated by the need to develop efficient inference for Bayesian MLN models, we develop two key ideas. First, we develop the class of Marginally Latent Matrix-T Process (Marginally LTP) models. We demonstrate that many popular MLN models, including those with latent linear, non-linear, and dynamic linear structure are special cases of this class. Second, we develop an efficient inference scheme for Marginally LTP models with specific accelerations for the MLN subclass. Through application to MLN models, we demonstrate that our inference scheme are both highly accurate and often 4-5 orders of magnitude faster than MCMC",
    "volume": "main",
    "checked": true,
    "id": "b186404c24dfc39ca1fd1aa6afb2f2abcd827bbc",
    "citation_count": 25
  },
  "https://jmlr.org/papers/v23/20-040.html": {
    "title": "Deep Learning in Target Space",
    "abstract": "Deep learning uses neural networks which are parameterised by their weights.  The neural networks are usually trained by tuning the weights to directly minimise a given loss function.  In this paper we propose to re-parameterise the weights into targets for the firing strengths of the individual nodes in the network. Given a set of targets, it is possible to calculate the weights which make the firing strengths best meet those targets. It is argued that using targets for training addresses the problem of exploding gradients, by a process which we call cascade untangling, and  makes the loss-function surface smoother to traverse, and so leads to easier, faster training, and also potentially better generalisation, of the neural network.  It also allows for easier learning of deeper and recurrent network structures. The necessary conversion of targets to weights comes at an extra computational expense, which is in many cases manageable.  Learning in target space can be combined with existing neural-network optimisers, for extra gain.  Experimental results show the speed of using target space, and examples of improved generalisation, for fully-connected networks and convolutional networks, and the ability to recall and process long time sequences and perform natural-language processing with recurrent networks",
    "volume": "main",
    "checked": true,
    "id": "bdbb7b4bb6bf9388770994a87065a213a2ea5669",
    "citation_count": 3
  },
  "https://jmlr.org/papers/v23/20-1111.html": {
    "title": "Scaling Laws from the Data Manifold Dimension",
    "abstract": "When data is plentiful, the test loss achieved by well-trained neural networks scales as a power-law $L \\propto N^{-\\alpha}$ in the number of network parameters $N$. This empirical scaling law holds for a wide variety of data modalities, and may persist over many orders of magnitude. The scaling law can be explained if neural models are effectively just performing regression on a data manifold of intrinsic dimension $d$.  This simple theory predicts that the scaling exponents $\\alpha \\approx 4/d$ for cross-entropy and mean-squared error losses.  We confirm the theory by independently measuring the intrinsic dimension and the scaling exponents in a teacher/student framework, where we can study a  variety of $d$ and $\\alpha$ by dialing the properties of  random teacher networks.  We also test the theory with CNN image classifiers on several datasets and with GPT-type language models",
    "volume": "main",
    "checked": true,
    "id": "b17c2f0c2dc3f98e85322889dc144e79d183fcff",
    "citation_count": 11
  },
  "https://jmlr.org/papers/v23/20-112.html": {
    "title": "Interpolating Predictors in High-Dimensional Factor Regression",
    "abstract": "This work studies  finite-sample properties of the risk of the minimum-norm interpolating predictor in high-dimensional regression models.   If the effective rank of the covariance matrix $\\Sigma$ of the $p$ regression features is much larger than the sample size $n$,  we show that the min-norm interpolating  predictor is not desirable, as its risk approaches the risk of trivially predicting the response by 0. However, our detailed finite-sample analysis reveals, surprisingly, that  this behavior is not present when  the regression response and the features are jointly low-dimensional, following a widely used  factor regression model. Within this popular model class, and when the effective rank of $\\Sigma$ is smaller than $n$, while still allowing for $p \\gg n$, both the bias and the variance terms of the excess risk can be controlled, and the risk of the minimum-norm interpolating predictor approaches optimal benchmarks. Moreover, through a  detailed analysis of the bias term, we exhibit model classes under   which our upper bound on the excess risk approaches zero, while the corresponding upper bound  in the recent work arXiv:1906.11300 diverges. Furthermore,  we show that the minimum-norm interpolating predictor analyzed under the factor regression model, despite being model-agnostic and devoid of tuning parameters, can have similar risk to predictors based on principal components regression and ridge regression, and  can improve over LASSO based predictors, in the high-dimensional regime",
    "volume": "main",
    "checked": true,
    "id": "bfdc4dfacaf543092cf97896c9260c3eee33f0b4",
    "citation_count": 8
  },
  "https://jmlr.org/papers/v23/20-1152.html": {
    "title": "Near Optimality of Finite Memory Feedback Policies in Partially Observed Markov Decision Processes",
    "abstract": "In the theory of Partially Observed Markov Decision Processes (POMDPs), existence of optimal policies have in general been established via converting the original partially observed stochastic control problem to a fully observed one on the belief space, leading to a belief-MDP. However, computing an optimal policy for this fully observed model, and so for the original POMDP, using classical dynamic or linear programming methods is challenging even if the original system has finite state and action spaces, since the state space of the fully observed belief-MDP model is always uncountable. Furthermore, there exist very few rigorous value function approximation and optimal policy approximation results, as regularity conditions needed often require a tedious study involving the spaces of probability measures leading to properties such as Feller continuity. In this paper, we study a planning problem for POMDPs where the system dynamics and measurement channel model are assumed to be known. We construct an approximate belief model by discretizing the belief space using only finite window information variables. We then find optimal policies for the approximate model and we rigorously establish near optimality of the constructed finite window control policies in POMDPs under mild non-linear filter stability conditions and the assumption that the measurement and action sets are finite (and the state space is real vector valued). We also establish a rate of convergence result which relates the finite window memory size and the approximation error bound, where the rate of convergence is exponential under explicit and testable exponential filter stability conditions. While there exist many experimental results and few rigorous asymptotic convergence results, an explicit rate of convergence result is new in the literature, to our knowledge",
    "volume": "main",
    "checked": true,
    "id": "337566d4f14e00e32cfba465ae6a50a1e07404ca",
    "citation_count": 17
  },
  "https://jmlr.org/papers/v23/20-1165.html": {
    "title": "Approximate Information State for Approximate Planning and Reinforcement Learning in Partially Observed Systems",
    "abstract": "We propose a theoretical framework for approximate planning and learning in partially observed systems. Our framework is based on the fundamental notion of information state. We provide two definitions of information state---i) a function of history which is sufficient to compute the expected reward and predict its next value; ii) a function of the history which can be recursively updated and is sufficient to compute the expected reward and predict the next observation. An information state always leads to a dynamic programming decomposition. Our key result is to show that if a function of the history (called AIS) approximately satisfies the properties of the information state, then there is a corresponding approximate dynamic program. We show that the policy computed using this is approximately optimal with bounded loss of optimality. We show that several approximations in state, observation and action spaces in literature can be viewed as instances of AIS. In some of these cases, we obtain tighter bounds. A salient feature of AIS is that it can be learnt from data. We present AIS based  multi-time scale policy gradient algorithms and detailed numerical experiments with low, moderate and high dimensional environments",
    "volume": "main",
    "checked": true,
    "id": "abde7540643e5093cba41a2e4554116bb9241980",
    "citation_count": 35
  },
  "https://jmlr.org/papers/v23/20-1188.html": {
    "title": "Solving Large-Scale Sparse PCA to Certifiable (Near) Optimality",
    "abstract": "Sparse principal component analysis (PCA) is a popular dimensionality reduction technique for obtaining principal components which are linear combinations of a small subset of the original features. Existing approaches  cannot supply certifiably optimal principal components with more than $p=100s$ of variables. By reformulating sparse PCA as a convex mixed-integer semidefinite optimization problem, we design a cutting-plane method which solves the problem to certifiable optimality at the scale of selecting $k=5$ covariates from $p=300$ variables, and provides small bound gaps at a larger scale. We also propose a convex relaxation and greedy rounding scheme that provides bound gaps of $1-2\\%$ in practice within minutes for $p=100$s or hours for $p=1,000$s and is therefore a viable alternative to the exact method at scale. Using real-world financial and medical data sets, we illustrate our approach's ability to derive interpretable principal components tractably at scale",
    "volume": "main",
    "checked": true,
    "id": "41ff82e7d0f7dbe1db108ef65cfcd5574cfc0884",
    "citation_count": 18
  },
  "https://jmlr.org/papers/v23/20-1219.html": {
    "title": "On Generalizations of Some Distance Based Classifiers for HDLSS Data",
    "abstract": "In high dimension, low sample size (HDLSS) settings, classifiers based on Euclidean distances like the nearest neighbor classifier and the average distance classifier perform quite poorly if differences between locations of the underlying populations get masked by scale differences. To rectify this problem, several modifications of these classifiers have been proposed in the literature. However, existing methods are confined to location and scale differences only, and they often fail to discriminate among populations differing outside of the first two moments. In this article, we propose some simple transformations of these classifiers resulting in improved performance even when the underlying populations have the same location and scale. We further propose a generalization of these classifiers based on the idea of grouping of variables. High-dimensional behavior of the proposed classifiers is studied theoretically. Numerical experiments with a variety of simulated examples as well as an extensive analysis of benchmark data sets from three different databases exhibit advantages of the proposed methods",
    "volume": "main",
    "checked": true,
    "id": "20d0213faf58dc8a4283b1f3174e834fe87330cb",
    "citation_count": 3
  },
  "https://jmlr.org/papers/v23/20-1248.html": {
    "title": "A Stochastic Bundle Method for Interpolation",
    "abstract": "We propose a novel method for training deep neural networks that are capable of interpolation, that is, driving the empirical loss to zero. At each iteration, our method constructs a stochastic approximation of the learning objective. The approximation, known as a bundle, is a pointwise maximum of linear functions. Our bundle contains a constant function that lower bounds the empirical loss. This enables us to compute an automatic adaptive learning rate, thereby providing an accurate solution. In addition, our bundle includes linear approximations computed at the current iterate and other linear estimates of the DNN parameters. The use of these additional approximations makes our method significantly more robust to its hyperparameters. Based on its desirable empirical properties, we term our method Bundle Optimisation for Robust and Accurate Training (BORAT). In order to operationalise BORAT, we design a novel algorithm for optimising the bundle approximation efficiently at each iteration. We establish the theoretical convergence of BORAT in both convex and non-convex settings. Using standard publicly available data sets, we provide a thorough comparison of BORAT to other single hyperparameter optimisation algorithms. Our experiments demonstrate BORAT matches the state-of-the-art generalisation performance for these methods and is the most robust",
    "volume": "main",
    "checked": true,
    "id": "89c47c33bc00438ea1c80c717ed00e04d2ca39e5",
    "citation_count": 2
  },
  "https://jmlr.org/papers/v23/20-1297.html": {
    "title": "TFPnP: Tuning-free Plug-and-Play Proximal Algorithms with Applications to Inverse Imaging Problems",
    "abstract": "Plug-and-Play (PnP) is a non-convex optimization framework that combines proximal algorithms, for example, the alternating direction method of multipliers (ADMM), with advanced denoising priors. Over the past few years, great empirical success has been obtained by PnP algorithms, especially for the ones that integrate deep learning-based denoisers. However, a key problem of PnP approaches is the need for manual parameter tweaking which is essential to obtain high-quality results across the high discrepancy in imaging conditions and varying scene content. In this work, we present a class of tuning-free PnP proximal algorithms that can determine parameters such as denoising strength, termination time, and other optimization-specific parameters automatically. A core part of our approach is a policy network for automated parameter search which can be effectively learned via a mixture of model-free and model-based deep reinforcement learning strategies. We demonstrate, through rigorous numerical and visual experiments, that the learned policy can customize parameters to different settings, and is often more efficient and effective than existing handcrafted criteria. Moreover, we discuss several practical considerations of  PnP denoisers, which together with our learned policy yield state-of-the-art results. This advanced performance is prevalent on both linear and nonlinear exemplar inverse imaging problems, and in particular shows promising results on compressed sensing MRI, sparse-view CT, single-photon imaging, and phase retrieval",
    "volume": "main",
    "checked": false,
    "id": "e19fd9d3262afee23c52ffb4959993b476f0934e",
    "citation_count": 9
  },
  "https://jmlr.org/papers/v23/20-1361.html": {
    "title": "Spatial Multivariate Trees for Big Data Bayesian Regression",
    "abstract": "High resolution geospatial data are challenging because standard geostatistical models based on Gaussian processes are known to not scale to large data sizes. While progress has been made towards methods that can be computed more efficiently, considerably less attention has been devoted to methods for large scale data that allow the description of complex relationships between several outcomes recorded at high resolutions by different sensors. Our Bayesian multivariate regression models based on spatial multivariate trees (SpamTrees) achieve scalability via conditional independence assumptions on latent random effects following a treed directed acyclic graph. Information-theoretic arguments and considerations on computational efficiency guide the construction of the tree and the related efficient sampling algorithms in imbalanced multivariate settings. In addition to simulated data examples, we illustrate SpamTrees using a large climate data set which combines satellite data with land-based station data. Software and source code are available on CRAN at https://CRAN.R-project.org/package=spamtree",
    "volume": "main",
    "checked": true,
    "id": "cc5c183b32604c881af121f606b34223202825a5",
    "citation_count": 7
  },
  "https://jmlr.org/papers/v23/20-1402.html": {
    "title": "Decimated Framelet System on Graphs and Fast G-Framelet Transforms",
    "abstract": "Graph representation learning has many real-world applications, from self-driving LiDAR, 3D computer vision to drug repurposing, protein classification, social networks analysis. An adequate representation of graph data is vital to the learning performance of a statistical or machine learning model for graph-structured data. This paper proposes a novel multiscale representation system for graph data, called decimated framelets, which form a localized tight frame on the graph. The decimated framelet system allows storage of the graph data representation on a coarse-grained chain and processes the graph data at multi scales where at each scale, the data is stored on a subgraph. Based on this, we establish decimated G-framelet transforms for the decomposition and reconstruction of the graph data at multi resolutions via a constructive data-driven filter bank. The graph framelets are built on a chain-based orthonormal basis that supports fast graph Fourier transforms. From this, we give a fast algorithm for the decimated G-framelet transforms, or FGT, that has linear computational complexity O(N) for a graph of size N. The effectiveness for constructing the decimated framelet system and the FGT is demonstrated by a simulated example of random graphs and real-world applications, including multiresolution analysis for traffic network and representation learning of graph neural networks for graph classification tasks",
    "volume": "main",
    "checked": true,
    "id": "0fe2771552444f63435e0433e2c6fe6b851d20aa",
    "citation_count": 19
  },
  "https://jmlr.org/papers/v23/20-1433.html": {
    "title": "Universal Approximation in Dropout Neural Networks",
    "abstract": "We prove two universal approximation theorems for a range of dropout neural networks. These are feed-forward neural networks in which each edge is given a random $\\{0,1\\}$-valued filter, that have two modes of operation: in the first each edge output is multiplied by its random filter, resulting in a random output, while in the second each edge output is multiplied by the expectation of its filter, leading to a deterministic output. It is common to use the random mode during training and the deterministic mode during testing and prediction. Both theorems are of the following form: Given a function to approximate and a threshold $\\varepsilon>0$, there exists a dropout network that is $\\varepsilon$-close in probability and in $L^q$. The first theorem applies to dropout networks in the random mode. It assumes little on the activation function, applies to a wide class of networks, and can even be applied to approximation schemes other than neural networks. The core is an algebraic property that shows that deterministic networks can be exactly matched in expectation by random networks. The second theorem makes stronger assumptions and gives a stronger result. Given a function to approximate, it provides existence of a network that approximates in both modes simultaneously. Proof components are a recursive replacement of edges by independent copies, and a special first-layer replacement that couples the resulting larger network to the input. The functions to be approximated are assumed to be elements of general normed spaces, and the approximations are measured in the corresponding norms. The networks are constructed explicitly. Because of the different methods of proof, the two results give independent insight into the approximation properties of random dropout networks. With this, we establish that dropout neural networks broadly satisfy a universal-approximation property",
    "volume": "main",
    "checked": true,
    "id": "1c9587fc64e7b0223fcc609e6046a3f614168c92",
    "citation_count": 2
  },
  "https://jmlr.org/papers/v23/20-188.html": {
    "title": "Supervised Dimensionality Reduction and Visualization using Centroid-Encoder",
    "abstract": "We propose a new tool for visualizing complex, and potentially large and high-dimensional, data sets called Centroid-Encoder (CE).  The architecture of the Centroid-Encoder is similar to the autoencoder neural network but it has a modified target, i.e., the class centroid in the ambient space.  As such, CE incorporates label information and performs a supervised data visualization.  The training of CE is done in the usual way with a training set whose parameters are tuned using a validation set.  The evaluation of the resulting CE visualization is performed on a sequestered test set where the generalization of the model is assessed both visually and quantitatively. We present a detailed comparative analysis of the method using a wide variety of data sets and techniques, both supervised and unsupervised, including NCA, non-linear NCA, t-distributed NCA, t-distributed MCML, supervised UMAP, supervised PCA, Colored Maximum Variance Unfolding, supervised Isomap, Parametric Embedding, supervised Neighbor Retrieval Visualizer, and Multiple Relational Embedding. An analysis of variance using PCA demonstrates that a non-linear preprocessing by the CE transformation of the data captures more variance than PCA by dimension",
    "volume": "main",
    "checked": true,
    "id": "3e60377fc7e0f4306758bf718ab975d543e02e95",
    "citation_count": 11
  },
  "https://jmlr.org/papers/v23/20-233.html": {
    "title": "Evolutionary Variational Optimization of Generative Models",
    "abstract": "We combine two popular optimization approaches to derive learning algorithms for generative models: variational optimization and evolutionary algorithms. The combination is realized for generative models with discrete latents by using truncated posteriors as the family of variational distributions. The variational parameters of truncated posteriors are sets of latent states. By interpreting these states as genomes of individuals and by using the variational lower bound to define a fitness, we can apply evolutionary algorithms to realize the variational loop. The used variational distributions are very flexible and we show that evolutionary algorithms can effectively and efficiently optimize the variational bound. Furthermore, the variational loop is generally applicable (âblack boxâ) with no analytical derivations required. To show general applicability, we apply the approach to three generative models (we use Noisy-OR Bayes Nets, Binary Sparse Coding, and Spike-and-Slab Sparse Coding). To demonstrate effectiveness and efficiency of the novel variational approach, we use the standard competitive benchmarks of image denoising and inpainting. The benchmarks allow quantitative comparisons to a wide range of methods including probabilistic approaches, deep deterministic and generative networks, and non-local image processing methods. In the category of âzero-shotâ learning (when only the corrupted image is used for training), we observed the evolutionary variational algorithm to significantly improve the state-of-the-art in many benchmark settings. For one well-known inpainting benchmark, we also observed state-of-the-art performance across all categories of algorithms although we only train on the corrupted image. In general, our investigations highlight the importance of research on optimization methods for generative models to achieve performance improvements",
    "volume": "main",
    "checked": true,
    "id": "3ae8c0b134c6dd9c8b7922faa0821b9b94b55846",
    "citation_count": 3
  },
  "https://jmlr.org/papers/v23/20-247.html": {
    "title": "LSAR: Efficient Leverage Score Sampling Algorithm for the Analysis of Big Time Series Data",
    "abstract": "We apply methods from randomized numerical linear algebra (RandNLA) to develop improved algorithms for the analysis of large-scale time series data. We first develop a new fast algorithm to estimate the leverage scores of an autoregressive (AR) model in big data regimes. We show that the accuracy of approximations lies within $(1+\\mathcal{O}({\\varepsilon}))$ of the true leverage scores with high probability. These theoretical results are subsequently exploited to develop an efficient algorithm, called LSAR, for fitting an appropriate AR model to big time series data. Our proposed algorithm is guaranteed, with high probability, to find the maximum likelihood estimates of the parameters of the underlying true AR model and has a worst case running time that significantly improves those of the state-of-the-art alternatives in big data regimes. Empirical results on large-scale synthetic as well as real data highly support the theoretical results and reveal the efficacy of this new approach",
    "volume": "main",
    "checked": true,
    "id": "a63be609648e0ab0b03f7f848ad711e3e821b999",
    "citation_count": 10
  },
  "https://jmlr.org/papers/v23/20-315.html": {
    "title": "Fast and Robust Rank Aggregation against Model Misspecification",
    "abstract": "In rank aggregation (RA), a collection of preferences from different users are summarized into a total order under the assumption of homogeneity of users. Model misspecification in RA arises since the homogeneity assumption fails to be satisfied in the complex real-world situation. Existing robust RAs usually resort to an augmentation of the ranking model to account for additional noises, where the collected preferences can be treated as a noisy perturbation of idealized preferences. Since the majority of robust RAs rely on certain perturbation assumptions,  they cannot generalize well to agnostic noise-corrupted preferences in the real world. In this paper, we propose CoarsenRank, which possesses robustness against model misspecification. Specifically, the properties of our CoarsenRank are summarized as follows: (1) CoarsenRank is designed for mild model misspecification, which assumes there exist the ideal preferences (consistent with model assumption) that locate in a neighborhood of the actual preferences. (2) CoarsenRank then performs regular RAs over a neighborhood of the preferences instead of the original data set directly. Therefore, CoarsenRank enjoys robustness against model misspecification within a neighborhood. (3) The neighborhood of the data set is defined via their empirical data distributions. Further, we put an exponential prior on the unknown size of the neighborhood and derive a much-simplified posterior formula for CoarsenRank under particular divergence measures. (4) CoarsenRank is further instantiated to Coarsened Thurstone, Coarsened Bradly-Terry, and Coarsened Plackett-Luce with three popular probability ranking models. Meanwhile, tractable optimization strategies are introduced with regards to each instantiation respectively. In the end, we apply CoarsenRank on four real-world data sets. Experiments show that CoarsenRank is fast and robust, achieving consistent improvements over baseline methods",
    "volume": "main",
    "checked": true,
    "id": "9022eed64cc58615ba698ade6ac587c6a8ae3989",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v23/20-316.html": {
    "title": "On Biased Stochastic Gradient Estimation",
    "abstract": "We present a uniform analysis of biased stochastic gradient methods for minimizing convex, strongly convex, and non-convex composite objectives, and identify settings where bias is useful in stochastic gradient estimation. The framework we present allows us to extend proximal support to biased algorithms, including SAG and SARAH, for the first time in the convex setting. We also use our framework to develop a new algorithm, Stochastic Average Recursive GradiEnt (SARGE), that achieves the oracle complexity lower-bound for non-convex, finite-sum objectives and requires strictly fewer calls to a stochastic gradient oracle per iteration than SVRG and SARAH. We support our theoretical results with numerical experiments that demonstrate the benefits of certain biased gradient estimators",
    "volume": "main",
    "checked": true,
    "id": "21c12f53af2eeac9a804277dafad3f061df016ba",
    "citation_count": 3
  },
  "https://jmlr.org/papers/v23/20-357.html": {
    "title": "Efficient MCMC Sampling with Dimension-Free Convergence Rate using ADMM-type Splitting",
    "abstract": "Performing exact Bayesian inference for complex models is computationally intractable. Markov chain Monte Carlo (MCMC) algorithms can provide reliable approximations of the posterior distribution but are expensive for large data sets and high-dimensional models. A standard approach to mitigate this complexity consists in using subsampling techniques or distributing the data across a cluster. However, these approaches are typically unreliable in high-dimensional scenarios. We focus here on a recent alternative class of MCMC schemes exploiting a splitting strategy akin to the one used by the celebrated alternating direction method of multipliers (ADMM) optimization algorithm. These methods appear to provide empirically state-of-the-art performance but their theoretical behavior in high dimension is currently unknown. In this paper, we propose a detailed theoretical study of one of these algorithms known as the split Gibbs sampler. Under regularity conditions, we establish explicit convergence rates for this scheme using Ricci curvature and coupling ideas. We support our theory with numerical illustrations",
    "volume": "main",
    "checked": true,
    "id": "10ada57dc786925a5f7104ebf214fa00c0b56a0e",
    "citation_count": 23
  },
  "https://jmlr.org/papers/v23/20-520.html": {
    "title": "MurTree: Optimal Decision Trees via Dynamic Programming and Search",
    "abstract": "Decision tree learning is a widely used approach in machine learning, favoured in applications that require concise and interpretable models. Heuristic methods are traditionally used to quickly produce models with reasonably high accuracy. A commonly criticised point, however, is that the resulting trees may not necessarily be the best representation of the data in terms of accuracy and size. In recent years, this motivated the development of optimal classification tree algorithms that globally optimise the decision tree in contrast to heuristic methods that perform a sequence of locally optimal decisions. We follow this line of work and provide a novel algorithm for learning optimal classification trees based on dynamic programming and search. Our algorithm supports constraints on the depth of the tree and number of nodes. The success of our approach is attributed to a series of specialised techniques that exploit properties unique to classification trees. Whereas algorithms for optimal classification trees have traditionally been plagued by high runtimes and limited scalability, we show in a detailed experimental study that our approach uses only a fraction of the time required by the state-of-the-art and can handle datasets with tens of thousands of instances, providing several orders of magnitude improvements and notably contributing towards the practical use of optimal decision trees",
    "volume": "main",
    "checked": true,
    "id": "bc20cd93a1eaeb92f0221e4cfa3b0387fd7d1141",
    "citation_count": 16
  },
  "https://jmlr.org/papers/v23/20-644.html": {
    "title": "Data-Derived Weak Universal Consistency",
    "abstract": "Many current applications in data science need rich model classes to adequately represent the statistics that may be driving the observations. Such rich model classes may be too complex to admit uniformly consistent estimators. In such cases, it is conventional to settle for estimators with guarantees on convergence rate where the performance can be bounded in a model-dependent way, i.e. pointwise consistent estimators. But this viewpoint has the practical drawback that estimator performance is a function of the unknown model within the model class that is being estimated. Even if an estimator is consistent, how well it is doing at any given time may not be clear, no matter what the sample size of the observations. In these cases, a line of analysis favors sample dependent guarantees. We explore this framework by studying rich model classes that may only admit pointwise consistency guarantees, yet enough information about the unknown model driving the observations needed to gauge estimator accuracy can be inferred from the sample at hand. In this paper we obtain a novel characterization of lossless compression problems over a countable alphabet in the data-derived framework in terms of what we term deceptive distributions. We also show that the ability to estimate the redundancy of compressing memoryless sources is equivalent to learning the underlying single-letter marginal in a data-derived fashion. We expect that the methodology underlying such characterizations in a data-derived estimation framework will be broadly applicable to a wide range of estimation problems, enabling a more systematic approach to data-derived guarantees",
    "volume": "main",
    "checked": true,
    "id": "32e12d5a8a6cb7ac94d4cedf4a6e2fd3bc5156cb",
    "citation_count": 0
  },
  "https://jmlr.org/papers/v23/20-707.html": {
    "title": "Novel Min-Max Reformulations of Linear Inverse Problems",
    "abstract": "In this article, we dwell into the class of so-called ill-posed Linear Inverse Problems (LIP) which simply refer to the task of recovering the entire signal from its relatively few random linear measurements. Such problems arise in a variety of settings with applications ranging from medical image processing, recommender systems, etc. We propose a slightly generalized version of the error constrained linear inverse problem and obtain a novel and equivalent convex-concave min-max reformulation by providing an exposition to its convex geometry. Saddle points of the min-max problem are completely characterized in terms of a solution to the LIP, and vice versa. Applying simple saddle point seeking ascend-descent type algorithms to solve the min-max problems provides novel and simple algorithms to find a solution to the LIP. Moreover, the reformulation of an LIP as the min-max problem provided in this article is crucial in developing methods to solve the dictionary learning problem with almost sure recovery constraints",
    "volume": "main",
    "checked": true,
    "id": "a02a3128806357d08473daddea178f72ffa6092d",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v23/20-720.html": {
    "title": "Theoretical Convergence of Multi-Step Model-Agnostic Meta-Learning",
    "abstract": "As a popular meta-learning approach, the model-agnostic meta-learning (MAML) algorithm has been widely used due to its  simplicity and effectiveness. However, the convergence of the general multi-step MAML still remains unexplored. In this paper, we develop a new theoretical framework to provide such convergence guarantee for two types of objective functions that are of interest in practice: (a) resampling case (e.g., reinforcement learning), where loss functions take the form in expectation and new data are sampled as the algorithm runs; and (b) finite-sum case (e.g., supervised learning), where loss functions take the finite-sum form with given samples. For both cases, we characterize the convergence rate and the computational complexity to attain an $\\epsilon$-accurate solution for multi-step MAML in the general nonconvex setting. In particular, our results suggest that an inner-stage stepsize needs to be chosen inversely proportional to the number $N$ of inner-stage steps in order for $N$-step MAML to have guaranteed convergence. From the technical perspective, we develop novel techniques to deal with the nested structure of the meta gradient for multi-step MAML, which can be of independent interest",
    "volume": "main",
    "checked": true,
    "id": "508b65cb9a4ddc077dc421dd2fb75f99f03919ac",
    "citation_count": 31
  },
  "https://jmlr.org/papers/v23/20-735.html": {
    "title": "A Class of Conjugate Priors for Multinomial Probit Models which Includes the Multivariate Normal One",
    "abstract": "Multinomial probit models are routinely-implemented representations for learning how the class probabilities of categorical response data change with $p$ observed predictors. Although several frequentist methods have been developed for estimation, inference and classification within such a class of models, Bayesian inference is still lagging behind. This is due to the apparent absence of a tractable class of conjugate priors, that may facilitate posterior inference on the multinomial probit coefficients. Such an issue has motivated increasing efforts toward the development of effective Markov chain Monte Carlo methods, but state-of-the-art solutions still face severe computational bottlenecks, especially in high dimensions. In this article, we show that the entire class of unified skew-normal (SUN) distributions is conjugate to several multinomial probit models. Leveraging this result and the SUN properties, we improve upon state-of-the-art solutions for posterior inference and classification both in terms of closed-form results for several functionals of interest, and also by developing novel computational methods relying either on independent and identically distributed samples from the exact posterior or on scalable and accurate variational approximations based on blocked partially-factorized representations. As illustrated in simulations and in a gastrointestinal lesions application, the magnitude of the improvements relative to current methods is particularly evident, in practice, when the focus is on high-dimensional studies",
    "volume": "main",
    "checked": true,
    "id": "f7137a56d699106d75b920e5f1870f737acf035c",
    "citation_count": 18
  },
  "https://jmlr.org/papers/v23/20-782.html": {
    "title": "An improper estimator with optimal excess risk in misspecified density estimation and logistic regression",
    "abstract": "We introduce a procedure for conditional density estimation under logarithmic loss, which we call SMP (Sample Minmax Predictor). This estimator minimizes a new general excess risk bound for statistical learning. On standard examples, this bound scales as $d/n$ with $d$ the model dimension and $n$ the sample size, and critically remains valid under model misspecification. Being an improper (out-of-model) procedure, SMP improves over within-model estimators such as the maximum likelihood estimator, whose excess risk degrades under misspecification. Compared to approaches reducing to the sequential problem, our bounds remove suboptimal $\\log n$ factors and can handle unbounded classes. For the Gaussian linear model, the predictions and risk bound of SMP are governed by leverage scores of covariates, nearly matching the optimal risk in the well-specified case without conditions on the noise variance or approximation error of the linear model. For logistic regression, SMP provides a non-Bayesian approach to calibration of probabilistic predictions relying on virtual samples, and can be computed by solving two logistic regressions. It achieves a non-asymptotic excess risk of $O((d + B^2R^2)/n)$, where $R$ bounds the norm of features and $B$ that of the comparison parameter; by contrast, no within-model estimator can achieve better rate than $\\min({B R}/{\\sqrt{n}}, {d e^{BR}}/{n} )$ in general. This provides a more practical alternative to Bayesian approaches, which require approximate posterior sampling, thereby partly addressing a question raised by Foster et al. (2018)",
    "volume": "main",
    "checked": true,
    "id": "f955c7409dbf56faf5f27f3ccb2bee7dce333186",
    "citation_count": 19
  },
  "https://jmlr.org/papers/v23/20-807.html": {
    "title": "Active Learning for Nonlinear System Identification with Guarantees",
    "abstract": "While the identification of nonlinear dynamical systems is a fundamental building block of model-based reinforcement learning and feedback control, its sample complexity is only understood for systems that either have discrete states and actions or for systems that can be identified from data generated by i.i.d. random inputs. Nonetheless, many interesting dynamical systems have continuous states and actions and can only be identified through a judicious choice of inputs. Motivated by practical settings, we study a class of nonlinear dynamical systems whose state transitions depend linearly on a known feature embedding of state-action pairs. To estimate such systems in finite time identification methods must explore all directions in feature space. We propose an active learning approach that achieves this by repeating three steps: trajectory planning, trajectory tracking, and re-estimation of the system from all available data. We show that our method estimates nonlinear dynamical systems at a parametric rate, similar to the statistical rate of standard linear regression",
    "volume": "main",
    "checked": true,
    "id": "712e3a8b0291413ee44f27058853cfd1e5dad7b6",
    "citation_count": 65
  },
  "https://jmlr.org/papers/v23/20-874.html": {
    "title": "Model Averaging Is Asymptotically Better Than Model Selection For Prediction",
    "abstract": "We compare the performance of six model average predictors---Mallows' model averaging, stacking, Bayes model averaging,  bagging, random forests, and boosting---to the components used to form them.In all six cases we identify conditions under which the model average predictor is consistent for its intended limit and performs as well or better than any of its components asymptotically.   This is well known empirically, especially for complex problems, although theoretical results do not seem to have been formally established. We have focused our attention on the regression context since that is wheremodel averaging techniques differ most often from current practice",
    "volume": "main",
    "checked": true,
    "id": "b4f60f3499b0f395c04340990c4849559772c930",
    "citation_count": 3
  },
  "https://jmlr.org/papers/v23/20-900.html": {
    "title": "SODEN: A Scalable Continuous-Time Survival Model through Ordinary Differential Equation Networks",
    "abstract": "In this paper, we propose a flexible model for survival analysis using neural networks along with scalable optimization algorithms. One key technical challenge for directly applying maximum likelihood estimation (MLE) to censored data is that evaluating the objective function and its gradients with respect to model parameters requires the calculation of integrals. To address this challenge, we recognize from a novel perspective that the MLE for censored data can be viewed as a differential-equation constrained optimization problem. Following this connection, we model the distribution of event time through an ordinary differential equation and utilize efficient ODE solvers and adjoint sensitivity analysis to numerically evaluate the likelihood and the gradients. Using this approach, we are able to 1) provide a broad family of continuous-time survival distributions without strong structural assumptions, 2) obtain powerful feature representations using neural networks, and 3) allow efficient estimation of the model in large-scale applications using stochastic gradient descent. Through both simulation studies and real-world data examples, we demonstrate the effectiveness of the proposed method in comparison to existing state-of-the-art deep learning survival analysis models. The implementation of the proposed SODEN approach has been made publicly available at https://github.com/jiaqima/SODEN",
    "volume": "main",
    "checked": true,
    "id": "30827027a052aa641deffd04a99f8592a1446299",
    "citation_count": 15
  },
  "https://jmlr.org/papers/v23/20-918.html": {
    "title": "Optimality and Stability in Non-Convex Smooth Games",
    "abstract": "Convergence to a saddle point for convex-concave functions has been studied for decades, while recent years has seen a surge of interest in non-convex (zero-sum) smooth games, motivated by their recent wide applications. It remains an intriguing research challenge how local optimal points are defined and which algorithm can converge to such points. An interesting concept is known as the local minimax point, which strongly correlates with the widely-known gradient descent ascent algorithm. This paper aims to provide a comprehensive analysis of local minimax points, such as their relation with other solution concepts and their optimality conditions. We find that local saddle points can be regarded as a special type of local minimax points, called uniformly local minimax points, under mild continuity assumptions. In (non-convex) quadratic games, we show that local minimax points are (in some sense) equivalent to global minimax points. Finally, we study the stability of gradient algorithms near local minimax points. Although gradient algorithms can converge to local/global minimax points in the non-degenerate case, they would often fail in general cases. This implies the necessity of either novel algorithms or concepts beyond saddle points and minimax points in non-convex smooth games",
    "volume": "main",
    "checked": true,
    "id": "249cd065522eb302cef33048cf636dfc09e39dbc",
    "citation_count": 9
  },
  "https://jmlr.org/papers/v23/20-924.html": {
    "title": "Accelerated Zeroth-Order and First-Order Momentum Methods from Mini to Minimax Optimization",
    "abstract": "In the paper, we propose a class of accelerated zeroth-order and first-order momentum methods for both  nonconvex mini-optimization and minimax-optimization. Specifically, we propose a new accelerated zeroth-order momentum (Acc-ZOM) method for black-box mini-optimization where only function values can be obtained. Moreover, we prove that our Acc-ZOM method achieves a lower query complexity of $\\tilde{O}(d^{3/4}\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point, which improves the best known result by a factor of $O(d^{1/4})$ where $d$ denotes the variable dimension. In particular, our  Acc-ZOM does not need large batches required in the existing zeroth-order stochastic algorithms. Meanwhile, we propose an accelerated zeroth-order momentum descent ascent (Acc-ZOMDA) method for black-box minimax  optimization, where only function values can be obtained. Our Acc-ZOMDA obtains a low query complexity of $\\tilde{O}((d_1+d_2)^{3/4}\\kappa_y^{4.5}\\epsilon^{-3})$ without requiring large batches for finding an $\\epsilon$-stationary point, where $d_1$ and $d_2$ denote variable dimensions and $\\kappa_y$ is condition number. Moreover, we propose an accelerated first-order momentum descent ascent (Acc-MDA) method for minimax optimization,  whose explicit gradients are accessible. Our Acc-MDA achieves a low  gradient complexity of $\\tilde{O}(\\kappa_y^{4.5}\\epsilon^{-3})$ without requiring large batches for finding an $\\epsilon$-stationary point. In particular, our Acc-MDA can obtain a lower gradient complexity of $\\tilde{O}(\\kappa_y^{2.5}\\epsilon^{-3})$ with a batch size $O(\\kappa_y^4)$, which improves the best known result by a factor of $O(\\kappa_y^{1/2})$. Extensive experimental results on black-box adversarial attack to deep neural networks and poisoning attack to logistic regression demonstrate efficiency of our algorithms",
    "volume": "main",
    "checked": true,
    "id": "89a7adbde49871101704c43cd9d454355252183f",
    "citation_count": 28
  },
  "https://jmlr.org/papers/v23/21-0059.html": {
    "title": "Projected Statistical Methods for Distributional Data on the Real Line with the Wasserstein Metric",
    "abstract": "We present a novel class of projected methods to perform statistical analysis on a data set of probability distributions on  the real line, with the 2-Wasserstein metric. We focus in particular on Principal Component Analysis (PCA) and regression. To define these models, we exploit a representation of the Wasserstein space closely related to its weak Riemannian structure by mapping the data to a suitable linear space and using a metric projection operator to constrain the results in the Wasserstein space. By carefully choosing the tangent point, we are able to derive fast empirical methods, exploiting a constrained B-spline approximation.  As a byproduct of our approach, we are also able to derive faster routines for previous work on PCA for distributions. By means of simulation studies, we compare our approaches to previously proposed methods, showing that our projected PCA has similar performance for a fraction of the computational cost and that the projected regression is extremely flexible even under misspecification. Several theoretical properties of the models are investigated, and asymptotic consistency is proven. Two real world applications to Covid-19 mortality in the US and wind speed forecasting are discussed",
    "volume": "main",
    "checked": true,
    "id": "837f223d79c2baf176f05f3141b1b1ce4bf53f55",
    "citation_count": 10
  },
  "https://jmlr.org/papers/v23/21-0061.html": {
    "title": "Score Matched Neural Exponential Families for Likelihood-Free Inference",
    "abstract": "Bayesian Likelihood-Free Inference (LFI) approaches allow to obtain posterior distributions for stochastic models with intractable likelihood, by relying on model simulations. In Approximate Bayesian Computation (ABC), a popular LFI method, summary statistics are used to reduce data dimensionality. ABC algorithms adaptively tailor simulations to the observation in order to sample from an approximate posterior, whose form depends on the chosen statistics. In this work, we introduce a new way to learn ABC statistics: we first generate parameter-simulation pairs from the model independently on the observation; then, we use Score Matching to train a neural conditional exponential family to approximate the likelihood. The exponential family is the largest class of distributions with fixed-size sufficient statistics; thus, we use them in ABC, which is intuitively appealing and has state-of-the-art performance. In parallel, we insert our likelihood approximation in an MCMC for doubly intractable distributions to draw posterior samples. We can repeat that for any number of observations with no additional model simulations, with performance comparable to related approaches. We validate our methods on toy models with known likelihood and a large-dimensional time-series model",
    "volume": "main",
    "checked": true,
    "id": "8d2de0c4f67a56d9ba579f9bb8551f785e65a37f",
    "citation_count": 15
  },
  "https://jmlr.org/papers/v23/21-0100.html": {
    "title": "(f,Gamma)-Divergences: Interpolating between f-Divergences and Integral Probability Metrics",
    "abstract": "We develop a rigorous and general framework for  constructing  information-theoretic divergences that subsume both $f$-divergences and integral probability metrics (IPMs),  such as  the  $1$-Wasserstein distance. We prove under which assumptions these divergences, hereafter referred to as $(f,\\Gamma)$-divergences,  provide a notion of `distance' between probability measures and show that they can be expressed as a two-stage mass-redistribution/mass-transport  process. The  $(f,\\Gamma)$-divergences inherit features  from IPMs,   such as   the ability  to compare distributions which are not absolutely continuous, as well as   from $f$-divergences, namely   the strict concavity of their variational representations and the ability to control heavy-tailed distributions  for particular choices of $f$. When combined, these features  establish a divergence with improved properties for estimation, statistical learning, and uncertainty quantification applications. Using statistical learning as an example, we demonstrate their advantage in training generative adversarial networks (GANs) for heavy-tailed, not-absolutely continuous sample distributions. We also show improved performance and stability over gradient-penalized Wasserstein GAN in image generation",
    "volume": "main",
    "checked": false,
    "id": "1ae10d88c31aaa4fba29e6e8c50499ce9404d4ed",
    "citation_count": 2
  },
  "https://jmlr.org/papers/v23/21-0338.html": {
    "title": "Structure-adaptive Manifold Estimation",
    "abstract": "We consider a problem of manifold estimation from noisy observations. Many manifold learning procedures locally approximate a manifold by a weighted average over a small neighborhood. However, in the presence of large noise, the assigned weights become so corrupted that the averaged estimate shows very poor performance. We suggest a structure-adaptive procedure, which simultaneously reconstructs a smooth manifold and estimates projections of the point cloud onto this manifold. The proposed approach iteratively refines the weights on each step, using the structural information obtained at previous steps. After several iterations, we obtain nearly âoracleâ weights, so that the final estimates are nearly efficient even in the presence of relatively large noise. In our theoretical study, we establish tight lower and upper bounds proving  asymptotic optimality of the method for manifold estimation under the Hausdorff loss, provided that the noise degrades to zero fast enough",
    "volume": "main",
    "checked": true,
    "id": "317577d449dfeb6c0c8b94b5f7cdf5835adba02b",
    "citation_count": 10
  },
  "https://jmlr.org/papers/v23/21-0345.html": {
    "title": "The correlation-assisted missing data estimator",
    "abstract": "We introduce a novel approach to estimation problems in settings with missing data. Our proposal -- the Correlation-Assisted Missing data (CAM) estimator -- works by exploiting the relationship between the observations with missing features and those without missing features in order to obtain improved prediction accuracy.  In particular, our theoretical results elucidate general conditions under which the proposed CAM estimator has lower mean squared error than the widely used complete-case approach in a range of estimation problems.  We showcase in detail how the CAM estimator can be applied to $U$-Statistics to obtain an unbiased, asymptotically Gaussian estimator that has lower variance than the complete-case $U$-Statistic.  Further, in nonparametric density estimation and regression problems, we construct our CAM estimator using kernel functions, and show it has lower asymptotic mean squared error than the corresponding complete-case kernel estimator.  We also include practical demonstrations throughout the paper using simulated data and the Terneuzen birth cohort and Brandsma datasets available from CRAN",
    "volume": "main",
    "checked": true,
    "id": "17688c565bef0a68cb42b73e3b1b0eb1f91cfc77",
    "citation_count": 4
  },
  "https://jmlr.org/papers/v23/21-0368.html": {
    "title": "Approximation and Optimization Theory for Linear Continuous-Time Recurrent Neural Networks",
    "abstract": "We perform a systematic study of the approximation properties and optimization dynamics of recurrent neural networks (RNNs) when applied to learn input-output relationships in temporal data. We consider the simple but representative setting of using continuous-time linear RNNs to learn from data generated by linear relationships. On the approximation side, we prove a direct and an inverse approximation theorem of linear functionals using RNNs, which reveal the intricate connections between memory structures in the target and the corresponding approximation efficiency. In particular, we show that temporal relationships can be effectively approximated by RNNs if and only if the former possesses sufficient memory decay. On the optimization front, we perform detailed analysis of the optimization dynamics, including a precise understanding of the difficulty that may arise in learning relationships with long-term memory. The term âcurse of memoryâ is coined to describe the uncovered phenomena, akin to the âcurse of dimensionâ that plagues high-dimensional function approximation. These results form a relatively complete picture of the interaction of memory and recurrent structures in the linear dynamical setting",
    "volume": "main",
    "checked": true,
    "id": "17f365fffb0c426b9269c6f863d6d6d14e1e2d0d",
    "citation_count": 13
  },
  "https://jmlr.org/papers/v23/21-0439.html": {
    "title": "Sampling Permutations for Shapley Value Estimation",
    "abstract": "Game-theoretic attribution techniques based on Shapley values are used to interpret black-box machine learning models, but their exact calculation is generally NP-hard, requiring approximation methods for non-trivial models. As the computation of Shapley values can be expressed as a summation over a set of permutations, a common approach is to sample a subset of these permutations for approximation. Unfortunately, standard Monte Carlo sampling methods can exhibit slow convergence, and more sophisticated quasi-Monte Carlo methods have not yet been applied to the space of permutations. To address this, we investigate new approaches based on two classes of approximation methods and compare them empirically. First, we demonstrate quadrature techniques in a RKHS containing functions of permutations, using the Mallows kernel in combination with kernel herding and sequential Bayesian quadrature. The RKHS perspective also leads to quasi-Monte Carlo type error bounds, with a tractable discrepancy measure defined on permutations. Second, we exploit connections between the hypersphere $\\mathbb{S}^{d-2}$ and permutations to create practical algorithms for generating permutation samples with good properties. Experiments show the above techniques provide significant improvements for Shapley value estimates over existing methods, converging to a smaller RMSE in the same number of model evaluations",
    "volume": "main",
    "checked": true,
    "id": "61b9ec1dcc22aa7415e2633b63978e8876bb278b",
    "citation_count": 39
  },
  "https://jmlr.org/papers/v23/21-0451.html": {
    "title": "PAC Guarantees and Effective Algorithms for Detecting Novel Categories",
    "abstract": "Open category detection is the problem of detecting âalien\" test instances that belong to categories or classes that were not present in the training data. In many applications, reliably detecting such aliens is central to ensuring the safety and accuracy of test set predictions. Unfortunately, there are no algorithms that provide theoretical guarantees on their ability to detect aliens under general assumptions. Further, while there are algorithms for open category detection, there are few empirical results that directly report alien detection rates. Thus, there are significant theoretical and empirical gaps in our understanding of open category detection. In this paper, we take a step toward addressing this gap by studying a simple, but practically-relevant variant of open category detection. In our setting, we are provided with a âclean\" training set that contains only the target categories of interest and an unlabeled âcontaminatedâ training set that contains a fraction $\\alpha$ of alien examples. Under the assumption that we know an upper bound on $\\alpha$, we develop an algorithm that gives PAC-style guarantees on the alien detection rate, while aiming to minimize false alarms. Given an overall budget on the amount of training data, we also derive the optimal allocation of samples between the mixture and the clean data sets. Experiments on synthetic and standard benchmark datasets evaluate the regimes in which the algorithm can be effective and provide a baseline for further advancements. In addition, for the situation when an upper bound for $\\alpha$ is not available, we employ nine different anomaly proportion estimators, and run experiments on both synthetic and standard benchmark data sets to compare their performance",
    "volume": "main",
    "checked": true,
    "id": "c45ea2d5fbd0d4dffd513331f8bdd7e2579614b0",
    "citation_count": 5
  },
  "https://jmlr.org/papers/v23/21-0519.html": {
    "title": "Optimal Transport for Stationary Markov Chains via Policy Iteration",
    "abstract": "We study the optimal transport problem for pairs of stationary finite-state Markov chains, with an emphasis on the computation of optimal transition couplings. Transition couplings are a constrained family of transport plans that capture the dynamics of Markov chains. Solutions of the optimal transition coupling (OTC) problem correspond to alignments of the two chains that minimize long-term average cost. We establish a connection between the OTC problem and Markov decision processes, and show that solutions of the OTC problem can be obtained via an adaptation of policy iteration. For settings with large state spaces, we develop a fast approximate algorithm based on an entropy-regularized version of the OTC problem, and provide bounds on its per-iteration complexity. We establish a stability result for both the regularized and unregularized algorithms, from which a statistical consistency result follows as a corollary. We validate our theoretical results empirically through a simulation study, demonstrating that the approximate algorithm exhibits faster overall runtime with low error. Finally, we extend the setting and application of our methods to hidden Markov models, and illustrate the potential use of the proposed algorithms in practice with an application to computer-generated music",
    "volume": "main",
    "checked": true,
    "id": "03ab9ac5fc0ac6cb9102ef9360d5a98163fd5474",
    "citation_count": 6
  },
  "https://jmlr.org/papers/v23/21-0560.html": {
    "title": "Beyond Sub-Gaussian Noises: Sharp Concentration Analysis for Stochastic Gradient Descent",
    "abstract": "In this paper, we study the concentration property of stochastic gradient descent (SGD) solutions. In existing concentration analyses, researchers impose restrictive requirements on the gradient noise, such as boundedness or sub-Gaussianity. We consider a  much richer class of noise where only finitely-many moments are required, thus allowing heavy-tailed noises. In particular, we obtain Nagaev type high-probability upper bounds for the estimation errors of averaged stochastic gradient descent (ASGD) in a linear model. Specifically, we prove that, after $T$ steps of SGD, the ASGD estimate achieves an $O(\\sqrt{\\log(1/\\delta)/T} + (\\delta T^{q-1})^{-1/q})$ error rate with probability at least $1-\\delta$, where $q>2$ controls the tail of the gradient noise. In comparison, one has the $O(\\sqrt{\\log(1/\\delta)/T})$ error rate for sub-Gaussian noises. We also show that the Nagaev type upper bound is almost tight through an example, where the exact asymptotic form of the tail probability can be derived.  Our concentration analysis indicates that, in the case of heavy-tailed noises, the polynomial dependence on the failure probability $\\delta$ is generally unavoidable for the error rate of SGD",
    "volume": "main",
    "checked": true,
    "id": "1b41f5976e09ae4844b8f6e09d0f0ff149a802eb",
    "citation_count": 11
  },
  "https://jmlr.org/papers/v23/21-0635.html": {
    "title": "Cascaded Diffusion Models for High Fidelity Image Generation",
    "abstract": "We show that cascaded diffusion models are capable of generating high fidelity images on the class-conditional ImageNet generation benchmark, without any assistance from auxiliary image classifiers to boost sample quality. A cascaded diffusion model comprises a pipeline of multiple diffusion models that generate images of increasing resolution, beginning with a standard diffusion model at the lowest resolution, followed by one or more super-resolution diffusion models that successively upsample the image and add higher resolution details. We find that the sample quality of a cascading pipeline relies crucially on conditioning augmentation, our proposed method of data augmentation of the lower resolution conditioning inputs to the super-resolution models. Our experiments show that conditioning augmentation prevents compounding error during sampling in a cascaded model, helping us to train cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at 128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and classification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256x256, outperforming VQ-VAE-2",
    "volume": "main",
    "checked": true,
    "id": "0f183bcfe65781c06b1a48a6f56e0f3c63e8e4a4",
    "citation_count": 367
  },
  "https://jmlr.org/papers/v23/21-0669.html": {
    "title": "Overparameterization of Deep ResNet: Zero Loss and Mean-field Analysis",
    "abstract": "Finding parameters in a deep neural network (NN) that fit training data is a nonconvex optimization problem, but a basic first-order optimization method (gradient descent) finds a global optimizer with perfect fit (zero-loss) in many practical situations. We examine this phenomenon for the case of Residual Neural Networks (ResNet) with smooth activation functions in a limiting regime in which both the number of layers (depth) and the number of weights in each layer (width) go to infinity. First, we use a mean-field-limit argument to prove that the gradient descent for parameter training becomes a gradient flow for a probability distribution that is characterized by a partial differential equation (PDE) in the large-NN limit. Next, we show that under certain assumptions, the solution to the PDE converges in the training time to a zero-loss solution. Together, these results suggest that the training of the ResNet gives a near-zero loss if the ResNet is large enough. We give estimates of the depth and width needed to reduce the loss below a given threshold, with high probability",
    "volume": "main",
    "checked": true,
    "id": "5b71c3d7d98372f4e83ecfac0d2148055b762a77",
    "citation_count": 13
  },
  "https://jmlr.org/papers/v23/21-0735.html": {
    "title": "Innovations Autoencoder and its Application in One-class Anomalous Sequence Detection",
    "abstract": "An innovations sequence of a time series is a sequence of independent and identically distributed random variables with which the original time series has a causal representation.  The innovation at a time is statistically independent of the  history of the time series.  As such, it represents the new information contained at present but not in the past.  Because of its simple probability structure, the innovations sequence is the most efficient signature of the original. Unlike the principle or independent component representations, an innovations sequence preserves not only the complete statistical properties but also the temporal order of the original time series. An long-standing open problem is to find a computationally tractable way to extract an innovations sequence of non-Gaussian processes.  This paper presents a deep learning approach, referred to as Innovations Autoencoder (IAE), that extracts innovations sequences using a causal convolutional neural network. An application of IAE to the one-class anomalous sequence detection problem with unknown anomaly and anomaly-free models is also presented",
    "volume": "main",
    "checked": true,
    "id": "9b7d0c52f6c436ff4402e3633518bbb94e115600",
    "citation_count": 2
  },
  "https://jmlr.org/papers/v23/21-0758.html": {
    "title": "Analytically Tractable Hidden-States Inference in Bayesian Neural Networks",
    "abstract": "With few exceptions, neural networks have been relying on backpropagation and gradient descent as the inference engine in order to learn the model parameters, because closed-form Bayesian inference for neural networks has been considered to be intractable. In this paper, we show how we can leverage the tractable approximate Gaussian inference's (TAGI) capabilities to infer hidden states, rather than only using it for inferring the network's parameters. One novel aspect is that it allows inferring hidden states through the imposition of constraints designed to achieve specific objectives, as illustrated through three examples: (1) the generation of adversarial-attack examples, (2) the usage of a neural network as a black-box optimization method, and (3) the application of inference on continuous-action reinforcement learning. In these three examples, the constrains are in (1), a target label chosen to fool a neural network, and in (2 and 3) the derivative of the network with respect to its input that is set to zero in order to infer the optimal input values that are either maximizing or minimizing it. These applications showcase how tasks that were previously reserved to gradient-based optimization approaches can now be approached with analytically tractable inference",
    "volume": "main",
    "checked": true,
    "id": "e56990ee3d249a6258bc6495fa84948ceceac79c",
    "citation_count": 2
  },
  "https://jmlr.org/papers/v23/21-0791.html": {
    "title": "Toolbox for Multimodal Learn (scikit-multimodallearn)",
    "abstract": "scikit-multimodallearn is a Python library for multimodal supervised learning, licensed under Free BSD, and compatible with the well-known scikit-learn toolbox (Fabian Pedregosa, 2011). This paper details the content of the library, including a specific multimodal data formatting and classification and regression algorithms. Use cases and examples are also provided",
    "volume": "main",
    "checked": true,
    "id": "c91b4c9272db2c6f7057a6a7d2f16858a1e779a9",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v23/21-0840.html": {
    "title": "LinCDE: Conditional Density Estimation via Lindsey's Method",
    "abstract": "Conditional density estimation is a fundamental problem in statistics, with scientific and practical applications in biology, economics, finance and environmental studies, to name a few.  In this paper, we propose a conditional density estimator based on gradient boosting and Lindsey's method (LinCDE). LinCDE admits flexible modeling of the density family and can capture distributional characteristics like modality and shape. In particular, when suitably parametrized, LinCDE will produce smooth and non-negative density estimates. Furthermore, like boosted regression trees, LinCDE does automatic feature selection. We demonstrate LinCDE's efficacy through extensive simulations and three real data examples",
    "volume": "main",
    "checked": true,
    "id": "0546f0fce62b17e117cf8344e57c7bb113818696",
    "citation_count": 5
  },
  "https://jmlr.org/papers/v23/21-0862.html": {
    "title": "DoubleML - An Object-Oriented Implementation of Double Machine Learning in Python",
    "abstract": "DoubleML is an open-source Python library implementing the double machine learning framework of Chernozhukov et al. (2018) for a variety of causal models. It contains functionalities for valid statistical inference on causal parameters when the estimation of nuisance parameters is based on machine learning methods. The object-oriented implementation of DoubleML provides a high flexibility in terms of model specifications and makes it easily extendable. The package is distributed under the MIT license and relies on core libraries from the scientific Python ecosystem: scikit-learn, numpy, pandas, scipy, statsmodels and joblib. Source code, documentation and an extensive user guide can be found at https://github.com/DoubleML/doubleml-for-py and https://docs.doubleml.org",
    "volume": "main",
    "checked": true,
    "id": "fc294d70337105961dd577b0d772fa9ff5f5812d",
    "citation_count": 15
  },
  "https://jmlr.org/papers/v23/21-0888.html": {
    "title": "SMAC3: A Versatile Bayesian Optimization Package for Hyperparameter Optimization",
    "abstract": "Algorithm parameters, in particular hyperparameters of machine learning algorithms, can substantially impact their performance. To support users in determining well-performing hyperparameter configurations for their algorithms, datasets and applications at hand, SMAC3 offers a robust and flexible framework for Bayesian Optimization, which can improve performance within a few evaluations. It offers several facades and pre-sets for typical use cases, such as optimizing hyperparameters, solving low dimensional continuous (artificial) global optimization problems and configuring algorithms to perform well across multiple problem instances. The SMAC3 package is available under a permissive BSD-license at https://github.com/automl/SMAC3",
    "volume": "main",
    "checked": true,
    "id": "c3d07f6838e9c50c0bdd0e5b4b761c6f7244b617",
    "citation_count": 124
  },
  "https://jmlr.org/papers/v23/21-0936.html": {
    "title": "Bayesian Pseudo Posterior Mechanism under Asymptotic Differential Privacy",
    "abstract": "We propose a Bayesian pseudo posterior mechanism to generate record-level synthetic databases equipped with an $(\\epsilon,\\pi)-$ probabilistic differential privacy (pDP) guarantee, where $\\pi$ denotes the probability that any observed database exceeds $\\epsilon$.  The pseudo posterior mechanism employs a data record-indexed, risk-based weight vector with weight values $\\in [0, 1]$ that surgically downweight the likelihood contributions for high-risk records for model estimation and the generation of record-level synthetic data for public release. The pseudo posterior synthesizer constructs a weight for each datum record by using the Lipschitz bound for that record under a log-pseudo likelihood utility function that generalizes the exponential mechanism (EM) used to construct a formally private data generating mechanism.  By selecting weights to remove likelihood contributions with non-finite log-likelihood values, we guarantee a finite local privacy guarantee for our pseudo posterior mechanism at every sample size.  Our results may be applied to any synthesizing model envisioned by the data disseminator in a computationally tractable way that only involves estimation of a pseudo posterior distribution for parameters, $\\theta$, unlike recent approaches that use naturally-bounded utility functions implemented through the EM.  We specify conditions that guarantee the asymptotic contraction of $\\pi$ to $0$ over the space of databases, such that the form of the guarantee provided by our method is asymptotic. We illustrate our pseudo posterior mechanism on the sensitive family income variable from the Consumer Expenditure Surveys database published by the U.S. Bureau of Labor Statistics. We show that utility is better preserved in the synthetic data for our pseudo posterior mechanism as compared to the EM, both estimated using the same non-private synthesizer, due to our use of targeted downweighting",
    "volume": "main",
    "checked": true,
    "id": "31be01f26c7a1d86a7916726fe5ac1d750bfd8af",
    "citation_count": 6
  },
  "https://jmlr.org/papers/v23/21-1155.html": {
    "title": "solo-learn: A Library of Self-supervised Methods for Visual Representation Learning",
    "abstract": "This paper presents solo-learn, a library of self-supervised methods for visual representation learning. Implemented in Python, using Pytorch and Pytorch lightning, the library fits both research and industry needs by featuring distributed training pipelines with mixed-precision, faster data loading via Nvidia DALI, online linear evaluation for better prototyping, and many additional training tricks.  Our goal is to provide an easy-to-use library comprising a large amount of Self-supervised Learning (SSL) methods, that can be easily extended and fine-tuned by the community. solo-learn opens up avenues for exploiting large-budget SSL solutions on inexpensive smaller infrastructures and seeks to democratize SSL by making it accessible to all. The source code is available at https://github.com/vturrisi/solo-learn",
    "volume": "main",
    "checked": true,
    "id": "e95a2817efcbc12b2fa5a7ca3b6cb8c57c20715f",
    "citation_count": 62
  },
  "https://jmlr.org/papers/v23/21-1427.html": {
    "title": "Inherent Tradeoffs in Learning Fair Representations",
    "abstract": "Real-world applications of machine learning tools in high-stakes domains are often regulated to be fair, in the sense that the predicted target should satisfy some quantitative notion of parity with respect to a protected attribute. However, the exact tradeoff between fairness and accuracy is not entirely clear, even for the basic paradigm of classification problems. In this paper, we characterize an inherent tradeoff between statistical parity and accuracy in the classification setting by providing a lower bound on the sum of group-wise errors of any fair classifiers. Our impossibility theorem could be interpreted as a certain uncertainty principle in fairness: if the base rates differ among groups, then any fair classifier satisfying statistical parity has to incur a large error on at least one of the groups. We further extend this result to give a lower bound on the joint error of any (approximately) fair classifiers, from the perspective of learning fair representations. To show that our lower bound is tight, assuming oracle access to Bayes (potentially unfair) classifiers, we also construct an algorithm that returns a randomized classifier which is both optimal (in terms of accuracy) and fair. Interestingly, when the protected attribute can take more than two values, an extension of this lower bound does not admit an analytic solution. Nevertheless, in this case, we show that the lower bound can be efficiently computed by solving a linear program, which we term as the TV-Barycenter problem, a barycenter problem under the TV-distance. On the upside, we prove that if the group-wise Bayes optimal classifiers are close, then learning fair representations leads to an alternative notion of fairness, known as the accuracy parity, which states that the error rates are close between groups. Finally, we also conduct experiments on real-world datasets to confirm our theoretical findings",
    "volume": "main",
    "checked": true,
    "id": "6460a9624fa3a9f6bd06096130644492e714f3f7",
    "citation_count": 125
  },
  "https://jmlr.org/papers/v23/19-297.html": {
    "title": "A Statistical Approach for Optimal Topic Model Identification",
    "abstract": "Latent Dirichlet Allocation is a popular machine-learning technique that identifies latent structures in a corpus of documents. This paper addresses the ongoing concern that formal procedures for determining the optimal LDA configuration do not exist by introducing a set of parametric tests that rely on the assumed multinomial distribution specification underlying the original LDA model. Our methodology defines a set of rigorous statistical procedures that identify and evaluate the optimal topic model. The U.S. Presidential Inaugural Address Corpus is used as a case study to show the numerical results. We find that 92 topics best describe the corpus. We further validate the method through a simulation study confirming the superiority of our approach compared to other standard heuristic metrics like the perplexity index",
    "volume": "main",
    "checked": true,
    "id": "b1d00986e9e03a78f4fa3ac7e6335c0e3d336959",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v23/19-480.html": {
    "title": "Causal Classification: Treatment Effect Estimation vs. Outcome Prediction",
    "abstract": "The goal of causal classification is to identify individuals whose outcome would be positively changed by a treatment. Examples include targeting advertisements and targeting retention incentives to reduce churn. Causal classification is challenging because we observe individuals under only one condition (treated or untreated), so we do not know who was influenced by the treatment, but we may estimate the potential outcomes under each condition to decide whom to treat by estimating treatment effects. Curiously, we often see practitioners using simple outcome prediction instead, for example, predicting if someone will purchase if shown the ad. Rather than disregarding this as naive behavior, we present a theoretical analysis comparing treatment effect estimation and outcome prediction when addressing causal classification. We focus on the key question: \"When (if ever) is simple outcome prediction preferable to treatment effect estimation for causal classification?\" The analysis reveals a causal bias--variance tradeoff. First, when the treatment effect estimation depends on two outcome predictions, larger sampling variance may lead to more errors than the (biased) outcome prediction approach. Second, a stronger signal-to-noise ratio in outcome prediction implies that the bias can help with intervention decisions when outcomes are informative of effects. The theoretical results, as well as simulations, illustrate settings where outcome prediction should actually be better, including cases where (1) the bias may be partially corrected by choosing a different threshold, (2) outcomes and treatment effects are correlated, and (3) data to estimate counterfactuals are limited. A major practical implication is that, for some applications, it might be feasible to make good intervention decisions without any data on how individuals actually behave when intervened.  Finally, we show that for a real online advertising application, outcome prediction models indeed excel at causal classification",
    "volume": "main",
    "checked": true,
    "id": "37427e8cd101798f413816a183999bcd4213be26",
    "citation_count": 12
  },
  "https://jmlr.org/papers/v23/19-513.html": {
    "title": "A Unifying Framework for Variance-Reduced Algorithms for Findings Zeroes of Monotone operators",
    "abstract": "It is common to encounter large-scale monotone inclusion problems where the objective has a finite sum structure.  We develop a general framework for variance-reduced forward-backward splitting algorithms for this problem.  This framework includes a number of existing deterministic and variance-reduced algorithms for function minimization as special cases, and it is also applicable to more general problems such as saddle-point problems and variational inequalities.  With a carefully constructed Lyapunov function, we show that the algorithms covered by our framework enjoy a linear convergence rate in expectation under mild assumptions. We further consider Catalyst acceleration and asynchronous implementation to reduce the algorithmic complexity and computation time. We apply our proposed framework to a policy evaluation problem and a  strongly monotone two-player game, both of which fall outside the realm of function minimization",
    "volume": "main",
    "checked": false,
    "id": "a44fbdca73028d45469fe92d00192446302c0ee4",
    "citation_count": 3
  },
  "https://jmlr.org/papers/v23/19-597.html": {
    "title": "Sparse Additive Gaussian Process Regression",
    "abstract": "In this paper we introduce a novel model for Gaussian process (GP) regression in the fully Bayesian setting. Motivated by the ideas of sparsification, localization and Bayesian additive modeling, our model is built around a recursive partitioning (RP) scheme. Within each RP partition, a sparse GP (SGP) regression model is fitted. A Bayesian additive framework then combines multiple layers of partitioned SGPs, capturing both global trends and local refinements with efficient computations. The model addresses both the problem of efficiency in fitting a full Gaussian process regression model and the problem of prediction performance associated with a single SGP. Our approach mitigates the issue of pseudo-input selection and avoids the need for complex inter-block correlations in existing methods.  The crucial trade-off becomes choosing between many simpler local model components or fewer complex global model components, which the practitioner can sensibly tune. Implementation is via a Metropolis-Hasting Markov chain Monte-Carlo algorithm with Bayesian back-fitting. We compare our model against popular alternatives on simulated and real datasets, and find the performance is competitive, while the fully Bayesian procedure enables the quantification of model uncertainties",
    "volume": "main",
    "checked": true,
    "id": "0932140cf21d0db3e514d795e3619f410292a0a3",
    "citation_count": 7
  },
  "https://jmlr.org/papers/v23/19-599.html": {
    "title": "The AIM and EM Algorithms for Learning from Coarse Data",
    "abstract": "Statistical learning from incomplete data is typically performed under an assumption of ignorability for the mechanism that causes missing values. Notably, the expectation maximization (EM) algorithm is based on the assumption that values are missing at random. Most approaches that tackle non-ignorable mechanisms are based on specific modeling assumptions for these mechanisms. The adaptive imputation and maximization (AIM) algorithm has  been introduced in earlier  work as a general paradigm for learning from incomplete data without any assumptions on the process that causes observations to be incomplete.   In this paper we give a thorough analysis of the theoretical properties of the AIM algorithm, and its  relationship with EM. We identify conditions under which EM and AIM are in fact equivalent, and show that when these conditions are not met, then AIM can produce consistent estimates in non-ignorable incomplete data scenarios where EM becomes inconsistent. Convergence results for AIM are obtained that closely mirror the available convergence  guarantees for EM. We develop the general theory of the AIM algorithm for discrete data settings, and then develop a general discretization approach that allows to apply the method also to incomplete continuous data.  We demonstrate the practical usability of the AIM algorithm by prototype implementations for  parameter learning from continuous Gaussian data, and from discrete Bayesian network data. Extensive experiments  show that the theoretical differences between AIM and EM can be observed in practice, and that a combination of the two methods leads to robust performance for both ignorable and non-ignorable mechanisms",
    "volume": "main",
    "checked": true,
    "id": "6b5e9c4b95e962c722d2cd22e1f19a6539d76876",
    "citation_count": 0
  },
  "https://jmlr.org/papers/v23/19-697.html": {
    "title": "Additive nonlinear quantile regression in ultra-high dimension",
    "abstract": "We propose a method for simultaneous estimation and variable selection of an additive quantile regression model that can be used with high dimensional data. Quantile regression is an appealing method for analyzing high dimensional data because it can correctly model heteroscedastic relationships, is robust to outliers in the response, sparsity levels can change with quantiles, and it provides a thorough analysis of the conditional distribution of the response. An additive nonlinear model can capture more complex relationships, while avoiding the curse of dimensionality. The additive nonlinear model is fit using B-splines and a nonconvex group penalty is used for simultaneous estimation and variable selection. We derive the asymptotic properties of the estimator, including an oracle property, under general conditions that allow for the number of covariates, $p_n$, and the number of true covariates, $q_n$, to increase with the sample size, $n$. In addition, we propose a coordinate descent algorithm that reduces the computational cost compared to the linear programming approach typically used for solving quantile regression problems. The performance of the method is tested using Monte Carlo simulations, an analysis of fat content of meat conditional on a 100 channel spectrum of absorbances and predicting TRIM32 expression using gene expression data from the eyes of rats",
    "volume": "main",
    "checked": true,
    "id": "e0174bc372d86c5b625b88791522cc7fb8b4ebae",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v23/19-750.html": {
    "title": "Stochastic Zeroth-Order Optimization under Nonstationarity and Nonconvexity",
    "abstract": "Stochastic zeroth-order optimization algorithms have been predominantly analyzed under the assumption that the objective function being optimized is time-invariant. Motivated by dynamic matrix sensing and completion problems, and online reinforcement learning problems, in this work, we propose and analyze stochastic zeroth-order optimization algorithms when the objective being optimized changes with time. Considering general nonconvex functions, we propose nonstationary versions of regret measures based on first-order and second-order optimal solutions, and provide the corresponding regret bounds.  For the case of first-order optimal solution based regret measures, we provide regret bounds in both the low- and high-dimensional settings. For the case of second-order optimal solution based regret, we propose zeroth-order versions of the stochastic cubic-regularized Newton's method based on estimating the Hessian matrices in the bandit setting via second-order Gaussian Stein's identity. Our nonstationary regret bounds in terms of second-order optimal solutions have interesting consequences for avoiding saddle points in the nonstationary setting",
    "volume": "main",
    "checked": true,
    "id": "9a3e99f04e7199192bed9f4e7842b7dd69a54b35",
    "citation_count": 3
  },
  "https://jmlr.org/papers/v23/19-843.html": {
    "title": "On the Complexity of Approximating Multimarginal Optimal Transport",
    "abstract": "We study the complexity of approximating the multimarginal optimal transport (MOT) distance, a generalization of the classical optimal transport distance, considered here between $m$ discrete probability distributions supported each on $n$ support points. First, we show that the standard linear programming (LP) representation of the MOT problem is not a minimum-cost flow problem when $m \\geq 3$. This negative result implies that some combinatorial algorithms, e.g., network simplex method, are not suitable for approximating the MOT problem, while the worst-case complexity bound for the deterministic interior-point algorithm remains a quantity of $\\tilde{\\mathcal{O}}(n^{3m})$. We then propose two simple and deterministic algorithms for approximating the MOT problem. The first algorithm, which we refer to as multimarginal Sinkhorn algorithm, is a provably efficient multimarginal generalization of the Sinkhorn algorithm. We show that it achieves a complexity bound of $\\tilde{\\mathcal{O}}(m^3n^m\\varepsilon^{-2})$ for a tolerance $\\varepsilon \\in (0, 1)$. This provides a first near-linear time complexity bound guarantee for approximating the MOT problem and matches the best known complexity bound for the Sinkhorn algorithm in the classical OT setting when $m = 2$. The second algorithm, which we refer to as accelerated multimarginal Sinkhorn algorithm, achieves the acceleration by incorporating an estimate sequence and the complexity bound is $\\tilde{\\mathcal{O}}(m^3n^{m+1/3}\\varepsilon^{-4/3})$. This bound is better than that of the first algorithm in terms of $1/\\varepsilon$, and accelerated alternating minimization algorithm (Tupitsa et al., 2020)  in terms of $n$. Finally, we compare our new algorithms with the commercial LP solver Gurobi. Preliminary results on synthetic data and real images demonstrate the effectiveness and efficiency of our algorithms",
    "volume": "main",
    "checked": true,
    "id": "294a53b7e34cc302ae351389109178db5318c1ff",
    "citation_count": 47
  },
  "https://jmlr.org/papers/v23/20-064.html": {
    "title": "New Insights for the Multivariate Square-Root Lasso",
    "abstract": "We study the multivariate square-root lasso, a method for fitting the multivariate response linear regression model with dependent errors. This estimator minimizes the nuclear norm of the residual matrix plus a convex penalty. Unlike existing methods that require explicit estimates of the error precision (inverse covariance) matrix, the multivariate square-root lasso implicitly accounts for error dependence and is the solution to a convex optimization problem. We establish error bounds which reveal that like the univariate square-root lasso, the multivariate square-root lasso is pivotal with respect to the unknown error covariance matrix. In addition, we propose a variation of the alternating direction method of multipliers algorithm to compute the estimator and discuss an accelerated first order algorithm that can be applied in certain cases. In both simulation studies and a genomic data application, we show that the multivariate square-root lasso can outperform more computationally intensive methods that require explicit estimation of the error precision matrix",
    "volume": "main",
    "checked": true,
    "id": "03c32fa5afeccead41226cbff06a95cb97626375",
    "citation_count": 2
  },
  "https://jmlr.org/papers/v23/20-069.html": {
    "title": "Are All Layers Created Equal?",
    "abstract": "Understanding deep neural networks is a major research objective with notable experimental and theoretical attention in recent years. The practical success of excessively large networks underscores the need for better theoretical analyses and justifications. In this paper we focus on layer-wise functional structure and behavior in overparameterized deep models. To do so, we study empirically the layers' robustness to post-training re-initialization and re-randomization of the parameters. We provide experimental results which give evidence for the heterogeneity of layers. Morally, layers of large deep neural networks can be categorized as either \"robust\" or \"critical\". Resetting the robust layers to their initial values does not result in adverse decline in performance. In many cases, robust layers hardly change throughout training. In contrast, re-initializing critical layers vastly degrades the performance of the network with test error essentially dropping to random guesses. Our study provides further evidence that mere parameter counting or norm calculations are too coarse in studying generalization of deep models, and \"flatness\" and robustness analysis of trained models need to be examined while taking into account the respective network architectures",
    "volume": "main",
    "checked": true,
    "id": "4ac62731b802c727f916e8deefda1a992991505d",
    "citation_count": 125
  },
  "https://jmlr.org/papers/v23/20-099.html": {
    "title": "Scaling-Translation-Equivariant Networks with Decomposed Convolutional Filters",
    "abstract": "Encoding the scale information explicitly into the representation learned by a convolutional neural network (CNN) is beneficial for many computer vision tasks especially when dealing with multiscale inputs. We study, in this paper, a scaling-translation-equivariant ($\\mathcal{ST}$-equivariant) CNN with joint convolutions across the space and  the scaling group, which is shown to be both sufficient and necessary to achieve equivariance for the regular representation of the scaling-translation group $\\mathcal{ST}$. To reduce the model complexity and computational burden,  we decompose the convolutional filters under two pre-fixed separable bases and truncate the expansion to  low-frequency components. A further benefit of the truncated filter expansion is the improved deformation robustness of the equivariant representation, a property which is theoretically analyzed and empirically verified. Numerical experiments demonstrate that the proposed scaling-translation-equivariant network with decomposed convolutional filters (ScDCFNet) achieves significantly improved performance in multiscale image classification and better interpretability than regular CNNs at a reduced model size",
    "volume": "main",
    "checked": true,
    "id": "555e185181ea83b6a99c0c9fa42ea0052c82905c",
    "citation_count": 7
  },
  "https://jmlr.org/papers/v23/20-1027.html": {
    "title": "Asymptotic Network Independence and Step-Size for a Distributed Subgradient Method",
    "abstract": "We consider whether distributed subgradient methods can achieve a linear speedup over a centralized subgradient method. While it might be hoped that distributed network of $n$ nodes that can compute $n$ times more subgradients in parallel compared to a single node might, as a result, be $n$ times faster,  existing bounds for distributed optimization methods are often consistent with a slowdown rather than speedup compared to a single node.  We show that a  distributed subgradient method has this âlinear speedupâ property when using a class of square-summable-but-not-summable step-sizes which include $1/t^{\\beta}$ when $\\beta \\in (1/2,1)$; for such step-sizes, we show that after a  transient period whose size depends on the spectral gap of the network, the method achieves a performance guarantee that does not depend on the network or the number of nodes. We also show that the same method can fail to have this âasymptotic network independenceâ property under the optimally decaying step-size $1/\\sqrt{t}$ and, as a consequence, can fail to provide a linear speedup compared to a single node with $1/\\sqrt{t}$ step-size",
    "volume": "main",
    "checked": true,
    "id": "56c92b8e320cfca1462e534bc5f7a2427bce2986",
    "citation_count": 0
  },
  "https://jmlr.org/papers/v23/20-108.html": {
    "title": "Generalized Sparse Additive Models",
    "abstract": "We present a unified framework for estimation and analysis of generalized additive models in high dimensions. The framework defines a large class of penalized regression estimators, encompassing many existing methods. An efficient computational algorithm for this class is presented that easily scales to thousands of observations and features. We prove minimax optimal convergence bounds for this class under a weak compatibility condition. In addition, we characterize the rate of convergence when this compatibility condition is not met. Finally, we also show that the optimal penalty parameters for structure and sparsity penalties in our framework are linked, allowing cross-validation to be conducted over only a single tuning parameter. We complement our theoretical results with empirical studies comparing some existing methods within this framework",
    "volume": "main",
    "checked": true,
    "id": "f062388abb54c0ba11a8dd04f34bc6ed35707147",
    "citation_count": 8
  },
  "https://jmlr.org/papers/v23/20-1103.html": {
    "title": "Multiple-Splitting Projection Test for High-Dimensional Mean Vectors",
    "abstract": "We propose a multiple-splitting projection test (MPT) for one-sample mean vectors in high-dimensional settings. The idea of projection test is to project high-dimensional samples to a 1-dimensional space using an optimal projection direction such that traditional tests can be carried out with projected samples. However, estimation of the optimal projection direction has not been systematically studied in the literature. In this work, we bridge the gap by proposing a consistent estimation via regularized quadratic optimization. To retain type I error rate, we adopt a data-splitting strategy when constructing test statistics. To mitigate the power loss due to data-splitting, we further propose a test via multiple splits to enhance the testing power. We show that the $p$-values resulted from multiple splits are exchangeable.  Unlike existing methods which tend to conservatively combine dependent $p$-values, we develop an exact level $\\alpha$ test that explicitly utilizes the exchangeability structure to achieve better power. Numerical studies show that the proposed test well retains the type I error rate and is more powerful than state-of-the-art tests",
    "volume": "main",
    "checked": true,
    "id": "56885ac5d88b7431e9c86b870fad0fd8552fa145",
    "citation_count": 4
  },
  "https://jmlr.org/papers/v23/20-1135.html": {
    "title": "Batch Normalization Preconditioning for Neural Network Training",
    "abstract": "Batch normalization (BN) is a popular and ubiquitous method in deep learning that has been shown to decrease training time and improve generalization performance of neural networks. Despite its success, BN is not theoretically well understood. It is not suitable for use with very small mini-batch sizes or online learning. In this paper, we propose a new method called Batch Normalization Preconditioning (BNP). Instead of applying normalization explicitly through a batch normalization layer as is done in BN, BNP applies normalization by conditioning the parameter gradients directly during training. This is designed to improve the Hessian matrix of the loss function and hence convergence during training. One benefit is that BNP is not constrained on the mini-batch size and works in the online learning setting. Furthermore, its connection to BN provides theoretical insights on how BN improves training and how BN is applied to special architectures such as convolutional neural networks. For a theoretical foundation, we also present a novel Hessian condition number based convergence theory for a locally convex but not strong-convex loss, which is applicable to networks with a scale-invariant property",
    "volume": "main",
    "checked": true,
    "id": "4d40b03963663fd16f906f0887ed7f33b96dbd11",
    "citation_count": 0
  },
  "https://jmlr.org/papers/v23/20-1180.html": {
    "title": "A Kernel Two-Sample Test for Functional Data",
    "abstract": "We propose a nonparametric two-sample test procedure based on Maximum Mean Discrepancy (MMD) for testing the hypothesis that two samples of functions have the same underlying distribution, using kernels defined on function spaces. This construction is motivated by a scaling analysis of the efficiency of MMD-based tests for datasets of increasing dimension. Theoretical properties of kernels on function spaces and their associated MMD  are established and employed to ascertain the efficacy of the newly proposed test, as well as to assess the effects of using functional reconstructions based on discretised function samples.  The theoretical results are demonstrated over a range of synthetic and real world datasets",
    "volume": "main",
    "checked": true,
    "id": "4afaaba05a7aca55e160a4cc7fe52a9e892a1e13",
    "citation_count": 22
  },
  "https://jmlr.org/papers/v23/20-1340.html": {
    "title": "All You Need is a Good Functional Prior for Bayesian Deep Learning",
    "abstract": "The Bayesian treatment of neural networks dictates that a prior distribution is specified over their weight and bias parameters. This poses a challenge because modern neural networks are characterized by a large number of parameters, and the choice of these priors has an uncontrolled effect on the induced functional prior, which is the distribution of the functions obtained by sampling the parameters from their prior distribution. We argue that this is a hugely limiting aspect of Bayesian deep learning, and this work tackles this limitation in a practical and effective way. Our proposal is to reason in terms of functional priors, which are easier to elicit, and to âtuneâ the priors of neural network parameters in a way that they reflect such functional priors. Gaussian processes offer a rigorous framework to define prior distributions over functions, and we propose a novel and robust framework to match their prior with the functional prior of neural networks based on the minimization of their Wasserstein distance. We provide vast experimental evidence that coupling these priors with scalable Markov chain Monte Carlo sampling offers systematically large performance improvements over alternative choices of priors and state-of-the-art approximate Bayesian deep learning approaches. We consider this work a considerable step in the direction of making the long-standing challenge of carrying out a fully Bayesian treatment of neural networks, including convolutional neural networks, a concrete possibility",
    "volume": "main",
    "checked": true,
    "id": "6e37049b182c91461d56fa6984860a285cb059de",
    "citation_count": 29
  },
  "https://jmlr.org/papers/v23/20-1358.html": {
    "title": "Mutual Information Constraints for Monte-Carlo Objectives to Prevent Posterior Collapse Especially in Language Modelling",
    "abstract": "Posterior collapse is a common failure mode of density models trained as variational autoencoders, wherein they model the data without relying on their latent variables, rendering these variables useless. We focus on two factors contributing to posterior collapse, that have been studied separately in the literature. First, the underspecification of the model, which in an extreme but common case allows posterior collapse to be the theoretical optimium. Second, the looseness of the variational lower bound and the related underestimation of the utility of the latents. We weave these two strands of research together, specifically the tighter bounds of multi-sample Monte-Carlo objectives and constraints on the mutual information between the observable and the latent variables. The main obstacle is that the usual method of estimating the mutual information as the average Kullback-Leibler divergence between the easily available variational posterior q(z|x) and the prior does not work with Monte-Carlo objectives because their q(z|x) is not a direct approximation to the model's true posterior p(z|x). Hence, we construct estimators of the Kullback-Leibler divergence of the true posterior from the prior by recycling samples used in the objective, with which we train models of continuous and discrete latents at much improved rate-distortion and no posterior collapse. While alleviated, the tradeoff between modelling the data and using the latents still remains, and we urge for evaluating inference methods across a range of mutual information values",
    "volume": "main",
    "checked": true,
    "id": "b2a548611bb389224ded70a6240da81d99caf8fe",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v23/20-1375.html": {
    "title": "Joint Inference of Multiple Graphs from Matrix Polynomials",
    "abstract": "Inferring graph structure from observations on the nodes is an important and popular network science task. Departing from the more common inference of a single graph, we study the problem of jointly inferring multiple graphs from the observation of signals at their nodes (graph signals), which are assumed to be stationary in the sought graphs. Graph stationarity implies that the mapping between the covariance of the signals and the sparse matrix representing the underlying graph is given by a matrix polynomial. A prominent example is that of Markov random fields, where the inverse of the covariance yields the sparse matrix of interest. From a modeling perspective, stationary graph signals can be used to model linear network processes evolving on a set of (not necessarily known) networks. Leveraging that matrix polynomials commute, a convex optimization method along with sufficient conditions that guarantee the recovery of the true graphs are provided when perfect covariance information is available. Particularly important from an empirical viewpoint, we provide high-probability bounds on the recovery error as a function of the number of signals observed and other key problem parameters. Numerical experiments demonstrate the effectiveness of the proposed method with perfect covariance information as well as its robustness in the noisy regime",
    "volume": "main",
    "checked": true,
    "id": "69e8593db688d351f98848323a80dd9af7228926",
    "citation_count": 21
  },
  "https://jmlr.org/papers/v23/20-1384.html": {
    "title": "Efficient Change-Point Detection for Tackling Piecewise-Stationary Bandits",
    "abstract": "We introduce GLRklUCB, a novel algorithm for the piecewise iid non-stationary bandit problem with bounded rewards. This algorithm combines an efficient bandit algorithm, klUCB, with an efficient, parameter-free, change-point detector, the Bernoulli Generalized Likelihood Ratio Test, for which we provide new theoretical guarantees of independent interest. Unlike previous non-stationary bandit algorithms using a change-point detector, GLRklUCB does not need to be calibrated based on prior knowledge on the arms' means. We prove that this algorithm can attain a $O(\\sqrt{TA\\Upsilon_T\\log(T)})$ regret in $T$ rounds on some âeasyâ instances in which there is sufficient delay between two change-points, where $A$ is the number of arms and $\\Upsilon_T$ the number of change-points, without prior knowledge of $\\Upsilon_T$. In contrast with recently proposed algorithms that are agnostic to $\\Upsilon_T$, we perform a numerical study showing that GLRklUCB is also very efficient in practice, beyond easy instances",
    "volume": "main",
    "checked": true,
    "id": "028b8be623117abd699c612b821e74a989ee43a2",
    "citation_count": 11
  },
  "https://jmlr.org/papers/v23/20-1393.html": {
    "title": "Multi-Agent Online Optimization with Delays: Asynchronicity, Adaptivity, and Optimism",
    "abstract": "In this paper, we provide a general framework for studying multi-agent online learning problems in the presence of delays and asynchronicities. Specifically, we propose and analyze a class of adaptive dual averaging schemes in which agents only need to accumulate gradient feedback received from the whole system, without requiring any between-agent coordination. In the single-agent case, the adaptivity of the proposed method allows us to extend a range of existing results to problems with potentially unbounded delays between playing an action and receiving the corresponding feedback. In the multi-agent case, the situation is significantly more complicated because agents may not have access to a global clock to use as a reference point; to overcome this, we focus on the information that is available for producing each prediction rather than the actual delay associated with each feedback. This allows us to derive adaptive learning strategies with optimal regret bounds, even in a fully decentralized, asynchronous environment. Finally, we also analyze an âoptimisticâ variant of the proposed algorithm which is capable of exploiting the predictability of problems with a slower variation and leads to improved regret bounds",
    "volume": "main",
    "checked": true,
    "id": "1d0520703b279e5ae9fc0cca6acdd3da1b2c3efc",
    "citation_count": 15
  },
  "https://jmlr.org/papers/v23/20-1426.html": {
    "title": "Stacking for Non-mixing Bayesian Computations: The Curse and Blessing of Multimodal Posteriors",
    "abstract": "When working with multimodal Bayesian posterior distributions, Markov chain Monte Carlo (MCMC) algorithms have difficulty moving between modes, and default variational or mode-based approximate inferences will understate posterior uncertainty. And, even if the most important modes can be found, it is difficult to evaluate their relative weights in the posterior. Here we propose an approach using parallel runs of MCMC, variational, or mode-based inference to hit as many modes or separated regions as possible and then combine these using Bayesian stacking, a scalable method for constructing a weighted average of distributions. The result from stacking efficiently samples from multimodal posterior distribution, minimizes cross validation prediction error, and represents the posterior uncertainty better than variational inference, but it is not necessarily equivalent, even asymptotically, to fully Bayesian inference. We present theoretical consistency with an example where the stacked inference approximates the true data generating process from the misspecified model and a non-mixing sampler, from which the predictive performance is better than full Bayesian inference, hence the multimodality can be considered a blessing rather than a curse under model misspecification. We demonstrate practical implementation in several model families: latent Dirichlet allocation, Gaussian process regression, hierarchical regression, horseshoe variable selection, and neural networks",
    "volume": "main",
    "checked": true,
    "id": "9b6fbc7bd422e78d0a58a5ffed62816e444e2d22",
    "citation_count": 29
  },
  "https://jmlr.org/papers/v23/20-1474.html": {
    "title": "Posterior Asymptotics for Boosted Hierarchical Dirichlet Process Mixtures",
    "abstract": "Bayesian hierarchical models are powerful tools for learning common latent features across multiple data sources. The Hierarchical Dirichlet Process (HDP) is invoked when the number of latent components is a priori unknown. While there is a rich literature on finite sample properties and performance of hierarchical processes, the analysis of their frequentist posterior asymptotic properties is still at an early stage. Here we establish theoretical guarantees for recovering the true data generating process when the data are modeled as mixtures over the HDP or a generalization of the HDP, which we term boosted because of the faster growth in the number of discovered latent features. By extending Schwartz's theory to partially exchangeable sequences we show that posterior contraction rates are crucially affected by the relationship between the sample sizes corresponding to the different groups. The effect varies according to the smoothness level of the true data distributions. In the supersmooth case,  when the generating densities are Gaussian mixtures, we recover the parametric rate up to a logarithmic factor, provided that the sample sizes are related in a polynomial fashion. Under ordinary smoothness assumptions more caution is needed as a polynomial deviation in the sample sizes could drastically deteriorate the convergence to the truth",
    "volume": "main",
    "checked": true,
    "id": "7eda8920082c36644a83f558a9343695c4961be9",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v23/20-204.html": {
    "title": "Dependent randomized rounding for clustering and partition systems with knapsack constraints",
    "abstract": "Clustering problems are fundamental to unsupervised learning. There is an increased emphasis on fairness in machine learning and AI; one representative notion of fairness is that no single group should be over-represented among the cluster-centers. This, and much more general clustering problems, can be formulated with âknapsack\" and âpartition\" constraints. We develop new randomized algorithms targeting such problems, and study two in particular: multi-knapsack median and multi-knapsack center. Our rounding algorithms give new approximation and pseudo-approximation algorithms for these problems. One key technical tool, which may be of independent interest, is a new tail bound analogous to Feige (2006) for sums of random variables with unbounded variances. Such bounds can be useful in inferring properties of large networks using few samples",
    "volume": "main",
    "checked": true,
    "id": "81c2bd30611bc3227b3490b0941a7b066b43c051",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v23/20-231.html": {
    "title": "FuDGE: A Method to Estimate a Functional Differential Graph in a High-Dimensional Setting",
    "abstract": "We consider the problem of estimating the difference between two undirected functional graphical models with shared structures. In many applications, data are naturally regarded as a vector of random functions rather than as a vector of scalars. For example, electroencephalography (EEG) data are treated more appropriately as functions of time. In such a problem, not only can the number of functions measured per sample be large, but each function is itself an infinite dimensional object, making estimation of model parameters challenging. This is further complicated by the fact that curves are usually observed only at discrete time points. We first define a functional differential graph that captures the differences between two functional graphical models and formally characterize when the functional differential graph is well defined. We then propose a method, FuDGE, that directly estimates the functional differential graph without first estimating each individual graph. This is particularly beneficial in settings where the individual graphs are dense but the differential graph is sparse. We show that FuDGE consistently estimates the functional differential graph even in a high-dimensional setting for both fully observed and discretely observed function paths. We illustrate the finite sample properties of our method through simulation studies. We also propose a competing method, the Joint Functional Graphical Lasso, which generalizes the Joint Graphical Lasso to the functional setting. Finally, we apply our method to EEG data to uncover differences in functional brain connectivity between a group of individuals with alcohol use disorder and a control group",
    "volume": "main",
    "checked": true,
    "id": "367bf3faf378851b324ec9dc389e863864514b50",
    "citation_count": 2
  },
  "https://jmlr.org/papers/v23/20-290.html": {
    "title": "Prior Adaptive Semi-supervised Learning with Application to EHR Phenotyping",
    "abstract": "Electronic Health Record (EHR) data, a rich source for biomedical research, have been successfully used to gain novel insight into a wide range of diseases. Despite its potential, EHR is currently underutilized for discovery research due to its major limitation in the lack of precise phenotype information. To overcome such difficulties, recent efforts have been devoted to developing supervised algorithms to accurately predict phenotypes based on relatively small training datasets with gold-standard labels extracted via chart review. However, supervised methods typically require a sizable training set to yield generalizable algorithms, especially when the number of candidate features is large. In this paper, we propose a semi-supervised (SS) EHR phenotyping method that borrows information from both a small, labeled dataset (where both the label Y and the feature set X are observed) and a much larger, weakly-labeled dataset in which the feature set X is accompanied only by a surrogate label S that is available to all patients. Under a working prior assumption that S is related to X only through Y and allowing it to hold approximately, we propose a prior adaptive semi-supervised (PASS) estimator that incorporates the prior knowledge by shrinking the estimator towards a direction derived under the prior. We derive asymptotic theory for the proposed estimator and justify its efficiency and robustness to prior information of poor quality. We also demonstrate its superiority over existing estimators under various scenarios via simulation studies and on three real-world EHR phenotyping studies at a large tertiary hospital",
    "volume": "main",
    "checked": true,
    "id": "288be55d1e0bcdc716c931bde00b3670cc867aea",
    "citation_count": 2
  },
  "https://jmlr.org/papers/v23/20-543.html": {
    "title": "Distributed Bayesian Varying Coefficient Modeling Using a Gaussian Process Prior",
    "abstract": "Varying coefficient models (VCMs) are widely used for estimating nonlinear regression functions for functional data. Their Bayesian variants using Gaussian process priors on the functional coefficients, however, have received limited attention in massive data applications, mainly due to the prohibitively slow posterior computations using Markov chain Monte Carlo (MCMC) algorithms. We address this problem using a divide-and-conquer Bayesian approach. We first create a large number of data subsamples with much smaller sizes. Then, we formulate the VCM as a linear mixed-effects model and develop a data augmentation algorithm for obtaining MCMC draws on all the subsets in parallel. Finally, we aggregate the MCMC-based estimates of subset posteriors into a single Aggregated Monte Carlo (AMC) posterior, which is used as a computationally efficient alternative to the true posterior distribution. Theoretically, we derive minimax optimal posterior convergence rates for the AMC posteriors of both the varying coefficients and the mean regression function. We provide quantification on the orders of subset sample sizes and the number of subsets. The empirical results show that the combination schemes that satisfy our theoretical assumptions, including the AMC posterior, have better estimation performance than their main competitors across diverse simulations and in a real data analysis",
    "volume": "main",
    "checked": true,
    "id": "ed5134cf0c0a1f71a3650baef05479167cc1910f",
    "citation_count": 6
  },
  "https://jmlr.org/papers/v23/20-682.html": {
    "title": "A Distribution Free Conditional Independence Test with Applications to Causal Discovery",
    "abstract": "This paper is concerned with test of the conditional independence. We first establish an equivalence between the conditional independence and the mutual independence. Based on the equivalence, we propose an index to measure the conditional dependence by quantifying the mutual dependence among the transformed variables. The proposed index has several appealing properties. (a) It is distribution free since the limiting null distribution of the proposed index does not depend on the population distributions of the data. Hence the critical values can be tabulated by simulations. (b) The proposed index ranges from zero to one, and equals zero if and only if the conditional independence holds. Thus, it has nontrivial power under the alternative hypothesis. (c) It is robust to outliers and heavy-tailed data since it is invariant to conditional strictly monotone transformations. (d) It has low computational cost since it incorporates a simple closed-form expression and can be implemented in quadratic time. (e) It is insensitive to tuning parameters involved in the calculation of the proposed index. (f) The new index is applicable for multivariate random vectors as well as for discrete data. All these properties enable us to use the new index as statistical inference tools for various data. The effectiveness of the method is illustrated through extensive simulations and a real application on causal discovery",
    "volume": "main",
    "checked": true,
    "id": "0b28df2be0aad5c06792389dbcee0e2f2b98cff7",
    "citation_count": 6
  },
  "https://jmlr.org/papers/v23/20-786.html": {
    "title": "Robust and scalable manifold learning via landmark diffusion for long-term medical signal processing",
    "abstract": "Motivated by analyzing long-term physiological time series, we design a robust and scalable spectral embedding algorithm that we refer to as RObust and Scalable Embedding via LANdmark Diffusion (Roseland).  The key is designing a diffusion process on the dataset where the diffusion is done via a small subset called the landmark set. Roseland is theoretically justified under the manifold model, and its computational complexity is comparable with commonly applied subsampling scheme such as the Nystr\\\"om extension. Specifically, when there are $n$ data points in $\\mathbb{R}^q$ and $n^\\beta$ points in the landmark set, where $\\beta\\in (0,1)$, the computational complexity of Roseland is $O(n^{1+2\\beta}+qn^{1+\\beta})$, while that of Nystrom is $O(n^{2.81\\beta}+qn^{1+2\\beta})$. To demonstrate the potential of Roseland, we apply it to { three} datasets and compare it with several other existing algorithms. First, we apply Roseland to the task of spectral clustering using the MNIST dataset (70,000 images), achieving 85\\% accuracy when the dataset is clean and 78\\% accuracy when the dataset is noisy. Compared with other subsampling schemes, overall Roseland achieves a better performance. Second, we apply Roseland to the task of image segmentation using images from COCO. Finally, we demonstrate how to apply Roseland to explore long-term arterial blood pressure waveform dynamics during a liver transplant operation lasting for 12 hours. In conclusion, Roseland is scalable and robust, and it has a potential for analyzing large datasets",
    "volume": "main",
    "checked": true,
    "id": "768b0e28e482c262a2bd227f34e5838ccc6e4347",
    "citation_count": 3
  },
  "https://jmlr.org/papers/v23/20-797.html": {
    "title": "CD-split and HPD-split: Efficient Conformal Regions in High Dimensions",
    "abstract": "Conformal methods create prediction bands that control average coverage assuming solely i.i.d. data. Although the literature has mostly focused on  prediction intervals, more general regions can often better represent uncertainty. For instance, a bimodal target is better represented by the union of two intervals. Such prediction regions are obtained by CD-split, which combines the split method and a data-driven partition of the feature space which scales to high dimensions. CD-split however contains many tuning parameters, and their role is not clear. In this paper, we provide new insights on CD-split by exploring its theoretical properties. In particular, we show that CD-split converges asymptotically to the oracle highest predictive density set and satisfies local and asymptotic conditional validity. We also present simulations that show how to tune CD-split. Finally, we introduce HPD-split, a variation of CD-split that requires less tuning, and show that it shares the same theoretical guarantees as CD-split. In a wide variety of our simulations, CD-split and HPD-split have better conditional coverage and yield smaller prediction regions than other methods",
    "volume": "main",
    "checked": true,
    "id": "c6a71a4f5c4e6acbe0565aa99ee00f1d02f571b1",
    "citation_count": 15
  },
  "https://jmlr.org/papers/v23/20-843.html": {
    "title": "Generalized Ambiguity Decomposition for Ranking Ensemble Learning",
    "abstract": "Error decomposition analysis is a key problem for ensemble learning, which indicates that proper combination of multiple models can achieve better performance than any individual one. Existing theoretical research of ensemble learning focuses on regression or classification tasks. There is limited theoretical research for ranking ensemble. In this paper, we first generalize the ambiguity decomposition theory from regression ensemble to ranking ensemble, which proves the effectiveness of ranking ensemble with consideration of list-wise ranking information. According to the generalized theory, we propose an explicit diversity measure for ranking ensemble, which can be used to enhance the diversity of ensemble and improve the performance of ensemble model. Furthermore, we adopt an adaptive learning scheme to learn query-dependent ensemble weights, which can fit into the generalized theory and help to further improve the performance of ensemble model. Extensive experiments on recommendation and information retrieval tasks demonstrate the effectiveness and theoretical advantages of the proposed method compared with several state-of-the-art methods",
    "volume": "main",
    "checked": true,
    "id": "ebf58cf29c7f29ebdd9b547a658b88dc9835a6bb",
    "citation_count": 2
  },
  "https://jmlr.org/papers/v23/20-852.html": {
    "title": "Machine Learning on Graphs: A Model and Comprehensive Taxonomy",
    "abstract": "There has been a surge of recent interest in graph representation learning (GRL). GRL methods have generally fallen into three main categories, based on the availability of labeled data. The first, network embedding, focuses on learning unsupervised representations of relational structure. The second, graph regularized neural networks, leverages graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. However, despite the popularity of these areas there has been surprisingly little work on unifying the three paradigms. Here, we aim to bridge the gap between network embedding, graph regularization and graph neural networks. We propose a comprehensive taxonomy of GRL methods, aiming to unify several disparate bodies of work. Specifically, we propose the GraphEDM framework, which generalizes popular algorithms for semi-supervised learning (e.g. GraphSage, GCN, GAT), and unsupervised learning (e.g. DeepWalk, node2vec) of graph representations into a single consistent approach. To illustrate the generality of GraphEDM, we fit over thirty existing methods into this framework. We believe that this unifying view both provides a solid foundation for understanding the intuition behind these methods, and enables future research in the area",
    "volume": "main",
    "checked": true,
    "id": "83e89d56d0d0e1dfd8b52213e6cc2e191aaaf34b",
    "citation_count": 184
  },
  "https://jmlr.org/papers/v23/20-910.html": {
    "title": "Accelerating Adaptive Cubic Regularization of Newton's Method via Random Sampling",
    "abstract": "In this paper, we consider an unconstrained optimization model where the objective is a sum of a large number of possibly nonconvex functions, though overall the objective is assumed to be smooth and convex. Our bid to solving such model uses the framework of cubic regularization of Newton's method. As well known, the crux in cubic regularization is its utilization of the Hessian information, which may be computationally expensive for large-scale problems. To tackle this, we resort to approximating the Hessian matrix via sub-sampling. In particular, we propose to compute an approximated Hessian matrix by either uniformly or non-uniformly sub-sampling the components of the objective. Based upon such sampling strategy, we develop accelerated adaptive cubic regularization approaches and provide theoretical guarantees on global iteration complexity of $\\O(\\epsilon^{-1/3})$ with high probability, which matches that of the original accelerated cubic regularization methods Jiang et al. (2020) using the full Hessian information. Interestingly, we also show that in the worst case scenario our algorithm still achieves an $O(\\epsilon^{-5/6}\\log(\\epsilon^{-1}))$ iteration complexity bound. The proof techniques are new to our knowledge and can be of independent interets. Experimental results on the regularized logistic regression problems demonstrate a clear effect of acceleration on several real data sets",
    "volume": "main",
    "checked": true,
    "id": "79143faaa8ca9713773e90e2f988018c51850edf",
    "citation_count": 6
  },
  "https://jmlr.org/papers/v23/20-940.html": {
    "title": "When Hardness of Approximation Meets Hardness of Learning",
    "abstract": "A supervised learning algorithm has access to a distribution of labeled examples, and needs to return a function (hypothesis) that correctly labels the examples. The hypothesis of the learner is taken from some fixed class of functions (e.g., linear classifiers, neural networks etc.). A failure of the learning algorithm can occur due to two possible reasons: wrong choice of hypothesis class (hardness of approximation), or failure to find the best function within the hypothesis class (hardness of learning). Although both approximation and learnability are important for the success of the algorithm, they are typically studied separately. In this work, we show a single hardness property that implies both hardness of approximation using linear classes and shallow networks, and hardness of learning using correlation queries and gradient-descent. This allows us to obtain new results on hardness of approximation and learnability of parity functions, DNF formulas and $AC^0$ circuits",
    "volume": "main",
    "checked": true,
    "id": "56eddf3905efdf7503225b96190d3d77f6582fb8",
    "citation_count": 8
  },
  "https://jmlr.org/papers/v23/21-0030.html": {
    "title": "Gauss-Legendre Features for Gaussian Process Regression",
    "abstract": "Gaussian processes provide a powerful probabilistic kernel learning framework, which allows learning high quality nonparametric regression models via methods such as Gaussian process regression. Nevertheless, the learning phase of Gaussian process regression requires massive computations which are not realistic for large datasets. In this paper, we present a Gauss-Legendre quadrature based approach for scaling up Gaussian process regression via a low rank approximation of the kernel matrix. We utilize the structure of the low rank approximation to achieve effective hyperparameter learning, training and prediction. Our method is very much inspired by the well-known random Fourier features approach, which also builds low-rank approximations via numerical integration. However, our method is capable of generating high quality approximation to the kernel using an amount of features which is poly-logarithmic in the number of training points, while similar guarantees will require an amount that is at the very least linear in the number of training points when using random Fourier features. Furthermore, the structure of the low-rank approximation that our method builds is subtly different from the one generated by random Fourier features, and this enables much more efficient hyperparameter learning. The utility of our method for learning with low-dimensional datasets is demonstrated using numerical experiments",
    "volume": "main",
    "checked": true,
    "id": "f93a12884c60d46d906f447a7c8cff64e98de1b6",
    "citation_count": 3
  },
  "https://jmlr.org/papers/v23/21-0052.html": {
    "title": "Regularized K-means Through Hard-Thresholding",
    "abstract": "We study a framework for performing regularized K-means, based on direct  penalization of the size of the cluster centers. Different penalization strategies are considered and compared in a theoretical analysis and an extensive Monte Carlo simulation study. Based on the results, we propose a new method called hard-threshold K-means (HTK-means), which uses an â0 penalty to induce sparsity. HTK-means is a fast and competitive sparse clustering method which is easily interpretable, as is illustrated on several real data examples. In this context, new graphical displays are presented and used to gain further insight into the data sets",
    "volume": "main",
    "checked": true,
    "id": "b1fd54c0397d2d7eb3d41f5273e766106fc30fb2",
    "citation_count": 0
  },
  "https://jmlr.org/papers/v23/21-0054.html": {
    "title": "Multiple Testing in Nonparametric Hidden Markov Models: An Empirical Bayes Approach",
    "abstract": "Given a nonparametric Hidden Markov Model (HMM) with two states, the question of constructing efficient multiple testing procedures is considered, treating the states as unknown null and alternative hypotheses. A procedure is introduced, based on nonparametric empirical Bayes ideas, that controls the False Discovery Rate (FDR) at a user-specified level. Guarantees on power are also provided, in the form of a control of the true positive rate. One of the key steps in the construction requires supremum-norm convergence of preliminary estimators of the emission densities of the HMM. We provide the existence of such estimators, with convergence at the optimal minimax rate, for the case of a HMM with $J\\ge 2$ states, which is of independent interest",
    "volume": "main",
    "checked": true,
    "id": "1ca6f8b9b31fe6c84e8c2aa469cc92d667727ee3",
    "citation_count": 7
  },
  "https://jmlr.org/papers/v23/21-0055.html": {
    "title": "Attraction-Repulsion Spectrum in Neighbor Embeddings",
    "abstract": "Neighbor embeddings are a family of methods for visualizing complex high-dimensional data sets using kNN graphs. To find the low-dimensional embedding, these algorithms combine an attractive force between neighboring pairs of points with a repulsive force between all points. One of the most popular examples of such algorithms is t-SNE. Here we empirically show that changing the balance between the attractive and the repulsive forces in t-SNE using the exaggeration parameter yields a spectrum of embeddings, which is characterized by a simple trade-off: stronger attraction can better represent continuous manifold structures, while stronger repulsion can better represent discrete cluster structures and yields higher kNN recall. We find that UMAP embeddings correspond to t-SNE with increased attraction; mathematical analysis shows that this is because the negative sampling optimization strategy employed by UMAP strongly lowers the effective repulsion. Likewise, ForceAtlas2, commonly used for visualizing developmental single-cell transcriptomic data, yields embeddings corresponding to t-SNE with the attraction increased even more. At the extreme of this spectrum lie Laplacian eigenmaps. Our results demonstrate that many prominent neighbor embedding algorithms can be placed onto the attraction-repulsion spectrum, and highlight the inherent trade-offs between them",
    "volume": "main",
    "checked": true,
    "id": "19a818c3166ff924c39c7bbe283f57e5659179c8",
    "citation_count": 25
  },
  "https://jmlr.org/papers/v23/21-0082.html": {
    "title": "Rethinking Nonlinear Instrumental Variable Models through Prediction Validity",
    "abstract": "Instrumental variables (IV) are widely used in the social and health sciences in situations where a researcher would like to measure a causal effect but cannot perform an experiment. For valid causal inference in an IV model, there must be external (exogenous) variation that (i) has a sufficiently large impact on the variable of interest (called the relevance assumption) and where (ii) the only pathway through which the external variation impacts the outcome is via the variable of interest (called the exclusion restriction).  For statistical inference, researchers must also make assumptions about the functional form of the relationship between the three variables. Current practice assumes (i) and (ii) are met, then postulates a functional form with limited input from the data. In this paper, we describe a framework that leverages machine learning to validate these typically unchecked but consequential assumptions in the IV framework, providing the researcher empirical evidence about the quality of the instrument given the data at hand. Central to the proposed approach is the idea of prediction validity. Prediction validity checks that error terms -- which should be independent from the instrument -- cannot be modeled with machine learning any better than a model that is identically zero. We use prediction validity to develop both one-stage and two-stage approaches for IV, and demonstrate their performance on an example relevant to climate change policy",
    "volume": "main",
    "checked": true,
    "id": "25a10e0f53d146accafa3322c0bc55f0c6e200f0",
    "citation_count": 2
  },
  "https://jmlr.org/papers/v23/21-0084.html": {
    "title": "Unlabeled Data Help in Graph-Based Semi-Supervised Learning: A Bayesian Nonparametrics Perspective",
    "abstract": "In this paper we analyze the graph-based approach to semi-supervised learning under a manifold assumption. We adopt a Bayesian perspective and demonstrate that, for a suitable choice of prior constructed with sufficiently many unlabeled data, the posterior contracts around the truth at a rate that is minimax optimal up to a logarithmic factor. Our theory covers both regression and classification",
    "volume": "main",
    "checked": true,
    "id": "195949a892b4a3551cbbddf606c2025dd46eeaea",
    "citation_count": 4
  },
  "https://jmlr.org/papers/v23/21-0085.html": {
    "title": "PECOS: Prediction for Enormous and Correlated Output Spaces",
    "abstract": "Many large-scale applications amount to finding relevant results from an enormous output space of potential candidates. For example, finding the best matching product from a large catalog or suggesting related search phrases on a search engine. The size of the output space for these problems can range from millions to billions, and can even be infinite in some applications. Moreover, training data is often limited for the âlong-tailâ items in the output space. Fortunately, items in the output space are often correlated thereby presenting an opportunity to alleviate the data sparsity issue. In this paper, we propose the Prediction for Enormous and Correlated Output Spaces (PECOS) framework, a versatile and modular machine learning framework for solving prediction problems for very large output spaces, and apply it to the eXtreme Multilabel Ranking (XMR) problem: given an input instance, find and rank the most relevant items from an enormous but fixed and finite output space. We propose a three phase framework for PECOS: (i) in the first phase, PECOS organizes the output space using a semantic indexing scheme, (ii) in the second phase, PECOS uses the indexing to narrow down the output space by orders of magnitude using a machine learned matching scheme, and (iii) in the third phase, PECOS ranks the matched items using a final ranking scheme. The versatility and modularity of PECOS allows for easy plug-and-play of various choices for the indexing, matching, and ranking phases. The indexing and matching phases alleviate the data sparsity issue by leveraging correlations across different items in the output space. For the critical matching phase, we develop a recursive machine learned matching strategy with both linear and neural matchers. When applied to eXtreme Multilabel Ranking where the input instances are in textual form, we find that the recursive Transformer matcher gives state-of-the-art accuracy results, at the cost of two orders of magnitude increased training time compared to the recursive linear matcher. For example, on a dataset where the output space is of size 2.8 million, the recursive Transformer matcher results in a 6% increase in precision@1 (from 48.6% to 54.2%) over the recursive linear matcher but takes 100x more time to train. Thus it is up to the practitioner to evaluate the trade-offs and decide whether the increased training time and infrastructure cost is warranted for their application; indeed, the flexibility of the PECOS framework seamlessly allows different strategies to be used. We also develop very fast inference procedures which allow us to perform XMR predictions in real time; for example, inference takes less than 1 millisecond per input on the dataset with 2.8 million labels. The PECOS software is available at https://libpecos.org",
    "volume": "main",
    "checked": true,
    "id": "3402d9f64094b8a2b3541fd22ecee1229889564f",
    "citation_count": 40
  },
  "https://jmlr.org/papers/v23/21-0093.html": {
    "title": "Distributed Learning of Finite Gaussian Mixtures",
    "abstract": "Advances in information technology have led to extremely large datasets that are often kept in different storage centers. Existing statistical methods must be adapted to overcome the resulting computational obstacles while retaining statistical validity and efficiency. In this situation, the split-and-conquer strategy is among the most effective solutions to many statistical problems, including quantile processes, regression analysis, principal eigenspaces, and exponential families. This paper applies this strategy to develop a distributed learning procedure of finite Gaussian mixtures. We recommend a reduction strategy and invent an effective majorization-minimization algorithm. The new estimator is consistent and retains root-n consistency under some general conditions. Experiments based on simulated and real-world datasets show that the proposed estimator has comparable statistical performance with the global estimator based on the full dataset, if the latter is feasible. It can even outperform the global estimator for the purpose of clustering if the model assumption does not fully match the real-world data. It also has better statistical and computational performance than some existing split-and-conquer approaches",
    "volume": "main",
    "checked": true,
    "id": "184f0b01395bfd2492a5b17534a6a75a525c9376",
    "citation_count": 2
  },
  "https://jmlr.org/papers/v23/21-0129.html": {
    "title": "Total Stability of SVMs and Localized SVMs",
    "abstract": "Regularized kernel-based methods such as support vector machines (SVMs) typically depend on the underlying probability measure $\\mathrm{P}$ (respectively an empirical measure $\\mathrm{D}_n$ in applications) as well as on the regularization parameter $\\lambda$ and the kernel $k$. Whereas classical statistical robustness only considers the effect of small perturbations in $\\mathrm{P}$, the present paper investigates the influence of simultaneous slight variations in the whole triple $(\\mathrm{P},\\lambda,k)$, respectively $(\\mathrm{D}_n,\\lambda_n,k)$, on the resulting predictor. Existing results from the literature are considerably generalized and improved. In order to also make them applicable to big data, where regular SVMs suffer from their super-linear computational requirements, we show how our results can be transferred to the context of localized learning. Here, the effect of slight variations in the applied regionalization, which might for example stem from changes in $\\mathrm{P}$ respectively $\\mathrm{D}_n$, is considered as well",
    "volume": "main",
    "checked": true,
    "id": "f9b7e94d3d944468b60a65b750cc048066a78e1b",
    "citation_count": 2
  },
  "https://jmlr.org/papers/v23/21-0133.html": {
    "title": "Towards An Efficient Approach for the Nonconvex lp Ball Projection: Algorithm and Analysis",
    "abstract": "This paper primarily focuses on computing the Euclidean projection of a vector onto the lp ball in which p â (0,1). Such a problem emerges as the core building block in statistical machine learning and signal processing tasks because of its ability to promote the sparsity of the desired solution. However, efficient numerical algorithms for finding the projections are still not available, particularly in large-scale optimization. To meet this challenge, we first derive the first-order necessary optimality conditions of this problem. Based on this characterization, we develop a novel numerical approach for computing the stationary point by solving a sequence of projections onto the reweighted l1-balls. This method is practically simple to implement and computationally efficient. Moreover, the proposed algorithm is shown to converge uniquely under mild conditions and has a worst-case O(1/\\sqrt{k}) convergence rate. Numerical experiments demonstrate the efficiency of our proposed algorithm",
    "volume": "main",
    "checked": true,
    "id": "1ba11b6722e213ea733f217f34a70a5adfce9469",
    "citation_count": 6
  },
  "https://jmlr.org/papers/v23/21-0175.html": {
    "title": "Sufficient reductions in regression with mixed predictors",
    "abstract": "Most data sets comprise of measurements on continuous and categorical variables. Yet, modeling high-dimensional mixed predictors has received limited attention in the regression and classification statistical literature. We study the general regression problem of inferring on a variable of interest based on high dimensional mixed continuous and binary predictors. The aim is to find a lower dimensional function of the mixed predictor vector that contains all the modeling information in the mixed predictors for the response, which can be either continuous or categorical. The approach we propose identifies sufficient reductions by reversing the regression and modeling the mixed predictors conditional on the response. We derive the maximum likelihood estimator of the sufficient reductions, asymptotic tests for dimension, and a regularized estimator, which simultaneously achieves variable (feature) selection and dimension reduction (feature extraction). We study the performance of the proposed method and compare it with other approaches through simulations and  real data examples",
    "volume": "main",
    "checked": true,
    "id": "27b624ad94df4f209fa2bba72e783c5f4cec97c7",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v23/21-0186.html": {
    "title": "The EM Algorithm is Adaptively-Optimal for Unbalanced Symmetric Gaussian Mixtures",
    "abstract": "This paper studies the problem of estimating the means $\\pm\\theta_{*}\\in\\mathbb{R}^{d}$ of a symmetric two-component Gaussian mixture $\\delta_{*}\\cdot N(\\theta_{*},I)+(1-\\delta_{*})\\cdot N(-\\theta_{*},I)$, where the weights $\\delta_{*}$ and $1-\\delta_{*}$ are unequal. Assuming that $\\delta_{*}$ is known, we show that the population version of the EM algorithm globally converges if the initial estimate has non-negative inner product with the mean of the larger weight component. This can be achieved by the trivial initialization $\\theta_{0}=0$. For the empirical iteration based on $n$ samples, we show that when initialized at $\\theta_{0}=0$, the EM algorithm adaptively achieves the minimax error rate $\\tilde{O}\\Big(\\min\\Big\\{\\frac{1}{(1-2\\delta_{*})}\\sqrt{\\frac{d}{n}},\\frac{1}{\\|\\theta_{*}\\|}\\sqrt{\\frac{d}{n}},\\left(\\frac{d}{n}\\right)^{1/4}\\Big\\}\\Big)$ in no more than $O\\Big(\\frac{1}{\\|\\theta_{*}\\|(1-2\\delta_{*})}\\Big)$ iterations (with high probability). We also consider the EM iteration for estimating the weight $\\delta_{*}$, assuming a fixed mean $\\theta$ (which is possibly mismatched to $\\theta_{*}$). For the empirical iteration of $n$ samples, we show that the minimax error rate $\\tilde{O}\\Big(\\frac{1}{\\|\\theta_{*}\\|}\\sqrt{\\frac{d}{n}}\\Big)$ is achieved in no more than $O\\Big(\\frac{1}{\\|\\theta_{*}\\|^{2}}\\Big)$ iterations. These results robustify and complement recent results of Wu and Zhou (2019) obtained for the equal weights case $\\delta_{*}=1/2$",
    "volume": "main",
    "checked": true,
    "id": "7b8b17859f295d0cf501dc154ad40fcd2644e68b",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v23/21-023.html": {
    "title": "Efficient Least Squares for Estimating Total Effects under Linearity and Causal Sufficiency",
    "abstract": "Recursive linear structural equation models are widely used to postulate causal mechanisms underlying observational data. In these models, each variable equals a linear combination of a subset of the remaining variables plus an error term. When there is no unobserved confounding or selection bias, the error terms are assumed to be independent. We consider estimating a total causal effect in this setting. The causal structure is assumed to be known only up to a maximally oriented partially directed acyclic graph (MPDAG), a general class of graphs that can represent a Markov equivalence class of directed acyclic graphs (DAGs) with added background knowledge. We propose a simple estimator based on recursive least squares, which can consistently estimate any identified total causal effect, under point or joint intervention. We show that this estimator is the most efficient among all regular estimators that are based on the sample covariance, which includes covariate adjustment and the estimators employed by the joint-IDA algorithm. Notably, our result holds without assuming Gaussian errors",
    "volume": "main",
    "checked": true,
    "id": "3c1faddf612311c5ddedc26f03275581bcb2c0ac",
    "citation_count": 12
  },
  "https://jmlr.org/papers/v23/21-0282.html": {
    "title": "Globally Injective ReLU Networks",
    "abstract": "Injectivity plays an important role in generative models where it enables inference; in inverse problems and compressed sensing with generative priors it is a precursor to well posedness. We establish sharp characterizations of injectivity of fully-connected and convolutional ReLU layers and networks. First, through a layerwise analysis, we show that an expansivity factor of two is necessary and sufficient for injectivity by constructing appropriate weight matrices. We show that global injectivity with iid Gaussian matrices, a commonly used tractable model, requires larger expansivity between 3.4 and 10.5. We also characterize the stability of inverting an injective network via worst-case Lipschitz constants of the inverse. We then use arguments from differential topology to study injectivity of deep networks and prove that any Lipschitz map can be approximated by an injective ReLU network. Finally, using an argument based on random projections, we show that an end-to-end---rather than layerwise---doubling of the dimension suffices for injectivity. Our results establish a theoretical basis for the study of nonlinear inverse and inference problems using neural networks",
    "volume": "main",
    "checked": true,
    "id": "3cf0ee01456fc99c80c2bdaaa191d7a74d3968fa",
    "citation_count": 14
  },
  "https://jmlr.org/papers/v23/21-0314.html": {
    "title": "Riemannian Stochastic Proximal Gradient Methods for Nonsmooth Optimization over the Stiefel Manifold",
    "abstract": "Riemannian optimization has drawn a lot of attention due to its wide applications in practice. Riemannian stochastic first-order algorithms have been studied in the literature to solve large-scale machine learning problems over Riemannian manifolds. However, most of the existing Riemannian stochastic algorithms require the objective function to be differentiable, and they do not apply to the case where the objective function is nonsmooth. In this paper, we present two Riemannian stochastic proximal gradient methods for minimizing nonsmooth function over the Stiefel manifold. The two methods, named R-ProxSGD and R-ProxSPB, are generalizations of proximal SGD and proximal SpiderBoost in Euclidean setting to the Riemannian setting. Analysis on the incremental first-order oracle (IFO) complexity of the proposed algorithms is provided. Specifically, the R-ProxSPB algorithm finds an $\\epsilon$-stationary point with $O(\\epsilon^{-3})$ IFOs in the online case, and $O(n+\\sqrt{n}\\epsilon^{-2})$ IFOs in the finite-sum case with $n$ being the number of summands in the objective. Experimental results on online sparse PCA and robust low-rank matrix completion show that our proposed methods significantly outperform the existing methods that use Riemannian subgradient information",
    "volume": "main",
    "checked": true,
    "id": "66508a022305246bc7a160b0ec9693de1adfe418",
    "citation_count": 14
  },
  "https://jmlr.org/papers/v23/21-0387.html": {
    "title": "IALE: Imitating Active Learner Ensembles",
    "abstract": "Active learning prioritizes the labeling of the most informative data samples. However, the performance of active learning heuristics depends on both the structure of the underlying model architecture and the data. We propose IALE, an imitation learning scheme that imitates the selection of the best-performing expert heuristic at each stage of the learning cycle in a batch-mode pool-based setting. We use Dagger to train a transferable policy on a dataset and later apply it to different datasets and deep classifier architectures. The policy reflects on the best choices from multiple expert heuristics given the current state of the active learning process, and learns to select samples in a complementary way that unifies the expert strategies. Our experiments on well-known image datasets show that we outperform state of the art imitation learners and heuristics",
    "volume": "main",
    "checked": true,
    "id": "11c1a0319b517074895a18e8abbedc11cb700459",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v23/21-0403.html": {
    "title": "Bayesian subset selection and variable importance for interpretable prediction and classification",
    "abstract": "Subset selection is a valuable tool for interpretable learning, scientific discovery, and data compression. However, classical subset selection is often avoided due to selection instability, lack of regularization, and difficulties with post-selection inference. We address these challenges from a Bayesian perspective. Given any Bayesian predictive model M, we extract a family of near-optimal subsets of variables for linear prediction or classification. This strategy deemphasizes the role of a single âbestâ subset and instead advances the broader perspective that often many subsets are highly competitive. The acceptable family of subsets offers a new pathway for model interpretation and is neatly summarized by key members such as the smallest acceptable subset, along with new (co-) variable importance metrics based on whether variables (co-) appear in all, some, or no acceptable subsets. More broadly, we apply Bayesian decision analysis to derive the optimal linear coefficients for any subset of variables. These coefficients inherit both regularization and predictive uncertainty quantification via M. For both simulated and real data, the proposed approach exhibits better prediction, interval estimation, and variable selection than competing Bayesian and frequentist selection methods. These tools are applied to a large education dataset with highly correlated covariates. Our analysis provides unique insights into the combination of environmental, socioeconomic, and demographic factors that predict educational outcomes, and identifies over 200 distinct subsets of variables that offer near-optimal out-of-sample predictive accuracy",
    "volume": "main",
    "checked": true,
    "id": "a531b219cdcb404e1a99e6c66db9c36f054287e4",
    "citation_count": 4
  },
  "https://jmlr.org/papers/v23/21-0425.html": {
    "title": "Conditions and Assumptions for Constraint-based Causal Structure Learning",
    "abstract": "We formalize constraint-based structure learning of the \"true\" causal graph from observed data when unobserved variables are also existent. We provide conditions for a \"natural\" family of constraint-based structure-learning algorithms that output graphs that are Markov equivalent to the causal graph. Under the faithfulness assumption, this natural family contains all exact structure-learning algorithms. We also provide a set of assumptions, under which any natural structure-learning algorithm outputs Markov equivalent graphs to the causal graph. These assumptions can be thought of as a relaxation of faithfulness, and most of them can be directly tested from (the underlying distribution) of the data, particularly when one focuses on structural causal models. We specialize the definitions and results for structural causal models",
    "volume": "main",
    "checked": true,
    "id": "109bbe6bb5f3021c2c1449da5831b87728830982",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v23/21-0511.html": {
    "title": "EiGLasso for Scalable Sparse Kronecker-Sum Inverse Covariance Estimation",
    "abstract": "In many real-world data, complex dependencies are present both among samples and among features. The Kronecker sum or the Cartesian product of two graphs, each modeling dependencies across features and across samples, has been used as an inverse covariance matrix for a matrix-variate Gaussian distribution as an alternative to Kronecker-product inverse covariance matrix due to its more intuitive sparse structure. However, the existing methods for sparse Kronecker-sum inverse covariance estimation are limited in that they do not scale to more than a few hundred features and samples and that unidentifiable parameters pose challenges in estimation. In this paper, we introduce EiGLasso, a highly scalable method for sparse Kronecker-sum inverse covariance estimation, based on Newton's method combined with eigendecomposition of the sample and feature graphs to exploit the Kronecker-sum structure. EiGLasso further reduces computation time by approximating the Hessian matrix, based on the eigendecomposition of the two graphs. EiGLasso achieves quadratic convergence with the exact Hessian and linear convergence with the approximate Hessian. We describe a simple new approach to estimating the unidentifiable parameters that generalizes the existing methods. On simulated and real-world data, we demonstrate that EiGLasso achieves two to three orders-of-magnitude speed-up, compared to the existing methods",
    "volume": "main",
    "checked": true,
    "id": "f781941b595bcb19aff7fd932f3c59d45dd265ad",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v23/21-0542.html": {
    "title": "Advantage of Deep Neural Networks for Estimating Functions with Singularity on Hypersurfaces",
    "abstract": "We develop a minimax rate analysis to describe the reason that deep neural networks (DNNs) perform better than other standard methods. For nonparametric regression problems, it is well known that many standard methods attain the minimax optimal rate of estimation errors for smooth functions, and thus, it is not straightforward to identify the theoretical advantages of DNNs. This study tries to fill this gap by considering the estimation for a class of non-smooth functions that have singularities on hypersurfaces. Our findings are as follows: (i) We derive the generalization error of a DNN estimator and prove that its convergence rate is almost optimal. (ii) We elucidate a phase diagram of estimation problems, which describes the situations where the DNNs outperform a general class of estimators, including kernel methods, Gaussian process methods, and others. We additionally show that DNNs outperform harmonic analysis based estimators. This advantage of DNNs comes from the fact that a shape of singularity can be successfully handled by their multi-layered structure",
    "volume": "main",
    "checked": true,
    "id": "d5a248f3795113923ef0b004dd490d584d8256b2",
    "citation_count": 8
  },
  "https://jmlr.org/papers/v23/21-0622.html": {
    "title": "Sum of Ranked Range Loss for Supervised Learning",
    "abstract": "In forming learning objectives, one oftentimes needs to aggregate a set of individual values to a single output. Such cases occur in the aggregate loss, which  combines individual losses of a learning model over each training sample, and in the individual loss for multi-label learning, which combines prediction scores over all class labels. In this work, we introduce the sum of ranked range (SoRR) as a general approach to form learning objectives. A ranked range is a consecutive sequence of sorted values of a set of real numbers. The minimization of SoRR is solved with the difference of convex algorithm (DCA). We explore two applications in machine learning of the minimization of the SoRR framework, namely the AoRR aggregate loss for binary/multi-class classification at the sample level and the TKML individual loss for multi-label/multi-class classification at the label level. A combination loss of AoRR and TKML is proposed as a new learning objective for improving the robustness of multi-label learning in the face of outliers in sample and labels alike. Our empirical results highlight the effectiveness of the proposed optimization frameworks and demonstrate the applicability of proposed losses using synthetic and real data sets",
    "volume": "main",
    "checked": true,
    "id": "7a8dc2c45668f6c23e313c7fa80c5a0c97e943df",
    "citation_count": 7
  },
  "https://jmlr.org/papers/v23/21-0630.html": {
    "title": "The Two-Sided Game of Googol",
    "abstract": "The secretary problem or game of Googol are classic models for online selection problems. In this paper we consider a variant of the problem and explore its connections to data-driven online selection. Specifically, we are given $n$ cards with arbitrary non-negative numbers written on both sides. The cards are randomly placed on $n$ consecutive positions on a table, and for each card, the visible side is also selected at random. The player sees the visible side of all cards and wants to select the card with the maximum hidden value. To this end, the player flips the first card, sees its hidden value and decides whether to pick it or drop it and continue with the next card. We study algorithms for two natural objectives: maximizing the probability of selecting the maximum hidden value, and maximizing the expectation of the selected hidden value. For the former objective we obtain a simple $0.45292$-competitive algorithm. For the latter, we obtain a $0.63518$-competitive algorithm. Our main contribution is to set up a model allowing to transform probabilistic optimal stopping problems into purely combinatorial ones. For instance, we can apply our results to obtain lower bounds for the single sample prophet secretary problem",
    "volume": "main",
    "checked": true,
    "id": "384a9ed1cbb5fa095af37a450a3bfc0c47d0ceb8",
    "citation_count": 0
  },
  "https://jmlr.org/papers/v23/21-0631.html": {
    "title": "ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction",
    "abstract": "This work attempts to provide a plausible theoretical framework that aims to interpret modern deep (convolutional) networks from the principles of data compression and discriminative representation. We argue that for high-dimensional multi-class data, the optimal linear discriminative representation maximizes the coding rate difference between the whole dataset and the average of all the subsets. We show that the basic iterative gradient ascent scheme for optimizing the rate reduction objective naturally leads to a multi-layer deep network, named ReduNet, which shares common characteristics of modern deep networks. The deep layered architectures, linear and nonlinear operators, and even parameters of the network are all explicitly constructed layer-by-layer via forward propagation, although they are amenable to fine-tuning via back propagation. All components of so-obtained âwhite-boxâ network have precise optimization, statistical, and geometric interpretation. Moreover, all linear operators of the so-derived network naturally become multi-channel convolutions when we enforce classification to be rigorously shift-invariant. The derivation in the invariant setting suggests a trade-off between sparsity and invariance, and also indicates that such a deep convolution network is significantly more efficient to construct and learn in the spectral domain. Our preliminary simulations and experiments clearly verify the effectiveness of both the rate reduction objective and the associated ReduNet. All code and data are available at https://github.com/Ma-Lab-Berkeley",
    "volume": "main",
    "checked": true,
    "id": "939b502c7ddfc60ac63ea20bddca0ae54004f2fb",
    "citation_count": 53
  },
  "https://jmlr.org/papers/v23/21-0681.html": {
    "title": "CauchyâSchwarz Regularized Autoencoder",
    "abstract": "Recent work in unsupervised learning has focused on efficient inference and learning in latent variables models. Training these models by maximizing the evidence (marginal likelihood) is typically intractable. Thus, a common approximation is to maximize the Evidence Lower BOund (ELBO) instead. Variational autoencoders (VAE) are a powerful and widely-used class of generative models that optimize the ELBO efficiently for large datasets. However, the VAE's default Gaussian choice for the prior imposes a strong constraint on its ability to represent the true posterior, thereby degrading overall performance. A Gaussian mixture model (GMM) would be a richer prior but cannot be handled efficiently within the VAE framework because of the intractability of the Kullback-Leibler divergence for GMMs. We deviate from the common VAE framework in favor of one with an analytical solution for Gaussian mixture prior. To perform efficient inference for GMM priors, we introduce a new constrained objective based on the Cauchy-Schwarz divergence, which can be computed analytically for GMMs. This new objective allows us to incorporate richer, multi-modal priors into the autoencoding framework. We provide empirical studies on a range of datasets and show that our objective improves upon variational auto-encoding models in density estimation, unsupervised clustering, semi-supervised learning, and face analysis",
    "volume": "main",
    "checked": false,
    "id": "220be271b6e3c2c8c57b47222f79370f4fc0e17e",
    "citation_count": 8
  },
  "https://jmlr.org/papers/v23/21-0732.html": {
    "title": "An Error Analysis of Generative Adversarial Networks for Learning Distributions",
    "abstract": "This paper studies how well generative adversarial networks (GANs) learn probability distributions from finite samples. Our main results establish the convergence rates of GANs under a collection of integral probability metrics defined through H\\\"{o}lder classes, including the Wasserstein distance as a special case. We also show that GANs are able to adaptively learn data distributions with low-dimensional structures or have H\\\"{o}lder densities, when the network architectures are chosen properly. In particular, for distributions concentrated around a low-dimensional set, we show that the learning rates of GANs do not depend on the high ambient dimension, but on the lower intrinsic dimension. Our analysis is based on a new oracle inequality decomposing the estimation error into the generator and discriminator approximation error and the statistical error, which may be of independent interest",
    "volume": "main",
    "checked": true,
    "id": "9d36d9897d8d528b278afd1db7efeef5aa259e18",
    "citation_count": 11
  },
  "https://jmlr.org/papers/v23/21-0847.html": {
    "title": "OVERT: An Algorithm for Safety Verification of Neural Network Control Policies for Nonlinear Systems",
    "abstract": "Deep learning methods can be used to produce control policies, but certifying their safety is challenging. The resulting networks are nonlinear and often very large. In response to this challenge, we present OVERT: a sound algorithm for safety verification of nonlinear discrete-time closed loop dynamical systems with neural network control policies. The novelty of OVERT lies in combining ideas from the classical formal methods literature with ideas from the newer neural network verification literature.  The central concept of OVERT is to abstract nonlinear functions with a set of optimally tight piecewise linear bounds. Such piecewise linear bounds are designed for seamless integration into ReLU neural network verification tools.  OVERT can be used to prove bounded-time safety properties by either computing reachable sets or solving feasibility queries directly.  We demonstrate various examples of safety verification for several classical benchmark examples.  OVERT compares favorably to existing methods both in computation time and in tightness of the reachable set",
    "volume": "main",
    "checked": true,
    "id": "c1cb6f0f4d271cb46d8b8f9d65bd9aade0e4265c",
    "citation_count": 26
  },
  "https://jmlr.org/papers/v23/21-0904.html": {
    "title": "Under-bagging Nearest Neighbors for Imbalanced Classification",
    "abstract": "In this paper, we propose an ensemble learning algorithm called under-bagging $k$-nearest neighbors (under-bagging $k$-NN) for imbalanced classification problems. On the theoretical side, by developing a new learning theory analysis, we show that with properly chosen parameters, i.e., the number of nearest neighbors $k$, the expected sub-sample size $s$, and the bagging rounds $B$, optimal convergence rates for under-bagging $k$-NN can be achieved under mild assumptions w.r.t. the arithmetic mean (AM) of recalls. Moreover, we show that with a relatively small $B$, the expected sub-sample size $s$ can be much smaller than the number of training data $n$ at each bagging round, and the number of nearest neighbors $k$ can be reduced simultaneously, especially when the data are highly imbalanced, which leads to substantially lower time complexity and roughly the same space complexity. On the practical side, we conduct numerical experiments to verify the theoretical results on the benefits of the under-bagging technique by the promising AM performance and efficiency of our proposed algorithm",
    "volume": "main",
    "checked": true,
    "id": "aac7ede3f3acb446f2f43dc31a7b2a694e4a7320",
    "citation_count": 5
  },
  "https://jmlr.org/papers/v23/21-092.html": {
    "title": "A spectral-based analysis of the separation between two-layer neural networks and linear methods",
    "abstract": "We propose a spectral-based approach to analyze how two-layer neural networks separate from linear methods in terms of approximating high-dimensional functions. We show that quantifying this separation can be reduced to estimating the Kolmogorov width of two-layer neural networks, and the latter can be further characterized by using the spectrum of an associated kernel. Different from previous work, our approach allows obtaining upper bounds, lower bounds, and identifying explicit hard functions in a united manner. We provide a systematic study of how the choice of activation functions affects the separation, in particular the  dependence on the input dimension. Specifically, for nonsmooth activation functions, we extend known results to more activation functions with sharper bounds. As concrete examples, we prove that any single neuron can  instantiate the separation between neural networks and random feature models. For smooth activation functions, one surprising finding is that the separation is negligible unless the norms of inner-layer weights are polynomially large  with respect to the input dimension. By contrast, the separation for nonsmooth activation functions is independent of the norms of inner-layer weights",
    "volume": "main",
    "checked": true,
    "id": "4b3b143ff31b676a9826a7395cdacc924d5c2e9d",
    "citation_count": 6
  },
  "https://jmlr.org/papers/v23/21-0998.html": {
    "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
    "abstract": "In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) models defy this and instead select different parameters for each incoming example. The result is a sparsely-activated model---with an outrageous number of parameters---but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by  complexity, communication costs, and training instability. We address these with the introduction of the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques mitigate the instabilities, and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the \"Colossal Clean Crawled Corpus\", and achieve a 4x speedup over the T5-XXL model",
    "volume": "main",
    "checked": true,
    "id": "fdacf2a732f55befdc410ea927091cad3b791f13",
    "citation_count": 800
  },
  "https://jmlr.org/papers/v23/21-1027.html": {
    "title": "Online Mirror Descent and Dual Averaging: Keeping Pace in the Dynamic Case",
    "abstract": "Online mirror descent (OMD) and dual averaging (DA)---two fundamental algorithms for online convex optimization---are known to have very similar (and sometimes identical) performance guarantees when used with a fixed learning rate. Under dynamic learning rates, however, OMD is provably inferior to DA and suffers linear regret, even in common settings such as prediction with expert advice. We modify the OMD algorithm through a simple technique that we call stabilization. We give essentially the same abstract regret bound for OMD with stabilization and for DA by modifying the classical OMD convergence analysis in a careful and modular way that allows for straightforward and flexible proofs. Simple corollaries of these bounds show that OMD with stabilization and DA enjoy the same performance guarantees in many applications---even under dynamic learning rates. We also shed light on the similarities between OMD and DA and show simple conditions under which stabilized-OMD and DA generate the same iterates. Finally, we show how to effectively use dual-stabilization with composite cost functions with simple adaptations to both the algorithm and its analysis",
    "volume": "main",
    "checked": true,
    "id": "39bd78dad19c8d0ecc3dfa09bf8100223288640f",
    "citation_count": 15
  },
  "https://jmlr.org/papers/v23/21-1109.html": {
    "title": "Depth separation beyond radial functions",
    "abstract": "High-dimensional depth separation results for neural networks show that certain functions can be efficiently approximated by two-hidden-layer networks but not by one-hidden-layer ones in high-dimensions. Existing results of this type mainly focus on functions with an underlying radial or one-dimensional structure, which are usually not encountered in practice. The first contribution of this paper is to extend such results to a more general class of functions, namely functions with piece-wise oscillatory structure, by building on the proof strategy of (Eldan and Shamir, 2016). We complement these results by showing that, if the domain radius and the rate of oscillation of the objective function are constant, then approximation by one-hidden-layer networks holds at a $\\mathrm{poly}(d)$ rate for any fixed error threshold. The mentioned results show that one-hidden-layer networks fail to approximate high-energy functions whose Fourier representation is spread in the frequency domain, while they succeed at approximating functions having a sparse Fourier representation. However, the choice of the domain represents a source of gaps between these positive and negative approximation results. We conclude the paper focusing on a compact approximation domain, namely the sphere $\\S$ in dimension $d$, where we provide a characterization of both functions which are efficiently approximable by one-hidden-layer networks and of functions which are provably not, in terms of their Fourier expansion",
    "volume": "main",
    "checked": true,
    "id": "f466134ab745a35e6c98a77021806f7abbc96db2",
    "citation_count": 10
  },
  "https://jmlr.org/papers/v23/21-1138.html": {
    "title": "Provable Tensor-Train Format Tensor Completion by Riemannian Optimization",
    "abstract": "The tensor train (TT) format enjoys appealing advantages in handling structural high-order tensors. The recent decade has witnessed the wide applications of TT-format tensors from diverse disciplines, among which tensor completion has drawn considerable attention. Numerous fast algorithms, including the Riemannian gradient descent (RGrad),  have been proposed for the TT-format tensor completion. However, the theoretical guarantees of these algorithms are largely missing or sub-optimal, partly due to the complicated and recursive algebraic operations in TT-format decomposition. Moreover, existing results established for the tensors of other formats, for example, Tucker and CP, are inapplicable because the algorithms treating TT-format tensors are substantially different and more involved. In this paper, we provide, to our best knowledge, the first theoretical guarantees of the convergence of RGrad algorithm for TT-format tensor completion, under a nearly optimal sample size condition. The RGrad algorithm converges linearly with a constant contraction rate that is free of tensor condition number without the necessity of re-conditioning. We also propose a novel approach, referred to as the  sequential second-order moment method, to attain a warm initialization under a similar sample size requirement. As a byproduct, our result even significantly refines the prior investigation of RGrad algorithm for matrix completion.  Lastly,  statistically (near) optimal rate is derived for RGrad algorithm if the observed entries consist of random sub-Gaussian noise.  Numerical experiments confirm our theoretical discovery and showcase the computational speedup gained by the TT-format decomposition",
    "volume": "main",
    "checked": true,
    "id": "049e1d56499f84fc63398acd8583ef3298da8a54",
    "citation_count": 11
  },
  "https://jmlr.org/papers/v23/21-1177.html": {
    "title": "Darts: User-Friendly Modern Machine Learning for Time Series",
    "abstract": "We present Darts, a Python machine learning library for time series, with a focus on forecasting. Darts offers a variety of models, from classics such as ARIMA to state-of-the-art deep neural networks. The emphasis of the library is on offering modern machine learning functionalities, such as supporting multidimensional series, fitting models on multiple series, training on large datasets, incorporating external data, ensembling models, and providing a rich support for probabilistic forecasting. At the same time, great care goes into the API design to make it user-friendly and easy to use. For instance, all models can be used using fit()/predict(), similar to scikit-learn",
    "volume": "main",
    "checked": true,
    "id": "22733aac53e89446aed76dd1983bf2d74567ba88",
    "citation_count": 60
  },
  "https://jmlr.org/papers/v23/21-1199.html": {
    "title": "Foolish Crowds Support Benign Overfitting",
    "abstract": "We prove a lower bound on the excess risk of sparse interpolating procedures for linear regression with Gaussian data in the overparameterized regime. We apply this result to obtain a lower bound for basis pursuit (the minimum $\\ell_1$-norm interpolant) that implies that its excess risk can converge at an exponentially slower rate than OLS (the minimum $\\ell_2$-norm interpolant), even when the ground truth is sparse.  Our analysis exposes the benefit of an effect analogous to the âwisdom of the crowdâ, except here the harm arising from fitting the noise is ameliorated by spreading it among many directions---the variance reduction arises from a foolish crowd",
    "volume": "main",
    "checked": true,
    "id": "b0e189466e6c9860b334bad142a3489cf809617f",
    "citation_count": 11
  },
  "https://jmlr.org/papers/v23/21-1212.html": {
    "title": "Neural Estimation of Statistical Divergences",
    "abstract": "Statistical divergences (SDs), which quantify the dissimilarity between probability distributions, are a basic constituent of statistical inference and machine learning. A modern method for estimating those divergences relies on parametrizing an empirical variational form by a neural network (NN) and optimizing over parameter space. Such neural estimators are abundantly used in practice, but corresponding performance guarantees are partial and call for further exploration. We establish non-asymptotic absolute error bounds for a neural estimator realized by a shallow NN, focusing on four popular $\\mathsf{f}$-divergences---Kullback-Leibler, chi-squared, squared Hellinger, and total variation. Our analysis relies on non-asymptotic function approximation theorems and tools from empirical process theory to bound the two sources of error involved: function approximation and empirical estimation. The bounds characterize the effective error in terms of NN size and the number of samples, and reveal scaling rates that ensure consistency. For compactly supported distributions, we further show that neural estimators of the first three divergences above with appropriate NN growth-rate are minimax rate-optimal, achieving the parametric convergence rate",
    "volume": "main",
    "checked": true,
    "id": "1704554524b800b4a38b02925a97bd4d7e360282",
    "citation_count": 17
  },
  "https://jmlr.org/papers/v23/21-1232.html": {
    "title": "Kernel Packet: An Exact and Scalable Algorithm for Gaussian Process Regression with MatÃ©rn Correlations",
    "abstract": "We develop an exact and scalable algorithm for one-dimensional Gaussian process regression with MatÃ©rn correlations whose smoothness parameter $\\nu$ is a half-integer. The proposed algorithm only requires $\\mathcal{O}(\\nu^3 n)$ operations and $\\mathcal{O}(\\nu n)$ storage. This leads to a linear-cost solver since $\\nu$ is chosen to be fixed and usually very small in most applications. The proposed method can be applied to multi-dimensional problems if a full grid or a sparse grid design is used. The proposed method is based on a novel theory for MatÃ©rn correlation functions. We find that a suitable rearrangement of these correlation functions can produce a compactly supported function, called a \"kernel packet\". Using a set of kernel packets as basis functions leads to a sparse representation of the covariance matrix that results in the proposed algorithm. Simulation studies show that the proposed algorithm, when applicable, is significantly superior to the existing alternatives in both the computational time and predictive accuracy",
    "volume": "main",
    "checked": false,
    "id": "32816e98cd9ea45c7984727f99839fe0b79e6339",
    "citation_count": 4
  },
  "https://jmlr.org/papers/v23/21-1290.html": {
    "title": "Power Iteration for Tensor PCA",
    "abstract": "In this paper, we study the power iteration algorithm for the asymmetric spiked tensor model, as introduced in  Richard and Montanari (2014). We give necessary and sufficient conditions for the convergence of the power iteration algorithm. When the power iteration algorithm converges, for the rank one spiked tensor model, we show the estimators for the spike strength and linear functionals of the signal are asymptotically Gaussian; for the multi-rank spiked tensor model, we show the estimators are asymptotically mixtures of Gaussian. This new phenomenon is different from the spiked matrix model. Using these asymptotic results of our estimators, we construct valid and efficient confidence intervals for spike strengths and linear functionals of the signals",
    "volume": "main",
    "checked": true,
    "id": "c68722f86c8611286ffbc5d59bff58e656745b21",
    "citation_count": 8
  },
  "https://jmlr.org/papers/v23/21-1312.html": {
    "title": "On the Approximation of Cooperative Heterogeneous Multi-Agent Reinforcement Learning (MARL) using Mean Field Control (MFC)",
    "abstract": "Mean field control (MFC) is an effective way to mitigate the curse of dimensionality of cooperative multi-agent reinforcement learning (MARL) problems. This work considers a collection of $N_{\\mathrm{pop}}$  heterogeneous agents that can be segregated into $K$ classes such that the $k$-th class contains $N_k$ homogeneous agents. We  aim to prove approximation guarantees of the MARL problem for this heterogeneous system by its corresponding MFC problem.  We consider three  scenarios where the reward and transition dynamics of all agents are respectively taken to be functions of $(1)$ joint state and action distributions across all classes, $(2)$ individual distributions of each class, and $(3)$ marginal distributions of the entire population. We show that, in these cases, the $K$-class MARL problem can be approximated by MFC with errors given  as $e_1=\\mathcal{O}(\\frac{\\sqrt{|\\mathcal{X}|}+\\sqrt{|\\mathcal{U}|}}{N_{\\mathrm{pop}}}\\sum_{k}\\sqrt{N_k})$, $e_2=\\mathcal{O}(\\left[\\sqrt{|\\mathcal{X}|}+\\sqrt{|\\mathcal{U}|}\\right]\\sum_{k}\\frac{1}{\\sqrt{N_k}})$ and $e_3=\\mathcal{O}\\left(\\left[\\sqrt{|\\mathcal{X}|}+\\sqrt{|\\mathcal{U}|}\\right]\\left[\\frac{A}{N_{\\mathrm{pop}}}\\sum_{k\\in[K]}\\sqrt{N_k}+\\frac{B}{\\sqrt{N_{\\mathrm{pop}}}}\\right]\\right)$, respectively,  where $A, B$ are some constants and $|\\mathcal{X}|,|\\mathcal{U}|$ are the sizes of state and action spaces of each agent. Finally, we design a Natural Policy Gradient (NPG) based algorithm that, in the three cases stated above, can converge to an optimal MARL policy within $\\mathcal{O}(e_j)$ error with a sample complexity of $\\mathcal{O}(e_j^{-3})$, $j\\in\\{1,2,3\\}$, respectively",
    "volume": "main",
    "checked": true,
    "id": "115451ea78b73a0938676edbc55918d9b2acb1a0",
    "citation_count": 19
  },
  "https://jmlr.org/papers/v23/21-1365.html": {
    "title": "Mean-field Analysis of Piecewise Linear Solutions for Wide ReLU Networks",
    "abstract": "Understanding the properties of neural networks trained via stochastic gradient descent (SGD) is at the heart of the theory of deep learning. In this work, we take a mean-field view, and consider a two-layer ReLU network trained via noisy-SGD for a univariate regularized regression problem. Our main result is that SGD with vanishingly small noise injected in the gradients is biased towards a simple solution: at convergence, the ReLU network implements a piecewise linear map of the inputs, and the number of âknotâ points -- i.e., points where the tangent of the ReLU network estimator changes -- between two consecutive training inputs is at most three. In particular, as the number of neurons of the network grows, the SGD dynamics is captured by the solution of a gradient flow and, at convergence, the distribution of the weights approaches the unique minimizer of a related free energy, which has a Gibbs form. Our key technical contribution consists in the analysis of the estimator resulting from this minimizer: we show that its second derivative vanishes everywhere, except at some specific locations which represent  the âknotâ points. We also provide empirical evidence that knots at locations distinct from the data points might occur, as predicted by our theory",
    "volume": "main",
    "checked": true,
    "id": "35edf382754a600cc8f67232dc5ff017d22a0fc3",
    "citation_count": 7
  },
  "https://jmlr.org/papers/v23/18-045.html": {
    "title": "Let's Make Block Coordinate Descent Converge Faster: Faster Greedy Rules, Message-Passing, Active-Set Complexity, and Superlinear Convergence",
    "abstract": "Block coordinate descent (BCD) methods are widely used for large-scale numerical optimization because of their cheap iteration costs, low memory requirements, amenability to parallelization, and ability to exploit problem structure. Three main algorithmic choices influence the performance of BCD methods: the block partitioning strategy, the block selection rule, and the block update rule. In this paper we explore all three of these  building blocks and propose variations for each that can significantly improve the progress made by each BCD iteration. We (i) propose new greedy block-selection strategies that guarantee more progress per iteration than the Gauss-Southwell rule; (ii) explore practical issues like how to implement the new rules when using \"variable\" blocks; (iii)  explore the use of message-passing to  compute matrix or Newton updates efficiently on huge blocks for problems with sparse dependencies between variables; and (iv) consider optimal active manifold identification, which leads to bounds on the \"active-set complexity\" of BCD methods and leads to superlinear convergence for certain problems with sparse solutions (and in some cases finite termination at an optimal solution). We support all of our findings with numerical results for the classic machine learning problems of least squares, logistic regression, multi-class logistic regression, label propagation, and L1-regularization",
    "volume": "main",
    "checked": true,
    "id": "aac612fd5661eb405705b80ff39974e019b3e16b",
    "citation_count": 7
  },
  "https://jmlr.org/papers/v23/19-1047.html": {
    "title": "An Optimization-centric View on Bayes' Rule: Reviewing and Generalizing Variational Inference",
    "abstract": "We advocate an optimization-centric view of Bayesian inference. Our inspiration is the representation of Bayes' rule as infinite-dimensional optimization (Csiszar, 1975; Donsker and Varadhan, 1975; Zellner, 1988). Equipped with this perspective, we study Bayesian inference when one does not have access to (1) well-specified priors, (2) well-specified likelihoods, (3) infinite computing power. While these three assumptions underlie the standard Bayesian paradigm, they are typically inappropriate for modern Machine Learning applications. We propose addressing this through an optimization-centric generalization of Bayesian posteriors that we call the Rule of Three (RoT). The RoT can be justified axiomatically and recovers Bayesian, PAC-Bayesian and VI posteriors as special cases. While the RoT is primarily a conceptual and theoretical device, it also encompasses a novel sub-class of tractable posteriors which we call Generalized Variational Inference (GVI) posteriors. Just as the RoT, GVI posteriors are specified by three arguments: a loss, a divergence and a variational family. They also possess a number of desirable properties, including modularity, Frequentist consistency and an interpretation as approximate ELBO. We explore applications of GVI posteriors, and show that they can be used to improve robustness and posterior marginals on Bayesian Neural Networks and Deep Gaussian Processes",
    "volume": "main",
    "checked": true,
    "id": "1f6d82f06f77cc91705f7be5fc656e523a517da5",
    "citation_count": 23
  },
  "https://jmlr.org/papers/v23/19-644.html": {
    "title": "Manifold Coordinates with Physical Meaning",
    "abstract": "Manifold embedding algorithms map high-dimensional data down to coordinates in a much lower-dimensional space. One of the aims of dimension reduction is to find intrinsic coordinates that describe the data manifold. The coordinates returned by the embedding algorithm are abstract, and finding their physical or domain-related meaning is not formalized and often left to domain experts. This paper studies the problem of recovering the meaning of the new low-dimensional representation in an  automatic, principled fashion.  We propose a method to explain embedding coordinates of a manifold as non-linear compositions of functions from a user-defined dictionary. We show that this problem can be set up as a sparse linear Group Lasso recovery problem, find sufficient recovery conditions, and demonstrate its effectiveness on data",
    "volume": "main",
    "checked": true,
    "id": "70b128a1aeb62eae2f7af74695413ca8487d3386",
    "citation_count": 4
  },
  "https://jmlr.org/papers/v23/19-816.html": {
    "title": "Transfer Learning in Information Criteria-based Feature Selection",
    "abstract": "This paper investigates the effectiveness of transfer learning based on information criteria. We propose a procedure that combines transfer learning with Mallows' Cp (TLCp) and prove that it outperforms the conventional Mallows' Cp criterion in terms of accuracy and stability. Our theoretical results indicate that, for any sample size in the target domain, the proposed TLCp estimator performs better than the Cp estimator by the mean squared error (MSE) metric {in the case of orthogonal predictors}, provided that i) the dissimilarity between the tasks from source domain and target domain is small, and ii) the procedure parameters (complexity penalties) are tuned according to certain explicit rules. Moreover, we show that our transfer learning framework can be extended to other feature selection criteria, such as the Bayesian information criterion. By analyzing the solution of the orthogonalized Cp, we identify an estimator that asymptotically approximates the solution of the Cp criterion in the case of non-orthogonal predictors. Similar results are obtained for the non-orthogonal TLCp. Finally, simulation studies and applications with real data demonstrate the usefulness of the TLCp scheme",
    "volume": "main",
    "checked": true,
    "id": "3bd6e4987850ed67942e2425a146881017043507",
    "citation_count": 2
  },
  "https://jmlr.org/papers/v23/20-1360.html": {
    "title": "Recovery and Generalization in Over-Realized Dictionary Learning",
    "abstract": "In over two decades of research, the field of dictionary learning has gathered a large collection of successful applications, and theoretical guarantees for model recovery are known only whenever optimization is carried out in the same model class as that of the underlying dictionary. This work characterizes the surprising phenomenon that dictionary recovery can be facilitated by searching over the space of larger over-realized models. This observation is general and independent of the specific dictionary learning algorithm used. We thoroughly demonstrate this observation in practice and provide an analysis of this phenomenon by tying recovery measures to generalization bounds. In particular, we show that model recovery can be upper-bounded by the empirical risk, a model-dependent quantity and the generalization gap, reflecting our empirical findings. We further show that an efficient and provably correct distillation approach can be employed to recover the correct atoms from the over-realized model. As a result, our meta-algorithm provides dictionary estimates with consistently better recovery of the ground-truth model",
    "volume": "main",
    "checked": true,
    "id": "9d6eb01364c80d4f3cc49ae7e2a2a8ef1afc99c0",
    "citation_count": 6
  },
  "https://jmlr.org/papers/v23/20-1368.html": {
    "title": "Low-rank Tensor Learning with Nonconvex Overlapped Nuclear Norm Regularization",
    "abstract": "Nonconvex regularization has been popularly used in low-rank matrix learning. However, extending  it for low-rank tensor learning is still computationally expensive. To address this problem, we develop an efficient solver for use with a nonconvex extension of the overlapped nuclear norm regularizer. Based on the proximal average algorithm, the proposed algorithm can avoid expensive tensor folding/unfolding operations. A special âsparse plus low-rank\" structure is maintained throughout the iterations, and allows fast computation of the individual proximal steps. Empirical convergence is further improved with the use of adaptive momentum. We  provide convergence guarantees to critical points on smooth losses and also on objectives satisfying the Kurdyka-Lojasiewicz condition. While the optimization problem is nonconvex and nonsmooth, we show that its critical points still have good statistical performance on the tensor completion problem. Experiments on various synthetic and real-world data sets show that the proposed algorithm is efficient in both time and space and more accurate than the existing state-of-the-art",
    "volume": "main",
    "checked": true,
    "id": "094fa98b092d83efd041e22592a4d521010dd182",
    "citation_count": 0
  },
  "https://jmlr.org/papers/v23/20-277.html": {
    "title": "On the Efficiency of Entropic Regularized Algorithms for Optimal Transport",
    "abstract": "We present several new complexity results for the entropic regularized algorithms that approximately solve the optimal transport (OT) problem between two discrete probability measures with at most $n$ atoms. First, we improve the complexity bound of a greedy variant of Sinkhorn, known as Greenkhorn, from $\\tilde{O}(n^2\\varepsilon^{-3})$ to $\\tilde{O}(n^2\\varepsilon^{-2})$. Notably, our result can match the best known complexity bound of Sinkhorn and help clarify why Greenkhorn significantly outperforms Sinkhorn in practice in terms of row/column updates as observed by Altschuler et al. (2017). Second, we propose a new algorithm, which we refer to as APDAMD and which generalizes an adaptive primal-dual accelerated gradient descent (APDAGD) algorithm (Dvurechensky et al., 2018) with a prespecified mirror mapping $\\phi$. We prove that APDAMD achieves the complexity bound of $\\tilde{O}(n^2\\sqrt{\\delta}\\varepsilon^{-1})$ in which $\\delta>0$ stands for the regularity of $\\phi$. In addition, we show by a counterexample that the complexity bound of $\\tilde{O}(\\min\\{n^{9/4}\\varepsilon^{-1}, n^2\\varepsilon^{-2}\\})$ proved for APDAGD before is invalid and give a refined complexity bound of $\\tilde{O}(n^{5/2}\\varepsilon^{-1})$. Further, we develop a deterministic accelerated variant of Sinkhorn via appeal to estimated sequence and prove the complexity bound of $\\tilde{O}(n^{7/3}\\varepsilon^{-4/3})$. As such, we see that accelerated variant of Sinkhorn outperforms Sinkhorn and Greenkhorn in terms of $1/\\varepsilon$ and APDAGD and accelerated alternating minimization (AAM) (Guminov et al., 2021) in terms of $n$. Finally, we conduct the experiments on synthetic and real data and the numerical results show the efficiency of Greenkhorn, APDAMD and accelerated Sinkhorn in practice",
    "volume": "main",
    "checked": true,
    "id": "42237ae711d748b5425cdfb351f9c1babe872d15",
    "citation_count": 22
  },
  "https://jmlr.org/papers/v23/20-321.html": {
    "title": "Exact simulation of diffusion first exit times: algorithm acceleration",
    "abstract": "In order to describe or estimate different quantities related to a specific random variable, it is of prime interest to numerically generate such a variate. In specific situations, the exact generation of random variables might be either momentarily unavailable or too expensive in terms of computation time. It therefore needs to be replaced by an approximation procedure. As was previously the case, the ambitious exact simulation of first exit times for diffusion processes was unreachable though it concerns many applications in different fields like mathematical finance, neuroscience or reliability. The usual way to describe first exit times was to use discretization schemes, that are of course approximation procedures. Recently, Herrmann and Zucca proposed a new algorithm, the so-called GDET-algorithm (General Diffusion Exit Time), which permits to simulate exactly the first exit time for one-dimensional diffusions. The only drawback of exact simulation methods using an acceptance-rejection sampling is their time consumption. In this paper the authors highlight an acceleration procedure for the GDET-algorithm based on a multi-armed bandit model. The efficiency of this acceleration is pointed out through numerical examples",
    "volume": "main",
    "checked": true,
    "id": "ee9c4558df3e94ab0baa0b49f1391376229c606e",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v23/20-411.html": {
    "title": "No Weighted-Regret Learning in Adversarial Bandits with Delays",
    "abstract": "Consider a scenario where a player chooses an action in each round $t$ out of $T$ rounds and observes the incurred cost after a delay of $d_{t}$ rounds. The cost functions and the delay sequence are chosen by an adversary. We show that in a non-cooperative game, the expected weighted ergodic distribution of play converges to the set of coarse correlated equilibria if players use algorithms that have âno weighted-regretâ in the above scenario, even if they have linear regret due to too large delays. For a two-player zero-sum game, we show that no weighted-regret is sufficient for the weighted ergodic average of play to converge to the set of Nash equilibria. We prove that the FKM algorithm with $n$ dimensions achieves an expected regret of $O\\left(nT^{\\frac{3}{4}}+\\sqrt{n}T^{\\frac{1}{3}}D^{\\frac{1}{3}}\\right)$ and the EXP3 algorithm with $K$ arms achieves an expected regret of $O\\left(\\sqrt{\\log  K\\left(KT+D\\right)}\\right)$ even when $D=\\sum_{t=1}^{T}d_{t}$ and $T$ are unknown. These bounds use a novel doubling trick that, under mild assumptions, provably retains the regret bound for when $D$ and $T$ are known. Using these bounds, we show that FKM and EXP3 have no weighted-regret even for $d_{t}=O\\left(t\\log t\\right)$. Therefore, algorithms with no weighted-regret can be used to approximate a CCE of a finite or convex unknown game that can only be simulated with bandit feedback, even if the simulation involves significant delays",
    "volume": "main",
    "checked": true,
    "id": "96caf3f311ed0428e0d3d32dca8175ab973284e7",
    "citation_count": 3
  },
  "https://jmlr.org/papers/v23/20-617.html": {
    "title": "Non-asymptotic and Accurate Learning of Nonlinear Dynamical Systems",
    "abstract": "We consider the problem of learning a nonlinear dynamical system governed by a nonlinear state equation $h_{t+1}=\\phi(h_t,u_t;\\theta)+w_t$. Here $\\theta$ is the unknown system dynamics, $h_t$ is the state, $u_t$ is the input and $w_t$ is the additive noise vector. We study gradient based algorithms to learn the system dynamics $\\theta$ from samples obtained from a single finite trajectory. If the system is run by a stabilizing input policy, then using a mixing-time argument we show that temporally-dependent samples can be approximated by i.i.d. samples. We then develop new guarantees for the uniform convergence of the gradient of the empirical loss induced by these i.i.d. samples. Unlike existing works, our bounds are noise sensitive which allows for learning the ground-truth dynamics with high accuracy and small sample complexity. When combined, our results facilitate efficient learning of a broader class of nonlinear dynamical systems as compared to the prior works. We specialize our guarantees to  entrywise nonlinear activations and verify our theory in various numerical experiments",
    "volume": "main",
    "checked": true,
    "id": "7bec38574b2f16ef689d0f303b9a008b714e4858",
    "citation_count": 32
  },
  "https://jmlr.org/papers/v23/20-944.html": {
    "title": "The Importance of Being Correlated: Implications of Dependence in Joint Spectral Inference across Multiple Networks",
    "abstract": "Spectral inference on multiple networks is a rapidly-developing subfield of graph statistics. Recent work has demonstrated that joint, or simultaneous, spectral embedding of multiple independent networks can deliver more accurate estimation than individual spectral decompositions of those same networks. Such inference procedures typically rely heavily on independence assumptions across the multiple network realizations, and even in this case, little attention has been paid to the induced network correlation that can be a consequence of such joint embeddings. In this paper, we present a generalized omnibus embedding methodology and we provide a detailed analysis of this embedding across both independent and correlated networks, the latter of which significantly extends the reach of such procedures, and we describe how this omnibus embedding can itself induce correlation. This leads us to distinguish betwee inherent correlation---that is, the correlation that arises naturally in multisample network data---and induced correlation, which is an artifice of the joint embedding methodology. We show that the generalized omnibus embedding procedure is flexible and robust, and we prove both consistency and a central limit theorem for the embedded points. We examine how induced and inherent correlation can impact inference for network time series data, and we provide network analogues of classical questions such as the effective sample size for more generally correlated data. Further, we show how an appropriately calibrated generalized omnibus embedding can detect changes in real biological networks that previous embedding procedures could not discern, confirming that the effect of inherent and induced correlation can be subtle and transformative. By allowing for and deconstructing both forms of correlation, our methodology widens the scope of spectral techniques for network inference, with import in theory and practice",
    "volume": "main",
    "checked": true,
    "id": "3ddd9dcb7d52cd3c7c8126f8a6157886456e19ff",
    "citation_count": 8
  },
  "https://jmlr.org/papers/v23/20-984.html": {
    "title": "A Perturbation-Based Kernel Approximation Framework",
    "abstract": "Kernel methods are powerful tools in various data analysis tasks. Yet, in many cases, their time and space complexity render them impractical for large datasets. Various kernel approximation methods were proposed to overcome this issue, with the most prominent method being the Nystr{\\\"o}m method. In this paper, we derive a perturbation-based kernel approximation framework building upon results from classical perturbation theory. We provide an  error analysis for this framework, and prove that in fact, it generalizes the Nystr{\\\"o}m method and several of its variants. Furthermore, we show that our framework gives rise to new kernel approximation schemes, that can be tuned to take advantage of the structure of the approximated kernel matrix. We support our theoretical results numerically and demonstrate the advantages of our approximation framework on both synthetic and real-world data",
    "volume": "main",
    "checked": true,
    "id": "1fe7a26cda92994a40f6e03aa6c86952195efdda",
    "citation_count": 0
  },
  "https://jmlr.org/papers/v23/21-0225.html": {
    "title": "Reverse-mode differentiation in arbitrary tensor network format: with application to supervised learning",
    "abstract": "This paper describes an efficient reverse-mode differentiation algorithm for contraction operations for arbitrary and unconventional tensor network topologies. The approach leverages the tensor contraction tree of Evenbly and Pfeifer (2014), which provides an instruction set for the contraction sequence of a network. We show that this tree can be efficiently leveraged for differentiation of a full tensor network contraction using a recursive scheme that exploits (1) the bilinear property of contraction and (2) the property that trees have a single path from root to leaves. While differentiation of tensor-tensor contraction is already possible in most automatic differentiation packages, we show that exploiting these two additional properties in the specific context of contraction sequences can improve efficiency.  Following a description of the algorithm and computational complexity analysis, we investigate its utility for gradient-based supervised learning for low-rank function recovery and for fitting real-world unstructured datasets. We demonstrate improved performance over alternating least-squares optimization approaches and the capability to handle heterogeneous and arbitrary tensor network formats. When compared to alternating minimization algorithms, we find that the gradient-based approach requires a smaller oversampling ratio (number of samples compared to number model parameters) for recovery. This increased efficiency extends to fitting unstructured data of varying dimensionality and when employing a variety of tensor network formats. Here, we show improved learning using the hierarchical Tucker method over the tensor-train in high-dimensional settings on a number of benchmark problems",
    "volume": "main",
    "checked": true,
    "id": "915a5843d858ab32055f56be0c4dda24e9c84dce",
    "citation_count": 0
  },
  "https://jmlr.org/papers/v23/21-0226.html": {
    "title": "A Momentumized, Adaptive, Dual Averaged Gradient Method",
    "abstract": "We introduce MADGRAD, a novel optimization method in the family of AdaGrad adaptive gradient methods. MADGRAD shows excellent performance on deep learning optimization problems from multiple fields, including classification and image-to-image tasks in vision, and recurrent and bidirectionally-masked models in natural language processing. For each of these tasks, MADGRAD matches or outperforms both SGD and ADAM in test set performance, even on problems for which adaptive methods normally perform poorly",
    "volume": "main",
    "checked": true,
    "id": "3b53a5408ae4fd417448e09360bbb6d39371de2e",
    "citation_count": 0
  },
  "https://jmlr.org/papers/v23/21-037.html": {
    "title": "A Generalized Projected Bellman Error for Off-policy Value Estimation in Reinforcement Learning",
    "abstract": "Many reinforcement learning algorithms rely on value estimation, however, the most widely used algorithms---namely temporal difference algorithms---can diverge under both off-policy sampling and nonlinear function approximation. Many algorithms have been developed for off-policy value estimation based on the linear mean squared projected Bellman error (MSPBE) and are sound under linear function approximation. Extending these methods to the nonlinear case has been largely unsuccessful. Recently, several methods have been introduced that approximate a different objective---the mean-squared Bellman error (MSBE)---which naturally facilitate nonlinear approximation. In this work, we build on these insights and introduce a new generalized MSPBE that extends the linear MSPBE to the nonlinear setting. We show how this generalized objective unifies previous work and obtain new bounds for the value error of the solutions of the generalized objective. We derive an easy-to-use, but sound, algorithm to minimize the generalized objective, and show that it is more stable across runs, is less sensitive to hyperparameters, and performs favorably across four control domains with neural network function approximation",
    "volume": "main",
    "checked": true,
    "id": "301e76d7635bac4c881167b420bd4502cf1f590c",
    "citation_count": 11
  },
  "https://jmlr.org/papers/v23/21-0382.html": {
    "title": "Adversarial Robustness Guarantees for Gaussian Processes",
    "abstract": "Gaussian processes (GPs) enable principled computation of model uncertainty, making them attractive for safety-critical applications. Such scenarios demand that GP decisions are not only accurate, but also robust to perturbations. In this paper we present a framework to analyse adversarial robustness of GPs, defined as invariance of the model's decision to bounded perturbations. Given a compact subset of the input space $T\\subseteq \\mathbb{R}^d$, a point $x^*$ and a GP, we provide provable guarantees of adversarial robustness of the GP by computing lower and upper bounds on its prediction range in $T$. We develop a branch-and-bound scheme to refine the bounds and show, for any $\\epsilon > 0$, that our algorithm is guaranteed to converge to values $\\epsilon$-close to the actual values in finitely many iterations. The algorithm is anytime and can handle both regression and classification tasks, with analytical formulation for most kernels used in practice. We evaluate our methods on a collection of synthetic and standard benchmark data sets, including SPAM, MNIST and FashionMNIST. We study the effect of approximate inference techniques on robustness and demonstrate how our method can be used for interpretability. Our empirical results suggest that the adversarial robustness of GPs increases with accurate posterior estimation",
    "volume": "main",
    "checked": true,
    "id": "6f18c264adbb8cf4d243bdac07368d1472b87d5d",
    "citation_count": 5
  },
  "https://jmlr.org/papers/v23/21-0386.html": {
    "title": "On the Robustness to Misspecification of Î±-posteriors and Their Variational Approximations",
    "abstract": "$\\alpha$-posteriors and their variational approximations distort standard posterior inference by downweighting the likelihood and introducing variational approximation errors. We show that such distortions, if tuned appropriately, reduce the Kullback--Leibler (KL) divergence from the true, but perhaps infeasible, posterior distribution when there is potential parametric model misspecification. To make this point, we derive a Bernstein--von Mises theorem showing convergence in total variation distance of $\\alpha$-posteriors and their variational approximations to limiting Gaussian distributions. We use these limiting distributions to evaluate the KL divergence between true and reported posteriors. We show that the KL divergence is minimized by choosing $\\alpha$ strictly smaller than one, assuming there is a vanishingly small probability of model misspecification. The optimized value of $\\alpha$ becomes smaller as the misspecification becomes more severe. The optimized KL divergence increases logarithmically in the magnitude of misspecification and not linearly as with the usual posterior. Moreover, the optimized variational approximations of $\\alpha$-posteriors can induce additional robustness to model misspecification beyond that obtained by optimally downweighting the likelihood",
    "volume": "main",
    "checked": false,
    "id": null,
    "citation_count": 0
  },
  "https://jmlr.org/papers/v23/21-0419.html": {
    "title": "Online Nonnegative CP-dictionary Learning for Markovian Data",
    "abstract": "Online Tensor Factorization (OTF) is a  fundamental tool in learning low-dimensional interpretable features from streaming multi-modal data. While various algorithmic and theoretical aspects of OTF have been investigated recently, a general convergence guarantee to stationary points of the objective function without any incoherence or sparsity assumptions is still lacking even for the i.i.d. case. In this work, we introduce a novel algorithm that learns a CANDECOMP/PARAFAC (CP) basis from a given stream of tensor-valued data under general constraints, including nonnegativity constraints that induce interpretability of the learned CP basis. We prove that our algorithm converges almost surely to the set of stationary points of the objective function under the hypothesis that the sequence of data tensors is generated by an underlying Markov chain. Our setting covers the classical i.i.d. case as well as a wide range of application contexts including data streams generated by independent or MCMC sampling. Our result closes a gap between OTF and Online Matrix Factorization in global convergence analysis for CP-decompositions. Experimentally, we show that our algorithm converges much faster than standard algorithms for nonnegative tensor factorization tasks on both synthetic and real-world data. Also, we demonstrate the utility of our algorithm on a diverse set of examples from image, video, and time-series data, illustrating how one may learn qualitatively different CP-dictionaries from the same tensor data by exploiting the tensor structure in multiple ways",
    "volume": "main",
    "checked": true,
    "id": "bad48796eb14803bf82c654451a4ba4e3f6d15ee",
    "citation_count": 4
  },
  "https://jmlr.org/papers/v23/21-0486.html": {
    "title": "Implicit Differentiation for Fast Hyperparameter Selection in Non-Smooth Convex Learning",
    "abstract": "Finding the optimal hyperparameters of a model can be cast as a bilevel optimization problem, typically solved using zero-order techniques. In this work we study first-order methods when the inner optimization problem is convex but non-smooth. We show that the forward-mode differentiation of proximal gradient descent and proximal coordinate descent yield sequences of Jacobians converging toward the exact Jacobian. Using implicit differentiation, we show it is possible to leverage the non-smoothness of the inner problem to speed up the computation. Finally, we provide a bound on the error made on the hypergradient when the inner optimization problem is solved approximately. Results on regression and classification problems reveal computational benefits for hyperparameter optimization, especially when multiple hyperparameters are required",
    "volume": "main",
    "checked": true,
    "id": "6bac2ed393f6ef68a6dc63a32466df90abf223d5",
    "citation_count": 13
  },
  "https://jmlr.org/papers/v23/21-0663.html": {
    "title": "EV-GAN: Simulation of extreme events with ReLU neural networks",
    "abstract": "Feedforward neural networks based on Rectified linear units (ReLU) cannot efficiently approximate quantile functions which are not bounded, especially in the case of heavy-tailed distributions. We thus propose a new parametrization for the generator of a Generative adversarial network (GAN) adapted to this framework, basing on extreme-value theory. An analysis of the uniform error between the extreme quantile and its GAN approximation is provided: We establish that the rate of convergence of the error is mainly driven by the second-order parameter of the data distribution. The above results are illustrated on simulated data and real financial data. It appears that our approach outperforms the classical GAN in a wide range of situations including high-dimensional and dependent data",
    "volume": "main",
    "checked": true,
    "id": "57cb7b8e888302b9ebad2070ec3d0e1bcd79921e",
    "citation_count": 6
  },
  "https://jmlr.org/papers/v23/21-0730.html": {
    "title": "Universal Approximation of Functions on Sets",
    "abstract": "Modelling functions of sets, or equivalently, permutation-invariant functions, is a long-standing challenge in machine learning. Deep Sets is a popular method which is known to be a universal approximator for continuous set functions. We provide a theoretical analysis of Deep Sets which shows that this universal approximation property is only guaranteed if the model's latent space is sufficiently high-dimensional. If the latent space is even one dimension lower than necessary, there exist piecewise-affine functions for which Deep Sets performs no better than a naÃ¯ve constant baseline, as judged by worst-case error. Deep Sets may be viewed as the most efficient incarnation of the Janossy pooling paradigm. We identify this paradigm as encompassing most currently popular set-learning methods. Based on this connection, we discuss the implications of our results for set learning more broadly, and identify some open questions on the universality of Janossy pooling in general",
    "volume": "main",
    "checked": true,
    "id": "c7c625fc31168b64ed3191f916e446e08baaf9d2",
    "citation_count": 16
  },
  "https://jmlr.org/papers/v23/21-0808.html": {
    "title": "Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning",
    "abstract": "Intrinsically motivated spontaneous exploration is a key enabler of autonomous developmental learning in human children. It enables the discovery of skill repertoires through autotelic learning, i.e. the self-generation, self-selection, self-ordering and self-experimentation of learning goals. We present an algorithmic approach called Intrinsically Motivated Goal Exploration Processes (IMGEP) to enable similar properties of autonomous learning in machines. The IMGEP architecture relies on several principles: 1) self-generation of goals, generalized as parameterized fitness functions; 2) selection of goals based on intrinsic rewards; 3) exploration with incremental goal-parameterized policy search and exploitation with a batch learning algorithm; 4) systematic reuse of information acquired when targeting a goal for improving towards other goals. We present a particularly efficient form of IMGEP, called AMB, that uses a population-based policy and an object-centered spatio-temporal modularity. We provide several implementations of this architecture and demonstrate their ability to automatically generate a learning curriculum within several experimental setups. One of these experiments includes a real humanoid robot exploring multiple spaces of goals with several hundred continuous dimensions and with distractors. While no particular target goal is provided to these autotelic agents, this curriculum allows the discovery of diverse skills that act as stepping stones for learning more complex skills, e.g. nested tool use",
    "volume": "main",
    "checked": true,
    "id": "1418c9da011db25fa95a32989d5a578bc3bc4601",
    "citation_count": 199
  },
  "https://jmlr.org/papers/v23/21-0934.html": {
    "title": "Truncated Emphatic Temporal Difference Methods for Prediction and Control",
    "abstract": "Emphatic Temporal Difference (TD) methods are a class of off-policy Reinforcement Learning (RL) methods involving the use of followon traces.  Despite the theoretical success of emphatic TD methods in addressing the notorious deadly triad of off-policy RL, there are still two open problems. First, followon traces typically suffer from large variance, making them hard to use in practice.  Second, though Yu (2015) confirms the asymptotic convergence of some emphatic TD methods for prediction problems, there is still no finite sample analysis for any emphatic TD method for prediction, much less control. In this paper,  we address those two open problems simultaneously via using truncated followon traces in emphatic TD methods. Unlike the original followon traces, which depend on all previous history, truncated followon traces depend on only finite history, reducing variance and enabling the finite sample analysis of our proposed emphatic TD methods for both prediction and control",
    "volume": "main",
    "checked": true,
    "id": "896ed77fc008cc5d6fd3c0a38b96163e2e4f1397",
    "citation_count": 3
  },
  "https://jmlr.org/papers/v23/21-0947.html": {
    "title": "Policy Evaluation and Temporal-Difference Learning in Continuous Time and Space: A Martingale Approach",
    "abstract": "We propose a unified framework to study policy evaluation (PE) and the associated temporal difference (TD) methods for reinforcement learning in continuous time and space. We show that PE is equivalent to maintaining the martingale condition of a process. From this perspective, we find that the mean-square TD error approximates the quadratic variation of the martingale and thus is not a suitable objective for PE. We present two methods to use the martingale characterization for designing PE algorithms. The first one minimizes a âmartingale loss function\", whose solution is proved to be the best approximation of the true value function in the mean--square sense. This method interprets the classical gradient Monte-Carlo algorithm. The second method is based on a system of equations called the âmartingale orthogonality conditions\" with test functions. Solving these equations in different ways recovers various classical TD algorithms, such as TD($\\lambda$), LSTD, and GTD. Different choices of test functions determine in what sense the resulting solutions approximate the true value function. Moreover, we prove that any convergent time-discretized algorithm converges to its continuous-time counterpart as the mesh size goes to zero, and we provide the convergence rate. We demonstrate the theoretical results and corresponding algorithms with numerical experiments and applications",
    "volume": "main",
    "checked": true,
    "id": "580cf22496ebd4697383250fca1d335125aa9cd0",
    "citation_count": 18
  },
  "https://jmlr.org/papers/v23/21-0991.html": {
    "title": "Principal Components Bias in Over-parameterized Linear Models, and its Manifestation in Deep Neural Networks",
    "abstract": "Recent work suggests that convolutional neural networks of different architectures learn to classify images in the same order. To understand this phenomenon, we revisit the over-parametrized deep linear network model. Our analysis reveals that, when the hidden layers are wide enough, the convergence rate of this model's parameters is exponentially faster along the directions of the larger principal components of the data, at a rate governed by the corresponding singular values. We term this convergence pattern the Principal Components bias (PC-bias). Empirically, we show how the PC-bias streamlines the order of learning of both linear and non-linear networks, more prominently at earlier stages of learning. We then compare our results to the simplicity bias, showing that both biases can be seen independently, and affect the order of learning in different ways. Finally, we discuss how the PC-bias may explain some benefits of early stopping and its connection to PCA, and why deep networks converge more slowly with random labels",
    "volume": "main",
    "checked": true,
    "id": "6c40eaba86bb4447bdd47f4af5c062aa25ca5922",
    "citation_count": 5
  },
  "https://jmlr.org/papers/v23/21-0999.html": {
    "title": "Statistical Rates of Convergence for Functional Partially Linear Support Vector Machines for Classification",
    "abstract": "In this paper, we consider the learning rate of support vector machines with both a functional predictor and a high-dimensional multivariate vectorial predictor. Similar to the literature on learning in reproducing kernel Hilbert spaces, a source condition and a capacity condition are used to characterize the convergence rate of the estimator. It is highly non-trivial to establish the possibly faster rate of the linear part. Using a key basic inequality comparing losses at two carefully constructed points, we establish the learning rate of the linear part which is the same as if the functional part is known. The proof relies on empirical processes and the Rademacher complexity bound in the semi-nonparametric setting as analytic tools, Young's inequality for operators, as well as a novel  âapproximate convexity\" assumption",
    "volume": "main",
    "checked": true,
    "id": "3c33a0fd241e7a018de0a319b7898205f6aa35ba",
    "citation_count": 0
  },
  "https://jmlr.org/papers/v23/21-1043.html": {
    "title": "A universally consistent learning rule with a universally monotone error",
    "abstract": "We present a universally consistent learning rule whose expected error is monotone non-increasing with the sample size under every data distribution. The question of existence of such rules was brought up in 1996 by Devroye, GyÃ¶rfi and Lugosi (who called them âsmartâ). Our rule is fully deterministic, a data-dependent partitioning rule constructed in an arbitrary domain (a standard Borel space) using a cyclic order. The central idea is to only partition at each step those cyclic intervals that exhibit a sufficient empirical diversity of labels, thus avoiding a region where the error function is convex",
    "volume": "main",
    "checked": true,
    "id": "30231f778e2af729569817d1a897e4ed7c797f32",
    "citation_count": 3
  },
  "https://jmlr.org/papers/v23/21-1124.html": {
    "title": "ktrain: A Low-Code Library for Augmented Machine Learning",
    "abstract": "We present ktrain, a low-code Python library that makes machine learning more accessible and easier to apply. As a wrapper to TensorFlow and many other libraries (e.g., transformers, scikit-learn, stellargraph), it is designed to make sophisticated, state-of-the-art machine learning models simple to build, train, inspect, and apply by both beginners and experienced practitioners. Featuring modules that support text data (e.g., text classification, sequence tagging, open-domain question-answering), vision data (e.g., image classification), graph data (e.g., node classification, link prediction), and tabular data, ktrain presents a simple unified interface enabling one to quickly solve a wide range of tasks in as little as three or four \"commands\" or lines of code",
    "volume": "main",
    "checked": true,
    "id": "6d38ed5d85c30a6e88ee207dce96ba432d1b6af2",
    "citation_count": 88
  },
  "https://jmlr.org/papers/v23/21-1159.html": {
    "title": "Structure Learning for Directed Trees",
    "abstract": "Knowing the causal structure of a system is of fundamental interest in many areas of science and can aid the design of prediction algorithms that work well under manipulations to the system. The causal structure becomes identifiable from the observational distribution under certain restrictions. To learn the structure from data, score-based methods evaluate different graphs according to the quality of their fits. However, for large, continuous, and nonlinear models, these rely on heuristic optimization approaches with no general guarantees of recovering the true causal structure. In this paper, we consider structure learning of directed trees. We propose a fast and scalable method based on ChuâLiuâEdmondsâ algorithm we call causal additive trees (CAT). For the case of Gaussian errors, we prove consistency in an asymptotic regime with a vanishing identifiability gap. We also introduce two methods for testing substructure hypotheses with asymptotic family-wise error rate control that is valid post-selection and in unidentified settings. Furthermore, we study the identifiability gap, which quantifies how much better the true causal model fits the observational distribution, and prove that it is lower bounded by local properties of the causal model. Simulation studies demonstrate the favorable performance of CAT compared to competing structure learning methods",
    "volume": "main",
    "checked": true,
    "id": "0d67962478e708067af6f87ded89d4016f836b1c",
    "citation_count": 6
  },
  "https://jmlr.org/papers/v23/21-1189.html": {
    "title": "Fairness-Aware PAC Learning from Corrupted Data",
    "abstract": "Addressing fairness concerns about machine learning models is a crucial step towards their long-term adoption in real-world automated systems. While many approaches have been developed for training fair models from data, little is known about the robustness of these methods to data corruption. In this work we consider fairness-aware learning under worst-case data manipulations. We show that an adversary can in some situations force any learner to return an overly biased classifier, regardless of the sample size and with or without degrading accuracy, and that the strength of the excess bias increases for learning problems with underrepresented protected groups in the data. We also prove that our hardness results are tight up to constant factors. To this end, we study two natural learning algorithms that optimize for both accuracy and fairness and show that these algorithms enjoy guarantees that are order-optimal in terms of the corruption ratio and the protected groups frequencies in the large data limit",
    "volume": "main",
    "checked": true,
    "id": "cf90ea47e52f52164e457036a93d1a8e9912341f",
    "citation_count": 7
  },
  "https://jmlr.org/papers/v23/21-1270.html": {
    "title": "Topologically penalized regression on manifolds",
    "abstract": "We study a regression problem on a compact manifold M. In order to take advantage of the underlying geometry and topology of the data, the regression task is performed on the basis of the first several eigenfunctions of the Laplace-Beltrami operator of the manifold, that are regularized with topological penalties. The proposed penalties are based on the topology of the sub-level sets of either the eigenfunctions or the estimated function. The overall approach is shown to yield promising and competitive performance on various applications to both synthetic and real data sets. We also provide theoretical guarantees on the regression function estimates, on both its prediction error and its smoothness (in a topological sense). Taken together, these results support the relevance of our approach in the case where the targeted function is âtopologically smoothâ",
    "volume": "main",
    "checked": true,
    "id": "6ded049a0180468593c5d4723aa8aff58dc0816b",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v23/21-1282.html": {
    "title": "Explicit Convergence Rates of Greedy and Random Quasi-Newton Methods",
    "abstract": "Optimization is important in machine learning problems, and quasi-Newton methods have a reputation as the most efficient numerical methods for smooth unconstrained optimization. In this paper, we study the explicit superlinear convergence rates of quasi-Newton methods and address two open problems mentioned by Rodomanov and Nesterov (2021b). First, we extend Rodomanov and Nesterov (2021b)âs results to random quasi-Newton methods, which include common DFP, BFGS, SR1 methods. Such random methods employ a random direction for updating the approximate Hessian matrix in each iteration. Second, we focus on the specific quasi-Newton methods: SR1 and BFGS methods. We provide improved versions of greedy and random methods with provable better explicit (local) superlinear convergence rates. Our analysis is closely related to the approximation of a given Hessian matrix, unconstrained quadratic objective, as well as the general strongly convex, smooth, and strongly self-concordant functions",
    "volume": "main",
    "checked": true,
    "id": "d6122e33bda6e3fc0d6b7968bbe01731d5c23ee0",
    "citation_count": 7
  },
  "https://jmlr.org/papers/v23/21-1390.html": {
    "title": "Scaling and Scalability: Provable Nonconvex Low-Rank Tensor Estimation from Incomplete Measurements",
    "abstract": "Tensors, which provide a powerful and flexible model for representing multi-attribute data and multi-way interactions, play an indispensable role in modern data science  across various fields in science and engineering. A fundamental task is to faithfully recover the tensor from highly incomplete measurements in a statistically and computationally efficient manner. Harnessing the low-rank structure of tensors in the Tucker decomposition, this paper develops a scaled gradient descent (ScaledGD) algorithm to directly recover the tensor factors with tailored spectral initializations, and shows that it provably converges at a linear rate independent of the condition number of the ground truth tensor for two canonical problems --- tensor completion and tensor regression --- as soon as the sample size is above the order of $n^{3/2}$ ignoring other parameter dependencies, where $n$ is the dimension of the tensor. This leads to an extremely scalable approach to low-rank tensor estimation compared with prior art, which suffers from at least one of the following drawbacks: extreme sensitivity to ill-conditioning, high per-iteration costs in terms of memory and computation, or poor sample complexity guarantees. To the best of our knowledge, ScaledGD is the first algorithm that achieves near-optimal statistical and computational complexities simultaneously for low-rank tensor completion with the Tucker decomposition. Our algorithm highlights the power of appropriate preconditioning in accelerating nonconvex statistical estimation, where the iteration-varying preconditioners promote desirable invariance properties of the trajectory with respect to the underlying symmetry in low-rank tensor factorization",
    "volume": "main",
    "checked": true,
    "id": "53fbb69107893ca5b1475a315bbbdbe36d9f2022",
    "citation_count": 22
  },
  "https://jmlr.org/papers/v23/19-104.html": {
    "title": "Solving L1-regularized SVMs and Related Linear Programs: Revisiting the Effectiveness of Column and Constraint Generation",
    "abstract": "The linear Support Vector Machine (SVM) is a classic classification technique in machine learning. Motivated by applications in high dimensional statistics, we consider penalized SVM problems involving the minimization of a hinge-loss function with a convex sparsity-inducing regularizer such as: the L1-norm on the coefficients, its grouped generalization and the sorted L1-penalty (aka Slope). Each problem can be expressed as a Linear Program (LP) and is computationally challenging when the number of features and/or samples is large---the current state of algorithms for these problems is rather nascent when compared to the usual L2-regularized linear SVM. To this end, we propose new computational algorithms for these LPs by bringing together techniques from (a) classical column (and constraint) generation methods and (b) first-order methods for non-smooth convex optimization---techniques that appear to be rarely used together for solving large scale LPs. These components have their respective strengths; and while they are found to be useful as separate entities, they appear to be more powerful in practice when used together in the context of solving large-scale LPs such as the ones studied herein. Our approach complements the strengths of (a) and (b)---leading to a scheme that seems to significantly outperform commercial solvers as well as specialized implementations for these problems. We present numerical results on a series of real and synthetic data sets demonstrating the surprising effectiveness of classic column/constraint generation methods in the context of challenging LP-based machine learning tasks",
    "volume": "main",
    "checked": true,
    "id": "37993202e943011d70bb0e09fc4eed56153b7df1",
    "citation_count": 3
  },
  "https://jmlr.org/papers/v23/19-350.html": {
    "title": "Improved Classification Rates for Localized SVMs",
    "abstract": "Localized support vector machines solve SVMs on many spatially defined small chunks and besides their computational benefit compared to global SVMs one of their main characteristics is the freedom of choosing arbitrary kernel and regularization parameter on each cell. We take advantage of this observation to derive global learning rates for localized SVMs with Gaussian kernels and hinge loss. It turns out that our rates outperform under suitable sets of assumptions known classification rates for localized SVMs, for global SVMs, and other learning algorithms based on e.g., plug-in rules or trees. The localized SVM rates are achieved under a set of margin conditions, which describe the behavior of the data-generating distribution, and no assumption on the existence of a density is made. Moreover, we show that our rates are obtained adaptively, that is without knowing the margin parameters in advance. The statistical analysis of the excess risk relies on a simple partitioning based technique, which splits the input space into a subset that is close to the decision boundary and into a subset that is sufficiently far away. A crucial condition to derive then improved global rates is a margin condition that relates the distance to the decision boundary to the amount of noise",
    "volume": "main",
    "checked": true,
    "id": "917132c8ef690e38985c084b354546e58661b933",
    "citation_count": 4
  },
  "https://jmlr.org/papers/v23/19-511.html": {
    "title": "Generalization Bounds and Representation Learning for Estimation of Potential Outcomes and Causal Effects",
    "abstract": "Practitioners in diverse fields such as healthcare, economics and education are eager to apply machine learning to improve decision making. The cost and impracticality of performing experiments and a recent monumental increase in electronic record keeping has brought attention to the problem of evaluating decisions based on non-experimental observational data. This is the setting of this work. In particular, we study estimation of individual-level potential outcomes and causal effects---such as a single patient's response to alternative medication---from recorded contexts, decisions and outcomes. We give generalization bounds on the error in estimated outcomes based on distributional distance measures between re-weighted samples of groups receiving different treatments. We provide conditions under which our bounds are tight and show how they relate to results for unsupervised domain adaptation. Led by our theoretical results, we devise algorithms which learn representations and weighting functions that minimize our bounds by regularizing the representation's induced treatment group distance, and encourage sharing of information between treatment groups. Finally, an experimental evaluation on real and synthetic data shows the value of our proposed representation architecture and regularization scheme",
    "volume": "main",
    "checked": true,
    "id": "10216a7d19a1d3ccb29f2b599fb3b485ef3e336a",
    "citation_count": 53
  },
  "https://jmlr.org/papers/v23/19-571.html": {
    "title": "Unbiased estimators for random design regression",
    "abstract": "In linear regression we wish to estimate the optimum linear least squares predictor for a distribution over $d$-dimensional input points and real-valued responses, based on a small sample. Under standard random design analysis, where the sample is drawn i.i.d. from the input distribution, the least squares solution for that sample can be viewed as the natural estimator of the optimum. Unfortunately, this estimator almost always incurs an undesirable bias coming from the randomness of the input points, which is a significant bottleneck in model averaging. In this paper we show that it is possible to draw a non-i.i.d. sample of input points such that, regardless of the response model, the least squares solution is an unbiased estimator of the optimum. Moreover, this sample can be produced efficiently by augmenting a previously drawn i.i.d. sample with an additional set of $d$ points, drawn jointly according to a certain determinantal point process constructed from the input distribution rescaled by the squared volume spanned by the points. Motivated by this, we develop a theoretical framework for studying volume-rescaled sampling, and in the process prove a number of new matrix expectation identities. We use them to show that for any input distribution and $\\epsilon>0$ there is a random design consisting of $O(d\\log d+ d/\\epsilon)$ points from which an unbiased estimator can be constructed whose expected square loss over the entire distribution is bounded by $1+\\epsilon$ times the loss of the optimum. We provide efficient algorithms for constructing such unbiased estimators in a number of practical settings.  In one such setting, we let the input distribution be uniform over a large dataset of $n\\gg d$ points. Here, we obtain the first unbiased least squares estimator that can be constructed in time nearly-linear in the data size, resulting in strong guarantees for model averaging. We achieve these computational gains by introducing a new algorithmic technique, called distortion-free intermediate sampling, which is the first method to enable sampling from determinantal point processes in time polynomial in the sample size",
    "volume": "main",
    "checked": true,
    "id": "3c755f518a7cafe4d7836c98279342232d92e7a3",
    "citation_count": 11
  },
  "https://jmlr.org/papers/v23/19-918.html": {
    "title": "A Worst Case Analysis of Calibrated Label Ranking Multi-label Classification Method",
    "abstract": "Most multi-label classification methods are evaluated on real datasets, which is a good practice for comparing the performance among methods on the average scenario. Due to the large amount of factors to consider, this empirical approach does not explain, nor does show the factors impacting the performance. A reasonable way to understand some of the performanceâs factors of multi-label methods independently of the context is to find a mathematical proof about them. In this paper, mathematical proofs are given for the multi-label method ranking by pairwise comparison and its extension for classification named by calibrated label ranking, showing their performance on a worst case scenario for five multi-label metrics. The pairwise approach adopted by ranking by pairwise comparison enables the algorithm to achieve the optimal performance on Spearman rank correlation. However, the findings presented in this paper clearly show that the same pairwise approach adopted by the algorithm is also a crucial factor contributing to a very poor performance on other multi-label metrics",
    "volume": "main",
    "checked": false,
    "id": "0864bfb39aac3ec9391d62e32b7f76540aeba2e3",
    "citation_count": 0
  },
  "https://jmlr.org/papers/v23/20-021.html": {
    "title": "D-GCCA: Decomposition-based Generalized Canonical Correlation Analysis for Multi-view High-dimensional Data",
    "abstract": "Modern biomedical studies often collect multi-view data, that is, multiple types of data measured on the same set of objects. A popular model in high-dimensional multi-view data analysis is to decompose each viewâs data matrix into a low-rank common-source matrix generated by latent factors common across all data views, a low-rank distinctive-source matrix corresponding to each view, and an additive noise matrix. We propose a novel decomposition method for this model, called decomposition-based generalized canonical correlation analysis (D-GCCA). The D-GCCA rigorously defines the decomposition on the L2 space of random variables in contrast to the Euclidean dot product space used by most existing methods, thereby being able to provide the estimation consistency for the low-rank matrix recovery. Moreover, to well calibrate common latent factors, we impose a desirable orthogonality constraint on distinctive latent factors. Existing methods, however, inadequately consider such orthogonality and may thus suffer from substantial loss of undetected common-source variation. Our D-GCCA takes one step further than generalized canonical correlation analysis by separating common and distinctive components among canonical variables, while enjoying an appealing interpretation from the perspective of principal component analysis. Furthermore, we propose to use the variable-level proportion of signal variance explained by common or distinctive latent factors for selecting the variables most influenced. Consistent estimators of our D-GCCA method are established with good finite-sample numerical performance, and have closed-form expressions leading to efficient computation especially for large-scale data. The superiority of D-GCCA over state-of-the-art methods is also corroborated in simulations and real-world data examples",
    "volume": "main",
    "checked": true,
    "id": "0acd47b53f6ed07e3c4b8aa53e9c4f8415eb1c23",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v23/20-1060.html": {
    "title": "Scalable and Efficient Hypothesis Testing with Random Forests",
    "abstract": "Throughout the last decade, random forests have established themselves as among the most accurate and popular supervised learning methods. While their black-box nature has made their mathematical analysis difficult, recent work has established important statistical properties like consistency and asymptotic normality by considering subsampling in lieu of bootstrapping. Though such results open the door to traditional inference procedures, all formal methods suggested thus far place severe restrictions on the testing framework and their computational overhead often precludes their practical scientific use. Here we propose a hypothesis test to formally assess feature significance, which uses permutation tests to circumvent computationally infeasible estimates of nuisance parameters. This test is intended to be analogous to the F-test for linear regression. We establish asymptotic validity of the test via exchangeability arguments and show that the test maintains high power with orders of magnitude fewer computations. Importantly, the procedure scales easily to big data settings where large training and testing sets may be employed, conducting statistically valid inference without the need to construct additional models. Simulations and applications to ecological data, where random forests have recently shown promise, are provided",
    "volume": "main",
    "checked": true,
    "id": "a9c31376c9c503b8031806d687c819a3e247ebed",
    "citation_count": 14
  },
  "https://jmlr.org/papers/v23/20-1121.html": {
    "title": "Interlocking Backpropagation: Improving depthwise model-parallelism",
    "abstract": "The number of parameters in state of the art neural networks has drastically increased in recent years. This surge of interest in large scale neural networks has motivated the development of new distributed training strategies enabling such models. One such strategy is model-parallel distributed training. Unfortunately, model-parallelism can suffer from poor resource utilisation, which leads to wasted resources. In this work, we improve upon recent developments in an idealised model-parallel optimisation setting: local learning. Motivated by poor resource utilisation in the global setting and poor task performance in the local setting, we introduce a class of intermediary strategies between local and global learning referred to as interlocking backpropagation. These strategies preserve many of the compute-efficiency advantages of local optimisation, while recovering much of the task performance achieved by global optimisation. We assess our strategies on both image classification ResNets and Transformer language models, finding that our strategy consistently out-performs local learning in terms of task performance, and out-performs global learning in training efficiency",
    "volume": "main",
    "checked": true,
    "id": "ff56d18c46b6aa3c4fc03927ee0402ae0410522c",
    "citation_count": 5
  },
  "https://jmlr.org/papers/v23/20-1239.html": {
    "title": "Projection-free Distributed Online Learning with Sublinear Communication Complexity",
    "abstract": "To deal with complicated constraints via locally light computations in distributed online learning, a recent study has presented a projection-free algorithm called distributed online conditional gradient (D-OCG), and achieved an $O(T^{3/4})$ regret bound for convex losses, where $T$ is the number of total rounds. However, it requires $T$ communication rounds, and cannot utilize the strong convexity of losses. In this paper, we propose an improved variant of D-OCG, namely D-BOCG, which can attain the same $O(T^{3/4})$ regret bound with only $O(\\sqrt{T})$ communication rounds for convex losses, and a better regret bound of $O(T^{2/3}(\\log T)^{1/3})$ with fewer $O(T^{1/3}(\\log T)^{2/3})$ communication rounds for strongly convex losses. The key idea is to adopt a delayed update mechanism that reduces the communication complexity, and redefine the surrogate loss function in D-OCG for exploiting the strong convexity. Furthermore, we provide lower bounds to demonstrate that the $O(\\sqrt{T})$ communication rounds required by D-BOCG are optimal (in terms of $T$) for achieving the $O(T^{3/4})$ regret with convex losses, and the $O(T^{1/3}(\\log T)^{2/3})$ communication rounds required by D-BOCG are near-optimal (in terms of $T$) for achieving the $O(T^{2/3}(\\log T)^{1/3})$ regret with strongly convex losses up to polylogarithmic factors. Finally, to handle the more challenging bandit setting, in which only the loss value is available, we incorporate the classical one-point gradient estimator into D-BOCG, and obtain similar theoretical guarantees",
    "volume": "main",
    "checked": true,
    "id": "16b74aea7270a325d68dafc1d19425572feb7234",
    "citation_count": 4
  },
  "https://jmlr.org/papers/v23/20-1258.html": {
    "title": "Learning Rates as a Function of Batch Size: A Random Matrix Theory Approach to Neural Network Training",
    "abstract": "We study the effect of mini-batching on the loss landscape of deep neural networks using spiked, field-dependent random matrix theory.  We demonstrate that the magnitude of the extremal values of the batch Hessian are larger than those of the empirical Hessian. We also derive similar results for the Generalised Gauss-Newton matrix approximation of the Hessian. As a consequence of our theorems we derive an analytical expressions for the maximal learning rates as a function of batch size, informing practical training regimens for both stochastic gradient descent (linear scaling) and adaptive algorithms, such as Adam (square root scaling), for smooth, non-convex deep neural networks. Whilst the linear scaling for stochastic gradient descent has been derived under more restrictive conditions, which we generalise, the square root scaling rule for adaptive optimisers is, to our knowledge, completely novel. We validate our claims on the VGG/WideResNet architectures on the CIFAR-100 and ImageNet data sets. Based on our investigations of the sub-sampled Hessian we develop a stochastic Lanczos quadrature based on the fly learning rate and momentum learner, which avoids the need for expensive multiple evaluations for these key hyper-parameters and shows good preliminary results on the Pre-Residual Architecture for CIFAR-100. We further investigate the similarity between the Hessian spectrum of a multi-layer perceptron, trained on Gaussian mixture data, compared to that of deep neural networks trained on natural images. We find striking similarities, with both exhibiting rank degeneracy, a bulk spectrum and outliers to that spectrum. Furthermore, we show that ZCA whitening can remove such outliers early on in training before class separation occurs, but that outliers persist in later training",
    "volume": "main",
    "checked": true,
    "id": "3cf27875b5d8626aa82ecc77b27af659c7bca741",
    "citation_count": 13
  },
  "https://jmlr.org/papers/v23/20-1265.html": {
    "title": "Training and Evaluation of Deep Policies Using Reinforcement Learning and Generative Models",
    "abstract": "We present a data-efficient framework for solving sequential decision-making problems which exploits the combination of reinforcement learning (RL) and latent variable generative models. The framework, called GenRL, trains deep  policies by introducing an action latent variable such that the feed-forward policy search can be divided into two parts: (i) training a sub-policy that outputs a distribution over the action latent variable given a state of the system, and (ii) unsupervised training of a generative model that outputs a sequence of motor actions conditioned on the latent action variable. GenRL enables safe exploration and alleviates the data-inefficiency problem as it exploits prior knowledge about valid sequences of motor actions. Moreover, we provide a set of measures for evaluation of generative models such that we are able to predict the performance of the RL policy training prior to the actual training on a physical robot. We experimentally determine the characteristics of generative models that have most influence on the performance of the final policy training on two robotics tasks: shooting a hockey puck and throwing a basketball. Furthermore, we empirically demonstrate that GenRL is the only method which can safely and efficiently solve the robotics tasks compared to two state-of-the-art RL methods",
    "volume": "main",
    "checked": true,
    "id": "70caab1d6cd0d8910f79d6635fbb7edbfeb1b45a",
    "citation_count": 0
  },
  "https://jmlr.org/papers/v23/20-1353.html": {
    "title": "Improved Generalization Bounds for Adversarially Robust Learning",
    "abstract": "We consider a model of robust learning in an adversarial environment. The learner gets uncorrupted training data with access to possible corruptions that may be affected by the adversary during testing. The learner's goal is to build a robust classifier, which will be tested on future adversarial examples.  The adversary is limited to $k$ possible corruptions for each input. We model the learner-adversary interaction as a zero-sum game. This model is closely related to the adversarial examples model of Schmidt et al. (2018); Madry et al. (2017).  Our main results consist of generalization bounds for the binary and multiclass classification, as well as the real-valued case (regression). For the binary classification setting, we both tighten the generalization bound of Feige et al. (2015), and are also able to handle infinite hypothesis classes. The sample complexity is improved from $O(\\frac{1}{\\epsilon^4}\\log(\\frac{|H|}{\\delta}))$ to $O\\big(\\frac{1}{\\epsilon^2}(kVC(H)\\log^{\\frac{3}{2}+\\alpha}(kVC(H))+\\log(\\frac{1}{\\delta})\\big)$ for any $\\alpha > 0$. Additionally, we extend the algorithm and generalization bound from the binary to the multiclass and real-valued cases. Along the way, we obtain results on fat-shattering dimension and Rademacher complexity of $k$-fold maxima over function classes; these may be of independent interest.  For binary classification, the algorithm of Feige et al. (2015)  uses a regret minimization algorithm and an ERM oracle as a black box; we adapt it for the multiclass and regression settings. The algorithm provides us with near-optimal policies for the players on a given training sample",
    "volume": "main",
    "checked": true,
    "id": "14ef7f9a7c4ac71cb13435d1ad80cd2a434fadba",
    "citation_count": 7
  },
  "https://jmlr.org/papers/v23/20-1466.html": {
    "title": "Signature Moments to Characterize Laws of Stochastic Processes",
    "abstract": "The sequence of moments of a vector-valued random variable can characterize its law. We study the analogous problem for path-valued random variables, that is stochastic processes, by using so-called robust signature moments. This allows us to derive a metric of maximum mean discrepancy type for laws of stochastic processes and study the topology it induces on the space of laws of stochastic processes. This metric can be kernelized using the signature kernel which allows to efficiently compute it. As an application, we provide a non-parametric two-sample hypothesis test for laws of stochastic processes",
    "volume": "main",
    "checked": true,
    "id": "ac4f02ab7efb3c1fd48c4ad871f227e351173bfe",
    "citation_count": 77
  },
  "https://jmlr.org/papers/v23/20-219.html": {
    "title": "Asymptotic Analysis of Sampling Estimators for Randomized Numerical Linear Algebra Algorithms",
    "abstract": "The statistical analysis of Randomized Numerical Linear Algebra (RandNLA) algorithms within the past few years has mostly focused on their performance as point estimators. However, this is insufficient for conducting statistical inference, e.g., constructing confidence intervals and hypothesis testing, since the distribution of the estimator is lacking. In this article, we develop an asymptotic analysis to derive the distribution of RandNLA sampling estimators for the least-squares problem. In particular, we derive the asymptotic distribution of a general sampling estimator with arbitrary sampling probabilities in a fixed design setting. The analysis is conducted in two complementary settings, i.e., when the objective of interest is to approximate the full sample estimator, and when it is to infer the underlying ground truth model parameters. For each setting, we show that the sampling estimator is asymptotically normally distributed under mild regularity conditions. Moreover, the sampling estimator is asymptotically unbiased in both settings. Based on our asymptotic analysis, we use two criteria, the Asymptotic Mean Squared Error (AMSE) and the Expected Asymptotic Mean Squared Error (EAMSE), to identify optimal sampling probabilities. Several of these optimal sampling probability distributions are new to the literature, e.g., the root leverage sampling estimator and the predictor length sampling estimator. Our theoretical results clarify the role of leverage in the sampling process, and our empirical results demonstrate improvements over existing methods",
    "volume": "main",
    "checked": true,
    "id": "c931fc9b34af59b289f7c33e8637953ef850234c",
    "citation_count": 30
  },
  "https://jmlr.org/papers/v23/20-664.html": {
    "title": "Logarithmic Regret for Episodic Continuous-Time Linear-Quadratic Reinforcement Learning over a Finite-Time Horizon",
    "abstract": "We study finite-time horizon continuous-time linear-quadratic reinforcement learning problems in an episodic setting, where both  the state and control coefficients are unknown to the controller. We first propose a least-squares algorithm based on continuous-time observations and controls, and establish a logarithmic regret bound of magnitude $\\mathcal{O}((\\ln M)(\\ln\\ln M) )$, with $M$ being the number of learning episodes. The analysis consists of two components:  perturbation analysis, which exploits the regularity and robustness of the associated Riccati differential equation; and parameter estimation error, which relies on sub-exponential properties of continuous-time least-squares estimators. We further propose a practically implementable least-squares  algorithm based on discrete-time observations and piecewise constant controls, which achieves similar logarithmic regret with an additional term depending explicitly on the time stepsizes used in the algorithm",
    "volume": "main",
    "checked": true,
    "id": "14e5fe4515a46edb04cd6fa069c33cc823d891ce",
    "citation_count": 20
  },
  "https://jmlr.org/papers/v23/20-717.html": {
    "title": "KL-UCB-Switch: Optimal Regret Bounds for Stochastic Bandits from Both a Distribution-Dependent and a Distribution-Free Viewpoints",
    "abstract": "We consider $K$-armed stochastic bandits and consider cumulative regret bounds up to time $T$. We are interested in strategies achieving simultaneously a distribution-free regret bound of optimal order $\\sqrt{KT}$ and a distribution-dependent regret that is asymptotically optimal, that is, matching the $\\kappa \\ln T$ lower bound by Lai and Robbins (1985) and Burnetas and Katehakis (1996), where $\\kappa$ is the optimal problem-dependent constant. This constant $\\kappa$ depends on the model $\\mathcal{D}$ considered (the family of possible distributions over the arms). MÃ©nard and Garivier (2017) provided strategies achieving such a bi-optimality in the parametric case of models given by one-dimensional exponential families, while Lattimore (2016, 2018) did so for the family of (sub)Gaussian distributions with variance less than $1$. We extend this result to the non-parametric case of all distributions over $[0,1]$. We do so by combining the MOSS strategy by Audibert and Bubeck (2009), which enjoys a distribution-free regret bound of optimal order $\\sqrt{KT}$, and the KL-UCB strategy by CappÃ© et al. (2013), for which we provide in passing the first analysis of an optimal distribution-dependent $\\kappa\\ln T$ regret bound in the model of all distributions over $[0,1]$. We were able to obtain this non-parametric bi-optimality result while working hard to streamline the proofs (of previously known regret bounds and thus of the new analyses carried out); a second merit of the present contribution is therefore to provide a review of proofs of classical regret bounds for index-based strategies for $K$-armed stochastic bandits",
    "volume": "main",
    "checked": true,
    "id": "cafdca00be5c8808cd561262622603b1d01530de",
    "citation_count": 27
  },
  "https://jmlr.org/papers/v23/20-748.html": {
    "title": "Matrix Completion with Covariate Information and Informative Missingness",
    "abstract": "We study the problem of matrix completion when the missingness of the matrix entries is dependent on the unobserved response values themselves and hence the missingness itself is informative. Furthermore, we allow to take into account the covariate information to establish its relation with the response and hence enable prediction. We devise a novel procedure to simultaneously complete the partially observed matrix and assess the covariate effect. Allowing the matrix dimensions as well as the number of covariates to grow ultra-high, under the classic low-rank matrix and sparse covariate effect assumptions, we rigorously establish the statistical guarantee of our procedure and the algorithmic convergence. The method is demonstrated via simulation studies and is used to analyze a Yelp data set and a MovieLens data set",
    "volume": "main",
    "checked": true,
    "id": "8442e069cf6e4de35a0921d3f1e5592ebb16cdba",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v23/20-830.html": {
    "title": "Training Two-Layer ReLU Networks with Gradient Descent is Inconsistent",
    "abstract": "We prove that two-layer (Leaky)ReLU networks initialized by e.g. the widely used method proposed by He et al. (2015) and trained using gradient descent on a least-squares loss are not universally consistent. Specifically, we describe a large class of one-dimensional data-generating distributions for which, with high probability, gradient descent only finds a bad local minimum of the optimization landscape, since it is unable to move the biases far away from their initialization at zero. It turns out that in these cases, the found network essentially performs linear regression even if the target function is non-linear. We further provide numerical evidence that this happens in practical situations, for some multi-dimensional distributions and that stochastic gradient descent exhibits similar behavior. We also provide empirical results on how the choice of initialization and optimizer can influence this behavior",
    "volume": "main",
    "checked": true,
    "id": "6c958f4f8e8074ecf7f3e44c794040faf0e834ca",
    "citation_count": 6
  },
  "https://jmlr.org/papers/v23/20-964.html": {
    "title": "Extensions to the Proximal Distance Method of Constrained Optimization",
    "abstract": "The current paper studies the problem of minimizing a loss $f(\\boldsymbol{x})$ subject to constraints of the form $\\boldsymbol{D}\\boldsymbol{x} \\in S$, where $S$ is a closed set, convex or not, and $\\boldsymbol{D}$ is a matrix that fuses parameters. Fusion constraints can capture smoothness, sparsity, or more general constraint patterns. To tackle this generic class of problems, we combine the Beltrami-Courant penalty method of optimization with the proximal distance principle. The latter is driven by minimization of penalized objectives $f(\\boldsymbol{x})+\\frac{\\rho}{2}\\text{dist}(\\boldsymbol{D}\\boldsymbol{x},S)^2$ involving large tuning constants $\\rho$ and the squared Euclidean distance of $\\boldsymbol{D}\\boldsymbol{x}$ from $S$. The next iterate $\\boldsymbol{x}_{n+1}$ of the corresponding proximal distance algorithm is constructed from the current iterate $\\boldsymbol{x}_n$ by minimizing the majorizing surrogate function $f(\\boldsymbol{x})+\\frac{\\rho}{2}\\|\\boldsymbol{D}\\boldsymbol{x}-\\mathcal{P}_{S}(\\boldsymbol{D}\\boldsymbol{x}_n)\\|^2$. For fixed $\\rho$ and a subanalytic loss $f(\\boldsymbol{x})$ and a subanalytic constraint set $S$, we prove convergence to a stationary point. Under stronger assumptions, we provide convergence rates and demonstrate linear local convergence.  We also construct a steepest descent variant to avoid costly linear system solves. To benchmark our algorithms, we compare their results to those delivered by the alternating direction method of multipliers. Our extensive numerical tests include problems on metric projection, convex regression, convex clustering, total variation image denoising, and projection of a matrix to a good condition number. These experiments demonstrate the superior speed and acceptable accuracy of our steepest variant on high-dimensional problems",
    "volume": "main",
    "checked": true,
    "id": "2f192ce59e4756e0ccffc88a9d0a728ed2930e92",
    "citation_count": 8
  },
  "https://jmlr.org/papers/v23/21-0078.html": {
    "title": "Boulevard: Regularized Stochastic Gradient Boosted Trees and Their Limiting Distribution",
    "abstract": "This paper examines a novel gradient boosting framework for regression. We regularize gradient boosted trees by introducing subsampling and employ a modified shrinkage algorithm so that at every boosting stage the estimate is given by an average of trees. The resulting algorithm, titled \"Boulevard'\", is shown to converge as the number of trees grows. This construction allows us to demonstrate a central limit theorem for this limit, providing a characterization of uncertainty for predictions. A simulation study and real world examples provide support for both the predictive accuracy of the model and its limiting behavior",
    "volume": "main",
    "checked": true,
    "id": "fff01d36f93675c917e362494f28506046c83bc6",
    "citation_count": 11
  },
  "https://jmlr.org/papers/v23/21-0190.html": {
    "title": "Statistical Optimality and Stability of Tangent Transform Algorithms in Logit Models",
    "abstract": "A systematic approach to finding variational approximation in an otherwise intractable non-conjugate model is to exploit the general principle of convex duality by minorizing the marginal likelihood that renders the problem tractable. While such approaches are popular in the context of variational inference in non-conjugate Bayesian models, theoretical guarantees on statistical optimality and algorithmic convergence are lacking. Focusing on logistic regression models, we provide mild conditions on the data generating process to derive non-asymptotic upper bounds to the risk incurred by the variational optima. We demonstrate that these assumptions can be completely relaxed if one considers a slight variation of the algorithm by raising the likelihood to a fractional power. Next, we utilize the theory of dynamical systems to provide convergence guarantees for such algorithms in logistic and multinomial logit regression. In particular, we establish local asymptotic stability of the algorithm without any assumptions on the data-generating process. We explore a special case involving a semi-orthogonal design under which a global convergence is obtained. The theory is further illustrated using several numerical studies",
    "volume": "main",
    "checked": true,
    "id": "d5f44a9eef2f7f7e1c5658b4a5857fc520553c7c",
    "citation_count": 2
  },
  "https://jmlr.org/papers/v23/21-0211.html": {
    "title": "A Primer for Neural Arithmetic Logic Modules",
    "abstract": "Neural Arithmetic Logic Modules have become a growing area of interest, though remain a niche field. These modules are neural networks which aim to achieve systematic generalisation in learning arithmetic and/or logic operations such as $\\{+, -, \\times, \\div, \\leq, \\textrm{AND}\\}$ while also being interpretable. This paper is the first in discussing the current state of progress of this field, explaining key works, starting with the Neural Arithmetic Logic Unit (NALU). Focusing on the shortcomings of the NALU, we provide an in-depth analysis to reason about design choices of recent modules. A cross-comparison between modules is made on experiment setups and findings, where we highlight inconsistencies in a fundamental experiment causing the inability to directly compare across papers. To alleviate the existing inconsistencies, we create a benchmark which compares all existing arithmetic NALMs. We finish by providing a novel discussion of existing applications for NALU and research directions requiring further exploration",
    "volume": "main",
    "checked": true,
    "id": "def7c67c3688bc459fbef3ce50a466a0cbf411a9",
    "citation_count": 5
  },
  "https://jmlr.org/papers/v23/21-0218.html": {
    "title": "Estimating Density Models with Truncation Boundaries using Score Matching",
    "abstract": "Truncated densities are probability density functions defined on truncated domains. They share the same parametric form with their non-truncated counterparts up to a normalizing constant. Since the computation of their normalizing constants is usually infeasible, Maximum Likelihood Estimation cannot be easily applied to estimate truncated density models. Score Matching (SM) is a powerful tool for fitting parameters using only unnormalized models. However, it cannot be directly applied here as boundary conditions that derive a tractable SM objective are not satisfied by truncated densities. This paper studies parameter estimation for truncated probability densities using SM. The estimator minimizes a weighted Fisher divergence. The weight function is simply the shortest distance from a data point to the domain's boundary. We show this choice of weight function naturally arises from minimizing the Stein discrepancy and upper bounding the finite-sample estimation error. We demonstrate the usefulness of our method via numerical experiments and a study on the Chicago crime data set. We also show that the proposed density estimation can correct the outlier-trimming bias caused by aggressive outlier detection methods",
    "volume": "main",
    "checked": true,
    "id": "1f5f4ce5487b91b6110ff531c95a7ca074dbe1be",
    "citation_count": 5
  },
  "https://jmlr.org/papers/v23/21-0222.html": {
    "title": "Adversarial Classification: Necessary Conditions and Geometric Flows",
    "abstract": "We study a version of adversarial classification where an adversary is empowered to corrupt data inputs up to some distance $\\varepsilon$, using tools from variational analysis. In particular, we describe necessary conditions associated with the optimal classifier subject to such an adversary. Using the necessary conditions, we derive a geometric evolution equation which can be used to track the change in classification boundaries as $\\varepsilon$ varies. This evolution equation may be described as an uncoupled system of differential equations in one dimension, or as a mean curvature type equation in higher dimension. In one dimension, and under mild assumptions on the data distribution, we rigorously prove that one can use the initial value problem starting from $\\varepsilon=0$, which is simply the Bayes classifier, in order to solve for the global minimizer of the adversarial problem for small values of $\\varepsilon$. In higher dimensions we provide a similar result, albeit conditional to the existence of regular solutions of the initial value problem. In the process of proving our main results we obtain a result of independent interest connecting the original adversarial problem with an optimal transport problem under no assumptions on whether classes are balanced or not. Numerical examples illustrating these ideas are also presented",
    "volume": "main",
    "checked": true,
    "id": "58feef30d55a56e172f4852e903afa3763ecc021",
    "citation_count": 13
  },
  "https://jmlr.org/papers/v23/21-0296.html": {
    "title": "Active Structure Learning of Bayesian Networks in an Observational Setting",
    "abstract": "We study active structure learning of Bayesian networks in an observational setting, in which there are external limitations on the number of variable values that can be observed from the same sample. Random samples are drawn from the joint distribution of the network variables, and the algorithm iteratively selects which variables to observe in the next   sample. We propose a new active learning algorithm for this setting, that finds with a high probability a structure with a score that is $\\epsilon$-close to the optimal score. We show that for a class of distributions that we term stable, a sample complexity reduction of up to a factor of $\\widetilde{\\Omega}(d^3)$ can be obtained, where $d$ is the number of network variables. We further show that in the worst case, the sample complexity of the active algorithm is guaranteed to be almost the same as that of a naive baseline algorithm. To supplement the theoretical results, we report experiments that compare the performance of the new active algorithm to the naive baseline and demonstrate the sample complexity improvements. Code for the algorithm and for the experiments is provided at https://github.com/noabdavid/activeBNSL",
    "volume": "main",
    "checked": true,
    "id": "0f1e74656f170f8c9e2e6ac9aa6bf6556e30ea81",
    "citation_count": 3
  },
  "https://jmlr.org/papers/v23/21-0308.html": {
    "title": "Learning to Optimize: A Primer and A Benchmark",
    "abstract": "Learning to optimize (L2O) is an emerging approach that leverages machine learning to develop optimization methods, aiming at reducing the laborious iterations of hand engineering. It automates the design of an optimization method based on its performance on a set of training problems. This data-driven procedure generates methods that can efficiently solve problems similar to those in training. In sharp contrast, the typical and traditional designs of optimization methods are theory-driven, so they obtain performance guarantees over the classes of problems specified by the theory. The difference makes L2O suitable for repeatedly solving a particular optimization problem over a specific distribution of data, while it typically fails on out-of-distribution problems. The practicality of L2O depends on the type of target optimization, the chosen architecture of the method to learn, and the training procedure. This new paradigm has motivated a community of researchers to explore L2O and report their findings. This article is poised to be the first comprehensive survey and benchmark of L2O for continuous optimization. We set up taxonomies, categorize existing works and research directions, present insights, and identify open challenges. We benchmarked many existing L2O approaches on a few representative optimization problems. For reproducible research and fair benchmarking purposes, we released our software implementation and data in the package Open-L2O at https://github.com/VITA-Group/Open-L2O",
    "volume": "main",
    "checked": true,
    "id": "7816a6ad4826954e88ce0ff17ab1b34572d3607d",
    "citation_count": 98
  },
  "https://jmlr.org/papers/v23/21-0402.html": {
    "title": "Clustering with Semidefinite Programming and Fixed Point Iteration",
    "abstract": "We introduce a novel method for clustering using a semidefinite programming (SDP) relaxation of the Max k-Cut problem.  The approach is based on a new methodology for rounding the solution of an SDP relaxation using iterated linear optimization.  We show the vertices of the Max k-Cut relaxation correspond to partitions of the data into at most k sets.  We also show the vertices are attractive fixed points of iterated linear optimization.  Each step of this iterative process solves a relaxation of the closest vertex problem and leads to a new clustering problem where the underlying clusters are more clearly defined.  Our experiments show that using fixed point iteration for rounding the Max k-Cut SDP relaxation leads to significantly better results when compared to randomized rounding",
    "volume": "main",
    "checked": true,
    "id": "8756ce3d175b319fd5591084fc4f852e49bae13e",
    "citation_count": 0
  },
  "https://jmlr.org/papers/v23/21-0431.html": {
    "title": "Deep Limits and a Cut-Off Phenomenon for Neural Networks",
    "abstract": "We consider dynamical and geometrical aspects of deep learning. For many standard choices of layer maps we display semi-invariant metrics which quantify differences between data or decision functions. This allows us, when considering random layer maps and using non-commutative ergodic theorems, to deduce that certain limits exist when letting the number of layers tend to infinity. We also examine the random initialization of standard networks where we observe a surprising cut-off phenomenon in terms of the number of layers, the depth of the network. This could be a relevant parameter when choosing an appropriate number of layers for a given learning task, or for selecting a good initialization procedure. More generally, we hope that the notions and results in this paper can provide a framework, in particular a geometric one, for a part of the theoretical understanding of deep neural networks",
    "volume": "main",
    "checked": true,
    "id": "2608061d541a244262a03720da26908accddb583",
    "citation_count": 3
  },
  "https://jmlr.org/papers/v23/21-0545.html": {
    "title": "A Bregman Learning Framework for Sparse Neural Networks",
    "abstract": "We propose a learning framework based on stochastic Bregman iterations, also known as mirror descent, to train sparse neural networks with an inverse scale space approach. We derive a baseline algorithm called LinBreg, an accelerated version using momentum, and AdaBreg, which is a Bregmanized generalization of the Adam algorithm. In contrast to established methods for sparse training the proposed family of algorithms constitutes a regrowth strategy for neural networks that is solely optimization-based without additional heuristics.  Our Bregman learning framework starts the training with very few initial parameters, successively adding only significant ones to obtain a sparse and expressive network. The proposed approach is extremely easy and efficient, yet supported by the rich mathematical theory of inverse scale space methods. We derive a statistically profound sparse parameter initialization strategy and provide a rigorous stochastic convergence analysis of the loss decay and additional convergence proofs in the convex regime. Using only $3.4\\%$ of the parameters of ResNet-18 we achieve $90.2\\%$ test accuracy on CIFAR-10, compared to $93.6\\%$ using the dense network. Our algorithm also unveils an autoencoder architecture for a denoising task. The proposed framework also has a huge potential for integrating sparse backpropagation and resource-friendly training. Code is available at https://github.com/TimRoith/BregmanLearning",
    "volume": "main",
    "checked": true,
    "id": "e47da8038b5b44d6452c92e3e4641f90f3b5ecff",
    "citation_count": 9
  },
  "https://jmlr.org/papers/v23/21-0570.html": {
    "title": "Gaussian process regression: Optimality, robustness, and relationship with kernel ridge regression",
    "abstract": "Gaussian process regression is widely used in many fields, for example, machine learning, reinforcement learning and uncertainty quantification. One key component of Gaussian process regression is the unknown correlation function, which needs to be specified. In this paper, we investigate what would happen if the correlation function is misspecified. We derive upper and lower error bounds for Gaussian process regression with possibly misspecified correlation functions.  We find that when the sampling scheme is quasi-uniform, the optimal convergence rate can be attained even if the smoothness of the imposed correlation function exceeds that of the true correlation function. We also obtain convergence rates of kernel ridge regression with misspecified kernel function, where the underlying truth is a deterministic function. Our study reveals a close connection between the convergence rates of Gaussian process regression and kernel ridge regression, which is aligned with the relationship between sample paths of Gaussian process and the corresponding reproducing kernel Hilbert space. This work establishes a bridge between Bayesian learning based on Gaussian process and frequentist kernel methods with reproducing kernel Hilbert space",
    "volume": "main",
    "checked": true,
    "id": "2a0403db648e7bc934a3416d179135d2554fc806",
    "citation_count": 5
  },
  "https://jmlr.org/papers/v23/21-0577.html": {
    "title": "Uniform deconvolution for Poisson Point Processes",
    "abstract": "We focus on the estimation of the intensity of a Poisson process in the presence of a uniform noise. We propose a kernel-based procedure fully calibrated in theory and practice. We show that our adaptive estimator is optimal from the oracle and minimax points of view, and provide new lower bounds when the intensity belongs to a Sobolev ball.  By developing the Goldenshluger-Lepski methodology in the case of deconvolution for Poisson processes, we propose an optimal data-driven selection of the kernel bandwidth. Our method is illustrated on the spatial distribution of replication origins and sequence motifs along the human genome",
    "volume": "main",
    "checked": true,
    "id": "8a772847d99207cbb0c321858c364e99bdb1e4d8",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v23/21-0711.html": {
    "title": "Distributed Bootstrap for Simultaneous Inference Under High Dimensionality",
    "abstract": "We propose a distributed bootstrap method for simultaneous inference on high-dimensional massive data that are stored and processed with many machines. The method produces an $\\ell_\\infty$-norm confidence region based on a communication-efficient de-biased lasso, and we propose an efficient cross-validation approach to tune the method at every iteration. We theoretically prove a lower bound on the number of communication rounds $\\tau_{\\min}$ that warrants the statistical accuracy and efficiency. Furthermore, $\\tau_{\\min}$ only increases logarithmically with the number of workers and the intrinsic dimensionality, while nearly invariant to the nominal dimensionality. We test our theory by extensive simulation studies, and a variable screening task on a semi-synthetic dataset based on the US Airline On-Time Performance dataset. The code to reproduce the numerical results is available in Supplementary Material",
    "volume": "main",
    "checked": true,
    "id": "823d232ff89b015046dac6b453ad01e5e8c9b59b",
    "citation_count": 5
  },
  "https://jmlr.org/papers/v23/21-0716.html": {
    "title": "Universal Approximation Theorems for Differentiable Geometric Deep Learning",
    "abstract": "This paper addresses the growing need to process non-Euclidean data, by introducing a geometric deep learning (GDL) framework for building universal feedforward-type models compatible with differentiable manifold geometries. We show that our GDL models can approximate any continuous target function uniformly on compact sets of a controlled maximum diameter. We obtain curvature-dependent lower-bounds on this maximum diameter and upper-bounds on the depth of our approximating GDL models. Conversely, we find that there is always a continuous function between any two non-degenerate compact manifolds that any \"locally-defined\" GDL model cannot uniformly approximate. Our last main result identifies data-dependent conditions guaranteeing that the GDL model implementing our approximation breaks \"the curse of dimensionality.\" We find that any \"real-world\" (i.e. finite) dataset always satisfies our condition and, conversely, any dataset satisfies our requirement if the target function is smooth. As applications, we confirm the universal approximation capabilities of the following GDL models: Ganea et al. (2018)'s hyperbolic feedforward networks, the architecture implementing Krishnan et al. (2015)'s deep Kalman-Filter, and deep softmax classifiers. We build universal extensions/variants of: the SPD-matrix regressor of Meyer et al. (2011), and Fletcher (2003)'s Procrustean regressor. In the Euclidean setting, our results imply a quantitative version of Kidger and Lyons (2020)'s approximation theorem and a data-dependent version of Yarotsky and Zhevnerchuk (2019)'s uncursed approximation rates",
    "volume": "main",
    "checked": true,
    "id": "75c22db09c17f36cff0a1d47a446db734fd7e7c8",
    "citation_count": 16
  },
  "https://jmlr.org/papers/v23/21-0738.html": {
    "title": "InterpretDL: Explaining Deep Models in PaddlePaddle",
    "abstract": "Techniques to explain the predictions of deep neural networks (DNNs) have been largely required for gaining insights into the black boxes. We introduce InterpretDL, a toolkit of explanation algorithms based on PaddlePaddle, with uniformed programming interfaces and \"plug-and-play\" designs. A few lines of codes are needed to obtain the explanation results without modifying the structure of the model. InterpretDL currently contains 16 algorithms, explaining training phases, datasets, global and local behaviors of post-trained deep models. InterpretDL also provides a number of tutorial examples and showcases to demonstrate the capability of InterpretDL working on a wide range of deep learning models, e.g., Convolutional Neural Networks (CNNs), Multi-Layer Preceptors (MLPs), Transformers, etc., for various tasks in both Computer Vision (CV) and Natural Language Processing (NLP). Furthermore, InterpretDL modularizes the implementations, making efforts to support the compatibility across frameworks. The project is available at https://github.com/PaddlePaddle/InterpretDL",
    "volume": "main",
    "checked": true,
    "id": "57aa8eb01f1da8136396648c862f29a0cc54c775",
    "citation_count": 6
  },
  "https://jmlr.org/papers/v23/21-0739.html": {
    "title": "Meta-analysis of heterogeneous data: integrative sparse regression in high-dimensions",
    "abstract": "We consider the task of meta-analysis in high-dimensional settings in which the data sources  are similar but non-identical. To borrow strength across such heterogeneous datasets, we introduce a global parameter that emphasizes interpretability and statistical efficiency in the presence of heterogeneity. We also propose a one-shot estimator of the global parameter that preserves the anonymity of the data sources and converges at a rate that depends on the size of the combined dataset.   For high-dimensional linear model settings, we demonstrate the superiority of our  identification restrictions in adapting to a previously seen data distribution as well as predicting for a new/unseen data distribution. Finally, we demonstrate the benefits of our approach on a large-scale drug treatment dataset involving several different cancer cell-lines",
    "volume": "main",
    "checked": true,
    "id": "b651bbcc1d3f44d23021b0ec01eb70d3aab95c48",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v23/21-0755.html": {
    "title": "A Forward Approach for Sufficient Dimension Reduction in Binary Classification",
    "abstract": "Since the proposal of the seminal sliced inverse regression (SIR), inverse-type methods have proved to be canonical in sufficient dimension reduction (SDR). However, they often underperform in binary classification because the binary responses yield two slices at most. In this article, we develop a forward SDR approach in binary classification based on weighted large-margin classifiers. First, we show that the gradient of a large-margin classifier is unbiased for SDR as long as the corresponding loss function is Fisher consistent. This leads us to propose the weighted outer-product of gradients (wOPG) estimator. The wOPG estimator can recover the central subspace exhaustively without linearity (or constant variance) conditions, which despite being routinely required, they are untestable assumption. We propose the gradient-based formulation for the large-margin classifier to estimate the gradient function of the classifier directly. We also establish the consistency of the proposed wOPG estimator and demonstrate its promising finite-sample performance through both simulated and real data examples",
    "volume": "main",
    "checked": false,
    "id": "eed3b7ec67e5e9c04ea2a78cff550355c6fc86e4",
    "citation_count": 0
  },
  "https://jmlr.org/papers/v23/21-0795.html": {
    "title": "A Nonconvex Framework for Structured Dynamic Covariance Recovery",
    "abstract": "We propose a flexible, yet interpretable model for high-dimensional data with time-varying second-order statistics, motivated and applied to functional neuroimaging data. Our approach implements the neuroscientific hypothesis of discrete cognitive processes by factorizing covariances into sparse spatial and smooth temporal components. Although this factorization results in parsimony and domain interpretability, the resulting estimation problem is nonconvex. We design a two-stage optimization scheme with a tailored spectral initialization, combined with iteratively refined alternating projected gradient descent. We\r\nprove a linear convergence rate up to a nontrivial statistical error for the proposed descent scheme and establish sample complexity guarantees for the estimator. Empirical results using simulated data and brain imaging data illustrate that our approach outperforms existing baselines",
    "volume": "main",
    "checked": true,
    "id": "889e25bc8a6ba7ead028098a90a0b9bba6d05fc3",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v23/21-0819.html": {
    "title": "Three rates of convergence or separation via U-statistics in a dependent framework",
    "abstract": "Despite the ubiquity of U-statistics in modern Probability and Statistics, their non-asymptotic analysis in a dependent framework may have been overlooked. In a recent work, a new concentration inequality for U-statistics of order two for uniformly ergodic discrete time Markov chains has been proved. In this paper, we put this theoretical breakthrough into action by pushing further the current state of knowledge in three different active fields of research. First, we establish a new exponential inequality for the estimation of spectra of integral operators with MCMC methods. The novelty is that this result holds for kernels with positive and negative eigenvalues, which is new as far as we know. In addition, we investigate generalization performance of online algorithms working with pairwise loss functions and Markov chain samples. We provide an online-to-batch conversion result by showing how we can extract a low risk hypothesis from the sequence of hypotheses generated by any online learner. We finally give a non-asymptotic analysis of a goodness-of-fit test on the density of the stationary measure of a Markov chain. We identify some classes of alternatives over which our test based on the $L^2$ distance has a prescribed power",
    "volume": "main",
    "checked": true,
    "id": "4f44f42718489e34d27bc97d5f3d59e70873b9fd",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v23/21-1060.html": {
    "title": "abess: A Fast Best-Subset Selection Library in Python and R",
    "abstract": "We introduce a new library named abess that implements a unified framework of best-subset selection for solving diverse machine learning problems, e.g., linear regression, classification, and principal component analysis. Particularly, abess certifiably gets the optimal solution within polynomial time with high probability under the linear model. Our efficient implementation allows abess to attain the solution of best-subset selection problems as fast as or even 20x faster than existing competing variable (model) selection toolboxes. Furthermore, it supports common variants like best subset of groups selection and $\\ell_2$ regularized best-subset selection. The core of the library is programmed in C++. For ease of use, a Python library is designed for convenient integration with scikit-learn, and it can be installed from the Python Package Index (PyPI). In addition, a user-friendly R library is available at the Comprehensive R Archive Network (CRAN). The source code is available at: https://github.com/abess-team/abess",
    "volume": "main",
    "checked": false,
    "id": "ec5b2f80eb4fcec20a4df0cff776c1aa32fcb72d",
    "citation_count": 3
  },
  "https://jmlr.org/papers/v23/21-1065.html": {
    "title": "Testing Whether a Learning Procedure is Calibrated",
    "abstract": "A learning procedure takes as input a dataset and performs inference for the parameters $\\theta$ of a model that is assumed to have given rise to the dataset. Here we consider learning procedures whose output is a probability distribution, representing uncertainty about $\\theta$ after seeing the dataset. Bayesian inference is a prime example of such a procedure, but one can also construct other learning procedures that return distributional output. This paper studies conditions for a learning procedure to be considered calibrated, in the sense that the true data-generating parameters are plausible as samples from its distributional output. A learning procedure whose inferences and predictions are systematically over- or under-confident will fail to be calibrated. On the other hand, a learning procedure that is calibrated need not be statistically efficient. A hypothesis-testing framework is developed in order to assess, using simulation, whether a learning procedure is calibrated. Several vignettes are presented to illustrate different aspects of the framework",
    "volume": "main",
    "checked": true,
    "id": "7b74837342c8646d308c2fdf86cdf736985b9ce2",
    "citation_count": 7
  },
  "https://jmlr.org/papers/v23/21-1120.html": {
    "title": "Selective Machine Learning of the Average Treatment Effect with an Invalid Instrumental Variable",
    "abstract": "Instrumental variable  methods have been widely used to identify causal effects in the presence of unmeasured confounding. A key identification condition known as the exclusion restriction states that the instrument cannot have a direct effect on the outcome which is not mediated by the exposure in view. In the health and social sciences, such an assumption is often not credible. To address this concern, we consider identification conditions of the population average treatment effect with an invalid instrumental variable which does not satisfy  the exclusion restriction, and derive the efficient influence function targeting the identifying functional under a nonparametric observed data model. We propose a novel multiply robust locally efficient estimator of the average treatment effect that is consistent in the union of multiple parametric nuisance models, as well as a multiply debiased machine learning estimator for which the nuisance parameters are estimated using generic machine learning methods, that effectively exploit various forms of linear or nonlinear structured sparsity in the nuisance parameter space. When one cannot be confident that any of these machine learners is consistent at sufficiently fast rates to ensure $\\surd{n}$-consistency for the average treatment effect, we introduce new criteria for selective machine learning which leverage the multiple robustness property in order to ensure small bias. The proposed methods are illustrated through extensive simulations  and a data analysis evaluating the causal effect of 401(k) participation on savings",
    "volume": "main",
    "checked": true,
    "id": "626f1bf034fea2588def912dd9bddc2e4f131914",
    "citation_count": 3
  },
  "https://jmlr.org/papers/v23/21-1128.html": {
    "title": "Contraction rates for sparse variational approximations in Gaussian process regression",
    "abstract": "We study the theoretical properties of a variational Bayes method in the Gaussian Process regression model. We consider the inducing variables method and derive sufficient conditions for obtaining contraction rates for the corresponding variational Bayes (VB) posterior. As examples we show that for three particular covariance kernels (MatÃ©rn, squared exponential, random series prior) the VB approach can achieve optimal, minimax contraction rates for a sufficiently large number of appropriately chosen inducing variables. The theoretical findings are demonstrated by numerical experiments",
    "volume": "main",
    "checked": true,
    "id": "856c8f861d9f3871330a7bac1f541971320226c9",
    "citation_count": 10
  },
  "https://jmlr.org/papers/v23/21-1146.html": {
    "title": "Stochastic DCA with Variance Reduction and Applications in Machine Learning",
    "abstract": "We design stochastic Difference-of-Convex-functions Algorithms (DCA) for solving a class of structured  Difference-of-Convex-functions (DC) problems. As the standard DCA requires the full information of (sub)gradients which could be expensive in large-scale settings, stochastic approaches rely upon stochastic information instead. However, stochastic estimations generate additional variance terms making stochastic algorithms unstable. Therefore, we integrate some novel variance reduction techniques including SVRG and SAGA into our design. The almost sure convergence to critical points of the proposed algorithms is established and the algorithms' complexities are analyzed. To study the efficiency of our algorithms, we apply them to three important problems in machine learning: nonnegative principal component analysis, group variable selection in multiclass logistic regression, and sparse linear regression. Numerical experiments have shown the merits of our proposed algorithms in comparison with other state-of-the-art stochastic methods for solving nonconvex large-sum problems",
    "volume": "main",
    "checked": true,
    "id": "4d7e3977d66238faf6783780240e06bea9fe191d",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v23/21-1148.html": {
    "title": "Nonconvex Matrix Completion with Linearly Parameterized Factors",
    "abstract": "Techniques of matrix completion aim to impute a large portion of missing entries in a data matrix through a small portion of observed ones. In practice, prior information and special structures are usually employed in order to improve the accuracy of matrix completion. In this paper, we propose a unified nonconvex optimization framework for matrix completion with linearly parameterized factors. In particular, by introducing a condition referred to as Correlated Parametric Factorization, we conduct a unified geometric analysis for the nonconvex objective by establishing uniform upper bounds for low-rank estimation resulting from any local minimizer. Perhaps surprisingly, the condition of Correlated Parametric Factorization holds for important examples including subspace-constrained matrix completion and skew-symmetric matrix completion. The effectiveness of our unified nonconvex optimization method is also empirically illustrated by extensive numerical simulations",
    "volume": "main",
    "checked": true,
    "id": "843a81b7ecce898507fa7b5516d94dc7bf36727b",
    "citation_count": 1
  },
  "https://jmlr.org/papers/v23/21-1197.html": {
    "title": "tntorch: Tensor Network Learning with PyTorch",
    "abstract": "We present tntorch, a tensor learning framework that supports multiple decompositions (including Candecomp/Parafac, Tucker, and Tensor Train) under a unified interface. With our library, the user can learn and handle low-rank tensors with automatic differentiation, seamless GPU support, and the convenience of PyTorch's API. Besides decomposition algorithms, tntorch implements differentiable tensor algebra, rank truncation, cross-approximation, batch processing, comprehensive tensor arithmetics, and more",
    "volume": "main",
    "checked": true,
    "id": "b4a9f7bbcc133f724959d3933f3294f4b913a0b8",
    "citation_count": 12
  },
  "https://jmlr.org/papers/v23/21-1251.html": {
    "title": "Ranking and Tuning Pre-trained Models: A New Paradigm for Exploiting Model Hubs",
    "abstract": "Model hubs with many pre-trained models (PTMs) have become a cornerstone of deep learning. Although built at a high cost, they remain under-exploited---practitioners usually pick one PTM from the provided model hub by popularity and then fine-tune the PTM to solve the target task. This na\\\"ive but common practice poses two obstacles to full exploitation of pre-trained model hubs: first, the PTM selection by popularity has no optimality guarantee, and second, only one PTM is used while the remaining PTMs are ignored. An alternative might be to consider all possible combinations of PTMs and extensively fine-tune each combination, but this would not only be prohibitive computationally but may also lead to statistical over-fitting. In this paper, we propose a new paradigm for exploiting model hubs that is intermediate between these extremes.  The paradigm is characterized by two aspects: (1) We use an evidence maximization procedure to estimate the maximum value of label evidence given features extracted by pre-trained models.  This procedure can rank all the PTMs in a model hub for various types of PTMs and tasks before fine-tuning. (2) The best ranked PTM can either be fine-tuned and deployed if we have no preference for the model's architecture or the target PTM can be tuned by the top $K$ ranked PTMs via a Bayesian procedure that we propose. This procedure, which we refer to as B-Tuning, not only improves upon specialized methods designed for tuning homogeneous PTMs, but also applies to the challenging problem of tuning heterogeneous PTMs where it yields a new level of benchmark performance",
    "volume": "main",
    "checked": false,
    "id": "752e38d9978f146bd7e3ec0bf9b583b4c9d845b2",
    "citation_count": 11
  },
  "https://jmlr.org/papers/v23/21-1262.html": {
    "title": "A Unified Statistical Learning Model for Rankings and Scores with Application to Grant Panel Review",
    "abstract": "Rankings and scores are two common data types used by judges to express preferences and/or perceptions of quality in a collection of objects. Numerous models exist to study data of each type separately, but no unified statistical model captures both data types simultaneously without first performing data conversion. We propose the Mallows-Binomial model to close this gap, which combines a Mallows $\\phi$ ranking model with Binomial score models through shared parameters that quantify object quality, a consensus ranking, and the level of consensus among judges. We propose an efficient tree-search algorithm to calculate the exact MLE of model parameters, study statistical properties of the model both analytically and through simulation, and apply our model to real data from an instance of grant panel review that collected both scores and partial rankings. Furthermore, we demonstrate how model outputs can be used to rank objects with confidence. The proposed model is shown to sensibly combine information from both scores and rankings to quantify object quality and measure consensus with appropriate levels of statistical uncertainty",
    "volume": "main",
    "checked": true,
    "id": "244321e8d420cc1a4f7f560ec5397fc0b71f3a86",
    "citation_count": 8
  },
  "https://jmlr.org/papers/v23/21-1273.html": {
    "title": "Efficient Inference for Dynamic Flexible Interactions of Neural Populations",
    "abstract": "Hawkes process provides an effective statistical framework for analyzing the interactions of neural spiking activities. Although utilized in many real applications, the classic Hawkes process is incapable of modeling inhibitory interactions among neural population. Instead, the nonlinear Hawkes process allows for modeling a more flexible influence pattern with excitatory or inhibitory interactions. This work proposes a flexible nonlinear Hawkes process variant based on sigmoid nonlinearity. To ease inference, three sets of auxiliary latent variables (Polya-Gamma variables, latent marked Poisson processes and sparsity variables) are augmented to make functional connection weights appear in a Gaussian form, which enables simple iterative algorithms with analytical updates. As a result, the efficient Gibbs sampler, expectation-maximization algorithm and mean-field approximation are derived to estimate the interactions among neural populations. Furthermore, to reconcile with time-varying neural systems, the proposed time-invariant model is extended to a dynamic version by introducing a Markov state process. Similarly, three analytical iterative inference algorithms: Gibbs sampler, EM algorithm and mean-field approximation are derived. We compare the accuracy and efficiency of these inference algorithms on synthetic data, and further experiment on real neural recordings to demonstrate that the developed models achieve superior performance over the state-of-the-art competitors",
    "volume": "main",
    "checked": true,
    "id": "e35dfb56ae93897f8d2e8af092155dc0d53cea36",
    "citation_count": 2
  },
  "https://jmlr.org/papers/v23/21-138.html": {
    "title": "Multi-Agent Multi-Armed Bandits with Limited Communication",
    "abstract": "We consider the problem where $N$ agents collaboratively interact with an instance of a stochastic $K$ arm bandit problem for $K \\gg N$. The agents aim to simultaneously minimize the cumulative regret over all the agents for a total of $T$ time steps, the number of communication rounds, and the number of bits in each communication round.  We present Limited Communication Collaboration - Upper Confidence Bound (LCC-UCB), a doubling-epoch based algorithm where each agent communicates only after the end of the epoch and shares the index of the best arm it knows. With our algorithm, LCC-UCB, each agent enjoys a regret of $\\tilde{O}\\left(\\sqrt{({K/N}+ N)T}\\right)$, communicates for $O(\\log T)$ steps and broadcasts $O(\\log K)$ bits in each communication step. We extend the work to sparse graphs with maximum degree $K_G$ and diameter $D$ to propose LCC-UCB-GRAPH which enjoys a regret bound of $\\tilde{O}\\left(D\\sqrt{(K/N+ K_G)DT}\\right)$. Finally, we empirically show that the LCC-UCB and the LCC-UCB-GRAPH algorithms perform well and outperform strategies that communicate through a central node",
    "volume": "main",
    "checked": true,
    "id": "668ec4e9bd5048befa2dce8cfa781ce1b3da4f43",
    "citation_count": 15
  }
}