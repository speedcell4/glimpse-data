{
  "https://openreview.net/forum?id=iKPC7N85Pf": {
    "title": "Predicting the Encoding Error of SIRENs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeremy Vonderfecht",
      "Feng Liu"
    ]
  },
  "https://openreview.net/forum?id=ydPHjgf6h0": {
    "title": "Adversarial Imitation Learning from Visual Observations using Latent Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vittorio Giammarino",
      "James Queeney",
      "Ioannis Paschalidis"
    ]
  },
  "https://openreview.net/forum?id=PUpZXvNqmb": {
    "title": "From Identifiable Causal Representations to Controllable Counterfactual Generation: A Survey on Causal Generative Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aneesh Komanduri",
      "Xintao Wu",
      "Yongkai Wu",
      "Feng Chen"
    ]
  },
  "https://openreview.net/forum?id=dltUedmUVT": {
    "title": "Temporal Difference Learning with Compressed Updates: Error-Feedback meets Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aritra Mitra",
      "George J. Pappas",
      "Hamed Hassani"
    ]
  },
  "https://openreview.net/forum?id=9UgUMFW67X": {
    "title": "Genetic InfoMax: Exploring Mutual Information Maximization in High-Dimensional Imaging Genetics Studies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaochen Xie",
      "Ziqian Xie",
      "Sheikh Muhammad Saiful Islam",
      "Degui Zhi",
      "Shuiwang Ji"
    ]
  },
  "https://openreview.net/forum?id=k3d5C0YvfK": {
    "title": "Enhancing Compositional Generalization via Compositional Feature Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxiang Wang",
      "Haozhe Si",
      "Huajie Shao",
      "Han Zhao"
    ]
  },
  "https://openreview.net/forum?id=PIL3YWXmx2": {
    "title": "Statistical and Computational Complexities of BFGS Quasi-Newton Method for Generalized Linear Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiujiang Jin",
      "Tongzheng Ren",
      "Nhat Ho",
      "Aryan Mokhtari"
    ]
  },
  "https://openreview.net/forum?id=C3FXHxMVuq": {
    "title": "CascadedGaze: Efficiency in Global Context Extraction for Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amirhosein Ghasemabadi",
      "Muhammad Kamran Janjua",
      "Mohammad Salameh",
      "CHUNHUA ZHOU",
      "Fengyu Sun",
      "Di Niu"
    ]
  },
  "https://openreview.net/forum?id=57ETChLAOE": {
    "title": "Revisiting stochastic submodular maximization with cardinality constraint: A bandit perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pratik Jawanpuria",
      "Bamdev Mishra",
      "Karthik Gurumoorthy"
    ]
  },
  "https://openreview.net/forum?id=FI1XvwpchC": {
    "title": "[Re] Explaining Temporal Graph Models through an Explorer-Navigator Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miklós Hamar",
      "Matey Krastev",
      "Kristiyan Danielov Hristov",
      "David Beglou"
    ]
  },
  "https://openreview.net/forum?id=EE1CBKC0SZ": {
    "title": "TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongfu Jiang",
      "Yishan Li",
      "Ge Zhang",
      "Wenhao Huang",
      "Bill Yuchen Lin",
      "Wenhu Chen"
    ]
  },
  "https://openreview.net/forum?id=098mb06uhA": {
    "title": "Extreme Risk Mitigation in Reinforcement Learning using Extreme Value Theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karthik Somayaji NS",
      "Yu Wang",
      "Malachi Schram",
      "Jan Drgona",
      "Mahantesh M Halappanavar",
      "Frank Liu",
      "Peng Li"
    ]
  },
  "https://openreview.net/forum?id=TdJ7lpzAkD": {
    "title": "Interpretable Additive Tabular Transformer Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anton Frederik Thielmann",
      "Arik Reuter",
      "Thomas Kneib",
      "David Rügamer",
      "Benjamin Säfken"
    ]
  },
  "https://openreview.net/forum?id=zO4aAVHxPe": {
    "title": "Geometrical aspects of lattice gauge equivariant convolutional neural networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David I. Müller",
      "Jimmy Aronsson",
      "Daniel Schuh"
    ]
  },
  "https://openreview.net/forum?id=r2dx1s1lqG": {
    "title": "Boosting Data-Driven Mirror Descent with Randomization, Equivariance, and Acceleration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hong Ye Tan",
      "Subhadip Mukherjee",
      "Junqi Tang",
      "Carola-Bibiane Schönlieb"
    ]
  },
  "https://openreview.net/forum?id=PGLbZpVk2n": {
    "title": "Causal Discovery from Time Series with Hybrids of Constraint-Based and Noise-Based Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daria Bystrova",
      "Charles K. Assaad",
      "Julyan Arbel",
      "Emilie Devijver",
      "Eric Gaussier",
      "Wilfried Thuiller"
    ]
  },
  "https://openreview.net/forum?id=LHl2I2rWZa": {
    "title": "GLASU: A Communication-Efficient Algorithm for Federated Learning with Vertically Distributed Graph Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinwei Zhang",
      "Mingyi Hong",
      "Jie Chen"
    ]
  },
  "https://openreview.net/forum?id=KLojVqdj2y": {
    "title": "Training Graph Neural Networks Subject to a Tight Lipschitz Constraint",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simona Ioana Juvina",
      "Ana Antonia Neacșu",
      "Jérôme Rony",
      "Jean-Christophe Pesquet",
      "Corneliu Burileanu",
      "Ismail Ben Ayed"
    ]
  },
  "https://openreview.net/forum?id=zOKAmm8R9B": {
    "title": "GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mianchu Wang",
      "Rui Yang",
      "Xi Chen",
      "Hao Sun",
      "Meng Fang",
      "Giovanni Montana"
    ]
  },
  "https://openreview.net/forum?id=fdyHzoGT8g": {
    "title": "Depth Scaling in Graph Neural Networks: Understanding the Flat Curve Behavior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diana Gomes",
      "Kyriakos Efthymiadis",
      "Ann Nowe",
      "Peter Vrancx"
    ]
  },
  "https://openreview.net/forum?id=WHAmxfLjeJ": {
    "title": "Understanding Smoothness of Vector Gaussian Processes on Product Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emilio Porcu",
      "Ana Paula Peron",
      "Eugenio Massa",
      "Xavier Emery"
    ]
  },
  "https://openreview.net/forum?id=kxHIK4x8qc": {
    "title": "MAGDiff: Covariate Data Set Shift Detection via Activation Graphs of Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charles Arnal",
      "Felix Hensel",
      "Mathieu Carrière",
      "Théo Lacombe",
      "Hiroaki Kurihara",
      "Yuichi Ike",
      "Frederic Chazal"
    ]
  },
  "https://openreview.net/forum?id=oCfamUtecN": {
    "title": "Regret Bounds for Noise-Free Cascaded Kernelized Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Li",
      "Jonathan Scarlett"
    ]
  },
  "https://openreview.net/forum?id=KLBD13bsVl": {
    "title": "The Interplay of Uncertainty Modeling and Deep Active Learning: An Empirical Analysis in Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Denis Huseljic",
      "Marek Herde",
      "Yannick Nagel",
      "Lukas Rauch",
      "Paulius Strimaitis",
      "Bernhard Sick"
    ]
  },
  "https://openreview.net/forum?id=RZPN8cgqST": {
    "title": "InduCE: Inductive Counterfactual Explanations for Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samidha Verma",
      "Burouj Armgaan",
      "Sourav Medya",
      "Sayan Ranu"
    ]
  },
  "https://openreview.net/forum?id=7VB5db72lr": {
    "title": "G4SATBench: Benchmarking and Advancing SAT Solving with Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoyu Li",
      "Jinpei Guo",
      "Xujie Si"
    ]
  },
  "https://openreview.net/forum?id=vsez76EAV8": {
    "title": "PaDPaF: Partial Disentanglement with Partially-Federated GANs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdulla Jasem Almansoori",
      "Samuel Horváth",
      "Martin Takáč"
    ]
  },
  "https://openreview.net/forum?id=KVUtlM60HM": {
    "title": "Archetypal Analysis++: Rethinking the Initialization Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Mair",
      "Jens Sjölund"
    ]
  },
  "https://openreview.net/forum?id=8UfhCZjOV7": {
    "title": "[Re] On the Reproducibility of Post-Hoc Concept Bottleneck Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nesta Midavaine",
      "Gregory Hok Tjoan Go",
      "Diego Canez",
      "Ioana Simion",
      "Satchit Chatterji"
    ]
  },
  "https://openreview.net/forum?id=vxxi7xzzn7": {
    "title": "Data Pruning Can Do More: A Comprehensive Data Pruning Approach for Object Re-identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zi Yang",
      "Haojin Yang",
      "Soumajit Majumder",
      "Jorge Cardoso",
      "Guillermo Gallego"
    ]
  },
  "https://openreview.net/forum?id=kPIU8PnJPo": {
    "title": "Scaling Vision-and-Language Navigation With Offline RL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Valay Bundele",
      "Mahesh Bhupati",
      "Biplab Banerjee",
      "Aditya Grover"
    ]
  },
  "https://openreview.net/forum?id=2bURaH6RN8": {
    "title": "Momentum-Based Policy Gradient with Second-Order Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saber Salehkaleybar",
      "Mohammadsadegh Khorasani",
      "Negar Kiyavash",
      "Niao He",
      "Patrick Thiran"
    ]
  },
  "https://openreview.net/forum?id=HNqEKZDDRc": {
    "title": "Offline Reinforcement Learning via Tsallis Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingwei Zhu",
      "Matthew Kyle Schlegel",
      "Han Wang",
      "Martha White"
    ]
  },
  "https://openreview.net/forum?id=AYJ3m7BocI": {
    "title": "A Survey on Transferability of Adversarial Examples Across Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jindong Gu",
      "Xiaojun Jia",
      "Pau de Jorge",
      "Wenqian Yu",
      "Xinwei Liu",
      "Avery Ma",
      "Yuan Xun",
      "Anjun Hu",
      "Ashkan Khakzar",
      "Zhijiang Li",
      "Xiaochun Cao",
      "Philip Torr"
    ]
  },
  "https://openreview.net/forum?id=XHEhjDxPDl": {
    "title": "Solving Inverse Problems with Model Mismatch using Untrained Neural Networks within Model-based Architectures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peimeng Guan",
      "Naveed Iqbal",
      "Mark A. Davenport",
      "Mudassir Masood"
    ]
  },
  "https://openreview.net/forum?id=qc2lmWkvk4": {
    "title": "Hybrid Federated Learning for Feature & Sample Heterogeneity: Algorithms and Implementation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinwei Zhang",
      "Wotao Yin",
      "Mingyi Hong",
      "Tianyi Chen"
    ]
  },
  "https://openreview.net/forum?id=epcLNhkoEL": {
    "title": "Fixed Budget Best Arm Identification in Unimodal Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debamita Ghosh",
      "Manjesh Kumar Hanawal",
      "Nikola Zlatanov"
    ]
  },
  "https://openreview.net/forum?id=yf4ciZcgrg": {
    "title": "Restricted Random Pruning at Initialization for High Compression Range",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hikari Otsuka",
      "Yasuyuki Okoshi",
      "Ángel López García-Arias",
      "Kazushi Kawamura",
      "Thiem Van Chu",
      "Daichi Fujiki",
      "Masato Motomura"
    ]
  },
  "https://openreview.net/forum?id=zdtSqZnkx1": {
    "title": "Continual HyperTransformer: A Meta-Learner for Continual Few-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Max Vladymyrov",
      "Andrey Zhmoginov",
      "Mark Sandler"
    ]
  },
  "https://openreview.net/forum?id=qItxVbWyfe": {
    "title": "Federated Learning with Convex Global and Local Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuan He",
      "Le Peng",
      "Ju Sun"
    ]
  },
  "https://openreview.net/forum?id=cueEUSG7lE": {
    "title": "Group Fairness in Reinforcement Learning via Multi-Objective Rewards",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jack Blandin",
      "Ian A. Kash"
    ]
  },
  "https://openreview.net/forum?id=oyISaaeHwD": {
    "title": "On Good Practices for Task-Specific Distillation of Large Pretrained Visual Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juliette Marrie",
      "Michael Arbel",
      "Julien Mairal",
      "Diane Larlus"
    ]
  },
  "https://openreview.net/forum?id=aVOzWH1Nc5": {
    "title": "Dynamic Online Ensembles of Basis Expansions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Waxman",
      "Petar Djuric"
    ]
  },
  "https://openreview.net/forum?id=p1a6ruIZCT": {
    "title": "IMEX-Reg: Implicit-Explicit Regularization in the Function Space for Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prashant Shivaram Bhat",
      "Bharath Chennamkulam Renjith",
      "Elahe Arani",
      "Bahram Zonooz"
    ]
  },
  "https://openreview.net/forum?id=N0Sc0KY0AH": {
    "title": "Improving Subgraph-GNNs via Edge-Level Ego-Network Encodings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nurudin Alvarez-Gonzalez",
      "Andreas Kaltenbrunner",
      "Vicenç Gómez"
    ]
  },
  "https://openreview.net/forum?id=KleJZ9ZzYw": {
    "title": "DP-ImgSyn: Dataset Alignment for Obfuscated, Differentially Private Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Efstathia Soufleri",
      "Deepak Ravikumar",
      "Kaushik Roy"
    ]
  },
  "https://openreview.net/forum?id=WYGiqSVstK": {
    "title": "Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksandar Stanić",
      "Sergi Caelles",
      "Michael Tschannen"
    ]
  },
  "https://openreview.net/forum?id=Wiklo5VpG7": {
    "title": "From Stability to Chaos: Analyzing Gradient Descent Dynamics in Quadratic Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuxing Chen",
      "Krishna Balasubramanian",
      "Promit Ghosal",
      "Bhavya Kumar Agrawalla"
    ]
  },
  "https://openreview.net/forum?id=Io3jDUC4DP": {
    "title": "Using Skew to Assess the Quality of GAN-generated Image Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Luzi",
      "Helen Jenne",
      "Carlos Ortiz Marrero",
      "Ryan Murray"
    ]
  },
  "https://openreview.net/forum?id=JQoWmeNaC2": {
    "title": "Reproducibility Study of \"Explaining RL Decisions with Trajectories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clio Feng",
      "Colin Bot",
      "Bart den Boef",
      "Bart Aaldering"
    ]
  },
  "https://openreview.net/forum?id=raD846nj2q": {
    "title": "Identify Ambiguous Tasks Combining Crowdsourced Labels by Weighting Areas Under the Margin",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanguy Lefort",
      "Benjamin Charlier",
      "Alexis Joly",
      "Joseph Salmon"
    ]
  },
  "https://openreview.net/forum?id=mrJi5kdKA4": {
    "title": "DSI2I: Dense Style for Unpaired Exemplar-based Image-to- Image Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baran Ozaydin",
      "Tong Zhang",
      "Sabine Susstrunk",
      "Mathieu Salzmann"
    ]
  },
  "https://openreview.net/forum?id=yL15ys5swq": {
    "title": "Improving Diffusion Models for Scene Text Editing with Dual Encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiabao Ji",
      "Guanhua Zhang",
      "Zhaowen Wang",
      "Bairu Hou",
      "Zhifei Zhang",
      "Brian L. Price",
      "Shiyu Chang"
    ]
  },
  "https://openreview.net/forum?id=ongi2oe3Fr": {
    "title": "Continuous U-Net: Faster, Greater and Noiseless",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun-Wun Cheng",
      "Christina Runkel",
      "Lihao Liu",
      "Raymond H. Chan",
      "Carola-Bibiane Schönlieb",
      "Angelica I Aviles-Rivero"
    ]
  },
  "https://openreview.net/forum?id=hw7inQwRxB": {
    "title": "Decentralized Decoupled Training for Federated Long-Tailed Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenkai Yang",
      "Deli Chen",
      "Hao Zhou",
      "Fandong Meng",
      "Jie Zhou",
      "Xu Sun"
    ]
  },
  "https://openreview.net/forum?id=bzTfO4mURl": {
    "title": "FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiran Xu",
      "Zeyu Wang",
      "Jieru Mei",
      "Liangqiong Qu",
      "Alan Yuille",
      "Cihang Xie",
      "Yuyin Zhou"
    ]
  },
  "https://openreview.net/forum?id=Egb0tUZnOY": {
    "title": "Understanding Sparse Neural Networks from their Topology via Multipartite Graph Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elia Cunegatti",
      "Matteo Farina",
      "Doina Bucur",
      "Giovanni Iacca"
    ]
  },
  "https://openreview.net/forum?id=m1OXBLH0dH": {
    "title": "Stochastic Direct Search Methods for Blind Resource Allocation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juliette Achddou",
      "Olivier Cappé",
      "Aurélien Garivier"
    ]
  },
  "https://openreview.net/forum?id=aHk3vctnf1": {
    "title": "Routers in Vision Mixture of Experts: An Empirical Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianlin Liu",
      "Mathieu Blondel",
      "Carlos Riquelme Ruiz",
      "Joan Puigcerver"
    ]
  },
  "https://openreview.net/forum?id=6rWuWbVmgz": {
    "title": "Sketch and shift: a robust decoder for compressive clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ayoub Belhadji",
      "Rémi Gribonval"
    ]
  },
  "https://openreview.net/forum?id=y9IDfODRns": {
    "title": "Inference from Real-World Sparse Measurements",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arnaud Pannatier",
      "Kyle Matoba",
      "François Fleuret"
    ]
  },
  "https://openreview.net/forum?id=iulMde3dP1": {
    "title": "What Has Been Overlooked in Contrastive Source-Free Domain Adaptation: Leveraging Source-Informed Latent Augmentation within Neighborhood Context",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Wang",
      "Wonho Bae",
      "Jiahong Chen",
      "Kuangen Zhang",
      "Leonid Sigal",
      "Clarence W. de Silva"
    ]
  },
  "https://openreview.net/forum?id=dwFRov8xhr": {
    "title": "E-Valuating Classifier Two-Sample Tests",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Teodora Pandeva",
      "Tim Bakker",
      "Christian A. Naesseth",
      "Patrick Forré"
    ]
  },
  "https://openreview.net/forum?id=z5AXLMBWdU": {
    "title": "Semantic Positive Pairs for Enhancing Visual Representation Learning of Instance Discrimination methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Alkhalefi",
      "Georgios Leontidis",
      "Mingjun Zhong"
    ]
  },
  "https://openreview.net/forum?id=Ew73inSyhG": {
    "title": "What do larger image classifiers memorise?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michal Lukasik",
      "Vaishnavh Nagarajan",
      "Ankit Singh Rawat",
      "Aditya Krishna Menon",
      "Sanjiv Kumar"
    ]
  },
  "https://openreview.net/forum?id=PtBzWCaCYB": {
    "title": "Integrated Variational Fourier Features for Fast Spatial Modelling with Gaussian Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Talay M Cheema",
      "Carl Edward Rasmussen"
    ]
  },
  "https://openreview.net/forum?id=P1vzXDklar": {
    "title": "Time Series Continuous Modeling for Imputation and Forecasting with Implicit Neural Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Etienne Le Naour",
      "Louis Serrano",
      "Léon Migus",
      "Yuan Yin",
      "Ghislain Agoua",
      "Nicolas Baskiotis",
      "patrick gallinari",
      "Vincent Guigue"
    ]
  },
  "https://openreview.net/forum?id=tP1PBrMUlX": {
    "title": "Synthesizing Libraries of Programs with Auxiliary Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Habibur Rahman",
      "Thirupathi Reddy Emireddy",
      "Kenneth Tjhia",
      "Elham Parhizkar",
      "Levi Lelis"
    ]
  },
  "https://openreview.net/forum?id=4KLwep6mA1": {
    "title": "Choosing Wisely and Learning Deeply: Selective Cross-Modality Distillation via CLIP for Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jixuan Leng",
      "Yijiang Li",
      "Haohan Wang"
    ]
  },
  "https://openreview.net/forum?id=EBNJ33Fcrl": {
    "title": "Anticipatory Music Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "John Thickstun",
      "David Leo Wright Hall",
      "Chris Donahue",
      "Percy Liang"
    ]
  },
  "https://openreview.net/forum?id=ZFZnvGXXMm": {
    "title": "Fooling Contrastive Language-Image Pre-Trained Models with CLIPMasterPrints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthias Freiberger",
      "Peter Kun",
      "Christian Igel",
      "Anders Sundnes Løvlie",
      "Sebastian Risi"
    ]
  },
  "https://openreview.net/forum?id=HSQTv3R8Iz": {
    "title": "A True-to-the-model Axiomatic Benchmark for Graph-based Explainers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Corrado Monti",
      "Paolo Bajardi",
      "Francesco Bonchi",
      "André Panisson",
      "Alan Perotti"
    ]
  },
  "https://openreview.net/forum?id=8bnsoL2IyJ": {
    "title": "Variance-aware decision making with linear function approximation under heavy-tailed rewards",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Li",
      "Qiang Sun"
    ]
  },
  "https://openreview.net/forum?id=QSvb6jBXML": {
    "title": "ZigZag: Universal Sampling-free Uncertainty Estimation Through Two-Step Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikita Durasov",
      "Nik Dorndorf",
      "Hieu Le",
      "Pascal Fua"
    ]
  },
  "https://openreview.net/forum?id=wTGjn7JvYK": {
    "title": "On the Optimization and Generalization of Multi-head Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Puneesh Deora",
      "Rouzbeh Ghaderi",
      "Hossein Taheri",
      "Christos Thrampoulidis"
    ]
  },
  "https://openreview.net/forum?id=uCZJaqJchs": {
    "title": "Personalised Federated Learning On Heterogeneous Feature Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alain Rakotomamonjy",
      "Maxime Vono",
      "Hamlet Jesse Medina Ruiz",
      "Liva Ralaivola"
    ]
  },
  "https://openreview.net/forum?id=qYceFeHgm4": {
    "title": "MUBen: Benchmarking the Uncertainty of Molecular Representation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinghao Li",
      "Lingkai Kong",
      "Yuanqi Du",
      "Yue Yu",
      "Yuchen Zhuang",
      "Wenhao Mu",
      "Chao Zhang"
    ]
  },
  "https://openreview.net/forum?id=BRl7fqMwaJ": {
    "title": "GSURE-Based Diffusion Model Training with Corrupted Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bahjat Kawar",
      "Noam Elata",
      "Tomer Michaeli",
      "Michael Elad"
    ]
  },
  "https://openreview.net/forum?id=2la55BeWwy": {
    "title": "A note on regularised NTK dynamics with an application to PAC-Bayesian training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eugenio Clerico",
      "Benjamin Guedj"
    ]
  },
  "https://openreview.net/forum?id=PuhF0hyDq1": {
    "title": "New Evaluation Metrics Capture Quality Degradation due to LLM Watermarking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karanpartap Singh",
      "James Zou"
    ]
  },
  "https://openreview.net/forum?id=YY2iA0hfia": {
    "title": "Does Representation Similarity Capture Function Similarity?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Hayne",
      "Heejung Jung",
      "R. Carter"
    ]
  },
  "https://openreview.net/forum?id=TZdEgwZ6f3": {
    "title": "Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Seale Smith",
      "Yen-Chang Hsu",
      "Lingyu Zhang",
      "Ting Hua",
      "Zsolt Kira",
      "Yilin Shen",
      "Hongxia Jin"
    ]
  },
  "https://openreview.net/forum?id=3kYgouAfqk": {
    "title": "BP($\\mathbf{\\lambda}$): Online Learning via Synthetic Gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joseph Oliver Pemberton",
      "Rui Ponte Costa"
    ]
  },
  "https://openreview.net/forum?id=kZFKwApeQO": {
    "title": "GUARD: A Safe Reinforcement Learning Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiye Zhao",
      "Yifan Sun",
      "Feihan Li",
      "Rui Chen",
      "Ruixuan Liu",
      "Tianhao Wei",
      "Changliu Liu"
    ]
  },
  "https://openreview.net/forum?id=IzmLJ1t49R": {
    "title": "Incremental Extractive Opinion Summarization Using Cover Trees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Somnath Basu Roy Chowdhury",
      "Nicholas Monath",
      "Kumar Avinava Dubey",
      "Manzil Zaheer",
      "Andrew McCallum",
      "Amr Ahmed",
      "Snigdha Chaturvedi"
    ]
  },
  "https://openreview.net/forum?id=qunyX9WYr6": {
    "title": "Persistent Local Homology in Graph Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghua Wang",
      "Yan HU",
      "Ziyun Huang",
      "Di Wang",
      "Jinhui Xu"
    ]
  },
  "https://openreview.net/forum?id=I8FMYa2BdP": {
    "title": "Adversarially Robust Spiking Neural Networks Through Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ozan Ozdenizci",
      "Robert Legenstein"
    ]
  },
  "https://openreview.net/forum?id=axBIMcGZn9": {
    "title": "Continual Learning: Applications and the Road Forward",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eli Verwimp",
      "Rahaf Aljundi",
      "Shai Ben-David",
      "Matthias Bethge",
      "Andrea Cossu",
      "Alexander Gepperth",
      "Tyler L. Hayes",
      "Eyke Hüllermeier",
      "Christopher Kanan",
      "Dhireesha Kudithipudi",
      "Christoph H. Lampert",
      "Martin Mundt",
      "Razvan Pascanu",
      "Adrian Popescu",
      "Andreas S. Tolias",
      "Joost van de Weijer",
      "Bing Liu",
      "Vincenzo Lomonaco",
      "Tinne Tuytelaars",
      "Gido M van de Ven"
    ]
  },
  "https://openreview.net/forum?id=ekvsBtCBUK": {
    "title": "Anomaly detection with semi-supervised classification based on risk estimators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Le Thi Khanh Hien",
      "Sukanya Patra",
      "Souhaib Ben Taieb"
    ]
  },
  "https://openreview.net/forum?id=2M9CUnYnBA": {
    "title": "Exponential Moving Average of Weights in Deep Learning: Dynamics and Benefits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Morales-Brotons",
      "Thijs Vogels",
      "Hadrien Hendrikx"
    ]
  },
  "https://openreview.net/forum?id=qH4YFMyhce": {
    "title": "Scalable Hierarchical Self-Attention with Learnable Hierarchy for Long-Range Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thuan Nguyen Anh Trang",
      "Khang Nhat Ngo",
      "Hugo Sonnery",
      "Thieu Vo",
      "Siamak Ravanbakhsh",
      "Truong Son Hy"
    ]
  },
  "https://openreview.net/forum?id=AoOi9Zgdsv": {
    "title": "The Cross-entropy of Piecewise Linear Probability Density Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom S. F. Haines"
    ]
  },
  "https://openreview.net/forum?id=QvipGVdE6L": {
    "title": "3D Molecular Generation via Virtual Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuqi Lu",
      "Lin Yao",
      "Xi Chen",
      "Hang Zheng",
      "Di He",
      "Guolin Ke"
    ]
  },
  "https://openreview.net/forum?id=KokkP2nQ24": {
    "title": "How good is Good-Turing for Markov samples?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prafulla Chandra",
      "Andrew Thangaraj",
      "Nived Rajaraman"
    ]
  },
  "https://openreview.net/forum?id=NgK5etmhz9": {
    "title": "State-wise Constrained Policy Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiye Zhao",
      "Rui Chen",
      "Yifan Sun",
      "Feihan Li",
      "Tianhao Wei",
      "Changliu Liu"
    ]
  },
  "https://openreview.net/forum?id=3ludyxPbb6": {
    "title": "Provable Membership Inference Privacy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zachary Izzo",
      "Jinsung Yoon",
      "Sercan O Arik",
      "James Zou"
    ]
  },
  "https://openreview.net/forum?id=1fbTGC3BUD": {
    "title": "Adaptive Conformal Regression with Split-Jackknife+ Scores",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolas Deutschmann",
      "Mattia Rigotti",
      "Maria Rodriguez Martinez"
    ]
  },
  "https://openreview.net/forum?id=5psgQEHn6t": {
    "title": "Neural networks can be FLOP-efficient integrators of 1D oscillatory integrands",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anshuman Sinha",
      "Spencer H Bryngelson"
    ]
  },
  "https://openreview.net/forum?id=RGewtLtvHz": {
    "title": "Towards generalizing deep-audio fake detection networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantin Gasenzer",
      "Moritz Wolter"
    ]
  },
  "https://openreview.net/forum?id=t4nnCi5AO6": {
    "title": "Scaling (Down) CLIP: A Comprehensive Analysis of Data,Architecture, and Training Strategies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichao Li",
      "Cihang Xie",
      "Ekin Dogus Cubuk"
    ]
  },
  "https://openreview.net/forum?id=w4DXLzBPPw": {
    "title": "Low-Rank Tensor-Network Encodings for Video-to-Action Behavioral Cloning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian Chen",
      "Doruk Aksoy",
      "David J Gorsich",
      "Shravan Veerapaneni",
      "Alex Gorodetsky"
    ]
  },
  "https://openreview.net/forum?id=DIGkJhGeqi": {
    "title": "EHRDiff : Exploring Realistic EHR Synthesis with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyi Yuan",
      "Songchi Zhou",
      "Sheng Yu"
    ]
  },
  "https://openreview.net/forum?id=iBgmoMTlaz": {
    "title": "Understanding Fairness Surrogate Functions in Algorithmic Fairness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Yao",
      "Zhanke Zhou",
      "Zhicong Li",
      "Bo Han",
      "Yong Liu"
    ]
  },
  "https://openreview.net/forum?id=BkEqk7pS1I": {
    "title": "Finite-Time Analysis of Entropy-Regularized Neural Natural Actor-Critic Algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Semih Cayci",
      "Niao He",
      "R. Srikant"
    ]
  },
  "https://openreview.net/forum?id=cPDVjsOytS": {
    "title": "PopulAtion Parameter Averaging (PAPA)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexia Jolicoeur-Martineau",
      "Emy Gervais",
      "Kilian FATRAS",
      "Yan Zhang",
      "Simon Lacoste-Julien"
    ]
  },
  "https://openreview.net/forum?id=Y4YWzBiTEV": {
    "title": "The Missing U for Efficient Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sergio Calvo Ordoñez",
      "Chun-Wun Cheng",
      "Jiahao Huang",
      "Lipei Zhang",
      "Guang Yang",
      "Carola-Bibiane Schönlieb",
      "Angelica I Aviles-Rivero"
    ]
  },
  "https://openreview.net/forum?id=N05OnQG1BA": {
    "title": "CoDeC: Communication-Efficient Decentralized Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sakshi Choudhary",
      "Sai Aparna Aketi",
      "Gobinda Saha",
      "Kaushik Roy"
    ]
  },
  "https://openreview.net/forum?id=75OwvzZZBT": {
    "title": "Bias Amplification Enhances Minority Group Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaotang Li",
      "Jiarui Liu",
      "Wei Hu"
    ]
  },
  "https://openreview.net/forum?id=j5T4pcLbcY": {
    "title": "Fast and Expressive Gesture Recognition using a Combination-Homomorphic Electromyogram Encoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niklas Smedemark-Margulies",
      "Yunus Bicer",
      "Elifnur Sunger",
      "Tales Imbiriba",
      "Eugene Tunik",
      "Deniz Erdogmus",
      "Mathew Yarossi",
      "Robin Walters"
    ]
  },
  "https://openreview.net/forum?id=kxYqgSkH8I": {
    "title": "Optimization with Access to Auxiliary Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "El Mahdi Chayti",
      "Sai Praneeth Karimireddy"
    ]
  },
  "https://openreview.net/forum?id=zn3fB4VVF0": {
    "title": "Navigating Noise: A Study of How Noise Influences Generalisation and Calibration of Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martin Ferianc",
      "Ondrej Bohdal",
      "Timothy Hospedales",
      "Miguel R. D. Rodrigues"
    ]
  },
  "https://openreview.net/forum?id=OyXS4ZIqd3": {
    "title": "On the Robustness of Neural Collapse and the Neural Collapse of Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingtong Su",
      "Ya Shi Zhang",
      "Nikolaos Tsilivis",
      "Julia Kempe"
    ]
  },
  "https://openreview.net/forum?id=bZ80b0wb9d": {
    "title": "Discrete Graph Auto-Encoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoann Boget",
      "Magda Gregorova",
      "Alexandros Kalousis"
    ]
  },
  "https://openreview.net/forum?id=wE9kpJSemv": {
    "title": "Indexed Minimum Empirical Divergence-Based Algorithms for Linear Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Bian",
      "Vincent Y. F. Tan"
    ]
  },
  "https://openreview.net/forum?id=Fu4mwB0XIU": {
    "title": "PID Control-Based Self-Healing to Improve the Robustness of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuotong Chen",
      "Zihu Wang",
      "Yifan Yang",
      "Qianxiao Li",
      "Zheng Zhang"
    ]
  },
  "https://openreview.net/forum?id=c5o4HUypqm": {
    "title": "Synthetic data shuffling accelerates the convergence of federated learning under data heterogeneity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Li",
      "Yasin Esfandiari",
      "Mikkel N. Schmidt",
      "Tommy Sonne Alstrøm",
      "Sebastian U Stich"
    ]
  },
  "https://openreview.net/forum?id=LO02YHxrxd": {
    "title": "Graph Neural Networks Formed via Layer-wise Ensembles of Heterogeneous Base Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiuhai Chen",
      "Jonas Mueller",
      "Vassilis N. Ioannidis",
      "Tom Goldstein",
      "David Wipf"
    ]
  },
  "https://openreview.net/forum?id=lQE2AcbYge": {
    "title": "Online Continuous Hyperparameter Optimization for Generalized Linear Contextual Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Kang",
      "Cho-Jui Hsieh",
      "Thomas Lee"
    ]
  },
  "https://openreview.net/forum?id=VBAKc4DtZ1": {
    "title": "Faster Convergence of Local SGD for Over-Parameterized Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiancheng Qin",
      "S. Rasoul Etesami",
      "Cesar A Uribe"
    ]
  },
  "https://openreview.net/forum?id=zOGJxw07Z6": {
    "title": "Asynchronous Training Schemes in Distributed Learning with Time Delay",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxiang Wang",
      "Zhanhong Jiang",
      "Chao Liu",
      "Soumik Sarkar",
      "Dongxiang Jiang",
      "Young M Lee"
    ]
  },
  "https://openreview.net/forum?id=qNGo6ghWFB": {
    "title": "Merging by Matching Models in Task Parameter Subspaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Derek Tam",
      "Mohit Bansal",
      "Colin Raffel"
    ]
  },
  "https://openreview.net/forum?id=6yzIuqKGnq": {
    "title": "INSPIRE: Incorporating Diverse Feature Preferences in Recourse",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prateek Yadav",
      "Peter Hase",
      "Mohit Bansal"
    ]
  },
  "https://openreview.net/forum?id=1QjCzP0KIw": {
    "title": "Unsupervised 3D Scene Representation Learning via Movable Object Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Honglin Chen",
      "Wanhee Lee",
      "Hong-Xing Yu",
      "Rahul Mysore Venkatesh",
      "Joshua B. Tenenbaum",
      "Daniel Bear",
      "Jiajun Wu",
      "Daniel LK Yamins"
    ]
  },
  "https://openreview.net/forum?id=OycfV3Mhfq": {
    "title": "Convergence Analysis of Fractional Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashwani Aggarwal"
    ]
  },
  "https://openreview.net/forum?id=i02A009I5a": {
    "title": "VidEdit: Zero-Shot and Spatially Aware Text-Driven Video Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Couairon",
      "Clément Rambour",
      "Jean-Emmanuel HAUGEARD",
      "Nicolas THOME"
    ]
  },
  "https://openreview.net/forum?id=oYIjw37pTP": {
    "title": "An optimal control perspective on diffusion-based generative modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julius Berner",
      "Lorenz Richter",
      "Karen Ullrich"
    ]
  },
  "https://openreview.net/forum?id=xqAVkqrLjx": {
    "title": "HQ-VAE: Hierarchical Discrete Representation Learning with Variational Bayes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhta Takida",
      "Yukara Ikemiya",
      "Takashi Shibuya",
      "Kazuki Shimada",
      "Woosung Choi",
      "Chieh-Hsin Lai",
      "Naoki Murata",
      "Toshimitsu Uesaka",
      "Kengo Uchida",
      "Wei-Hsiang Liao",
      "Yuki Mitsufuji"
    ]
  },
  "https://openreview.net/forum?id=chbRsWwjax": {
    "title": "InfoNCE is variational inference in a recognition parameterised model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Laurence Aitchison",
      "Stoil Krasimirov Ganev"
    ]
  },
  "https://openreview.net/forum?id=cgSXpAR4Gl": {
    "title": "Optimal Transport Perturbations for Safe Reinforcement Learning with Robustness Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Queeney",
      "Erhan Can Ozcan",
      "Ioannis Paschalidis",
      "Christos Cassandras"
    ]
  },
  "https://openreview.net/forum?id=mhawjZcmrJ": {
    "title": "New Guarantees for Learning Revenue Maximizing Menus of Lotteries and Two-Part Tariffs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maria Florina Balcan",
      "Hedyeh Beyhaghi"
    ]
  },
  "https://openreview.net/forum?id=Xxw0edFFQC": {
    "title": "Optical Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxwell Anderson",
      "Shi-Yuan Ma",
      "Tianyu Wang",
      "Logan Wright",
      "Peter McMahon"
    ]
  },
  "https://openreview.net/forum?id=Db5c3Wxj9E": {
    "title": "Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Lei",
      "Dongkuan Xu",
      "Ruqi Zhang",
      "Bani Mallick"
    ]
  },
  "https://openreview.net/forum?id=WbbgOHpoPX": {
    "title": "Revisiting Random Weight Perturbation for Efficiently Improving Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Li",
      "Qinghua Tao",
      "Weihao Yan",
      "Yingwen Wu",
      "Zehao Lei",
      "Kun Fang",
      "Mingzhen He",
      "Xiaolin Huang"
    ]
  },
  "https://openreview.net/forum?id=HxfqTdLIRF": {
    "title": "Double Descent and Overfitting under Noisy Inputs and Distribution Shift for Linear Denoisers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chinmaya Kausik",
      "Kashvi Srivastava",
      "Rishi Sonthalia"
    ]
  },
  "https://openreview.net/forum?id=daXqjb6dVE": {
    "title": "From Differential Privacy to Bounds on Membership Inference: Less can be More",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anvith Thudi",
      "Ilia Shumailov",
      "Franziska Boenisch",
      "Nicolas Papernot"
    ]
  },
  "https://openreview.net/forum?id=1fNcpcdr1o": {
    "title": "CiPR: An Efficient Framework with Cross-instance Positive Relations for Generalized Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaozhe Hao",
      "Kai Han",
      "Kwan-Yee K. Wong"
    ]
  },
  "https://openreview.net/forum?id=H2EeStRTQn": {
    "title": "Introducing \"Forecast Utterance\" for Conversational Data Science",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md. Mahadi Hassan",
      "Alex Knipper",
      "Shubhra Kanti Karmaker Santu"
    ]
  },
  "https://openreview.net/forum?id=ehfRiF0R3a": {
    "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanzhi Wang",
      "Yuqi Xie",
      "Yunfan Jiang",
      "Ajay Mandlekar",
      "Chaowei Xiao",
      "Yuke Zhu",
      "Linxi Fan",
      "Anima Anandkumar"
    ]
  },
  "https://openreview.net/forum?id=Pe6hldOUkw": {
    "title": "Optimal Inference in Contextual Stochastic Block Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "O Duranthon",
      "Lenka Zdeborova"
    ]
  },
  "https://openreview.net/forum?id=ux9BrxPCl8": {
    "title": "Grokking Beyond Neural Networks: An Empirical Exploration with Model Complexity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jack William Miller",
      "Charles O'Neill",
      "Thang D Bui"
    ]
  },
  "https://openreview.net/forum?id=s1qh12FReM": {
    "title": "Compressing the Activation Maps in Deep Convolutional Neural Networks and Its Regularizing Effect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minh Hoang Vu",
      "Anders Garpebring",
      "Tufve Nyholm",
      "Tommy Löfstedt"
    ]
  },
  "https://openreview.net/forum?id=bQKHMSE4SH": {
    "title": "Towards Understanding Dual BN In Hybrid Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenshuang Zhang",
      "Chaoning Zhang",
      "Kang Zhang",
      "Axi Niu",
      "Junmo Kim",
      "In So Kweon"
    ]
  },
  "https://openreview.net/forum?id=1LoVwFkZNo": {
    "title": "ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deyao Zhu",
      "Jun Chen",
      "Kilichbek Haydarov",
      "Xiaoqian Shen",
      "Wenxuan Zhang",
      "Mohamed Elhoseiny"
    ]
  },
  "https://openreview.net/forum?id=q4iSLPoFe7": {
    "title": "Revisiting Generalized p-Laplacian Regularized Framelet GCNs: Convergence, Energy Dynamic and as Non-Linear Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dai Shi",
      "Zhiqi Shao",
      "Yi Guo",
      "Qibin Zhao",
      "Junbin Gao"
    ]
  },
  "https://openreview.net/forum?id=H4OE7toXpa": {
    "title": "Inverse Kernel Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengrui Li",
      "Anqi Wu"
    ]
  },
  "https://openreview.net/forum?id=iA2KQyoun1": {
    "title": "Granger Causal Interaction Skill Chains",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caleb Chuck",
      "Kevin Black",
      "Aditya Arjun",
      "Yuke Zhu",
      "Scott Niekum"
    ]
  },
  "https://openreview.net/forum?id=yUmJ483OB0": {
    "title": "Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongjun Yang",
      "Gibbeum Lee",
      "Jaewoong Cho",
      "Dimitris Papailiopoulos",
      "Kangwook Lee"
    ]
  },
  "https://openreview.net/forum?id=8GI1SXqJBk": {
    "title": "Maximizing Global Model Appeal in Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yae Jee Cho",
      "Divyansh Jhunjhunwala",
      "Tian Li",
      "Virginia Smith",
      "Gauri Joshi"
    ]
  },
  "https://openreview.net/forum?id=f9l4eiPKpV": {
    "title": "Learning Sparse Graphs for Functional Regression using Graph-induced Operator-valued Kernels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akash Saha",
      "Balamurugan Palaniappan"
    ]
  },
  "https://openreview.net/forum?id=JYbnJ92TJf": {
    "title": "Addressing Attribute Bias with Adversarial Support-Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Kehrenberg",
      "Myles Bartlett",
      "Viktoriia Sharmanska",
      "Novi Quadrianto"
    ]
  },
  "https://openreview.net/forum?id=ZSWKdRi2cU": {
    "title": "Hyper-parameter Tuning for Fair Classification without Sensitive Attribute Access",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshaj Kumar Veldanda",
      "Ivan Brugere",
      "Sanghamitra Dutta",
      "Alan Mishler",
      "Siddharth Garg"
    ]
  },
  "https://openreview.net/forum?id=7PNJzAxkij": {
    "title": "E(n)-equivariant Graph Neural Cellular Automata",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gennaro Gala",
      "Daniele Grattarola",
      "Erik Quaeghebeur"
    ]
  },
  "https://openreview.net/forum?id=WOrdoKbxh6": {
    "title": "Layer-diverse Negative Sampling for Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Duan",
      "Jie Lu",
      "Yu Guang Wang",
      "Junyu Xuan"
    ]
  },
  "https://openreview.net/forum?id=N2wx9UVHkH": {
    "title": "Personalized Federated Learning with Spurious Features: An Adversarial Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyang Wang",
      "Han Zhao",
      "Klara Nahrstedt",
      "Sanmi Koyejo"
    ]
  },
  "https://openreview.net/forum?id=QySD5r7PeE": {
    "title": "A Pseudo-Metric between Probability Distributions based on Depth-Trimmed Regions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guillaume Staerman",
      "Pavlo Mozharovskyi",
      "Pierre Colombo",
      "Stephan Clémençon",
      "Florence d'Alché-Buc"
    ]
  },
  "https://openreview.net/forum?id=9CcgO0LhKG": {
    "title": "World Models via Policy-Guided Trajectory Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marc Rigter",
      "Jun Yamada",
      "Ingmar Posner"
    ]
  },
  "https://openreview.net/forum?id=YCgX7sJRF1": {
    "title": "Adapting Contrastive Language-Image Pretrained (CLIP) Models for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikolas Adaloglou",
      "Felix Michels",
      "Tim Kaiser",
      "Markus Kollmann"
    ]
  },
  "https://openreview.net/forum?id=5VotySkajV": {
    "title": "Multi-conditioned Graph Diffusion for Neural Architecture Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohan Asthana",
      "Joschua Conrad",
      "Youssef Dawoud",
      "Maurits Ortmanns",
      "Vasileios Belagiannis"
    ]
  },
  "https://openreview.net/forum?id=KJRoQvRWNs": {
    "title": "How does over-squashing affect the power of GNNs?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Di Giovanni",
      "T. Konstantin Rusch",
      "Michael Bronstein",
      "Andreea Deac",
      "Marc Lackenby",
      "Siddhartha Mishra",
      "Petar Veličković"
    ]
  },
  "https://openreview.net/forum?id=cAthubStyG": {
    "title": "AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Tornede",
      "Difan Deng",
      "Theresa Eimer",
      "Joseph Giovanelli",
      "Aditya Mohan",
      "Tim Ruhkopf",
      "Sarah Segel",
      "Daphne Theodorakopoulos",
      "Tanja Tornede",
      "Henning Wachsmuth",
      "Marius Lindauer"
    ]
  },
  "https://openreview.net/forum?id=ue9igTDLN2": {
    "title": "Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adyasha Maharana",
      "Amita Kamath",
      "Christopher Clark",
      "Mohit Bansal",
      "Aniruddha Kembhavi"
    ]
  },
  "https://openreview.net/forum?id=aD0ExytnEK": {
    "title": "Controlling the Inductive Bias of Wide Neural Networks by Modifying the Kernel's Spectrum",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amnon Geifman",
      "Daniel Barzilai",
      "Ronen Basri",
      "Meirav Galun"
    ]
  },
  "https://openreview.net/forum?id=HhbqHBBrfZ": {
    "title": "Attending to Graph Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luis Müller",
      "Mikhail Galkin",
      "Christopher Morris",
      "Ladislav Rampášek"
    ]
  },
  "https://openreview.net/forum?id=2iOOvQmJBK": {
    "title": "Discovering Model Structure of Dynamical Systems with Combinatorial Bayesian Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Rath",
      "Alexander von Rohr",
      "Andreas Schultze",
      "Sebastian Trimpe",
      "Burkhard Corves"
    ]
  },
  "https://openreview.net/forum?id=1ZGA5mSkoB": {
    "title": "An Improved Federated Clustering Algorithm with Model-based Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harsh Vardhan",
      "Avishek Ghosh",
      "Arya Mazumdar"
    ]
  },
  "https://openreview.net/forum?id=qBZeQBEDIW": {
    "title": "Series of Hessian-Vector Products for Tractable Saddle-Free Newton Optimisation of Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elre Talea Oldewage",
      "Ross M Clarke",
      "José Miguel Hernández-Lobato"
    ]
  },
  "https://openreview.net/forum?id=8W6IDyFZgC": {
    "title": "Demographically-Informed Prediction Discrepancy Index: Early Warnings of Demographic Biases for Unlabeled Populations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Mansilla",
      "Estanislao Claucich",
      "Rodrigo Echeveste",
      "Diego H Milone",
      "Enzo Ferrante"
    ]
  },
  "https://openreview.net/forum?id=vTBjBtGioE": {
    "title": "Fast Training of Diffusion Models with Masked Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongkai Zheng",
      "Weili Nie",
      "Arash Vahdat",
      "Anima Anandkumar"
    ]
  },
  "https://openreview.net/forum?id=ZOqJCP4eMk": {
    "title": "Functional Linear Regression of Cumulative Distribution Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Zhang",
      "Anuran Makur",
      "Kamyar Azizzadenesheli"
    ]
  },
  "https://openreview.net/forum?id=4TnFbv16hK": {
    "title": "Bias/Variance is not the same as Approximation/Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gavin Brown",
      "Riccardo Ali"
    ]
  },
  "https://openreview.net/forum?id=HhjSalvWVe": {
    "title": "Incorporating Prior Knowledge into Neural Networks through an Implicit Composite Kernel",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Jiang",
      "Tongshu Zheng",
      "Yiling Liu",
      "David Carlson"
    ]
  },
  "https://openreview.net/forum?id=Ai9XpjGxjl": {
    "title": "Using Sum-Product Networks to Assess Uncertainty in Deep Active Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamadsadegh Khosravani",
      "Sandra Zilles"
    ]
  },
  "https://openreview.net/forum?id=UVE7LllpXe": {
    "title": "How Much Pre-training Is Enough to Discover a Good Subnetwork?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cameron R. Wolfe",
      "Fangshuo Liao",
      "Qihan Wang",
      "Junhyung Lyle Kim",
      "Anastasios Kyrillidis"
    ]
  },
  "https://openreview.net/forum?id=7yswRA8zzw": {
    "title": "Pull-back Geometry of Persistent Homology Encodings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuang Liang",
      "Renata Turkes",
      "Jiayi Li",
      "Nina Otter",
      "Guido Montufar"
    ]
  },
  "https://openreview.net/forum?id=bG3ICt3E0C": {
    "title": "MC Layer Normalization for calibrated uncertainty in Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Frick",
      "Diego Antognini",
      "Ioana Giurgiu",
      "Benjamin F Grewe",
      "Cristiano Malossi",
      "Rong J.B. Zhu",
      "Mattia Rigotti"
    ]
  },
  "https://openreview.net/forum?id=Uv3XVAEgG6": {
    "title": "Kernel Normalized Convolutional Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reza Nasirigerdeh",
      "Reihaneh Torkzadehmahani",
      "Daniel Rueckert",
      "Georgios Kaissis"
    ]
  },
  "https://openreview.net/forum?id=kNCZ95mw7N": {
    "title": "A VAE-based Framework for Learning Multi-Level Neural Granger-Causal Connectivity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahe Lin",
      "Huitian Lei",
      "George Michailidis"
    ]
  },
  "https://openreview.net/forum?id=KutEe24Yai": {
    "title": "Exploit CAM by itself: Complementary Learning System for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has long been suffering from fragmentary object regions led by Class Activation Map (CAM), which is incapable of generating fine-grained masks for semantic segmentation. To guide CAM to find more non-discriminating object patterns, this paper turns to an interesting working mechanism in agent learning named Complementary Learning System (CLS). CLS holds that the neocortex builds a sensation of general knowledge, while the hippocampus specially learns specific details, completing the learned patterns. Motivated by this simple but effective learning pattern, we propose a General-Specific Learning Mechanism (GSLM) to explicitly drive a coarse-grained CAM to a fine-grained pseudo mask. Specifically, GSLM develops a General Learning Module (GLM) and a Specific Learning Module (SLM). The GLM is trained with image-level supervision to extract coarse and general localization representations from CAM. Based on the general knowledge in the GLM, the SLM progressively exploits the specific spatial knowledge from the localization representations, expanding the CAM in an explicit way. To this end, we propose the Seed Reactivation to help SLM reactivate non-discriminating regions by setting a boundary for activation values, which successively identifies more regions of CAM. Without extra refinement processes, our method is able to achieve improvements for CAM of over 20.0% mIoU on PASCAL VOC 2012 and 10.0% mIoU on MS COCO 2014 datasets, representing a new state-of-the-art among existing WSSS methods. The code is publicly available at: https://github.com/tmlr-group/GSLM",
    "checked": true,
    "id": "9ce6fc2d6392853ffbc41f2c2d6da2405bf13d3f",
    "semantic_title": "exploit cam by itself: complementary learning system for weakly supervised semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Wankou Yang",
      "Jiren Mai",
      "Fei Zhang",
      "Tongliang Liu",
      "Bo Han"
    ]
  },
  "https://openreview.net/forum?id=xo3hI5MwvU": {
    "title": "Learning from Natural Language Feedback",
    "volume": "main",
    "abstract": "The potential for pre-trained large language models (LLMs) to use natural language feedback at inference time has been an exciting recent development. We build upon this observation by formalizing an algorithm for learning from natural language feedback at training time instead, which we call Imitation learning from Language Feedback (ILF). ILF requires only a small amount of human-written feedback during training and does not require the same feedback at test time, making it both user-friendly and sample-efficient. We further show that ILF can be seen as a form of minimizing the KL divergence to the target distribution and demonstrate proof-of-concepts on text summarization and program synthesis tasks. For code generation, ILF improves a Codegen-Mono 6.1B model's pass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic Python Problems (MBPP) benchmark, outperforming both fine-tuning on MBPP and fine-tuning on repaired programs written by humans. For summarization, we show that ILF can be combined with learning from human preferences to improve a GPT-3 model's summarization performance to be comparable to human quality, outperforming fine-tuning on human-written summaries. Overall, our results suggest that learning from human-written natural language feedback is both more effective and sample-efficient than training exclusively on demonstrations for improving an LLM's performance on a variety of tasks",
    "checked": true,
    "id": "c43a6f12b062a50617244611af180a8146e792de",
    "semantic_title": "learning from natural language feedback",
    "citation_count": 3,
    "authors": [
      "Angelica Chen",
      "Jérémy Scheurer",
      "Jon Ander Campos",
      "Tomasz Korbak",
      "Jun Shern Chan",
      "Samuel R. Bowman",
      "Kyunghyun Cho",
      "Ethan Perez"
    ]
  },
  "https://openreview.net/forum?id=lVE1VeGQwg": {
    "title": "Manifold Contrastive Learning with Variational Lie Group Operators",
    "volume": "main",
    "abstract": "Self-supervised learning of deep neural networks has become a prevalent paradigm for learning representations that transfer to a variety of downstream tasks. Similar to proposed models of the ventral stream of biological vision, it is observed that these networks lead to a separation of category manifolds in the representations of the penultimate layer. Although this observation matches the manifold hypothesis of representation learning, current self-supervised approaches are limited in their ability to explicitly model this manifold. Indeed, current approaches often only apply a pre-specified set of augmentations for \"positive pairs\" during learning. In this work, we propose a contrastive learning approach that directly models the latent manifold using Lie group operators parameterized by coefficients with a sparsity-promoting prior. A variational distribution over these coefficients provides a generative model of the manifold, with samples which provide feature augmentations applicable both during contrastive training and downstream tasks. Additionally, learned coefficient distributions provide a quantification of which transformations are most likely at each point on the manifold while preserving identity. We demonstrate benefits in self-supervised benchmarks for image datasets, as well as a downstream semi-supervised task. In the former case, we demonstrate that the proposed methods can effectively apply manifold feature augmentations and improve learning both with and without a projection head. In the latter case, we demonstrate that feature augmentations sampled from learned Lie group operators can improve classification performance when using few labels",
    "checked": true,
    "id": "5861aeb9659b9449f7482e0d5543933216577727",
    "semantic_title": "manifold contrastive learning with variational lie group operators",
    "citation_count": 0,
    "authors": [
      "Kion Fallah",
      "Alec Helbling",
      "Kyle A. Johnsen",
      "Christopher John Rozell"
    ]
  },
  "https://openreview.net/forum?id=805jKZ0Gqf": {
    "title": "Pseudo-Differential Neural Operator: Generalize Fourier Neural operator for Learning Solution Operators of Partial Differential Equations",
    "volume": "main",
    "abstract": "Learning mapping between two function spaces has attracted considerable research attention. However, learning the solution operator of partial differential equations (PDEs) remains a challenge in scientific computing. Fourier neural operator (FNO) is recently proposed to learn the solution operators with an excellent performance. In this study, we propose a novel pseudo-differential integral operator (PDIO) to analyze and generalize the Fourier integral operator in FNO. PDIO is inspired by a pseudo-differential operator, which is a generalization of a differential operator and characterized by a certain symbol. We parameterize the symbol by using a neural network and show that the neural-network-based symbol is contained in a smooth symbol class. Subsequently, we prove that the PDIO is a bounded linear operator, and thus is continuous in the Sobolev space. We combine the PDIO with the neural operator to develop a pseudo-differential neural operator (PDNO) to learn the nonlinear solution operator of PDEs. We experimentally validate the effectiveness of the proposed model by using Darcy flow and the Navier-Stokes equation. The results reveal that the proposed PDNO outperforms the existing neural operator approaches in most experiments",
    "checked": false,
    "id": "72551363d7f8e457239bb7852ee64b6e06d437d6",
    "semantic_title": "pseudo-differential neural operator: generalized fourier neural operator for learning solution operators of partial differential equations",
    "citation_count": 2,
    "authors": [
      "Jin Young Shin",
      "Jae Yong Lee",
      "Hyung Ju Hwang"
    ]
  },
  "https://openreview.net/forum?id=TTRDCVnbjI": {
    "title": "Are Population Graphs Really as Powerful as Believed?",
    "volume": "main",
    "abstract": "Population graphs and their use in combination with graph neural networks (GNNs) have demonstrated promising results for multi-modal medical data integration and improving disease diagnosis and prognosis. Several different methods for constructing these graphs and advanced graph learning techniques have been established to maximise the predictive power of GNNs on population graphs. However, in this work, we raise the question of whether existing methods are really strong enough by showing that simple baseline methods --such as random forests or linear regressions--, perform on par with advanced graph learning models on several population graph datasets for a variety of different clinical applications. We use the commonly used public population graph datasets TADPOLE and ABIDE, a brain age estimation and a cardiac dataset from the UK Biobank, and a real-world in-house COVID dataset. We (a) investigate the impact of different graph construction methods, graph convolutions, and dataset size and complexity on GNN performance and (b) discuss the utility of GNNs for multi-modal data integration in the context of population graphs. Based on our results, we argue towards the need for \"better\" graph construction methods or innovative applications for population graphs to render them beneficial",
    "checked": true,
    "id": "5b636a833df6bd16462f508e7d00458f8ee639cd",
    "semantic_title": "are population graphs really as powerful as believed?",
    "citation_count": 0,
    "authors": [
      "Tamara T. Müller",
      "Sophie Starck",
      "Kyriaki-Margarita Bintsi",
      "Alexander Ziller",
      "Rickmer Braren",
      "Georgios Kaissis",
      "Daniel Rueckert"
    ]
  },
  "https://openreview.net/forum?id=sPlhAIp6mk": {
    "title": "Multitask Learning Can Improve Worst-Group Outcomes",
    "volume": "main",
    "abstract": "In order to create machine learning systems that serve a variety of users well, it is vital to not only achieve high average performance but also ensure equitable outcomes across diverse groups. However, most machine learning methods are designed to improve a model's average performance on a chosen end task without consideration for their impact on worst group error. Multitask learning (MTL) is one such widely used technique. In this paper, we seek not only to understand the impact of MTL on worst-group accuracy but also to explore its potential as a tool to address the challenge of group-wise fairness. We primarily consider the standard setting of fine-tuning a pre-trained model, where, following recent work \\citep{gururangan2020don, dery2023aang}, we multitask the end task with the pre-training objective constructed from the end task data itself. In settings with few or no group annotations, we find that multitasking often, but not consistently, achieves better worst-group accuracy than Just-Train-Twice (JTT; \\citet{pmlr-v139-liu21f}) -- a representative distributionally robust optimization (DRO) method. Leveraging insights from synthetic data experiments, we propose to modify standard MTL by regularizing the joint multitask representation space. We run a large number of fine-tuning experiments across computer vision and natural language processing datasets and find that our regularized MTL approach \\emph{consistently} outperforms JTT on both average and worst-group outcomes. Our official code can be found here: \\href{https://github.com/atharvajk98/MTL-group-robustness.git}{\\url{https://github.com/atharvajk98/MTL-group-robustness}}",
    "checked": true,
    "id": "e36dbc1ae9b5116b798d519948de097c3bf19e5b",
    "semantic_title": "multitask learning can improve worst-group outcomes",
    "citation_count": 0,
    "authors": [
      "Atharva Kulkarni",
      "Lucio M. Dery",
      "Amrith Setlur",
      "Aditi Raghunathan",
      "Ameet Talwalkar",
      "Graham Neubig"
    ]
  },
  "https://openreview.net/forum?id=3nprbNR3HB": {
    "title": "ASPEST: Bridging the Gap Between Active Learning and Selective Prediction",
    "volume": "main",
    "abstract": "Selective prediction aims to learn a reliable model that abstains from making predictions when uncertain. These predictions can then be deferred to humans for further evaluation. As an everlasting challenge for machine learning, in many real-world scenarios, the distribution of test data is different from the training data. This results in more inaccurate predictions, and often increased dependence on humans, which can be difficult and expensive. Active learning aims to lower the overall labeling effort, and hence human dependence, by querying the most informative examples. Selective prediction and active learning have been approached from different angles, with the connection between them missing. In this work, we introduce a new learning paradigm, active selective prediction, which aims to query more informative samples from the shifted target domain while increasing accuracy and coverage. For this new paradigm, we propose a simple yet effective approach, ASPEST, that utilizes ensembles of model snapshots with self-training with their aggregated outputs as pseudo labels. Extensive experiments on numerous image, text and structured datasets, which suffer from domain shifts, demonstrate that ASPEST can significantly outperform prior work on selective prediction and active learning (e.g. on the MNIST$\\to$SVHN benchmark with the labeling budget of 100, ASPEST improves the AUACC metric from 79.36% to 88.84%) and achieves more optimal utilization of humans in the loop",
    "checked": true,
    "id": "217d5e0c5ed75f45256e14e122035eeb9af1722b",
    "semantic_title": "aspest: bridging the gap between active learning and selective prediction",
    "citation_count": 1,
    "authors": [
      "Jiefeng Chen",
      "Jinsung Yoon",
      "Sayna Ebrahimi",
      "Sercan O Arik",
      "Somesh Jha",
      "Tomas Pfister"
    ]
  },
  "https://openreview.net/forum?id=OUWG6O4yo9": {
    "title": "Statistical Component Separation for Targeted Signal Recovery in Noisy Mixtures",
    "volume": "main",
    "abstract": "Separating signals from an additive mixture may be an unnecessarily hard problem when one is only interested in specific properties of a given signal. In this work, we tackle simpler \"statistical component separation\" problems that focus on recovering a predefined set of statistical descriptors of a target signal from a noisy mixture. Assuming access to samples of the noise process, we investigate a method devised to match the statistics of the solution candidate corrupted by noise samples with those of the observed mixture. We first analyze the behavior of this method using simple examples with analytically tractable calculations. Then, we apply it in an image denoising context employing 1) wavelet-based descriptors, 2) ConvNet-based descriptors on astrophysics and ImageNet data. In the case of 1), we show that our method better recovers the descriptors of the target data than a standard denoising method in most situations. Additionally, despite not constructed for this purpose, it performs surprisingly well in terms of peak signal-to-noise ratio on full signal reconstruction. In comparison, representation 2) appears less suitable for image denoising. Finally, we extend this method by introducing a diffusive stepwise algorithm which gives a new perspective to the initial method and leads to promising results for image denoising under specific circumstances",
    "checked": true,
    "id": "63a2bd0261a3f28f52327c8f43f4c8c44e2f1fed",
    "semantic_title": "statistical component separation for targeted signal recovery in noisy mixtures",
    "citation_count": 0,
    "authors": [
      "Bruno Régaldo-Saint Blancard",
      "Michael Eickenberg"
    ]
  },
  "https://openreview.net/forum?id=WeiRR8h87X": {
    "title": "Budgeted Online Model Selection and Fine-Tuning via Federated Learning",
    "volume": "main",
    "abstract": "Online model selection involves selecting a model from a set of candidate models `on the fly' to perform prediction on a stream of data. The choice of candidate models henceforth has a crucial impact on the performance. Although employing a larger set of candidate models naturally leads to more flexibility in model selection, this may be infeasible in cases where prediction tasks are performed on edge devices with limited memory. Faced with this challenge, the present paper proposes an online federated model selection framework where a group of learners (clients) interacts with a server with sufficient memory such that the server stores all candidate models. However, each client only chooses to store a subset of models that can be fit into its memory and performs its own prediction task using one of the stored models. Furthermore, employing the proposed algorithm, clients and the server collaborate to fine-tune models to adapt them to a non-stationary environment. Theoretical analysis proves that the proposed algorithm enjoys sub-linear regret with respect to the best model in hindsight. Experiments on real datasets demonstrate the effectiveness of the proposed algorithm",
    "checked": true,
    "id": "fa74e4a136cc81ffe79e7b936de451d947310a99",
    "semantic_title": "budgeted online model selection and fine-tuning via federated learning",
    "citation_count": 0,
    "authors": [
      "Pouya M. Ghari",
      "Yanning Shen"
    ]
  },
  "https://openreview.net/forum?id=icoP08mrQJ": {
    "title": "Leveraging Endo- and Exo-Temporal Regularization for Black-box Video Domain Adaptation",
    "volume": "main",
    "abstract": "To enable video models to be applied seamlessly across video tasks in different environments, various Video Unsupervised Domain Adaptation (VUDA) methods have been proposed to improve the robustness and transferability of video models. Despite improvements made in model robustness, these VUDA methods require access to both source data and source model parameters for adaptation, raising serious data privacy and model portability issues. To cope with the above concerns, this paper firstly formulates Black-box Video Domain Adaptation (BVDA) as a more realistic yet challenging scenario where the source video model is provided only as a black-box predictor. While a few methods for Black-box Domain Adaptation (BDA) are proposed in the image domain, these methods cannot apply to the video domain since video modality has more complicated temporal features that are harder to align. To address BVDA, we propose a novel Endo and eXo-TEmporal Regularized Network (EXTERN) by applying mask-to-mix strategies and video-tailored regularizations. They are the endo-temporal regularization and exo-temporal regularization, which are performed across both clip and temporal features, while distilling knowledge from the predictions obtained from the black-box predictor. Empirical results demonstrate the state-of-the-art performance of EXTERN across various cross-domain closed-set and partial-set action recognition benchmarks, which even surpasses most existing video domain adaptation methods with source data accessibility. Code will be available at https://xuyu0010.github.io/b2vda.html",
    "checked": true,
    "id": "5c22f7d443a98ff02dd7c15c4473979205b42098",
    "semantic_title": "leveraging endo- and exo-temporal regularization for black-box video domain adaptation",
    "citation_count": 1,
    "authors": [
      "Yuecong Xu",
      "Jianfei Yang",
      "Haozhi Cao",
      "Min Wu",
      "Xiaoli Li",
      "Lihua Xie",
      "Zhenghua Chen"
    ]
  },
  "https://openreview.net/forum?id=Ryf1TVCjBz": {
    "title": "Correlation Clustering with Active Learning of Pairwise Similarities",
    "volume": "main",
    "abstract": "Correlation clustering is a well-known unsupervised learning setting that deals with positive and negative pairwise similarities. In this paper, we study the case where the pairwise similarities are not given in advance and must be queried in a cost-efficient way. Thereby, we develop a generic active learning framework for this task that benefits from several advantages, e.g., flexibility in the type of feedback that a user/annotator can provide, adaptation to any correlation clustering algorithm and query strategy, and robustness to noise. In addition, we propose and analyze a number of novel query strategies suited to this setting. We demonstrate the effectiveness of our framework and the proposed query strategies via several experimental studies",
    "checked": true,
    "id": "f672dec6afd24e63652363a58b69614cf0bd11de",
    "semantic_title": "correlation clustering with active learning of pairwise similarities",
    "citation_count": 1,
    "authors": [
      "Linus Aronsson",
      "Morteza Haghir Chehreghani"
    ]
  },
  "https://openreview.net/forum?id=uxNfN2PU1W": {
    "title": "Effective Latent Differential Equation Models via Attention and Multiple Shooting",
    "volume": "main",
    "abstract": "Scientific Machine Learning (SciML) is a burgeoning field that synergistically combines domain-aware and interpretable models with agnostic machine learning techniques. In this work, we introduce GOKU-UI, an evolution of the SciML generative model GOKU-nets. GOKU-UI not only broadens the original model's spectrum to incorporate other classes of differential equations, such as Stochastic Differential Equations (SDEs), but also integrates attention mechanisms and a novel multiple shooting training strategy in the latent space. These modifications have led to a significant increase in its performance in both reconstruction and forecast tasks, as demonstrated by our evaluation on simulated and empirical data. Specifically, GOKU-UI outperformed all baseline models on synthetic datasets even with a training set 16-fold smaller, underscoring its remarkable data efficiency. Furthermore, when applied to empirical human brain data, while incorporating stochastic Stuart-Landau oscillators into its dynamical core, our proposed enhancements markedly increased the model's effectiveness in capturing complex brain dynamics. GOKU-UI demonstrated a reconstruction error five times lower than other baselines, and the multiple shooting method reduced the GOKU-nets prediction error for future brain activity up to 15 seconds ahead. By training GOKU-UI on resting state fMRI data, we encoded whole-brain dynamics into a latent representation, learning a low-dimensional dynamical system model that could offer insights into brain functionality and open avenues for practical applications such as the classification of mental states or psychiatric conditions. Ultimately, our research provides further impetus for the field of Scientific Machine Learning, showcasing the potential for advancements when established scientific insights are interwoven with modern machine learning",
    "checked": true,
    "id": "95ce5e9ab22aacfdfae412dbcd80d8b246364e15",
    "semantic_title": "effective latent differential equation models via attention and multiple shooting",
    "citation_count": 0,
    "authors": [
      "Germán Abrevaya",
      "Mahta Ramezanian-Panahi",
      "Jean-Christophe Gagnon-Audet",
      "Pablo Polosecki",
      "Irina Rish",
      "Silvina Ponce Dawson",
      "Guillermo Cecchi",
      "Guillaume Dumas"
    ]
  },
  "https://openreview.net/forum?id=9TqAUYB6tC": {
    "title": "Global Convergence of SGD For Logistic Loss on Two Layer Neural Nets",
    "volume": "main",
    "abstract": "In this note, we demonstrate a first-of-its-kind provable convergence of SGD to the global minima of appropriately regularized logistic empirical risk of depth $2$ nets -- for arbitrary data with any number of gates with adequately smooth and bounded activations, like sigmoid and tanh, and for a class of distributions from which the initial weight is sampled. We also prove an exponentially fast convergence rate for continuous time SGD that also applies to smooth unbounded activations like SoftPlus. Our key idea is to show that the logistic loss function on any size neural net can be Frobenius norm regularized by a width-independent parameter such that the regularized loss is a ``Villani function'' -- and thus be able to build on recent progress with analyzing SGD on such objectives",
    "checked": true,
    "id": "2b6040d20a0bf41d1d20caf886de57371ed464b0",
    "semantic_title": "global convergence of sgd for logistic loss on two layer neural nets",
    "citation_count": 0,
    "authors": [
      "Pulkit Gopalani",
      "Samyak Jha",
      "Anirbit Mukherjee"
    ]
  },
  "https://openreview.net/forum?id=uGVFtjvI3v": {
    "title": "Why should autoencoders work?",
    "volume": "main",
    "abstract": "Deep neural network autoencoders are routinely used computationally for model reduction. They allow recognizing the intrinsic dimension of data that lie in a $k$-dimensional subset $K$ of an input Euclidean space $\\mathbb{R}^n$. The underlying idea is to obtain both an encoding layer that maps $\\mathbb{R}^n$ into $\\mathbb{R}^k$ (called the bottleneck layer or the space of latent variables) and a decoding layer that maps $\\mathbb{R}^k$ back into $\\mathbb{R}^n$, in such a way that the input data from the set $K$ is recovered when composing the two maps. This is achieved by adjusting parameters (weights) in the network to minimize the discrepancy between the input and the reconstructed output. Since neural networks (with continuous activation functions) compute continuous maps, the existence of a network that achieves perfect reconstruction would imply that $K$ is homeomorphic to a $k$-dimensional subset of $\\mathbb{R}^k$, so clearly there are topological obstructions to finding such a network. On the other hand, in practice the technique is found to ``work'' well, which leads one to ask if there is a way to explain this effectiveness. We show that, up to small errors, indeed the method is guaranteed to work. This is done by appealing to certain facts from differential topology. A computational example is also included to illustrate the ideas",
    "checked": true,
    "id": "31e6d857ac83fe4dd2d9084d5c0485cb6b42504b",
    "semantic_title": "why should autoencoders work?",
    "citation_count": 0,
    "authors": [
      "Matthew Kvalheim",
      "Eduardo Sontag"
    ]
  },
  "https://openreview.net/forum?id=n2gAD8Fdzk": {
    "title": "Enhancing Robustness to Class-Conditional Distribution Shift in Long-Tailed Recognition",
    "volume": "main",
    "abstract": "For long-tailed recognition problem, beyond imbalanced label distribution, unreliable empirical data distribution due to instance scarcity has recently emerged as a concern. It inevitably causes Class-Conditional Distribution (CCD) shift between training and test. Data augmentation and head-to-tail information transfer methods indirectly alleviate the problem by synthesizing novel examples but may remain biased. In this paper, we conduct a thorough study on the impact of CCD shift and propose Distributionally Robust Augmentation (DRA) to directly train models robust to the shift. DRA admits a novel generalization bound reflecting the benefit of distributional robustness to CCD shift for long-tailed recognition. Extensive experiments show DRA greatly improves existing re-balancing and data augmentation methods when cooperating with them. It also alleviates the recently discovered saddle-point issue, verifying its ability to achieve enhanced robustness",
    "checked": true,
    "id": "4a4752eba69c7ce66ba4c0edb7c94a94f6ea4b8f",
    "semantic_title": "enhancing robustness to class-conditional distribution shift in long-tailed recognition",
    "citation_count": 0,
    "authors": [
      "Keliang Li",
      "Hong Chang",
      "Shiguang Shan",
      "Xilin CHEN"
    ]
  },
  "https://openreview.net/forum?id=Eg8Rnb0Hdd": {
    "title": "Expected Pinball Loss For Quantile Regression And Inverse CDF Estimation",
    "volume": "main",
    "abstract": "We analyze and improve a recent strategy to train a quantile regression model by minimizing an expected pinball loss over all quantiles. Through an asymptotic convergence analysis, we show that minimizing the expected pinball loss can be more efficient at estimating single quantiles than training with the standard pinball loss for that quantile, an insight that generalizes the known deficiencies of the sample quantile in the unconditioned setting. Then, to guarantee a legitimate inverse CDF, we propose using flexible deep lattice networks with a monotonicity constraint on the quantile input to guarantee non-crossing quantiles, and show lattice models can be regularized to the same location-scale family. Our analysis and experiments on simulated and real datasets show that the proposed method produces state-of-the-art legitimate inverse CDF estimates that are likely to be as good or better for specific target quantiles",
    "checked": true,
    "id": "559c57ec361e8086588a299b50dfc95083493018",
    "semantic_title": "expected pinball loss for quantile regression and inverse cdf estimation",
    "citation_count": 2,
    "authors": [
      "Taman Narayan",
      "Serena Lutong Wang",
      "Kevin Robert Canini",
      "Maya Gupta"
    ]
  },
  "https://openreview.net/forum?id=OZbn8ULouY": {
    "title": "The Slingshot Effect: A Late-Stage Optimization Anomaly in Adaptive Gradient Methods",
    "volume": "main",
    "abstract": "Adaptive gradient methods, notably Adam ~\\citep{kingma2014adam, loshchilov2017decoupled}, have become indispensable for optimizing neural networks, particularly in conjunction with Transformers ~\\citep{vaswani2017attention, dosovitskiy2020an}. In this paper, we present a novel optimization anomaly called the \\emph{Slingshot Effect}, which manifests during extremely late stages of training. We identify a distinctive characteristic of this phenomenon through cyclic phase transitions between stable and unstable training regimes, as evidenced by the cyclic behavior of the norm of the last layer's weights. Although the Slingshot Effect can be easily reproduced in more general settings, it does not align with any known optimization theories, emphasizing the need for in-depth examination. Moreover, we make a noteworthy observation that Grokking, as reported by ~\\citet{power2021grokking}, occurs predominantly during the onset of the Slingshot Effects and is absent without it, even in the absence of explicit regularization. This finding suggests a surprising inductive bias of adaptive gradient optimizers at late training stages, urging a revised theoretical analysis of their origin. Our study sheds light on an intriguing optimization behavior that has significant implications for understanding the inner workings of adaptive gradient methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vimal Thilak",
      "Etai Littwin",
      "Shuangfei Zhai",
      "Omid Saremi",
      "Roni Paiss",
      "Joshua M. Susskind"
    ]
  },
  "https://openreview.net/forum?id=mqMzerrVOB": {
    "title": "Mixed Nash for Robust Federated Learning",
    "volume": "main",
    "abstract": "We study robust federated learning (FL) within a game theoretic framework to alleviate the server vulnerabilities to even an informed adversary who can tailor training-time attacks. Specifically, we introduce RobustTailor, a simulation-based framework that prevents the adversary from being omniscient and derives its convergence guarantees. RobustTailor improves robustness to training-time attacks significantly while preserving almost the same privacy guarantees as standard robust aggregation schemes in FL. Empirical results under challenging attacks show that RobustTailor performs close to an upper bound with perfect knowledge of honest clients",
    "checked": true,
    "id": "aea10c5336e7f321af5800aafbf6faac3374b2dd",
    "semantic_title": "mixed nash for robust federated learning",
    "citation_count": 1,
    "authors": [
      "Wanyun Xie",
      "Thomas Pethick",
      "Ali Ramezani-Kebrya",
      "Volkan Cevher"
    ]
  },
  "https://openreview.net/forum?id=WFI9xhJrxF": {
    "title": "Policy Gradient with Kernel Quadrature",
    "volume": "main",
    "abstract": "Reward evaluation of episodes becomes a bottleneck in a broad range of reinforcement learning tasks. Our aim in this paper is to select a small but representative subset of a large batch of episodes, only on which we actually compute rewards for more efficient policy gradient iterations. We build a Gaussian process modeling of discounted returns or rewards to derive a positive definite kernel on the space of episodes, run an ``episodic\" kernel quadrature method to compress the information of sample episodes, and pass the reduced episodes to the policy network for gradient updates. We present the theoretical background of this procedure as well as its numerical illustrations in MuJoCo tasks",
    "checked": true,
    "id": "ffbf87817e86739040ef7e80169d55db707ea947",
    "semantic_title": "policy gradient with kernel quadrature",
    "citation_count": 0,
    "authors": [
      "Satoshi Hayakawa",
      "Tetsuro Morimura"
    ]
  },
  "https://openreview.net/forum?id=oCBsxCov2g": {
    "title": "PNeRV: A Polynomial Neural Representation for Videos",
    "volume": "main",
    "abstract": "Extracting Implicit Neural Representations (INRs) on video data poses unique challenges due to the additional temporal dimension. In the context of videos, INRs have predominantly relied on a frame-only parameterization, which sacrifices the spatiotemporal continuity observed in pixel-level (spatial) representations. To mitigate this, we introduce Polynomial Neural Representation for Videos (PNeRV), a parameter-wise efficient, patch-wise INR for videos that preserves spatiotemporal continuity. PNeRV leverages the modeling capabilities of Polynomial Neural Networks to perform the modulation of a continuous spatial (patch) signal with a continuous time (frame) signal. We further propose a custom Hierarchical Patch-wise Spatial Sampling Scheme that ensures spatial continuity while retaining parameter efficiency. We also employ a carefully designed Positional Embedding methodology to further enhance PNeRV's performance. Our extensive experimentation demonstrates that PNeRV outperforms the baselines in conventional Implicit Neural Representation tasks like compression along with downstream applications that require spatiotemporal continuity in the underlying representation. PNeRV not only addresses the challenges posed by video data in the realm of INRs but also opens new avenues for advanced video processing and analysis",
    "checked": true,
    "id": "c216d8d79503107bf68897f9206d81cf153823b8",
    "semantic_title": "pnerv: a polynomial neural representation for videos",
    "citation_count": 0,
    "authors": [
      "Sonam Gupta",
      "Snehal Singh Tomar",
      "Grigorios Chrysos",
      "Sukhendu Das",
      "Rajagopalan N Ambasamduram"
    ]
  },
  "https://openreview.net/forum?id=daX2UkLMS0": {
    "title": "Exploring Simple, High Quality Out-of-Distribution Detection with L2 Normalization",
    "volume": "main",
    "abstract": "We demonstrate that L2 normalization over feature space can produce capable performance for Out-of-Distribution (OoD) detection for some models and datasets. Although it does not demonstrate outright state-of-the-art performance, this method is notable for its extreme simplicity: it requires only two addition lines of code, and does not need specialized loss functions, image augmentations, outlier exposure or extra parameter tuning. We also observe that training may be more efficient for some datasets and architectures. Notably, only 60 epochs with ResNet18 on CIFAR10 (or 100 epochs with ResNet50) can produce performance within two percentage points (AUROC) of several state-of-the-art methods for some near and far OoD datasets. We provide theoretical and empirical support for this method, and demonstrate viability across five architectures and three In-Distribution (ID) datasets",
    "checked": true,
    "id": "9b952ed671fa82de294ae53c275c443f393896bd",
    "semantic_title": "exploring simple, high quality out-of-distribution detection with l2 normalization",
    "citation_count": 0,
    "authors": [
      "Jarrod Haas",
      "William Yolland",
      "Bernhard T Rabus"
    ]
  },
  "https://openreview.net/forum?id=TySx8fsSSU": {
    "title": "On the Out-of-Distribution Coverage of Combining Split Conformal Prediction and Bayesian Deep Learning",
    "volume": "main",
    "abstract": "Bayesian deep learning and conformal prediction are two methods that have been used to convey uncertainty and increase safety in machine learning systems. We focus on combining Bayesian deep learning with split conformal prediction and how the addition of conformal prediction affects out-of-distribution coverage that we would otherwise see; particularly in the case of multiclass image classification. We suggest that if the model is generally underconfident on the calibration set, then the resultant conformal sets may exhibit worse out-of-distribution coverage compared to simple predictive credible sets (i.e. not using conformal prediction). Conversely, if the model is overconfident on the calibration set, the use of conformal prediction may improve out-of-distribution coverage. In particular, we study the extent to which the addition of conformal prediction increases or decreases out-of-distribution coverage for a variety of inference techniques. In particular, (i) stochastic gradient descent, (ii) deep ensembles, (iii) mean-field variational inference, (iv) stochastic gradient Hamiltonian Monte Carlo, and (v) Laplace approximation. Our results suggest that the application of conformal prediction to different predictive deep learning methods can have significantly different consequences",
    "checked": true,
    "id": "5e7707e2133d965ff48718ecf9ce61b19c65b570",
    "semantic_title": "on the out-of-distribution coverage of combining split conformal prediction and bayesian deep learning",
    "citation_count": 0,
    "authors": [
      "Paul Scemama",
      "Ariel Kapusta"
    ]
  },
  "https://openreview.net/forum?id=RUNiIDU8P7": {
    "title": "Estimating Optimal Policy Value in Linear Contextual Bandits Beyond Gaussianity",
    "volume": "main",
    "abstract": "In many bandit problems, the maximal reward achievable by a policy is often unknown in advance. We consider the problem of estimating the optimal policy value in the sublinear data regime before the optimal policy is even learnable. We refer to this as $V^*$ estimation. It was previously shown that fast $V^*$ estimation is possible but only in disjoint linear bandits with Gaussian covariates. Whether this is possible for more realistic context distributions has remained an open and important question for tasks such as model selection. In this paper, we first provide lower bounds showing that this general problem is hard. However, under stronger assumptions, we give an algorithm and analysis proving that $\\widetilde{\\mathcal{O}}(\\sqrt{d})$ sublinear estimation of $V^*$ is indeed information-theoretically possible, where $d$ is the dimension. We subsequently introduce a practical and computationally efficient algorithm that estimates a problem-specific upper bound on $V^*$, valid for general distributions and tight for Gaussian context distributions. We prove our algorithm requires only $\\widetilde{\\mathcal{O}}(\\sqrt{d})$ samples to estimate the upper bound. We use this upper bound in conjunction with the estimator to derive novel and improved guarantees for several applications in bandit model selection and testing for treatment effects. We present promising experimental benefits on a semi-synthetic simulation using historical data on warfarin treatment dosage outcomes",
    "checked": false,
    "id": "50115a89c089fd3914633f67c1e8418ef4cd92cf",
    "semantic_title": "estimating optimal policy value in general linear contextual bandits",
    "citation_count": 0,
    "authors": [
      "Jonathan Lee",
      "Weihao Kong",
      "Aldo Pacchiano",
      "Vidya Muthukumar",
      "Emma Brunskill"
    ]
  },
  "https://openreview.net/forum?id=48pHFcg0YO": {
    "title": "DynaConF: Dynamic Forecasting of Non-Stationary Time Series",
    "volume": "main",
    "abstract": "Deep learning has shown impressive results in a variety of time series forecasting tasks, where modeling the conditional distribution of the future given the past is the essence. However, when this conditional distribution is non-stationary, it poses challenges for these models to learn consistently and to predict accurately. In this work, we propose a new method to model non-stationary conditional distributions over time by clearly decoupling stationary conditional distribution modeling from non-stationary dynamics modeling. Our method is based on a Bayesian dynamic model that can adapt to conditional distribution changes and a deep conditional distribution model that handles multivariate time series using a factorized output space. Our experimental results on synthetic and real-world datasets show that our model can adapt to non-stationary time series better than state-of-the-art deep learning solutions",
    "checked": false,
    "id": "d111d372e1f0ae5982c7a52726b1bf9c502466e9",
    "semantic_title": "dynaconf: dynamic forecasting of non-stationary time-series",
    "citation_count": 2,
    "authors": [
      "Siqi Liu",
      "Andreas Lehrmann"
    ]
  },
  "https://openreview.net/forum?id=uXGUSX8GoY": {
    "title": "QDC: Quantum Diffusion Convolution Kernels on Graphs",
    "volume": "main",
    "abstract": "Graph convolutional neural networks (GCNs) operate by aggregating messages over local neighborhoods given the prediction task under interest. Many GCNs can be understood as a form of generalized diffusion of input features on the graph, and significant work has been dedicated to improving predictive accuracy by altering the ways of message passing. In this work, we propose a new convolution kernel that effectively rewires the graph according to the occupation correlations of the vertices by trading on the generalized diffusion paradigm for the propagation of a quantum particle over the graph. We term this new convolution kernel the Quantum Diffusion Convolution (QDC) operator. In addition, we introduce a multiscale variant that combines messages from the QDC operator and the traditional combinatorial Laplacian. To understand our method, we explore the spectral dependence of homophily and the importance of quantum dynamics in the construction of a bandpass filter. Through these studies, as well as experiments on a range of datasets, we observe that QDC improves predictive performance on the widely used benchmark datasets when compared to similar methods",
    "checked": true,
    "id": "82aa9b8b8ccf2f879665397d023c4ca1173e6b34",
    "semantic_title": "qdc: quantum diffusion convolution kernels on graphs",
    "citation_count": 2,
    "authors": [
      "Thomas Markovich"
    ]
  },
  "https://openreview.net/forum?id=torWsEui9N": {
    "title": "Image Reconstruction via Deep Image Prior Subspaces",
    "volume": "main",
    "abstract": "Deep learning has been widely used for solving image reconstruction tasks but its deployability has been held back due to the shortage of high-quality paired training data. Unsupervised learning methods, e.g., deep image prior (DIP), naturally fill this gap, but bring a host of new issues: the susceptibility to overfitting due to a lack of robust early stopping strategies and unstable convergence. We present a novel approach to tackle these issues by restricting DIP optimisation to a sparse linear subspace of its parameters, employing a synergy of dimensionality reduction techniques and second order optimisation methods. The low-dimensionality of the subspace reduces DIP's tendency to fit noise and allows the use of stable second order optimisation methods, e.g., natural gradient descent or L-BFGS. Experiments across both image restoration and tomographic tasks of different geometry and ill-posedness show that second order optimisation within a low-dimensional subspace is favourable in terms of optimisation stability to reconstruction fidelity trade-off",
    "checked": true,
    "id": "174910a4357196a37632efa3022afb5bd254e676",
    "semantic_title": "image reconstruction via deep image prior subspaces",
    "citation_count": 0,
    "authors": [
      "Riccardo Barbano",
      "Javier Antoran",
      "Johannes Leuschner",
      "José Miguel Hernández-Lobato",
      "Bangti Jin",
      "Zeljko Kereta"
    ]
  },
  "https://openreview.net/forum?id=0yMuNezwJ1": {
    "title": "On the Dual Problem of Convexified Convolutional Neural Networks",
    "volume": "main",
    "abstract": "We study the dual problem of convexified convolutional neural networks (DCCNNs). First, we introduce a primal learning problem motivated by convexified convolutional neural networks (CCNNs), and then construct the dual convex training program through careful analysis of the Karush-Kuhn-Tucker (KKT) conditions and Fenchel conjugates. Our approach reduces the computational overhead of constructing a large kernel matrix and more importantly, eliminates the ambiguity of factorizing the matrix. Due to the low-rank structure in CCNNs and the related subdifferential of nuclear norms, there is no closed-form expression to recover the primal solution from the dual solution. To overcome this, we propose a highly novel weight recovery algorithm, which takes the dual solution and the kernel information as the input, and recovers the linear weight and the output of convolutional layer, instead of weight parameter. Furthermore, our recovery algorithm exploits the low-rank structure and imposes a small number of filters indirectly, which reduces the parameter size. As a result, DCCNNs inherit all the statistical benefits of CCNNs, while enjoying a more formal and efficient workflow",
    "checked": false,
    "id": "1b533197353c25814dde96bf5b0c7cfcb5fedc18",
    "semantic_title": "on the dual problem of convexiﬁed convolutional neural networks",
    "citation_count": 0,
    "authors": [
      "Site Bai",
      "Chuyang Ke",
      "Jean Honorio"
    ]
  },
  "https://openreview.net/forum?id=xkiflfKCw3": {
    "title": "Evaluating Spatial Understanding of Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) show remarkable capabilities across a variety of tasks. Despite the models only seeing text in training, several recent studies suggest that LLM representations implicitly capture aspects of the underlying grounded concepts. Here, we explore LLM representations of a particularly salient kind of grounded knowledge --- spatial relationships. We design natural-language navigation tasks and evaluate the ability of LLMs, in particular GPT-3.5-turbo, GPT-4, and Llama2 series models, to represent and reason about spatial structures. We also compare these abilities to human performance on the same tasks. These tasks reveal substantial variability in LLM performance across different spatial structures, including square, hexagonal, and triangular grids, rings, and trees. In extensive error analysis, we find that LLMs' mistakes reflect both spatial and non-spatial factors. These findings suggest that LLMs appear to capture certain aspects of spatial structure implicitly, but room for improvement remains",
    "checked": true,
    "id": "a00cb75a2bf2ee20b778ec5587f802ea2db013e3",
    "semantic_title": "evaluating spatial understanding of large language models",
    "citation_count": 1,
    "authors": [
      "Yutaro Yamada",
      "Yihan Bao",
      "Andrew Kyle Lampinen",
      "Jungo Kasai",
      "Ilker Yildirim"
    ]
  },
  "https://openreview.net/forum?id=3PbxuMNQkp": {
    "title": "Robust Learning Rate Selection for Stochastic Optimization via Splitting Diagnostic",
    "volume": "main",
    "abstract": "This paper proposes SplitSGD, a new dynamic learning rate schedule for stochastic optimization. This method decreases the learning rate for better adaptation to the local geometry of the objective function whenever a stationary phase is detected, that is, the iterates are likely to bounce at around a vicinity of a local minimum. The detection is performed by splitting the single thread into two and using the inner product of the gradients from the two threads as a measure of stationarity. Owing to this simple yet provably valid stationarity detection, SplitSGD is easy-to-implement and essentially does not incur additional computational cost than standard SGD. Through a series of extensive experiments, we show that this method is appropriate for both convex problems and training (non-convex) neural networks, with performance compared favorably to other stochastic optimization methods. Importantly, this method is observed to be very robust with a set of default parameters for a wide range of problems and, moreover, can yield better generalization performance than other adaptive gradient methods such as Adam",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matteo Sordello",
      "Niccolo Dalmasso",
      "Hangfeng He",
      "Weijie J Su"
    ]
  },
  "https://openreview.net/forum?id=d3xwrfAG4V": {
    "title": "Transfer Learning for High-dimensional Quantile Regression with Statistical Guarantee",
    "volume": "main",
    "abstract": "The task of transfer learning is to improve estimation/inference of a target model by migrating data from closely related source populations. In this article, we propose transfer learning algorithms for high-dimensional Quantile Regression (QR) models with the technique of convolution-type smoothing. Given the transferable source populations, we derive $\\ell_1/\\ell_2$-estimation error bounds for the estimators of the target regression coefficients under mild conditions. Theoretical analysis shows that the upper bounds are improved over those of the classical penalized QR estimator with only the target data, as long as the target and the sources are sufficiently similar to each other. When the set of informative sources is unknown, a transferable source detection algorithm is proposed to detect informative sources from all available sources. Thorough simulation studies justify our theoretical analysis",
    "checked": true,
    "id": "e5b434cb641b572f60bec85d335227273ca76f52",
    "semantic_title": "transfer learning for high-dimensional quantile regression with statistical guarantee",
    "citation_count": 0,
    "authors": [
      "Sheng Qiao",
      "Yong He",
      "Wenxin Zhou"
    ]
  },
  "https://openreview.net/forum?id=JdXzKSyqbH": {
    "title": "Recovering Exact Support in Federated lasso without Optimization",
    "volume": "main",
    "abstract": "Federated learning provides a framework to address the challenges of distributed computing, data ownership, and privacy over a large number of distributed clients with low computational and communication capabilities. In this paper, we study the problem of learning the exact support of sparse linear regression in the federated learning setup. We provide a simple communication efficient algorithm that only needs one-shot communication with the centralized server to compute the exact support by majority voting. Our method does not require the clients to solve any optimization problem and thus, can be run on devices with low computational capabilities. Our method is naturally robust to the problems of client failure, model poisoning, and straggling clients. We formally prove that our method requires a number of samples per client that is polynomial with respect to the support size, but independent of the dimension of the problem. We require the number of distributed clients to be logarithmic in the dimension of the problem. For certain classes of predictor variables (e.g. mutually independent, correlated Gaussian, etc.), the overall sample complexity matches the optimal sample complexity of the non-federated centralized setting. Furthermore, our method is easy to implement and has an overall polynomial time complexity",
    "checked": true,
    "id": "559e73fc2e9caacb4e8fa86103198150d479d884",
    "semantic_title": "recovering exact support in federated lasso without optimization",
    "citation_count": 0,
    "authors": [
      "Adarsh Barik",
      "Jean Honorio"
    ]
  },
  "https://openreview.net/forum?id=hpKJkVoThY": {
    "title": "Models of human preference for learning reward functions",
    "volume": "main",
    "abstract": "The utility of reinforcement learning is limited by the alignment of reward functions with the interests of human stakeholders. One promising method for alignment is to learn the reward function from human-generated preferences between pairs of trajectory segments, a type of reinforcement learning from human feedback (RLHF). These human preferences are typically assumed to be informed solely by partial return, the sum of rewards along each segment. We find this assumption to be flawed and propose modeling human preferences instead as informed by each segment's regret, a measure of a segment's deviation from optimal decision-making. Given infinitely many preferences generated according to regret, we prove that we can identify a reward function equivalent to the reward function that generated those preferences, and we prove that the previous partial return model lacks this identifiability property in multiple contexts. We empirically show that our proposed regret preference model outperforms the partial return preference model with finite training data in otherwise the same setting. Additionally, we find that our proposed regret preference model better predicts real human preferences and also learns reward functions from these preferences that lead to policies that are better human-aligned. Overall, this work establishes that the choice of preference model is impactful, and our proposed regret preference model provides an improvement upon a core assumption of recent research. We have open sourced our experimental code, the human preferences dataset we gathered, and our training and preference elicitation interfaces for gathering such a dataset",
    "checked": true,
    "id": "9f9b61e429e85e37d6df0e3c478a074f7e6cb9fc",
    "semantic_title": "models of human preference for learning reward functions",
    "citation_count": 18,
    "authors": [
      "W. Bradley Knox",
      "Stephane Hatgis-Kessell",
      "Serena Booth",
      "Scott Niekum",
      "Peter Stone",
      "Alessandro G Allievi"
    ]
  },
  "https://openreview.net/forum?id=ebiAFpQ0Lw": {
    "title": "NorMatch: Matching Normalizing Flows with Discriminative Classifiers for Semi-Supervised Learning",
    "volume": "main",
    "abstract": "Semi-Supervised Learning (SSL) aims to learn a model using a tiny labeled set and massive amounts of unlabeled data. To better exploit the unlabeled data the latest SSL methods use pseudo-labels predicted from \\emph{a single discriminative classifier}. However, the generated pseudo-labels are inevitably linked to inherent confirmation bias and noise which greatly affects the model performance. In this work, we introduce a new framework for SSL named NorMatch. Firstly, we introduce a new uncertainty estimation scheme based on normalizing flows, as an auxiliary classifier, to enforce highly certain pseudo-labels yielding a boost of the discriminative classifiers. Secondly, we introduce a threshold-free sample weighting strategy to exploit better both high and low confidence pseudo-labels. Furthermore, we utilize normalizing flows to model, in an unsupervised fashion, the distribution of unlabeled data. This modelling assumption can further improve the performance of generative classifiers via unlabeled data, and thus, implicitly contributing to training a better discriminative classifier. We demonstrate, through numerical and visual results, that NorMatch achieves state-of-the-art performance on several datasets",
    "checked": true,
    "id": "383a44d071b8125ff864ffb3c5129e1d03cdee10",
    "semantic_title": "normatch: matching normalizing flows with discriminative classifiers for semi-supervised learning",
    "citation_count": 0,
    "authors": [
      "Zhongying Deng",
      "Rihuan Ke",
      "Carola-Bibiane Schönlieb",
      "Angelica I Aviles-Rivero"
    ]
  },
  "https://openreview.net/forum?id=CyjG4ZKCtE": {
    "title": "Mitigating Off-Policy Bias in Actor-Critic Methods with One-Step Q-learning: A Novel Correction Approach",
    "volume": "main",
    "abstract": "Compared to on-policy counterparts, off-policy model-free deep reinforcement learning can improve data efficiency by repeatedly using the previously gathered data. However, off-policy learning becomes challenging when the discrepancy between the underlying distributions of the agent's policy and collected data increases. Although the well-studied importance sampling and off-policy policy gradient techniques were proposed to compensate for this discrepancy, they usually require a collection of long trajectories and induce additional problems such as vanishing/exploding gradients or discarding many useful experiences, which eventually increases the computational complexity. Moreover, their generalization to either continuous action domains or policies approximated by deterministic deep neural networks is strictly limited. To overcome these limitations, we introduce a novel policy similarity measure to mitigate the effects of such discrepancy in continuous control. Our method offers an adequate single-step off-policy correction that is applicable to deterministic policy networks. Theoretical and empirical studies demonstrate that it can achieve a \"safe\" off-policy learning and substantially improve the state-of-the-art by attaining higher returns in fewer steps than the competing methods through an effective schedule of the learning rate in Q-learning and policy optimization",
    "checked": true,
    "id": "1dd9c9eb635fe8f77b1c4d5ed958f26612f90c2b",
    "semantic_title": "mitigating off-policy bias in actor-critic methods with one-step q-learning: a novel correction approach",
    "citation_count": 0,
    "authors": [
      "Baturay Saglam",
      "Doğan Can Çiçek",
      "Furkan Burak Mutlu",
      "Suleyman Kozat"
    ]
  },
  "https://openreview.net/forum?id=SSqOqAwpN7": {
    "title": "Provable Guarantees for Sparsity Recovery with Deterministic Missing Data Patterns",
    "volume": "main",
    "abstract": "We study the problem of consistently recovering the sparsity pattern of a regression parameter vector from correlated observations governed by deterministic missing data patterns using Lasso. We consider the case in which the observed dataset is censored by a deterministic, non-uniform filter. Recovering the sparsity pattern in datasets with deterministic missing structure can be arguably more challenging than recovering in a uniformly-at-random scenario. In this paper, we propose an efficient algorithm for missing value imputation by utilizing the topological property of the censorship filter. We then provide novel theoretical results for exact recovery of the sparsity pattern using the proposed imputation strategy. Our analysis shows that, under certain statistical and topological conditions, the hidden sparsity pattern can be recovered consistently with high probability in polynomial time and logarithmic sample complexity",
    "checked": true,
    "id": "6f13e295b6d50e025a35784d954b5d44ba0b67c7",
    "semantic_title": "provable guarantees for sparsity recovery with deterministic missing data patterns",
    "citation_count": 0,
    "authors": [
      "Chuyang Ke",
      "Jean Honorio"
    ]
  },
  "https://openreview.net/forum?id=dUVejidXO7": {
    "title": "Visual Prompt Based Personalized Federated Learning",
    "volume": "main",
    "abstract": "As a popular paradigm of distributed learning, personalized federated learning (PFL) allows personalized models to improve generalization ability and robustness by utilizing knowledge from all distributed clients. Most existing PFL algorithms tackle personalization in a model-centric way, such as personalized layer partition, model regularization, and model interpolation, which all fail to take into account the data characteristics of distributed clients. In this paper, we propose a novel PFL framework for image classification tasks, dubbed pFedPT, that leverages personalized visual prompts to implicitly represent local data distribution information of clients and provides that information to the aggregation model to help with classification tasks. Specifically, in each round of pFedPT training, each client generates a local personalized prompt related to local data distribution. Then, the local model is trained on the input composed of raw data and a visual prompt to learn the distribution information contained in the prompt. During model testing, the aggregated model obtains client-specific knowledge of the data distributions based on the prompts, which can be seen as an adaptive fine-tuning of the aggregation model to improve model performances on different clients. Furthermore, the visual prompt can be added as an orthogonal method to implement personalization on the client for existing FL methods to boost their performance. Experiments on the CIFAR10 and CIFAR100 datasets show that pFedPT outperforms several state-of-the-art (SOTA) PFL algorithms by a large margin in various settings. The code is available at: https://github.com/hkgdifyu/pFedPT",
    "checked": true,
    "id": "92c2c090ad911db57166821f8494de60fafe7d1d",
    "semantic_title": "visual prompt based personalized federated learning",
    "citation_count": 8,
    "authors": [
      "Guanghao Li",
      "Wansen Wu",
      "Yan Sun",
      "Li Shen",
      "Baoyuan Wu",
      "Dacheng Tao"
    ]
  },
  "https://openreview.net/forum?id=qKIvn9xL1R": {
    "title": "CR-MoE: Consistent Routed Mixture-of-Experts for Scaling Contrastive Learning",
    "volume": "main",
    "abstract": "While Contrastive Learning (CL) achieves great success in many downstream tasks, its good performance heavily relies on a large model capacity. As previous methods focus on scaling dense models, training and inference costs increase rapidly with model sizes, leading to large resource consumption. In this paper, we explore CL with an efficient scaling method, Mixture of Experts (MoE), to obtain a large but sparse model. We start by plugging in the state-of-the-art CL method to MoE. However, this naive combination fails to visibly improve performance despite a much larger capacity. A closer look reveals that the naive MoE+CL model has a strong tendency to route two augmented views of the same image token to different subsets of experts: such ``cross-view instability\" breaks the weight-sharing nature in CL and misleads the invariant feature learning. To address this issue, we introduce a new regularization mechanism, by enforcing expert-routing similarity between different views of the same image (or its overlapped patch tokens), while promoting expert-routing diversity of patches from different images. The resultant method, called CR-MoE, improves by 1.7 points in terms of 1\\% semi-supervised learning accuracy on ImageNet, compared to the naive combination baseline. It further surpasses the state-of-the-art CL methods on ImageNet pre-training of Vision Transformer (ViT) by 2.8 points, at the same computational cost. Our findings validate CR-MoE as an effective and efficient image representation learner. Code is available at https://github.com/VITA-Group/CRMoE",
    "checked": true,
    "id": "40891680185dd23b1270cb814ea555811c3b4618",
    "semantic_title": "cr-moe: consistent routed mixture-of-experts for scaling contrastive learning",
    "citation_count": 0,
    "authors": [
      "Ziyu Jiang",
      "Guoqing Zheng",
      "Yu Cheng",
      "Ahmed Hassan Awadallah",
      "Zhangyang Wang"
    ]
  },
  "https://openreview.net/forum?id=uqQPyWFDhY": {
    "title": "Error Bounds for Flow Matching Methods",
    "volume": "main",
    "abstract": "Score-based generative models are a popular class of generative modelling techniques relying on stochastic differential equations (SDEs). From their inception, it was realized that it was also possible to perform generation using ordinary differential equations (ODEs) rather than SDEs. This led to the introduction of the probability flow ODE approach and denoising diffusion implicit models. Flow matching methods have recently further extended these ODE-based approaches and approximate a flow between two arbitrary probability distributions. Previous work derived bounds on the approximation error of diffusion models under the stochastic sampling regime, given assumptions on the $L^2$ loss. We present error bounds for the flow matching procedure using fully deterministic sampling, assuming an $L^2$ bound on the approximation error and a certain regularity condition on the data distributions",
    "checked": true,
    "id": "465ea43827d145fef68e100bb367208ad85678bb",
    "semantic_title": "error bounds for flow matching methods",
    "citation_count": 12,
    "authors": [
      "Joe Benton",
      "George Deligiannidis",
      "Arnaud Doucet"
    ]
  },
  "https://openreview.net/forum?id=17ESEjETbP": {
    "title": "Non-Uniform Smoothness for Gradient Descent",
    "volume": "main",
    "abstract": "The analysis of gradient descent-type methods typically relies on the Lipschitz continuity of the objective gradient. This generally requires an expensive hyperparameter tuning process to appropriately calibrate a stepsize for a given problem. In this work we introduce a local first-order smoothness oracle (LFSO) which generalizes the Lipschitz continuous gradients smoothness condition and is applicable to any twice-differentiable function. We show that this oracle can encode all relevant problem information for tuning stepsizes for a suitably modified gradient descent method and give global and local convergence results. We also show that LFSOs in this modified first-order method can yield global linear convergence rates for non-strongly convex problems with extremely flat minima, and thus improve over the lower bound on rates achievable by general (accelerated) first-order methods",
    "checked": true,
    "id": "ab21e7ee715a02b76c209e20d90a58f758cbcad8",
    "semantic_title": "non-uniform smoothness for gradient descent",
    "citation_count": 1,
    "authors": [
      "Albert S. Berahas",
      "Lindon Roberts",
      "Fred Roosta"
    ]
  },
  "https://openreview.net/forum?id=Jy2IgzjoFH": {
    "title": "Hierarchical Neural Simulation-Based Inference Over Event Ensembles",
    "volume": "main",
    "abstract": "When analyzing real-world data it is common to work with event ensembles, which comprise sets of observations that collectively constrain the parameters of an underlying model of interest. Such models often have a hierarchical structure, where ``local'' parameters impact individual events and ``global'' parameters influence the entire dataset. We introduce practical approaches for frequentist and Bayesian dataset-wide probabilistic inference in cases where the likelihood is intractable, but simulations can be realized via a hierarchical forward model. We construct neural estimators for the likelihood(-ratio) or posterior and show that explicitly accounting for the model's hierarchical structure can lead to significantly tighter parameter constraints. We ground our discussion using case studies from the physical sciences, focusing on examples from particle physics and cosmology",
    "checked": true,
    "id": "69667fd30ec42d34e78345bb6e19071e6e5a9b5b",
    "semantic_title": "hierarchical neural simulation-based inference over event ensembles",
    "citation_count": 1,
    "authors": [
      "Lukas Heinrich",
      "Siddharth Mishra-Sharma",
      "Chris Pollard",
      "Philipp Windischhofer"
    ]
  },
  "https://openreview.net/forum?id=HyqSwNhM3x": {
    "title": "What is the Solution for State-Adversarial Multi-Agent Reinforcement Learning?",
    "volume": "main",
    "abstract": "Various methods for Multi-Agent Reinforcement Learning (MARL) have been developed with the assumption that agents' policies are based on accurate state information. However, policies learned through Deep Reinforcement Learning (DRL) are susceptible to adversarial state perturbation attacks. In this work, we propose a State-Adversarial Markov Game (SAMG) and make the first attempt to investigate different solution concepts of MARL under state uncertainties. Our analysis shows that the commonly used solution concepts of optimal agent policy and robust Nash equilibrium do not always exist in SAMGs. To circumvent this difficulty, we consider a new solution concept called robust agent policy, where agents aim to maximize the worst-case expected state value. We prove the existence of robust agent policy for finite state and finite action SAMGs. Additionally, we propose a Robust Multi-Agent Adversarial Actor-Critic (RMA3C) algorithm to learn robust policies for MARL agents under state uncertainties. Our experiments demonstrate that our algorithm outperforms existing methods when faced with state perturbations and greatly improves the robustness of MARL policies. Our code is public on https://songyanghan.github.io/what_is_solution/",
    "checked": true,
    "id": "cfd2c668504c0a97e73fe6e40fe7fc869aa5a40a",
    "semantic_title": "what is the solution for state-adversarial multi-agent reinforcement learning?",
    "citation_count": 19,
    "authors": [
      "Songyang Han",
      "Sanbao Su",
      "Sihong He",
      "Shuo Han",
      "Haizhao Yang",
      "Shaofeng Zou",
      "Fei Miao"
    ]
  },
  "https://openreview.net/forum?id=Y2ru0LuQeS": {
    "title": "MESSY Estimation: Maximum-Entropy based Stochastic and Symbolic densitY Estimation",
    "volume": "main",
    "abstract": "We introduce MESSY estimation, a Maximum-Entropy based Stochastic and Symbolic densitY estimation method. The proposed approach recovers probability density functions symbolically from samples using moments of a Gradient flow in which the ansatz serves as the driving force. In particular, we construct a gradient-based drift-diffusion process that connects samples of the unknown distribution function to a guess symbolic expression. We then show that when the guess distribution has the maximum entropy form, the parameters of this distribution can be found efficiently by solving a linear system of equations constructed using the moments of the provided samples. Furthermore, we use Symbolic regression to explore the space of smooth functions and find optimal basis functions for the exponent of the maximum entropy functional leading to good conditioning. The cost of the proposed method for each set of selected basis functions is linear with the number of samples and quadratic with the number of basis functions. However, the underlying acceptance/rejection procedure for finding optimal and well-conditioned bases adds to the computational cost. We validate the proposed MESSY estimation method against other benchmark methods for the case of a bi-modal and a discontinuous density, as well as a density at the limit of physical realizability. We find that the addition of a symbolic search for basis functions improves the accuracy of the estimation at a reasonable additional computational cost. Our results suggest that the proposed method outperforms existing density recovery methods in the limit of a small to moderate number of samples by providing a low-bias and tractable symbolic description of the unknown density at a reasonable computational cost",
    "checked": true,
    "id": "f248be199edf5dce636f958814d50f9b9e52bc34",
    "semantic_title": "messy estimation: maximum-entropy based stochastic and symbolic density estimation",
    "citation_count": 3,
    "authors": [
      "Tony Tohme",
      "Mohsen Sadr",
      "KAMAL YOUCEF-TOUMI",
      "Nicolas Hadjiconstantinou"
    ]
  },
  "https://openreview.net/forum?id=g01OVahtN9": {
    "title": "Controlling Federated Learning for Covertness",
    "volume": "main",
    "abstract": "A learner aims to minimize a function $f$ by repeatedly querying a distributed oracle that provides noisy gradient evaluations. At the same time, the learner seeks to hide $\\arg\\min f$ from a malicious eavesdropper that observes the learner's queries. This paper considers the problem of \\textit{covert} or \\textit{learner-private} optimization, where the learner has to dynamically choose between learning and obfuscation by exploiting the stochasticity. The problem of controlling the stochastic gradient algorithm for covert optimization is modeled as a Markov decision process, and we show that the dynamic programming operator has a supermodular structure implying that the optimal policy has a monotone threshold structure. A computationally efficient policy gradient algorithm is proposed to search for the optimal querying policy without knowledge of the transition probabilities. As a practical application, our methods are demonstrated on a hate speech classification task in a federated setting where an eavesdropper can use the optimal weights to generate toxic content, which is more easily misclassified. Numerical results show that when the learner uses the optimal policy, an eavesdropper can only achieve a validation accuracy of $52\\%$ with no information and $69\\%$ when it has a public dataset with $10\\%$ positive samples compared to $83\\%$ when the learner employs a greedy policy",
    "checked": true,
    "id": "2a864f9dbd52e8965818cce75b2b51fecc6d47f4",
    "semantic_title": "controlling federated learning for covertness",
    "citation_count": 3,
    "authors": [
      "Adit Jain",
      "Vikram Krishnamurthy"
    ]
  },
  "https://openreview.net/forum?id=ynG5Ak7n7Q": {
    "title": "The Fair Value of Data Under Heterogeneous Privacy Constraints in Federated Learning",
    "volume": "main",
    "abstract": "Modern data aggregation often involves a platform collecting data from a network of users with various privacy options. Platforms must solve the problem of how to allocate incentives to users to convince them to share their data. This paper puts forth an idea for a fair amount to compensate users for their data at a given privacy level based on an axiomatic definition of fairness, along the lines of the celebrated Shapley value. To the best of our knowledge, these are the first fairness concepts for data that explicitly consider privacy constraints. We also formulate a heterogeneous federated learning problem for the platform with privacy level options for users. By studying this problem, we investigate the amount of compensation users receive under fair allocations with different privacy levels, amounts of data, and degrees of heterogeneity. We also discuss what happens when the platform is forced to design fair incentives. Under certain conditions we find that when privacy sensitivity is low, the platform will set incentives to ensure that it collects all the data with the lowest privacy options. When the privacy sensitivity is above a given threshold, the platform will provide no incentives to users. Between these two extremes, the platform will set the incentives so some fraction of the users chooses the higher privacy option and the others chooses the lower privacy option",
    "checked": true,
    "id": "3fa0bf2a4f563f25514bafe79098d2ea18f0f56d",
    "semantic_title": "the fair value of data under heterogeneous privacy constraints in federated learning",
    "citation_count": 3,
    "authors": [
      "Justin Singh Kang",
      "Ramtin Pedarsani",
      "Kannan Ramchandran"
    ]
  },
  "https://openreview.net/forum?id=pWsfWDnJDa": {
    "title": "Out-of-Distribution Optimality of Invariant Risk Minimization",
    "volume": "main",
    "abstract": "Deep Neural Networks often inherit spurious correlations embedded in training data and hence may fail to generalize to unseen domains, which have different distributions from the domain to provide training data. M. Arjovsky et al. (2019) introduced the concept out-of-distribution (o.o.d.) risk, which is the maximum risk among all domains, and formulated the issue caused by spurious correlations as a minimization problem of the o.o.d. risk. Invariant Risk Minimization (IRM) is considered to be a promising approach to minimize the o.o.d. risk: IRM estimates a minimum of the o.o.d. risk by solving a bi-level optimization problem. While IRM has attracted considerable attention with empirical success, it comes with few theoretical guarantees. Especially, a solid theoretical guarantee that the bi-level optimization problem gives the minimum of the o.o.d. risk has not yet been established. Aiming at providing a theoretical justification for IRM, this paper rigorously proves that a solution to the bi-level optimization problem minimizes the o.o.d. risk under certain conditions. The result also provides sufficient conditions on distributions providing training data and on a dimension of feature space for the bi-leveled optimization problem to minimize the o.o.d. risk",
    "checked": true,
    "id": "1b326b04bfe326295e588e18ebfd94b26a878c85",
    "semantic_title": "out-of-distribution optimality of invariant risk minimization",
    "citation_count": 0,
    "authors": [
      "Shoji Toyota",
      "Kenji Fukumizu"
    ]
  },
  "https://openreview.net/forum?id=ZLVbQEu4Ab": {
    "title": "When is Momentum Extragradient Optimal? A Polynomial-Based Analysis",
    "volume": "main",
    "abstract": "The extragradient method has gained popularity due to its robust convergence properties for differentiable games. Unlike single-objective optimization, game dynamics involve complex interactions reflected by the eigenvalues of the game vector field's Jacobian scattered across the complex plane. This complexity can cause the simple gradient method to diverge, even for bilinear games, while the extragradient method achieves convergence. Building on the recently proven accelerated convergence of the momentum extragradient method for bilinear games \\citep{azizian2020accelerating}, we use a polynomial-based analysis to identify three distinct scenarios where this method exhibits further accelerated convergence. These scenarios encompass situations where the eigenvalues reside on the (positive) real line, lie on the real line alongside complex conjugates, or exist solely as complex conjugates. Furthermore, we derive the hyperparameters for each scenario that achieve the fastest convergence rate",
    "checked": true,
    "id": "1412997ab05fabe40a9dec6e1d8622c8480a8c71",
    "semantic_title": "when is momentum extragradient optimal? a polynomial-based analysis",
    "citation_count": 0,
    "authors": [
      "Junhyung Lyle Kim",
      "Gauthier Gidel",
      "Anastasios Kyrillidis",
      "Fabian Pedregosa"
    ]
  },
  "https://openreview.net/forum?id=Wqn8zirthg": {
    "title": "DDLP: Unsupervised Object-centric Video Prediction with Deep Dynamic Latent Particles",
    "volume": "main",
    "abstract": "We propose a new object-centric video prediction algorithm based on the deep latent particle (DLP) representation of Daniel and Tamar (2022). In comparison to existing slot- or patch-based representations, DLPs model the scene using a set of keypoints with learned parameters for properties such as position and size, and are both efficient and interpretable. Our method, \\textit{deep dynamic latent particles} (DDLP), yields state-of-the-art object-centric video prediction results on several challenging datasets. The interpretable nature of DDLP allows us to perform ``what-if'' generation -- predict the consequence of changing properties of objects in the initial frames, and DLP's compact structure enables efficient diffusion-based unconditional video generation. Videos, code and pre-trained models are available: https://taldatech.github.io/ddlp-web",
    "checked": true,
    "id": "7d22e6d110a2be18ef7236fb2238fd85463de4b2",
    "semantic_title": "ddlp: unsupervised object-centric video prediction with deep dynamic latent particles",
    "citation_count": 3,
    "authors": [
      "Tal Daniel",
      "Aviv Tamar"
    ]
  },
  "https://openreview.net/forum?id=vWTZO1RXZR": {
    "title": "Introspective Experience Replay: Look Back When Surprised",
    "volume": "main",
    "abstract": "In reinforcement learning (RL), experience replay-based sampling techniques are crucial in promoting convergence by eliminating spurious correlations. However, widely used methods such as uniform experience replay (UER) and prioritized experience replay (PER) have been shown to have sub-optimal convergence and high seed sensitivity, respectively. To address these issues, we propose a novel approach called Introspective Experience Replay (IER) that selectively samples batches of data points prior to surprising events. Our method is inspired from the reverse experience replay (RER) technique, which has been shown to reduce bias in the output of Q-learning-type algorithms with linear function approximation. However, RER is not always practically reliable when using neural function approximation. Through empirical evaluations, we demonstrate that IER with neural function approximation yields reliable and superior performance compared to UER, PER, and hindsight experience replay (HER) across most tasks",
    "checked": true,
    "id": "e6bd20a796c48f9c17ef55c6a821ec678a341b3b",
    "semantic_title": "introspective experience replay: look back when surprised",
    "citation_count": 0,
    "authors": [
      "Ramnath Kumar",
      "Dheeraj Mysore Nagaraj"
    ]
  },
  "https://openreview.net/forum?id=O9RUANpPmb": {
    "title": "Domain-Generalizable Multiple-Domain Clustering",
    "volume": "main",
    "abstract": "This work generalizes the problem of unsupervised domain generalization to the case in which no labeled samples are available (completely unsupervised). We are given unlabeled samples from multiple source domains, and we aim to learn a shared predictor that assigns examples to semantically related clusters. Evaluation is done by predicting cluster assignments in previously unseen domains. Towards this goal, we propose a two-stage training framework: (1) self-supervised pre-training for extracting domain invariant semantic features. (2) multi-head cluster prediction with pseudo labels, which rely on both the feature space and cluster head prediction, further leveraging a novel prediction-based label smoothing scheme. We demonstrate empirically that our model is more accurate than baselines that require fine-tuning using samples from the target domain or some level of supervision. Our code is available at \\url{https://github.com/AmitRozner/domain-generalizable-multiple-domain-clustering}",
    "checked": true,
    "id": "dfe2fffe974ec43dff94f02228d545270e3e63fb",
    "semantic_title": "domain-generalizable multiple-domain clustering",
    "citation_count": 3,
    "authors": [
      "Amit Rozner",
      "Barak Battash",
      "Lior Wolf",
      "Ofir Lindenbaum"
    ]
  },
  "https://openreview.net/forum?id=vsCpILiWHu": {
    "title": "RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation",
    "volume": "main",
    "abstract": "The ability to leverage heterogeneous robotic experience from different robots and tasks to quickly master novel skills and embodiments has the potential to transform robot learning. Inspired by recent advances in foundation models for vision and language, we propose a multi-embodiment, multi-task generalist agent for robotic manipulation. This agent, named RoboCat, is a visual goal-conditioned decision transformer capable of consuming action-labelled visual experience. This data spans a large repertoire of motor control skills from simulated and real robotic arms with varying sets of observations and actions. With RoboCat, we demonstrate the ability to generalise to new tasks and robots, both zero-shot as well as through adaptation using only 100–1000 examples for the target task. We also show how a trained model itself can be used to generate data for subsequent training iterations, thus providing a basic building block for an autonomous improvement loop. We investigate the agent's capabilities, with large-scale evaluations both in simulation and on three different real robot embodiments. We find that as we grow and diversify its training data, RoboCat not only shows signs of cross-task transfer, but also becomes more efficient at adapting to new tasks",
    "checked": true,
    "id": "2562fe379554d201aad312f786903f4c60b68acf",
    "semantic_title": "robocat: a self-improving generalist agent for robotic manipulation",
    "citation_count": 21,
    "authors": [
      "Konstantinos Bousmalis",
      "Giulia Vezzani",
      "Dushyant Rao",
      "Coline Manon Devin",
      "Alex X. Lee",
      "Maria Bauza Villalonga",
      "Todor Davchev",
      "Yuxiang Zhou",
      "Agrim Gupta",
      "Akhil Raju",
      "Antoine Laurens",
      "Claudio Fantacci",
      "Valentin Dalibard",
      "Martina Zambelli",
      "Murilo Fernandes Martins",
      "Rugile Pevceviciute",
      "Michiel Blokzijl",
      "Misha Denil",
      "Nathan Batchelor",
      "Thomas Lampe",
      "Emilio Parisotto",
      "Konrad Zolna",
      "Scott Reed",
      "Sergio Gómez Colmenarejo",
      "Jonathan Scholz",
      "Abbas Abdolmaleki",
      "Oliver Groth",
      "Jean-Baptiste Regli",
      "Oleg Sushkov",
      "Thomas Rothörl",
      "Jose Enrique Chen",
      "Yusuf Aytar",
      "David Barker",
      "Joy Ortiz",
      "Martin Riedmiller",
      "Jost Tobias Springenberg",
      "Raia Hadsell",
      "Francesco Nori",
      "Nicolas Heess"
    ]
  },
  "https://openreview.net/forum?id=Igxp7FC8uf": {
    "title": "Fixed-Budget Best-Arm Identification in Sparse Linear Bandits",
    "volume": "main",
    "abstract": "We study the best-arm identification problem in sparse linear bandits under the fixed-budget setting. In sparse linear bandits, the unknown feature vector $\\theta^*$ may be of large dimension $d$, but only a few, say $s \\ll d$ of these features have non-zero values. We design a two-phase algorithm, Lasso and Optimal-Design- (Lasso-OD) based linear best-arm identification. The first phase of Lasso-OD leverages the sparsity of the feature vector by applying the thresholded Lasso introduced by Zhou (2009), which estimates the support of $\\theta^*$ correctly with high probability using rewards from the selected arms and a judicious choice of the design matrix. The second phase of Lasso-OD applies the OD-LinBAI algorithm by Yang and Tan (2022) on that estimated support. We derive a non-asymptotic upper bound on the error probability of Lasso-OD by carefully choosing hyperparameters (such as Lasso's regularization parameter) and balancing the error probabilities of both phases. For fixed sparsity $s$ and budget $T$, the exponent in the error probability of Lasso-OD depends on $s$ but not on the dimension $d$, yielding a significant performance improvement for sparse and high-dimensional linear bandits. Furthermore, we show that Lasso-OD is almost minimax optimal in the exponent. Finally, we provide numerical examples to demonstrate the significant performance improvement over the existing algorithms for non-sparse linear bandits such as OD-LinBAI, BayesGap, Peace, LinearExploration, and GSE",
    "checked": true,
    "id": "e59f0a8bceffaefb7f646723e53afcb3ff3bc9e0",
    "semantic_title": "fixed-budget best-arm identification in sparse linear bandits",
    "citation_count": 1,
    "authors": [
      "Recep Can Yavas",
      "Vincent Y. F. Tan"
    ]
  },
  "https://openreview.net/forum?id=6BDHUkSPna": {
    "title": "Understanding the Role of Layer Normalization in Label-Skewed Federated Learning",
    "volume": "main",
    "abstract": "Layer normalization (LN) is a widely adopted deep learning technique especially in the era of foundation models. Recently, LN has been shown to be surprisingly effective in federated learning (FL) with non-i.i.d. data. However, exactly why and how it works remains mysterious. In this work, we reveal the profound connection between layer normalization and the label shift problem in federated learning. To understand layer normalization better in FL, we identify the key contributing mechanism of normalization methods in FL, called feature normalization (FN), which applies normalization to the latent feature representation before the classifier head. Although LN and FN do not improve expressive power, they control feature collapse and local overfitting to heavily skewed datasets, and thus accelerates global training. Empirically, we show that normalization leads to drastic improvements on standard benchmarks under extreme label shift. Moreover, we conduct extensive ablation studies to understand the critical factors of layer normalization in FL. Our results verify that FN is an essential ingredient inside LN to significantly improve the convergence of FL while remaining robust to learning rate choices, especially under extreme label shift where each client has access to few classes",
    "checked": true,
    "id": "dcf939f84b63e38a0235868ac3e7b9f99a196765",
    "semantic_title": "understanding the role of layer normalization in label-skewed federated learning",
    "citation_count": 1,
    "authors": [
      "Guojun Zhang",
      "Mahdi Beitollahi",
      "Alex Bie",
      "Xi Chen"
    ]
  },
  "https://openreview.net/forum?id=KKARKoPcEA": {
    "title": "Learning to Abstain From Uninformative Data",
    "volume": "main",
    "abstract": "Learning and decision-making in domains with naturally high noise-to-signal ratios – such as Finance or Healthcare – is often challenging, while the stakes are very high. In this paper, we study the problem of learning and acting under a general noisy generative process. In this problem, the data distribution has a significant proportion of uninformative samples with high noise in the label, while part of the data contains useful information represented by low label noise. This dichotomy is present during both training and inference, which requires the proper handling of uninformative data during both training and testing. We propose a novel approach to learning under these conditions via a loss inspired by the selective learning theory. By minimizing this loss, the model is guaranteed to make a near-optimal decision by distinguishing informative data from uninformative data and making predictions. We build upon the strength of our theoretical guarantees by describing an iterative algorithm, which jointly optimizes both a predictor and a selector, and evaluates its empirical performance in a variety of settings",
    "checked": true,
    "id": "6df82e80441fc2a67c6ad81879001313fab36568",
    "semantic_title": "learning to abstain from uninformative data",
    "citation_count": 0,
    "authors": [
      "Yikai Zhang",
      "Songzhu Zheng",
      "Mina Dalirrooyfard",
      "Pengxiang Wu",
      "Anderson Schneider",
      "Anant Raj",
      "Yuriy Nevmyvaka",
      "Chao Chen"
    ]
  },
  "https://openreview.net/forum?id=6wpInwnzs8": {
    "title": "WaveBench: Benchmarking Data-driven Solvers for Linear Wave Propagation PDEs",
    "volume": "main",
    "abstract": "Wave-based imaging techniques play a critical role in diverse scientific, medical, and industrial endeavors, from discovering hidden structures beneath the Earth's surface to ultrasound diagnostics. They rely on accurate solutions to the forward and inverse problems for partial differential equations (PDEs) that govern wave propagation. Surrogate PDE solvers based on machine learning emerged as an effective approach to computing the solutions more efficiently than via classical numerical schemes. However, existing datasets for PDE surrogates offer only limited coverage of the wave propagation phenomenon. In this paper, we present WaveBench, a comprehensive collection of benchmark datasets for wave propagation PDEs. WaveBench (1) contains 24 datasets that cover a wide range of forward and inverse problems for time-harmonic and time-varying wave phenomena; (2) includes a user-friendly PyTorch environment for comparing learning-based methods; and (3) comprises reference performance and model checkpoints of popular PDE surrogates such as Fourier neural operators and U-Nets. Our evaluation on WaveBench demonstrates the impressive performance of PDE surrogates on in-distribution samples, while simultaneously unveiling their limitations on out-of-distribution samples, indicating room for future improvements. We anticipate that WaveBench will stimulate the development of accurate wave-based imaging techniques through machine learning",
    "checked": true,
    "id": "ad319a06082af904a1c1b4ee82a96e9b159d2eff",
    "semantic_title": "wavebench: benchmarking data-driven solvers for linear wave propagation pdes",
    "citation_count": 0,
    "authors": [
      "Tianlin Liu",
      "Jose Antonio Lara Benitez",
      "Florian Faucher",
      "AmirEhsan Khorashadizadeh",
      "Maarten V. de Hoop",
      "Ivan Dokmanić"
    ]
  },
  "https://openreview.net/forum?id=fUhOb14sQv": {
    "title": "Using Motion Cues to Supervise Single-frame Body Pose & Shape Estimation in Low Data Regimes",
    "volume": "main",
    "abstract": "When enough annotated training data is available, supervised deep-learning algorithms excel at estimating human body pose and shape using a single camera. The effects of too little such data being available can be mitigated by using other information sources, such as databases of body shapes, to learn priors. Unfortunately, such sources are not always available either. We show that, in such cases, easy to-obtain unannotated videos can be used instead to provide the required supervisory signals. Given a trained model using too little annotated data, we compute poses in consecutive frames along with the optical flow between them. We then enforce consistency between the image optical flow and the one that can be inferred from the change in pose from one frame to the next. This provides enough additional supervision to effectively refine the network weights and to perform on par with methods trained using far more annotated data",
    "checked": false,
    "id": "04ebb3d96065b4296c85dcd030ac1ea9654c9412",
    "semantic_title": "using motion cues to supervise single-frame body pose and shape estimation in low data regimes",
    "citation_count": 0,
    "authors": [
      "Andrey Davydov",
      "Alexey Sidnev",
      "Artsiom Sanakoyeu",
      "Yuhua Chen",
      "Mathieu Salzmann",
      "Pascal Fua"
    ]
  },
  "https://openreview.net/forum?id=E8m8oySvPJ": {
    "title": "LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations",
    "volume": "main",
    "abstract": "Can a Large Language Model (LLM) solve simple abstract reasoning problems? We explore this broad question through a systematic analysis of GPT on the Abstraction and Reasoning Corpus (ARC), a representative benchmark of abstract reasoning ability from limited examples in which solutions require some \"core knowledge\" of concepts such as objects, goal states, counting, and basic geometry. GPT-4 solves only 13/50 of the most straightforward ARC tasks when using textual encodings for their two-dimensional input-output grids. Our failure analysis reveals that GPT-4's capacity to identify objects and reason about them is significantly influenced by the sequential nature of the text that represents an object within a text encoding of a task. To test this hypothesis, we design a new benchmark, the 1D-ARC, which consists of one-dimensional (array-like) tasks that are more conducive to GPT-based reasoning, and where it indeed performs better than on the (2D) ARC. To alleviate this issue, we propose an object-based representation that is obtained through an external tool, resulting in nearly doubling the performance on solved ARC tasks and near-perfect scores on the easier 1D-ARC. Although the state-of-the-art GPT-4 is unable to \"reason\" perfectly within non-language domains such as the 1D-ARC or a simple ARC subset, our study reveals that the use of object-based representations can significantly improve its reasoning ability. Visualizations, GPT logs, and data are available at https://khalil-research.github.io/LLM4ARC",
    "checked": true,
    "id": "8826311d922135dbf0cfdb4a661ebab347e3b826",
    "semantic_title": "llms and the abstraction and reasoning corpus: successes, failures, and the importance of object-based representations",
    "citation_count": 14,
    "authors": [
      "Yudong Xu",
      "Wenhao Li",
      "Pashootan Vaezipoor",
      "Scott Sanner",
      "Elias Boutros Khalil"
    ]
  },
  "https://openreview.net/forum?id=fJAwemcvpL": {
    "title": "Candidate Set Re-ranking for Composed Image Retrieval with Dual Multi-modal Encoder",
    "volume": "main",
    "abstract": "Composed image retrieval aims to find an image that best matches a given multi-modal user query consisting of a reference image and text pair. Existing methods commonly pre-compute image embeddings over the entire corpus and compare these to a reference image embedding modified by the query text at test time. Such a pipeline is very efficient at test time since fast vector distances can be used to evaluate candidates, but modifying the reference image embedding guided only by a short textual description can be difficult, especially independent of potential candidates. An alternative approach is to allow interactions between the query and every possible candidate, i.e., reference-text-candidate triplets, and pick the best from the entire set. Though this approach is more discriminative, for large-scale datasets the computational cost is prohibitive since pre-computation of candidate embeddings is no longer possible. We propose to combine the merits of both schemes using a two-stage model. Our first stage adopts the conventional vector distancing metric and performs a fast pruning among candidates. Meanwhile, our second stage employs a dual-encoder architecture, which effectively attends to the input triplet of reference-text-candidate and re-ranks the candidates. Both stages utilize a vision-and-language pre-trained network, which has proven beneficial for various downstream tasks. Our method consistently outperforms state-of-the-art approaches on standard benchmarks for the task. Our implementation is available at https://github.com/Cuberick-Orion/Candidate-Reranking-CIR",
    "checked": true,
    "id": "5eceaeac5d45d49e1d5698947ed8292ff3fccd81",
    "semantic_title": "candidate set re-ranking for composed image retrieval with dual multi-modal encoder",
    "citation_count": 4,
    "authors": [
      "Zheyuan Liu",
      "Weixuan Sun",
      "Damien Teney",
      "Stephen Gould"
    ]
  },
  "https://openreview.net/forum?id=jesKcQxQ7j": {
    "title": "A Review of the Applications of Deep Learning-Based Emergent Communication",
    "volume": "main",
    "abstract": "Emergent communication, or emergent language, is the field of research which studies how human language-like communication systems emerge de novo in deep multi-agent reinforcement learning environments. The possibilities of replicating the emergence of a complex behavior like language have strong intuitive appeal, yet it is necessary to complement this with clear notions of how such research can be applicable to other fields of science, technology, and engineering. This paper comprehensively reviews the applications of emergent communication research across machine learning, natural language processing, linguistics, and cognitive science. Each application is illustrated with a description of its scope, an explication of emergent communication's unique role in addressing it, a summary of the extant literature working towards the application, and brief recommendations for near-term research directions",
    "checked": true,
    "id": "5b19fb4c441856490cbfe8c3eab05187e4063d33",
    "semantic_title": "a review of the applications of deep learning-based emergent communication",
    "citation_count": 0,
    "authors": [
      "Brendon Boldt",
      "David R Mortensen"
    ]
  },
  "https://openreview.net/forum?id=mH6TelHVKD": {
    "title": "Data-Dependent Generalization Bounds for Neural Networks with ReLU",
    "volume": "main",
    "abstract": "We try to establish that one of the correct data-dependent quantities to look at while trying to prove generalization bounds, even for overparameterized neural networks, are the gradients encountered by stochastic gradient descent while training the model. If these are small, then the model generalizes. To make this conclusion rigorous, we weaken the notion of uniform stability of a learning algorithm in a probabilistic way by positing the notion of almost sure (a.s.) support stability and showing that algorithms that have this form of stability have generalization error tending to 0 as the training set size increases. Further, we show that for Stochastic Gradient Descent to be a.s. support stable we only need the loss function to be a.s. locally Lipschitz and locally Smooth at the training points, thereby showing low generalization error with weaker conditions than have been used in the literature. We then show that Neural Networks with ReLU activation and a doubly differentiable loss function possess these properties. Our notion of stability is the first data-dependent notion to be able to show good generalization bounds for non-convex functions with learning rates strictly slower than $1/t$ at the $t$-th step. Finally, we present experimental evidence to validate our theoretical results",
    "checked": true,
    "id": "26363cbad28c93b60485a21ee9cbf7da4c293ad3",
    "semantic_title": "data-dependent generalization bounds for neural networks with relu",
    "citation_count": 0,
    "authors": [
      "Harsh Pandey",
      "Amitabha Bagchi",
      "Srikanta J. Bedathur",
      "Arindam Bhattacharya"
    ]
  },
  "https://openreview.net/forum?id=5nBqY1y96B": {
    "title": "Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs",
    "volume": "main",
    "abstract": "Large language models (LLMs) have achieved widespread success on a variety of in-context few-shot tasks, but this success is typically evaluated via correctness rather than consistency. We argue that self-consistency is an important criteria for valid multi-step reasoning in tasks where the solution is composed of the answers to multiple sub-steps. We propose two types of self-consistency that are particularly important for multi-step reasoning -- hypothetical consistency (a model's ability to predict what its output would be in a hypothetical other context) and compositional consistency (consistency of a model's final outputs when intermediate sub-steps are replaced with the model's outputs for those steps). We demonstrate that multiple variants of the GPT-3/-4 models exhibit poor consistency rates across both types of consistency on a variety of tasks",
    "checked": true,
    "id": "3200a0d6fef7164f0341cf1938f584da6057ffd6",
    "semantic_title": "two failures of self-consistency in the multi-step reasoning of llms",
    "citation_count": 9,
    "authors": [
      "Angelica Chen",
      "Jason Phang",
      "Alicia Parrish",
      "Vishakh Padmakumar",
      "Chen Zhao",
      "Samuel R. Bowman",
      "Kyunghyun Cho"
    ]
  },
  "https://openreview.net/forum?id=OdDsCaacZ0": {
    "title": "MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments",
    "volume": "main",
    "abstract": "Self-supervised learning can be used for mitigating the greedy needs of Vision Transformer networks for very large fully-annotated datasets. Different classes of self-supervised learning offer representations with either good contextual reasoning properties, e.g., using masked image modeling strategies, or invariance to image perturbations, e.g., with contrastive methods. In this work, we propose a single-stage and standalone method, MOCA, which unifies both desired properties using novel mask-and-predict objectives defined with high-level features (instead of pixel-level details). Moreover, we show how to effectively employ both learning paradigms in a synergistic and computation-efficient way. Doing so, we achieve new state-of-the-art results on low-shot settings and strong experimental results in various evaluation protocols with a training that is at least 3 times faster than prior methods. We provide the implementation code at https://github.com/valeoai/MOCA",
    "checked": true,
    "id": "39ffd546e3b64b6313035164ed899339456307a2",
    "semantic_title": "moca: self-supervised representation learning by predicting masked online codebook assignments",
    "citation_count": 1,
    "authors": [
      "Spyros Gidaris",
      "Andrei Bursuc",
      "Oriane Siméoni",
      "Antonín Vobecký",
      "Nikos Komodakis",
      "Matthieu Cord",
      "Patrick Perez"
    ]
  },
  "https://openreview.net/forum?id=emXh4M7TyH": {
    "title": "Transfer Learning for Bayesian Optimization on Heterogeneous Search Spaces",
    "volume": "main",
    "abstract": "Bayesian optimization (BO) is a popular black-box function optimization method, which makes sequential decisions based on a Bayesian model, typically a Gaussian process (GP), of the function. To ensure the quality of the model, transfer learning approaches have been developed to automatically design GP priors by learning from observations on \"training\" functions. These training functions are typically required to have the same domain as the \"test\" function (black-box function to be optimized). In this paper, we introduce MPHD, a model pre-training method on heterogeneous domains, which uses a neural net mapping from domain-specific contexts to specifications of hierarchical GPs. MPHD can be seamlessly integrated with BO to transfer knowledge across heterogeneous search spaces. Our theoretical and empirical results demonstrate the validity of MPHD and its superior performance on challenging black-box function optimization tasks",
    "checked": true,
    "id": "7889cf92ff5120430db0371ff5c70aeb151ac71d",
    "semantic_title": "transfer learning for bayesian optimization on heterogeneous search spaces",
    "citation_count": 1,
    "authors": [
      "Zhou Fan",
      "Xinran Han",
      "Zi Wang"
    ]
  },
  "https://openreview.net/forum?id=zc0Y0cAuTV": {
    "title": "A Multilinear Least-Squares Formulation for Sparse Tensor Canonical Correlation Analysis",
    "volume": "main",
    "abstract": "Tensor data are becoming important recently in various applications, e.g., image and video recognition, which pose new challenges for data modeling and analysis approaches, such as high-order relations of large complexity, varying data scale and gross noise. In this paper, we consider the problem of sparse canonical correlation analysis for arbitrary tensor data. Although several methods have been proposed for this task, there are still limitations hindering its practical applications. To this end, we present a general Sparse Tensor Canonical Correlation Analysis (gSTCCA) method from a multilinear least-squares perspective. Specifically, we formulate the problem as a constrained multilinear least-squares problem with tensor-structured sparsity regularization based on CANDECOMP/PARAFAC (CP) decomposition. Then we present a divide-and-conquer deflation approach to tackle the problem by successive rank-one tensor estimation of the residual tensors, where the overall model is broken up into a set of unconstrained linear least-squares problems that can be efficiently solved. Through extensive experiments conducted on five different datasets for recognition tasks, we demonstrate that the proposed method achieves promising performance compared to the SOTA vector- and tensor-based canonical correlation analysis methods in terms of classification accuracy, model sparsity, and robustness to missing and noisy data. The code is publicly available at https://github.com/junfish/gSTCCA",
    "checked": true,
    "id": "c3435b396472eb8231e3dc153858e12b1db47ed6",
    "semantic_title": "a multilinear least-squares formulation for sparse tensor canonical correlation analysis",
    "citation_count": 0,
    "authors": [
      "Jun Yu",
      "Zhaoming Kong",
      "Kun Chen",
      "Xin Zhang",
      "Yong Chen",
      "Lifang He"
    ]
  },
  "https://openreview.net/forum?id=xLg8ljlEba": {
    "title": "Generalizing Neural Additive Models via Statistical Multimodal Analysis",
    "volume": "main",
    "abstract": "Interpretable models are gaining increasing attention in the machine learning community, and significant progress is being made to develop simple, interpretable, yet powerful deep learning approaches. Generalized Additive Models (GAM) and Neural Additive Models (NAM) are prime examples. Despite these methods' great potential and popularity in critical applications, e.g., medical applications, they fail to generalize to distributions with more than one mode (multimodal\\footnote{In this paper, multimodal refers to the context of distributions, wherein a distribution possesses more than one mode.}). The main reason behind this limitation is that these \"all-fit-one\" models collapse multiple relationships by being forced to fit the data unimodally. We address this critical limitation by proposing interpretable multimodal network frameworks capable of learning a Mixture of Neural Additive Models (MNAM). The proposed MNAM learns relationships between input features and outputs in a multimodal fashion and assigns a probability to each mode. The proposed method shares similarities with Mixture Density Networks (MDN) while keeping the interpretability that characterizes GAM and NAM. We demonstrate how the proposed MNAM balances between rich representations and interpretability with numerous empirical observations and pedagogical studies. We present and discuss different training alternatives and provided extensive practical evaluation to assess the proposed framework. The code is available at \\href{https://github.com/youngkyungkim93/MNAM}{https://github.com/youngkyungkim93/MNAM}",
    "checked": false,
    "id": "2511437a4198b6e3c972382aff50b78a852438f0",
    "semantic_title": "generalizing neural additive models via statistical multi-modal analysis",
    "citation_count": 0,
    "authors": [
      "Young Kyung Kim",
      "Juan Matias Di Martino",
      "Guillermo Sapiro"
    ]
  },
  "https://openreview.net/forum?id=5G3PI1hEdw": {
    "title": "A Joint Study of Phrase Grounding and Task Performance in Vision and Language Models",
    "volume": "main",
    "abstract": "Key to tasks that require reasoning about natural language in visual contexts is grounding words and phrases to image regions. However, observing this grounding in contemporary models is complex, even if it is generally expected to take place if the task is addressed in a way that is conductive to generalization. We propose a framework to jointly study task performance and phrase grounding, and propose three benchmarks to study the relation between the two. Our results show that contemporary models demonstrate inconsistency between their ability to ground phrases and solve tasks. We show how this can be addressed through brute-force training on ground phrasing annotations, and analyze the dynamics it creates. Code and data are available at https://github.com/lil-lab/phrase_grounding",
    "checked": true,
    "id": "b38a634a2902a333cce1f97789df03ae3189ed76",
    "semantic_title": "a joint study of phrase grounding and task performance in vision and language models",
    "citation_count": 1,
    "authors": [
      "Noriyuki Kojima",
      "Hadar Averbuch-Elor",
      "Yoav Artzi"
    ]
  },
  "https://openreview.net/forum?id=RwmWODTNFE": {
    "title": "Size Lowerbounds for Deep Operator Networks",
    "volume": "main",
    "abstract": "Deep Operator Networks are an increasingly popular paradigm for solving regression in infinite dimensions and hence solve families of PDEs in one shot. In this work, we aim to establish a first-of-its-kind data-dependent lowerbound on the size of DeepONets required for them to be able to reduce empirical error on noisy data. In particular, we show that for low training errors to be obtained on $n$ data points it is necessary that the common output dimension of the branch and the trunk net be scaling as $\\Omega \\left ( \\sqrt[\\leftroot{-1}\\uproot{-1}4]{n} \\right )$. This inspires our experiments with DeepONets solving the advection-diffusion-reaction PDE, where we demonstrate the possibility that at a fixed model size, to leverage increase in this common output dimension and get monotonic lowering of training error, the size of the training data might necessarily need to scale at least quadratically with it",
    "checked": true,
    "id": "cf80dd08c9a3783c62585203a71f50e6e1a238f8",
    "semantic_title": "size lowerbounds for deep operator networks",
    "citation_count": 2,
    "authors": [
      "Anirbit Mukherjee",
      "Amartya Roy"
    ]
  },
  "https://openreview.net/forum?id=0T2OTVCCC1": {
    "title": "Extending Path-Dependent NJ-ODEs to Noisy Observations and a Dependent Observation Framework",
    "volume": "main",
    "abstract": "The \\emph{Path-Dependent Neural Jump Ordinary Differential Equation (PD-NJ-ODE)} is a model for predicting continuous-time stochastic processes with irregular and incomplete observations. In particular, the method learns optimal forecasts given irregularly sampled time series of incomplete past observations. So far the process itself and the coordinate-wise observation times were assumed to be independent and observations were assumed to be noiseless. In this work we discuss two extensions to lift these restrictions and provide theoretical guarantees as well as empirical examples for them. In particular, we can lift the assumption of independence by extending the theory to much more realistic settings of conditional independence without any need to change the algorithm. Moreover, we introduce a new loss function, which allows us to deal with noisy observations and explain why the previously used loss function did not lead to a consistent estimator",
    "checked": true,
    "id": "4c384b8a6b5ff1550926eeddb8f32f7f77771469",
    "semantic_title": "extending path-dependent nj-odes to noisy observations and a dependent observation framework",
    "citation_count": 1,
    "authors": [
      "William Andersson",
      "Jakob Heiss",
      "Florian Krach",
      "Josef Teichmann"
    ]
  },
  "https://openreview.net/forum?id=3s7ior0WZ5": {
    "title": "Blind Biological Sequence Denoising with Self-Supervised Set Learning",
    "volume": "main",
    "abstract": "Biological sequence analysis relies on the ability to denoise the imprecise output of sequencing platforms. We consider a common setting where a short sequence is read out repeatedly using a high-throughput long-read platform to generate multiple subreads, or noisy obser- vations of the same sequence. Denoising these subreads with alignment-based approaches often fails when too few subreads are available or error rates are too high. In this paper, we propose a novel method for blindly denoising sets of sequences without directly observing clean source sequence labels. Our method, Self-Supervised Set Learning (SSSL), gathers subreads together in an embedding space and estimates a single set embedding as the mid- point of the subreads in both the latent and sequence spaces. This set embedding represents the \"average\" of the subreads and can be decoded into a prediction of the clean sequence. In experiments on simulated long-read DNA data, SSSL methods denoise small reads of ≤ 6 subreads with 17% fewer errors and large reads of > 6 subreads with 8% fewer errors compared to the best baseline. On a real dataset of antibody sequences, SSSL improves over baselines on two self-supervised metrics, with a significant improvement on difficult small reads that comprise over 60% of the test set. By accurately denoising these reads, SSSL promises to better realize the potential of high-throughput DNA sequencing data for downstream scientific applications",
    "checked": true,
    "id": "512d4baaacb28578464db7697a49ee5dcd69f915",
    "semantic_title": "blind biological sequence denoising with self-supervised set learning",
    "citation_count": 0,
    "authors": [
      "Nathan Hoyen Ng",
      "Ji Won Park",
      "Jae Hyeon Lee",
      "Ryan Lewis Kelly",
      "Stephen Ra",
      "Kyunghyun Cho"
    ]
  },
  "https://openreview.net/forum?id=wyU3Q4gahM": {
    "title": "Unsupervised Discovery of Steerable Factors When Graph Deep Generative Models Are Entangled",
    "volume": "main",
    "abstract": "Deep generative models (DGMs) have been widely developed for graph data. However, much less investigation has been carried out on understanding the latent space of such pretrained graph DGMs. These understandings possess the potential to provide constructive guidelines for crucial tasks, such as graph controllable generation. Thus in this work, we are interested in studying this problem and propose GraphCG, a method for the unsupervised discovery of steerable factors in the latent space of pretrained graph DGMs. We first examine the representation space of three pretrained graph DGMs with six disentanglement metrics, and we observe that the pretrained representation space is entangled. Motivated by this observation, GraphCG learns the steerable factors via maximizing the mutual information between semantic-rich directions, where the controlled graph moving along the same direction will share the same steerable factors. We quantitatively verify that GraphCG outperforms four competitive baselines on two graph DGMs pretrained on two molecule datasets. Additionally, we qualitatively illustrate seven steerable factors learned by GraphCG on five pretrained DGMs over five graph datasets, including two for molecules and three for point clouds",
    "checked": true,
    "id": "7856380962bcdd4a98ca7c8381dc7c80f963623e",
    "semantic_title": "unsupervised discovery of steerable factors when graph deep generative models are entangled",
    "citation_count": 0,
    "authors": [
      "Shengchao Liu",
      "Chengpeng Wang",
      "Jiarui Lu",
      "Weili Nie",
      "Hanchen Wang",
      "Zhuoxinran Li",
      "Bolei Zhou",
      "Jian Tang"
    ]
  },
  "https://openreview.net/forum?id=n2YifD4Dxo": {
    "title": "Are you using test log-likelihood correctly?",
    "volume": "main",
    "abstract": "Test log-likelihood is commonly used to compare different models of the same data or different approximate inference algorithms for fitting the same probabilistic model. We present simple examples demonstrating how comparisons based on test log-likelihood can contradict comparisons according to other objectives. Specifically, our examples show that (i) approximate Bayesian inference algorithms that attain higher test log-likelihoods need not also yield more accurate posterior approximations and (ii) conclusions about forecast accuracy based on test log-likelihood comparisons may not agree with conclusions based on root mean squared error",
    "checked": true,
    "id": "b6e363f617667014aea2d609136816fad22affc7",
    "semantic_title": "are you using test log-likelihood correctly?",
    "citation_count": 2,
    "authors": [
      "Sameer Deshpande",
      "Soumya Ghosh",
      "Tin D. Nguyen",
      "Tamara Broderick"
    ]
  },
  "https://openreview.net/forum?id=M2m618iIPk": {
    "title": "Blockwise Self-Supervised Learning at Scale",
    "volume": "main",
    "abstract": "Current state-of-the-art deep networks are all powered by backpropagation. However, long backpropagation paths as found in end-to-end training are biologically implausible, as well as inefficient in terms of energy consumption. In this paper, we explore alternatives to full backpropagation in the form of blockwise learning rules, leveraging the latest developments in self-supervised learning. We show that a blockwise pretraining procedure consisting of training independently the 4 main blocks of layers of a ResNet-50 with Barlow Twins' loss function at each block performs almost as well as end-to-end backpropagation on ImageNet: a linear probe trained on top of our blockwise pretrained model obtains a top-1 classification accuracy of 70.48\\%, only 1.1\\% below the accuracy of an end-to-end pretrained network (71.57\\% accuracy). We perform extensive experiments to understand the impact of different components within our method and explore a variety of adaptations of self-supervised learning to the blockwise paradigm, building an exhaustive understanding of the critical avenues for scaling local learning rules to large networks, with implications ranging from hardware design to neuroscience",
    "checked": true,
    "id": "a09a197325be3fb2e865692b164e8827042201d1",
    "semantic_title": "blockwise self-supervised learning at scale",
    "citation_count": 9,
    "authors": [
      "Shoaib Siddiqui",
      "David Krueger",
      "Yann LeCun",
      "Stephane Deny"
    ]
  },
  "https://openreview.net/forum?id=zSeoG5dRHK": {
    "title": "Temporally Rich Deep Learning Models for Magnetoencephalography",
    "volume": "main",
    "abstract": "Deep learning has been used in a wide range of applications, but it has only very recently been applied to Magnetoencephalography (MEG). MEG is a neurophysiological technique used to investigate a variety of cognitive processes such as language and learning, and an emerging technology in the quest to identify neural correlates of cognitive impairments such as those occurring in dementia. Recent work has shown that it is possible to apply deep learning to MEG to categorise induced responses to stimuli across subjects. While novel in the application of deep learning, such work has generally used relatively simple neural network (NN) models compared to those being used in domains such as computer vision and natural language processing. In these other domains, there is a long history in developing complex NN models that combine spatial and temporal information. We propose more complex NN models that focus on modelling temporal relationships in the data, and apply them to the challenges of MEG data. We apply these models to an extended range of MEG-based tasks, and find that they substantially outperform existing work on a range of tasks, particularly but not exclusively temporally-oriented ones. We also show that an autoencoder-based preprocessing component that focuses on the temporal aspect of the data can improve the performance of existing models. Our source code is available at https://github.com/tim-chard/DeepLearningForMEG",
    "checked": false,
    "id": "56b36b5339450db4ab8f4dafeb54a0313690e40e",
    "semantic_title": "temporally rich deep learning models for magnetoen-cephalography",
    "citation_count": 0,
    "authors": [
      "Tim Chard",
      "Mark Dras",
      "Paul Sowman",
      "Steve Cassidy",
      "Jia Wu"
    ]
  },
  "https://openreview.net/forum?id=KhMLfEIoUm": {
    "title": "Disciplined Saddle Programming",
    "volume": "main",
    "abstract": "We consider convex-concave saddle point problems, and more generally convex optimization problems we refer to as saddle problems, which include the partial supremum or infimum of convex-concave saddle functions. Saddle problems arise in a wide range of applications, including game theory, machine learning, and finance. It is well known that a saddle problem can be reduced to a single convex optimization problem by dualizing either the convex (min) or concave (max) objectives, reducing a min-max problem into a min-min (or max-max) problem. Carrying out this conversion by hand can be tedious and error prone. In this paper we introduce disciplined saddle programming (DSP), a domain specific language (DSL) for specifying saddle problems, for which the dualizing trick can be automated. The language and methods are based on recent work by Juditsky and Nemirovski, who developed the idea of conic-representable saddle point programs, and showed how to carry out the required dualization automatically using conic duality. Juditsky and Nemirovski's conic representation of saddle problems extends Nesterov and Nemirovski's earlier development of conic representable convex problems; DSP can be thought of as extending disciplined convex programming (DCP) to saddle problems. Just as DCP makes it easy for users to formulate and solve complex convex problems, DSP allows users to easily formulate and solve saddle problems. Our method is implemented in an open-source package, also called DSP",
    "checked": true,
    "id": "3e544c28b964c434885dee829b7c83ab3814d0f0",
    "semantic_title": "disciplined saddle programming",
    "citation_count": 5,
    "authors": [
      "Philipp Schiele",
      "Eric Sager Luxenberg",
      "Stephen P. Boyd"
    ]
  },
  "https://openreview.net/forum?id=Sj7bFPeR6W": {
    "title": "Federated Sampling with Langevin Algorithm under Isoperimetry",
    "volume": "main",
    "abstract": "Federated learning uses a set of techniques to efficiently distribute the training of a machine learning algorithm across several devices, who own the training data. These techniques critically rely on reducing the communication cost---the main bottleneck---between the devices and a central server. Federated learning algorithms usually take an optimization approach: they are algorithms for minimizing the training loss subject to communication (and other) constraints. In this work, we instead take a Bayesian approach for the training task, and propose a communication-efficient variant of the Langevin algorithm to sample \\textit{a posteriori}. The latter approach is more robust and provides more knowledge of the \\textit{a posteriori} distribution than its optimization counterpart. We analyze our algorithm without assuming that the target distribution is strongly log-concave. Instead, we assume the weaker log Sobolev inequality, which allows for nonconvexity",
    "checked": true,
    "id": "c76ac22ce1f27098be1ec07e50948c5ec465d259",
    "semantic_title": "federated sampling with langevin algorithm under isoperimetry",
    "citation_count": 0,
    "authors": [
      "Lukang Sun",
      "Adil Salim",
      "Peter Richtárik"
    ]
  },
  "https://openreview.net/forum?id=rQqzt4gYcc": {
    "title": "TensorVAE: a simple and efficient generative model for conditional molecular conformation generation",
    "volume": "main",
    "abstract": "Efficient generation of 3D conformations of a molecule from its 2D graph is a key challenge in in-silico drug discovery. Deep learning (DL) based generative modelling has recently become a potent tool to tackling this challenge. However, many existing DL-based methods are either indirect–leveraging inter-atomic distances or direct–but requiring numerous sampling steps to generate conformations. In this work, we propose a simple model abbreviated TensorVAE capable of generating conformations directly from a 2D molecular graph in a single step. The main novelty of the proposed method is focused on feature engineering. We develop a novel encoding and feature extraction mechanism relying solely on standard convolution operation to generate token-like feature vector for each atom. These feature vectors are then transformed through standard transformer encoders under a conditional Variational Autoencoder framework for generating conformations directly. We show through experiments on two benchmark datasets that with intuitive feature engineering, a relatively simple and standard model can provide promising generative capability outperforming more than a dozen state-of-the-art models employing more sophisticated and specialized generative architecture",
    "checked": true,
    "id": "3e16332485dba71d2ee6f0a2b2f1d76225601cb4",
    "semantic_title": "tensorvae: a simple and efficient generative model for conditional molecular conformation generation",
    "citation_count": 0,
    "authors": [
      "Hongyang Yu",
      "Hongjiang Yu"
    ]
  },
  "https://openreview.net/forum?id=qyfz0QrkqP": {
    "title": "PixMIM: Rethinking Pixel Reconstruction in Masked Image Modeling",
    "volume": "main",
    "abstract": "Masked Image Modeling (MIM) has achieved promising progress with the advent of Masked Autoencoders (MAE) and BEiT. However, subsequent works have complicated the framework with new auxiliary tasks or extra pre-trained models, inevitably increasing computational overhead. This paper undertakes a fundamental analysis of MIM from the perspective of pixel reconstruction, which examines the input image patches and reconstruction target, and highlights two critical but previously overlooked bottlenecks. Based on this analysis, we propose a remarkably simple and effective method, PixMIM, that entails two strategies: 1) filtering the high-frequency components from the reconstruction target to de-emphasize the network's focus on texture-rich details and 2) adopting a conservative data transform strategy to alleviate the problem of missing foreground in MIM training. PixMIM can be easily integrated into most existing pixel-based MIM approaches (i.e., using raw images as reconstruction target) with negligible additional computation. Without bells and whistles, our method consistently improves four MIM approaches, MAE, MFF, ConvMAE, and LSMAE, across various downstream tasks. We believe this effective plug-and-play method will serve as a strong baseline for self-supervised learning and provide insights for future improvements of the MIM framework. Code and models will be available",
    "checked": true,
    "id": "0a84577447a81d34aa9363a6790a65eb600f8384",
    "semantic_title": "pixmim: rethinking pixel reconstruction in masked image modeling",
    "citation_count": 10,
    "authors": [
      "Yuan Liu",
      "Songyang Zhang",
      "Jiacheng Chen",
      "Kai Chen",
      "Dahua Lin"
    ]
  },
  "https://openreview.net/forum?id=eN9CjU3h1b": {
    "title": "MMD-Regularized Unbalanced Optimal Transport",
    "volume": "main",
    "abstract": "We study the unbalanced optimal transport (UOT) problem, where the marginal constraints are enforced using Maximum Mean Discrepancy (MMD) regularization. Our work is motivated by the observation that the literature on UOT is focused on regularization based on $\\phi$-divergence (e.g., KL divergence). Despite the popularity of MMD, its role as a regularizer in the context of UOT seems less understood. We begin by deriving a specific dual of MMD-regularized UOT (MMD-UOT), which helps us prove several useful properties. One interesting outcome of this duality result is that MMD-UOT induces novel metrics, which not only lift the ground metric like the Wasserstein but are also sample-wise efficient to estimate like the MMD. Further, for real-world applications involving non-discrete measures, we present an estimator for the transport plan that is supported only on the given ($m$) samples. Under certain conditions, we prove that the estimation error with this finitely-supported transport plan is also $\\mathcal{O}(1/\\sqrt{m})$. As far as we know, such error bounds that are free from the curse of dimensionality are not known for $\\phi$-divergence regularized UOT. Finally, we discuss how the proposed estimator can be computed efficiently using accelerated gradient descent. Our experiments show that MMD-UOT consistently outperforms popular baselines, including KL-regularized UOT and MMD, in diverse machine learning applications",
    "checked": false,
    "id": "4a3d67894e699f7dcba0b54a5b0684702126e8bf",
    "semantic_title": "unbalanced low-rank optimal transport solvers",
    "citation_count": 1,
    "authors": [
      "Piyushi Manupriya",
      "SakethaNath Jagarlapudi",
      "Pratik Jawanpuria"
    ]
  },
  "https://openreview.net/forum?id=oGIR0ic3jU": {
    "title": "Bandits Corrupted by Nature: Lower Bounds on Regret and Robust Optimistic Algorithms",
    "volume": "main",
    "abstract": "We study the corrupted bandit problem, i.e. a stochastic multi-armed bandit problem with $k$ unknown reward distributions, which are heavy-tailed and corrupted by a history-independent adversary or Nature. To be specific, the reward obtained by playing an arm comes from corresponding heavy-tailed reward distribution with probability $1-\\varepsilon \\in (0.5,1]$ and an arbitrary corruption distribution of unbounded support with probability $\\varepsilon \\in [0,0.5)$. First, we provide \\textit{a problem-dependent lower bound on the regret} of any corrupted bandit algorithm. The lower bounds indicate that the corrupted bandit problem is harder than the classical stochastic bandit problem with subGaussian or heavy-tail rewards. Following that, we propose a novel UCB-type algorithm for corrupted bandits, namely \\texttt{HubUCB}, that builds on Huber's estimator for robust mean estimation. Leveraging a novel concentration inequality of Huber's estimator, we prove that \\texttt{HubUCB} achieves a near-optimal regret upper bound. Since computing Huber's estimator has quadratic complexity, we further introduce a sequential version of Huber's estimator that exhibits linear complexity. We leverage this sequential estimator to design \\texttt{SeqHubUCB} that enjoys similar regret guarantees while reducing the computational burden. Finally, we experimentally illustrate the efficiency of \\texttt{HubUCB} and \\texttt{SeqHubUCB} in solving corrupted bandits for different reward distributions and different levels of corruptions",
    "checked": false,
    "id": "71be523a35f799fb7d5bf9278f386d6cdb65b34d",
    "semantic_title": "bandits corrupted by nature: lower bounds on regret and robust optimistic algorithm",
    "citation_count": 4,
    "authors": [
      "Timothée Mathieu",
      "Debabrota Basu",
      "Odalric-Ambrym Maillard"
    ]
  },
  "https://openreview.net/forum?id=eTgxr7gPuU": {
    "title": "High-dimensional Bayesian Optimization via Covariance Matrix Adaptation Strategy",
    "volume": "main",
    "abstract": "Bayesian Optimization (BO) is an effective method for finding the global optimum of expensive black-box functions. However, it is well known that applying BO to high-dimensional optimization problems is challenging. To address this issue, a promising solution is to use a local search strategy that partitions the search domain into local regions with high likelihood of containing the global optimum, and then use BO to optimize the objective function within these regions. In this paper, we propose a novel technique for defining the local regions using the Covariance Matrix Adaptation (CMA) strategy. Specifically, we use CMA to learn a search distribution that can estimate the probabilities of data points being the global optimum of the objective function. Based on this search distribution, we then define the local regions consisting of data points with high probabilities of being the global optimum. Our approach serves as a meta-algorithm as it can incorporate existing black-box BO optimizers, such as BO, TuRBO, and BAxUS, to find the global optimum of the objective function within our derived local regions. We evaluate our proposed method on various benchmark synthetic and real-world problems. The results demonstrate that our method outperforms existing state-of-the-art techniques",
    "checked": true,
    "id": "7c1ee8b8c4065b0ae6a81946696b6bc72e1514d7",
    "semantic_title": "high-dimensional bayesian optimization via covariance matrix adaptation strategy",
    "citation_count": 0,
    "authors": [
      "Lam Ngo",
      "Huong Ha",
      "Jeffrey Chan",
      "Vu Nguyen",
      "Hongyu Zhang"
    ]
  },
  "https://openreview.net/forum?id=cvOpIhQQMN": {
    "title": "A general framework for formulating structured variable selection",
    "volume": "main",
    "abstract": "In variable selection, a selection rule that prescribes the permissible sets of selected variables (called a \"selection dictionary\") is desirable due to the inherent structural constraints among the candidate variables. Such selection rules can be complex in real-world data analyses, and failing to incorporate such restrictions could not only compromise the interpretability of the model but also lead to decreased prediction accuracy. However, no general framework has been proposed to formalize selection rules and their applications, which poses a significant challenge for practitioners seeking to integrate these rules into their analyses. In this work, we establish a framework for structured variable selection that can incorporate universal structural constraints. We develop a mathematical language for constructing arbitrary selection rules, where the selection dictionary is formally defined. We demonstrate that all selection rules can be expressed as combinations of operations on constructs, facilitating the identification of the corresponding selection dictionary. We use a detailed and complex example to illustrate the developed framework. Once this selection dictionary is derived, practitioners can apply their own user-defined criteria to select the optimal model. Additionally, our framework enhances existing penalized regression methods for variable selection by providing guidance on how to appropriately group variables to achieve the desired selection rule. Furthermore, our innovative framework opens the door to establishing new $\\ell_0$-based penalized regression techniques that can be tailored to respect arbitrary selection rules, thereby expanding the possibilities for more robust and tailored model development",
    "checked": false,
    "id": "300a0c3e03c00714f292a29708ac5a9e46956b25",
    "semantic_title": "a machine learning framework for neighbor generation in metaheuristic search",
    "citation_count": 1,
    "authors": [
      "GUANBO WANG",
      "Mireille Schnitzer",
      "Tom Chen",
      "Rui Wang",
      "Robert W Platt"
    ]
  },
  "https://openreview.net/forum?id=gllUnpYuXg": {
    "title": "Towards fully covariant machine learning",
    "volume": "main",
    "abstract": "Any representation of data involves arbitrary investigator choices. Because those choices are external to the data-generating process, each choice leads to an exact symmetry, corresponding to the group of transformations that takes one possible representation to another. These are the passive symmetries; they include coordinate freedom, gauge symmetry, and units covariance, all of which have led to important results in physics. In machine learning, the most visible passive symmetry is the relabeling or permutation symmetry of graphs. The active symmetries are those that must be established by observation and experiment. They include, for instance, translations invariances or rotation invariances of physical law. These symmetries are the subject of most of the equivariant machine learning literature. Our goal, in this conceptual contribution, is to understand the implications for machine learning of the many passive and active symmetries in play. We discuss dos and don'ts for machine learning practice if passive symmetries are to be respected. We discuss links to causal modeling and argue that the implementation of passive symmetries is particularly valuable when the goal of the learning problem is to generalize out of sample. We conjecture that the implementation of passive symmetries might help machine learning in the same ways that it transformed physics in the twentieth century",
    "checked": true,
    "id": "f6fe532c25cd480a66f097ac1f1f9e53a7a55d26",
    "semantic_title": "towards fully covariant machine learning",
    "citation_count": 1,
    "authors": [
      "Soledad Villar",
      "David W Hogg",
      "Weichi Yao",
      "George A Kevrekidis",
      "Bernhard Schölkopf"
    ]
  },
  "https://openreview.net/forum?id=VDy6LgErFM": {
    "title": "Empowering GNNs via Edge-Aware Weisfeiler-Leman Algorithm",
    "volume": "main",
    "abstract": "Message passing graph neural networks (GNNs) are known to have their expressiveness upper-bounded by 1-dimensional Weisfeiler-Leman (1-WL) algorithm. To achieve more powerful GNNs, existing attempts either require \\emph{ad hoc} features, or involve operations that incur high time and space complexities. In this work, we propose a \\textit{general} and \\textit{provably powerful} GNN framework that preserves the \\textit{scalability} of the message passing scheme. In particular, we first propose to empower 1-WL for graph isomorphism test by considering edges among neighbors, giving rise to NC-1-WL. The expressiveness of NC-1-WL is shown to be strictly above 1-WL and below 3-WL theoretically. Further, we propose the NC-GNN framework as a differentiable neural version of NC-1-WL. Our simple implementation of NC-GNN is provably as powerful as NC-1-WL. Experiments demonstrate that our NC-GNN performs effectively and efficiently on various benchmarks",
    "checked": true,
    "id": "5036b6ddd2e19e601cc391b516daab0ba7e63761",
    "semantic_title": "empowering gnns via edge-aware weisfeiler-leman algorithm",
    "citation_count": 0,
    "authors": [
      "Meng Liu",
      "Haiyang Yu",
      "Shuiwang Ji"
    ]
  },
  "https://openreview.net/forum?id=BNP4MxzDEI": {
    "title": "To Transfer or Not to Transfer: Suppressing Concepts from Source Representations",
    "volume": "main",
    "abstract": "With the proliferation of large pre-trained models in various domains, transfer learning has gained prominence where intermediate representations from these models can be leveraged to train better (target) task-specific models, with possibly limited labeled data. Although transfer learning can be beneficial in many applications, it can transfer undesirable information to target tasks that may severely curtail its performance in the target domain or raise ethical concerns related to privacy and/or fairness. In this paper, we propose a novel approach for suppressing the transfer of user-determined semantic concepts (viz. color, glasses, etc.) in intermediate source representations to target tasks without retraining the source model which can otherwise be expensive or even infeasible. Notably, we tackle a bigger challenge in the input data as a given intermediate source representation is biased towards the source task, thus possibly further entangling the desired concepts. We evaluate our approach qualitatively and quantitatively in the visual domain showcasing its efficacy for classification and generative source models. Finally, we provide a concept selection approach that automatically suppresses the undesirable concepts",
    "checked": true,
    "id": "6206021e2c5838be293f31f40dde408433420e0c",
    "semantic_title": "to transfer or not to transfer: suppressing concepts from source representations",
    "citation_count": 0,
    "authors": [
      "Vijay Sadashivaiah",
      "Keerthiram Murugesan",
      "Ronny Luss",
      "Pin-Yu Chen",
      "Chris Sims",
      "James Hendler",
      "Amit Dhurandhar"
    ]
  },
  "https://openreview.net/forum?id=DPvwr4HJdt": {
    "title": "On the Choice of Learning Rate for Local SGD",
    "volume": "main",
    "abstract": "Distributed data-parallel optimization accelerates the training of neural networks, but requires constant synchronization of gradients between the workers, which can become a bottleneck. One way to reduce communication overhead is to use Local SGD, where each worker asynchronously takes multiple local gradient steps, after which the model weights are averaged. In this work, we discuss the choice of learning rate for Local SGD, showing that it faces an intricate trade-off. Unlike in the synchronous case, its gradient estimate is biased, with the bias dependent on the learning rate itself. Thus using learning rate scaling techniques designed for faster convergence in the synchronous case with Local SGD results in a performance degradation as previously observed. To analyze the manifestation of this bias, we study convergence behaviour of Local SGD and synchronous data-parallel SGD when using their optimal learning rates. Our experiments show that the optimal learning rate for Local SGD differs substantially from that of SGD, and when using it the performance of Local SGD matches that of SGD. However, this performance comes at the cost of added training iterations, rendering Local SGD faster than SGD only when communication is much more time-consuming than computation. This suggests that Local SGD may be of limited practical utility",
    "checked": true,
    "id": "0b15ec181efd73af6d73160c6ccddce306dfd7fa",
    "semantic_title": "on the choice of learning rate for local sgd",
    "citation_count": 0,
    "authors": [
      "Lukas Balles",
      "Prabhu Teja S",
      "Cedric Archambeau"
    ]
  },
  "https://openreview.net/forum?id=bfsNmgN5je": {
    "title": "Semantic similarity prediction is better than other semantic similarity measures",
    "volume": "main",
    "abstract": "Semantic similarity between natural language texts is typically measured either by looking at the overlap between subsequences (e.g., BLEU) or by using embeddings (e.g., BERTScore, S-BERT). Within this paper, we argue that when we are only interested in measuring the semantic similarity, it is better to directly predict the similarity using a fine-tuned model for such a task. Using a fine-tuned model for the Semantic Textual Similarity Benchmark tasks (STS-B) from the GLUE benchmark, we define the STSScore approach and show that the resulting similarity is better aligned with our expectations on a robust semantic similarity measure than other approaches",
    "checked": true,
    "id": "96bfa287ace006d6b9fd9a73a84571e6c8cb7908",
    "semantic_title": "semantic similarity prediction is better than other semantic similarity measures",
    "citation_count": 1,
    "authors": [
      "Steffen Herbold"
    ]
  },
  "https://openreview.net/forum?id=R7H43YD6Lo": {
    "title": "Prismer: A Vision-Language Model with Multi-Task Experts",
    "volume": "main",
    "abstract": "Recent vision-language models have shown impressive multi-modal generation capabilities. However, typically they require training huge models on massive datasets. As a more scalable alternative, we introduce Prismer, a data- and parameter-efficient vision-language model that leverages an ensemble of task-specific experts. Prismer only requires training of a small number of components, with the majority of network weights inherited from multiple readily-available, pre-trained experts, and kept frozen during training. By leveraging experts from a wide range of domains, we show Prismer can efficiently pool this expert knowledge and adapt it to various vision-language reasoning tasks. In our experiments, we show that Prismer achieves fine-tuned and few-shot learning performance which is competitive with current state-of-the-arts, whilst requiring up to two orders of magnitude less training data. Code is available at https://github.com/NVlabs/prismer",
    "checked": true,
    "id": "f02d56e630986997e0aea3d92bf53e0f363ce401",
    "semantic_title": "prismer: a vision-language model with multi-task experts",
    "citation_count": 17,
    "authors": [
      "Shikun Liu",
      "Linxi Fan",
      "Edward Johns",
      "Zhiding Yu",
      "Chaowei Xiao",
      "Anima Anandkumar"
    ]
  },
  "https://openreview.net/forum?id=4i1MXH8Sle": {
    "title": "CAREER: A Foundation Model for Labor Sequence Data",
    "volume": "main",
    "abstract": "Labor economists regularly analyze employment data by fitting predictive models to small, carefully constructed longitudinal survey datasets. Although machine learning methods offer promise for such problems, these survey datasets are too small to take advantage of them. In recent years large datasets of online resumes have also become available, providing data about the career trajectories of millions of individuals. However, standard econometric models cannot take advantage of their scale or incorporate them into the analysis of survey data. To this end we develop CAREER, a foundation model for job sequences. CAREER is first fit to large, passively-collected resume data and then fine-tuned to smaller, better-curated datasets for economic inferences. We fit CAREER to a dataset of 24 million job sequences from resumes, and adjust it on small longitudinal survey datasets. We find that CAREER forms accurate predictions of job sequences, outperforming econometric baselines on three widely-used economics datasets. We further find that CAREER can be used to form good predictions of other downstream variables. For example, incorporating CAREER into a wage model provides better predictions than the econometric models currently in use",
    "checked": true,
    "id": "6325d9a0bb710687e12241a47d4df0eadc6a6d9d",
    "semantic_title": "career: a foundation model for labor sequence data",
    "citation_count": 0,
    "authors": [
      "Keyon Vafa",
      "Emil Palikot",
      "Tianyu Du",
      "Ayush Kanodia",
      "Susan Athey",
      "David Blei"
    ]
  },
  "https://openreview.net/forum?id=z3ZlnaOM0d": {
    "title": "Hyperspherical Prototype Node Clustering",
    "volume": "main",
    "abstract": "The general workflow of deep node clustering is to encode the nodes into node embeddings via graph neural networks and uncover clustering decisions from them, so clustering performance is heavily affected by the embeddings. However, existing works only consider preserving the semantics of the graph but ignore the inter-cluster separability of the nodes, so there's no guarantee that the embeddings can present a clear clustering structure. To remedy this deficiency, we propose Hyperspherical Prototype Node Clustering (HPNC), an end-to-end clustering paradigm that explicitly enhances the inter-cluster separability of learned node embeddings. Concretely, we constrain the embedding space to a unit-hypersphere, enabling us to scatter the cluster prototypes over the space with maximized pairwise distances. Then, we employ a graph autoencoder to map nodes onto the same hypersphere manifold. Consequently, cluster affinities can be directly retrieved from cosine similarities between node embeddings and prototypes. A clustering-oriented loss is imposed to sharpen the affinity distribution so that the learned node embeddings are encouraged to have small intra-cluster distances and large inter-cluster distances. Based on the proposed HPNC paradigm, we devise two schemes (HPNC-IM and HPNC-DEC) with distinct clustering backbones. Empirical results on popular benchmark datasets demonstrate the superiority of our method compared to other state-of-the-art clustering methods, and visualization results illustrate improved separability of the learned embeddings",
    "checked": true,
    "id": "b6dccff36f94daca36c7455517eff1b1fc317f84",
    "semantic_title": "hyperspherical prototype node clustering",
    "citation_count": 0,
    "authors": [
      "Jitao Lu",
      "Danyang Wu",
      "Feiping Nie",
      "Rong Wang",
      "Xuelong Li"
    ]
  },
  "https://openreview.net/forum?id=rFecyFpFUp": {
    "title": "AdaFed: Fair Federated Learning via Adaptive Common Descent Direction",
    "volume": "main",
    "abstract": "Federated learning (FL) is a promising technology via which some edge devices/clients collaboratively train a machine learning model orchestrated by a server. Learning an unfair model is known as a critical problem in federated learning, where the trained model may unfairly advantage or disadvantage some of the devices. To tackle this problem, in this work, we propose AdaFed. The goal of AdaFed is to find an updating direction for the server along which (i) all the clients' loss functions are decreasing; and (ii) more importantly, the loss functions for the clients with larger values decrease with a higher rate. AdaFed adaptively tunes this common direction based on the values of local gradients and loss functions. We validate the effectiveness of AdaFed on a suite of federated datasets, and demonstrate that AdaFed outperforms state-of-the-art fair FL methods",
    "checked": true,
    "id": "f87c32ce9af0da40a49382810f9f79e679f17b2f",
    "semantic_title": "adafed: fair federated learning via adaptive common descent direction",
    "citation_count": 2,
    "authors": [
      "Shayan Mohajer Hamidi",
      "EN-HUI YANG"
    ]
  },
  "https://openreview.net/forum?id=nzG9KGssSe": {
    "title": "Variational autoencoder with weighted samples for high-dimensional non-parametric adaptive importance sampling",
    "volume": "main",
    "abstract": "Adaptive importance sampling is a well-known family of algorithms for density approximation, generation and Monte Carlo integration including rare event estimation. The main common denominator of this family of algorithms is to perform density estimation with weighted samples at each iteration. However, the classical existing methods to do so, such as kernel smoothing or approximation by a Gaussian distribution, suffer from the curse of dimensionality and/or a lack of flexibility. Both are limitations in high dimension and when we do not have any prior knowledge on the form of the target distribution, such as its number of modes. Variational autoencoders are probabilistic tools able to represent with fidelity high-dimensional data in a lower dimensional space. They constitute a parametric family of distributions robust faced to the dimension and since they are based on deep neural networks, they are flexible enough to be considered as non-parametric models. In this paper, we propose to use a variational autoencoder as the auxiliary importance sampling distribution by extending the existing framework to weighted samples. We integrate the proposed procedure in existing adaptive importance sampling algorithms and we illustrate its practical interest on diverse examples",
    "checked": true,
    "id": "e1a08b6a72895499c5609cee6de40ea4bf9cf281",
    "semantic_title": "variational autoencoder with weighted samples for high-dimensional non-parametric adaptive importance sampling",
    "citation_count": 0,
    "authors": [
      "Julien Demange-Chryst",
      "Francois Bachoc",
      "Jérôme Morio",
      "Timothé Krauth"
    ]
  },
  "https://openreview.net/forum?id=ALRWXT1RLZ": {
    "title": "Separability Analysis for Causal Discovery in Mixture of DAGs",
    "volume": "main",
    "abstract": "Directed acyclic graphs (DAGs) are effective for compactly representing causal systems and specifying the causal relationships among the system's constituents. Specifying such causal relationships in some systems requires a mixture of multiple DAGs -- a single DAG is insufficient. Some examples include time-varying causal systems or aggregated subgroups of a population. Recovering the causal structure of the systems represented by single DAGs is investigated extensively, but it remains mainly open for the systems represented by a mixture of DAGs. A major difference between single- versus mixture-DAG recovery is the existence of node pairs that are separable in the individual DAGs but become inseparable in their mixture. This paper provides the theoretical foundations for analyzing such inseparable node pairs. Specifically, the notion of \\emph{emergent edges} is introduced to represent such inseparable pairs that do not exist in the single DAGs but emerge in their mixtures. Necessary conditions for identifying the emergent edges are established. Operationally, these conditions serve as sufficient conditions for separating a pair of nodes in the mixture of DAGs. These results are further extended, and matching necessary and sufficient conditions for identifying the emergent edges in tree-structured DAGs are established. Finally, a novel graphical representation is formalized to specify these conditions, and an algorithm is provided for inferring the learnable causal relations",
    "checked": true,
    "id": "61523d0574d1cb6e8148edfc9c436a5bdc526ba7",
    "semantic_title": "separability analysis for causal discovery in mixture of dags",
    "citation_count": 0,
    "authors": [
      "Burak Varici",
      "Dmitriy Katz",
      "Dennis Wei",
      "Prasanna Sattigeri",
      "Ali Tajer"
    ]
  },
  "https://openreview.net/forum?id=0CM7Hfsy61": {
    "title": "Unleashing the Potential of Acquisition Functions in High-Dimensional Bayesian Optimization",
    "volume": "main",
    "abstract": "Bayesian optimization (BO) is widely used to optimize expensive-to-evaluate black-box functions. It first builds a surrogate for the objective and quantifies its uncertainty. It then decides where to sample by maximizing an acquisition function (AF) defined by the surrogate model. However, when dealing with high-dimensional problems, finding the global maximum of the AF becomes increasingly challenging. In such cases, the manner in which the AF maximizer is initialized plays a pivotal role. An inappropriate initialization can severely limit the potential of AF. This paper investigates a largely understudied problem concerning the impact of AF maximizer initialization on exploiting AFs' capability. Our large-scale empirical study shows that the widely used random initialization strategy may fail to harness the potential of an AF. Based on this observation, we propose a better initialization approach by employing multiple heuristic optimizers to leverage the historical data of black-box optimization to generate initial points for an AF maximizer. We evaluate our approach with a variety of heavily studied synthetic test functions and real-world applications. Experimental results show that our techniques, while simple, can significantly enhance the standard BO and outperform state-of-the-art methods by a large margin in most test cases",
    "checked": true,
    "id": "d6600d43daaaeaad75e343a1a4bc8de16e9b0a79",
    "semantic_title": "unleashing the potential of acquisition functions in high-dimensional bayesian optimization",
    "citation_count": 1,
    "authors": [
      "Jiayu Zhao",
      "Renyu Yang",
      "SHENGHAO QIU",
      "Zheng Wang"
    ]
  },
  "https://openreview.net/forum?id=6SofFlwhEv": {
    "title": "On the Adversarial Robustness of Camera-based 3D Object Detection",
    "volume": "main",
    "abstract": "In recent years, camera-based 3D object detection has gained widespread attention for its ability to achieve high performance with low computational cost. However, the robustness of these methods to adversarial attacks has not been thoroughly examined, especially when considering their deployment in safety-critical domains like autonomous driving. In this study, we conduct the first comprehensive investigation of the robustness of leading camera-based 3D object detection approaches under various adversarial conditions. We systematically analyze the resilience of these models under two attack settings: white-box and black-box; focusing on two primary objectives: classification and localization. Additionally, we delve into two types of adversarial attack techniques: pixel-based and patch-based. Our experiments yield four interesting findings: (a) bird's-eye-view-based representations exhibit stronger robustness against localization attacks; (b) depth-estimation-free approaches have the potential to show stronger robustness; (c) accurate depth estimation effectively improves robustness for depth-estimation-based methods; (d) incorporating multi-frame benign inputs can effectively mitigate adversarial attacks. We hope our findings can steer the development of future camera-based object detection models with enhanced adversarial robustness. The code is available at: https://github.com/Daniel-xsy/BEV-Attack",
    "checked": true,
    "id": "c60c9928b4c1dfd259d190a24c338eadb8b4b5fd",
    "semantic_title": "on the adversarial robustness of camera-based 3d object detection",
    "citation_count": 10,
    "authors": [
      "Shaoyuan Xie",
      "Zichao Li",
      "Zeyu Wang",
      "Cihang Xie"
    ]
  },
  "https://openreview.net/forum?id=txpYITR8oa": {
    "title": "AmbientFlow: Invertible generative models from incomplete, noisy measurements",
    "volume": "main",
    "abstract": "Generative models have gained popularity for their potential applications in imaging science, such as image reconstruction, posterior sampling and data sharing. Flow-based generative models are particularly attractive due to their ability to tractably provide exact density estimates along with fast, inexpensive and diverse samples. Training such models, however, requires a large, high quality dataset of objects. In applications such as computed imaging, it is often difficult to acquire such data due to requirements such as long acquisition time or high radiation dose, while acquiring noisy or partially observed measurements of these objects is more feasible. In this work, we propose AmbientFlow, a framework for learning flow-based generative models directly from noisy and incomplete data. Using variational Bayesian methods, a novel framework for establishing flow-based generative models from noisy, incomplete data is proposed. Extensive numerical studies demonstrate the effectiveness of AmbientFlow in learning the object distribution. The utility of AmbientFlow in a downstream inference task of image reconstruction is demonstrated",
    "checked": true,
    "id": "1356a03fcbcbfa9b5a60ccdfbd6310dcbe3ac4c1",
    "semantic_title": "ambientflow: invertible generative models from incomplete, noisy measurements",
    "citation_count": 2,
    "authors": [
      "Varun A. Kelkar",
      "Rucha Deshpande",
      "Arindam Banerjee",
      "Mark Anastasio"
    ]
  },
  "https://openreview.net/forum?id=fovUNTilp9": {
    "title": "Multi-Horizon Representations with Hierarchical Forward Models for Reinforcement Learning",
    "volume": "main",
    "abstract": "Learning control from pixels is difficult for reinforcement learning (RL) agents because representation learning and policy learning are intertwined. Previous approaches remedy this issue with auxiliary representation learning tasks, but they either do not consider the temporal aspect of the problem or only consider single-step transitions, which may cause learning inefficiencies if important environmental changes take many steps to manifest. We propose Hierarchical $k$-Step Latent (HKSL), an auxiliary task that learns multiple representations via a hierarchy of forward models that learn to communicate and an ensemble of $n$-step critics that all operate at varying magnitudes of step skipping. We evaluate HKSL in a suite of 30 robotic control tasks with and without distractors and a task of our creation. We find that HKSL either converges to higher or optimal episodic returns more quickly than several alternative representation learning approaches. Furthermore, we find that HKSL's representations capture task-relevant details accurately across timescales (even in the presence of distractors) and that communication channels between hierarchy levels organize information based on both sides of the communication process, both of which improve sample efficiency",
    "checked": true,
    "id": "50dc64ab218420b121da5c956371b2d35d1a2ae2",
    "semantic_title": "multi-horizon representations with hierarchical forward models for reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Trevor McInroe",
      "Lukas Schäfer",
      "Stefano V Albrecht"
    ]
  },
  "https://openreview.net/forum?id=aYkYajcJDN": {
    "title": "Neural Task Synthesis for Visual Programming",
    "volume": "main",
    "abstract": "Generative neural models hold great promise in enhancing programming education by synthesizing new content. We seek to design neural models that can automatically generate programming tasks for a given specification in the context of visual programming domains. Despite the recent successes of large generative models like GPT-4, our initial results show that these models are ineffective in synthesizing visual programming tasks and struggle with logical and spatial reasoning. We propose a novel neuro-symbolic technique, NeurTaskSyn, that can synthesize programming tasks for a specification given in the form of desired programming concepts exercised by its solution code and constraints on the visual task. NeurTaskSyn has two components: the first component is trained via imitation learning procedure to generate possible solution codes, and the second component is trained via reinforcement learning procedure to guide an underlying symbolic execution engine that generates visual tasks for these codes. We demonstrate the effectiveness of NeurTaskSyn through an extensive empirical evaluation and a qualitative study on reference tasks taken from the Hour of Code: Classic Maze challenge by Code.org and the Intro to Programming with Karel course by CodeHS.com",
    "checked": true,
    "id": "59a4a5db0a913b99b7afe1c6b2bf6e24f0d31857",
    "semantic_title": "neural task synthesis for visual programming",
    "citation_count": 9,
    "authors": [
      "Victor-Alexandru Pădurean",
      "Georgios Tzannetos",
      "Adish Singla"
    ]
  },
  "https://openreview.net/forum?id=umggDfMHha": {
    "title": "Federated Classification in Hyperbolic Spaces via Secure Aggregation of Convex Hulls",
    "volume": "main",
    "abstract": "Hierarchical and tree-like data sets arise in many relevant applications, including language processing, graph data mining, phylogeny and genomics. It is known that tree-like data cannot be embedded into Euclidean spaces of finite dimension with small distortion, and that this problem can be mitigated through the use of hyperbolic spaces. When such data also has to be processed in a distributed and privatized setting, it becomes necessary to work with new federated learning methods tailored to hyperbolic spaces. As an initial step towards the development of the field of federated learning in hyperbolic spaces, we propose the first known approach to federated classification in hyperbolic spaces. Our contributions are as follows. First, we develop distributed versions of convex SVM classifiers for Poincar\\'e discs. In this setting, the information conveyed from clients to the global classifier are convex hulls of clusters present in individual client data. Second, to avoid label switching issues, we introduce a number-theoretic approach for label recovery based on the so-called integer $B_h$ sequences. Third, we compute the complexity of the convex hulls in hyperbolic spaces to assess the extent of data leakage; at the same time, in order to limit the communication cost for the hulls, we propose a new quantization method for the Poincar\\'e disc coupled with Reed-Solomon-like encoding. Fourth, at the server level, we introduce a new approach for aggregating convex hulls of the clients based on balanced graph partitioning. We test our method on a collection of diverse data sets, including hierarchical single-cell RNA-seq data from different patients distributed across different repositories that have stringent privacy constraints. The classification accuracy of our method is up to $\\sim11\\%$ better than its Euclidean counterpart, demonstrating the importance of privacy-preserving learning in hyperbolic spaces. Our implementation for the proposed method is available at \\url{https://github.com/sauravpr/hyperbolic_federated_classification}",
    "checked": true,
    "id": "460a12b01476b681528bb693498157795bc9c929",
    "semantic_title": "federated classification in hyperbolic spaces via secure aggregation of convex hulls",
    "citation_count": 0,
    "authors": [
      "Saurav Prakash",
      "Jin Sima",
      "Chao Pan",
      "Eli Chien",
      "Olgica Milenkovic"
    ]
  },
  "https://openreview.net/forum?id=8sg2I9zXgO": {
    "title": "Personalized Algorithmic Recourse with Preference Elicitation",
    "volume": "main",
    "abstract": "Algorithmic Recourse (AR) is the problem of computing a sequence of actions that -- once performed by a user -- overturns an undesirable machine decision. It is paramount that the sequence of actions does not require too much effort for users to implement. Yet, most approaches to AR assume that actions cost the same for all users, and thus may recommend unfairly expensive recourse plans to certain users. Prompted by this observation, we introduce PEAR, the first human-in-the-loop approach capable of providing personalized algorithmic recourse tailored to the needs of any end-user. PEAR builds on insights from Bayesian Preference Elicitation to iteratively refine an estimate of the costs of actions by asking choice set queries to the target user. The queries themselves are computed by maximizing the Expected Utility of Selection, a principled measure of information gain accounting for uncertainty on both the cost estimate and the user's responses. PEAR integrates elicitation into a Reinforcement Learning agent coupled with Monte Carlo Tree Search to quickly identify promising recourse plans. Our empirical evaluation on real-world datasets highlights how PEAR produces high-quality personalized recourse in only a handful of iterations",
    "checked": true,
    "id": "2ae21771e4d66a4835e02bdca0836a4ec5d8e7d4",
    "semantic_title": "personalized algorithmic recourse with preference elicitation",
    "citation_count": 2,
    "authors": [
      "Giovanni De Toni",
      "Paolo Viappiani",
      "Stefano Teso",
      "Bruno Lepri",
      "Andrea Passerini"
    ]
  },
  "https://openreview.net/forum?id=MppUW90uU2": {
    "title": "A Fully Decentralized Surrogate for Multi-Agent Policy Optimization",
    "volume": "main",
    "abstract": "The study of fully decentralized learning or independent learning in cooperative multi-agent reinforcement learning has a history of decades. Recent empirical studies have shown that independent PPO (IPPO) can achieve good performance, comparable to or even better than the methods of centralized training with decentralized execution, in several benchmarks. However, a decentralized actor-critic algorithm with a convergence guarantee is still an open problem. In this paper, we propose decentralized policy optimization (DPO), a decentralized actor-critic algorithm with monotonic improvement and convergence guarantee. We derive a novel decentralized surrogate for policy optimization such that the monotonic improvement of joint policy can be guaranteed by each agent independently optimizing the surrogate. For practical implementation, this decentralized surrogate can be realized by two adaptive coefficients for policy optimization at each agent. Empirically, we evaluate DPO, IPPO, and independent Q-learning (IQL) in a variety of cooperative multi-agent tasks, covering discrete and continuous action spaces, as well as fully and partially observable environments. The results show DPO outperforms both IPPO and IQL in most tasks, which serves as evidence for our theoretical results. The code is available at https://github.com/PKU-RL/DPO",
    "checked": true,
    "id": "7e7fc9a94dbced6707b3679d2f7665c5fa346347",
    "semantic_title": "a fully decentralized surrogate for multi-agent policy optimization",
    "citation_count": 0,
    "authors": [
      "Kefan Su",
      "Zongqing Lu"
    ]
  },
  "https://openreview.net/forum?id=EDqCY6ihbr": {
    "title": "A Globally Convergent Algorithm for Neural Network Parameter Optimization Based on Difference-of-Convex Functions",
    "volume": "main",
    "abstract": "We propose an algorithm for optimizing the parameters of single hidden layer neural networks. Specifically, we derive a blockwise difference-of-convex (DC) functions representation of the objective function. Based on the latter, we propose a block coordinate descent (BCD) approach that we combine with a tailored difference-of-convex functions algorithm (DCA). We prove global convergence of the proposed algorithm. Furthermore, we mathematically analyze the convergence rate of parameters and the convergence rate in value (i.e., the training loss). We give conditions under which our algorithm converges linearly or even faster depending on the local shape of the loss function. We confirm our theoretical derivations numerically and compare our algorithm against state-of-the-art gradient-based solvers in terms of both training loss and test loss",
    "checked": true,
    "id": "484108d2f939be95808d2f5c8a38f1d057b16f4e",
    "semantic_title": "a globally convergent algorithm for neural network parameter optimization based on difference-of-convex functions",
    "citation_count": 0,
    "authors": [
      "Daniel Tschernutter",
      "Mathias Kraus",
      "Stefan Feuerriegel"
    ]
  },
  "https://openreview.net/forum?id=pfbVayaUMc": {
    "title": "Online Reference Tracking For Linear Systems with Unknown Dynamics and Unknown Disturbances",
    "volume": "main",
    "abstract": "This paper presents an online learning mechanism to address the challenge of state tracking for unknown linear systems under general adversarial disturbances. The reference trajectory is assumed to be generated by unknown exosystem dynamics, which relaxes the common assumption of known dynamics for exosystems. Learning a tracking control policy for unknown systems with unknown exosystem dynamics under general disturbances is challenging and surprisingly unsettled. To face this challenge, the presented online learning algorithm has two stages: In the first stage, an algorithm identifies the dynamics of the uncertain system, and in the second stage, an online parametrized memory-augmented controller accounts for the identification error, unknown exosystem dynamics as well as disturbances. The controller's parameters are learned to optimize a general convex cost function, and learning the control parameters is formulated as an online convex optimization problem. This approach uses the memory of previous disturbances and reference values to capture their effects on performance over time. Besides, it implicitly learns the dynamics of the exosystems. The algorithm enables online tuning of controller parameters to achieve state tracking and disturbance rejection while minimizing general convex costs. It is shown that the algorithm achieves a policy regret of $\\mathcal{O}({T}^{\\frac{2}{3}})$. In the simulation results, the performance of the presented tracking algorithm was compared with the certainty equivalent $H_{\\infty}$-control and linear quadratic regulator",
    "checked": false,
    "id": "9a58ccd4f6933f73b43aa2a4c6e4780dc66f97cb",
    "semantic_title": "online reference tracking for linear systems with un-known dynamics and unknown disturbances",
    "citation_count": 1,
    "authors": [
      "Nariman Niknejad",
      "Farnaz Adib Yaghmaie",
      "Hamidreza Modares"
    ]
  },
  "https://openreview.net/forum?id=a68SUt6zFt": {
    "title": "DINOv2: Learning Robust Visual Features without Supervision",
    "volume": "main",
    "abstract": "The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP on most of the benchmarks at image and pixel levels",
    "checked": true,
    "id": "5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891",
    "semantic_title": "dinov2: learning robust visual features without supervision",
    "citation_count": 800,
    "authors": [
      "Maxime Oquab",
      "Timothée Darcet",
      "Théo Moutakanni",
      "Huy V. Vo",
      "Marc Szafraniec",
      "Vasil Khalidov",
      "Pierre Fernandez",
      "Daniel HAZIZA",
      "Francisco Massa",
      "Alaaeldin El-Nouby",
      "Mido Assran",
      "Nicolas Ballas",
      "Wojciech Galuba",
      "Russell Howes",
      "Po-Yao Huang",
      "Shang-Wen Li",
      "Ishan Misra",
      "Michael Rabbat",
      "Vasu Sharma",
      "Gabriel Synnaeve",
      "Hu Xu",
      "Herve Jegou",
      "Julien Mairal",
      "Patrick Labatut",
      "Armand Joulin",
      "Piotr Bojanowski"
    ]
  },
  "https://openreview.net/forum?id=NYdThkjNW1": {
    "title": "Boomerang: Local sampling on image manifolds using diffusion models",
    "volume": "main",
    "abstract": "The inference stage of diffusion models can be seen as running a reverse-time diffusion stochastic differential equation, where samples from a Gaussian latent distribution are transformed into samples from a target distribution that usually reside on a low-dimensional manifold, e.g., an image manifold. The intermediate values between the initial latent space and the image manifold can be interpreted as noisy images, with the amount of noise determined by the forward diffusion process noise schedule. We utilize this interpretation to present Boomerang, an approach for local sampling of image manifolds exploiting the reverse diffusion process dynamics. As implied by its name, Boomerang local sampling involves adding noise to an input image, moving it closer to the latent space, and then mapping it back to the image manifold through a partial reverse diffusion process. Thus, Boomerang generates images on the manifold that are ``similar,'' but nonidentical, to the original input image. We can control the proximity of the generated images to the original by adjusting the amount of noise added. Furthermore, due to the stochastic nature of the partial reverse diffusion process in Boomerang, the generated images display a certain degree of stochasticity, allowing us to obtain ample local samples from the manifold without encountering any duplicates. Boomerang offers the flexibility to work seamlessly with any pretrained diffusion model, such as Stable Diffusion, without necessitating any adjustments to the reverse diffusion process. We present three applications for local sampling using Boomerang. First, we provide a framework for constructing privacy-preserving datasets having controllable degrees of anonymity. Second, we show that using Boomerang for data augmentation increases generalization performance and outperforms state-of-the-art synthetic data augmentation. Lastly, we introduce a perceptual image enhancement framework powered by Boomerang, which enables resolution enhancement",
    "checked": true,
    "id": "e607c4516c7f2dda67b326ddda8c5afd1b2ff2a9",
    "semantic_title": "boomerang: local sampling on image manifolds using diffusion models",
    "citation_count": 12,
    "authors": [
      "Lorenzo Luzi",
      "Paul M Mayer",
      "Josue Casco-Rodriguez",
      "Ali Siahkoohi",
      "Richard Baraniuk"
    ]
  },
  "https://openreview.net/forum?id=KcmWZSk53y": {
    "title": "Improved Regret Bounds for Linear Adversarial MDPs via Linear Optimization",
    "volume": "main",
    "abstract": "Learning Markov decision processes (MDP) in an adversarial environment has been a challenging problem. The problem becomes even more challenging with function approximation since the underlying structure of the loss function and transition kernel are especially hard to estimate in a varying environment. In fact, the state-of-the-art results for linear adversarial MDP achieve a regret of $\\tilde{\\mathcal{O}}({K^{6/7}})$ ($K$ denotes the number of episodes), which admits a large room for improvement. In this paper, we propose a novel explore-exploit algorithm framework and investigate the problem with a new view, which reduces linear MDP into linear optimization by subtly setting the feature maps of the bandit arms of linear optimization. This new technique, under an exploratory assumption, yields an improved bound of $\\tilde{\\mathcal{O}}({K^{4/5}})$ for linear adversarial MDP without access to a transition simulator. The new view could be of independent interest for solving other MDP problems that possess a linear structure",
    "checked": true,
    "id": "64c44e4d7f405ae8df334980642fbb21942a889f",
    "semantic_title": "improved regret bounds for linear adversarial mdps via linear optimization",
    "citation_count": 9,
    "authors": [
      "Fang Kong",
      "XiangCheng Zhang",
      "Baoxiang Wang",
      "Shuai Li"
    ]
  },
  "https://openreview.net/forum?id=RyZB4qXEgt": {
    "title": "Neural Circuit Diagrams: Robust Diagrams for the Communication, Implementation, and Analysis of Deep Learning Architectures",
    "volume": "main",
    "abstract": "Diagrams matter. Unfortunately, the deep learning community has no standard method for diagramming architectures. The current combination of linear algebra notation and ad-hoc diagrams fails to offer the necessary precision to understand architectures in all their detail. However, this detail is critical for faithful implementation, mathematical analysis, further innovation, and ethical assurances. I present neural circuit diagrams, a graphical language tailored to the needs of communicating deep learning architectures. Neural circuit diagrams naturally keep track of the changing arrangement of data, precisely show how operations are broadcast over axes, and display the critical parallel behavior of linear operations. A lingering issue with existing diagramming methods is the inability to simultaneously express the detail of axes and the free arrangement of data, which neural circuit diagrams solve. Their compositional structure is analogous to code, creating a close correspondence between diagrams and implementation. In this work, I introduce neural circuit diagrams for an audience of machine learning researchers. After introducing neural circuit diagrams, I cover a host of architectures to show their utility and breed familiarity. This includes the transformer architecture, convolution (and its difficult-to-explain extensions), residual networks, the U-Net, and the vision transformer. I include a Jupyter notebook that provides evidence for the close correspondence between diagrams and code. Finally, I examine backpropagation using neural circuit diagrams. I show their utility in providing mathematical insight and analyzing algorithms' time and space complexities",
    "checked": true,
    "id": "3be78b45d1cc14a496a7eafdc0e5cfec34cbb030",
    "semantic_title": "neural circuit diagrams: robust diagrams for the communication, implementation, and analysis of deep learning architectures",
    "citation_count": 2,
    "authors": [
      "Vincent Abbott"
    ]
  },
  "https://openreview.net/forum?id=YRKS2J0x36": {
    "title": "DyG2Vec: Efficient Representation Learning for Dynamic Graphs",
    "volume": "main",
    "abstract": "Temporal graph neural networks have shown promising results in learning inductive representations by automatically extracting temporal patterns. However, previous works often rely on complex memory modules or inefficient random walk methods to construct temporal representations. To address these limitations, we present an efficient yet effective attention-based encoder that leverages temporal edge encodings and window-based subgraph sampling to generate task-agnostic embeddings. Moreover, we propose a joint-embedding architecture using non-contrastive SSL to learn rich temporal embeddings without labels. Experimental results on 7 benchmark datasets indicate that on average, our model outperforms SoTA baselines on the future link prediction task by 4.23% for the transductive setting and 3.30% for the inductive setting while only requiring 5-10x less training/inference time. Lastly, different aspects of the proposed framework are investigated through experimental analysis and ablation studies. The code is publicly available at https://github.com/huawei-noah/noah-research/tree/master/graph_atlas",
    "checked": true,
    "id": "07fdc2b9c97b371b83a7596d488b0735c78400a0",
    "semantic_title": "dyg2vec: efficient representation learning for dynamic graphs",
    "citation_count": 0,
    "authors": [
      "Mohammad Alomrani",
      "Mahdi Biparva",
      "Yingxue Zhang",
      "Mark Coates"
    ]
  },
  "https://openreview.net/forum?id=nYjSkOy8ij": {
    "title": "A Survey on Out-of-Distribution Detection in NLP",
    "volume": "main",
    "abstract": "Out-of-distribution (OOD) detection is essential for the reliable and safe deployment of machine learning systems in the real world. Great progress has been made over the past years. This paper presents the first review of recent advances in OOD detection with a particular focus on natural language processing approaches. First, we provide a formal definition of OOD detection and discuss several related fields. We then categorize recent algorithms into three classes according to the data they used: (1) OOD data available, (2) OOD data unavailable + in-distribution (ID) label available, and (3) OOD data unavailable + ID label unavailable. Third, we introduce datasets, applications, and metrics. Finally, we summarize existing work and present potential future research topics",
    "checked": true,
    "id": "dcfca93185c49811ec6cf7c995eea58cf88c7bb3",
    "semantic_title": "a survey on out-of-distribution detection in nlp",
    "citation_count": 10,
    "authors": [
      "Hao Lang",
      "Yinhe Zheng",
      "Yixuan Li",
      "Jian SUN",
      "Fei Huang",
      "Yongbin Li"
    ]
  },
  "https://openreview.net/forum?id=vyRBsqj5iG": {
    "title": "Proximal Mean Field Learning in Shallow Neural Networks",
    "volume": "main",
    "abstract": "We propose a custom learning algorithm for shallow over-parameterized neural networks, i.e., networks with single hidden layer having infinite width. The infinite width of the hidden layer serves as an abstraction for the over-parameterization. Building on the recent mean field interpretations of learning dynamics in shallow neural networks, we realize mean field learning as a computational algorithm, rather than as an analytical tool. Specifically, we design a Sinkhorn regularized proximal algorithm to approximate the distributional flow for the learning dynamics over weighted point clouds. In this setting, a contractive fixed point recursion computes the time-varying weights, numerically realizing the interacting Wasserstein gradient flow of the parameter distribution supported over the neuronal ensemble. An appealing aspect of the proposed algorithm is that the measure-valued recursions allow meshless computation. We demonstrate the proposed computational framework of interacting weighted particle evolution on binary and multi-class classification. Our algorithm performs gradient descent of the free energy associated with the risk functional",
    "checked": true,
    "id": "be4c0c6d3590401ea41a0eca481beb9ea20b22c1",
    "semantic_title": "proximal mean field learning in shallow neural networks",
    "citation_count": 0,
    "authors": [
      "Alexis Teter",
      "Iman Nodozi",
      "Abhishek Halder"
    ]
  },
  "https://openreview.net/forum?id=42BKnT2qW3": {
    "title": "Synaptic Interaction Penalty: Appropriate Penalty Term for Energy-Efficient Spiking Neural Networks",
    "volume": "main",
    "abstract": "Spiking neural networks (SNNs) are energy-efficient neural networks because of their spiking nature. However, as the spike firing rate of SNNs increases, the energy consumption does as well, and thus, the advantage of SNNs diminishes. Here, we tackle this problem by introducing a novel penalty term for the spiking activity into the objective function in the training phase. Our method is designed so as to optimize the energy consumption metric directly without modifying the network architecture. Therefore, the proposed method can reduce the energy consumption more than other methods while maintaining the accuracy. We conducted experiments for image classification tasks, and the results indicate the effectiveness of the proposed method, which mitigates the dilemma of the energy--accuracy trade-off",
    "checked": true,
    "id": "94a680ceb5f106b41b3d0f10369ae06c44bcf6be",
    "semantic_title": "synaptic interaction penalty: appropriate penalty term for energy-efficient spiking neural networks",
    "citation_count": 0,
    "authors": [
      "Kazuma Suetake",
      "Takuya Ushimaru",
      "Ryuji Saiin",
      "Yoshihide Sawada"
    ]
  },
  "https://openreview.net/forum?id=n8fZ6mY6PB": {
    "title": "Exploring Format Consistency for Instruction Tuning",
    "volume": "main",
    "abstract": "Instruction tuning has emerged as a promising approach to enhancing large language models in following human instructions. It is shown that increasing the diversity and number of instructions in the training data can consistently enhance generalization performance, which facilitates a recent endeavor to collect various instructions and integrate existing instruction tuning datasets into larger collections. However, different users have their unique ways of expressing instructions, and there often exist variations across different datasets in the instruction styles and formats, i.e., format inconsistency. In this work, a framework named Unified Instruction Tuning (UIT) is proposed, which calls OpenAI APIs for automatic format transfer among different instruction tuning datasets such as PromptSource, FLAN and CrossFit. With the framework, we (1) demonstrate the necessity of maintaining format consistency in instruction tuning; (2) improve the generalization performance on unseen instructions on T5-LM-xl; (3) provide a novel perplexity-based denoising method to reduce the noise of automatic format transfer to make the UIT framework more practical and a smaller offline model based on GPT-J that achieves comparable format transfer capability to OpenAI APIs to reduce costs in practice. Further analysis regarding variations of targeted formats and other effects is intended. The code and trained models will soon be available",
    "checked": true,
    "id": "46ac88bb0acbf736840ff8a392cec2bf43d917e1",
    "semantic_title": "exploring format consistency for instruction tuning",
    "citation_count": 7,
    "authors": [
      "Shihao Liang",
      "Runchu Tian",
      "Kunlun Zhu",
      "Yujia Qin",
      "Huadong Wang",
      "Xin Cong",
      "Zhiyuan Liu",
      "Xiaojiang Liu",
      "Maosong Sun"
    ]
  },
  "https://openreview.net/forum?id=EWv9XGOpB3": {
    "title": "Variational Classification: A Probabilistic Generalization of the Softmax Classifier",
    "volume": "main",
    "abstract": "We present a latent variable model for classification that provides a novel probabilistic interpretation of neural network softmax classifiers. We derive a variational objective to train the model, analogous to the evidence lower bound (ELBO) used to train variational auto-encoders, that generalises the cross-entropy loss used to train classification models. Treating inputs to the softmax layer as samples of a latent variable, our abstracted perspective reveals a potential inconsistency between their anticipated distribution, required for accurate label predictions to be output, and the empirical distribution found in practice. We augment the variational objective to mitigate such inconsistency and encourage a chosen latent distribution, instead of the implicit assumption in off-the-shelf softmax classifiers. Overall, we provide new theoretical insight into the inner workings of widely-used softmax classification. Empirical evaluation on image and text classification datasets demonstrates that our proposed approach, variational classification, maintains classification accuracy while the reshaped latent space improves other desirable properties of a classifier, such as calibration, adversarial robustness, robustness to distribution shift and sample efficiency useful in low data settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shehzaad Zuzar Dhuliawala",
      "Mrinmaya Sachan",
      "Carl Allen"
    ]
  },
  "https://openreview.net/forum?id=H7gLN5nqVF": {
    "title": "Deep Generalized Prediction Set Classifier and Its Theoretical Guarantees",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhou Wang",
      "Xingye Qiao"
    ]
  },
  "https://openreview.net/forum?id=xRy1YRcHWj": {
    "title": "As large as it gets – Studying Infinitely Large Convolutions via Neural Implicit Frequency Filters",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julia Grabinski",
      "Janis Keuper",
      "Margret Keuper"
    ]
  },
  "https://openreview.net/forum?id=Gh0cxhbz3c": {
    "title": "On the Convergence of Adaptive Gradient Methods for Nonconvex Optimization",
    "volume": "featured",
    "abstract": "Adaptive gradient methods are workhorses in deep learning. However, the convergence guarantees of adaptive gradient methods for nonconvex optimization have not been thoroughly studied. In this paper, we provide a fine-grained convergence analysis for a general class of adaptive gradient methods including AMSGrad, RMSProp and AdaGrad. For smooth nonconvex functions, we prove that adaptive gradient methods in expectation converge to a first-order stationary point. Our convergence rate is better than existing results for adaptive gradient methods in terms of dimension. In addition, we also prove high probability bounds on the convergence rates of AMSGrad, RMSProp as well as AdaGrad, which have not been established before. Our analyses shed light on better understanding the mechanism behind adaptive gradient methods in optimizing nonconvex objectives",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongruo Zhou",
      "Jinghui Chen",
      "Yuan Cao",
      "Ziyan Yang",
      "Quanquan Gu"
    ]
  },
  "https://openreview.net/forum?id=hFALpTb4fR": {
    "title": "LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models",
    "volume": "featured",
    "abstract": "Recent advancements in text-to-image diffusion models have yielded impressive results in generating realistic and diverse images. However, these models still struggle with complex prompts, such as those that involve numeracy and spatial reasoning. This work proposes to enhance prompt understanding capabilities in diffusion models. Our method leverages a pretrained large language model (LLM) for grounded generation in a novel two-stage process. In the first stage, the LLM generates a scene layout that comprises captioned bounding boxes from a given prompt describing the desired image. In the second stage, a novel controller guides an off-the-shelf diffusion model for layout-grounded image generation. Both stages utilize existing pretrained models without additional model parameter optimization. Our method significantly outperforms the base diffusion model and several strong baselines in accurately generating images according to prompts that require various capabilities, doubling the generation accuracy across four tasks on average. Furthermore, our method enables instruction-based multi-round scene specification and can handle prompts in languages not supported by the underlying diffusion model. We anticipate that our method will unleash users' creativity by accurately following more complex prompts. Our code, demo, and benchmark are available at: https://llm-grounded-diffusion.github.io",
    "checked": true,
    "id": "e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0",
    "semantic_title": "llm-grounded diffusion: enhancing prompt understanding of text-to-image diffusion models with large language models",
    "citation_count": 55,
    "authors": [
      "Long Lian",
      "Boyi Li",
      "Adam Yala",
      "Trevor Darrell"
    ]
  },
  "https://openreview.net/forum?id=r9p9CV52MV": {
    "title": "ModuLoRA: Finetuning 2-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers",
    "volume": "featured",
    "abstract": "We propose a memory-efficient finetuning algorithm for large language models (LLMs) that supports finetuning LLMs with 65B parameters in 2/3/4-bit precision on as little as one 24GB GPU. Our method, modular low-rank adaptation (ModuLoRA), integrates any user-specified weight quantizer with finetuning via low-rank adapters (LoRAs). Our approach relies on a simple quantization-agnostic backward pass that adaptively materializes low-precision LLM weights from a custom black-box quantization module. This approach enables finetuning 2-bit and 3-bit LLMs for the first time---leveraging state-of-the-art 2-bit QuIP# quantization and 3-bit OPTQ quantization---outperforming finetuning that relies on less sophisticated 4-bit and 8-bit methods. In our experiments, ModuLoRA attains competitive performance on text classification, natural language infernece, and instruction following tasks using significantly less memory than existing approaches, and we also surpass the state-of-the-art ROUGE score on a popular summarization task. We release ModuLoRA together with a series of low-precision models as part of LLMTOOLS, a user-friendly library for quantizing, running, and finetuning LLMs on consumer GPUs",
    "checked": true,
    "id": "e3314e18032898b33a2e2b067a45ae4cb02e1a95",
    "semantic_title": "modulora: finetuning 2-bit llms on consumer gpus by integrating with modular quantizers",
    "citation_count": 1,
    "authors": [
      "Junjie Yin",
      "Jiahao Dong",
      "Yingheng Wang",
      "Christopher De Sa",
      "Volodymyr Kuleshov"
    ]
  },
  "https://openreview.net/forum?id=NmpjDHWIvg": {
    "title": "Wasserstein Distributionally Robust Policy Evaluation and Learning for Contextual Bandits",
    "volume": "featured",
    "abstract": "Off-policy evaluation and learning are concerned with assessing a given policy and learning an optimal policy from offline data without direct interaction with the environment. Often, the environment in which the data are collected differs from the environment in which the learned policy is applied. To account for the effect of different environments during learning and execution, distributionally robust optimization (DRO) methods have been developed that compute worst-case bounds on the policy values assuming that the distribution of the new environment lies within an uncertainty set. Typically, this uncertainty set is defined based on the KL divergence around the empirical distribution computed from the logging dataset. However, the KL uncertainty set fails to encompass distributions with varying support and lacks awareness of the geometry of the distribution support. As a result, KL approaches fall short in addressing practical environment mismatches and lead to over-fitting to worst-case scenarios. To overcome these limitations, we propose a novel DRO approach that employs the Wasserstein distance instead. While Wasserstein DRO is generally computationally more expensive compared to KL DRO, we present a regularized method and a practical (biased) stochastic gradient descent method to optimize the policy efficiently. We also provide a theoretical analysis of the finite sample complexity and iteration complexity for our proposed method. We further validate our approach using a public dataset that was recorded in a randomized stoke trial",
    "checked": true,
    "id": "96c77909fd7ee5bb27213ddf4309475873f8a958",
    "semantic_title": "wasserstein distributionally robust policy evaluation and learning for contextual bandits",
    "citation_count": 1,
    "authors": [
      "Yi Shen",
      "Pan Xu",
      "Michael Zavlanos"
    ]
  },
  "https://openreview.net/forum?id=TQfQUksaC8": {
    "title": "Pathologies of Predictive Diversity in Deep Ensembles",
    "volume": "featured",
    "abstract": "Classic results establish that encouraging predictive diversity improves performance in ensembles of low-capacity models, e.g. through bagging or boosting. Here we demonstrate that these intuitions do not apply to high-capacity neural network ensembles (deep ensembles), and in fact the opposite is often true. In a large scale study of nearly 600 neural network classification ensembles, we examine a variety of interventions that trade off component model performance for predictive diversity. While such interventions can improve the performance of small neural network ensembles (in line with standard intuitions), they harm the performance of the large neural network ensembles most often used in practice. Surprisingly, we also find that discouraging predictive diversity is often benign in large-network ensembles, fully inverting standard intuitions. Even when diversity-promoting interventions do not sacrifice component model performance (e.g. using heterogeneous architectures and training paradigms), we observe an opportunity cost associated with pursuing increased predictive diversity. Examining over 1000 ensembles, we observe that the performance benefits of diverse architectures/training procedures are easily dwarfed by the benefits of simply using higher-capacity models, despite the fact that such higher capacity models often yield significantly less predictive diversity. Overall, our findings demonstrate that standard intuitions around predictive diversity, originally developed for low-capacity ensembles, do not directly apply to modern high-capacity deep ensembles. This work clarifies fundamental challenges to the goal of improving deep ensembles by making them more diverse, while suggesting an alternative path: simply forming ensembles from ever more powerful (and less diverse) component models",
    "checked": true,
    "id": "20113225d61a160bdab4ad2228a2918e41232e29",
    "semantic_title": "pathologies of predictive diversity in deep ensembles",
    "citation_count": 4,
    "authors": [
      "Taiga Abe",
      "E. Kelly Buchanan",
      "Geoff Pleiss",
      "John Patrick Cunningham"
    ]
  },
  "https://openreview.net/forum?id=FDt2UGM1Nz": {
    "title": "DIG In: Evaluating Disparities in Image Generations with Indicators for Geographic Diversity",
    "volume": "featured",
    "abstract": "The unprecedented photorealistic results achieved by recent text-to-image generative systems and their increasing use as plug-and-play content creation solutions make it crucial to understand their potential biases. In this work, we introduce three indicators to evaluate the realism, diversity and prompt-generation consistency of text-to-image generative systems when prompted to generate objects from across the world. Our indicators complement qualitative analysis of the broader impact of such systems by enabling automatic and efficient benchmarking of geographic disparities, an important step towards building responsible visual content creation systems. We use our proposed indicators to analyze potential geographic biases in state-of-the-art visual content creation systems and find that: (1) models have less realism and diversity of generations when prompting for Africa and West Asia than Europe, (2) prompting with geographic information comes at a cost to prompt-consistency and diversity of generated images, and (3) models exhibit more region-level disparities for some objects than others. Perhaps most interestingly, our indicators suggest that progress in image generation quality has come at the cost of real-world geographic representation. Our comprehensive evaluation constitutes a crucial step towards ensuring a positive experience of visual content creation for everyone. Code is available at https://github.com/facebookresearch/DIG-In/",
    "checked": true,
    "id": "a00537a398ce1cacf3b6835a9817ade535d2dae2",
    "semantic_title": "dig in: evaluating disparities in image generations with indicators for geographic diversity",
    "citation_count": 2,
    "authors": [
      "Melissa Hall",
      "Candace Ross",
      "Adina Williams",
      "Nicolas Carion",
      "Michal Drozdzal",
      "Adriana Romero-Soriano"
    ]
  },
  "https://openreview.net/forum?id=BbvSU02jLg": {
    "title": "Transfer Learning with Informative Priors: Simple Baselines Better than Previously Reported",
    "volume": "reproducibility",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ethan Harvey",
      "Mikhail Petrov",
      "Michael C Hughes"
    ]
  },
  "https://openreview.net/forum?id=fCNqD2IuoD": {
    "title": "Reproducibility Study of \"Learning Perturbations to Explain Time Series Predictions",
    "volume": "reproducibility",
    "abstract": "In this work, we attempt to reproduce the results of Enguehard (2023), which introduced ExtremalMask, a mask-based perturbation method for explaining time series data. We investigated the key claims of this paper, namely that (1) the model outperformed other models in several key metrics on both synthetic and real data, and (2) the model performed better when using the loss function of the preservation game relative to that of the deletion game. Although discrepancies exist, our results generally support the core of the original paper's conclusions. Next, we interpret ExtremalMask's outputs using new visualizations and metrics and discuss the insights each interpretation provides. Finally, we test whether ExtremalMask create out of distribution samples, and found the model does not exhibit this flaw on our tested synthetic dataset. Overall, our results support and add nuance to the original paper's findings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiapeng Fan",
      "Luke Cadigan",
      "Paulius Skaisgiris",
      "Sebastian Uriel Arias"
    ]
  },
  "https://openreview.net/forum?id=VzKXbCzNoU": {
    "title": "Efficient Parallelized Simulation of Cyber-Physical Systems",
    "volume": "reproducibility",
    "abstract": "Advancements in accelerated physics simulations have greatly reduced training times for reinforcement learning policies, yet the conventional step-by-step agent-simulator interaction undermines simulation accuracy. In the real-world, interactions are asynchronous, with sensing, acting and processing happening simultaneously. Failing to capture this widens the sim2real gap and results in suboptimal real-world performance. In this paper, we address the challenges of simulating realistic asynchronicity and delays within parallelized simulations, crucial to bridging the sim2real gap in complex cyber-physical systems. Our approach efficiently parallelizes cyber-physical system simulations on accelerator hardware, including physics, sensors, actuators, processing components and their asynchronous interactions. We extend existing accelerated physics simulations with latency simulation capabilities by constructing a `supergraph' that encodes all data dependencies across parallelized simulation steps, ensuring accurate simulation. By finding the smallest supergraph, we minimize redundant computation. We validate our approach on two real-world systems and perform an extensive ablation, demonstrating superior performance compared to baseline methods",
    "checked": false,
    "id": "1980638d973ed7ab0684b148fbba8a13889bbb50",
    "semantic_title": "optimizing highly-parallel simulation-based verification of cyber-physical systems",
    "citation_count": 0,
    "authors": [
      "Bas van der Heijden",
      "Laura Ferranti",
      "Jens Kober",
      "Robert Babuska"
    ]
  },
  "https://openreview.net/forum?id=H1hLNjwrGy": {
    "title": "Reproducibility Study of \"Robust Fair Clustering: A Novel Fairness Attack and Defense Framework",
    "volume": "reproducibility",
    "abstract": "Clustering algorithms play a pivotal role in various societal applications, where fairness is paramount to prevent adverse impacts on individuals. In this study, we revisit the robustness of fair clustering algorithms against adversarial attacks, affirming previous research findings that highlighted their susceptibility and the resilience of the Consensus Fair Clustering (CFC) model. Beyond reproducing these critical results, our work extends the original analysis by refining the codebase for enhanced experimentation, introducing additional metrics and datasets to deepen the evaluation of fairness and clustering quality, and exploring novel attack strategies, including targeted attacks on new metrics and a combined approach for balance and entropy as well as an ablation study. These contributions validate the original claims about the vulnerability and resilience of fair clustering algorithms and broaden the research landscape by offering a more comprehensive toolkit for assessing adversarial robustness in fair clustering",
    "checked": false,
    "id": "6545eebb1162c70751119e173f8d51f5c112fcc1",
    "semantic_title": "robust fair clustering: a novel fairness attack and defense framework",
    "citation_count": 10,
    "authors": [
      "Iason Skylitsis",
      "Zheng Feng",
      "Idries Nasim",
      "Camille Niessink"
    ]
  },
  "https://openreview.net/forum?id=sHSKFYyINO": {
    "title": "InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers",
    "volume": "reproducibility",
    "abstract": "We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which—as we found out—produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact, on three out of five datasets, DeBERTA slightly outperformed monoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used by Bonifacio et al. (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available. https://github.com/searchivarius/inpars_light/",
    "checked": true,
    "id": "3a30217c4115777fb30c182c97cc77d34d065556",
    "semantic_title": "inpars-light: cost-effective unsupervised training of efficient rankers",
    "citation_count": 13,
    "authors": [
      "Leonid Boytsov",
      "Preksha Patel",
      "Vivek Sourabh",
      "Riddhi Nisar",
      "Sayani Kundu",
      "Ramya Ramanathan",
      "Eric Nyberg"
    ]
  },
  "https://openreview.net/forum?id=10R6iX6JHm": {
    "title": "We're Not Using Videos Effectively: An Updated Domain Adaptive Video Segmentation Baseline",
    "volume": "reproducibility",
    "abstract": "There has been abundant work in unsupervised domain adaptation for semantic segmentation (DAS) seeking to adapt a model trained on images from a labeled source domain to an unlabeled target domain. While the vast majority of prior work has studied this as a frame-level Image-DAS problem, a few Video-DAS works have sought to additionally leverage the temporal signal present in adjacent frames. However, Video-DAS works have historically studied a distinct set of benchmarks from Image-DAS, with minimal cross-benchmarking. In this work, we address this gap. Surprisingly, we find that (1) even after carefully controlling for data and model architecture, state-of-the-art Image-DAS methods (HRDA and HRDA+MIC)} outperform Video-DAS methods on established Video-DAS benchmarks (+14.5 mIoU on Viper$\\rightarrow$CityscapesSeq, +19.0 mIoU on Synthia$\\rightarrow$CityscapesSeq), and (2) naive combinations of Image-DAS and Video-DAS techniques only lead to marginal improvements across datasets. To avoid siloed progress between Image-DAS and Video-DAS, we open-source our codebase with support for a comprehensive set of Video-DAS and Image-DAS methods on a common benchmark. Code available at https://github.com/SimarKareer/UnifiedVideoDA",
    "checked": true,
    "id": "e96e71694d02a631e48f39874c42c6480ed7078e",
    "semantic_title": "we're not using videos effectively: an updated domain adaptive video segmentation baseline",
    "citation_count": 0,
    "authors": [
      "Simar Kareer",
      "Vivek Vijaykumar",
      "Harsh Maheshwari",
      "Judy Hoffman",
      "Prithvijit Chattopadhyay",
      "Viraj Uday Prabhu"
    ]
  },
  "https://openreview.net/forum?id=bsCCJHbO8A": {
    "title": "Efficient Large Language Models: A Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongwei Wan",
      "Xin Wang",
      "Che Liu",
      "Samiul Alam",
      "Yu Zheng",
      "Jiachen Liu",
      "Zhongnan Qu",
      "Shen Yan",
      "Yi Zhu",
      "Quanlu Zhang",
      "Mosharaf Chowdhury",
      "Mi Zhang"
    ]
  },
  "https://openreview.net/forum?id=IhXM3g2gxg": {
    "title": "A Short Survey on Importance Weighting for Machine Learning",
    "volume": "survey",
    "abstract": "Importance weighting is a fundamental procedure in statistics and machine learning that weights the objective function or probability distribution based on the importance of the instance in some sense. The simplicity and usefulness of the idea has led to many applications of importance weighting. For example, it is known that supervised learning under an assumption about the difference between the training and test distributions, called distribution shift, can guarantee statistically desirable properties through importance weighting by their density ratio. This survey summarizes the broad applications of importance weighting in machine learning and related research",
    "checked": true,
    "id": "8feca4f15b7fe6bd61e8d634ff4ef869378d4485",
    "semantic_title": "a short survey on importance weighting for machine learning",
    "citation_count": 0,
    "authors": [
      "Masanari Kimura",
      "Hideitsu Hino"
    ]
  },
  "https://openreview.net/forum?id=bNtr6SLgZf": {
    "title": "A Survey of Temporal Credit Assignment in Deep Reinforcement Learning",
    "volume": "survey",
    "abstract": "The Credit Assignment Problem (CAP) refers to the longstanding challenge of Reinforcement Learning agents to associate actions with their long-term consequences. Solving the CAP is a crucial step towards the successful deployment of RL in the real world since most decision problems provide feedback that is noisy, delayed, and with little or no information about the causes. These conditions make it hard to distinguish serendipitous outcomes from those caused by informed decision-making. However, the mathematical nature of credit and the CAP remains poorly understood and defined. In this survey, we review the state of the art of Temporal Credit Assignment (CA) in deep RL. We propose a unifying formalism for credit that enables equitable comparisons of state-of-the-art algorithms and improves our understanding of the trade-offs between the various methods. We cast the CAP as the problem of learning the influence of an action over an outcome from a finite amount of experience. We discuss the challenges posed by delayed effects, dilution, and a lack of action influence, and analyse how existing methods aim to address them. Finally, we survey the protocols to evaluate a credit assignment method and suggest ways to diagnose the sources of struggle for different methods. Overall, this survey provides an overview of the field for new-entry practitioners and researchers, it offers a coherent perspective for scholars looking to expedite the starting stages of a new study on the CAP, and it suggests potential directions for future research",
    "checked": true,
    "id": "53fb57d7aee2f4d96bc4f5b0b5e2a3c37f11020d",
    "semantic_title": "a survey of temporal credit assignment in deep reinforcement learning",
    "citation_count": 3,
    "authors": [
      "Eduardo Pignatelli",
      "Johan Ferret",
      "Matthieu Geist",
      "Thomas Mesnard",
      "Hado van Hasselt",
      "Laura Toni"
    ]
  },
  "https://openreview.net/forum?id=YgmBD2c9qX": {
    "title": "A Unified View of Differentially Private Deep Generative Modeling",
    "volume": "survey",
    "abstract": "The availability of rich and vast data sources has greatly advanced machine learning applications in various domains. However, data with privacy concerns comes with stringent regulations that frequently prohibit data access and data sharing. Overcoming these obstacles in compliance with privacy considerations is key for technological progress in many real-world application scenarios that involve sensitive data. Differentially private (DP) data publishing provides a compelling solution, where only a sanitized form of the data is publicly released, enabling privacy-preserving downstream analysis and reproducible research in sensitive domains. In recent years, various approaches have been proposed for achieving privacy-preserving high-dimensional data generation by private training on top of deep neural networks. In this paper, we present a novel unified view that systematizes these approaches. Our view provides a joint design space for systematically deriving methods that cater to different use cases. We then discuss the strengths, limitations, and inherent correlations between different approaches, aiming to shed light on crucial aspects and inspire future research. We conclude by presenting potential paths forward for the field of DP data generation, with the aim of steering the community toward making the next important steps in advancing privacy-preserving learning",
    "checked": true,
    "id": "724b9cd48f1a524aa889a76fdb5babdb0a24fdba",
    "semantic_title": "a unified view of differentially private deep generative modeling",
    "citation_count": 1,
    "authors": [
      "Dingfan Chen",
      "Raouf Kerkouche",
      "Mario Fritz"
    ]
  },
  "https://openreview.net/forum?id=1i6ZCvflQJ": {
    "title": "Cognitive Architectures for Language Agents",
    "volume": "survey",
    "abstract": "Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA). CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decision-making process to choose actions. We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, CoALA contextualizes today's language agents within the broader history of AI and outlines a path towards language-based general intelligence",
    "checked": true,
    "id": "e4bb1b1f97711a7634bf4bff72c56891be2222e6",
    "semantic_title": "cognitive architectures for language agents",
    "citation_count": 55,
    "authors": [
      "Theodore Sumers",
      "Shunyu Yao",
      "Karthik Narasimhan",
      "Thomas Griffiths"
    ]
  },
  "https://openreview.net/forum?id=fPQSxjqa2o": {
    "title": "From Continuous Dynamics to Graph Neural Networks: Neural Diffusion and Beyond",
    "volume": "survey",
    "abstract": "Graph neural networks (GNNs) have demonstrated significant promise in modelling relational data and have been widely applied in various fields of interest. The key mechanism behind GNNs is the so-called message passing where information is being iteratively aggregated to central nodes from their neighbourhood. Such a scheme has been found to be intrinsically linked to a physical process known as heat diffusion, where the propagation of GNNs naturally corresponds to the evolution of heat density. Analogizing the process of message passing to the heat dynamics allows to fundamentally understand the power and pitfalls of GNNs and consequently informs better model design. Recently, there emerges a plethora of works that proposes GNNs inspired from the continuous dynamics formulation, in an attempt to mitigate the known limitations of GNNs, such as oversmoothing and oversquashing. In this survey, we provide the first systematic and comprehensive review of studies that leverage the continuous perspective of GNNs. To this end, we introduce foundational ingredients for adapting continuous dynamics to GNNs, along with a general framework for the design of graph neural dynamics. We then review and categorize existing works based on their driven mechanisms and underlying dynamics. We also summarize how the limitations of classic GNNs can be addressed under the continuous framework. We conclude by identifying multiple open research directions",
    "checked": true,
    "id": "008ff48d90ea1776419ae0d2dbf528572dc2c0e0",
    "semantic_title": "from continuous dynamics to graph neural networks: neural diffusion and beyond",
    "citation_count": 7,
    "authors": [
      "Andi Han",
      "Dai Shi",
      "Lequan Lin",
      "Junbin Gao"
    ]
  },
  "https://openreview.net/forum?id=qhtHsvF5zj": {
    "title": "Automated Design of Metaheuristic Algorithms: A Survey",
    "volume": "survey",
    "abstract": "Metaheuristics have gained great success in academia and practice because their search logic can be applied to any problem with available solution representation, solution quality evaluation, and certain notions of locality. Manually designing metaheuristic algorithms for solving a target problem is criticized for being laborious, error-prone, and requiring intensive specialized knowledge. This gives rise to increasing interest in automated design of metaheuristic algorithms. With computing power to fully explore potential design choices, the automated design could reach and even surpass human-level design and could make high-performance algorithms accessible to a much wider range of researchers and practitioners. This paper presents a broad picture of automated design of metaheuristic algorithms, by conducting a survey on the common grounds and representative techniques in terms of design space, design strategies, performance evaluation strategies, and target problems in this field",
    "checked": true,
    "id": "29c96f5358556930fb86ba2242a4d8a074d9df95",
    "semantic_title": "automated design of metaheuristic algorithms: a survey",
    "citation_count": 5,
    "authors": [
      "Qi Zhao",
      "Qiqi Duan",
      "Bai Yan",
      "Shi Cheng",
      "Yuhui Shi"
    ]
  },
  "https://openreview.net/forum?id=kQmz1BMIYi": {
    "title": "Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation",
    "volume": "survey",
    "abstract": "The recent success of ChatGPT and GPT-4 has drawn widespread attention to multimodal dialogue systems. However, there is a lack of datasets in the academic community that can effectively evaluate the multimodal generation capabilities of Visual Language Models (VLMs) in textual-visual chat tasks. In this paper, we address this gap by introducing two novel multimodal datasets: the synthetic CLEVR-ATVC dataset (620K) and the manually pictured Fruit-ATVC dataset (50K). These datasets incorporate both visual and text-based inputs and outputs. Furthermore, to facilitate the accountability of multimodal systems in rejecting human requests, similar to language-based ChatGPT conversations, we introduce specific rules as supervisory signals within the datasets. This allows the trained VLM to provide a yes or no answer after engaging in visual and textual reasoning, accompanied by a language explanation to clarify the reasons behind the inability to execute the given human instruction. Our proposed method involves a two-stage training procedure, which includes training the image auto-encoder and the auto-regressive transformer from scratch. The first stage employs a discrete variational autoencoder (dVAE) to compress each image into concise tokens, which are then combined with text tokens into a single data stream. This stream is subsequently fed into the decoder-based transformer to generate visual re-creations and textual feedback in the second stage. We conduct comprehensive analyses of experimental results, focusing on re-created image quality, answer accuracy, and the model's behavior when faced with uncertainty and imperfect user queries. Through our explorations and findings, we aim to contribute valuable insights into the accountability of textual-visual generative models",
    "checked": true,
    "id": "53df959bcf6499c45e316086a96a624389a39a52",
    "semantic_title": "accountable textual-visual chat learns to reject human instructions in image re-creation",
    "citation_count": 0,
    "authors": [
      "Zhiwei Zhang",
      "Yuliang Liu"
    ]
  },
  "https://openreview.net/forum?id=tQVZgvXhZb": {
    "title": "A Unified View on Solving Objective Mismatch in Model-Based Reinforcement Learning",
    "volume": "survey",
    "abstract": "Model-based Reinforcement Learning (MBRL) aims to make agents more sample-efficient, adaptive, and explainable by learning an explicit model of the environment. While the capabilities of MBRL agents have significantly improved in recent years, how to best learn the model is still an unresolved question. The majority of MBRL algorithms aim at training the model to make accurate predictions about the environment and subsequently using the model to determine the most rewarding actions. However, recent research has shown that model predictive accuracy is often not correlated with action quality, tracing the root cause to the objective mismatch between accurate dynamics model learning and policy optimization of rewards. A number of interrelated solution categories to the objective mismatch problem have emerged as MBRL continues to mature as a research area. In this work, we provide an in-depth survey of these solution categories and propose a taxonomy to foster future research",
    "checked": true,
    "id": "690fe81e99b1486ff49c9fc4da9bcd1a7e674668",
    "semantic_title": "a unified view on solving objective mismatch in model-based reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Ran Wei",
      "Nathan Lambert",
      "Anthony D McDonald",
      "Alfredo Garcia",
      "Roberto Calandra"
    ]
  },
  "https://openreview.net/forum?id=sWlHhfijcS": {
    "title": "A Survey on Graph Construction for Geometric Deep Learning in Medicine: Methods and Recommendations",
    "volume": "survey",
    "abstract": "Graph neural networks are powerful tools that enable deep learning on non-Euclidean data structures like graphs, point clouds, and meshes. They leverage the connectivity of data points and can even benefit learning tasks on data, which is not naturally graph-structured -like point clouds. In these cases, the graph structure needs to be determined from the dataset, which adds a significant challenge to the learning process. This opens up a multitude of design choices for creating suitable graph structures, which have a substantial impact on the success of the graph learning task. However, so far no concrete guidance for choosing the most appropriate graph construction is available, not only due to the large variety of methods out there but also because of its strong connection to the dataset at hand. In medicine, for example, a large variety of different data types complicates the selection of graph construction methods even more. We therefore summarise the current state-of-the-art graph construction methods, especially for medical data. In this work, we introduce a categorisation scheme for graph types and graph construction methods. We identify two main strands of graph construction: static and adaptive methods, discuss their advantages and disadvantages, and formulate recommendations for choosing a suitable graph construction method. We furthermore discuss how a created graph structure can be assessed and to what degree it supports graph learning. We hope to support medical research with graph deep learning with this work by elucidating the wide variety of graph construction methods",
    "checked": true,
    "id": "21d8c97d588f6dfed0df00d8e2609da1bba8f07b",
    "semantic_title": "a survey on graph construction for geometric deep learning in medicine: methods and recommendations",
    "citation_count": 1,
    "authors": [
      "Tamara T. Müller",
      "Sophie Starck",
      "Alina Dima",
      "Stephan Wunderlich",
      "Kyriaki-Margarita Bintsi",
      "Kamilia Zaripova",
      "Rickmer Braren",
      "Daniel Rueckert",
      "Anees Kazi",
      "Georgios Kaissis"
    ]
  },
  "https://openreview.net/forum?id=iRDwUXYsSJ": {
    "title": "Differential Equation Scaling Limits of Shaped and Unshaped Neural Networks",
    "volume": "expert",
    "abstract": "Recent analyses of neural networks with shaped activations (i.e. the activation function is scaled as the network size grows) have led to scaling limits described by differential equations. However, these results do not a priori tell us anything about ``ordinary'' unshaped networks, where the activation is unchanged as the network size grows. In this article, we find similar differential equation based asymptotic characterization for two types of unshaped networks. Firstly, we show that the following two architectures converge to the same infinite-depth-and-width limit at initialization: (i) a fully connected ResNet with a $d^{-1/2}$ factor on the residual branch, where $d$ is the network depth. (ii) a multilayer perceptron (MLP) with depth $d \\ll$ width $n$ and shaped ReLU activation at rate $d^{-1/2}$. Secondly, for an unshaped MLP at initialization, we derive the first order asymptotic correction to the layerwise correlation. In particular, if $\\rho_\\ell$ is the correlation at layer $\\ell$, then $q_t = \\ell^2 (1 - \\rho_\\ell)$ with $t = \\frac{\\ell}{n}$ converges to an SDE with a singularity at $t=0$. These results together provide a connection between shaped and unshaped network architectures, and opens up the possibility of studying the effect of normalization methods and how it connects with shaping activation functions",
    "checked": true,
    "id": "196803e4e33df1c35f1f05ba988c9508cc80c989",
    "semantic_title": "differential equation scaling limits of shaped and unshaped neural networks",
    "citation_count": 1,
    "authors": [
      "Mufan Bill Li",
      "Mihai Nica"
    ]
  },
  "https://openreview.net/forum?id=lNAyUngGFK": {
    "title": "Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models",
    "volume": "expert",
    "abstract": "Fine-tuning language models~(LMs) on human-generated data remains a prevalent practice. However, the performance of such models is often limited by the quantity and diversity of high-quality human data. In this paper, we explore whether we can go beyond human data on tasks where we have access to scalar feedback, for example, on math problems where one can verify correctness. To do so, we investigate a simple self-training method based on expectation-maximization, which we call \\method, where we (1) generate samples from the model and filter them using binary feedback, (2) fine-tune the model on these samples, and (3) repeat this process a few times. Testing on advanced MATH reasoning and APPS coding benchmarks using PaLM-2 models, we find that \\method{} scales favorably with model size and significantly surpasses fine-tuning only on human data. Overall, our findings suggest self-training with feedback can reduce dependence on human-generated data",
    "checked": true,
    "id": "48362b169a235ca650918c489c8cea4c597da645",
    "semantic_title": "beyond human data: scaling self-training for problem-solving with language models",
    "citation_count": 31,
    "authors": [
      "Avi Singh",
      "John D Co-Reyes",
      "Rishabh Agarwal",
      "Ankesh Anand",
      "Piyush Patil",
      "Xavier Garcia",
      "Peter J Liu",
      "James Harrison",
      "Jaehoon Lee",
      "Kelvin Xu",
      "Aaron T Parisi",
      "Abhishek Kumar",
      "Alexander A Alemi",
      "Alex Rizkowsky",
      "Azade Nova",
      "Ben Adlam",
      "Bernd Bohnet",
      "Gamaleldin Fathy Elsayed",
      "Hanie Sedghi",
      "Igor Mordatch",
      "Isabelle Simpson",
      "Izzeddin Gur",
      "Jasper Snoek",
      "Jeffrey Pennington",
      "Jiri Hron",
      "Kathleen Kenealy",
      "Kevin Swersky",
      "Kshiteej Mahajan",
      "Laura A Culp",
      "Lechao Xiao",
      "Maxwell Bileschi",
      "Noah Constant",
      "Roman Novak",
      "Rosanne Liu",
      "Tris Warkentin",
      "Yamini Bansal",
      "Ethan Dyer",
      "Behnam Neyshabur",
      "Jascha Sohl-Dickstein",
      "Noah Fiedel"
    ]
  },
  "https://openreview.net/forum?id=CD9Snc73AW": {
    "title": "Improving and generalizing flow-based generative models with minibatch optimal transport",
    "volume": "expert",
    "abstract": "Continuous normalizing flows (CNFs) are an attractive generative modeling technique, but they have been held back by limitations in their simulation-based maximum likelihood training. We introduce the generalized conditional flow matching (CFM) technique, a family of simulation-free training objectives for CNFs. CFM features a stable regression objective like that used to train the stochastic flow in diffusion models but enjoys the efficient inference of deterministic flow models. In contrast to both diffusion models and prior CNF training algorithms, CFM does not require the source distribution to be Gaussian or require evaluation of its density. A variant of our objective is optimal transport CFM (OT-CFM), which creates simpler flows that are more stable to train and lead to faster inference, as evaluated in our experiments. Furthermore, we show that when the true OT plan is available, our OT-CFM method approximates dynamic OT. Training CNFs with CFM improves results on a variety of conditional and unconditional generation tasks, such as inferring single cell dynamics, unsupervised image translation, and Schrödinger bridge inference. The Python code is available at https://github.com/atong01/conditional-flow-matching",
    "checked": true,
    "id": "5396c55bee2a2abf2207e1cc5e5ae72c9edef9fa",
    "semantic_title": "improving and generalizing flow-based generative models with minibatch optimal transport",
    "citation_count": 53,
    "authors": [
      "Alexander Tong",
      "Kilian FATRAS",
      "Nikolay Malkin",
      "Guillaume Huguet",
      "Yanlei Zhang",
      "Jarrid Rector-Brooks",
      "Guy Wolf",
      "Yoshua Bengio"
    ]
  },
  "https://openreview.net/forum?id=YH3oERVYjF": {
    "title": "A density estimation perspective on learning from pairwise human preferences",
    "volume": "expert",
    "abstract": "Learning from human feedback (LHF)—and in particular learning from pairwise preferences—has recently become a crucial ingredient in training large language models (LLMs), and has been the subject of much research. Most recent works frame it as a reinforcement learning problem, where a reward function is learned from pairwise preference data and the LLM is treated as a policy which is adapted to maximize the rewards, often under additional regularization constraints. We propose an alternative interpretation which centers on the generative process for pairwise preferences and treats LHF as a density estimation problem. We provide theoretical and empirical results showing that for a family of generative processes defined via preference behavior distribution equations, training a reward function on pairwise preferences effectively models an annotator's implicit preference distribution. Finally, we discuss and present findings on \"annotator misspecification\"—failure cases where wrong modeling assumptions are made about annotator behavior, resulting in poorly-adapted models—suggesting that approaches that learn from pairwise human preferences could have trouble learning from a population of annotators with diverse viewpoints",
    "checked": true,
    "id": "12cf8be0865649b5e878051182b69f51e67fb8d4",
    "semantic_title": "a density estimation perspective on learning from pairwise human preferences",
    "citation_count": 3,
    "authors": [
      "Vincent Dumoulin",
      "Daniel D. Johnson",
      "Pablo Samuel Castro",
      "Hugo Larochelle",
      "Yann Dauphin"
    ]
  },
  "https://openreview.net/forum?id=vFSsRYGpjW": {
    "title": "Distributional GFlowNets with Quantile Flows",
    "volume": "expert",
    "abstract": "Generative Flow Networks (GFlowNets) are a new family of probabilistic samplers where an agent learns a stochastic policy for generating complex combinatorial structure through a series of decision-making steps. There have been recent successes in applying GFlowNets to a number of practical domains where diversity of the solutions is crucial, while reinforcement learning aims to learn an optimal solution based on the given reward function only and fails to discover diverse and high-quality solutions. However, the current GFlowNet framework is relatively limited in its applicability and cannot handle stochasticity in the reward function. In this work, we adopt a distributional paradigm for GFlowNets, turning each flow function into a distribution, thus providing more informative learning signals during training. By parameterizing each edge flow through their quantile functions, our proposed \\textit{quantile matching} GFlowNet learning algorithm is able to learn a risk-sensitive policy, an essential component for handling scenarios with risk uncertainty. Moreover, we find that the distributional approach can achieve substantial improvement on existing benchmarks compared to prior methods due to our enhanced training algorithm, even in settings with deterministic rewards",
    "checked": true,
    "id": "8031bfd2408b9fa41e85b346b9452ebd63073076",
    "semantic_title": "distributional gflownets with quantile flows",
    "citation_count": 13,
    "authors": [
      "Dinghuai Zhang",
      "Ling Pan",
      "Ricky T. Q. Chen",
      "Aaron Courville",
      "Yoshua Bengio"
    ]
  },
  "https://openreview.net/forum?id=Ytp9KFKZfZ": {
    "title": "Leveraging Function Space Aggregation for Federated Learning at Scale",
    "volume": "expert",
    "abstract": "The federated learning paradigm has motivated the development of methods for aggregating multiple client updates into a global server model, without sharing client data. Many federated learning algorithms, including the canonical Federated Averaging (FedAvg), take a direct (possibly weighted) average of the client parameter updates, motivated by results in distributed optimization. In this work, we adopt a function space perspective and propose a new algorithm, FedFish, that aggregates local approximations to the functions learned by clients, using an estimate based on their Fisher information. We evaluate FedFish on realistic, large-scale cross-device benchmarks. While the performance of FedAvg can suffer as client models drift further apart, we demonstrate that FedFish is more robust to longer local training. Our evaluation across several settings in image and language benchmarks shows that FedFish outperforms FedAvg as local training epochs increase. Further, FedFish results in global networks that are more amenable to efficient personalization via local fine-tuning on the same or shifted data distributions. For instance, federated pretraining on the C4 dataset, followed by few-shot personalization on Stack Overflow, results in a 7% improvement in next-token prediction by FedFish over FedAvg",
    "checked": true,
    "id": "ea88649be9fbc54d49833edcf8903df21afb5f6f",
    "semantic_title": "leveraging function space aggregation for federated learning at scale",
    "citation_count": 0,
    "authors": [
      "Nikita Dhawan",
      "Nicole Elyse Mitchell",
      "Zachary Charles",
      "Zachary Garrett",
      "Gintare Karolina Dziugaite"
    ]
  },
  "https://openreview.net/forum?id=UAT4j3Y7HP": {
    "title": "Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks",
    "volume": "expert",
    "abstract": "Real-world natural language processing systems need to be robust to human adversaries. Collecting examples of human adversaries for training is an effective but expensive solution. On the other hand, training on synthetic attacks with small perturbations---such as word-substitution---does not actually improve robustness to human adversaries. In this paper, we propose an adversarial training framework that uses limited human adversarial examples to generate more useful adversarial examples at scale. We demonstrate the advantages of this system on the ANLI and hate speech detection benchmark datasets---both collected via an iterative, adversarial human-and-model-in-the-loop procedure. Compared to training only on observed human attacks, also training on our synthetic adversarial examples improves model robustness to future rounds. In ANLI, we see accuracy gains on the current set of attacks (44.1\\%$\\,\\to\\,$50.1\\%) and on two future unseen rounds of human generated attacks (32.5\\%$\\,\\to\\,$43.4\\%, and 29.4\\%$\\,\\to\\,$40.2\\%). In hate speech detection, we see AUC gains on current attacks (0.76 $\\to$ 0.84) and a future round (0.77 $\\to$ 0.79). Attacks from methods that do not learn the distribution of existing human adversaries, meanwhile, degrade robustness",
    "checked": true,
    "id": "9262e201ecaf678429bf86481b1c1d1ae6332b44",
    "semantic_title": "break it, imitate it, fix it: robustness by generating human-like attacks",
    "citation_count": 0,
    "authors": [
      "Aradhana Sinha",
      "Ananth Balashankar",
      "Ahmad Beirami",
      "Thi Avrahami",
      "Jilin Chen",
      "Alex Beutel"
    ]
  },
  "https://openreview.net/forum?id=ga5SNulYet": {
    "title": "Wavelet Networks: Scale-Translation Equivariant Learning From Raw Time-Series",
    "volume": "expert",
    "abstract": "Leveraging the symmetries inherent to specific data domains for the construction of equivariant neural networks has lead to remarkable improvements in terms of data efficiency and generalization. However, most existing research focuses on symmetries arising from planar and volumetric data, leaving a crucial data source largely underexplored: *time-series*. In this work, we fill this gap by leveraging the symmetries inherent to time-series for the construction of equivariant neural network. We identify two core symmetries: *scale and translation*, and construct scale-translation equivariant neural networks for time-series learning. Intriguingly, we find that scale-translation equivariant mappings share strong resemblance with the *wavelet transform*. Inspired by this resemblance, we term our networks *Wavelet Networks*, and show that they perform nested non-linear wavelet-like time-frequency transforms. Empirical results show that Wavelet Networks outperform conventional CNNs on raw waveforms, and match strongly engineered spectrogram techniques across several tasks and time-series types, including audio, environmental sounds, and electrical signals. Our code is publicly available at https://github.com/dwromero/wavelet_networks",
    "checked": false,
    "id": "1ad1d4691c74f25a6d3548214bcd1b2c10611bce",
    "semantic_title": "sossf: landsat-8 image synthesis on the blending of sentinel-1 and modis data",
    "citation_count": 1,
    "authors": [
      "David W. Romero",
      "Erik J Bekkers",
      "Jakub M. Tomczak",
      "Mark Hoogendoorn"
    ]
  },
  "https://openreview.net/forum?id=lTOku838Zv": {
    "title": "Neural Implicit Manifold Learning for Topology-Aware Density Estimation",
    "volume": "expert",
    "abstract": "Natural data observed in $\\mathbb{R}^n$ is often constrained to an $m$-dimensional manifold $\\mathcal{M}$, where $m < n$. This work focuses on the task of building theoretically principled generative models for such data. Current generative models learn $\\mathcal{M}$ by mapping an $m$-dimensional latent variable through a neural network $f_\\theta: \\mathbb{R}^m \\to \\mathbb{R}^n$. These procedures, which we call pushforward models, incur a straightforward limitation: manifolds cannot in general be represented with a single parameterization, meaning that attempts to do so will incur either computational instability or the inability to learn probability densities within the manifold. To remedy this problem, we propose to model $\\mathcal{M}$ as a neural implicit manifold: the set of zeros of a neural network. We then learn the probability density within $\\mathcal{M}$ with a constrained energy-based model, which employs a constrained variant of Langevin dynamics to train and sample from the learned manifold. In experiments on synthetic and natural data, we show that our model can learn manifold-supported distributions with complex topologies more accurately than pushforward models",
    "checked": true,
    "id": "914b2aed4d0938d7dce4695d1c25069ea68a1d37",
    "semantic_title": "neural implicit manifold learning for topology-aware density estimation",
    "citation_count": 0,
    "authors": [
      "Brendan Leigh Ross",
      "Gabriel Loaiza-Ganem",
      "Anthony L. Caterini",
      "Jesse C. Cresswell"
    ]
  }
}