{
  "https://openreview.net/forum?id=OcFjqiJ98b": {
    "title": "PCNN: Probable-Class Nearest-Neighbor Explanations Improve Fine-Grained Image Classification Accuracy for AIs and Humans",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giang Nguyen",
      "Valerie Chen",
      "Mohammad Reza Taesiri",
      "Anh Nguyen"
    ]
  },
  "https://openreview.net/forum?id=WLcPrq6pu0": {
    "title": "Leveraging Task Structures for Improved Identifiability in Neural Network Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenlin Chen",
      "Julien Horwood",
      "Juyeon Heo",
      "José Miguel Hernández-Lobato"
    ]
  },
  "https://openreview.net/forum?id=u0eiu1MTS7": {
    "title": "Re-Thinking Inverse Graphics With Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Kulits",
      "Haiwen Feng",
      "Weiyang Liu",
      "Victoria Fernandez Abrevaya",
      "Michael J. Black"
    ]
  },
  "https://openreview.net/forum?id=YoWBLu74TL": {
    "title": "Path Development Network with Finite-dimensional Lie Group",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Lou",
      "Siran Li",
      "Hao Ni"
    ]
  },
  "https://openreview.net/forum?id=HwAZDVxkLX": {
    "title": "Biased Dueling Bandits with Stochastic Delayed Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bongsoo Yi",
      "Yue Kang",
      "Yao Li"
    ]
  },
  "https://openreview.net/forum?id=lXyZr9TLEU": {
    "title": "AdaStop: adaptive statistical testing for sound comparisons of Deep RL agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Timothée Mathieu",
      "Matheus Medeiros Centa",
      "Riccardo Della Vecchia",
      "Hector Kohler",
      "Alena Shilova",
      "Odalric-Ambrym Maillard",
      "Philippe Preux"
    ]
  },
  "https://openreview.net/forum?id=9OHAtWdFWB": {
    "title": "Robust Feature Inference: A Test-time Defense Strategy using Spectral Projections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anurag Singh",
      "Mahalakshmi Sabanayagam",
      "Krikamol Muandet",
      "Debarghya Ghoshdastidar"
    ]
  },
  "https://openreview.net/forum?id=isEFziui9p": {
    "title": "A Practical Guide to Sample-based Statistical Distances for Evaluating Generative Models in Science",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Bischoff",
      "Alana Darcher",
      "Michael Deistler",
      "Richard Gao",
      "Franziska Gerken",
      "Manuel Gloeckler",
      "Lisa Haxel",
      "Jaivardhan Kapoor",
      "Janne K Lappalainen",
      "Jakob H. Macke",
      "Guy Moss",
      "Matthijs Pals",
      "Felix C Pei",
      "Rachel Rapp",
      "A Erdem Sağtekin",
      "Cornelius Schröder",
      "Auguste Schulz",
      "Zinovia Stefanidi",
      "Shoji Toyota",
      "Linda Ulmer",
      "Julius Vetter"
    ]
  },
  "https://openreview.net/forum?id=1Kzzm22usl": {
    "title": "Learning a Decision Tree Algorithm with Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufan Zhuang",
      "Liyuan Liu",
      "Chandan Singh",
      "Jingbo Shang",
      "Jianfeng Gao"
    ]
  },
  "https://openreview.net/forum?id=dtNEvUOZmA": {
    "title": "InvariantStock: Learning Invariant Features for Mastering the Shifting Market",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyao Cao",
      "Jinan Zou",
      "Yuhang Liu",
      "Zhen Zhang",
      "Ehsan Abbasnejad",
      "Anton van den Hengel",
      "Javen Qinfeng Shi"
    ]
  },
  "https://openreview.net/forum?id=7pBKrcn199": {
    "title": "XAI-Based Detection of Adversarial Attacks on Deepfake Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Pinhasov",
      "Raz Lapid",
      "Rony Ohayon",
      "Moshe Sipper",
      "Yehudit Aperstein"
    ]
  },
  "https://openreview.net/forum?id=4c2pZzG94y": {
    "title": "Robust and Efficient Quantization-aware Training via Coreset Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xijie Huang",
      "Zechun Liu",
      "Shih-Yang Liu",
      "Kwang-Ting Cheng"
    ]
  },
  "https://openreview.net/forum?id=gVNyEVKjqf": {
    "title": "AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Hendrik Metzen",
      "Piyapat Saranrittichai",
      "Chaithanya Kumar Mummadi"
    ]
  },
  "https://openreview.net/forum?id=GSnGPgeoS5": {
    "title": "iHyperTime: Interpretable Time Series Generation with Implicit Neural Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elizabeth Fons",
      "Alejandro Sztrajman",
      "Yousef El-Laham",
      "Andrea Coletta",
      "Alexandros Iosifidis",
      "Svitlana Vyetrenko"
    ]
  },
  "https://openreview.net/forum?id=Kxtpa9rvM0": {
    "title": "Sensitivity-Aware Amortized Bayesian Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lasse Elsemüller",
      "Hans Olischläger",
      "Marvin Schmitt",
      "Paul-Christian Bürkner",
      "Ullrich Koethe",
      "Stefan T. Radev"
    ]
  },
  "https://openreview.net/forum?id=G7p8djzWOl": {
    "title": "Automatic Data Curation for Self-Supervised Learning: A Clustering-Based Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huy V. Vo",
      "Vasil Khalidov",
      "Timothée Darcet",
      "Théo Moutakanni",
      "Nikita Smetanin",
      "Marc Szafraniec",
      "Hugo Touvron",
      "camille couprie",
      "Maxime Oquab",
      "Armand Joulin",
      "Herve Jegou",
      "Patrick Labatut",
      "Piotr Bojanowski"
    ]
  },
  "https://openreview.net/forum?id=JoYMJJdvry": {
    "title": "Byzantine-Resilient Decentralized Multi-Armed Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingxuan Zhu",
      "Alec Koppel",
      "Alvaro Velasquez",
      "Ji Liu"
    ]
  },
  "https://openreview.net/forum?id=Ve4Puj2LVT": {
    "title": "On the Stochastic (Variance-Reduced) Proximal Gradient Method for Regularized Expected Reward Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ling Liang",
      "Haizhao Yang"
    ]
  },
  "https://openreview.net/forum?id=iBorskJRrg": {
    "title": "Accelerated Deep Active Learning with Graph-based Sub- Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Kushnir",
      "Shiyun Xu"
    ]
  },
  "https://openreview.net/forum?id=7tlYbcq5DY": {
    "title": "Affordable Generative Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangbin Yu",
      "Qin Zhang",
      "junyou li",
      "QIANG FU",
      "Deheng Ye"
    ]
  },
  "https://openreview.net/forum?id=6L6cD65pot": {
    "title": "Vision-Language Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xindi Wu",
      "Byron Zhang",
      "Zhiwei Deng",
      "Olga Russakovsky"
    ]
  },
  "https://openreview.net/forum?id=7DzU88VrNU": {
    "title": "Training-free Graph Neural Networks and the Power of Labels as Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryoma Sato"
    ]
  },
  "https://openreview.net/forum?id=aggyMifxLQ": {
    "title": "Defending Against Unknown Corrupted Agents: Reinforcement Learning of Adversarially Robust Nash Equilibria",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andi Nika",
      "Jonathan Nöther",
      "Adish Singla",
      "Goran Radanovic"
    ]
  },
  "https://openreview.net/forum?id=jcleXdnRA1": {
    "title": "C3DM: Constrained-Context Conditional Diffusion Models for Imitation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vaibhav Saxena",
      "Yotto koga",
      "Danfei Xu"
    ]
  },
  "https://openreview.net/forum?id=HVWODwbrFK": {
    "title": "Memorisation in Machine Learning: A Survey of Results",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dmitrii Usynin",
      "Moritz Knolle",
      "Georgios Kaissis"
    ]
  },
  "https://openreview.net/forum?id=4bo6XAnutd": {
    "title": "Privacy-Preserving Split Learning with Vision Transformers using Patch-Wise Random and Noisy CutMix",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungeun Oh",
      "Sihun Baek",
      "Jihong Park",
      "Hyelin Nam",
      "Praneeth Vepakomma",
      "Ramesh Raskar",
      "Mehdi Bennis",
      "Seong-Lyun Kim"
    ]
  },
  "https://openreview.net/forum?id=igJ2XPNYbJ": {
    "title": "APBench: A Unified Availability Poisoning Attack and Defenses Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianrui Qin",
      "Xitong Gao",
      "Juanjuan Zhao",
      "Kejiang Ye",
      "Cheng-zhong Xu"
    ]
  },
  "https://openreview.net/forum?id=I9HvzJbUbh": {
    "title": "DFML: Decentralized Federated Mutual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasser H. Khalil",
      "Amir Hossein Estiri",
      "Mahdi Beitollahi",
      "Nader Asadi",
      "Sobhan Hemati",
      "Xu Li",
      "Guojun Zhang",
      "Xi Chen"
    ]
  },
  "https://openreview.net/forum?id=ZSqP1RT8jC": {
    "title": "kNN-CLIP: Retrieval Enables Training-Free Segmentation on Continually Expanding Large Vocabularies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongrui Gui",
      "Shuyang Sun",
      "Runjia Li",
      "Jianhao Yuan",
      "Zhaochong An",
      "Karsten Roth",
      "Ameya Prabhu",
      "Philip Torr"
    ]
  },
  "https://openreview.net/forum?id=JRjD0YF3Yd": {
    "title": "Bayesian optimization with derivatives acceleration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guillaume Perrin",
      "rodolphe le riche"
    ]
  },
  "https://openreview.net/forum?id=XPLXYr7NlR": {
    "title": "Learning $k$-Level Structured Sparse Neural Networks Using Group Envelope Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yehonathan Refael",
      "Iftach Arbel",
      "Wasim Huleihel"
    ]
  },
  "https://openreview.net/forum?id=gfANevPraH": {
    "title": "Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maurits Bleeker",
      "Mariya Hendriksen",
      "Andrew Yates",
      "Maarten de Rijke"
    ]
  },
  "https://openreview.net/forum?id=db2pFKVcm1": {
    "title": "Variational Bayesian Imaging with an Efficient Surrogate Score-based Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Berthy Feng",
      "Katherine Bouman"
    ]
  },
  "https://openreview.net/forum?id=H8IaxrANWl": {
    "title": "Differentiating Through Integer Linear Programs with Quadratic Regularization and Davis-Yin Splitting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel McKenzie",
      "Howard Heaton",
      "Samy Wu Fung"
    ]
  },
  "https://openreview.net/forum?id=uvPnTWMLll": {
    "title": "Calibrating Deep Ensemble through Functional Variational Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijie Deng",
      "Feng Zhou",
      "Jianfei Chen",
      "Guoqiang Wu",
      "Jun Zhu"
    ]
  },
  "https://openreview.net/forum?id=6OmRkUHgs5": {
    "title": "Federated Variational Inference: Towards Improved Personalization and Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elahe Vedadi",
      "Joshua V. Dillon",
      "Philip Andrew Mansfield",
      "Karan Singhal",
      "Arash Afkanpour",
      "Warren Richard Morningstar"
    ]
  },
  "https://openreview.net/forum?id=XWQgXLYwv2": {
    "title": "How to choose the right transfer learning protocol? A qualitative analysis in a controlled set-up",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Federica Gerace",
      "Diego Doimo",
      "Stefano Sarao Mannelli",
      "Luca Saglietti",
      "Alessandro Laio"
    ]
  },
  "https://openreview.net/forum?id=CSv7GgKHb6": {
    "title": "Graph Harmony: Denoising and Nuclear-Norm Wasserstein Adaptation for Enhanced Domain Transfer in Graph-Structured Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengxi Wu",
      "Mohammad Rostami"
    ]
  },
  "https://openreview.net/forum?id=XzaSGIStXP": {
    "title": "Learning Hybrid Interpretable Models: Theory, Taxonomy, and Methods",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julien Ferry",
      "Gabriel Laberge",
      "Ulrich Aïvodji"
    ]
  },
  "https://openreview.net/forum?id=dixU4fozPQ": {
    "title": "Closing the gap between SVRG and TD-SVRG with Gradient Splitting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arsenii Mustafin",
      "Alex Olshevsky",
      "Ioannis Paschalidis"
    ]
  },
  "https://openreview.net/forum?id=A5ulGfDBON": {
    "title": "Finite-Time Analysis of Temporal Difference Learning with Experience Replay",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han-Dong Lim",
      "Donghwan Lee"
    ]
  },
  "https://openreview.net/forum?id=GZORXGxHHT": {
    "title": "The Cold Posterior Effect Indicates Underfitting, and Cold Posteriors Represent a Fully Bayesian Method to Mitigate It",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijie Zhang",
      "Yi-Shan Wu",
      "Luis A. Ortega",
      "Andres R Masegosa"
    ]
  },
  "https://openreview.net/forum?id=LV04KBaIQt": {
    "title": "Meta-Learning Approach for Joint Multimodal Signals with Multimodal Iterative Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sehun Lee",
      "Wonkwang Lee",
      "Gunhee Kim"
    ]
  },
  "https://openreview.net/forum?id=xNkASJL0F6": {
    "title": "Dynamic Structure Estimation from Bandit Feedback using Nonvanishing Exponential Sums",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Motoya Ohnishi",
      "Isao Ishikawa",
      "Yuko Kuroki",
      "Masahiro Ikeda"
    ]
  },
  "https://openreview.net/forum?id=e92dgUUfk0": {
    "title": "Improving Black-box Robustness with In-Context Rewriting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyle O'Brien",
      "Nathan Hoyen Ng",
      "Isha Puri",
      "Jorge Mendez-Mendez",
      "Hamid Palangi",
      "Yoon Kim",
      "Marzyeh Ghassemi",
      "Thomas Hartvigsen"
    ]
  },
  "https://openreview.net/forum?id=9sZsjfZV3q": {
    "title": "Efficient Action Robust Reinforcement Learning with Probabilistic Policy Execution Uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanlin Liu",
      "Zhihan Zhou",
      "Han Liu",
      "Lifeng Lai"
    ]
  },
  "https://openreview.net/forum?id=0VWXWPmctm": {
    "title": "Pre-trained Hypergraph Convolutional Neural Networks with Self-supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihe Deng",
      "Ruochi Zhang",
      "Pan Xu",
      "Jian Ma",
      "Quanquan Gu"
    ]
  },
  "https://openreview.net/forum?id=nH416rLxtI": {
    "title": "Variational Autoencoding of Dental Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johan Ziruo Ye",
      "Thomas Ørkild",
      "Peter Lempel Søndergard",
      "Søren Hauberg"
    ]
  },
  "https://openreview.net/forum?id=1qZyJQxOof": {
    "title": "The Survival Bandit Problem",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charles Riou",
      "Junya Honda",
      "Masashi Sugiyama"
    ]
  },
  "https://openreview.net/forum?id=MywlrEaFqR": {
    "title": "DIG-MILP: a Deep Instance Generator for Mixed-Integer Linear Programming with Feasibility Guarantee",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Peter Wang",
      "Jialin Liu",
      "Xiaohan Chen",
      "Xinshang Wang",
      "Pan Li",
      "Wotao Yin"
    ]
  },
  "https://openreview.net/forum?id=Br5esc2CXR": {
    "title": "Deep Backtracking Counterfactuals for Causally Compliant Explanations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Klaus-Rudolf Kladny",
      "Julius von Kügelgen",
      "Bernhard Schölkopf",
      "Michael Muehlebach"
    ]
  },
  "https://openreview.net/forum?id=ffBj12yh58": {
    "title": "Supervised Domain Adaptation Based on Marginal and Conditional Distributions Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ori Katz",
      "Ronen Talmon",
      "Uri Shaham"
    ]
  },
  "https://openreview.net/forum?id=s9ylaDLvdO": {
    "title": "Exploiting Hankel-Toeplitz Structures for Fast Computation of Kernel Precision Matrices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frida Marie Viset",
      "Anton Kullberg",
      "Frederiek Wesel",
      "Arno Solin"
    ]
  },
  "https://openreview.net/forum?id=5g5zFVj33K": {
    "title": "Doubly Robust Kernel Statistics for Testing Distributional Treatment Effects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jake Fawkes",
      "Robert Hu",
      "Robin J. Evans",
      "Dino Sejdinovic"
    ]
  },
  "https://openreview.net/forum?id=uHLDkQVtyC": {
    "title": "How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Subhabrata Dutta",
      "Joykirat Singh",
      "Soumen Chakrabarti",
      "Tanmoy Chakraborty"
    ]
  },
  "https://openreview.net/forum?id=WyPKLWPYsr": {
    "title": "Weighted Risk Invariance: Domain Generalization under Invariant Feature Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gina Wong",
      "Joshua Gleason",
      "Rama Chellappa",
      "Yoav Wald",
      "Anqi Liu"
    ]
  },
  "https://openreview.net/forum?id=ZJ4A3xhADV": {
    "title": "Federated Learning with Reduced Information Leakage and Computation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongxin Yin",
      "Xuwei Tan",
      "Xueru Zhang",
      "Mohammad Mahdi Khalili",
      "Mingyan Liu"
    ]
  },
  "https://openreview.net/forum?id=Ry5CXXm1sf": {
    "title": "Transformer-Based Models Are Not Yet Perfect At Learning to Emulate Structural Recursion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dylan Zhang",
      "Curt Tigges",
      "Zory Zhang",
      "Stella Biderman",
      "Maxim Raginsky",
      "Talia Ringer"
    ]
  },
  "https://openreview.net/forum?id=zOJ846BXhl": {
    "title": "Revisiting Non-separable Binary Classification and its Applications in Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Lau",
      "ISMAILA SECK",
      "Athanasios P Meliopoulos",
      "Wenke Lee",
      "Eugene Ndiaye"
    ]
  },
  "https://openreview.net/forum?id=JZVqDTNA59": {
    "title": "SEAL: Simultaneous Label Hierarchy Exploration And Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiquan Tan",
      "Zihao Wang",
      "Yifan Zhang"
    ]
  },
  "https://openreview.net/forum?id=1hcpXd9Jir": {
    "title": "Fast and Effective Weight Update for Pruned Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vladimír Boža"
    ]
  },
  "https://openreview.net/forum?id=LNvbgBFPMt": {
    "title": "A Self-Representation Learning Method for Unsupervised Feature Selection using Feature Space Basis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prayag Tiwari",
      "Farid Saberi Movahed",
      "Saeed Karami",
      "Farshad Saberi-Movahed",
      "Jens Lehmann",
      "Sahar Vahdati"
    ]
  },
  "https://openreview.net/forum?id=TAvGZm2Rqb": {
    "title": "Cost-Sensitive Learning to Defer to Multiple Experts with Workload Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jean Vieira Alves",
      "Diogo Leitão",
      "Sérgio Jesus",
      "Marco O. P. Sampaio",
      "Javier Liébana",
      "Pedro Saleiro",
      "Mario A. T. Figueiredo",
      "Pedro Bizarro"
    ]
  },
  "https://openreview.net/forum?id=voHKJOdCNw": {
    "title": "Task-Relevant Feature Selection with Prediction Focused Mixture Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhishek Sharma",
      "Catherine Zeng",
      "Sanjana Narayanan",
      "Sonali Parbhoo",
      "Roy H. Perlis",
      "Finale Doshi-Velez"
    ]
  },
  "https://openreview.net/forum?id=jG7ndW7UHp": {
    "title": "Convergence Analysis and Trajectory Comparison of Gradient Descent for Overparameterized Deep Linear Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongru Zhao",
      "Jinchao Xu"
    ]
  },
  "https://openreview.net/forum?id=AQk0UsituG": {
    "title": "Variational Learning ISTA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fabio Valerio Massoli",
      "Christos Louizos",
      "Arash Behboodi"
    ]
  },
  "https://openreview.net/forum?id=c8WJ4Vozb2": {
    "title": "Correcting Flaws in Common Disentanglement Metrics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Louis Mahon",
      "Lei Sha",
      "Thomas Lukasiewicz"
    ]
  },
  "https://openreview.net/forum?id=BK6Gc10tRy": {
    "title": "Overcoming Order in Autoregressive Graph Generation for Molecule Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edo Cohen-Karlik",
      "Eyal Rozenberg",
      "Daniel Freedman"
    ]
  },
  "https://openreview.net/forum?id=qSFToMqLcq": {
    "title": "Neural Clamping: Joint Input Perturbation and Temperature Scaling for Neural Network Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yung-Chen Tang",
      "Pin-Yu Chen",
      "Tsung-Yi Ho"
    ]
  },
  "https://openreview.net/forum?id=7HIOUZAoq5": {
    "title": "A replica analysis of under-bagging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takashi Takahashi"
    ]
  },
  "https://openreview.net/forum?id=YN0IcnXqsr": {
    "title": "Augment then Smooth: Reconciling Differential Privacy with Certified Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiapeng Wu",
      "Atiyeh Ashari Ghomi",
      "David Glukhov",
      "Jesse C. Cresswell",
      "Franziska Boenisch",
      "Nicolas Papernot"
    ]
  },
  "https://openreview.net/forum?id=BmI5p6wBi0": {
    "title": "Deep Unlearning: Fast and Efficient Gradient-free Class Forgetting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangamesh Kodge",
      "Gobinda Saha",
      "Kaushik Roy"
    ]
  },
  "https://openreview.net/forum?id=7jgu4oXsGM": {
    "title": "Dual-windowed Vision Transformer with Angular Self- Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weili Shi",
      "Sheng Li"
    ]
  },
  "https://openreview.net/forum?id=YfPzUX6DdO": {
    "title": "On the Importance of Uncertainty in Decision-Making with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolò Felicioni",
      "Lucas Maystre",
      "Sina Ghiassian",
      "Kamil Ciosek"
    ]
  },
  "https://openreview.net/forum?id=9B6LM2uoEs": {
    "title": "ITEM: Improving Training and Evaluation of Message-Passing based GNNs for top-k recommendation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yannis Karmim",
      "Elias Ramzi",
      "Raphael Fournier-S'niehotta",
      "Nicolas THOME"
    ]
  },
  "https://openreview.net/forum?id=uQYomAuo7M": {
    "title": "Interpreting Global Perturbation Robustness of Image Models using Axiomatic Spectral Importance Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roisin Luo",
      "James McDermott",
      "Colm O'Riordan"
    ]
  },
  "https://openreview.net/forum?id=iwCBWULItx": {
    "title": "DeepReShape: Redesigning Neural Networks for Efficient Private Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nandan Kumar Jha",
      "Brandon Reagen"
    ]
  },
  "https://openreview.net/forum?id=8uCNtJ2Fmo": {
    "title": "Exploiting Edge Features in Graph-based Learning with Fused Network Gromov-Wasserstein Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Yang",
      "Matthieu Labeau",
      "Florence d'Alché-Buc"
    ]
  },
  "https://openreview.net/forum?id=Nux7OVXpJ9": {
    "title": "Mini-Batch Optimization of Contrastive Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaewoong Cho",
      "Kartik Sreenivasan",
      "Keon Lee",
      "Kyunghoo Mun",
      "Soheun Yi",
      "Jeong-Gwan Lee",
      "Anna Lee",
      "Jy-yong Sohn",
      "Dimitris Papailiopoulos",
      "Kangwook Lee"
    ]
  },
  "https://openreview.net/forum?id=fa1ne8xDGn": {
    "title": "Improved motif-scaffolding with SE(3) flow matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jason Yim",
      "Andrew Campbell",
      "Emile Mathieu",
      "Andrew Y. K. Foong",
      "Michael Gastegger",
      "Jose Jimenez-Luna",
      "Sarah Lewis",
      "Victor Garcia Satorras",
      "Bastiaan S. Veeling",
      "Frank Noe",
      "Regina Barzilay",
      "Tommi Jaakkola"
    ]
  },
  "https://openreview.net/forum?id=gPtjyzXskg": {
    "title": "XAudit : A Learning-Theoretic Look at Auditing with Explanations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chhavi Yadav",
      "Michal Moshkovitz",
      "Kamalika Chaudhuri"
    ]
  },
  "https://openreview.net/forum?id=0Hm01Vc8zT": {
    "title": "Fair Representation in Submodular Subset Selection: A Pareto Optimization Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adriano Fazzone",
      "Yanhao Wang",
      "Francesco Bonchi"
    ]
  },
  "https://openreview.net/forum?id=QXLKnrymE1": {
    "title": "Representation Learning Dynamics of Self-Supervised Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pascal Esser",
      "Satyaki Mukherjee",
      "Debarghya Ghoshdastidar"
    ]
  },
  "https://openreview.net/forum?id=pRt1Vw1DPs": {
    "title": "Learning Counterfactually Invariant Predictors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Quinzan",
      "Cecilia Casolo",
      "Krikamol Muandet",
      "Yucen Luo",
      "Niki Kilbertus"
    ]
  },
  "https://openreview.net/forum?id=wLe1bG93yc": {
    "title": "A Note on the Convergence of Denoising Diffusion Probabilistic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sokhna Diarra Mbacke",
      "Omar Rivasplata"
    ]
  },
  "https://openreview.net/forum?id=P3Lyun7CZs": {
    "title": "Data Attribution for Diffusion Models: Timestep-induced Bias in Influence Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Xie",
      "Haoyu Li",
      "Andrew Bai",
      "Cho-Jui Hsieh"
    ]
  },
  "https://openreview.net/forum?id=RNsnSLdmV7": {
    "title": "Language Models Are Better Than Humans at Next-token Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Buck Shlegeris",
      "Fabien Roger",
      "Lawrence Chan",
      "Euan McLean"
    ]
  },
  "https://openreview.net/forum?id=Z8wcREe9qV": {
    "title": "Harnessing the Power of Federated Learning in Federated Contextual Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengshuai Shi",
      "Ruida Zhou",
      "Kun Yang",
      "Cong Shen"
    ]
  },
  "https://openreview.net/forum?id=Viz7KBqO4A": {
    "title": "Diversity-Preserving $K$--Armed Bandits, Revisited",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hedi Hadiji",
      "Sébastien Gerchinovitz",
      "Jean-Michel Loubes",
      "Gilles Stoltz"
    ]
  },
  "https://openreview.net/forum?id=7wybYcK1pw": {
    "title": "Masked multi-prediction for multi-aspect anomaly detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yassine Naji",
      "Romaric Audigier",
      "Aleksandr Setkov",
      "Angelique Loesch",
      "Michèle Gouiffès"
    ]
  },
  "https://openreview.net/forum?id=ZTcxp9xYr2": {
    "title": "Read Between the Layers: Leveraging Multi-Layer Representations for Rehearsal-Free Continual Learning with Pre-Trained Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyra Ahrens",
      "Hans Hergen Lehmann",
      "Jae Hee Lee",
      "Stefan Wermter"
    ]
  },
  "https://openreview.net/forum?id=WCUT6leXKf": {
    "title": "SeqLink: A Robust Neural-ODE Architecture for Modelling Partially Observed Time Series",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Futoon M. Abushaqra",
      "Hao Xue",
      "Yongli Ren",
      "Flora D. Salim"
    ]
  },
  "https://openreview.net/forum?id=oxAZv3QD6M": {
    "title": "XPL: A Cross-Model framework for Semi-Supervised Prompt Learning in Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omprakash Chakraborty",
      "Aadarsh Sahoo",
      "Rameswar Panda",
      "Abir Das"
    ]
  },
  "https://openreview.net/forum?id=u8K83M9mbG": {
    "title": "Revisiting Active Learning in the Era of Vision Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanket Rajan Gupte",
      "Josiah Aklilu",
      "Jeffrey J Nirschl",
      "Serena Yeung-Levy"
    ]
  },
  "https://openreview.net/forum?id=CuyJkNjIVd": {
    "title": "Homogenizing Non-IID Datasets via In-Distribution Knowledge Distillation for Decentralized Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deepak Ravikumar",
      "Gobinda Saha",
      "Sai Aparna Aketi",
      "Kaushik Roy"
    ]
  },
  "https://openreview.net/forum?id=VGNBUS9TrU": {
    "title": "Combine and Conquer: A Meta-Analysis on Data Shift and Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eduardo Dadalto Câmara Gomes",
      "Florence Alberge",
      "Pierre Duhamel",
      "Pablo Piantanida"
    ]
  },
  "https://openreview.net/forum?id=otTFPjziiK": {
    "title": "Directed Graph Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qitong Wang",
      "Georgios Kollias",
      "Vasileios Kalantzis",
      "Naoki Abe",
      "Mohammed J Zaki"
    ]
  },
  "https://openreview.net/forum?id=4CUkCG6ITe": {
    "title": "Contextual Policies Enable Efficient and Interpretable Inverse Reinforcement Learning for Populations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ville Tanskanen",
      "Chang Rajani",
      "Perttu Hämäläinen",
      "Christian Guckelsberger",
      "Arto Klami"
    ]
  },
  "https://openreview.net/forum?id=TwiSBZ0p9u": {
    "title": "NuTime: Numerically Multi-Scaled Embedding for Large- Scale Time-Series Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenguo Lin",
      "Xumeng Wen",
      "Wei Cao",
      "Congrui Huang",
      "Jiang Bian",
      "Stephen Lin",
      "Zhirong Wu"
    ]
  },
  "https://openreview.net/forum?id=P5D2gfi4Gg": {
    "title": "Intriguing Properties of Hyperbolic Embeddings in Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sarah Ibrahimi",
      "Mina Ghadimi Atigh",
      "Nanne Van Noord",
      "Pascal Mettes",
      "Marcel Worring"
    ]
  },
  "https://openreview.net/forum?id=Svt75kotzs": {
    "title": "Lyra: Orchestrating Dual Correction in Automated Theorem Proving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanyang Zheng",
      "Haiming Wang",
      "Enze Xie",
      "Zhengying Liu",
      "Jiankai Sun",
      "Huajian Xin",
      "Jianhao Shen",
      "Zhenguo Li",
      "Yu Li"
    ]
  },
  "https://openreview.net/forum?id=spJI4LSPIU": {
    "title": "Understanding the Role of Invariance in Transfer Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Till Speicher",
      "Vedant Nanda",
      "Krishna P. Gummadi"
    ]
  },
  "https://openreview.net/forum?id=8YcUJbxmmC": {
    "title": "Jigsaw Game: Federated Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JINXUAN XU",
      "Hong-You Chen",
      "Wei-Lun Chao",
      "Yuqian Zhang"
    ]
  },
  "https://openreview.net/forum?id=Q0nzpRcwWn": {
    "title": "Fast, accurate and lightweight sequential simulation-based inference using Gaussian locally linear mappings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Henrik Häggström",
      "Pedro L. C. Rodrigues",
      "Geoffroy Oudoumanessah",
      "Florence Forbes",
      "Umberto Picchini"
    ]
  },
  "https://openreview.net/forum?id=Dsavre6gjN": {
    "title": "SPriFed-OMP: A Differentially Private Federated Learning Algorithm for Sparse Basis Recovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ajinkya K Mulay",
      "Xiaojun Lin"
    ]
  },
  "https://openreview.net/forum?id=EzPRgIq2Tk": {
    "title": "Active Sequential Two-Sample Testing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weizhi Li",
      "Prad Kadambi",
      "Pouria Saidi",
      "Karthikeyan Natesan Ramamurthy",
      "Gautam Dasarathy",
      "Visar Berisha"
    ]
  },
  "https://openreview.net/forum?id=DimPeeCxKO": {
    "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Ibrahim",
      "Benjamin Thérien",
      "Kshitij Gupta",
      "Mats Leon Richter",
      "Quentin Gregory Anthony",
      "Eugene Belilovsky",
      "Timothée Lesort",
      "Irina Rish"
    ]
  },
  "https://openreview.net/forum?id=AIc48TjuSt": {
    "title": "Sparse Contextual CDF Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kamyar Azizzadenesheli",
      "William Lu",
      "Anuran Makur",
      "Qian Zhang"
    ]
  },
  "https://openreview.net/forum?id=ZAin13msOp": {
    "title": "D3: Data Diversity Design for Systematic Generalization in Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Rahimi",
      "Vanessa D'Amario",
      "Moyuru Yamada",
      "Kentaro Takemoto",
      "Tomotake Sasaki",
      "Xavier Boix"
    ]
  },
  "https://openreview.net/forum?id=bIiLXdtUVM": {
    "title": "Decoupling Pixel Flipping and Occlusion Strategy for Consistent XAI Benchmarks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Bluecher",
      "Johanna Vielhaben",
      "Nils Strodthoff"
    ]
  },
  "https://openreview.net/forum?id=YVD1QqWRaj": {
    "title": "PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruosen Li",
      "Teerth Patel",
      "Xinya Du"
    ]
  },
  "https://openreview.net/forum?id=V92PnXQ7UW": {
    "title": "BaSIS-Net: From Point Estimate to Predictive Distribution in Neural Networks - A Bayesian Sequential Importance Sampling Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giuseppina Carannante",
      "Nidhal Bouaynaya",
      "Ghulam Rasool",
      "Lyudmila Mihaylova"
    ]
  },
  "https://openreview.net/forum?id=7kWjB9zW90": {
    "title": "Object-Centric Relational Representations for Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Butera",
      "Andrea Cini",
      "Alberto Ferrante",
      "Cesare Alippi"
    ]
  },
  "https://openreview.net/forum?id=nYzws7sSzo": {
    "title": "A General-Purpose Multi-Modal OOD Detection Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Viet Quoc Duong",
      "Qiong Wu",
      "Zhengyi Zhou",
      "Eric Zavesky",
      "WenLing Hsu",
      "Han Zhao",
      "Huajie Shao"
    ]
  },
  "https://openreview.net/forum?id=hdQspgyFrk": {
    "title": "Federated TD Learning with Linear Function Approximation under Environmental Heterogeneity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Wang",
      "Aritra Mitra",
      "Hamed Hassani",
      "George J. Pappas",
      "James Anderson"
    ]
  },
  "https://openreview.net/forum?id=vqniLmUDvj": {
    "title": "ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiming Ren",
      "Huan Yang",
      "Ge Zhang",
      "Cong Wei",
      "Xinrun Du",
      "Wenhao Huang",
      "Wenhu Chen"
    ]
  },
  "https://openreview.net/forum?id=36OX7uRM5t": {
    "title": "Variational excess risk bound for general state space models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elisabeth Gassiat",
      "Sylvain Le Corff"
    ]
  },
  "https://openreview.net/forum?id=KpVJ6CGnwI": {
    "title": "$\\sigma$-PCA: a building block for neural learning of identifiable linear transformations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fahdi Kanavati",
      "Lucy Katsnith",
      "Masayuki Tsuneki"
    ]
  },
  "https://openreview.net/forum?id=RkaqxxAOfN": {
    "title": "Bytes Are All You Need: Transformers Operating Directly On File Bytes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxwell Horton",
      "Sachin Mehta",
      "Ali Farhadi",
      "Mohammad Rastegari"
    ]
  },
  "https://openreview.net/forum?id=lLVmIvZfry": {
    "title": "Improving Variational Autoencoder Estimation from Incomplete Data with Mixture Variational Families",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vaidotas Simkus",
      "Michael U. Gutmann"
    ]
  },
  "https://openreview.net/forum?id=XAD2kcBS50": {
    "title": "Conciliator steering: Imposing user preference in multi-objective reinforcement learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sara Pyykölä",
      "Klavdiya Olegovna Bochenina",
      "Laura Ruotsalainen"
    ]
  },
  "https://openreview.net/forum?id=L2jRavXRxs": {
    "title": "Can LLMs Effectively Leverage Graph Structural Information through Prompts, and Why?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin Huang",
      "Xingjian Zhang",
      "Qiaozhu Mei",
      "Jiaqi Ma"
    ]
  },
  "https://openreview.net/forum?id=SdCuffxg5A": {
    "title": "Solving Robust MDPs through No-Regret Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Etash Kumar Guha"
    ]
  },
  "https://openreview.net/forum?id=72mDxlzRZ1": {
    "title": "Fair Feature Importance Scores for Interpreting Decision Trees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Camille Olivia Little",
      "Debolina Halder Lina",
      "Genevera I. Allen"
    ]
  },
  "https://openreview.net/forum?id=nAQSUqEspb": {
    "title": "Todyformer: Towards Holistic Dynamic Graph Transformers with Structure-Aware Tokenization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahdi Biparva",
      "Raika Karimi",
      "Faezeh Faez",
      "Yingxue Zhang"
    ]
  },
  "https://openreview.net/forum?id=jESY2WTZCe": {
    "title": "The Disagreement Problem in Explainable Machine Learning: A Practitioner's Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Satyapriya Krishna",
      "Tessa Han",
      "Alex Gu",
      "Steven Wu",
      "Shahin Jabbari",
      "Himabindu Lakkaraju"
    ]
  },
  "https://openreview.net/forum?id=jDRNEoxVc7": {
    "title": "Choosing the parameter of the Fermat distance: navigating geometry and noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Frederic Chazal",
      "Laure Ferraris",
      "Pablo Groisman",
      "Matthieu Jonckheere",
      "Frederic Pascal",
      "Facundo Fabián Sapienza"
    ]
  },
  "https://openreview.net/forum?id=zF76Ga4EPs": {
    "title": "On the Unreasonable Effectiveness of Federated Averaging with Heterogeneous Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianyu Wang",
      "Rudrajit Das",
      "Gauri Joshi",
      "Satyen Kale",
      "Zheng Xu",
      "Tong Zhang"
    ]
  },
  "https://openreview.net/forum?id=ScAc73Y1oJ": {
    "title": "Revealing an Overlooked Challenge in Class-Incremental Graph Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daiqing Qi",
      "Handong Zhao",
      "Xiaowei Jia",
      "Sheng Li"
    ]
  },
  "https://openreview.net/forum?id=y3u8OpPHxz": {
    "title": "Selective Pre-training for Private Fine-tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Da Yu",
      "Sivakanth Gopi",
      "Janardhan Kulkarni",
      "Zinan Lin",
      "Saurabh Naik",
      "Tomasz Lukasz Religa",
      "Jian Yin",
      "Huishuai Zhang"
    ]
  },
  "https://openreview.net/forum?id=lmgf03HeqV": {
    "title": "Learning Tree-Structured Composition of Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyue Li",
      "Kailai Chen",
      "Predrag Radivojac",
      "Hongyang R. Zhang"
    ]
  },
  "https://openreview.net/forum?id=IZnrCGF9WI": {
    "title": "Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Fang",
      "Weijie Xu",
      "Fiona Anting Tan",
      "Ziqing Hu",
      "Jiani Zhang",
      "Yanjun Qi",
      "Srinivasan H. Sengamedu",
      "Christos Faloutsos"
    ]
  },
  "https://openreview.net/forum?id=thfoUZugvS": {
    "title": "Koopman Spectrum Nonlinear Regulators and Efficient Online Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Motoya Ohnishi",
      "Isao Ishikawa",
      "Kendall Lowrey",
      "Masahiro Ikeda",
      "Sham M. Kakade",
      "Yoshinobu Kawahara"
    ]
  },
  "https://openreview.net/forum?id=vgthYeRBAF": {
    "title": "Accurate Neural Network Pruning Requires Rethinking Sparse Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Denis Kuznedelev",
      "Eldar Kurtic",
      "Eugenia Iofinova",
      "Elias Frantar",
      "Alexandra Peste",
      "Dan Alistarh"
    ]
  },
  "https://openreview.net/forum?id=DN6sut5fyR": {
    "title": "Learning Network Granger causality using Graph Prior Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Zoroddu",
      "Pierre Humbert",
      "Laurent Oudre"
    ]
  },
  "https://openreview.net/forum?id=aIG2RAtNuX": {
    "title": "Best-of-Both-Worlds Linear Contextual Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masahiro Kato",
      "Shinji Ito"
    ]
  },
  "https://openreview.net/forum?id=RGQsUQDAd9": {
    "title": "Spike Accumulation Forwarding for Effective Training of Spiking Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryuji Saiin",
      "Tomoya Shirakawa",
      "Sota Yoshihara",
      "Yoshihide Sawada",
      "Hiroyuki Kusumoto"
    ]
  },
  "https://openreview.net/forum?id=i2SuGWtIIm": {
    "title": "Learning the essential in less than 2k additional weights - a simple approach to improve image classification stability under corruptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Bäuerle",
      "Patrick Müller",
      "Syed Muhammad Kazim",
      "Ivo Ihrke",
      "Margret Keuper"
    ]
  },
  "https://openreview.net/forum?id=PLIt3a4yTm": {
    "title": "Training-free linear image inverses via flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashwini Pokle",
      "Matthew J. Muckley",
      "Ricky T. Q. Chen",
      "Brian Karrer"
    ]
  },
  "https://openreview.net/forum?id=Yf8iHCfG4W": {
    "title": "Towards Unbiased Calibration using Meta-Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Wang",
      "Jacek Golebiowski"
    ]
  },
  "https://openreview.net/forum?id=gcf1anBL9e": {
    "title": "SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui-Jie Zhu",
      "Qihang Zhao",
      "Guoqi Li",
      "Jason Eshraghian"
    ]
  },
  "https://openreview.net/forum?id=PtNyIboDIG": {
    "title": "Cooperative Online Learning with Feedback Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolò Cesa-Bianchi",
      "Tommaso Cesari",
      "Riccardo Della Vecchia"
    ]
  },
  "https://openreview.net/forum?id=142xsInVfp": {
    "title": "On the numerical reliability of nonsmooth autodiff: a MaxPool case study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan Boustany"
    ]
  },
  "https://openreview.net/forum?id=ZeI104QZ8I": {
    "title": "Universal Neurons in GPT2 Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wes Gurnee",
      "Theo Horsley",
      "Zifan Carl Guo",
      "Tara Rezaei Kheirkhah",
      "Qinyi Sun",
      "Will Hathaway",
      "Neel Nanda",
      "Dimitris Bertsimas"
    ]
  },
  "https://openreview.net/forum?id=DLqPhQxgYu": {
    "title": "Generalized Oversampling for Learning from Imbalanced datasets and Associated Theory: Application in Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Stocksieker",
      "Denys Pommeret",
      "Arthur Charpentier"
    ]
  },
  "https://openreview.net/forum?id=abfi5plvQ4": {
    "title": "SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Yan",
      "Zhengyang Liang",
      "Yang Song",
      "Renjie Liao",
      "Lele Wang"
    ]
  },
  "https://openreview.net/forum?id=nB8foAclpo": {
    "title": "Bit-by-Bit: Investigating the Vulnerabilities of Binary Neural Networks to Adversarial Bit Flipping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shamik Kundu",
      "Sanjay Das",
      "Sayar Karmakar",
      "Arnab Raha",
      "Souvik Kundu",
      "Yiorgos Makris",
      "Kanad Basu"
    ]
  },
  "https://openreview.net/forum?id=hfrPag75Y0": {
    "title": "Directional Convergence Near Small Initializations and Saddles in Two-Homogeneous Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Kumar",
      "Jarvis Haupt"
    ]
  },
  "https://openreview.net/forum?id=lrZ2yiqOS2": {
    "title": "Towards Minimal Targeted Updates of Language Models with Targeted Negative Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lily H Zhang",
      "Rajesh Ranganath",
      "Arya Tafvizi"
    ]
  },
  "https://openreview.net/forum?id=9YqacugDER": {
    "title": "Towards Understanding Variants of Invariant Risk Minimization through the Lens of Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kotaro Yoshida",
      "Hiroki Naganuma"
    ]
  },
  "https://openreview.net/forum?id=73uyerai53": {
    "title": "TAP: The Attention Patch for Cross-Modal Knowledge Transfer from Unlabeled Modality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinsong Wang",
      "Shahin Shahrampour"
    ]
  },
  "https://openreview.net/forum?id=imAROs79Pb": {
    "title": "Mildly Constrained Evaluation Policy for Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linjie Xu",
      "zhengyao jiang",
      "Jinyu Wang",
      "Lei Song",
      "Jiang Bian"
    ]
  },
  "https://openreview.net/forum?id=e6sqttxEGX": {
    "title": "Deep End-to-end Causal Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomas Geffner",
      "Javier Antoran",
      "Adam Foster",
      "Wenbo Gong",
      "Chao Ma",
      "Emre Kiciman",
      "Amit Sharma",
      "Angus Lamb",
      "Martin Kukla",
      "Nick Pawlowski",
      "Agrin Hilmkil",
      "Joel Jennings",
      "Meyer Scetbon",
      "Miltiadis Allamanis",
      "Cheng Zhang"
    ]
  },
  "https://openreview.net/forum?id=3HE4vPNIfX": {
    "title": "A Survey on Fairness Without Demographics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrik Joslin Kenfack",
      "Samira Ebrahimi Kahou",
      "Ulrich Aïvodji"
    ]
  },
  "https://openreview.net/forum?id=LCPzaR9mML": {
    "title": "Multiple Kronecker RLS fusion-based link propagation for drug-side effect prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqing Qian",
      "Ziyu Zheng",
      "Prayag Tiwari",
      "Yijie Ding",
      "Quan Zou"
    ]
  },
  "https://openreview.net/forum?id=aHtZuZfHcf": {
    "title": "Knowledge Accumulation in Continually Learned Representations and the Issue of Feature Forgetting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Timm Hess",
      "Eli Verwimp",
      "Gido M van de Ven",
      "Tinne Tuytelaars"
    ]
  },
  "https://openreview.net/forum?id=8DWrIMuLya": {
    "title": "Estimating class separability of text embeddings with persistent homology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kostis Gourgoulias",
      "Najah Ghalyan",
      "Maxime Labonne",
      "yash satsangi",
      "Sean Moran",
      "Joseph Sabelja"
    ]
  },
  "https://openreview.net/forum?id=wC4ZID0H9a": {
    "title": "Exploring validation metrics for offline model-based optimisation with diffusion models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Beckham",
      "Alexandre Piché",
      "David Vazquez",
      "Christopher Pal"
    ]
  },
  "https://openreview.net/forum?id=nK5MazeIpn": {
    "title": "Solving the Tree Containment Problem Using Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arkadiy Dushatskiy",
      "Esther Julien",
      "Leen Stougie",
      "Leo van Iersel"
    ]
  },
  "https://openreview.net/forum?id=Sy6ZOStz5v": {
    "title": "A Simple Video Segmenter by Tracking Objects Along Axial Trajectories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ju He",
      "Qihang Yu",
      "Inkyu Shin",
      "Xueqing Deng",
      "Alan Yuille",
      "Xiaohui Shen",
      "Liang-Chieh Chen"
    ]
  },
  "https://openreview.net/forum?id=KxPjuiMgmm": {
    "title": "Targeted Active Learning for Bayesian Decision-Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Louis Filstroff",
      "Iiris Sundin",
      "Petrus Mikkola",
      "Aleksei Tiulpin",
      "Juuso Kylmäoja",
      "Samuel Kaski"
    ]
  },
  "https://openreview.net/forum?id=RA4yRhjoXw": {
    "title": "***FastDoc***: Domain-Specific Fast Continual Pre-training Technique using Document-Level Metadata and Taxonomy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abhilash Nandy",
      "Manav Nitin Kapadnis",
      "Sohan Patnaik",
      "Yash Parag Butala",
      "Pawan Goyal",
      "Niloy Ganguly"
    ]
  },
  "https://openreview.net/forum?id=BogwFMz5tU": {
    "title": "Smoothed Robustness Analysis: Bridging worst- and average-case robustness analyses via smoothed analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Rodrigues Crespo",
      "Jun-nosuke Teramae"
    ]
  },
  "https://openreview.net/forum?id=jD761b5OaE": {
    "title": "Hybrid Active Learning with Uncertainty-Weighted Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinan He",
      "Lile Cai",
      "Jingyi Liao",
      "Chuan-Sheng Foo"
    ]
  },
  "https://openreview.net/forum?id=RIFJsSzwKY": {
    "title": "Nuisances via Negativa: Adjusting for Spurious Correlations via Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aahlad Manas Puli",
      "Nitish Joshi",
      "Yoav Wald",
      "He He",
      "Rajesh Ranganath"
    ]
  },
  "https://openreview.net/forum?id=A6eqDMttcs": {
    "title": "Making Translators Privacy-aware on the User's Side",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryoma Sato"
    ]
  },
  "https://openreview.net/forum?id=6LePXHr2f3": {
    "title": "Convergences for Minimax Optimization Problems over Infinite-Dimensional Spaces Towards Stability in Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takashi Furuya",
      "Satoshi Okuda",
      "Kazuma Suetake",
      "Yoshihide Sawada"
    ]
  },
  "https://openreview.net/forum?id=JoU9khOwwr": {
    "title": "CoMIX: A Multi-agent Reinforcement Learning Training Architecture for Efficient Decentralized Coordination and Independent Decision-Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giovanni Minelli",
      "Mirco Musolesi"
    ]
  },
  "https://openreview.net/forum?id=Y7FbGcjOuD": {
    "title": "Achieving the Asymptotically Minimax Optimal Sample Complexity of Offline Reinforcement Learning: A DRO-Based Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Wang",
      "Jinjun Xiong",
      "Shaofeng Zou"
    ]
  },
  "https://openreview.net/forum?id=sHSkJqyQgW": {
    "title": "Promoting Exploration in Memory-Augmented Adam using Critical Momenta",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranshu Malviya",
      "Goncalo Mordido",
      "Aristide Baratin",
      "Reza Babanezhad Harikandeh",
      "Jerry Huang",
      "Simon Lacoste-Julien",
      "Razvan Pascanu",
      "Sarath Chandar"
    ]
  },
  "https://openreview.net/forum?id=rOvaUsF996": {
    "title": "Physics Informed Distillation for Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua Tian Jin Tee",
      "Kang Zhang",
      "Hee Suk Yoon",
      "Dhananjaya Nagaraja Gowda",
      "Chanwoo Kim",
      "Chang D. Yoo"
    ]
  },
  "https://openreview.net/forum?id=o8r84MzTQB": {
    "title": "Understanding and Improving Transfer Learning of Deep Models via Neural Collapse",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Li",
      "Sheng Liu",
      "Jinxin Zhou",
      "Xinyu Lu",
      "Carlos Fernandez-Granda",
      "Zhihui Zhu",
      "Qing Qu"
    ]
  },
  "https://openreview.net/forum?id=xDTKRLyaNN": {
    "title": "AGALE: A Graph-Aware Continual Learning Evaluation Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianqi Zhao",
      "Alan Hanjalic",
      "Megha Khosla"
    ]
  },
  "https://openreview.net/forum?id=o5kYH7bNe3": {
    "title": "VisionAD, a software package of performant anomaly detection algorithms, and Proportion Localised, an interpretable metric",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander D. J. Taylor",
      "Phillip Tregidgo",
      "Jonathan James Morrison",
      "Neill D. F. Campbell"
    ]
  },
  "https://openreview.net/forum?id=ufDh55J1ML": {
    "title": "Holistic Molecular Representation Learning via Multi-view Fragmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seojin Kim",
      "Jaehyun Nam",
      "Junsu Kim",
      "Hankook Lee",
      "Sungsoo Ahn",
      "Jinwoo Shin"
    ]
  },
  "https://openreview.net/forum?id=BTgHh0gSSc": {
    "title": "Recent Link Classification on Temporal Graphs Using Graph Profiler",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muberra Ozmen",
      "Thomas Markovich"
    ]
  },
  "https://openreview.net/forum?id=pjKcIzvXWR": {
    "title": "Hyperbolic Random Forests",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lars Doorenbos",
      "Pablo Márquez Neila",
      "Raphael Sznitman",
      "Pascal Mettes"
    ]
  },
  "https://openreview.net/forum?id=R7PReNELww": {
    "title": "Distributionally Robust Policy Evaluation under General Covariate Shift in Contextual Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihong Guo",
      "Hao Liu",
      "Yisong Yue",
      "Anqi Liu"
    ]
  },
  "https://openreview.net/forum?id=tbOYJwXhcY": {
    "title": "Misspecification-robust Sequential Neural Likelihood for Simulation-based Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan P. Kelly",
      "David J Nott",
      "David Tyler Frazier",
      "David J Warne",
      "Christopher Drovandi"
    ]
  },
  "https://openreview.net/forum?id=Nzy0XmCPuZ": {
    "title": "Rotate the ReLU to Sparsify Deep Networks Implicitly",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nancy Nayak",
      "Sheetal Kalyani"
    ]
  },
  "https://openreview.net/forum?id=Ft4kHrOawZ": {
    "title": "Bayesian Quantification with Black-Box Estimators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Albert Ziegler",
      "Paweł Czyż"
    ]
  },
  "https://openreview.net/forum?id=IKH5ziX9dk": {
    "title": "Simple Imputation Rules for Prediction with Missing Data: Theoretical Guarantees vs. Empirical Performance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dimitris Bertsimas",
      "Arthur Delarue",
      "Jean Pauphilet"
    ]
  },
  "https://openreview.net/forum?id=Z20FInfWlm": {
    "title": "DIGNet: Learning Decomposed Patterns in Representation Balancing for Treatment Effect Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyan HUANG",
      "WANG Siyi",
      "Cheuk Hang LEUNG",
      "Qi WU",
      "Dongdong WANG",
      "Zhixiang Huang"
    ]
  },
  "https://openreview.net/forum?id=10WARaIwFn": {
    "title": "Mildly Overparameterized ReLU Networks Have a Favorable Loss Landscape",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kedar Karhadkar",
      "Michael Murray",
      "Hanna Tseran",
      "Guido Montufar"
    ]
  },
  "https://openreview.net/forum?id=z8d7nT1HWw": {
    "title": "Augmenting Ad-Hoc IR Dataset for Interactive Conversational Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre ERBACHER",
      "Jian-Yun Nie",
      "Philippe Preux",
      "Laure Soulier"
    ]
  },
  "https://openreview.net/forum?id=spo705Fyv0": {
    "title": "Text Descriptions are Compressive and Invariant Representations for Visual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhili Feng",
      "Anna Bair",
      "J Zico Kolter"
    ]
  },
  "https://openreview.net/forum?id=i5yKW1pmjW": {
    "title": "Semi-Supervised Semantic Segmentation via Marginal Contextual Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moshe Kimhi",
      "Shai Kimhi",
      "Evgenii Zheltonozhskii",
      "Or Litany",
      "Chaim Baskin"
    ]
  },
  "https://openreview.net/forum?id=y1pPWFVfvR": {
    "title": "Multimodal Chain-of-Thought Reasoning in Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuosheng Zhang",
      "Aston Zhang",
      "Mu Li",
      "hai zhao",
      "George Karypis",
      "Alex Smola"
    ]
  },
  "https://openreview.net/forum?id=9fcZNAmnyh": {
    "title": "Contrastive Graph Autoencoder for Shape-based Polygon Retrieval from Large Geometry Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zexian Huang",
      "Kourosh Khoshelham",
      "Martin Tomko"
    ]
  },
  "https://openreview.net/forum?id=HU5DOUp6Sa": {
    "title": "Prototypical Self-Explainable Models Without Re-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Srishti Gautam",
      "Ahcene Boubekki",
      "Marina MC Höhne",
      "Michael Kampffmeyer"
    ]
  },
  "https://openreview.net/forum?id=QPuxjsjKCP": {
    "title": "Conservative Prediction via Data-Driven Confidence Minimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caroline Choi",
      "Fahim Tajwar",
      "Yoonho Lee",
      "Huaxiu Yao",
      "Ananya Kumar",
      "Chelsea Finn"
    ]
  },
  "https://openreview.net/forum?id=xYkdmEGhIM": {
    "title": "Physical Reasoning and Object Planning for Household Embodied Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ayush Agrawal",
      "Raghav Prabhakar",
      "Anirudh Goyal",
      "Dianbo Liu"
    ]
  },
  "https://openreview.net/forum?id=FpaCL1MO2C": {
    "title": "Robust Distortion-free Watermarks for Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohith Kuditipudi",
      "John Thickstun",
      "Tatsunori Hashimoto",
      "Percy Liang"
    ]
  },
  "https://openreview.net/forum?id=uSLNzzuiDJ": {
    "title": "Enhancing Low-Precision Sampling via Stochastic Gradient Hamiltonian Monte Carlo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Wang",
      "Yujie Chen",
      "Qifan Song",
      "Ruqi Zhang"
    ]
  },
  "https://openreview.net/forum?id=0zKvH7YiAq": {
    "title": "Improved Convergence of Score-Based Diffusion Models via Prediction-Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Pedrotti",
      "Jan Maas",
      "Marco Mondelli"
    ]
  },
  "https://openreview.net/forum?id=1iDpP3GWmS": {
    "title": "Online Tensor Max-Norm Regularization via Stochastic Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Wu"
    ]
  },
  "https://openreview.net/forum?id=T6RygOFZ6B": {
    "title": "A Study of the Effects of Transfer Learning on Adversarial Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pratik Vaishnavi",
      "Kevin Eykholt",
      "Amir Rahmati"
    ]
  },
  "https://openreview.net/forum?id=O3wmRh2SfT": {
    "title": "End-to-End Training Induces Information Bottleneck through Layer-Role Differentiation: A Comparative Analysis with Layer-wise Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keitaro Sakamoto",
      "Issei Sato"
    ]
  },
  "https://openreview.net/forum?id=Nm0WX86sKv": {
    "title": "Where Did the Gap Go? Reassessing the Long-Range Graph Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Tönshoff",
      "Martin Ritzert",
      "Eran Rosenbluth",
      "Martin Grohe"
    ]
  },
  "https://openreview.net/forum?id=vtiDUgGjyx": {
    "title": "What Does Softmax Probability Tell Us about Classifiers Ranking Across Diverse Test Conditions?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijie Tu",
      "Weijian Deng",
      "Liang Zheng",
      "Tom Gedeon"
    ]
  },
  "https://openreview.net/forum?id=yXnwrs2Tl6": {
    "title": "Certified Deductive Reasoning with Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriel Poesia",
      "Kanishk Gandhi",
      "Eric Zelikman",
      "Noah Goodman"
    ]
  },
  "https://openreview.net/forum?id=UuU6C6CUoF": {
    "title": "On the Inherent Privacy Properties of Discrete Denoising Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rongzhe Wei",
      "Eleonora Kreacic",
      "Haoyu Peter Wang",
      "Haoteng Yin",
      "Eli Chien",
      "Vamsi K. Potluru",
      "Pan Li"
    ]
  },
  "https://openreview.net/forum?id=MyQKcQAte6": {
    "title": "Online Continual Learning via Logit Adjusted Softmax",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhehao Huang",
      "Tao Li",
      "Chenhe Yuan",
      "Yingwen Wu",
      "Xiaolin Huang"
    ]
  },
  "https://openreview.net/forum?id=68LsWm2GuD": {
    "title": "Single Image Test-Time Adaptation for Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Klara Janouskova",
      "Tamir Shor",
      "Chaim Baskin",
      "Jiri Matas"
    ]
  },
  "https://openreview.net/forum?id=4E2XLydJiv": {
    "title": "Normed Spaces for Graph Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diaaeldin Taha",
      "Wei Zhao",
      "J. Maxwell Riestenberg",
      "Michael Strube"
    ]
  },
  "https://openreview.net/forum?id=ntWCJrlDD8": {
    "title": "Uncovering Sets of Maximum Dissimilarity on Random Process Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miguel de Carvalho",
      "Gabriel Martos"
    ]
  },
  "https://openreview.net/forum?id=9XRZtZRmEB": {
    "title": "Internal-Coordinate Density Modelling of Protein Structure: Covariance Matters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marloes Arts",
      "Jes Frellsen",
      "Wouter Boomsma"
    ]
  },
  "https://openreview.net/forum?id=jjmdiMiag7": {
    "title": "CLIP-QDA: An Explainable Concept Bottleneck Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rémi Kazmierczak",
      "Eloïse Berthier",
      "Goran Frehse",
      "Gianni Franchi"
    ]
  },
  "https://openreview.net/forum?id=jv1aPQINc4": {
    "title": "Independence Testing for Temporal Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cencheng Shen",
      "Jaewon Chung",
      "Ronak Mehta",
      "Ting Xu",
      "Joshua T Vogelstein"
    ]
  },
  "https://openreview.net/forum?id=IX5GX8SNtM": {
    "title": "Unleashing the Power of Visual Prompting At the Pixel Level",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyang Wu",
      "Xianhang Li",
      "Chen Wei",
      "Huiyu Wang",
      "Alan Yuille",
      "Yuyin Zhou",
      "Cihang Xie"
    ]
  },
  "https://openreview.net/forum?id=xl1KhKT3Xx": {
    "title": "RedMotion: Motion Prediction via Redundancy Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Royden Wagner",
      "Omer Sahin Tas",
      "Marvin Klemp",
      "Carlos Fernandez",
      "Christoph Stiller"
    ]
  },
  "https://openreview.net/forum?id=Yh8Y7a4myU": {
    "title": "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammadreza Salehi",
      "Mehrdad Farajtabar",
      "Maxwell Horton",
      "Fartash Faghri",
      "Hadi Pouransari",
      "Raviteja Vemulapalli",
      "Oncel Tuzel",
      "Ali Farhadi",
      "Mohammad Rastegari",
      "Sachin Mehta"
    ]
  },
  "https://openreview.net/forum?id=BQE4MTAfCE": {
    "title": "Amortized Bayesian Decision Making for simulation-based models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mila Gorecki",
      "Jakob H. Macke",
      "Michael Deistler"
    ]
  },
  "https://openreview.net/forum?id=mK6TwmInTg": {
    "title": "Appropriate Balance of Diversification and Intensification Improves Performance and Efficiency of Adversarial Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keiichiro Yamamura",
      "Issa Oe",
      "Nozomi Hata",
      "Hiroki Ishikura",
      "Katsuki Fujisawa"
    ]
  },
  "https://openreview.net/forum?id=iV0jktFZ5Y": {
    "title": "Improve Certified Training with Signal-to-Noise Ratio Loss to Decrease Neuron Variance and Increase Neuron Stability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhao Wei",
      "Ziwei Wang",
      "Peizhi Niu",
      "ABULIKEMU ABUDUWEILI",
      "Weiye Zhao",
      "Casidhe Hutchison",
      "Eric Sample",
      "Changliu Liu"
    ]
  },
  "https://openreview.net/forum?id=DWkJCSxKU5": {
    "title": "Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Lin",
      "Shubhendu Trivedi",
      "Jimeng Sun"
    ]
  },
  "https://openreview.net/forum?id=JkFEVbW6wE": {
    "title": "Enhancing Vision-Language Model with Unmasked Token Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihao Liu",
      "Jinliang Zheng",
      "Boxiao Liu",
      "Yu Liu",
      "Hongsheng Li"
    ]
  },
  "https://openreview.net/forum?id=KNAWoKKpi3": {
    "title": "MaskOCR: Scene Text Recognition with Masked Vision-Language Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengyuan Lyu",
      "Chengquan Zhang",
      "Shanshan Liu",
      "Meina Qiao",
      "Yangliu Xu",
      "Liang Wu",
      "Kun Yao",
      "Junyu Han",
      "Errui Ding",
      "Jingdong Wang"
    ]
  },
  "https://openreview.net/forum?id=84M8xwNxrc": {
    "title": "Differentially Private Kernel Inducing Points using features from ScatterNets (DP-KIP-ScatterNet) for Privacy Preserving Data Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Margarita Vinaroz",
      "Mijung Park"
    ]
  },
  "https://openreview.net/forum?id=lLE0mWzUrr": {
    "title": "Large Language Models can be Guided to Evade AI-generated Text Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ning Lu",
      "Shengcai Liu",
      "Rui He",
      "Yew-Soon Ong",
      "Qi Wang",
      "Ke Tang"
    ]
  },
  "https://openreview.net/forum?id=iKPC7N85Pf": {
    "title": "Predicting the Encoding Error of SIRENs",
    "volume": "main",
    "abstract": "Implicit Neural Representations (INRs), which encode signals such as images, videos, and 3D shapes in the weights of neural networks, are becoming increasingly popular. Among their many applications is signal compression, for which there is great interest in achieving the highest possible fidelity to the original signal subject to constraints such as neural network size, training (encoding) and inference (decoding) time. But training INRs can be a computationally expensive process, making it challenging to determine the best possible tradeoff under such constraints. Towards this goal, we propose a novel problem: predicting the encoding error (i.e. training loss) that an INR will reach on a given training signal. We present a method which predicts the encoding error that a popular INR network (SIREN) will reach, given its network hyperparameters and the signal to encode. This method is trained on a unique dataset of 300,000 SIRENs, trained across a variety of images and hyperparameters. Our predictive method demonstrates the feasibility of this regression problem, and allows users to anticipate the encoding error that a SIREN network will reach in milliseconds instead of minutes or longer. We also provide insights into the behavior of SIREN networks, such as why narrow SIRENs can have very high random variation in encoding error, and how the performance of SIRENs relates to JPEG compression",
    "checked": false,
    "id": "204265661c85df9a778219dc6f38321feb5fe64c",
    "semantic_title": "geom-deeponet: a point-cloud-based deep operator network for field predictions on 3d parameterized geometries",
    "citation_count": 0,
    "authors": [
      "Jeremy Vonderfecht",
      "Feng Liu"
    ]
  },
  "https://openreview.net/forum?id=ydPHjgf6h0": {
    "title": "Adversarial Imitation Learning from Visual Observations using Latent Information",
    "volume": "main",
    "abstract": "We focus on the problem of imitation learning from visual observations, where the learning agent has access to videos of experts as its sole learning source. The challenges of this framework include the absence of expert actions and the partial observability of the environment, as the ground-truth states can only be inferred from pixels. To tackle this problem, we first conduct a theoretical analysis of imitation learning in partially observable environments. We establish upper bounds on the suboptimality of the learning agent with respect to the divergence between the expert and the agent latent state-transition distributions. Motivated by this analysis, we introduce an algorithm called Latent Adversarial Imitation from Observations, which combines off-policy adversarial imitation techniques with a learned latent representation of the agent's state from sequences of observations. In experiments on high-dimensional continuous robotic tasks, we show that our model-free approach in latent space matches state-of-the-art performance. Additionally, we show how our method can be used to improve the efficiency of reinforcement learning from pixels by leveraging expert videos. To ensure reproducibility, we provide free access to all the learning curves and open-source our code",
    "checked": true,
    "id": "ca8ffc2615cc76bbb15ceb30076df134ae5cdc25",
    "semantic_title": "adversarial imitation learning from visual observations using latent information",
    "citation_count": 1,
    "authors": [
      "Vittorio Giammarino",
      "James Queeney",
      "Ioannis Paschalidis"
    ]
  },
  "https://openreview.net/forum?id=PUpZXvNqmb": {
    "title": "From Identifiable Causal Representations to Controllable Counterfactual Generation: A Survey on Causal Generative Modeling",
    "volume": "main",
    "abstract": "Deep generative models have shown tremendous capability in data density estimation and data generation from finite samples. While these models have shown impressive performance by learning correlations among features in the data, some fundamental shortcomings are their lack of explainability, tendency to induce spurious correlations, and poor out-of-distribution extrapolation. To remedy such challenges, recent work has proposed a shift toward causal generative models. Causal models offer several beneficial properties to deep generative models, such as distribution shift robustness, fairness, and interpretability. Structural causal models (SCMs) describe data-generating processes and model complex causal relationships and mechanisms among variables in a system. Thus, SCMs can naturally be combined with deep generative models. We provide a technical survey on causal generative modeling categorized into causal representation learning and controllable counterfactual generation methods. We focus on fundamental theory, methodology, drawbacks, datasets, and metrics. Then, we cover applications of causal generative models in fairness, privacy, out-of-distribution generalization, precision medicine, and biological sciences. Lastly, we discuss open problems and fruitful research directions for future work in the field",
    "checked": true,
    "id": "1d3ab3b346bd035d50590af912a7c21b16ffdb1c",
    "semantic_title": "from identifiable causal representations to controllable counterfactual generation: a survey on causal generative modeling",
    "citation_count": 3,
    "authors": [
      "Aneesh Komanduri",
      "Xintao Wu",
      "Yongkai Wu",
      "Feng Chen"
    ]
  },
  "https://openreview.net/forum?id=9UgUMFW67X": {
    "title": "Genetic InfoMax: Exploring Mutual Information Maximization in High-Dimensional Imaging Genetics Studies",
    "volume": "main",
    "abstract": "Genome-wide association studies (GWAS) are used to identify relationships between genetic variations and specific traits. When applied to high-dimensional medical imaging data, a key step is to extract lower-dimensional, yet informative representations of the data as traits. Representation learning for imaging genetics is largely under-explored due to the unique challenges posed by GWAS in comparison to typical visual representation learning. In this study, we tackle this problem from the mutual information (MI) perspective by identifying key limitations of existing methods. We introduce a trans-modal learning framework Genetic InfoMax (GIM), including a regularized MI estimator and a novel genetics-informed transformer to address the specific challenges of GWAS. We evaluate GIM on human brain 3D MRI data and establish standardized evaluation protocols to compare it to existing approaches. Our results demonstrate the effectiveness of GIM and a significantly improved performance on GWAS",
    "checked": true,
    "id": "353910469227c9251ae604f3c2085fc82054fb02",
    "semantic_title": "genetic infomax: exploring mutual information maximization in high-dimensional imaging genetics studies",
    "citation_count": 0,
    "authors": [
      "Yaochen Xie",
      "Ziqian Xie",
      "Sheikh Muhammad Saiful Islam",
      "Degui Zhi",
      "Shuiwang Ji"
    ]
  },
  "https://openreview.net/forum?id=k3d5C0YvfK": {
    "title": "Enhancing Compositional Generalization via Compositional Feature Alignment",
    "volume": "main",
    "abstract": "Real-world applications of machine learning models often confront data distribution shifts, wherein discrepancies exist between the training and test data distributions. In the common multi-domain multi-class setup, as the number of classes and domains scales up, it becomes infeasible to gather training data for every domain-class combination. This challenge naturally leads the quest for models with Compositional Generalization (CG) ability, where models can generalize to unseen domain-class combinations. To delve into the CG challenge, we develop CG-Bench, a suite of CG benchmarks derived from existing real-world image datasets, and observe that the prevalent pretraining-finetuning paradigm on foundational models, such as CLIP and DINOv2, struggles with the challenge. To address this challenge, we propose Compositional Feature Alignment (CFA), a simple two-stage finetuning technique that i) learns two orthogonal linear heads on a pretrained encoder with respect to class and domain labels, and ii) fine-tunes the encoder with the newly learned head frozen. We theoretically and empirically justify that CFA encourages compositional feature learning of pretrained models. We further conduct extensive experiments on CG-Bench for CLIP and DINOv2, two powerful pretrained vision foundation models. Experiment results show that CFA outperforms common finetuning techniques in compositional generalization, corroborating CFA's efficacy in compositional feature learning",
    "checked": true,
    "id": "5c8f4270dc18433425eebacd1b4e41a107556abf",
    "semantic_title": "enhancing compositional generalization via compositional feature alignment",
    "citation_count": 0,
    "authors": [
      "Haoxiang Wang",
      "Haozhe Si",
      "Huajie Shao",
      "Han Zhao"
    ]
  },
  "https://openreview.net/forum?id=PIL3YWXmx2": {
    "title": "Statistical and Computational Complexities of BFGS Quasi-Newton Method for Generalized Linear Models",
    "volume": "main",
    "abstract": "The gradient descent (GD) method has been used widely to solve parameter estimation in generalized linear models (GLMs), a generalization of linear models when the link function can be non-linear. In GLMs with a polynomial link function, it has been shown that in the high signal-to-noise ratio (SNR) regime, due to the problem's strong convexity and smoothness, GD converges linearly and reaches the final desired accuracy in a logarithmic number of iterations. In contrast, in the low SNR setting, where the problem becomes locally convex, GD converges at a slower rate and requires a polynomial number of iterations to reach the desired accuracy. Even though Newton's method can be used to resolve the flat curvature of the loss functions in the low SNR case, its computational cost is prohibitive in high-dimensional settings as it is $\\mathcal{O}(d^3)$, where $d$ the is the problem dimension. To address the shortcomings of GD and Newton's method, we propose the use of the BFGS quasi-Newton method to solve parameter estimation of the GLMs, which has a per iteration cost of $\\mathcal{O}(d^2)$. When the SNR is low, for GLMs with a polynomial link function of degree $p$, we demonstrate that the iterates of BFGS converge linearly to the optimal solution of the population least-square loss function, and the contraction coefficient of the BFGS algorithm is comparable to that of Newton's method. Moreover, the contraction factor of the linear rate is independent of problem parameters and only depends on the degree of the link function $p$. Also, for the empirical loss with $n$ samples, we prove that in the low SNR setting of GLMs with a polynomial link function of degree $p$, the iterates of BFGS reach a final statistical radius of $\\mathcal{O}((d/n)^{\\frac{1}{2p+2}})$ after at most $\\log(n/d)$ iterations. This complexity is significantly less than the number required for GD, which scales polynomially with $(n/d)$",
    "checked": true,
    "id": "166aee5a9b877d143b8eb005054a045be052c707",
    "semantic_title": "statistical and computational complexities of bfgs quasi-newton method for generalized linear models",
    "citation_count": 2,
    "authors": [
      "Qiujiang Jin",
      "Tongzheng Ren",
      "Nhat Ho",
      "Aryan Mokhtari"
    ]
  },
  "https://openreview.net/forum?id=C3FXHxMVuq": {
    "title": "CascadedGaze: Efficiency in Global Context Extraction for Image Restoration",
    "volume": "main",
    "abstract": "Image restoration tasks traditionally rely on convolutional neural networks. However, given the local nature of the convolutional operator, they struggle to capture global information. The promise of attention mechanisms in Transformers is to circumvent this problem, but it comes at the cost of intensive computational overhead. Many recent studies in image restoration have focused on solving the challenge of balancing performance and computational cost via Transformer variants. In this paper, we present CascadedGaze Network (CGNet), an encoder-decoder architecture that employs Global Context Extractor (GCE), a novel and efficient way to capture global information for image restoration. The GCE module leverages small kernels across convolutional layers to learn global dependencies, without requiring self-attention. Extensive experimental results show that our approach outperforms a range of state-of-the-art methods on denoising benchmark datasets including both real image denoising and synthetic image denoising, as well as on image deblurring task, while being more computationally efficient",
    "checked": true,
    "id": "168408d0eb1dfc9241411ebb0283d87d9e5595cb",
    "semantic_title": "cascadedgaze: efficiency in global context extraction for image restoration",
    "citation_count": 0,
    "authors": [
      "Amirhosein Ghasemabadi",
      "Muhammad Kamran Janjua",
      "Mohammad Salameh",
      "CHUNHUA ZHOU",
      "Fengyu Sun",
      "Di Niu"
    ]
  },
  "https://openreview.net/forum?id=57ETChLAOE": {
    "title": "Revisiting stochastic submodular maximization with cardinality constraint: A bandit perspective",
    "volume": "main",
    "abstract": "In this paper, we focus on the problem of maximizing non-negative, monotone, stochastic submodular functions under cardinality constraint. Recent works have explored continuous optimization algorithms via multi-linear extensions for such problems and provided appropriate approximation guarantees. We take a fresh look into this problem from a discrete, (stochastic) greedy perspective under a probably approximately correct (PAC) setting, i.e., the goal is to obtain solutions whose expected objective value is greater than or equal to $(1-1/e-\\epsilon){\\rm OPT}-\\nu$ with at least $1-\\delta$ probability, where ${\\rm OPT}$ is the optimal objective value. Using the theory of multi-armed bandits, we propose novel bandit stochastic greedy (BSG) algorithms in which selection of the next element at iteration $i$ is posed as a $(\\nu_i,\\delta_i)$-PAC best-arm identification problem. Given $(\\nu,\\delta)$-PAC parameters to BSG, we formally characterize a set $\\mathcal{A}(\\nu,\\delta)$ of per-iteration policies such that any policy from this set guarantees a $(\\nu,\\delta)$-PAC solution for the stochastic submodular maximization problem using BSG. We next discuss the problem of learning a policy in $\\mathcal{A}(\\nu,\\delta)$ by minimizing the computational cost. With our learned policy, we show that BSG has lower computational cost than existing stochastic submodular maximization approaches. An interesting outcome of our analysis is the development of both linear and almost-linear time algorithms for the exemplar based clustering problem with $(1-1/e-\\epsilon)$-approximation guarantee under a PAC setting. Lastly, we also study the problem of learning a policy for BSG under budget setting. Experiments on various problems illustrate the efficacy of our approach in terms of optimization quality as well as computational efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pratik Jawanpuria",
      "Bamdev Mishra",
      "Karthik S. Gurumoorthy"
    ]
  },
  "https://openreview.net/forum?id=EE1CBKC0SZ": {
    "title": "TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks",
    "volume": "main",
    "abstract": "We present TIGERScore, a \\textbf{T}rained metric that follows \\textbf{I}nstruction \\textbf{G}uidance to perform \\textbf{E}xplainable, and \\textbf{R}eference-free evaluation over a wide spectrum of text generation tasks. Different from other automatic evaluation methods that only provide arcane scores, TIGERScore is guided by natural language instruction to provide error analysis to pinpoint the mistakes in the generated text. Our metric is based on LLaMA-2, trained on our meticulously curated instruction-tuning dataset MetricInstruct which covers 6 text generation tasks and 23 text generation datasets. The dataset consists of 42K quadruple in the form of (instruction, input, system output $\\rightarrow$ error analysis). We collected the `system outputs' through from a large variety of models to cover different types of errors. To quantitatively assess our metric, we evaluate its correlation with human ratings on 5 held-in datasets, 2 held-out datasets and show that \\metricname{} can achieve the open-source SoTA correlation with human ratings across these datasets and almost approaches GPT-4 evaluator. As a reference-free metric, its correlation can even surpass the best existing reference-based metrics. To further qualitatively assess the rationale generated by our metric, we conduct human evaluation on the generated explanations and found that the explanations are 70.8\\% accurate. Through these experimental results, we believe \\metricname{} demonstrates the possibility of building universal explainable metrics to evaluate any text generation task",
    "checked": true,
    "id": "d238a9770d24d0725656ef6cf4789afebf2126e7",
    "semantic_title": "tigerscore: towards building explainable metric for all text generation tasks",
    "citation_count": 18,
    "authors": [
      "Dongfu Jiang",
      "Yishan Li",
      "Ge Zhang",
      "Wenhao Huang",
      "Bill Yuchen Lin",
      "Wenhu Chen"
    ]
  },
  "https://openreview.net/forum?id=098mb06uhA": {
    "title": "Extreme Risk Mitigation in Reinforcement Learning using Extreme Value Theory",
    "volume": "main",
    "abstract": "Risk-sensitive reinforcement learning (RL) has garnered significant attention in recent years due to the growing interest in deploying RL agents in real-world scenarios. A critical aspect of risk awareness involves modelling highly rare risk events (rewards) that could potentially lead to catastrophic outcomes. These infrequent occurrences present a formidable challenge for data-driven methods aiming to capture such risky events accurately. While risk-aware RL techniques do exist, they suffer from high variance estimation due to the inherent data scarcity. Our work proposes to enhance the resilience of RL agents when faced with very rare and risky events by focusing on refining the predictions of the extreme values predicted by the state-action value distribution. To achieve this, we formulate the extreme values of the state-action value function distribution as parameterized distributions, drawing inspiration from the principles of extreme value theory (EVT). We propose an extreme value theory based actor-critic approach, namely, Extreme Valued Actor-Critic (EVAC) which effectively addresses the issue of infrequent occurrence by leveraging EVT-based parameterization. Importantly, we theoretically demonstrate the advantages of employing these parameterized distributions in contrast to other risk-averse algorithms. Our evaluations show that the proposed method outperforms other risk averse RL algorithms on a diverse range of benchmark tasks, each encompassing distinct risk scenarios",
    "checked": true,
    "id": "dfdc9861a2ff4c9b49a5bfaeab5b5863542eaa49",
    "semantic_title": "extreme risk mitigation in reinforcement learning using extreme value theory",
    "citation_count": 0,
    "authors": [
      "Karthik Somayaji NS",
      "Yu Wang",
      "Malachi Schram",
      "Jan Drgona",
      "Mahantesh M Halappanavar",
      "Frank Liu",
      "Peng Li"
    ]
  },
  "https://openreview.net/forum?id=TdJ7lpzAkD": {
    "title": "Interpretable Additive Tabular Transformer Networks",
    "volume": "main",
    "abstract": "Attention based Transformer networks have not only revolutionized Natural Language Processing but have also achieved state-of-the-art results for tabular data modeling. The attention mechanism, in particular, has proven to be highly effective in accurately modeling categorical variables. Although deep learning models recently outperform tree-based models, they often lack a complete comprehension of the individual impact of features because of their opaque nature. In contrast, additive neural network structures have proven to be both predictive and interpretable. Within the context of explainable deep learning, we propose Neural Additive Tabular Transformer Networks (NATT), a modeling framework that combines the intelligibility of additive neural networks with the predictive power of Transformer models. NATT offers inherent intelligibility while achieving similar performance to complex deep learning models. To validate its efficacy, we conduct experiments on multiple datasets and find that NATT performs on par with state-of-the-art methods on tabular data and surpasses other interpretable approaches",
    "checked": false,
    "id": "23b9f7569077db7cba68d191bbbf0825fc137481",
    "semantic_title": "interpretable machine learning for tabpfn",
    "citation_count": 1,
    "authors": [
      "Anton Frederik Thielmann",
      "Arik Reuter",
      "Thomas Kneib",
      "David Rügamer",
      "Benjamin Säfken"
    ]
  },
  "https://openreview.net/forum?id=zO4aAVHxPe": {
    "title": "Geometrical aspects of lattice gauge equivariant convolutional neural networks",
    "volume": "main",
    "abstract": "Lattice gauge equivariant convolutional neural networks (L-CNNs) are a framework for convolutional neural networks that can be applied to non-Abelian lattice gauge theories without violating gauge symmetry. We demonstrate how L-CNNs can be equipped with global group equivariance. This allows us to extend the formulation to be equivariant not just under translations but under global lattice symmetries such as rotations and reflections. Additionally, we provide a geometric formulation of L-CNNs and show how convolutions in L-CNNs arise as a special case of gauge equivariant neural networks on SU(N) principal bundles",
    "checked": true,
    "id": "0182110de6f2471cfe57b109b7cfa59675598360",
    "semantic_title": "geometrical aspects of lattice gauge equivariant convolutional neural networks",
    "citation_count": 5,
    "authors": [
      "David I. Müller",
      "Jimmy Aronsson",
      "Daniel Schuh"
    ]
  },
  "https://openreview.net/forum?id=r2dx1s1lqG": {
    "title": "Boosting Data-Driven Mirror Descent with Randomization, Equivariance, and Acceleration",
    "volume": "main",
    "abstract": "Learning-to-optimize (L2O) is an emerging research area in large-scale optimization with applications in data science. Recently, researchers have proposed a novel L2O framework called learned mirror descent (LMD), based on the classical mirror descent (MD) algorithm with learnable mirror maps parameterized by input-convex neural networks. The LMD approach has been shown to significantly accelerate convex solvers while inheriting the convergence properties of the classical MD algorithm. This work proposes several practical extensions of the LMD algorithm, addressing its instability, scalability, and feasibility for high-dimensional problems. We first propose accelerated and stochastic variants of LMD, leveraging classical momentum-based acceleration and stochastic optimization techniques for improving the convergence rate and per-iteration computational complexity. Moreover, for the particular application of training neural networks, we derive and propose a novel and efficient parameterization for the mirror potential, exploiting the equivariant structure of the training problems to significantly reduce the dimensionality of the underlying problem. We provide theoretical convergence guarantees for our schemes under standard assumptions and demonstrate their effectiveness in various computational imaging and machine learning applications such as image inpainting, and the training of support vector machines and deep neural networks",
    "checked": true,
    "id": "50313f151af5c64775023ab95f64359ffacc2cd8",
    "semantic_title": "boosting data-driven mirror descent with randomization, equivariance, and acceleration",
    "citation_count": 1,
    "authors": [
      "Hong Ye Tan",
      "Subhadip Mukherjee",
      "Junqi Tang",
      "Carola-Bibiane Schönlieb"
    ]
  },
  "https://openreview.net/forum?id=PGLbZpVk2n": {
    "title": "Causal Discovery from Time Series with Hybrids of Constraint-Based and Noise-Based Algorithms",
    "volume": "main",
    "abstract": "Constraint-based methods and noise-based methods are two distinct families of methods proposed for uncovering causal graphs from observational data. However, both operate under strong assumptions that may be challenging to validate or could be violated in real-world scenarios. In response to these challenges, there is a growing interest in hybrid methods that amalgamate principles from both methods, showing robustness to assumption violations. This paper introduces a novel comprehensive framework for hybridizing constraint-based and noise-based methods designed to uncover causal graphs from observational time series. The framework is structured into two classes. The first class employs a noise-based strategy to identify a super graph, containing the true graph, followed by a constraint-based strategy to eliminate unnecessary edges. In the second class, a constraint-based strategy is applied to identify a skeleton, which is then oriented using a noise-based strategy. The paper provides theoretical guarantees for each class under the condition that all assumptions are satisfied, and it outlines some properties when assumptions are violated. To validate the efficacy of the framework, two algorithms from each class are experimentally tested on simulated data, realistic ecological data, and real datasets sourced from diverse applications. Notably, two novel datasets related to Information Technology monitoring are introduced within the set of considered real datasets. The experimental results underscore the robustness and effectiveness of the hybrid approaches across a broad spectrum of datasets",
    "checked": true,
    "id": "0c35c5a7b1425ebcdca5f359376b59272971815c",
    "semantic_title": "causal discovery from time series with hybrids of constraint-based and noise-based algorithms",
    "citation_count": 2,
    "authors": [
      "Daria Bystrova",
      "Charles K. Assaad",
      "Julyan Arbel",
      "Emilie Devijver",
      "Eric Gaussier",
      "Wilfried Thuiller"
    ]
  },
  "https://openreview.net/forum?id=LHl2I2rWZa": {
    "title": "GLASU: A Communication-Efficient Algorithm for Federated Learning with Vertically Distributed Graph Data",
    "volume": "main",
    "abstract": "Vertical federated learning (VFL) is a distributed learning paradigm, where computing clients collectively train a model based on the partial features of the same set of samples they possess. Current research on VFL focuses on the case when samples are independent, but it rarely addresses an emerging scenario when samples are interrelated through a graph. In this work, we train a graph neural network (GNN) through VFL, where each client owns a part of the node features and a different edge set. This data scenario incurs a significant communication overhead, not only because of the handling of distributed features but also due to neighborhood aggregation in a GNN. Moreover, the training analysis is faced with a challenge caused by the biased stochastic gradients. We propose a model-splitting method that splits a backbone GNN across the clients and the server and a communication-efficient algorithm, GLASU, to train such a model. GLASU adopts lazy aggregation and stale updates to skip communication in neighborhood aggregation and in model updates, respectively, greatly reducing communication while enjoying convergence guarantees. We conduct extensive numerical experiments on real-world datasets, showing that GLASU effectively trains a GNN that matches the accuracy of centralized training, while using only a fraction of the time due to communication saving",
    "checked": true,
    "id": "b400baf6862a918355934a9ea98bf1e0c431b789",
    "semantic_title": "glasu: a communication-efficient algorithm for federated learning with vertically distributed graph data",
    "citation_count": 0,
    "authors": [
      "Xinwei Zhang",
      "Mingyi Hong",
      "Jie Chen"
    ]
  },
  "https://openreview.net/forum?id=KLojVqdj2y": {
    "title": "Training Graph Neural Networks Subject to a Tight Lipschitz Constraint",
    "volume": "main",
    "abstract": "We propose a strategy for training a wide range of graph neural networks (GNNs) under tight Lipschitz bound constraints. Specifically, by leveraging graph spectral theory, we derive computationally tractable expressions of a tight Lipschitz constant. This allows us to propose a constrained-optimization approach to control the constant, ensuring robustness to adversarial perturbations. Unlike the existing methods for controlling the Lipschitz constant, our approach reduces the size of the handled matrices by a factor equal to the square of the number of nodes in the graph. We employ a stochastic projected subgradient algorithm, which operates in a block-coordinate manner, with the projection step performed via an accelerated iterative proximal algorithm. We focus on defending against attacks that perturb features while keeping the topology of the graph constant. This contrasts with most of the existing defenses, which tackle perturbations of the graph structure. We report experiments on various datasets in the context of node classification tasks, showing the effectiveness of our constrained GNN model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simona Ioana Juvina",
      "Ana Antonia Neacșu",
      "Jérôme Rony",
      "Jean-Christophe Pesquet",
      "Corneliu Burileanu",
      "Ismail Ben Ayed"
    ]
  },
  "https://openreview.net/forum?id=zOKAmm8R9B": {
    "title": "GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models",
    "volume": "main",
    "abstract": "Offline Goal-Conditioned RL (GCRL) offers a feasible paradigm for learning general-purpose policies from diverse and multi-task offline datasets. Despite notable recent progress, the predominant offline GCRL methods, mainly model-free, face constraints in handling limited data and generalizing to unseen goals. In this work, we propose Goal-conditioned Offline Planning (GOPlan), a novel model-based framework that contains two key phases: (1) pretraining a prior policy capable of capturing multi-modal action distribution within the multi-goal dataset; (2) employing the reanalysis method with planning to generate imagined trajectories for funetuning policies. Specifically, we base the prior policy on an advantage-weighted conditioned generative adversarial network, which facilitates distinct mode separation, mitigating the pitfalls of out-of-distribution (OOD) actions. For further policy optimization, the reanalysis method generates high-quality imaginary data by planning with learned models for both intra-trajectory and inter-trajectory goals. With thorough experimental evaluations, we demonstrate that GOPlan achieves state-of-the-art performance on various offline multi-goal navigation and manipulation tasks. Moreover, our results highlight the superior ability of GOPlan to handle small data budgets and generalize to OOD goals",
    "checked": true,
    "id": "1d97ed822e13688ef48eb005f428081fb08d6f28",
    "semantic_title": "goplan: goal-conditioned offline reinforcement learning by planning with learned models",
    "citation_count": 2,
    "authors": [
      "Mianchu Wang",
      "Rui Yang",
      "Xi Chen",
      "Hao Sun",
      "Meng Fang",
      "Giovanni Montana"
    ]
  },
  "https://openreview.net/forum?id=fdyHzoGT8g": {
    "title": "Depth Scaling in Graph Neural Networks: Understanding the Flat Curve Behavior",
    "volume": "main",
    "abstract": "Training deep Graph Neural Networks (GNNs) has proved to be a challenging task. A key goal of many new GNN architectures is to enable the depth scaling seen in other types of deep learning models. However, unlike deep learning methods in other domains, deep GNNs do not show significant performance boosts when compared to their shallow counterparts (resulting in a flat curve of performance over depth). In this work, we investigate some of the reasons why this goal of depth still eludes GNN researchers. We also question the effectiveness of current methods to train deep GNNs and show evidence of different types of pathological behavior in these networks. Our results suggest that current approaches hide the problems with deep GNNs rather than solve them, as current deep GNNs are only as discriminative as their respective shallow versions",
    "checked": false,
    "id": "3d02132a46105c722cab61429463c46d3b0cad1f",
    "semantic_title": "frontiers in soft condensed matter artificial",
    "citation_count": 1,
    "authors": [
      "Diana Gomes",
      "Kyriakos Efthymiadis",
      "Ann Nowe",
      "Peter Vrancx"
    ]
  },
  "https://openreview.net/forum?id=WHAmxfLjeJ": {
    "title": "Understanding Smoothness of Vector Gaussian Processes on Product Spaces",
    "volume": "main",
    "abstract": "Vector Gaussian processes are becoming increasingly important in machine learning and statistics, with applications to many branches of applied sciences. Recent efforts have al- lowed to understand smoothness in scalar Gaussian processes defined over manifolds as well as over product spaces involving manifolds. Under assumptions of Gaussianity and mean-square continuity, the smoothness of a zero- mean scalar process is in one-to-one correspondence with the smoothness of the covariance kernel. Unfortunately, such a result is not available for vector-valued random fields, as the way each component in the covariance kernel contributes to the smoothness of the vector field is unclear. This paper challenges the problem of quantifying smoothness of matrix-valued continuous kernels that are associated with mean-square continuous vector Gaussian processes defined over non-Euclidean product manifolds. After noting that a constructive RKHS approach is unsuitable for this specific task, we proceed through the analysis of spectral properties. Specifically, we find a spectral representation to quantify smoothness through Sobolev spaces that are adapted to certain measure spaces of product measures obtained through the ten- sor product of Haar measures with multivariate Gaussian measures. Our results allow to measure smoothness in a simple way, and open for the study of foundational properties of certain machine learning techniques over product spaces",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emilio Porcu",
      "Ana Paula Peron",
      "Eugenio Massa",
      "Xavier Emery"
    ]
  },
  "https://openreview.net/forum?id=kxHIK4x8qc": {
    "title": "MAGDiff: Covariate Data Set Shift Detection via Activation Graphs of Neural Networks",
    "volume": "main",
    "abstract": "Despite their successful application to a variety of tasks, neural networks remain limited, like other machine learning methods, by their sensitivity to shifts in the data: their performance can be severely impacted by differences in distribution between the data on which they were trained and that on which they are deployed. In this article, we propose a new family of representations, called MAGDiff, that we extract from any given neural network classifier and that allows for efficient covariate data shift detection without the need to train a new model dedicated to this task. These representations are computed by comparing the activation graphs of the neural network for samples belonging to the training distribution and to the target distribution, and yield powerful data- and task-adapted statistics for the two-sample tests commonly used for data set shift detection. We demonstrate this empirically by measuring the statistical powers of two-sample Kolmogorov-Smirnov (KS) tests on several different data sets and shift types, and showing that our novel representations induce significant improvements over a state-of-the-art baseline relying on the network output",
    "checked": false,
    "id": "b51271a90b386cb0ff723a8a43f1106cbd575833",
    "semantic_title": "magdiff: covariate data set shift detection via activation graphs of deep neural networks",
    "citation_count": 1,
    "authors": [
      "Charles Arnal",
      "Felix Hensel",
      "Mathieu Carrière",
      "Théo Lacombe",
      "Hiroaki Kurihara",
      "Yuichi Ike",
      "Frederic Chazal"
    ]
  },
  "https://openreview.net/forum?id=oCfamUtecN": {
    "title": "Regret Bounds for Noise-Free Cascaded Kernelized Bandits",
    "volume": "main",
    "abstract": "We consider optimizing a function network in the noise-free grey-box setting with RKHS function classes, where the exact intermediate results are observable. We assume that the structure of the network is known (but not the underlying functions comprising it), and we study three types of structures: (1) chain: a cascade of scalar-valued functions, (2) multi-output chain: a cascade of vector-valued functions, and (3) feed-forward network: a fully connected feed-forward network of scalar-valued functions. We propose a sequential upper confidence bound based algorithm GPN-UCB along with a general theoretical upper bound on the cumulative regret. In addition, we propose a non-adaptive sampling based method along with its theoretical upper bound on the simple regret for the Mat\\'ern kernel. We also provide algorithm-independent lower bounds on the simple regret and cumulative regret. Our regret bounds for GPN-UCB have the same dependence on the time horizon as the best known in the vanilla black-box setting, as well as near-optimal dependencies on other parameters (e.g., RKHS norm and network length)",
    "checked": true,
    "id": "587fb638a91e658854719987ba5294d9cdcb5b2a",
    "semantic_title": "regret bounds for noise-free cascaded kernelized bandits",
    "citation_count": 1,
    "authors": [
      "Zihan Li",
      "Jonathan Scarlett"
    ]
  },
  "https://openreview.net/forum?id=KLBD13bsVl": {
    "title": "The Interplay of Uncertainty Modeling and Deep Active Learning: An Empirical Analysis in Image Classification",
    "volume": "main",
    "abstract": "Deep active learning (AL) seeks to reduce the annotation costs required for training deep neural networks (DNNs). Often, deep AL strategies focus on instances where the predictive uncertainty of a DNN is high. Furthermore, Bayesian concepts to model uncertainty are frequently adopted. Despite considerable research, a detailed analysis of the role of uncertainty in deep AL is still missing, especially regarding aleatoric and epistemic uncertainty, both related to predictive uncertainty. This article provides an in-depth empirical study analyzing the interplay of uncertainty and deep AL in image classification. Our study investigates four hypotheses that provide an intuitive understanding of the effects of accurately estimating aleatoric and epistemic uncertainty on existing uncertainty-based AL strategies but also, in the opposite direction, the impact of uncertainty-based AL on the quality of uncertainty estimates that are needed in many applications. By analyzing these hypotheses on synthetic and real-world data, we find that accurate aleatoric estimates can even impair instance selection, while accurate epistemic estimates have negligible effects. Moreover, we provide a publicly available toolbox for deep AL with various models and strategies to facilitate further research and practical applications. Code is available at github.com/anonymous",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Denis Huseljic",
      "Marek Herde",
      "Yannick Nagel",
      "Lukas Rauch",
      "Paulius Strimaitis",
      "Bernhard Sick"
    ]
  },
  "https://openreview.net/forum?id=RZPN8cgqST": {
    "title": "InduCE: Inductive Counterfactual Explanations for Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) drive several real-world applications including drug-discovery, recommendation engines, and chip designing. Unfortunately, GNNs are a black-box since they do not allow human-intelligible explanations of their predictions. Counterfactual reasoning is an effort to overcome this limitation. Specifically, the objective is to minimally perturb the input graph to a GNN, so that its prediction changes. While several algorithms have been proposed towards counterfactual explanations of GNNs, majority suffer from three key limitations: (1) they only consider perturbations in the form of deletions of existing edges, (2) they perform an inefficient exploration of the combinatorial search space, (3) the counterfactual explanation model is transductive in nature, i.e., they do not generalize to unseen data. In this work, we propose an inductive algorithm called InduCE, that overcomes these limitations. Through extensive experiments on graph datasets, we show that incorporating edge additions, and modelling marginal effect of perturbations aid in generating better counterfactuals among available recourse. Furthermore, inductive modeling enables InduCE to directly predict counterfactual perturbations without requiring instance-specific training. This leads to significant computational speed-up over baselines and allows counterfactual analyses for GNNs at scale",
    "checked": false,
    "id": "83a2f57677ea625a80bb438f6080a5649d14b3bb",
    "semantic_title": "empowering counterfactual reasoning over graph neural networks through inductivity",
    "citation_count": 0,
    "authors": [
      "Samidha Verma",
      "Burouj Armgaan",
      "Sourav Medya",
      "Sayan Ranu"
    ]
  },
  "https://openreview.net/forum?id=7VB5db72lr": {
    "title": "G4SATBench: Benchmarking and Advancing SAT Solving with Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) have recently emerged as a promising approach for solving the Boolean Satisfiability Problem (SAT), offering potential alternatives to traditional backtracking or local search SAT solvers. However, despite the growing volume of literature in this field, there remains a notable absence of a unified dataset and a fair benchmark to evaluate and compare existing approaches. To address this crucial gap, we present G4SATBench, the first benchmark study that establishes a comprehensive evaluation framework for GNN-based SAT solvers. In G4SATBench, we meticulously curate a large and diverse set of SAT datasets comprising 7 problems with 3 difficulty levels and benchmark a broad range of GNN models across various prediction tasks, training objectives, and inference algorithms. To explore the learning abilities and comprehend the strengths and limitations of GNN-based SAT solvers, we also compare their solving processes with the heuristics in search-based SAT solvers. Our empirical results provide valuable insights into the performance of GNN-based SAT solvers and further suggest that existing GNN models can effectively learn a solving strategy akin to greedy local search but struggle to learn backtracking search in the latent space. Our codebase is available at https://github.com/zhaoyu-li/G4SATBench",
    "checked": true,
    "id": "490446ac0419c2f232b481e4c73117bcffe45576",
    "semantic_title": "g4satbench: benchmarking and advancing sat solving with graph neural networks",
    "citation_count": 1,
    "authors": [
      "Zhaoyu Li",
      "Jinpei Guo",
      "Xujie Si"
    ]
  },
  "https://openreview.net/forum?id=vsez76EAV8": {
    "title": "PaDPaF: Partial Disentanglement with Partially-Federated GANs",
    "volume": "main",
    "abstract": "Federated learning has become a popular machine learning paradigm with many potential real-life applications, including recommendation systems, the Internet of Things (IoT), healthcare, and self-driving cars. Though most current applications focus on classification-based tasks, learning personalized generative models remains largely unexplored, and their benefits in the heterogeneous setting still need to be better understood. This work proposes a novel architecture combining global client-agnostic and local client-specific generative models. We show that using standard techniques for training federated models, our proposed model achieves privacy and personalization by implicitly disentangling the globally-consistent representation (i.e. content) from the client-dependent variations (i.e. style). Using such decomposition, personalized models can generate locally unseen labels while preserving the given style of the client and can predict the labels for all clients with high accuracy by training a simple linear classifier on the global content features. Furthermore, disentanglement enables other essential applications, such as data anonymization, by sharing only the content. Extensive experimental evaluation corroborates our findings, and we also discuss a theoretical motivation for the proposed approach",
    "checked": false,
    "id": "89822ee881dd3b53b6a5dbb42cd2132d61ead7a3",
    "semantic_title": "partial disentanglement with partially-federated gans (padpaf)",
    "citation_count": 0,
    "authors": [
      "Abdulla Jasem Almansoori",
      "Samuel Horváth",
      "Martin Takáč"
    ]
  },
  "https://openreview.net/forum?id=KVUtlM60HM": {
    "title": "Archetypal Analysis++: Rethinking the Initialization Strategy",
    "volume": "main",
    "abstract": "Archetypal analysis is a matrix factorization method with convexity constraints. Due to local minima, a good initialization is essential, but frequently used initialization methods yield either sub-optimal starting points or are prone to get stuck in poor local minima. In this paper, we propose archetypal analysis++ (AA++), a probabilistic initialization strategy for archetypal analysis that sequentially samples points based on their influence on the objective function, similar to $k$-means++. In fact, we argue that $k$-means++ already approximates the proposed initialization method. Furthermore, we suggest to adapt an efficient Monte Carlo approximation of $k$-means++ to AA++. In an extensive empirical evaluation of 15 real-world data sets of varying sizes and dimensionalities and considering two pre-processing strategies, we show that AA++ almost always outperforms all baselines, including the most frequently used ones",
    "checked": true,
    "id": "33a7a6c4a7a9cf84c37646ec8a4acaad9d110a40",
    "semantic_title": "archetypal analysis++: rethinking the initialization strategy",
    "citation_count": 0,
    "authors": [
      "Sebastian Mair",
      "Jens Sjölund"
    ]
  },
  "https://openreview.net/forum?id=vxxi7xzzn7": {
    "title": "Data Pruning Can Do More: A Comprehensive Data Pruning Approach for Object Re-identification",
    "volume": "main",
    "abstract": "Previous studies have demonstrated that not each sample in a dataset is of equal importance during training. Data pruning aims to remove less important or informative samples while still achieving comparable results as training on the original (untruncated) dataset, thereby reducing storage and training costs. However, the majority of data pruning methods are applied to image classification tasks. To our knowledge, this work is the first to explore the feasibility of these pruning methods applied to object re-identification (ReID) tasks, while also presenting a more comprehensive data pruning approach. By fully leveraging the logit history during training, our approach offers a more accurate and comprehensive metric for quantifying sample importance, as well as correcting mislabeled samples and recognizing outliers. Furthermore, our approach is highly efficient, reducing the cost of importance score estimation by 10 times compared to existing methods. Our approach is a plug-and-play, architecture-agnostic framework that can eliminate/reduce 35%, 30%, and 5% of samples/training time on the VeRi, MSMT17 and Market1501 datasets, respectively, with negligible loss in accuracy (< 0.1%). The lists of important, mislabeled, and outlier samples from these ReID datasets are available at https://github.com/Zi-Y/data-pruning-reid",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zi Yang",
      "Haojin Yang",
      "Soumajit Majumder",
      "Jorge Cardoso",
      "Guillermo Gallego"
    ]
  },
  "https://openreview.net/forum?id=kPIU8PnJPo": {
    "title": "Scaling Vision-and-Language Navigation With Offline RL",
    "volume": "main",
    "abstract": "The study of vision-and-language navigation (VLN) has typically relied on expert trajectories, which may not always be available in real-world situations due to the significant effort required to collect them. On the other hand, existing approaches to training VLN agents that go beyond available expert data involve data augmentations or online exploration which can be tedious and risky. In contrast, it is easy to access large repositories of suboptimal offline trajectories. Inspired by research in offline reinforcement learning (ORL), we introduce a new problem setup of VLN-ORL which studies VLN using suboptimal demonstration data. We introduce a simple and effective reward-conditioned approach that can account for dataset suboptimality for training VLN agents, as well as benchmarks to evaluate progress and promote research in this area. We empirically study various noise models for characterizing dataset suboptimality among other unique challenges in VLN-ORL and instantiate it for the VLN⟳BERT and MTVM architectures in the R2R and RxR environments. Our experiments demonstrate that the proposed reward-conditioned approach leads to significant performance improvements, even in complex and intricate environments",
    "checked": true,
    "id": "43476ac68d991bb76f288fd979cdb240223b46f5",
    "semantic_title": "scaling vision-and-language navigation with offline rl",
    "citation_count": 0,
    "authors": [
      "Valay Bundele",
      "Mahesh Bhupati",
      "Biplab Banerjee",
      "Aditya Grover"
    ]
  },
  "https://openreview.net/forum?id=2bURaH6RN8": {
    "title": "Momentum-Based Policy Gradient with Second-Order Information",
    "volume": "main",
    "abstract": "Variance-reduced gradient estimators for policy gradient methods have been one of the main focus of research in the reinforcement learning in recent years as they allow acceleration of the estimation process. We propose a variance-reduced policy-gradient method, called SHARP, which incorporates second-order information into stochastic gradient descent (SGD) using momentum with a time-varying learning rate. SHARP algorithm is parameter-free, achieving $\\epsilon$-approximate first-order stationary point with $O(\\epsilon^{-3})$ number of trajectories, while using a batch size of $O(1)$ at each iteration. Unlike most previous work, our proposed algorithm does not require importance sampling which can compromise the advantage of variance reduction process. Moreover, the variance of estimation error decays with the fast rate of $O(1/t^{2/3})$ where $t$ is the number of iterations. Our extensive experimental evaluations show the effectiveness of the proposed algorithm on various control tasks and its advantage over the state of the art in practice",
    "checked": true,
    "id": "fa8c76c432f4fb6901f3545dd20fd02c74297c9b",
    "semantic_title": "momentum-based policy gradient with second-order information",
    "citation_count": 7,
    "authors": [
      "Saber Salehkaleybar",
      "Mohammadsadegh Khorasani",
      "Negar Kiyavash",
      "Niao He",
      "Patrick Thiran"
    ]
  },
  "https://openreview.net/forum?id=HNqEKZDDRc": {
    "title": "Offline Reinforcement Learning via Tsallis Regularization",
    "volume": "main",
    "abstract": "Offline reinforcement learning (RL) focuses on learning a good policy from a fixed dataset. The dataset is generated by an unknown behavior policy through interactions with the environment and contains only a subset of the state-action spaces. Standard off-policy algorithms often perform poorly in this setting, suffering from errorneously optimistic values incurred by the out-of-distribution (OOD) actions not present in the dataset. The optimisim cannot be corrected as no further interaction with the environment is possible. Imposing divergence regularization and in-sample constraints are among the most popular methods to overcoming the issue by ensuring that the learned policy stays close to the behavior policy to minimize the occurrence of OOD actions. This paper proposes Tsallis regularization for offline RL, which aligns the induced sparsemax policies to the in-sample constraint. Sparsemax interpolates existing methods utilizing hard-max and softmax policies, in that only a subset of actions contributes non-zero action probability as compared to softmax (all actions) and hard-max (single action). We leverage this property to model the behavior policy and show that under several assumptions the learned sparsemax policies may have sparsity-conditional KL divergence to the behavior policy, making Tsallis regularization especially suitable for the Behavior Cloning methods. We propose a novel actor-critic algorithm: Tsallis Advantage Weighted Actor-Critic (Tsallis AWAC) generalizing AWAC and analyze its performance in standard Mujoco environments. Our code is available at \\url{https://github.com/lingweizhu/tsallis_regularization}",
    "checked": false,
    "id": "45b5149363c6f7c22c5faa2789c79586ffc08b22",
    "semantic_title": "offline reinforcement learning with behavior value regularization",
    "citation_count": 0,
    "authors": [
      "Lingwei Zhu",
      "Matthew Kyle Schlegel",
      "Han Wang",
      "Martha White"
    ]
  },
  "https://openreview.net/forum?id=AYJ3m7BocI": {
    "title": "A Survey on Transferability of Adversarial Examples Across Deep Neural Networks",
    "volume": "main",
    "abstract": "The emergence of Deep Neural Networks (DNNs) has revolutionized various domains by enabling the resolution of complex tasks spanning image recognition, natural language processing, and scientific problem-solving. However, this progress has also brought to light a concerning vulnerability: adversarial examples. These crafted inputs, imperceptible to humans, can manipulate machine learning models into making erroneous predictions, raising concerns for safety-critical applications. An intriguing property of this phenomenon is the transferability of adversarial examples, where perturbations crafted for one model can deceive another, often with a different architecture. This intriguing property enables ``black-box'' attacks which circumvents the need for detailed knowledge of the target model. This survey explores the landscape of the adversarial transferability of adversarial examples. We categorize existing methodologies to enhance adversarial transferability and discuss the fundamental principles guiding each approach. While the predominant body of research primarily concentrates on image classification, we also extend our discussion to encompass other vision tasks and beyond. Challenges and opportunities are discussed, highlighting the importance of fortifying DNNs against adversarial vulnerabilities in an evolving landscape",
    "checked": true,
    "id": "47997f6fb2eb5bcc59a0266d11d07f6c08233d65",
    "semantic_title": "a survey on transferability of adversarial examples across deep neural networks",
    "citation_count": 10,
    "authors": [
      "Jindong Gu",
      "Xiaojun Jia",
      "Pau de Jorge",
      "Wenqian Yu",
      "Xinwei Liu",
      "Avery Ma",
      "Yuan Xun",
      "Anjun Hu",
      "Ashkan Khakzar",
      "Zhijiang Li",
      "Xiaochun Cao",
      "Philip Torr"
    ]
  },
  "https://openreview.net/forum?id=XHEhjDxPDl": {
    "title": "Solving Inverse Problems with Model Mismatch using Untrained Neural Networks within Model-based Architectures",
    "volume": "main",
    "abstract": "Model-based deep learning methods such as loop unrolling (LU) and deep equilibrium model (DEQ) extensions offer outstanding performance in solving inverse problems (IP). These methods unroll the optimization iterations into a sequence of neural networks that in effect learn a regularization function from data. While these architectures are currently state-of-the-art in numerous applications, their success heavily relies on the accuracy of the forward model. This assumption can be limiting in many physical applications due to model simplifications or uncertainties in the apparatus. To address forward model mismatch, we introduce an untrained forward model residual block within the model-based architecture to match the data consistency in the measurement domain for each instance. We propose two variants in well-known model-based architectures (LU and DEQ) and prove convergence under mild conditions. Our approach offers a unified solution that is less parameter-sensitive, requires no additional data, and enables simultaneous fitting of the forward model and reconstruction in a single pass, benefiting both linear and nonlinear inverse problems. The experiments show significant quality improvement in removing artifacts and preserving details across three distinct applications, encompassing both linear and nonlinear inverse problems. Moreover, we highlight reconstruction effectiveness in intermediate steps and showcase robustness to random initialization of the residual block and a higher number of iterations during evaluation",
    "checked": true,
    "id": "7a5b0f18597f7ca466c1f68e7673d2e929c4f441",
    "semantic_title": "solving inverse problems with model mismatch using untrained neural networks within model-based architectures",
    "citation_count": 0,
    "authors": [
      "Peimeng Guan",
      "Naveed Iqbal",
      "Mark A. Davenport",
      "Mudassir Masood"
    ]
  },
  "https://openreview.net/forum?id=qc2lmWkvk4": {
    "title": "Hybrid Federated Learning for Feature & Sample Heterogeneity: Algorithms and Implementation",
    "volume": "main",
    "abstract": "Federated learning (FL) is a popular distributed machine learning paradigm dealing with distributed and private data sets. Based on the data partition pattern, FL is often categorized into horizontal, vertical, and hybrid settings. All three settings have many applications, but the hybrid FL remains relatively less explored, because it deals with the challenging situation where {\\it both} the feature space and the data samples are {\\it heterogeneous}. This work designs a novel mathematical model that effectively allows the clients to aggregate distributed data with heterogeneous, and possibly overlapping features and samples. Our main idea is to partition each client's model into a feature extractor part and a classifier part, where the former can be used to process the input data, while the latter is used to perform the learning from the extracted features. The heterogeneous feature aggregation is done through building a server model, which assimilates local classifiers and feature extractors through a carefully designed matching mechanism. A communication-efficient algorithm is then designed to train both the client and server models. Finally, we conducted numerical experiments on multiple image classification data sets to validate the performance of the proposed algorithm. To our knowledge, this is the first formulation and algorithm developed for hybrid FL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinwei Zhang",
      "Wotao Yin",
      "Mingyi Hong",
      "Tianyi Chen"
    ]
  },
  "https://openreview.net/forum?id=epcLNhkoEL": {
    "title": "Fixed Budget Best Arm Identification in Unimodal Bandits",
    "volume": "main",
    "abstract": "We consider the best arm identification problem in a fixed budget stochastic multi-armed bandit in which arm means exhibit unimodal structure, i.e., there is only one local maximum. We establish that the probability of misidentifying the optimal arm within a budget of $T$ is lower bounded as $\\mathcal{O}\\left(\\exp\\left\\{-T/\\bar{H}\\right\\}\\right)$, where $\\bar{H}$ depends on the sub-optimality gaps of arms in the neighborhood of the optimal arm. % where $\\bar{H}\\leq 2\\Delta^{-2}$. In contrast to the lower bound for the unstructured case, the error exponent in this bound does not depend on the number of arms $K$ and is smaller by a factor $\\log K$, which captures the gain achievable by exploiting the unimodal structure. We then develop an algorithm named {\\it Fixed Budget Best Arm Unimodal Bandits ( FB-BAUB)} that exploits unimodality to achieve the gain. Specifically, we show that the error probability of \\algo{} is upper bounded as $\\mathcal{O}\\left(\\log_2 K\\exp\\left\\{-T\\Delta^2\\right\\}\\right)$, where $\\Delta$ is the gap between the neighboring arms and $\\bar{H}\\leq 2\\Delta^{-2}$. We demonstrate that \\algo{} outperforms the state-of-the-art algorithms through extensive simulations. Moreover, \\algo{} is parameter-free and simple to implement",
    "checked": false,
    "id": "e59f0a8bceffaefb7f646723e53afcb3ff3bc9e0",
    "semantic_title": "fixed-budget best-arm identification in sparse linear bandits",
    "citation_count": 1,
    "authors": [
      "Debamita Ghosh",
      "Manjesh Kumar Hanawal",
      "Nikola Zlatanov"
    ]
  },
  "https://openreview.net/forum?id=yf4ciZcgrg": {
    "title": "Restricted Random Pruning at Initialization for High Compression Range",
    "volume": "main",
    "abstract": "Pruning at Initialization (PaI) makes training overparameterized neural networks more efficient by reducing the overall computational cost from training to inference. Recent PaI studies showed that random pruning is more effective than ranking-based pruning, which learns connectivity. However, the effectiveness of each pruning method depends on the existence of skip connections and the compression ratio (the before-after pruning parameter ratio). While random pruning performs better than ranking-based pruning on architectures with skip connections, the superiority without skip connections is reversed in the high compression range. This paper proposes Minimum Connection Assurance (MiCA) that achieves higher accuracy than conventional PaI methods for architectures with and without skip connections, regardless of the compression ratio. MiCA preserves the random connection between the layers and maintains the performance at high compression ratios without the costly connection learning that ranking-based pruning requires. Experiments on image classification using CIFAR-10 and CIFAR-100 and node classification using OGBN-ArXiv show that MiCA enhances the compression ratio and accuracy trade-offs compared to existing PaI methods. In VGG-16 with CIFAR-10, MiCA improves the accuracy of random pruning by $27.0\\%$ at $10^{4.7}\\times$ compression ratio. Furthermore, experimental analysis reveals that increasing the utilization of the nodes through which information flows from the first layer is essential for maintaining high performance at a high compression ratio",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hikari Otsuka",
      "Yasuyuki Okoshi",
      "Ángel López García-Arias",
      "Kazushi Kawamura",
      "Thiem Van Chu",
      "Daichi Fujiki",
      "Masato Motomura"
    ]
  },
  "https://openreview.net/forum?id=zdtSqZnkx1": {
    "title": "Continual HyperTransformer: A Meta-Learner for Continual Few-Shot Learning",
    "volume": "main",
    "abstract": "We focus on the problem of learning without forgetting from multiple tasks arriving sequentially, where each task is defined using a few-shot episode of novel or already seen classes. We approach this problem using the recently published HyperTransformer (HT), a Transformer-based hypernetwork that generates specialized task-specific CNN weights directly from the support set. In order to learn from a continual sequence of tasks, we propose to recursively re-use the generated weights as input to the HT for the next task. This way, the generated CNN weights themselves act as a representation of previously learned tasks, and the HT is trained to update these weights so that the new task can be learned without forgetting past tasks. This approach is different from most continual learning algorithms that typically rely on using replay buffers, weight regularization or task-dependent architectural changes. We demonstrate that our proposed Continual HyperTransformer method equipped with a prototypical loss is capable of learning and retaining knowledge about past tasks for a variety of scenarios, including learning from mini-batches, and task-incremental and class-incremental learning scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Max Vladymyrov",
      "Andrey Zhmoginov",
      "Mark Sandler"
    ]
  },
  "https://openreview.net/forum?id=qItxVbWyfe": {
    "title": "Federated Learning with Convex Global and Local Constraints",
    "volume": "main",
    "abstract": "In practice, many machine learning (ML) problems come with constraints, and their applied domains involve distributed sensitive data that cannot be shared with others, e.g., in healthcare. Collaborative learning in such practical scenarios entails federated learning (FL) for ML problems with constraints, or FL with constraints for short. Despite the extensive developments of FL techniques in recent years, these techniques only deal with unconstrained FL problems or FL problems with simple constraints that are amenable to easy projections. There is little work dealing with FL problems with general constraints. To fill this gap, we take the first step toward building an algorithmic framework for solving FL problems with general constraints. In particular, we propose a new FL algorithm for constrained ML problems based on the proximal augmented Lagrangian (AL) method. %The subproblems of our proposed algorithm are solved by an inexact alternating direction method of multipliers (ADMM). Assuming convex objective and convex constraints plus other mild conditions, we establish the worst-case complexity of the proposed algorithm. Our numerical experiments show the effectiveness of our algorithm in performing Neyman-Pearson classification and fairness-aware learning with nonconvex constraints, in an FL setting",
    "checked": true,
    "id": "2a97b84b77e1582ac1b6c04c73728b480d89c968",
    "semantic_title": "federated learning with convex global and local constraints",
    "citation_count": 0,
    "authors": [
      "Chuan He",
      "Le Peng",
      "Ju Sun"
    ]
  },
  "https://openreview.net/forum?id=cueEUSG7lE": {
    "title": "Group Fairness in Reinforcement Learning via Multi-Objective Rewards",
    "volume": "main",
    "abstract": "Recent works extend classification group fairness measures to sequential decision processes such as reinforcement learning (RL) by measuring fairness as the difference in decision-maker utility (e.g. accuracy) of each group. This approach suffers when decision-maker utility is not perfectly aligned with group utility, such as in repeat loan applications where a false positive (loan default) impacts the groups (applicants) and decision-maker (lender) by different magnitudes. Some works remedy this by measuring fairness in terms of group utility, typically referred to as their \"qualification\", but few works offer solutions that yield group qualification equality. Those that do are prone to violating the \"no-harm\" principle where one or more groups' qualifications are lowered in order to achieve equality. In this work, we characterize this problem space as having three implicit objectives: maximizing decision-maker utility, maximizing group qualification, and minimizing the difference in qualification between groups. We provide a RL policy learning technique that optimizes for these objectives directly by constructing a multi-objective reward function that encodes these objectives as distinct reward signals. Under suitable parameterizations our approach is guaranteed to respect the \"no-harm\" principle",
    "checked": false,
    "id": "3d8375c704e0a95b6e2977d66118419449774ca4",
    "semantic_title": "adversar: adversarial search and rescue via multi-agent reinforcement learning",
    "citation_count": 3,
    "authors": [
      "Jack Blandin",
      "Ian A. Kash"
    ]
  },
  "https://openreview.net/forum?id=oyISaaeHwD": {
    "title": "On Good Practices for Task-Specific Distillation of Large Pretrained Visual Models",
    "volume": "main",
    "abstract": "Large pretrained visual models exhibit remarkable generalization across diverse recognition tasks. Yet, real-world applications often demand compact models tailored to specific problems. Variants of knowledge distillation have been devised for such a purpose, enabling task-specific compact models (the students) to learn from a generic large pretrained one (the teacher). In this paper, we show that the excellent robustness and versatility of recent pretrained models challenge common practices established in the literature, calling for a new set of optimal guidelines for task-specific distillation. To address the lack of samples in downstream tasks, we also show that a variant of Mixup based on stable diffusion complements standard data augmentation. This strategy eliminates the need for engineered text prompts and improves distillation of generic models into streamlined specialized networks",
    "checked": true,
    "id": "b168bd59ae55348b68ae4638e97be27cdd534120",
    "semantic_title": "on good practices for task-specific distillation of large pretrained visual models",
    "citation_count": 0,
    "authors": [
      "Juliette Marrie",
      "Michael Arbel",
      "Julien Mairal",
      "Diane Larlus"
    ]
  },
  "https://openreview.net/forum?id=aVOzWH1Nc5": {
    "title": "Dynamic Online Ensembles of Basis Expansions",
    "volume": "main",
    "abstract": "Practical Bayesian learning often requires (1) online inference, (2) dynamic models, and (3) ensembling over multiple different models. Recent advances have shown how to use random feature approximations to achieve scalable, online ensembling of Gaussian processes with desirable theoretical properties and fruitful applications. One key to these methods' success is the inclusion of a random walk on the model parameters, which makes models dynamic. We show that these methods can be generalized easily to any basis expansion model and that using alternative basis expansions, such as Hilbert space Gaussian processes, often results in better performance. To simplify the process of choosing a specific basis expansion, our method's generality also allows the ensembling of several entirely different models, for example, a Gaussian process and polynomial regression. Finally, we propose a novel method to ensemble static and dynamic models together",
    "checked": true,
    "id": "061122b4dbb461f2f6e32a8c499ba190d5f0b72a",
    "semantic_title": "dynamic online ensembles of basis expansions",
    "citation_count": 0,
    "authors": [
      "Daniel Waxman",
      "Petar Djuric"
    ]
  },
  "https://openreview.net/forum?id=p1a6ruIZCT": {
    "title": "IMEX-Reg: Implicit-Explicit Regularization in the Function Space for Continual Learning",
    "volume": "main",
    "abstract": "Continual learning (CL) remains one of the long-standing challenges for deep neural networks due to catastrophic forgetting of previously acquired knowledge. Although rehearsal-based approaches have been fairly successful in mitigating catastrophic forgetting, they suffer from overfitting on buffered samples and prior information loss, hindering generalization under low-buffer regimes. Inspired by how humans learn using strong inductive biases, we propose \\textbf{IMEX-Reg} to improve the generalization performance of experience rehearsal in CL under low buffer regimes. Specifically, we employ a two-pronged implicit-explicit regularization approach using contrastive representation learning (CRL) and consistency regularization. To further leverage the global relationship between representations learned using CRL, we propose a regularization strategy to guide the classifier toward the activation correlations in the unit hypersphere of the CRL. Our results show that IMEX-Reg significantly improves generalization performance and outperforms rehearsal-based approaches in several CL scenarios. It is also robust to natural and adversarial corruptions with less task-recency bias. Additionally, we provide theoretical insights to support our design decisions further",
    "checked": true,
    "id": "bafbf52e99050d485399daae86c53f302d3e6293",
    "semantic_title": "imex-reg: implicit-explicit regularization in the function space for continual learning",
    "citation_count": 0,
    "authors": [
      "Prashant Shivaram Bhat",
      "Bharath Chennamkulam Renjith",
      "Elahe Arani",
      "Bahram Zonooz"
    ]
  },
  "https://openreview.net/forum?id=N0Sc0KY0AH": {
    "title": "Improving Subgraph-GNNs via Edge-Level Ego-Network Encodings",
    "volume": "main",
    "abstract": "We present a novel edge-level ego-network encoding for learning on graphs that can boost Message Passing Graph Neural Networks (MP-GNNs) by providing additional node and edge features or extending message-passing formats. The proposed encoding is sufficient to distinguish Strongly Regular Graphs, a family of challenging 3-WL equivalent graphs. We show theoretically that such encoding is more expressive than node-based sub-graph MP-GNNs. In an empirical evaluation on four benchmarks with 10 graph datasets, our results match or improve previous baselines on expressivity, graph classification, graph regression, and proximity tasks---while reducing memory usage by 18.1x in certain real-world settings",
    "checked": true,
    "id": "c22a2ef9512c3e47d79744bab70861f2ebe0a5e3",
    "semantic_title": "improving subgraph-gnns via edge-level ego-network encodings",
    "citation_count": 0,
    "authors": [
      "Nurudin Alvarez-Gonzalez",
      "Andreas Kaltenbrunner",
      "Vicenç Gómez"
    ]
  },
  "https://openreview.net/forum?id=KleJZ9ZzYw": {
    "title": "DP-ImgSyn: Dataset Alignment for Obfuscated, Differentially Private Image Synthesis",
    "volume": "main",
    "abstract": "The availability of abundant data has catalyzed the expansion of deep learning vision algorithms. However, certain vision datasets cannot be publicly released due to privacy reasons. Releasing synthetic images instead of private images is a common approach to overcome this issue. A popular method to generate synthetic images is using Generative Adversarial Networks (GANs) with Differential Privacy (DP) guarantees. However, GAN-generated synthetic images are visually similar to private images. This is a severe limitation, particularly when the private dataset depicts visually sensitive and disturbing content. To address this, we propose a non-generative framework, Differentially Private Image Synthesis (DP-ImgSyn), to generate and release synthetic images for image classification tasks. These synthetic images: (1) have DP guarantees, (2) retain the utility of the private images, i.e., a model trained using synthetic images results in similar accuracy as a model trained on private images, (3) the synthetic images are visually dissimilar to private images. DP-ImgSyn consists of the following steps: First, a teacher model is trained on the private images using a DP training algorithm. Second, public images are used as initialization for the synthetic images which are optimized to align them with the private images. The optimization uses the teacher network's batch normalization layer statistics (mean, standard deviation) to inject information about the private images into the synthetic images. Third, the synthetic images and their soft labels, obtained from the teacher model, are released and can be deployed for neural network training on image classification tasks. Our experiments on various image classification datasets show that when using similar DP training mechanisms, our framework performs better than generative techniques (up to $\\approx$ 20% in terms of image classification accuracy)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Efstathia Soufleri",
      "Deepak Ravikumar",
      "Kaushik Roy"
    ]
  },
  "https://openreview.net/forum?id=WYGiqSVstK": {
    "title": "Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers",
    "volume": "main",
    "abstract": "Visual reasoning is dominated by end-to-end neural networks scaled to billions of model parameters and training examples. However, even the largest models struggle with compositional reasoning, generalization, fine-grained spatial and temporal reasoning, and counting. Visual reasoning with large language models (LLMs) as controllers can, in principle, address these limitations by decomposing the task and solving subtasks by orchestrating a set of (visual) tools. Recently, these models achieved great performance on tasks such as compositional visual question answering, visual grounding, and video temporal reasoning. Nevertheless, in their current form, these models heavily rely on human engineering of in-context examples in the prompt, which are often dataset- and task-specific and require significant labor by highly skilled programmers. In this work, we present a framework that mitigates these issues by introducing spatially and temporally abstract routines and by leveraging a small number of labeled examples to automatically generate in-context examples, thereby avoiding human-created in-context examples. On a number of visual reasoning tasks, we show that our framework leads to consistent gains in performance, makes LLMs as controllers setup more robust, and removes the need for human engineering of in-context examples",
    "checked": true,
    "id": "fc7feeaddc5a38c0d6f0d793737584e5f0bb7519",
    "semantic_title": "towards truly zero-shot compositional visual reasoning with llms as programmers",
    "citation_count": 2,
    "authors": [
      "Aleksandar Stanić",
      "Sergi Caelles",
      "Michael Tschannen"
    ]
  },
  "https://openreview.net/forum?id=Wiklo5VpG7": {
    "title": "From Stability to Chaos: Analyzing Gradient Descent Dynamics in Quadratic Regression",
    "volume": "main",
    "abstract": "We conduct a comprehensive investigation into the dynamics of gradient descent using large-order constant step-sizes in the context of quadratic regression models. Within this framework, we reveal that the dynamics can be encapsulated by a specific cubic map, naturally parameterized by the step-size. Through a fine-grained bifurcation analysis concerning the step-size parameter, we delineate five distinct training phases: (1) monotonic, (2) catapult, (3) periodic, (4) chaotic, and (5) divergent, precisely demarcating the boundaries of each phase. As illustrations, we provide examples involving phase retrieval and two-layer neural networks employing quadratic activation functions and constant outer-layers, utilizing orthogonal training data. Our simulations indicate that these five phases also manifest with generic non-orthogonal data. We also empirically investigate the generalization performance when training in the various non-monotonic (and non-divergent) phases. In particular, we observe that performing an ergodic trajectory averaging stabilizes the test error in non-monotonic (and non-divergent) phases",
    "checked": true,
    "id": "909f003b8f56302b2fe0594b93048bdc88d0ce59",
    "semantic_title": "from stability to chaos: analyzing gradient descent dynamics in quadratic regression",
    "citation_count": 2,
    "authors": [
      "Xuxing Chen",
      "Krishna Balasubramanian",
      "Promit Ghosal",
      "Bhavya Kumar Agrawalla"
    ]
  },
  "https://openreview.net/forum?id=Io3jDUC4DP": {
    "title": "Using Skew to Assess the Quality of GAN-generated Image Features",
    "volume": "main",
    "abstract": "The rapid advancement of Generative Adversarial Networks (GANs) necessitates the need to robustly evaluate these models. Among the established evaluation criteria, the Fréchet Inception Distance (FID) has been widely adopted due to its conceptual simplicity, fast computation time, and strong correlation with human perception. However, FID has inherent limitations, mainly stemming from its assumption that feature embeddings follow a Gaussian distribution, and therefore can be defined by their first two moments. As this does not hold in practice, in this paper we explore the importance of third-moments in image feature data and use this information to define a new measure, which we call the Skew Inception Distance (SID). We prove that SID is a pseudometric on probability distributions, show how it extends FID, and present a practical method for its computation. Our numerical experiments support that SID either tracks with FID or, in some cases, aligns more closely with human perception when evaluating image features of ImageNet data. Our work also shows that principal component analysis can be used to speed up the computation time of both FID and SID. Although we focus on using SID on image features for GAN evaluation, SID is applicable much more generally, including for the evaluation of other generative models",
    "checked": true,
    "id": "a1fbb3888406c6e9563c873602d104a5b0db705b",
    "semantic_title": "using skew to assess the quality of gan-generated image features",
    "citation_count": 0,
    "authors": [
      "Lorenzo Luzi",
      "Helen Jenne",
      "Carlos Ortiz Marrero",
      "Ryan Murray"
    ]
  },
  "https://openreview.net/forum?id=raD846nj2q": {
    "title": "Identify Ambiguous Tasks Combining Crowdsourced Labels by Weighting Areas Under the Margin",
    "volume": "main",
    "abstract": "In supervised learning — for instance in image classification — modern massive datasets are commonly labeled by a crowd of workers. The obtained labels in this crowdsourcing setting are then aggregated for training, generally leveraging a per-worker trust score. Yet, such workers oriented approaches discard the tasks' ambiguity. Ambiguous tasks might fool expert workers, which is often harmful for the learning step. In standard supervised learning settings -- with one label per task -- the Area Under the Margin (AUM) was tailored to identify mislabeled data. We adapt the AUM to identify ambiguous tasks in crowdsourced learning scenarios, introducing the Weighted Areas Under the Margin (WAUM). The WAUM is an average of AUMs weighted according to task-dependent scores. We show that the WAUM can help discarding ambiguous tasks from the training set, leading to better generalization performance. We report improvements over existing strategies for learning with a crowd, both on simulated settings, and on real datasets such as CIFAR-10H (a crowdsourced dataset with a high number of answered labels), LabelMe and Music (two datasets with few answered votes)",
    "checked": true,
    "id": "fa9aac3c7aba3ad4b2162c7ec2e0ad8ef4668005",
    "semantic_title": "identify ambiguous tasks combining crowdsourced labels by weighting areas under the margin",
    "citation_count": 1,
    "authors": [
      "Tanguy Lefort",
      "Benjamin Charlier",
      "Alexis Joly",
      "Joseph Salmon"
    ]
  },
  "https://openreview.net/forum?id=mrJi5kdKA4": {
    "title": "DSI2I: Dense Style for Unpaired Exemplar-based Image-to- Image Translation",
    "volume": "main",
    "abstract": "Unpaired exemplar-based image-to-image (UEI2I) translation aims to translate a source image to a target image domain with the style of a target image exemplar, without ground- truth input-translation pairs. Existing UEI2I methods represent style using one vector per image or rely on semantic supervision to define one style vector per object. Here, in contrast, we propose to represent style as a dense feature map, allowing for a finer-grained transfer to the source image without requiring any external semantic information. We then rely on perceptual and adversarial losses to disentangle our dense style and content representations. To stylize the source content with the exemplar style, we extract unsupervised cross-domain semantic correspondences and warp the exemplar style to the source content. We demon- strate the effectiveness of our method on four datasets using standard metrics together with a localized style metric we propose, which measures style similarity in a class-wise man- ner. Our results show that the translations produced by our approach are more diverse, preserve the source content better, and are closer to the exemplars when compared to the state-of-the-art methods",
    "checked": false,
    "id": "febb0ae0a29b1330da2b9172ff8ea83f1b040052",
    "semantic_title": "dsi2i: dense style for unpaired image-to-image translation",
    "citation_count": 1,
    "authors": [
      "Baran Ozaydin",
      "Tong Zhang",
      "Sabine Susstrunk",
      "Mathieu Salzmann"
    ]
  },
  "https://openreview.net/forum?id=yL15ys5swq": {
    "title": "Improving Diffusion Models for Scene Text Editing with Dual Encoders",
    "volume": "main",
    "abstract": "Scene text editing is a challenging task that involves modifying or inserting specified texts in an image while maintaining its natural and realistic appearance. Most previous approaches to this task rely on style-transfer models that crop out text regions and feed them into image transfer models, such as GANs. However, these methods are limited in their ability to change text style and are unable to insert texts into images. Recent advances in diffusion models have shown promise in overcoming these limitations with text-conditional image editing. However, our empirical analysis reveals that state-of-the-art diffusion models struggle with rendering correct text and controlling text style. To address these problems, we propose DIFFSTE to improve pre-trained diffusion models with a dual encoder design, which includes a character encoder for better text legibility and an instruction encoder for better style control. An instruction tuning framework is introduced to train our model to learn the mapping from the text instruction to the corresponding image with either the specified style or the style of the surrounding texts in the background. Such a training method further brings our method the zero-shot generalization ability to the following three scenarios: generating text with unseen font variation, e.g., italic and bold, mixing different fonts to construct a new font, and using more relaxed forms of natural language as the instructions to guide the generation task. We evaluate our approach on five datasets and demonstrate its superior performance in terms of text correctness, image naturalness, and style controllability",
    "checked": true,
    "id": "a820ba23594ba25d1db21116ddb5a55c806ee30a",
    "semantic_title": "improving diffusion models for scene text editing with dual encoders",
    "citation_count": 10,
    "authors": [
      "Jiabao Ji",
      "Guanhua Zhang",
      "Zhaowen Wang",
      "Bairu Hou",
      "Zhifei Zhang",
      "Brian L. Price",
      "Shiyu Chang"
    ]
  },
  "https://openreview.net/forum?id=ongi2oe3Fr": {
    "title": "Continuous U-Net: Faster, Greater and Noiseless",
    "volume": "main",
    "abstract": "Image segmentation is a fundamental task in image analysis and clinical practice. The current state-of-the-art techniques are based on U-shape type encoder-decoder networks with skip connections called U-Net. Despite the powerful performance reported by existing U-Net type networks, they suffer from several major limitations. These issues include the hard coding of the receptive field size, compromising the performance and computational cost, as well as the fact that they do not account for inherent noise in the data. They have problems associated with discrete layers, and do not offer any theoretical underpinning. In this work we introduce continuous U-Net, a novel family of networks for image segmentation. Firstly, continuous U-Net is a continuous deep neural network that introduces new dynamic blocks modelled by second order ordinary differential equations. Secondly, we provide theoretical guarantees for our network demonstrating faster convergence, higher robustness and less sensitivity to noise. Thirdly, we derive qualitative measures to tailor-made segmentation tasks. We demonstrate, through extensive numerical and visual results, that our model outperforms existing U-Net blocks for several medical image segmentation benchmarking datasets",
    "checked": true,
    "id": "798ac4aac5175b453cb9f00b34b2574167ca6b7b",
    "semantic_title": "continuous u-net: faster, greater and noiseless",
    "citation_count": 3,
    "authors": [
      "Chun-Wun Cheng",
      "Christina Runkel",
      "Lihao Liu",
      "Raymond H. Chan",
      "Carola-Bibiane Schönlieb",
      "Angelica I Aviles-Rivero"
    ]
  },
  "https://openreview.net/forum?id=hw7inQwRxB": {
    "title": "Decentralized Decoupled Training for Federated Long-Tailed Learning",
    "volume": "main",
    "abstract": "In the real world, the data samples often follow a long-tailed distribution, which poses a great challenge for Federated Learning (FL). That is, when the data is decentralized and long-tailed, FL may produce a poorly-behaved global model that is severely biased towards the head classes with the majority of the training samples. To settle this issue, decoupled training has recently been introduced to FL. Decoupled training aims to re-balance the biased classifier after the normal instance-balanced training, and has achieved promising results in centralized long-tailed learning. The current study directly adopts the decoupled training idea on the server side by re-training the classifier on a set of pseudo features, due to the unavailability of a global balanced dataset in FL. Unfortunately, this practice restricts the capacity of decoupled training in federated long-tailed learning as the low-quality pseudo features lead to a sub-optimal classifier. In this work, motivated by the distributed characteristic of FL, we propose a decentralized decoupled training mechanism by leveraging the abundant real data stored in the local. Specifically, we integrate the local real data with the global gradient prototypes to form the local balanced datasets, and thus re-balance the classifier during the local training. Furthermore, we introduce a supplementary classifier in the training phase to help model the global data distribution, which addresses the problem of contradictory optimization goals caused by performing classifier re-balancing locally. Extensive experiments show that our method consistently outperforms the existing state-of-the-art methods in various settings. Our code is available at https://github.com/keven980716/Federated_Learning_Experiments",
    "checked": false,
    "id": "2a6dc461610ac1380efd64c06416a55bedbff36b",
    "semantic_title": "global balanced experts for federated long-tailed learning",
    "citation_count": 1,
    "authors": [
      "Wenkai Yang",
      "Deli Chen",
      "Hao Zhou",
      "Fandong Meng",
      "Jie Zhou",
      "Xu Sun"
    ]
  },
  "https://openreview.net/forum?id=bzTfO4mURl": {
    "title": "FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning",
    "volume": "main",
    "abstract": "Federated learning (FL) is an emerging paradigm in machine learning, where a shared model is collaboratively learned using data from multiple devices to mitigate the risk of data leakage. While recent studies posit that Vision Transformer (ViT) outperforms Convolutional Neural Networks (CNNs) in addressing data heterogeneity in FL, the specific architectural components that underpin this advantage have yet to be elucidated. In this paper, we systematically investigate the impact of different architectural elements, such as activation functions and normalization layers, on the performance within heterogeneous FL. Through rigorous empirical analyses, we are able to offer the first-of-its-kind general guidance on micro-architecture design principles for heterogeneous FL. Intriguingly, our findings indicate that with strategic architectural modifications, pure CNNs can achieve a level of robustness that either matches or even exceeds that of ViTs when handling heterogeneous data clients in FL. Additionally, our approach is compatible with existing FL techniques and delivers state-of-the-art solutions across a broad spectrum of FL benchmarks",
    "checked": true,
    "id": "428832c52f141b2d6efcc58d118d5f7e661b4b7e",
    "semantic_title": "fedconv: enhancing convolutional neural networks for handling data heterogeneity in federated learning",
    "citation_count": 0,
    "authors": [
      "Peiran Xu",
      "Zeyu Wang",
      "Jieru Mei",
      "Liangqiong Qu",
      "Alan Yuille",
      "Cihang Xie",
      "Yuyin Zhou"
    ]
  },
  "https://openreview.net/forum?id=Egb0tUZnOY": {
    "title": "Understanding Sparse Neural Networks from their Topology via Multipartite Graph Representations",
    "volume": "main",
    "abstract": "Pruning-at-Initialization (PaI) algorithms provide Sparse Neural Networks (SNNs) which are computationally more efficient than their dense counterparts, and try to avoid performance degradation. While much emphasis has been directed towards \\emph{how} to prune, we still do not know \\emph{what topological metrics} of the SNNs characterize \\emph{good performance}. From prior work, we have layer-wise topological metrics by which SNN performance can be predicted: the Ramanujan-based metrics. To exploit these metrics, proper ways to represent network layers via Graph Encodings (GEs) are needed, with Bipartite Graph Encodings (BGEs) being the \\emph{de-facto} standard at the current stage. Nevertheless, existing BGEs neglect the impact of the inputs, and do not characterize the SNN in an end-to-end manner. Additionally, thanks to a thorough study of the Ramanujan-based metrics, we discover that they are only as good as the \\emph{layer-wise density} as performance predictors, when paired with BGEs. To close both gaps, we design a comprehensive topological analysis for SNNs with both linear and convolutional layers, via (i) a new input-aware Multipartite Graph Encoding (MGE) for SNNs and (ii) the design of new end-to-end topological metrics over the MGE. With these novelties, we show the following: (a) The proposed MGE allows to extract topological metrics that are much better predictors of the accuracy drop than metrics computed from current input-agnostic BGEs; (b) Which metrics are important at different sparsity levels and for different architectures; (c) A mixture of our topological metrics can rank PaI algorithms more effectively than Ramanujan-based metrics",
    "checked": true,
    "id": "1b7c102ec84a2753c97adbe8f701a8a929b7423d",
    "semantic_title": "understanding sparse neural networks from their topology via multipartite graph representations",
    "citation_count": 1,
    "authors": [
      "Elia Cunegatti",
      "Matteo Farina",
      "Doina Bucur",
      "Giovanni Iacca"
    ]
  },
  "https://openreview.net/forum?id=m1OXBLH0dH": {
    "title": "Stochastic Direct Search Methods for Blind Resource Allocation",
    "volume": "main",
    "abstract": "Motivated by programmatic advertising optimization, we consider the task of sequentially allocating budget across a set of resources. At every time step, a feasible allocation is chosen and only a corresponding random return is observed. The goal is to maximize the cumulative expected sum of returns. This is a realistic model for budget allocation across subdivisions of marketing campaigns, with the objective of maximizing the number of conversions. We study direct search (also known as pattern search) methods for linearly constrained and derivative-free optimization in the presence of noise, which apply in particular to sequential budget allocation. These algorithms, which do not rely on hierarchical partitioning of the resource space, are easy to implement; they respect the operational constraints of resource allocation by avoiding evaluation outside of the feasible domain; and, they are also compatible with warm start by being (approximate) descent algorithms. However, they have not yet been analyzed from the perspective of cumulative regret. We show that direct search methods achieves finite regret in the deterministic and unconstrained case. In the presence of evaluation noise and linear constraints, we propose a simple extension of direct search that achieves a regret upper-bound of the order of $T^{2/3}$. We also propose an accelerated version of the algorithm, relying on repeated sequential testing, that significantly improves the practical behavior of the approach",
    "checked": false,
    "id": "751ae792b0234269af7064173c1a5f99039c887f",
    "semantic_title": "regret analysis of the stochastic direct search method for blind resource allocation",
    "citation_count": 1,
    "authors": [
      "Juliette Achddou",
      "Olivier Cappé",
      "Aurélien Garivier"
    ]
  },
  "https://openreview.net/forum?id=aHk3vctnf1": {
    "title": "Routers in Vision Mixture of Experts: An Empirical Study",
    "volume": "main",
    "abstract": "Mixture-of-Experts (MoE) models are a promising way to scale up model capacity without significantly increasing computational cost. A key component of MoEs is the router, which decides which subset of parameters (experts) process which feature embeddings (tokens). In this paper, we present a comprehensive study of routers in MoEs for computer vision tasks. We introduce a unified MoE formulation that subsumes different MoEs with two parametric routing tensors. This formulation covers both sparse MoE, which uses a binary or hard assignment between experts and tokens, and soft MoE, which uses a soft assignment between experts and weighted combinations of tokens. Routers for sparse MoEs can be further grouped into two variants: Token Choice, which matches experts to each token, and Expert Choice, which matches tokens to each expert. We conduct head-to-head experiments with 6 different routers, including existing routers from prior work and new ones we introduce. We show that (i) many routers originally developed for language modeling can be adapted to perform strongly in vision tasks, (ii) in sparse MoE, Expert Choice routers generally outperform Token Choice routers, and (iii) soft MoEs generally outperform sparse MoEs with a fixed compute budget. These results provide new insights regarding the crucial role of routers in vision MoE models",
    "checked": true,
    "id": "a12bc2ddaa3a91edab682f6822e381ce9323e8fc",
    "semantic_title": "routers in vision mixture of experts: an empirical study",
    "citation_count": 1,
    "authors": [
      "Tianlin Liu",
      "Mathieu Blondel",
      "Carlos Riquelme Ruiz",
      "Joan Puigcerver"
    ]
  },
  "https://openreview.net/forum?id=6rWuWbVmgz": {
    "title": "Sketch and shift: a robust decoder for compressive clustering",
    "volume": "main",
    "abstract": "Compressive learning is an emerging approach to drastically reduce the memory footprint of large-scale learning, by first summarizing a large dataset into a low-dimensional sketch vector, and then decoding from this sketch the latent information needed for learning. In light of recent progress on information preservation guarantees for sketches based on random features, a major objective is to design easy-to-tune algorithms (called decoders) to robustly and efficiently extract this information. To address the underlying non-convex optimization problems, various heuristics have been proposed. In the case of compressive clustering, the standard heuristic is CL-OMPR, a variant of sliding Frank-Wolfe. Yet, CL-OMPR is hard to tune, and the examination of its robustness was overlooked. In this work, we undertake a scrutinized examination of CL-OMPR to circumvent its limitations. In particular, we show how this algorithm can fail to recover the clusters even in advantageous scenarios. To gain insight, we show how the deficiencies of this algorithm can be attributed to optimization difficulties related to the structure of a correlation function appearing at core steps of the algorithm. To address these limitations, we propose an alternative decoder offering substantial improvements over CL-OMPR. Its design is notably inspired from the mean shift algorithm, a classic approach to detect the local maxima of kernel density estimators. The proposed algorithm can extract clustering information from a sketch of the MNIST dataset that is 10 times smaller than previously",
    "checked": true,
    "id": "93cbc331e39c2e2c89db51cb362e969fa7c4ed29",
    "semantic_title": "sketch and shift: a robust decoder for compressive clustering",
    "citation_count": 0,
    "authors": [
      "Ayoub Belhadji",
      "Rémi Gribonval"
    ]
  },
  "https://openreview.net/forum?id=y9IDfODRns": {
    "title": "Inference from Real-World Sparse Measurements",
    "volume": "main",
    "abstract": "Real-world problems often involve complex and unstructured sets of measurements, which occurs when sensors are sparsely placed in either space or time. Being able to model this irregular spatiotemporal data and extract meaningful forecasts is crucial. Deep learning architectures capable of processing sets of measurements with positions varying from set to set, and extracting readouts anywhere are methodologically difficult. Current state-of-the-art models are graph neural networks and require domain-specific knowledge for proper setup. We propose an attention-based model focused on robustness and practical applicability, with two key design contributions. First, we adopt a ViT-like transformer that takes both context points and read-out positions as inputs, eliminating the need for an encoder-decoder structure. Second, we use a unified method for encoding both context and read-out positions. This approach is intentionally straightforward and integrates well with other systems. Compared to existing approaches, our model is simpler, requires less specialized knowledge, and does not suffer from a problematic bottleneck effect, all of which contribute to superior performance. We conduct in-depth ablation studies that characterize this problematic bottleneck in the latent representations of alternative models that inhibit information utilization and impede training efficiency. We also perform experiments across various problem domains, including high-altitude wind nowcasting, two-day weather forecasting, fluid dynamics, and heat diffusion. Our attention-based model consistently outperforms state-of-the-art models in handling irregularly sampled data. Notably, our model reduces the root mean square error (RMSE) for wind nowcasting from 9.24 to 7.98 and for heat diffusion tasks from 0.126 to 0.084",
    "checked": true,
    "id": "3b758b93b682328168265bcbe49f857d17bd1bc3",
    "semantic_title": "inference from real-world sparse measurements",
    "citation_count": 0,
    "authors": [
      "Arnaud Pannatier",
      "Kyle Matoba",
      "François Fleuret"
    ]
  },
  "https://openreview.net/forum?id=dwFRov8xhr": {
    "title": "E-Valuating Classifier Two-Sample Tests",
    "volume": "main",
    "abstract": "We introduce a powerful deep classifier two-sample test for high-dimensional data based on E-values, called E-C2ST. Our test combines ideas from existing work on split likelihood ratio tests and predictive independence tests. The resulting E-values are suitable for anytime-valid sequential two-sample tests. This feature allows for more effective use of data in constructing test statistics. Through simulations and real data applications, we empirically demonstrate that E-C2ST achieves enhanced statistical power by partitioning datasets into multiple batches, beyond the conventional two-split (training and testing) approach of standard two-sample classifier tests. This strategy increases the power of the test, while keeping the type I error well below the desired significance level",
    "checked": true,
    "id": "05599cb143f6162030153abf069a74cb0b7b3fd5",
    "semantic_title": "e-valuating classifier two-sample tests",
    "citation_count": 9,
    "authors": [
      "Teodora Pandeva",
      "Tim Bakker",
      "Christian A. Naesseth",
      "Patrick Forré"
    ]
  },
  "https://openreview.net/forum?id=z5AXLMBWdU": {
    "title": "Semantic Positive Pairs for Enhancing Visual Representation Learning of Instance Discrimination methods",
    "volume": "main",
    "abstract": "Self-supervised learning algorithms (SSL) based on instance discrimination have shown promising results, performing competitively or even outperforming supervised learning counterparts in some downstream tasks. Such approaches employ data augmentation to create two views of the same instance (i.e., positive pairs) and encourage the model to learn good representations by attracting these views closer in the embedding space without collapsing to the trivial solution. However, data augmentation is limited in representing positive pairs, and the repulsion process between the instances during contrastive learning may discard important features for instances that have similar categories. To address this issue, we propose an approach to identify those images with similar semantic content and treat them as positive instances, thereby reducing the chance of discarding important features during representation learning and increasing the richness of the latent representation. Our approach is generic and could work with any self-supervised instance discrimination frameworks such as MoCo and SimSiam. To evaluate our method, we run experiments on three benchmark datasets: ImageNet, STL-10 and CIFAR-10 with different instance discrimination SSL approaches. The experimental results show that our approach consistently outperforms the baseline methods across all three datasets; for instance, we improve upon the vanilla MoCo-v2 by 4.1% on ImageNet under a linear evaluation protocol over 800 epochs. We also report results on semi-supervised learning, transfer learning on downstream tasks, and object detection",
    "checked": true,
    "id": "3f111aeefd8fbf3f6e0765ee11cbc1a7a8526c16",
    "semantic_title": "semantic positive pairs for enhancing visual representation learning of instance discrimination methods",
    "citation_count": 2,
    "authors": [
      "Mohammad Alkhalefi",
      "Georgios Leontidis",
      "Mingjun Zhong"
    ]
  },
  "https://openreview.net/forum?id=Ew73inSyhG": {
    "title": "What do larger image classifiers memorise?",
    "volume": "main",
    "abstract": "The success of modern neural networks has prompted study of the connection between memorisation and generalisation: overparameterised models generalise well, despite being able to perfectly fit (\"memorise\") completely random labels. To carefully study this issue, Feldman (2019) proposed a metric to quantify the degree of memorisation of individual training examples, and empirically computed the corresponding memorisation profile of a ResNet on image classification benchmarks. While an exciting first glimpse into what real-world models memorise, this leaves open a fundamental question: do larger neural models memorise more? This aligns with the common practice of training models of different sizes, each offering different cost-quality trade-offs: while larger models are typically observed to have higher quality, it is of interest to understand whether this is merely a consequence of them memorising larger numbers of input-output patterns. We present a comprehensive empirical analysis of this question on image classification benchmarks. We find that training examples exhibit an unexpectedly diverse set of memorisation trajectories across model sizes: most samples experienced decreased memorisation under larger models, while the rest exhibit cap-shaped or increasing memorisation. We show that various proxies for the Feldman(2019) memorisation score fail to capture these fundamental trends. Lastly, we find that knowledge distillation — an effective and popular model compression technique — tends to inhibit memorisation, while also improving generalisation. Specifically, memorisation is mostly inhibited on examples with increasing memorisation trajectories, thus pointing at how distillation improves generalisation",
    "checked": true,
    "id": "3e5e8d3e149049e3b04d42ccdb80596189f21ca3",
    "semantic_title": "what do larger image classifiers memorise?",
    "citation_count": 2,
    "authors": [
      "Michal Lukasik",
      "Vaishnavh Nagarajan",
      "Ankit Singh Rawat",
      "Aditya Krishna Menon",
      "Sanjiv Kumar"
    ]
  },
  "https://openreview.net/forum?id=PtBzWCaCYB": {
    "title": "Integrated Variational Fourier Features for Fast Spatial Modelling with Gaussian Processes",
    "volume": "main",
    "abstract": "Sparse variational approximations are popular methods for scaling up inference and learning in Gaussian processes to larger datasets. For $N$ training points, exact inference has $O(N^3)$ cost; with $M \\ll N$ features, state of the art sparse variational methods have $O(NM^2)$ cost. Recently, methods have been proposed using more sophisticated features; these promise $O(M^3)$ cost, with good performance in low dimensional tasks such as spatial modelling, but they only work with a very limited class of kernels, excluding some of the most commonly used. In this work, we propose integrated Fourier features, which extends these performance benefits to a very broad class of stationary covariance functions. We motivate the method and choice of parameters from a convergence analysis and empirical exploration, and show practical speedup in synthetic and real world spatial regression tasks",
    "checked": true,
    "id": "3b0f66f2cd56bec2d0bf1e79f47b4f909a8c5c42",
    "semantic_title": "integrated variational fourier features for fast spatial modelling with gaussian processes",
    "citation_count": 0,
    "authors": [
      "Talay M Cheema",
      "Carl Edward Rasmussen"
    ]
  },
  "https://openreview.net/forum?id=P1vzXDklar": {
    "title": "Time Series Continuous Modeling for Imputation and Forecasting with Implicit Neural Representations",
    "volume": "main",
    "abstract": "We introduce a novel modeling approach for time series imputation and forecasting, tailored to address the challenges often encountered in real-world data, such as irregular samples, missing data, or unaligned measurements from multiple sensors. Our method relies on a continuous-time-dependent model of the series' evolution dynamics. It leverages adaptations of conditional, implicit neural representations for sequential data. A modulation mechanism, driven by a meta-learning algorithm, allows adaptation to unseen samples and extrapolation beyond observed time-windows for long-term predictions. The model provides a highly flexible and unified framework for imputation and forecasting tasks across a wide range of challenging scenarios. It achieves state-of-the-art performance on classical benchmarks and outperforms alternative time-continuous models",
    "checked": true,
    "id": "ee190efa2896b59c80c820aa6dcc8e75d33182be",
    "semantic_title": "time series continuous modeling for imputation and forecasting with implicit neural representations",
    "citation_count": 3,
    "authors": [
      "Etienne Le Naour",
      "Louis Serrano",
      "Léon Migus",
      "Yuan Yin",
      "Ghislain Agoua",
      "Nicolas Baskiotis",
      "patrick gallinari",
      "Vincent Guigue"
    ]
  },
  "https://openreview.net/forum?id=tP1PBrMUlX": {
    "title": "Synthesizing Libraries of Programs with Auxiliary Functions",
    "volume": "main",
    "abstract": "A common approach to program synthesis is to use a learned function to guide the search for a program that satisfies the user's intent. In this paper, we propose a method that offers search guidance, through a domain-dependent auxiliary function, that can be orthogonal to the guidance previous functions provide. Our method, which we call Auxiliary-Based Library Learning (Aulile), searches for a solution in the program space using a base algorithm. If this search does not produce a solution, Aulile enhances the language with a library of programs discovered in the search that optimizes for the auxiliary function. Then, it repeats the search with this library-augmented language. This process is repeated until a solution is found or the system reaches a timeout. We evaluate Aulile in string manipulation tasks. Aulile improved, in some cases by a large margin, the performance of several base algorithms that use different search and learning strategies: Bus, Bustle, Crossbeam, and Bee Search. Our results suggest that Aulile offers an effective method of injecting domain knowledge into existing systems through a library learning scheme that optimizes for an auxiliary function",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Habibur Rahman",
      "Thirupathi Reddy Emireddy",
      "Kenneth Tjhia",
      "Elham Parhizkar",
      "Levi Lelis"
    ]
  },
  "https://openreview.net/forum?id=4KLwep6mA1": {
    "title": "Choosing Wisely and Learning Deeply: Selective Cross-Modality Distillation via CLIP for Domain Generalization",
    "volume": "main",
    "abstract": "Domain Generalization (DG), a crucial research area, seeks to train models across multiple domains and test them on unseen ones. In this paper, we introduce a novel approach, namely, Selective Cross-Modality Distillation for Domain Generalization (SCMD). SCMD leverages the capabilities of large vision-language models, specifically CLIP, to train a more efficient model, ensuring it acquires robust generalization capabilities across unseen domains. Our primary contribution is a unique selection framework strategically designed to identify hard-to-learn samples for distillation. In parallel, we introduce a novel cross-modality module that seamlessly combines the projected features of the student model with the text embeddings from CLIP, ensuring the alignment of similarity distributions. We assess SCMD's performance on various benchmarks, where it empowers a ResNet50 to deliver state-of-the-art performance, surpassing existing domain generalization methods. Furthermore, we provide a theoretical analysis of our selection strategy, offering deeper insight into its effectiveness and potential in the field of DG",
    "checked": true,
    "id": "bd259204065d95e510dbfbb1dc15e967f2c5746e",
    "semantic_title": "choosing wisely and learning deeply: selective cross-modality distillation via clip for domain generalization",
    "citation_count": 0,
    "authors": [
      "Jixuan Leng",
      "Yijiang Li",
      "Haohan Wang"
    ]
  },
  "https://openreview.net/forum?id=EBNJ33Fcrl": {
    "title": "Anticipatory Music Transformer",
    "volume": "main",
    "abstract": "We introduce anticipation: a method for constructing a controllable generative model of a temporal point process (the event process) conditioned asynchronously on realizations of a second, correlated process (the control process). We achieve this by interleaving sequences of events and controls, such that controls appear following stopping times in the event sequence. This work is motivated by problems arising in the control of symbolic music generation. We focus on infilling control tasks, whereby the controls are a subset of the events themselves, and conditional generation completes a sequence of events given the fixed control events. We train anticipatory infilling models using the large and diverse Lakh MIDI music dataset. These models match the performance of autoregressive models for prompted generation, with the additional capability to perform infilling control tasks, including accompaniment. Human evaluators report that an anticipatory model produces accompaniments with similar musicality to even music composed by humans over a 20-second clip",
    "checked": true,
    "id": "ba3adfca2d111a5d61ad928a214c4715e3470d50",
    "semantic_title": "anticipatory music transformer",
    "citation_count": 4,
    "authors": [
      "John Thickstun",
      "David Leo Wright Hall",
      "Chris Donahue",
      "Percy Liang"
    ]
  },
  "https://openreview.net/forum?id=ZFZnvGXXMm": {
    "title": "Fooling Contrastive Language-Image Pre-Trained Models with CLIPMasterPrints",
    "volume": "main",
    "abstract": "Models leveraging both visual and textual data such as Contrastive Language-Image Pre-training (CLIP), are the backbone of many recent advances in artificial intelligence. In this work, we show that despite their versatility, such models are vulnerable to what we refer to as fooling master images. Fooling master images are capable of maximizing the confidence score of a CLIP model for a significant number of widely varying prompts, while being either unrecognizable or unrelated to the attacked prompts for humans. We demonstrate how fooling master images can be mined using stochastic gradient descent, projected gradient descent, or gradient-free optimisation. Contrary to many common adversarial attacks, the gradient-free optimisation approach allows us to mine fooling examples even when the weights of the model are not accessible. We investigate the properties of the mined fooling master images, and find that images trained on a small number of image captions potentially generalize to a much larger number of semantically related captions. Finally, we evaluate possible mitigation strategies and find that vulnerability to fooling master examples appears to be closely related to a modality gap in contrastive pre-trained multi-modal networks",
    "checked": true,
    "id": "cfc208f167e1a4b6390e1732bda6f320a7cca28d",
    "semantic_title": "fooling contrastive language-image pre-trained models with clipmasterprints",
    "citation_count": 0,
    "authors": [
      "Matthias Freiberger",
      "Peter Kun",
      "Christian Igel",
      "Anders Sundnes Løvlie",
      "Sebastian Risi"
    ]
  },
  "https://openreview.net/forum?id=HSQTv3R8Iz": {
    "title": "A True-to-the-model Axiomatic Benchmark for Graph-based Explainers",
    "volume": "main",
    "abstract": "Regulators, researchers, and practitioners recognize the urgency of explainability in artificial intelligence systems, including the ones based on machine learning for graph-structured data. Despite the large number of proposals, however, a common understanding of what constitutes a good explanation is still lacking: different explainers often arrive at different conclusions on the same problem instance, making it hard for practitioners to choose among them. Furthermore, explainers often produce explanations through opaque logic hard to understand and assess -- ironically mirroring the black box nature they aim to elucidate. Recent proposals in the literature for benchmarking graph-based explainers typically involve embedding specific logic into data, training a black-box model, and then empirically assessing how well the explanation matches the embedded logic, i.e., they test truthfulness to the data. In contrast, we propose a true-to-the-model axiomatic framework for auditing explainers in the task of node classification on graphs. Our proposal hinges on the fundamental idea that an explainer should discern if a model relies on a particular feature for classifying a node. Building on this concept, we develop three types of white-box classifiers, with clear internal logic, that are relevant in real-world applications. We then formally prove that the set of features that can induce a change in the classification correctly corresponds to a ground-truth set of predefined important features. This property allows us to use the white-box classifiers to build a testing framework. We apply this framework to both synthetic and real data and evaluate various state-of-the-art explainers, thus characterizing their behavior. Our findings highlight how explainers often react in a rather counter-intuitive fashion to technical details that might be easily overlooked. Our approach offers valuable insights and recommended practices for selecting the right explainer given the task at hand, and for developing new methods for explaining graph-learning models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Corrado Monti",
      "Paolo Bajardi",
      "Francesco Bonchi",
      "André Panisson",
      "Alan Perotti"
    ]
  },
  "https://openreview.net/forum?id=8bnsoL2IyJ": {
    "title": "Variance-aware decision making with linear function approximation under heavy-tailed rewards",
    "volume": "main",
    "abstract": "This paper studies how to achieve variance-aware regrets for online decision-making in the presence of heavy-tailed rewards with only finite variances. For linear stochastic bandits, we address the issue of heavy-tailed rewards by modifying the adaptive Huber regression and proposing AdaOFUL. AdaOFUL achieves a state-of-the-art regret bound of $\\widetilde{\\mathcal{O}}\\big(d\\big(\\sum_{t=1}^T \\nu_{t}^2\\big)^{1/2}+d\\big)$ as if the rewards were uniformly bounded, where $\\nu_{t}^2$ is the conditional variance of the reward at round $t$, $d$ is the feature dimension, {and $T$ is number of online rounds}. Building upon AdaOFUL, we propose VARA for linear MDPs, which achieves a variance-aware regret bound of $\\widetilde{\\mathcal{O}}(d\\sqrt{H\\mathcal{G}^*K})$. Here, $H$ is the length of episodes, $K$ is the number of episodes, and $\\mathcal{G}^*$ is a smaller instance-dependent quantity that can be bounded by other instance-dependent quantities when additional structural conditions on the MDP are satisfied. Overall, our modified adaptive Huber regression algorithm may serve as a useful building block in the design of algorithms for online problems with heavy-tailed rewards",
    "checked": false,
    "id": "d680fc0ea2eaa94239993f4328468703ac92eff7",
    "semantic_title": "variance-aware robust reinforcement learning with linear function approximation under heavy-tailed rewards",
    "citation_count": 7,
    "authors": [
      "Xiang Li",
      "Qiang Sun"
    ]
  },
  "https://openreview.net/forum?id=QSvb6jBXML": {
    "title": "ZigZag: Universal Sampling-free Uncertainty Estimation Through Two-Step Inference",
    "volume": "main",
    "abstract": "Whereas the ability of deep networks to produce useful predictions on many kinds of data has been amply demonstrated, estimating the reliability of these predictions remains challenging. Sampling approaches such as MC-Dropout and Deep Ensembles have emerged as the most popular ones for this purpose. Unfortunately, they require many forward passes at inference time, which slows them down. Sampling-free approaches can be faster but often suffer from other drawbacks, such as lower reliability of uncertainty estimates, difficulty of use, and limited applicability to different types of tasks and data. In this work, we introduce a sampling-free approach that is generic and easy to deploy, while producing reliable uncertainty estimates on par with state-of-the-art methods at a significantly lower computational cost. It is predicated on training the network to produce the same output with and without additional information about it. At inference time, when no prior information is given, we use the network's own prediction as the additional information. We then take the distance between the predictions with and without prior information as our uncertainty measure. We demonstrate our approach on several classification and regression tasks. We show that it delivers results on par with those of Ensembles but at a much lower computational cost",
    "checked": true,
    "id": "321aa1e91d2665d9fdada0737c3c0f6ba158eec3",
    "semantic_title": "zigzag: universal sampling-free uncertainty estimation through two-step inference",
    "citation_count": 2,
    "authors": [
      "Nikita Durasov",
      "Nik Dorndorf",
      "Hieu Le",
      "Pascal Fua"
    ]
  },
  "https://openreview.net/forum?id=wTGjn7JvYK": {
    "title": "On the Optimization and Generalization of Multi-head Attention",
    "volume": "main",
    "abstract": "The training and generalization dynamics of the Transformer's core mechanism, namely the Attention mechanism, remain under-explored. Besides, existing analyses primarily focus on single-head attention. Inspired by the demonstrated benefits of overparameterization when training fully-connected networks, we investigate the potential optimization and generalization advantages of using multiple attention heads. Towards this goal, we derive convergence and generalization guarantees for gradient-descent training of a single-layer multi-head self-attention model, under a suitable realizability condition on the data. We then establish primitive conditions on the initialization that ensure realizability holds. Finally, we demonstrate that these conditions are satisfied for a simple tokenized-mixture model. We expect the analysis can be extended to various data-model and architecture variations",
    "checked": true,
    "id": "6cee47349bf526bd63ee62da15b3c88701f97f15",
    "semantic_title": "on the optimization and generalization of multi-head attention",
    "citation_count": 11,
    "authors": [
      "Puneesh Deora",
      "Rouzbeh Ghaderi",
      "Hossein Taheri",
      "Christos Thrampoulidis"
    ]
  },
  "https://openreview.net/forum?id=uCZJaqJchs": {
    "title": "Personalised Federated Learning On Heterogeneous Feature Spaces",
    "volume": "main",
    "abstract": "Personalised federated learning (FL) approaches assume that raw data of all clients are defined in a common space \\emph{i.e.} all clients store their data according to the same schema. For real-world applications, this assumption is restrictive as clients, having their own systems to collect and then store data, may use {\\em heterogeneous} data representations. To bridge the gap between the assumption of a shared subspace and the more realistic situation of client-specific spaces, we propose a general framework coined FLIC that maps client's data onto a common feature space via local embedding functions, in a federated manner. Preservation of class information in the latent space is ensured by a distribution alignment with respect to a learned reference distribution. We provide the algorithmic details of FLIC as well as theoretical insights supporting the relevance of our methodology. We compare its performances against FL benchmarks involving heterogeneous input features spaces. Notably, we are the first to present a successful application of FL to Brain-Computer Interface signals acquired on a different number of sensors",
    "checked": true,
    "id": "abbc7f2b10d63c82b4bb5da61237078da3b55d76",
    "semantic_title": "personalised federated learning on heterogeneous feature spaces",
    "citation_count": 7,
    "authors": [
      "Alain Rakotomamonjy",
      "Maxime Vono",
      "Hamlet Jesse Medina Ruiz",
      "Liva Ralaivola"
    ]
  },
  "https://openreview.net/forum?id=qYceFeHgm4": {
    "title": "MUBen: Benchmarking the Uncertainty of Molecular Representation Models",
    "volume": "main",
    "abstract": "Large molecular representation models pre-trained on massive unlabeled data have shown great success in predicting molecular properties. However, these models may tend to overfit the fine-tuning data, resulting in over-confident predictions on test data that fall outside of the training distribution. To address this issue, uncertainty quantification (UQ) methods can be used to improve the models' calibration of predictions. Although many UQ approaches exist, not all of them lead to improved performance. While some studies have included UQ to improve molecular pre-trained models, the process of selecting suitable backbone and UQ methods for reliable molecular uncertainty estimation remains underexplored. To address this gap, we present MUBen, which evaluates different UQ methods for state-of-the-art backbone molecular representation models to investigate their capabilities. By fine-tuning various backbones using different molecular descriptors as inputs with UQ methods from different categories, we assess the influence of architectural decisions and training strategies on property prediction and uncertainty estimation. Our study offers insights for selecting UQ for backbone models, which can facilitate research on uncertainty-critical applications in fields such as materials science and drug discovery",
    "checked": true,
    "id": "d8c0ff00e7d8094cd00493923eea3dfffbc9e8ed",
    "semantic_title": "muben: benchmarking the uncertainty of molecular representation models",
    "citation_count": 3,
    "authors": [
      "Yinghao Li",
      "Lingkai Kong",
      "Yuanqi Du",
      "Yue Yu",
      "Yuchen Zhuang",
      "Wenhao Mu",
      "Chao Zhang"
    ]
  },
  "https://openreview.net/forum?id=BRl7fqMwaJ": {
    "title": "GSURE-Based Diffusion Model Training with Corrupted Data",
    "volume": "main",
    "abstract": "Diffusion models have demonstrated impressive results in both data generation and downstream tasks such as inverse problems, text-based editing, classification, and more. However, training such models usually requires large amounts of clean signals which are often difficult or impossible to obtain. In this work, we propose a novel training technique for generative diffusion models based only on corrupted data. We introduce a loss function based on the Generalized Stein's Unbiased Risk Estimator (GSURE), and prove that under some conditions, it is equivalent to the training objective used in fully supervised diffusion models. We demonstrate our technique on face images as well as Magnetic Resonance Imaging (MRI), where the use of undersampled data significantly alleviates data collection costs. Our approach achieves generative performance comparable to its fully supervised counterpart without training on any clean signals. In addition, we deploy the resulting diffusion model in various downstream tasks beyond the degradation present in the training set, showcasing promising results",
    "checked": true,
    "id": "6b6920ea541049f0035750ed9a2a2224c585f8c7",
    "semantic_title": "gsure-based diffusion model training with corrupted data",
    "citation_count": 10,
    "authors": [
      "Bahjat Kawar",
      "Noam Elata",
      "Tomer Michaeli",
      "Michael Elad"
    ]
  },
  "https://openreview.net/forum?id=2la55BeWwy": {
    "title": "A note on regularised NTK dynamics with an application to PAC-Bayesian training",
    "volume": "main",
    "abstract": "We establish explicit dynamics for neural networks whose training objective has a regularising term that constrains the parameters to remain close to their initial value. This keeps the network in a lazy training regime, where the dynamics can be linearised around the initialisation. The standard neural tangent kernel (NTK) governs the evolution during the training in the infinite-width limit, although the regularisation yields an additional term appears in the differential equation describing the dynamics. This setting provides an appropriate framework to study the evolution of wide networks trained to optimise generalisation objectives such as PAC-Bayes bounds, and hence contribute to a deeper theoretical understanding of such networks",
    "checked": true,
    "id": "9708d5b168cda3f07ed10fe4ee74fa9d35088ddc",
    "semantic_title": "a note on regularised ntk dynamics with an application to pac-bayesian training",
    "citation_count": 0,
    "authors": [
      "Eugenio Clerico",
      "Benjamin Guedj"
    ]
  },
  "https://openreview.net/forum?id=PuhF0hyDq1": {
    "title": "New Evaluation Metrics Capture Quality Degradation due to LLM Watermarking",
    "volume": "main",
    "abstract": "With the increasing use of large-language models (LLMs) like ChatGPT, watermarking has emerged as a promising approach for tracing machine-generated content. However, research on LLM watermarking often relies on simple perplexity or diversity-based measures to assess the quality of watermarked text, which can mask important limitations in watermarking. Here we introduce two new easy-to-use methods for evaluating watermarking algorithms for LLMs: 1) evaluation by LLM-judger with specific guidelines; and 2) binary classification on text embeddings to distinguish between watermarked and unwatermarked text. We apply these methods to characterize the effectiveness of current watermarking techniques. Our experiments, conducted across various datasets, reveal that current watermarking methods are moderately detectable by even simple classifiers, challenging the notion of watermarking subtlety. We also found, through the LLM judger, that watermarking impacts text quality, especially in degrading the coherence and depth of the response. Our findings underscore the trade-off between watermark robustness and text quality and highlight the importance of having more informative metrics to assess watermarking quality",
    "checked": true,
    "id": "92105d2f3716786528bd8e7498e13162b409cd37",
    "semantic_title": "new evaluation metrics capture quality degradation due to llm watermarking",
    "citation_count": 1,
    "authors": [
      "Karanpartap Singh",
      "James Zou"
    ]
  },
  "https://openreview.net/forum?id=YY2iA0hfia": {
    "title": "Does Representation Similarity Capture Function Similarity?",
    "volume": "main",
    "abstract": "Representation similarity metrics are widely used to compare learned representations in neural networks, as is evident in extensive literature investigating metrics that accurately capture information encoded in representations. However, aiming to capture all of the information available in representations may have little to do with what information is actually used by the downstream network. One solution is to experiment with interventions on network function. By ablating groups of units thought to carry information and observing whether those ablations affect network performance, we can focus on an outcome that mechanistically links representations to function. In this paper, we systematically test representation similarity metrics to evaluate their sensitivity to functional changes induced by ablation. We use network performance changes after ablation as a way to measure the influence of representation on function. These measures of function allow us to test how well similarity metrics capture changes in network performance versus changes to linear decodability. Network performance measures index the information used by the downstream network, while linear decoding methods index available information in the representation. We show that all of the tested metrics are more sensitive to decodable features than network performance. When comparing these metrics, Procrustes and CKA outperform regularized CCA-based methods on average. Although Procrustes and CKA outperform on average, these metrics have a diminished advantage when looking at network performance. We provide ablation tests of the utility of different representational similarity metrics. Our results suggest that interpretability methods will be more effective if they are based on representational similarity metrics that have been evaluated using ablation tests",
    "checked": false,
    "id": "e931235db8053bf87399bef6fe70b906bfc2ef63",
    "semantic_title": "an encompassed representation of timescale hierarchies in first-order reaction network",
    "citation_count": 0,
    "authors": [
      "Lucas Hayne",
      "Heejung Jung",
      "R. Carter"
    ]
  },
  "https://openreview.net/forum?id=TZdEgwZ6f3": {
    "title": "Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA",
    "volume": "main",
    "abstract": "Recent works demonstrate a remarkable ability to customize text-to-image diffusion models while only providing a few example images. What happens if you try to customize such models using multiple, fine-grained concepts in a sequential (i.e., continual) manner? In our work, we show that recent state-of-the-art customization of text-to-image models suffer from catastrophic forgetting when new concepts arrive sequentially. Specifically, when adding a new concept, the ability to generate high quality images of past, similar concepts degrade. To circumvent this forgetting, we propose a new method, C-LoRA, composed of a continually self-regularized low-rank adaptation in cross attention layers of the popular Stable Diffusion model. Furthermore, we use customization prompts which do not include the word of the customized object (i.e., \"person\" for a human face dataset) and are initialized as completely random embeddings. Importantly, our method induces only marginal additional parameter costs and requires no storage of user data for replay. We show that C-LoRA not only outperforms several baselines for our proposed setting of text-to-image continual customization, which we refer to as Continual Diffusion, but that we achieve a new state-of-the-art in the well-established rehearsal-free continual learning setting for image classification. The high achieving performance of C-LoRA in two separate domains positions it as a compelling solution for a wide range of applications, and we believe it has significant potential for practical impact",
    "checked": true,
    "id": "d0590894ef9edb5fce917cb8221e5ab0226522a9",
    "semantic_title": "continual diffusion: continual customization of text-to-image diffusion with c-lora",
    "citation_count": 45,
    "authors": [
      "James Seale Smith",
      "Yen-Chang Hsu",
      "Lingyu Zhang",
      "Ting Hua",
      "Zsolt Kira",
      "Yilin Shen",
      "Hongxia Jin"
    ]
  },
  "https://openreview.net/forum?id=3kYgouAfqk": {
    "title": "BP($\\mathbf{\\lambda}$): Online Learning via Synthetic Gradients",
    "volume": "main",
    "abstract": "Training recurrent neural networks typically relies on backpropagation through time (BPTT). BPTT depends on forward and backward passes to be completed, rendering the network locked to these computations before loss gradients are available. Recently, Jaderberg et al. proposed synthetic gradients to alleviate the need for full BPTT. In their implementation synthetic gradients are learned through a mixture of backpropagated gradients and bootstrapped synthetic gradients, analogous to the temporal difference (TD) algorithm in Reinforcement Learning (RL). However, as in TD learning, heavy use of bootstrapping can result in bias which leads to poor synthetic gradient estimates. Inspired by the accumulate $\\mathrm{TD}(\\lambda)$ in RL, we propose a fully online method for learning synthetic gradients which avoids the use of BPTT altogether: \\emph{accumulate} $BP(\\lambda)$. As in accumulate $\\mathrm{TD}(\\lambda)$, we show analytically that {accumulate~$\\mathrm{BP}(\\lambda)$} can control the level of bias by using a mixture of temporal difference errors and recursively defined eligibility traces. We next demonstrate empirically that our model outperforms the original implementation for learning synthetic gradients in a variety of tasks, and is particularly suited for capturing longer timescales. Finally, building on recent work we reflect on accumulate $\\mathrm{BP}(\\lambda)$ as a principle for learning in biological circuits. In summary, inspired by RL principles we introduce an algorithm capable of bias-free online learning via synthetic gradients",
    "checked": false,
    "id": "956b83eff99811bb0c782e4b90e64e95eef30117",
    "semantic_title": "bp(λ): online learning via synthetic gradients",
    "citation_count": 0,
    "authors": [
      "Joseph Oliver Pemberton",
      "Rui Ponte Costa"
    ]
  },
  "https://openreview.net/forum?id=kZFKwApeQO": {
    "title": "GUARD: A Safe Reinforcement Learning Benchmark",
    "volume": "main",
    "abstract": "Due to the trial-and-error nature, it is typically challenging to apply RL algorithms to safety-critical real-world applications, such as autonomous driving, human-robot interaction, robot manipulation, etc, where such errors are not tolerable. Recently, safe RL (i.e. constrained RL) has emerged rapidly in the literature, in which the agents explore the environment while satisfying constraints. Due to the diversity of algorithms and tasks, it remains difficult to compare existing safe RL algorithms. To fill that gap, we introduce GUARD, a Generalized Unified SAfe Reinforcement Learning Development Benchmark. GUARD has several advantages compared to existing benchmarks. First, GUARD is a generalized benchmark with a wide variety of RL agents, tasks, and safety constraint specifications. Second, GUARD comprehensively covers state-of-the-art safe RL algorithms with self-contained implementations. Third, GUARD is highly customizable in tasks and algorithms. We present a comparison of state-of-the-art on-policy safe RL algorithms in various task settings using GUARD and establish baselines that future work can build on",
    "checked": true,
    "id": "498d75d17609af4d5f235654423d90bbe3a5c621",
    "semantic_title": "guard: a safe reinforcement learning benchmark",
    "citation_count": 6,
    "authors": [
      "Weiye Zhao",
      "Yifan Sun",
      "Feihan Li",
      "Rui Chen",
      "Ruixuan Liu",
      "Tianhao Wei",
      "Changliu Liu"
    ]
  },
  "https://openreview.net/forum?id=IzmLJ1t49R": {
    "title": "Incremental Extractive Opinion Summarization Using Cover Trees",
    "volume": "main",
    "abstract": "Extractive opinion summarization involves automatically producing a summary of text about an entity (e.g., a product's reviews) by extracting representative sentences that capture prevalent opinions in the review set. Typically, in online marketplaces user reviews accrue over time, and opinion summaries must be updated periodically to provide customers with up-to-date information. In this work, we study the task of extractive opinion summarization in an incremental setting, where the underlying review set evolves over time. Many of the state-of-the-art extractive opinion summarization approaches are centrality-based, such as CentroidRank (Radev et al., 2004; Chowdhury et al., 2022). CentroidRank performs extractive summarization by selecting a subset of review sentences closest to the centroid in the representation space as the summary. However, these methods are not capable of operating efficiently in an incremental setting, where reviews arrive one at a time. In this paper, we present an efficient algorithm for accurately computing the CentroidRank summaries in an incremental setting. Our approach, CoverSumm, relies on indexing review representations in a cover tree and maintaining a reservoir of candidate summary review sentences. CoverSumm's efficacy is supported by a theoretical and empirical analysis of running time. Empirically, on a diverse collection of data (both real and synthetically created to illustrate scaling considerations), we demonstrate that CoverSumm is up to 25x faster than baseline methods, and capable of adapting to nuanced changes in data distribution. We also conduct human evaluations of the generated summaries and find that CoverSumm is capable of producing informative summaries consistent with the underlying review set",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Somnath Basu Roy Chowdhury",
      "Nicholas Monath",
      "Kumar Avinava Dubey",
      "Manzil Zaheer",
      "Andrew McCallum",
      "Amr Ahmed",
      "Snigdha Chaturvedi"
    ]
  },
  "https://openreview.net/forum?id=qunyX9WYr6": {
    "title": "Persistent Local Homology in Graph Learning",
    "volume": "main",
    "abstract": "In this study, we introduce Persistent Local Homology (PLH) for graphs, a novel method that synergizes persistent homology with local homology to analyze graph structures. We begin by mathematically formalizing PLH, defining it as the application of persistent homology to annular local subgraphs. This foundation paves the way for the development of a computational pipeline, specifically tailored for PLH, which we explore in various graph learning contexts. Despite its utility, a complexity analysis reveals potential computational bottlenecks in PLH application. To address this, we propose Reduced PLH (rPLH), an efficient variant designed to significantly lower computational complexity. Experimental evaluations with rPLH demonstrate its capability to retain the effectiveness of the original PLH while substantially reducing computational demands. The practical utility of PLH and rPLH is further corroborated through comprehensive experiments on both synthetic and real-world datasets, highlighting their broad applicability and potential in diverse analytical scenarios",
    "checked": false,
    "id": "9b304a749911503b991701d354c522372ae4e0af",
    "semantic_title": "on the expressivity of persistent homology in graph learning",
    "citation_count": 6,
    "authors": [
      "Minghua Wang",
      "Yan HU",
      "Ziyun Huang",
      "Di Wang",
      "Jinhui Xu"
    ]
  },
  "https://openreview.net/forum?id=I8FMYa2BdP": {
    "title": "Adversarially Robust Spiking Neural Networks Through Conversion",
    "volume": "main",
    "abstract": "Spiking neural networks (SNNs) provide an energy-efficient alternative to a variety of artificial neural network (ANN) based AI applications. As the progress in neuromorphic computing with SNNs expands their use in applications, the problem of adversarial robustness of SNNs becomes more pronounced. To the contrary of the widely explored end-to-end adversarial training based solutions, we address the limited progress in scalable robust SNN training methods by proposing an adversarially robust ANN-to-SNN conversion algorithm. Our method provides an efficient approach to embrace various computationally demanding robust learning objectives that have been proposed for ANNs. During a post-conversion robust finetuning phase, our method adversarially optimizes both layer-wise firing thresholds and synaptic connectivity weights of the SNN to maintain transferred robustness gains from the pre-trained ANN. We perform experimental evaluations in a novel setting proposed to rigorously assess the robustness of SNNs, where numerous adaptive adversarial attacks that account for the spike-based operation dynamics are considered. Results show that our approach yields a scalable state-of-the-art solution for adversarially robust deep SNNs with low-latency",
    "checked": true,
    "id": "075ad710f548810560e2844b39f6e89147cbc34c",
    "semantic_title": "adversarially robust spiking neural networks through conversion",
    "citation_count": 1,
    "authors": [
      "Ozan Ozdenizci",
      "Robert Legenstein"
    ]
  },
  "https://openreview.net/forum?id=ekvsBtCBUK": {
    "title": "Anomaly detection with semi-supervised classification based on risk estimators",
    "volume": "main",
    "abstract": "A significant limitation of one-class classification anomaly detection methods is their reliance on the assumption that unlabeled training data only contains normal instances. To overcome this impractical assumption, we propose two novel classification-based anomaly detection methods. Firstly, we introduce a semi-supervised shallow anomaly detection method based on an unbiased risk estimator. Secondly, we present a semi-supervised deep anomaly detection method utilizing a nonnegative (biased) risk estimator. We establish estimation error bounds and excess risk bounds for both risk minimizers. Additionally, we propose techniques to select appropriate regularization parameters that ensure the nonnegativity of the empirical risk in the shallow model under specific loss functions. Our extensive experiments provide evidence of the effectiveness of the risk-based anomaly detection methods",
    "checked": true,
    "id": "47456375b49e94df090ec87ccbfa7ecfa2ecc803",
    "semantic_title": "anomaly detection with semi-supervised classification based on risk estimators",
    "citation_count": 0,
    "authors": [
      "Le Thi Khanh Hien",
      "Sukanya Patra",
      "Souhaib Ben Taieb"
    ]
  },
  "https://openreview.net/forum?id=2M9CUnYnBA": {
    "title": "Exponential Moving Average of Weights in Deep Learning: Dynamics and Benefits",
    "volume": "main",
    "abstract": "Weight averaging of Stochastic Gradient Descent (SGD) iterates is a popular method for training deep learning models. While it is often used as part of complex training pipelines to improve generalization or serve as a `teacher' model, weight averaging lacks proper evaluation on its own. In this work, we present a systematic study of the Exponential Moving Average (EMA) of weights. We first explore the training dynamics of EMA, give guidelines for hyperparameter tuning, and highlight its good early performance, partly explaining its success as a teacher. We also observe that EMA requires less learning rate decay compared to SGD since averaging naturally reduces noise, introducing a form of implicit regularization. Through extensive experiments, we show that EMA solutions differ from last-iterate solutions. EMA models not only generalize better but also exhibit improved i) robustness to noisy labels, ii) prediction consistency, iii) calibration and iv) transfer learning. Therefore, we suggest that an EMA of weights is a simple yet effective plug-in to improve the performance of deep learning models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Morales-Brotons",
      "Thijs Vogels",
      "Hadrien Hendrikx"
    ]
  },
  "https://openreview.net/forum?id=qH4YFMyhce": {
    "title": "Scalable Hierarchical Self-Attention with Learnable Hierarchy for Long-Range Interactions",
    "volume": "main",
    "abstract": "Self-attention models have made great strides toward accurately modeling a wide array of data modalities, including, more recently, graph-structured data. This paper demonstrates that adaptive hierarchical attention can go a long way toward successfully applying transformers to graphs. Our proposed model Sequoia provides a powerful inductive bias towards long-range interaction modeling, leading to better generalization. We propose an end-to-end mechanism for a data-dependent construction of a hierarchy which in turn guides the self-attention mechanism. Using adaptive hierarchy provides a natural pathway toward sparse attention by constraining node-to-node interactions with the immediate family of each node in the hierarchy (e.g., parent, children, and siblings). This in turn dramatically reduces the computational complexity of a self-attention layer from quadratic to log-linear in terms of the input size while maintaining or sometimes even surpassing the standard transformer's ability to model long-range dependencies across the entire input. Experimentally, we report state-of-the-art performance on long-range graph benchmarks while remaining computationally efficient. Moving beyond graphs, we also display competitive performance on long-range sequence modeling, point-clouds classification, and segmentation when using a fixed hierarchy. Our source code is publicly available at https://github.com/HySonLab/HierAttention",
    "checked": true,
    "id": "0fa6bd29f51b82c3fe6e275129e4518ba646b59b",
    "semantic_title": "scalable hierarchical self-attention with learnable hierarchy for long-range interactions",
    "citation_count": 0,
    "authors": [
      "Thuan Nguyen Anh Trang",
      "Khang Nhat Ngo",
      "Hugo Sonnery",
      "Thieu Vo",
      "Siamak Ravanbakhsh",
      "Truong Son Hy"
    ]
  },
  "https://openreview.net/forum?id=AoOi9Zgdsv": {
    "title": "The Cross-entropy of Piecewise Linear Probability Density Functions",
    "volume": "main",
    "abstract": "The cross-entropy and its related terms from information theory (e.g.~entropy, Kullback–Leibler divergence) are used throughout artificial intelligence and machine learning. This includes many of the major successes, both current and historic, where they commonly appear as the natural objective of an optimisation procedure for learning model parameters, or their distributions. This paper presents a novel derivation of the differential cross-entropy between two 1D probability density functions represented as piecewise linear functions. Implementation challenges are resolved and experimental validation is presented, including a rigorous analysis of accuracy and a demonstration of using the presented result as the objective of a neural network. Previously, cross-entropy would need to be approximated via numerical integration, or equivalent, for which calculating gradients is impractical. Machine learning models with high parameter counts are optimised primarily with gradients, so if piecewise linear density representations are to be used then the presented analytic solution is essential. This paper contributes the necessary theory for the practical optimisation of information theoretic objectives when dealing with piecewise linear distributions directly. Removing this limitation expands the design space for future algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom S. F. Haines"
    ]
  },
  "https://openreview.net/forum?id=QvipGVdE6L": {
    "title": "3D Molecular Generation via Virtual Dynamics",
    "volume": "main",
    "abstract": "Structure-based drug design, a critical aspect of drug discovery, aims to identify high-affinity molecules for target protein pockets. Traditional virtual screening methods, which involve exhaustive searches within large molecular databases, are inefficient and limited in discovering novel molecules. The pocket-based 3D molecular generation model offers a promising alternative by directly generating molecules with 3D structures and binding positions in the pocket. In this paper, we present VD-Gen, a novel pocket-based 3D molecular generation pipeline. VD-Gen features a series of carefully designed stages to generate fine-grained 3D molecules with binding positions in the pocket cavity end-to-end. Rather than directly generating or sampling atoms with 3D positions in the pocket, VD-Gen randomly initializes multiple virtual particles within the pocket and learns to iteratively move them to approximate the distribution of molecular atoms in 3D space. After the iterative movement, a 3D molecule is extracted and further refined through additional iterative movement, yielding a high-quality 3D molecule with a confidence score. Comprehensive experimental results on pocket-based molecular generation demonstrate that VD-Gen can generate novel 3D molecules that fill the target pocket cavity with high binding affinities, significantly outperforming previous baselines",
    "checked": true,
    "id": "2bc52556e167ab5a84f032963a4dcbee5246df09",
    "semantic_title": "3d molecular generation via virtual dynamics",
    "citation_count": 4,
    "authors": [
      "Shuqi Lu",
      "Lin Yao",
      "Xi Chen",
      "Hang Zheng",
      "Di He",
      "Guolin Ke"
    ]
  },
  "https://openreview.net/forum?id=KokkP2nQ24": {
    "title": "How good is Good-Turing for Markov samples?",
    "volume": "main",
    "abstract": "The Good-Turing (GT) estimator for the missing mass (i.e., total probability of missing symbols) in $n$ samples is the number of symbols that appeared exactly once divided by $n$. For i.i.d. samples, the bias and squared-error risk of the GT estimator can be shown to fall as $1/n$ by bounding the expected error uniformly over all symbols. In this work, we study convergence of the GT estimator for missing stationary mass (i.e., total stationary probability of missing symbols) of Markov samples on an alphabet $\\mathcal{X}$ with stationary distribution $[\\pi_x:x\\in\\cX]$ and transition probability matrix (t.p.m.) $P$. This is an important and interesting problem because GT is widely used in applications with temporal dependencies such as language models assigning probabilities to word sequences, which are modelled as Markov. We show that convergence of GT depends on convergence of $(P^{\\sim x})^n$, where $P^{\\sim x}$ is $P$ with the $x$-th column zeroed out. This, in turn, depends on the Perron eigenvalue $\\lambda^{\\sim x}$ of $P^{\\sim x}$ and its relationship with $\\pi_x$ uniformly over $x$. For randomly generated t.p.ms and t.p.ms derived from New York Times and Charles Dickens corpora, we numerically exhibit such uniform-over-$x$ relationships between $\\lambda^{\\sim x}$ and $\\pi_x$. This supports the observed success of GT in language models and practical text data scenarios. For Markov chains with rank-2, diagonalizable t.p.ms having spectral gap $\\beta$, we show minimax rate upper and lower bounds of $1/(n\\beta^5)$ and $1/(n\\beta)$, respectively, for the estimation of stationary missing mass. This theoretical result extends the $1/n$ minimax rate for i.i.d. or rank-1 t.p.ms to rank-2 Markov, and is a first such minimax rate result for missing mass of Markov samples. We also show, through experiments, that the MSE of GT decays at a slower rate as the rank of the t.p.m increases",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prafulla Chandra",
      "Andrew Thangaraj",
      "Nived Rajaraman"
    ]
  },
  "https://openreview.net/forum?id=NgK5etmhz9": {
    "title": "State-wise Constrained Policy Optimization",
    "volume": "main",
    "abstract": "Reinforcement Learning (RL) algorithms have shown tremendous success in simulation environments, but their application to real-world problems faces significant challenges, with safety being a major concern. In particular, enforcing state-wise constraints is essential for many challenging tasks such as autonomous driving and robot manipulation. However, existing safe RL algorithms under the framework of Constrained Markov Decision Process (CMDP) do not consider state-wise constraints. To address this gap, we propose State-wise Constrained Policy Optimization (SCPO), the first general-purpose policy search algorithm for state-wise constrained reinforcement learning. SCPO provides guarantees for state-wise constraint satisfaction in expectation. In particular, we introduce the framework of Maximum Markov Decision Process, and prove that the worst-case safety violation is bounded under SCPO. We demonstrate the effectiveness of our approach on training neural network policies for extensive robot locomotion tasks, where the agent must satisfy a variety of state-wise safety constraints. Our results show that SCPO significantly outperforms existing methods and can handle state-wise constraints in high-dimensional robotics tasks",
    "checked": true,
    "id": "9af5047a2db2bbe3e3fea322a7e66bf86b0f4e97",
    "semantic_title": "state-wise constrained policy optimization",
    "citation_count": 5,
    "authors": [
      "Weiye Zhao",
      "Rui Chen",
      "Yifan Sun",
      "Feihan Li",
      "Tianhao Wei",
      "Changliu Liu"
    ]
  },
  "https://openreview.net/forum?id=3ludyxPbb6": {
    "title": "Provable Membership Inference Privacy",
    "volume": "main",
    "abstract": "In applications involving sensitive data, such as finance and healthcare, the necessity for preserving data privacy can be a significant barrier to machine learning model development. Differential privacy (DP) has emerged as one canonical standard for provable privacy. However, DP's strong theoretical guarantees often come at the cost of a large drop in its utility for machine learning; and DP guarantees themselves are difficult to interpret. In this work, we propose a novel privacy notion, membership inference privacy (MIP), as a step towards addressing these challenges. We give a precise characterization of the relationship between MIP and DP, and show that in some cases, MIP can be achieved using less amount of randomness compared to the amount required for guaranteeing DP, leading to smaller drop in utility. MIP guarantees are also easily interpretable in terms of the success rate of membership inference attacks in a simple random subsampling setting. As a proof of concept, we also provide a simple algorithm for guaranteeing MIP without needing to guarantee DP",
    "checked": true,
    "id": "1cc36266e3ce0ac751d431c48edbfd9d41f5931c",
    "semantic_title": "provable membership inference privacy",
    "citation_count": 4,
    "authors": [
      "Zachary Izzo",
      "Jinsung Yoon",
      "Sercan O Arik",
      "James Zou"
    ]
  },
  "https://openreview.net/forum?id=1fbTGC3BUD": {
    "title": "Adaptive Conformal Regression with Split-Jackknife+ Scores",
    "volume": "main",
    "abstract": "We introduce an extension of conformal predictions (CP) based on a combination of split-CP and the Jackknife+ procedure that enables tuning score functions to calibration data and designed to produce dynamically-sized prediction interval in regression settings. We motivate this method with theoretical results on distribution-dependent conditional coverage guarantees for split-CP and Jackknife+ prediction sets which are determined by the statistical dependence between input data and prediction scores. This dependence can be reduced by adapting the score function to the data distribution, thereby improving the conditional validity of conformal prediction sets. As an illustration, we construct a variant of the MADSplit conformal regression procedure where conditional mean estimates are computed in-distribution and show through empirical validation that our method is more robust to overfitting effects than the original method, while being more sample-efficient than modern ECDF-based methods",
    "checked": false,
    "id": "a8597796255050e3ab7b8df502d81ce1f81a5273",
    "semantic_title": "adaptive conformal regression with jackknife+ rescaled scores",
    "citation_count": 5,
    "authors": [
      "Nicolas Deutschmann",
      "Mattia Rigotti",
      "Maria Rodriguez Martinez"
    ]
  },
  "https://openreview.net/forum?id=5psgQEHn6t": {
    "title": "Neural networks can be FLOP-efficient integrators of 1D oscillatory integrands",
    "volume": "main",
    "abstract": "We demonstrate that neural networks can be FLOP-efficient integrators of one-dimensional oscillatory integrands. We train a feed-forward neural network to compute integrals of highly oscillatory 1D functions. The training set is a parametric combination of functions with varying characters and oscillatory behavior degrees. Numerical examples show that these networks are FLOP-efficient for sufficiently oscillatory integrands with an average FLOP gain of $10^{3}$ FLOPs. The network calculates oscillatory integrals better than traditional quadrature methods under the same computational budget or number of floating point operations. We find that feed-forward networks of 5 hidden layers are satisfactory for a relative accuracy of $10^{-3}$. The computational burden of inference of the neural network is relatively small, even compared to inner-product pattern quadrature rules. We postulate that our result follows from learning latent patterns in the oscillatory integrands that are otherwise opaque to traditional numerical integrators",
    "checked": true,
    "id": "e271d34c24d0407632418f6e908ae70c3c646941",
    "semantic_title": "neural networks can be flop-efficient integrators of 1d oscillatory integrands",
    "citation_count": 0,
    "authors": [
      "Anshuman Sinha",
      "Spencer H Bryngelson"
    ]
  },
  "https://openreview.net/forum?id=RGewtLtvHz": {
    "title": "Towards generalizing deep-audio fake detection networks",
    "volume": "main",
    "abstract": "Today's generative neural networks allow the creation of high-quality synthetic speech at scale. While we welcome the creative use of this new technology, we must also recognize the risks. As synthetic speech is abused for monetary and identity theft, we require a broad set of deepfake identification tools. Furthermore, previous work reported a limited ability of deep classifiers to generalize to unseen audio generators. We study the frequency domain fingerprints of current audio generators. Building on top of the discovered frequency footprints, we train excellent lightweight detectors that generalize. We report improved results on the WaveFake dataset and an extended version. To account for the rapid progress in the field, we extend the WaveFake dataset by additionally considering samples drawn from the novel Avocodo and BigVGAN networks. For illustration purposes, the supplementary material contains audio samples of generator artifacts",
    "checked": true,
    "id": "e59128960429cc0fcb93cf8b66fc66b0a2ae68dd",
    "semantic_title": "towards generalizing deep-audio fake detection networks",
    "citation_count": 1,
    "authors": [
      "Konstantin Gasenzer",
      "Moritz Wolter"
    ]
  },
  "https://openreview.net/forum?id=t4nnCi5AO6": {
    "title": "Scaling (Down) CLIP: A Comprehensive Analysis of Data,Architecture, and Training Strategies",
    "volume": "main",
    "abstract": "This paper investigates the performance of the Contrastive Language-Image Pre-training (CLIP) when scaled down to limited computation budgets. We explore CLIP along three dimensions: data, architecture, and training strategies. With regards to data, we demonstrate the significance of high-quality training data and show that a smaller dataset of high-quality data can outperform a larger dataset with lower quality. We also examine how model performance varies with different dataset sizes, suggesting that smaller ViT models are better suited for smaller datasets, while larger models perform better on larger datasets with fixed compute. Additionally, we provide guidance on when to choose a CNN-based architecture or a ViT-based architecture for CLIP training. We compare four CLIP training strategies - SLIP, FLIP, CLIP, and CLIP+Data Augmentation - and show that the choice of training strategy depends on the available compute resource. Our analysis reveals that CLIP+Data Augmentation can achieve comparable performance to CLIP using only half of the training data. This work provides practical insights into how to effectively train and deploy CLIP models, making them more accessible and affordable for practical use in various applications",
    "checked": false,
    "id": "51ee1cc17da1430bb5cdb4e1feba7591d9a2cb68",
    "semantic_title": "scaling (down) clip: a comprehensive analysis of data, architecture, and training strategies",
    "citation_count": 0,
    "authors": [
      "Zichao Li",
      "Cihang Xie",
      "Ekin Dogus Cubuk"
    ]
  },
  "https://openreview.net/forum?id=w4DXLzBPPw": {
    "title": "Low-Rank Tensor-Network Encodings for Video-to-Action Behavioral Cloning",
    "volume": "main",
    "abstract": "We describe a tensor-network latent-space encoding approach for increasing the scalability of behavioral cloning of a video game player's actions entirely from video streams of the gameplay. Specifically, we address challenges associated with the high computational requirements of traditional deep-learning based encoders such as convolutional variational autoencoders that prohibit their use in widely available hardware or for large scale data. Our approach uses tensor networks instead of deep variational autoencoders for this purpose, and it yields significant speedups with no loss of accuracy. Empirical results on ATARI games demonstrate that our approach leads to a speedup in the time it takes to encode data and train a predictor using the encodings (between 2.6× to 9.6× compared to autoencoders or variational autoencoders). Furthermore, the tensor train encoding can be efficiently trained on CPU as well, which leads to comparable or better training times than the autoencoder and variational autoencoder trained on GPU (0.9× to 5.4× faster). These results suggest significant possibilities in mitigating the need for cost and time-intensive hardware for training deep-learning architectures for behavioral cloning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian Chen",
      "Doruk Aksoy",
      "David J Gorsich",
      "Shravan Veerapaneni",
      "Alex Gorodetsky"
    ]
  },
  "https://openreview.net/forum?id=DIGkJhGeqi": {
    "title": "EHRDiff : Exploring Realistic EHR Synthesis with Diffusion Models",
    "volume": "main",
    "abstract": "Electronic health records (EHR) contain a wealth of biomedical information, serving as valuable resources for the development of precision medicine systems. However, privacy concerns have resulted in limited access to high-quality and large-scale EHR data for researchers, impeding progress in methodological development. Recent research has delved into synthesizing realistic EHR data through generative modeling techniques, where a majority of proposed methods relied on generative adversarial networks (GAN) and their variants for EHR synthesis. Despite GAN-based methods attaining state-of-the-art performance in generating EHR data, these approaches are difficult to train and prone to mode collapse. Recently introduced in generative modeling, diffusion models have established cutting-edge performance in image generation, but their efficacy in EHR data synthesis remains largely unexplored. In this study, we investigate the potential of diffusion models for EHR data synthesis and introduce a novel method, EHRDiff. Through extensive experiments, EHRDiff establishes new state-of-the-art quality for synthetic EHR data, protecting private information in the meanwhile",
    "checked": false,
    "id": "4e05bad5ed0b8ef54ae3813f27ac5c87cf1ddd8a",
    "semantic_title": "ehrdiff: exploring realistic ehr synthesis with diffusion models",
    "citation_count": 8,
    "authors": [
      "Hongyi Yuan",
      "Songchi Zhou",
      "Sheng Yu"
    ]
  },
  "https://openreview.net/forum?id=iBgmoMTlaz": {
    "title": "Understanding Fairness Surrogate Functions in Algorithmic Fairness",
    "volume": "main",
    "abstract": "It has been observed that machine learning algorithms exhibit biased predictions against certain population groups. To mitigate such bias while achieving comparable accuracy, a promising approach is to introduce surrogate functions of the concerned fairness definition and solve a constrained optimization problem. However, it is intriguing in previous work that such fairness surrogate functions may yield unfair results and high instability. In this work, in order to deeply understand them, taking a widely used fairness definition—demographic parity as an example, we show that there is a surrogate-fairness gap between the fairness definition and the fairness surrogate function. Also, the theoretical analysis and experimental results about the \"gap\" motivate us that the fairness and stability will be affected by the points far from the decision boundary, which is the large margin points issue investigated in this paper. To address it, we propose the general sigmoid surrogate to simultaneously reduce both the surrogate-fairness gap and the variance, and offer a rigorous fairness and stability upper bound. Interestingly, the theory also provides insights into two important issues that deal with the large margin points as well as obtaining a more balanced dataset are beneficial to fairness and stability. Furthermore, we elaborate a novel and general algorithm called Balanced Surrogate, which iteratively reduces the \"gap\" to mitigate unfairness. Finally, we provide empirical evidence showing that our methods consistently improve fairness and stability while maintaining accuracy comparable to the baselines in three real-world datasets",
    "checked": true,
    "id": "e83c814656dbe816e2db1c11f7a3f3bbd45673b3",
    "semantic_title": "understanding fairness surrogate functions in algorithmic fairness",
    "citation_count": 0,
    "authors": [
      "Wei Yao",
      "Zhanke Zhou",
      "Zhicong Li",
      "Bo Han",
      "Yong Liu"
    ]
  },
  "https://openreview.net/forum?id=BkEqk7pS1I": {
    "title": "Finite-Time Analysis of Entropy-Regularized Neural Natural Actor-Critic Algorithm",
    "volume": "main",
    "abstract": "Natural actor-critic (NAC) and its variants, equipped with the representation power of neural networks, have demonstrated impressive empirical success in solving Markov decision problems with large (potentially infinite) state spaces. In this paper, we present a finite-time analysis of NAC with neural network approximation, and identify the roles of neural networks, regularization and optimization techniques (e.g., gradient clipping and weight decay) to achieve provably good performance in terms of sample complexity, iteration complexity and overparametrization bounds for the actor and the critic. In particular, we prove that (i) entropy regularization and weight decay ensure stability by providing sufficient exploration to avoid near-deterministic and strictly suboptimal policies and (ii) regularization leads to sharp sample complexity and network width bounds in the regularized MDPs, yielding a favorable bias-variance tradeoff in policy optimization. In the process, we identify the importance of uniform approximation power of the actor neural network to achieve global optimality in policy optimization due to distributional shift",
    "checked": true,
    "id": "375ce9fe3849914cd8e021cc70bef147bcaa8fd9",
    "semantic_title": "finite-time analysis of entropy-regularized neural natural actor-critic algorithm",
    "citation_count": 8,
    "authors": [
      "Semih Cayci",
      "Niao He",
      "R. Srikant"
    ]
  },
  "https://openreview.net/forum?id=cPDVjsOytS": {
    "title": "PopulAtion Parameter Averaging (PAPA)",
    "volume": "main",
    "abstract": "Ensemble methods combine the predictions of multiple models to improve performance, but they require significantly higher computation costs at inference time. To avoid these costs, multiple neural networks can be combined into one by averaging their weights. However, this usually performs significantly worse than ensembling. Weight averaging is only beneficial when different enough to benefit from combining them, but similar enough to average well. Based on this idea, we propose PopulAtion Parameter Averaging (PAPA): a method that combines the generality of ensembling with the efficiency of weight averaging. PAPA leverages a population of diverse models (trained on different data orders, augmentations, and regularizations) while slowly pushing the weights of the networks toward the population average of the weights. We also propose PAPA variants (PAPA-all, and PAPA-2) that average weights rarely rather than continuously; all methods increase generalization, but PAPA tends to perform best. PAPA reduces the performance gap between averaging and ensembling, increasing the average accuracy of a population of models by up to 0.8% on CIFAR-10, 1.9% on CIFAR-100, and 1.6% on ImageNet when compared to training independent (non-averaged) models",
    "checked": true,
    "id": "2fb84dcdfa68e31742e20ffff3f32a04ec85d3da",
    "semantic_title": "population parameter averaging (papa)",
    "citation_count": 8,
    "authors": [
      "Alexia Jolicoeur-Martineau",
      "Emy Gervais",
      "Kilian FATRAS",
      "Yan Zhang",
      "Simon Lacoste-Julien"
    ]
  },
  "https://openreview.net/forum?id=Y4YWzBiTEV": {
    "title": "The Missing U for Efficient Diffusion Models",
    "volume": "main",
    "abstract": "Diffusion Probabilistic Models stand as a critical tool in generative modelling, enabling the generation of complex data distributions. This family of generative models yields record-breaking performance in tasks such as image synthesis, video generation, and molecule design. Despite their capabilities, their efficiency, especially in the reverse process, remains a challenge due to slow convergence rates and high computational costs. In this paper, we introduce an approach that leverages continuous dynamical systems to design a novel denoising network for diffusion models that is more parameter-efficient, exhibits faster convergence, and demonstrates increased noise robustness. Experimenting with Denoising Diffusion Probabilistic Models (DDPMs), our framework operates with approximately a quarter of the parameters, and $\\sim$ 30\\% of the Floating Point Operations (FLOPs) compared to standard U-Nets in DDPMs. Furthermore, our model is notably faster in inference than the baseline when measured in fair and equal conditions. We also provide a mathematical intuition as to why our proposed reverse process is faster as well as a mathematical discussion of the empirical tradeoffs in the denoising downstream task. Finally, we argue that our method is compatible with existing performance enhancement techniques, enabling further improvements in efficiency, quality, and speed",
    "checked": true,
    "id": "bac74fa84c99d08371f87e156a286c4bb8c8d512",
    "semantic_title": "the missing u for efficient diffusion models",
    "citation_count": 1,
    "authors": [
      "Sergio Calvo Ordoñez",
      "Chun-Wun Cheng",
      "Jiahao Huang",
      "Lipei Zhang",
      "Guang Yang",
      "Carola-Bibiane Schönlieb",
      "Angelica I Aviles-Rivero"
    ]
  },
  "https://openreview.net/forum?id=N05OnQG1BA": {
    "title": "CoDeC: Communication-Efficient Decentralized Continual Learning",
    "volume": "main",
    "abstract": "Training at the edge utilizes continuously evolving data generated at different locations. Privacy concerns prohibit the co-location of this spatially as well as temporally distributed data, deeming it crucial to design training algorithms that enable efficient continual learning over decentralized private data. Decentralized learning allows serverless training with spatially distributed data. A fundamental barrier in such setups is the high bandwidth cost of communicating model updates between agents. Moreover, existing works under this training paradigm are not inherently suitable for learning a temporal sequence of tasks while retaining the previously acquired knowledge. In this work, we propose CoDeC, a novel communication-efficient decentralized continual learning algorithm that addresses these challenges. We mitigate catastrophic forgetting while learning a distributed task sequence by incorporating orthogonal gradient projection within a gossip-based decentralized learning algorithm. Further, CoDeC includes a novel lossless communication compression scheme based on the gradient subspaces. We theoretically analyze the convergence rate for our algorithm and demonstrate through an extensive set of experiments that CoDeC successfully learns distributed continual tasks with minimal forgetting. The proposed compression scheme results in up to 4.8× reduction in communication costs without any loss in performance",
    "checked": true,
    "id": "14fc27f151a12e141e8cc84da050e64b8d90eb56",
    "semantic_title": "codec: communication-efficient decentralized continual learning",
    "citation_count": 1,
    "authors": [
      "Sakshi Choudhary",
      "Sai Aparna Aketi",
      "Gobinda Saha",
      "Kaushik Roy"
    ]
  },
  "https://openreview.net/forum?id=75OwvzZZBT": {
    "title": "Bias Amplification Enhances Minority Group Performance",
    "volume": "main",
    "abstract": "Neural networks produced by standard training are known to suffer from poor accuracy on rare subgroups despite achieving high accuracy on average, due to the correlations between certain spurious features and labels. Previous approaches based on worst-group loss minimization (e.g. Group-DRO) are effective in improving worse-group accuracy but require expensive group annotations for all the training samples. In this paper, we focus on the more challenging and realistic setting where group annotations are only available on a small validation set or are not available at all. We propose BAM, a novel two-stage training algorithm: in the first stage, the model is trained using a bias amplification scheme via introducing a learnable auxiliary variable for each training sample; in the second stage, we upweight the samples that the bias-amplified model misclassifies, and then continue training the same model on the reweighted dataset. Empirically, BAM achieves competitive performance compared with existing methods evaluated on spurious correlation benchmarks in computer vision and natural language processing. Moreover, we find a simple stopping criterion based on minimum class accuracy difference that can remove the need for group annotations, with little or no loss in worst-group accuracy. We perform extensive analyses and ablations to verify the effectiveness and robustness of our algorithm in varying class and group imbalance ratios",
    "checked": true,
    "id": "d1df8b8160e222525990b40dcef323eccc16a74c",
    "semantic_title": "bias amplification enhances minority group performance",
    "citation_count": 1,
    "authors": [
      "Gaotang Li",
      "Jiarui Liu",
      "Wei Hu"
    ]
  },
  "https://openreview.net/forum?id=j5T4pcLbcY": {
    "title": "Fast and Expressive Gesture Recognition using a Combination-Homomorphic Electromyogram Encoder",
    "volume": "main",
    "abstract": "We study the task of gesture recognition from electromyography (EMG), with the goal of enabling expressive human-computer interaction at high accuracy, while minimizing the time required for new subjects to provide calibration data. To fulfill these goals, we define combination gestures consisting of a direction component and a modifier component. New subjects only demonstrate the single component gestures and we seek to extrapolate from these to all possible single or combination gestures. We extrapolate to unseen combination gestures by combining the feature vectors of real single gestures to produce synthetic training data. This strategy allows us to provide a large and flexible gesture vocabulary, while not requiring new subjects to demonstrate combinatorially many example gestures. We pre-train an encoder and a combination operator using self-supervision, so that we can produce useful synthetic training data for unseen test subjects. To evaluate the proposed method, we collect a real-world EMG dataset, and measure the effect of augmented supervision against two baselines: a partially-supervised model trained with only single gesture data from the unseen subject, and a fully-supervised model trained with real single and real combination gesture data from the unseen subject. We find that the proposed method provides a dramatic improvement over the partially-supervised model, and achieves a useful classification accuracy that in some cases approaches the performance of the fully-supervised model",
    "checked": true,
    "id": "adc02afb466c4f6978e49a9026ae371fcc728a63",
    "semantic_title": "fast and expressive gesture recognition using a combination-homomorphic electromyogram encoder",
    "citation_count": 0,
    "authors": [
      "Niklas Smedemark-Margulies",
      "Yunus Bicer",
      "Elifnur Sunger",
      "Tales Imbiriba",
      "Eugene Tunik",
      "Deniz Erdogmus",
      "Mathew Yarossi",
      "Robin Walters"
    ]
  },
  "https://openreview.net/forum?id=kxYqgSkH8I": {
    "title": "Optimization with Access to Auxiliary Information",
    "volume": "main",
    "abstract": "We investigate the fundamental optimization question of minimizing a \\emph{target} function $f(x)$, whose gradients are expensive to compute or have limited availability, given access to some \\emph{auxiliary} side function $h(x)$ whose gradients are cheap or more available. This formulation captures many settings of practical relevance, such as i) re-using batches in SGD, ii) transfer learning, iii) federated learning, iv) training with compressed models/dropout, etcetera. We propose two generic new algorithms that apply in all these settings; we also prove that we can benefit from this framework under the Hessian similarity assumption between the target and side information. A benefit is obtained when this similarity measure is small; we also show a potential benefit from stochasticity when the auxiliary noise is correlated with that of the target function",
    "checked": true,
    "id": "49b9c5eeff14f7196d2fb551b01a83c2d96e1a2f",
    "semantic_title": "optimization with access to auxiliary information",
    "citation_count": 8,
    "authors": [
      "El Mahdi Chayti",
      "Sai Praneeth Karimireddy"
    ]
  },
  "https://openreview.net/forum?id=zn3fB4VVF0": {
    "title": "Navigating Noise: A Study of How Noise Influences Generalisation and Calibration of Neural Networks",
    "volume": "main",
    "abstract": "Enhancing the generalisation abilities of neural networks (NNs) through integrating noise such as MixUp or Dropout during training has emerged as a powerful and adaptable technique. Despite the proven efficacy of noise in NN training, there is no consensus regarding which noise sources, types and placements yield maximal benefits in generalisation and confidence calibration. This study thoroughly explores diverse noise modalities to evaluate their impacts on NN's generalisation and calibration under in-distribution or out-of-distribution settings, paired with experiments investigating the metric landscapes of the learnt representations, across a spectrum of NN architectures, tasks, and datasets. Our study shows that AugMix and weak augmentation exhibit cross-task effectiveness in computer vision, emphasising the need to tailor noise to specific domains. Our findings emphasise the efficacy of combining noises and successful hyperparameter transfer within a single domain but the difficulties in transferring the benefits to other domains. Furthermore, the study underscores the complexity of simultaneously optimising for both generalisation and calibration, emphasising the need for practitioners to carefully consider noise combinations and hyperparameter tuning for optimal performance in specific tasks and datasets",
    "checked": true,
    "id": "b48cd04a9a71d6808c4a9060c224a63a6b5b7edd",
    "semantic_title": "navigating noise: a study of how noise influences generalisation and calibration of neural networks",
    "citation_count": 2,
    "authors": [
      "Martin Ferianc",
      "Ondrej Bohdal",
      "Timothy Hospedales",
      "Miguel R. D. Rodrigues"
    ]
  },
  "https://openreview.net/forum?id=OyXS4ZIqd3": {
    "title": "On the Robustness of Neural Collapse and the Neural Collapse of Robustness",
    "volume": "main",
    "abstract": "Neural Collapse refers to the curious phenomenon in the end of training of a neural network, where feature vectors and classification weights converge to a very simple geometrical arrangement (a simplex). While it has been observed empirically in various cases and has been theoretically motivated, its connection with crucial properties of neural networks, like their generalization and robustness, remains unclear. In this work, we study the stability properties of these simplices. We find that the simplex structure disappears under small adversarial attacks, and that perturbed examples \"leap\" between simplex vertices. We further analyze the geometry of networks that are optimized to be robust against adversarial perturbations of the input, and find that Neural Collapse is a pervasive phenomenon in these cases as well, with clean and perturbed representations forming aligned simplices, and giving rise to a robust simple nearest-neighbor classifier. By studying the propagation of the amount of collapse inside the network, we identify novel properties of both robust and non-robust machine learning models, and show that earlier, unlike later layers maintain reliable simplices on perturbed data",
    "checked": true,
    "id": "72da387dda40b29fa25d28e324a4e9a0cc3feec9",
    "semantic_title": "on the robustness of neural collapse and the neural collapse of robustness",
    "citation_count": 1,
    "authors": [
      "Jingtong Su",
      "Ya Shi Zhang",
      "Nikolaos Tsilivis",
      "Julia Kempe"
    ]
  },
  "https://openreview.net/forum?id=bZ80b0wb9d": {
    "title": "Discrete Graph Auto-Encoder",
    "volume": "main",
    "abstract": "Despite advances in generative methods, accurately modeling the distribution of graphs remains a challenging task primarily because of the absence of predefined or inherent unique graph representation. Two main strategies have emerged to tackle this issue: 1) restricting the number of possible representations by sorting the nodes, or 2) using permutation-invariant/equivariant functions, specifically Graph Neural Networks (GNNs). In this paper, we introduce a new framework named Discrete Graph Auto-Encoder (DGAE), which leverages the strengths of both strategies and mitigate their respective limitations. In essence, we propose a strategy in 2 steps. We first use a permutation-equivariant auto-encoder to convert graphs into sets of discrete latent node representations, each node being represented by a sequence of quantized vectors. In the second step, we sort the sets of discrete latent representations and learn their distribution with a specifically designed auto-regressive model based on the Transformer architecture. Through multiple experimental evaluations, we demonstrate the competitive performances of our model in comparison to the existing state-of-the-art across various datasets. Various ablation studies support the interest of our method",
    "checked": true,
    "id": "014a80979ebeac7e3e688306caf217485885b463",
    "semantic_title": "discrete graph auto-encoder",
    "citation_count": 0,
    "authors": [
      "Yoann Boget",
      "Magda Gregorova",
      "Alexandros Kalousis"
    ]
  },
  "https://openreview.net/forum?id=wE9kpJSemv": {
    "title": "Indexed Minimum Empirical Divergence-Based Algorithms for Linear Bandits",
    "volume": "main",
    "abstract": "The Indexed Minimum Empirical Divergence (IMED) algorithm is a highly effective approach that offers a stronger theoretical guarantee of the asymptotic optimality compared to the Kullback--Leibler Upper Confidence Bound (KL-UCB) algorithm for the multi-armed bandit problem. Additionally, it has been observed to empirically outperform UCB-based algorithms and Thompson Sampling. Despite its effectiveness, the generalization of this algorithm to contextual bandits with linear payoffs has remained elusive. In this paper, we present novel linear versions of the IMED algorithm, which we call the family of LinIMED algorithms. We demonstrate that LinIMED provides a $\\widetilde{O}(d\\sqrt{T})$ upper regret bound where $d$ is the dimension of the context and $T$ is the time horizon. Furthermore, extensive empirical studies reveal that LinIMED and its variants outperform widely-used linear bandit algorithms such as LinUCB and Linear Thompson Sampling in some regimes",
    "checked": false,
    "id": "14e02786233acca32d923c32aa630b483b6e2efa",
    "semantic_title": "logarithmic regret in communicating mdps: leveraging known dynamics with bandits",
    "citation_count": 0,
    "authors": [
      "Jie Bian",
      "Vincent Y. F. Tan"
    ]
  },
  "https://openreview.net/forum?id=Fu4mwB0XIU": {
    "title": "PID Control-Based Self-Healing to Improve the Robustness of Large Language Models",
    "volume": "main",
    "abstract": "Despite the effectiveness of deep neural networks in numerous natural language processing applications, recent findings have exposed the vulnerability of these language models when minor perturbations are introduced. While appearing semantically indistinguishable to humans, these perturbations can significantly reduce the performance of well-trained language models, raising concerns about the reliability of deploying them in safe-critical situations. In this work, we construct a computationally efficient self-healing process to correct undesired model behavior during online inference when perturbations are applied to input data. This is formulated as a trajectory optimization problem in which the internal states of the neural network layers are automatically corrected using a PID (Proportional-Integral-Derivative) control mechanism. The P controller targets immediate state adjustments, while the I and D controllers consider past states and future dynamical trends, respectively. We leverage the geometrical properties of the training data to design effective linear PID controllers. This approach reduces the computational cost to that of using just the P controller, instead of the full PID control. Further, we introduce an analytical method for approximating the optimal control solutions, enhancing the real-time inference capabilities of this controlled system. Moreover, we conduct a theoretical error analysis of the analytic solution in a simplified setting. The proposed PID control-based self-healing is a low-cost framework that improves the robustness of pre-trained large language models, whether standard or robustly trained, against a wide range of perturbations",
    "checked": true,
    "id": "4302593601e57a1c4cb487d871723b9d11a8e2b2",
    "semantic_title": "pid control-based self-healing to improve the robustness of large language models",
    "citation_count": 0,
    "authors": [
      "Zhuotong Chen",
      "Zihu Wang",
      "Yifan Yang",
      "Qianxiao Li",
      "Zheng Zhang"
    ]
  },
  "https://openreview.net/forum?id=c5o4HUypqm": {
    "title": "Synthetic data shuffling accelerates the convergence of federated learning under data heterogeneity",
    "volume": "main",
    "abstract": "In federated learning, data heterogeneity is a critical challenge. A straightforward solution is to shuffle the clients' data to homogenize the distribution. However, this may violate data access rights, and how and when shuffling can accelerate the convergence of a federated optimization algorithm is not theoretically well understood. In this paper, we establish a precise and quantifiable correspondence between data heterogeneity and parameters in the convergence rate when a fraction of data is shuffled across clients. We discuss that shuffling can, in some cases, quadratically reduce the gradient dissimilarity with respect to the shuffling percentage, accelerating convergence. Inspired by the theory, we propose a practical approach that addresses the data access rights issue by shuffling locally generated synthetic data. The experimental results show that shuffling synthetic data improves the performance of multiple existing federated learning algorithms by a large margin",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Li",
      "Yasin Esfandiari",
      "Mikkel N. Schmidt",
      "Tommy Sonne Alstrøm",
      "Sebastian U Stich"
    ]
  },
  "https://openreview.net/forum?id=LO02YHxrxd": {
    "title": "Graph Neural Networks Formed via Layer-wise Ensembles of Heterogeneous Base Models",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) with numerical node features and graph structure as inputs have demonstrated superior performance on various semi-supervised learning tasks with graph data. However, the numerical node features utilized by GNNs are commonly extracted from raw data which is of text or tabular (numeric/categorical) type in most real-world applications. The best models for such data types in most standard supervised learning settings with IID (non-graph) data are not simple neural network layers and thus are not easily incorporated into a GNN. Here we propose a robust stacking framework that fuses graph-aware propagation with arbitrary models intended for IID data, which are ensembled and stacked in multiple layers. Our layer-wise framework leverages bagging and stacking strategies to enjoy strong generalization, in a manner which effectively mitigates label leakage and overfitting. Across a variety of graph datasets with tabular/text node features, our method achieves comparable or superior performance relative to both tabular/text and graph neural network models, as well as existing state-of-the-art hybrid strategies that combine the two",
    "checked": false,
    "id": "1339df4a9c07fdb622077b39cadcf0df0b4a20c2",
    "semantic_title": "ensemble quadratic assignment network for graph matching",
    "citation_count": 0,
    "authors": [
      "Jiuhai Chen",
      "Jonas Mueller",
      "Vassilis N. Ioannidis",
      "Tom Goldstein",
      "David Wipf"
    ]
  },
  "https://openreview.net/forum?id=lQE2AcbYge": {
    "title": "Online Continuous Hyperparameter Optimization for Generalized Linear Contextual Bandits",
    "volume": "main",
    "abstract": "In stochastic contextual bandits, an agent sequentially makes actions from a time-dependent action set based on past experience to minimize the cumulative regret. Like many other machine learning algorithms, the performance of bandits heavily depends on the values of hyperparameters, and theoretically derived parameter values may lead to unsatisfactory results in practice. Moreover, it is infeasible to use offline tuning methods like cross-validation to choose hyperparameters under the bandit environment, as the decisions should be made in real-time. To address this challenge, we propose the first online continuous hyperparameter tuning framework for contextual bandits to learn the optimal parameter configuration in practice within a search space on the fly. Specifically, we use a double-layer bandit framework named CDT (Continuous Dynamic Tuning) and formulate the hyperparameter optimization as a non-stationary continuum-armed bandit, where each arm represents a combination of hyperparameters, and the corresponding reward is the algorithmic result. For the top layer, we propose the Zooming TS algorithm that utilizes Thompson Sampling (TS) for exploration and a restart technique to get around the \\textit{switching} environment. The proposed CDT framework can be easily utilized to tune contextual bandit algorithms without any pre-specified candidate set for multiple hyperparameters. We further show that it could achieve a sublinear regret in theory and performs consistently better than all existing methods on both synthetic and real datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Kang",
      "Cho-Jui Hsieh",
      "Thomas Lee"
    ]
  },
  "https://openreview.net/forum?id=VBAKc4DtZ1": {
    "title": "Faster Convergence of Local SGD for Over-Parameterized Models",
    "volume": "main",
    "abstract": "Modern machine learning architectures are often highly expressive. They are usually over-parameterized and can interpolate the data by driving the empirical loss close to zero. We analyze the convergence of Local SGD (or FedAvg) for such over-parameterized models in the heterogeneous data setting and improve upon the existing literature by establishing the following convergence rates. For general convex loss functions, we establish an error bound of $\\mathcal {O}(1/T)$ under a mild data similarity assumption and an error bound of $\\mathcal {O}(K/T)$ otherwise, where $K$ is the number of local steps and $T$ is the total number of iterations. For non-convex loss functions we prove an error bound of $\\mathcal {O}(K/T)$. These bounds improve upon the best previous bound of $\\mathcal {O}(1/\\sqrt{nT})$ in both cases, where $n$ is the number of agents, when no assumption on the model being over-parameterized is made. We complete our results by providing problem instances in which our established convergence rates are tight to a constant factor with a reasonably small stepsize. Finally, we validate our theoretical results by performing large-scale numerical experiments that reveal the convergence behavior of Local SGD for practical over-parameterized deep learning models, in which the $\\mathcal {O}(1/T)$ convergence rate of Local SGD is clearly shown",
    "checked": true,
    "id": "e48364267077e5063492bc6b0d0e9ae22eda18e1",
    "semantic_title": "faster convergence of local sgd for over-parameterized models",
    "citation_count": 5,
    "authors": [
      "Tiancheng Qin",
      "S. Rasoul Etesami",
      "Cesar A Uribe"
    ]
  },
  "https://openreview.net/forum?id=zOGJxw07Z6": {
    "title": "Asynchronous Training Schemes in Distributed Learning with Time Delay",
    "volume": "main",
    "abstract": "In the context of distributed deep learning, the issue of stale weights or gradients could result in poor algorithmic performance. This issue is usually tackled by delay tolerant algorithms with some mild assumptions on the objective functions and step sizes. In this paper, we propose a different approach to develop a new algorithm, called \\textbf{P}redicting \\textbf{C}lipping \\textbf{A}synchronous \\textbf{S}tochastic \\textbf{G}radient \\textbf{D}escent (aka, PC-ASGD). Specifically, PC-ASGD has two steps - the \\textit{predicting step} leverages the gradient prediction using Taylor expansion to reduce the staleness of the outdated weights while the \\textit{clipping step} selectively drops the outdated weights to alleviate their negative effects. A tradeoff parameter is introduced to balance the effects between these two steps. Theoretically, we present the convergence rate considering the effects of delay of the proposed algorithm with constant step size when the smooth objective functions are weakly strongly-convex, general convex, and nonconvex. One practical variant of PC-ASGD is also proposed by adopting a condition to help with the determination of the tradeoff parameter. For empirical validation, we demonstrate the performance of the algorithm with four deep neural network architectures on three benchmark datasets",
    "checked": true,
    "id": "4527389934c209c050a482ac20ba86f14cf17b42",
    "semantic_title": "asynchronous training schemes in distributed learning with time delay",
    "citation_count": 1,
    "authors": [
      "Haoxiang Wang",
      "Zhanhong Jiang",
      "Chao Liu",
      "Soumik Sarkar",
      "Dongxiang Jiang",
      "Young M Lee"
    ]
  },
  "https://openreview.net/forum?id=qNGo6ghWFB": {
    "title": "Merging by Matching Models in Task Parameter Subspaces",
    "volume": "main",
    "abstract": "Model merging aims to cheaply combine individual task-specific models into a single multitask model. In this work, we view past merging methods as leveraging different notions of a ''task parameter subspace'' in which models are matched before being merged. We connect the task parameter subspace of a given model to its loss landscape and formalize how this approach to model merging can be seen as solving a linear system of equations. While past work has generally been limited to linear systems that have a closed-form solution, we consider using the conjugate gradient method to find a solution. We show that using the conjugate gradient method can outperform closed-form solutions, enables merging via linear systems that are otherwise intractable to solve, and flexibly allows choosing from a wide variety of initializations and estimates for the ''task parameter subspace''. We ultimately demonstrate that our merging framework called ''Matching Models in their Task Parameter Subspace'' (MATS) achieves state-of-the-art results in multitask and intermediate-task model merging. We release all of the code and checkpoints used in our work",
    "checked": true,
    "id": "e5a2c0759027c946df23cce37a8a3104d5c48841",
    "semantic_title": "merging by matching models in task parameter subspaces",
    "citation_count": 2,
    "authors": [
      "Derek Tam",
      "Mohit Bansal",
      "Colin Raffel"
    ]
  },
  "https://openreview.net/forum?id=6yzIuqKGnq": {
    "title": "INSPIRE: Incorporating Diverse Feature Preferences in Recourse",
    "volume": "main",
    "abstract": "Most recourse generation approaches optimize for indirect distance-based metrics like diversity, proximity, and sparsity, or a shared cost function across all users. A shared cost function in particular is an unrealistic assumption because users can have diverse feature preferences (FPs), i.e. the features they are willing to act upon to obtain recourse. In this work, we propose a novel method, INSPIRE to incorporate diverse feature preferences in both recourse generation and evaluation procedures by focusing on the cost incurred by a user when opting for a recourse. To achieve this, we first propose an objective function, Expected Minimum Cost (EMC) based on two key ideas: (1) the user should be comfortable adopting at least one solution when presented with multiple options, and (2) we can provide users with multiple options that cover a wide variety of FPs when the user's FPs are unknown. To optimize for EMC, we propose a novel discrete optimization algorithm, Cost-Optimized Local Search (COLS), that is guaranteed to improve the quality of the recourse set over iterations. Next, we propose a cost-based evaluation procedure that computes user satisfaction by simulating each user's cost function and then computing the incurred cost for the provided recourse set. Experiments on popular real-world datasets demonstrate that our method is more fair compared to baselines and satisfies up to 25.9% more users. We also show that our method is robust to misspecifications of the cost function distribution. Our code is available at \\href{https://github.com/prateeky2806/EMC-COLS-recourse}{https://github.com/prateeky2806/EMC-COLS-recourse}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prateek Yadav",
      "Peter Hase",
      "Mohit Bansal"
    ]
  },
  "https://openreview.net/forum?id=1QjCzP0KIw": {
    "title": "Unsupervised 3D Scene Representation Learning via Movable Object Inference",
    "volume": "main",
    "abstract": "Unsupervised, category-agnostic, object-centric 3D representation learning for complex scenes remains an open problem in computer vision. While a few recent methods can discover 3D objects from a single image, they remain struggling on scenes with diverse and complex object configurations as they discover objects mostly by appearance similarity which is insufficient for textured objects. In this work, we propose Movable Object Radiance Fields (MORF), aiming at scaling to complex scenes with diverse categories of objects. Inspired by cognitive science studies of object learning in babies, MORF learns 3D object representations via movable object inference. While obtaining 3D movable object signals requires multi-view videos of moving objects, we propose lifting a 2D movable object inference module that can be unsupervisedly pretrained on monocular videos. Thus, MORF requires only multi-view images of static training scenes. During testing, MORF can discover, reconstruct, and move unseen objects from novel categories, all from a single image of novel scenes. We propose a challenging simulated dataset with a diverse set of textured objects for training and testing. Experiments show that MORF extracts accurate object geometry and supports realistic object and scene reconstruction and editing, significantly outperforming the state-of-the-art",
    "checked": false,
    "id": "3508622191ed3d09942ebb7ab35f03058dbfd6b2",
    "semantic_title": "unsupervised 3d scene representation learning via mov-able object inference",
    "citation_count": 0,
    "authors": [
      "Honglin Chen",
      "Wanhee Lee",
      "Hong-Xing Yu",
      "Rahul Mysore Venkatesh",
      "Joshua B. Tenenbaum",
      "Daniel Bear",
      "Jiajun Wu",
      "Daniel LK Yamins"
    ]
  },
  "https://openreview.net/forum?id=OycfV3Mhfq": {
    "title": "Convergence Analysis of Fractional Gradient Descent",
    "volume": "main",
    "abstract": "Fractional derivatives are a well-studied generalization of integer order derivatives. Naturally, for optimization, it is of interest to understand the convergence properties of gradient descent using fractional derivatives. Convergence analysis of fractional gradient descent is currently limited both in the methods analyzed and the settings analyzed. This paper aims to fill in these gaps by analyzing variations of fractional gradient descent in smooth and convex, smooth and strongly convex, and smooth and non-convex settings. First, novel bounds will be established bridging fractional and integer derivatives. Then, these bounds will be applied to the aforementioned settings to prove linear convergence for smooth and strongly convex functions and $O(1/T)$ convergence for smooth and convex functions. Additionally, we prove $O(1/T)$ convergence for smooth and non-convex functions using an extended notion of smoothness - H\\\"older smoothness - that is more natural for fractional derivatives. Finally, empirical results will be presented on the potential speed up of fractional gradient descent over standard gradient descent as well as some preliminary theoretical results explaining this speed up",
    "checked": true,
    "id": "6be30d276a9462a971b46573c77f12aa6b6f7c1d",
    "semantic_title": "convergence analysis of fractional gradient descent",
    "citation_count": 1,
    "authors": [
      "Ashwani Aggarwal"
    ]
  },
  "https://openreview.net/forum?id=i02A009I5a": {
    "title": "VidEdit: Zero-Shot and Spatially Aware Text-Driven Video Editing",
    "volume": "main",
    "abstract": "Recently, diffusion-based generative models have achieved remarkable success for image generation and edition. However, existing diffusion-based video editing approaches lack the ability to offer precise control over generated content that maintains temporal consistency in long-term videos. On the other hand, atlas-based methods provide strong temporal consistency but are costly to edit a video and lack spatial control. In this work, we introduce VidEdit, a novel method for zero-shot text-based video editing that guarantees robust temporal and spatial consistency. In particular, we combine an atlas-based video representation with a pre-trained text-to-image diffusion model to provide a training-free and efficient video editing method, which by design fulfills temporal smoothness. To grant precise user control over generated content, we utilize conditional information extracted from off-the-shelf panoptic segmenters and edge detectors which guides the diffusion sampling process. This method ensures a fine spatial control on targeted regions while strictly preserving the structure of the original video. Our quantitative and qualitative experiments show that VidEdit outperforms state-of-the-art methods on DAVIS dataset, regarding semantic faithfulness, image preservation, and temporal consistency metrics. With this framework, processing a single video only takes approximately one minute, and it can generate multiple compatible edits based on a unique text prompt",
    "checked": true,
    "id": "33e7493ebe199b44620957e91f65f5b2de34df5e",
    "semantic_title": "videdit: zero-shot and spatially aware text-driven video editing",
    "citation_count": 17,
    "authors": [
      "Paul Couairon",
      "Clément Rambour",
      "Jean-Emmanuel HAUGEARD",
      "Nicolas THOME"
    ]
  },
  "https://openreview.net/forum?id=oYIjw37pTP": {
    "title": "An optimal control perspective on diffusion-based generative modeling",
    "volume": "main",
    "abstract": "We establish a connection between stochastic optimal control and generative models based on stochastic differential equations (SDEs), such as recently developed diffusion probabilistic models. In particular, we derive a Hamilton--Jacobi--Bellman equation that governs the evolution of the log-densities of the underlying SDE marginals. This perspective allows to transfer methods from optimal control theory to generative modeling. First, we show that the evidence lower bound is a direct consequence of the well-known verification theorem from control theory. Further, we can formulate diffusion-based generative modeling as a minimization of the Kullback--Leibler divergence between suitable measures in path space. Finally, we develop a novel diffusion-based method for sampling from unnormalized densities -- a problem frequently occurring in statistics and computational sciences. We demonstrate that our time-reversed diffusion sampler (DIS) can outperform other diffusion-based sampling approaches on multiple numerical examples",
    "checked": true,
    "id": "5f8c041cb8985ee922625b5e65bece10e646f31f",
    "semantic_title": "an optimal control perspective on diffusion-based generative modeling",
    "citation_count": 28,
    "authors": [
      "Julius Berner",
      "Lorenz Richter",
      "Karen Ullrich"
    ]
  },
  "https://openreview.net/forum?id=xqAVkqrLjx": {
    "title": "HQ-VAE: Hierarchical Discrete Representation Learning with Variational Bayes",
    "volume": "main",
    "abstract": "Vector quantization (VQ) is a technique to deterministically learn features with discrete codebook representations. It is commonly performed with a variational autoencoding model, VQ-VAE, which can be further extended to hierarchical structures for making high-fidelity reconstructions. However, such hierarchical extensions of VQ-VAE often suffer from the codebook/layer collapse issue, where the codebook is not efficiently used to express the data, and hence degrades reconstruction accuracy. To mitigate this problem, we propose a novel unified framework to stochastically learn hierarchical discrete representation on the basis of the variational Bayes framework, called hierarchically quantized variational autoencoder (HQ-VAE). HQ-VAE naturally generalizes the hierarchical variants of VQ-VAE, such as VQ-VAE-2 and residual-quantized VAE (RQ-VAE), and provides them with a Bayesian training scheme. Our comprehensive experiments on image datasets show that HQ-VAE enhances codebook usage and improves reconstruction performance. We also validated HQ-VAE in terms of its applicability to a different modality with an audio dataset",
    "checked": true,
    "id": "316073877918ffe26e5a3baeb28d1c2dec760d54",
    "semantic_title": "hq-vae: hierarchical discrete representation learning with variational bayes",
    "citation_count": 0,
    "authors": [
      "Yuhta Takida",
      "Yukara Ikemiya",
      "Takashi Shibuya",
      "Kazuki Shimada",
      "Woosung Choi",
      "Chieh-Hsin Lai",
      "Naoki Murata",
      "Toshimitsu Uesaka",
      "Kengo Uchida",
      "Wei-Hsiang Liao",
      "Yuki Mitsufuji"
    ]
  },
  "https://openreview.net/forum?id=chbRsWwjax": {
    "title": "InfoNCE is variational inference in a recognition parameterised model",
    "volume": "main",
    "abstract": "Here, we develop a new class of Bayesian latent variable model, the recognition parameterised model (RPM). RPMs have an implicit likelihood, which is defined in terms of the recognition model. Therefore, it is not possible to do traditional \"generation\" with RPMs. Instead, RPMs are designed to learn good latent representations of data (in modern parlance, they solve a self-supervised learning task). Indeed, the RPM implicit likelihood is specifically designed so that it drops out of the VI objective, the ELBO. That allows us to learn an RPM without a \"reconstruction\" step, which is believed to be at the root of poor latent representations learned by VAEs. Indeed, in a very specific setting where we learn the optimal prior, the RPM ELBO becomes equal to the mutual information (MI; up to a constant), establishing a connection to pre-existing self-supervised learning methods such as InfoNCE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Laurence Aitchison",
      "Stoil Krasimirov Ganev"
    ]
  },
  "https://openreview.net/forum?id=cgSXpAR4Gl": {
    "title": "Optimal Transport Perturbations for Safe Reinforcement Learning with Robustness Guarantees",
    "volume": "main",
    "abstract": "Robustness and safety are critical for the trustworthy deployment of deep reinforcement learning. Real-world decision making applications require algorithms that can guarantee robust performance and safety in the presence of general environment disturbances, while making limited assumptions on the data collection process during training. In order to accomplish this goal, we introduce a safe reinforcement learning framework that incorporates robustness through the use of an optimal transport cost uncertainty set. We provide an efficient implementation based on applying Optimal Transport Perturbations to construct worst-case virtual state transitions, which does not impact data collection during training and does not require detailed simulator access. In experiments on continuous control tasks with safety constraints, our approach demonstrates robust performance while significantly improving safety at deployment time compared to standard safe reinforcement learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Queeney",
      "Erhan Can Ozcan",
      "Ioannis Paschalidis",
      "Christos Cassandras"
    ]
  },
  "https://openreview.net/forum?id=mhawjZcmrJ": {
    "title": "New Guarantees for Learning Revenue Maximizing Menus of Lotteries and Two-Part Tariffs",
    "volume": "main",
    "abstract": "We advance a recently flourishing line of work at the intersection of learning theory and computational economics by studying the learnability of two classes of mechanisms prominent in economics, namely menus of lotteries and two-part tariffs. The former is a family of randomized mechanisms designed for selling multiple items, known to achieve revenue beyond deterministic mechanisms, while the latter is designed for selling multiple units (copies) of a single item with applications in real-world scenarios such as car or bike-sharing services. We focus on learning high-revenue mechanisms of this form from buyer valuation data in both distributional settings, where we have access to buyers' valuation samples up-front, and the more challenging and less-studied online settings, where buyers arrive one-at-a-time and no distributional assumption is made about their values. We provide a suite of results with regard to these two families of mechanisms. We provide the first online learning algorithms for menus of lotteries and two-part tariffs with strong regret-bound guarantees. Since the space of parameters is infinite and the revenue functions have discontinuities, the known techniques do not readily apply. However, we are able to provide a reduction to online learning over a finite number of experts, in our case, a finite number of parameters. Furthermore, in the limited buyers type case, we show a reduction to online linear optimization, which allows us to obtain no-regret guarantees by presenting buyers with menus that correspond to a barycentric spanner. In addition, we provide algorithms with improved running times over prior work for the distributional settings. Finally, we demonstrate how techniques from the recent literature in data-driven algorithm design are insufficient for our studied problems",
    "checked": false,
    "id": "969f3993d202bfc5207f49f0b32822edf2e61086",
    "semantic_title": "learning revenue maximizing menus of lotteries and two-part tariffs",
    "citation_count": 2,
    "authors": [
      "Maria Florina Balcan",
      "Hedyeh Beyhaghi"
    ]
  },
  "https://openreview.net/forum?id=Xxw0edFFQC": {
    "title": "Optical Transformers",
    "volume": "main",
    "abstract": "The rapidly increasing size of deep-learning models has renewed interest in alternatives to digital-electronic computers as a means to dramatically reduce the energy cost of running state-of-the-art neural networks. Optical matrix-vector multipliers are best suited to performing computations with very large operands, which suggests that large Transformer models could be a good target for them. In this paper, we investigate---through a combination of simulations and experiments on prototype optical hardware---the feasibility and potential energy benefits of running Transformer models on future optical accelerators that perform matrix-vector multiplication. We use simulations, with noise models validated by small-scale optical experiments, to show that optical accelerators for matrix-vector multiplication should be able to accurately run a typical Transformer architecture model for language processing. We demonstrate that optical accelerators can achieve the same (or better) perplexity as digital-electronic processors at 8-bit precision, provided that the optical hardware uses sufficiently many photons per inference, which translates directly to a requirement on optical energy per inference. We studied numerically how the requirement on optical energy per inference changes as a function of the Transformer width $d$ and found that the optical energy per multiply--accumulate (MAC) scales approximately as $\\frac{1}{d}$, giving an asymptotic advantage over digital systems. We also analyze the total system energy costs for optical accelerators running Transformers, including both optical and electronic costs, as a function of model size. We predict that well-engineered, large-scale optical hardware should be able to achieve a $100 \\times$ energy-efficiency advantage over current digital-electronic processors in running some of the largest current Transformer models, and if both the models and the optical hardware are scaled to the quadrillion-parameter regime, optical accelerators could have a $>8,000\\times$ energy-efficiency advantage. Under plausible assumptions about future improvements to electronics and Transformer quantization techniques (5× cheaper memory access, double the digital--analog conversion efficiency, and 4-bit precision), we estimate that the energy advantage for optical processors versus electronic processors operating at 300~fJ/MAC could grow to $>100,000\\times$",
    "checked": true,
    "id": "139ebdcaa3d5f60a3c5289bb61d5865141886d51",
    "semantic_title": "optical transformers",
    "citation_count": 7,
    "authors": [
      "Maxwell Anderson",
      "Shi-Yuan Ma",
      "Tianyu Wang",
      "Logan Wright",
      "Peter McMahon"
    ]
  },
  "https://openreview.net/forum?id=Db5c3Wxj9E": {
    "title": "Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real World",
    "volume": "main",
    "abstract": "Sparse training has emerged as a promising method for resource-efficient deep neural networks (DNNs) in real-world applications. However, the reliability of sparse models remains a crucial concern, particularly in detecting unknown out-of-distribution (OOD) data. This study addresses the knowledge gap by investigating the reliability of sparse training from an OOD perspective and reveals that sparse training exacerbates OOD unreliability. The lack of unknown information and the sparse constraints hinder the effective exploration of weight space and accurate differentiation between known and unknown knowledge. To tackle these challenges, we propose a new unknown-aware sparse training method, which incorporates a loss modification, auto-tuning strategy, and a voting scheme to guide weight space exploration and mitigate confusion between known and unknown information without incurring significant additional costs or requiring access to additional OOD data. Theoretical insights demonstrate how our method reduces model confidence when faced with OOD samples. Empirical experiments across multiple datasets, model architectures, and sparsity levels validate the effectiveness of our method, with improvements of up to \\textbf{8.4\\%} in AUROC while maintaining comparable or higher accuracy and calibration. This research enhances the understanding and readiness of sparse DNNs for deployment in resource-limited applications. Our code is available on: \\url{https://github.com/StevenBoys/MOON}",
    "checked": true,
    "id": "3483f2ac5e931d991ae11aa54996e23c7827a5ae",
    "semantic_title": "embracing unknown step by step: towards reliable sparse training in real world",
    "citation_count": 0,
    "authors": [
      "Bowen Lei",
      "Dongkuan Xu",
      "Ruqi Zhang",
      "Bani Mallick"
    ]
  },
  "https://openreview.net/forum?id=WbbgOHpoPX": {
    "title": "Revisiting Random Weight Perturbation for Efficiently Improving Generalization",
    "volume": "main",
    "abstract": "Improving the generalization ability of modern deep neural networks (DNNs) is a fundamental challenge in machine learning. Two branches of methods have been proposed to seek flat minima and improve generalization: one led by sharpness-aware minimization (SAM) minimizes the worst-case neighborhood loss through adversarial weight perturbation (AWP), and the other minimizes the expected Bayes objective with random weight perturbation (RWP). While RWP offers advantages in computation and is closely linked to AWP on a mathematical basis, its empirical performance has consistently lagged behind that of AWP. In this paper, we revisit the use of RWP for improving generalization and propose improvements from two perspectives: i) the trade-off between generalization and convergence and ii) the random perturbation generation. Through extensive experimental evaluations, we demonstrate that our enhanced RWP methods achieve greater efficiency in enhancing generalization, particularly in large-scale problems, while also offering comparable or even superior performance to SAM. The code is released at https://github.com/nblt/mARWP",
    "checked": true,
    "id": "3839d0b22aba059b28cc394dea285701592878ea",
    "semantic_title": "revisiting random weight perturbation for efficiently improving generalization",
    "citation_count": 1,
    "authors": [
      "Tao Li",
      "Qinghua Tao",
      "Weihao Yan",
      "Yingwen Wu",
      "Zehao Lei",
      "Kun Fang",
      "Mingzhen He",
      "Xiaolin Huang"
    ]
  },
  "https://openreview.net/forum?id=HxfqTdLIRF": {
    "title": "Double Descent and Overfitting under Noisy Inputs and Distribution Shift for Linear Denoisers",
    "volume": "main",
    "abstract": "Despite the importance of denoising in modern machine learning and ample empirical work on supervised denoising, its theoretical understanding is still relatively scarce. One concern about studying supervised denoising is that one might not always have noiseless training data from the test distribution. It is more reasonable to have access to noiseless training data from a different dataset than the test dataset. Motivated by this, we study supervised denoising and noisy-input regression under distribution shift. We add three considerations to increase the applicability of our theoretical insights to real-life data and modern machine learning. First, while most past theoretical work assumes that the data covariance matrix is full-rank and well-conditioned, empirical studies have shown that real-life data is approximately low-rank. Thus, we assume that our data matrices are low-rank. Second, we drop independence assumptions on our data. Third, the rise in computational power and dimensionality of data have made it important to study non-classical regimes of learning. Thus, we work in the non-classical proportional regime, where data dimension $d$ and number of samples $N$ grow as $d/N = c + o(1)$. For this setting, we derive \\rishi{data-dependent, instance specific} expressions for the test error for both denoising and noisy-input regression, and study when overfitting the noise is benign, tempered or catastrophic. We show that the test error exhibits double descent under general distribution shift, providing insights for data augmentation and the role of noise as an implicit regularizer. We also perform experiments using real-life data, where we match the theoretical predictions with under 1\\% MSE error for low-rank data",
    "checked": true,
    "id": "d2b4d78284021b523df506784779eaf4c75ef89e",
    "semantic_title": "double descent and overfitting under noisy inputs and distribution shift for linear denoisers",
    "citation_count": 1,
    "authors": [
      "Chinmaya Kausik",
      "Kashvi Srivastava",
      "Rishi Sonthalia"
    ]
  },
  "https://openreview.net/forum?id=daXqjb6dVE": {
    "title": "From Differential Privacy to Bounds on Membership Inference: Less can be More",
    "volume": "main",
    "abstract": "Differential Privacy (DP) is the de facto standard for reasoning about the privacy of a training algorithm. Yet, learning with DP often yields poor performance unless one trains on a large dataset. In this paper, we instead outline how training on less data can be beneficial when we are only interested in defending against specific attacks; we take the canonical example of defending against membership inference. To arrive at this result, we first derive (tight) bounds on the success of all membership inference attacks. These bounds do not replace DP, rather they introduce a complementary interpretation of a DP algorithm's ability to defend against membership inference specifically. Because our bound more tightly captures the effect of how training data was selected, we can show that decreasing the sampling rate when constructing the training dataset has a disparate effect on the bound when compared to strengthening the DP guarantee. Thus, when the privacy protection we care about is defending against membership inference, training on less data can yield more advantageous trade-offs between preventing membership inference and utility than strengthening the DP guarantee. We empirically illustrate this on MNIST, CIFAR10 and SVHN-extended",
    "checked": false,
    "id": "3a766e0bdc981032b54d1f02c5174379a8b1530b",
    "semantic_title": "nearly tight black-box auditing of differentially private machine learning",
    "citation_count": 0,
    "authors": [
      "Anvith Thudi",
      "Ilia Shumailov",
      "Franziska Boenisch",
      "Nicolas Papernot"
    ]
  },
  "https://openreview.net/forum?id=1fNcpcdr1o": {
    "title": "CiPR: An Efficient Framework with Cross-instance Positive Relations for Generalized Category Discovery",
    "volume": "main",
    "abstract": "We tackle the issue of generalized category discovery (GCD). GCD considers the open-world problem of automatically clustering a partially labelled dataset, in which the unlabelled data may contain instances from both novel categories and labelled classes. In this paper, we address the GCD problem with an unknown category number for the unlabelled data. We propose a framework, named CiPR, to bootstrap the representation by exploiting cross-instance positive relations in the partially labelled data for contrastive learning, which have been neglected in existing methods. To obtain reliable cross-instance relations to facilitate representation learning, we introduce a semi-supervised hierarchical clustering algorithm, named selective neighbor clustering (SNC), which can produce a clustering hierarchy directly from the connected components of a graph constructed from selective neighbors. We further present a method to estimate the unknown class number using SNC with a joint reference score that considers clustering indexes of both labelled and unlabelled data, and extend SNC to allow label assignment for the unlabelled instances with a given class number. We thoroughly evaluate our framework on public generic image recognition datasets and challenging fine-grained datasets, and establish a new state-of-the-art. Code: https://github.com/haoosz/CiPR",
    "checked": true,
    "id": "c2c114fb7747dc7eb240a8657a810ab229c4ee49",
    "semantic_title": "cipr: an efficient framework with cross-instance positive relations for generalized category discovery",
    "citation_count": 4,
    "authors": [
      "Shaozhe Hao",
      "Kai Han",
      "Kwan-Yee K. Wong"
    ]
  },
  "https://openreview.net/forum?id=H2EeStRTQn": {
    "title": "Introducing \"Forecast Utterance\" for Conversational Data Science",
    "volume": "main",
    "abstract": "Envision an intelligent agent capable of assisting users in conducting forecasting tasks through intuitive, natural conversations, without requiring in-depth knowledge of the underlying machine learning (ML) processes. A significant challenge for the agent in this endeavor is to accurately comprehend the user's prediction goals and, consequently, formulate precise ML tasks. In this paper, we take a pioneering step towards this ambitious goal by introducing a new concept called Forecast Utterance and then focus on the automatic and accurate interpretation of users' prediction goals from these utterances. Specifically, we frame the task as a slot-filling problem, where each slot corresponds to a specific aspect of the goal prediction task. We then employ two zero-shot methods for solving the slot-filling task, namely: 1) Entity Extraction (EE), and 2) Question-Answering (QA) techniques. Our experiments, evaluated with three meticulously crafted data sets, validate the viability of our ambitious goal and demonstrate the effectiveness of both EE and QA techniques in interpreting Forecast Utterances",
    "checked": true,
    "id": "f8bd251dfb1f0640440eafc3e535821f2d007dd5",
    "semantic_title": "introducing \"forecast utterance\" for conversational data science",
    "citation_count": 0,
    "authors": [
      "Md. Mahadi Hassan",
      "Alex Knipper",
      "Shubhra Kanti Karmaker Santu"
    ]
  },
  "https://openreview.net/forum?id=ehfRiF0R3a": {
    "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
    "volume": "main",
    "abstract": "We introduce Voyager, the first LLM-powered embodied lifelong learning agent in an open-ended world that continuously explores, acquires diverse skills, and makes novel discoveries without human intervention in Minecraft. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's capability rapidly and alleviates catastrophic forgetting. Empirically, Voyager demonstrates strong in-context lifelong learning capabilities. It outperforms prior SOTA by obtaining 3.1x more unique items, unlocking tech tree milestones up to 15.3x faster, and traveling 2.3x longer distances. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize",
    "checked": true,
    "id": "f197bf0fc2f228483f6af3285000d54d8d97f9eb",
    "semantic_title": "voyager: an open-ended embodied agent with large language models",
    "citation_count": 365,
    "authors": [
      "Guanzhi Wang",
      "Yuqi Xie",
      "Yunfan Jiang",
      "Ajay Mandlekar",
      "Chaowei Xiao",
      "Yuke Zhu",
      "Linxi Fan",
      "Anima Anandkumar"
    ]
  },
  "https://openreview.net/forum?id=Pe6hldOUkw": {
    "title": "Optimal Inference in Contextual Stochastic Block Models",
    "volume": "main",
    "abstract": "The contextual stochastic block model (CSBM) was proposed for unsupervised community detection on attributed graphs where both the graph and the high-dimensional node information correlate with node labels. In the context of machine learning on graphs, the CSBM has been widely used as a synthetic dataset for evaluating the performance of graph-neural networks (GNNs) for semi-supervised node classification. We consider a probabilistic Bayes-optimal formulation of the inference problem and we derive a belief-propagation-based algorithm for the semi-supervised CSBM; we conjecture it is optimal in the considered setting and we provide its implementation. We show that there can be a considerable gap between the accuracy reached by this algorithm and the performance of the GNN architectures proposed in the literature. This suggests that the CSBM, along with the comparison to the performance of the optimal algorithm, readily accessible via our implementation, can be instrumental in the development of more performant GNN architectures",
    "checked": true,
    "id": "e73d0775b3964906594e0f0980515642d656db26",
    "semantic_title": "optimal inference in contextual stochastic block models",
    "citation_count": 2,
    "authors": [
      "O Duranthon",
      "Lenka Zdeborova"
    ]
  },
  "https://openreview.net/forum?id=ux9BrxPCl8": {
    "title": "Grokking Beyond Neural Networks: An Empirical Exploration with Model Complexity",
    "volume": "main",
    "abstract": "In some settings neural networks exhibit a phenomenon known as \\textit{grokking}, where they achieve perfect or near-perfect accuracy on the validation set long after the same performance has been achieved on the training set. In this paper, we discover that grokking is not limited to neural networks but occurs in other settings such as Gaussian process (GP) classification, GP regression, linear regression and Bayesian neural networks. We also uncover a mechanism by which to induce grokking on algorithmic datasets via the addition of dimensions containing spurious information. The presence of the phenomenon in non-neural architectures shows that grokking is not restricted to settings considered in current theoretical and empirical studies. Instead, grokking may be possible in any model where solution search is guided by complexity and error",
    "checked": true,
    "id": "0c8b7af8ebb4c1039ba730803bc3e82dd02f8b62",
    "semantic_title": "grokking beyond neural networks: an empirical exploration with model complexity",
    "citation_count": 2,
    "authors": [
      "Jack William Miller",
      "Charles O'Neill",
      "Thang D Bui"
    ]
  },
  "https://openreview.net/forum?id=s1qh12FReM": {
    "title": "Compressing the Activation Maps in Deep Convolutional Neural Networks and Its Regularizing Effect",
    "volume": "main",
    "abstract": "Deep learning has dramatically improved performance in various image analysis applications in the last few years. However, recent deep learning architectures can be very large, with up to hundreds of layers and millions or even billions of model parameters that are impossible to fit into commodity graphics processing units. We propose a novel approach for compressing high-dimensional activation maps, the most memory-consuming part when training modern deep learning architectures. The proposed method can be used to compress the feature maps of a single layer, multiple layers, or the entire network according to specific needs. To this end, we also evaluated three different methods to compress the activation maps: Wavelet Transform, Discrete Cosine Transform, and Simple Thresholding. We performed experiments in two classification tasks for natural images and two semantic segmentation tasks for medical images. Using the proposed method, we could reduce the memory usage for activation maps by up to 95%. Additionally, we show that the proposed method induces a regularization effect that acts on the layer weight gradients",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minh Hoang Vu",
      "Anders Garpebring",
      "Tufve Nyholm",
      "Tommy Löfstedt"
    ]
  },
  "https://openreview.net/forum?id=bQKHMSE4SH": {
    "title": "Towards Understanding Dual BN In Hybrid Adversarial Training",
    "volume": "main",
    "abstract": "There is a growing concern about applying batch normalization (BN) in adversarial training (AT), especially when the model is trained on both adversarial samples and clean samples (termed Hybrid-AT). With the assumption that adversarial and clean samples are from two different domains, a common practice in prior works is to adopt Dual BN, where BN$_{adv}$ and BN$_{clean}$ are used for adversarial and clean branches, respectively. A popular belief for motivating Dual BN is that estimating normalization statistics of this mixture distribution is challenging and thus disentangling it for normalization achieves stronger robustness. In contrast to this belief, we reveal that disentangling statistics plays a less role than disentangling affine parameters in model training. This finding aligns with prior work (Rebuffi et al., 2023), and we build upon their research for further investigations. We demonstrate that the domain gap between adversarial and clean samples is not very large, which is counter-intuitive considering the significant influence of adversarial perturbation on the model accuracy. We further propose a two-task hypothesis which serves as the empirical foundation and a unified framework for Hybrid-AT improvement. We also investigate Dual BN in test-time and reveal that affine parameters characterize the robustness during inference. Overall, our work sheds new light on understanding the mechanism of Dual BN in Hybrid-AT and its underlying justification",
    "checked": true,
    "id": "0204ff243f758f325153371c1ce97dc4ce19d786",
    "semantic_title": "towards understanding dual bn in hybrid adversarial training",
    "citation_count": 0,
    "authors": [
      "Chenshuang Zhang",
      "Chaoning Zhang",
      "Kang Zhang",
      "Axi Niu",
      "Junmo Kim",
      "In So Kweon"
    ]
  },
  "https://openreview.net/forum?id=1LoVwFkZNo": {
    "title": "ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions",
    "volume": "main",
    "abstract": "Asking insightful questions is crucial for acquiring knowledge and expanding our understanding of the world. However, the importance of questioning has been largely overlooked in AI research, where models have been primarily developed to answer questions. With the recent advancements of large language models (LLMs) like ChatGPT, we discover their capability to ask high-quality questions when provided with a suitable prompt. This discovery presents a new opportunity to develop an automatic questioning system. In this paper, we introduce ChatCaptioner, a novel automatic-questioning method deployed in image captioning. Here, ChatGPT is prompted to ask a series of informative questions about images to BLIP-2, a strong vision question-answering model. In ChatCaptioner, we investigate whether two AI models, unable to individually describe images in detail, can collaborate through an automated, visually guided dialogue to generate a better and more enriched image description than a single AI model. We conduct human-subject evaluations on common image caption datasets such as COCO, Conceptual Caption, and WikiArt, and compare ChatCaptioner with BLIP-2 as well as ground truth. Our results demonstrate that ChatCaptioner's captions are significantly more informative, receiving three times as many votes from human evaluators as BLIP-2 alone for providing the most image information. Besides, ChatCaptioner identifies 53% more objects within the image than BLIP-2 alone measured by WordNet synset matching",
    "checked": true,
    "id": "69cfdc8df16ae63b7acba4ac6f727f78b86893c3",
    "semantic_title": "chatgpt asks, blip-2 answers: automatic questioning towards enriched visual descriptions",
    "citation_count": 63,
    "authors": [
      "Deyao Zhu",
      "Jun Chen",
      "Kilichbek Haydarov",
      "Xiaoqian Shen",
      "Wenxuan Zhang",
      "Mohamed Elhoseiny"
    ]
  },
  "https://openreview.net/forum?id=q4iSLPoFe7": {
    "title": "Revisiting Generalized p-Laplacian Regularized Framelet GCNs: Convergence, Energy Dynamic and as Non-Linear Diffusion",
    "volume": "main",
    "abstract": "This paper presents a comprehensive theoretical analysis of the graph p-Laplacian regularized framelet network (pL-UFG) to establish a solid understanding of its properties. We conduct a convergence analysis on pL-UFG, addressing the gap in the understanding of its asymptotic behaviors. Further by investigating the generalized Dirichlet energy of pL-UFG, we demonstrate that the Dirichlet energy remains non-zero throughout convergence, ensuring the avoidance of over-smoothing issues. Additionally, we elucidate the energy dynamic perspective, highlighting the synergistic relationship between the implicit layer in pL-UFG and graph framelets. This synergy enhances the model's adaptability to both homophilic and heterophilic data. Notably, we reveal that pL-UFG can be interpreted as a generalized non-linear diffusion process, thereby bridging the gap between pL-UFG and differential equations on the graph. Importantly, these multifaceted analyses lead to unified conclusions that offer novel insights for understanding and implementing pL-UFG, as well as other graph neural network (GNN) models. Finally, based on our dynamic analysis, we propose two novel pL-UFG models with manually controlled energy dynamics. We demonstrate empirically and theoretically that our proposed models not only inherit the advantages of pL-UFG but also significantly reduce computational costs for training on large-scale graph datasets",
    "checked": true,
    "id": "5334fab208faeac2d164086cc4254a45abaf4883",
    "semantic_title": "revisiting generalized p-laplacian regularized framelet gcns: convergence, energy dynamic and as non-linear diffusion",
    "citation_count": 1,
    "authors": [
      "Dai Shi",
      "Zhiqi Shao",
      "Yi Guo",
      "Qibin Zhao",
      "Junbin Gao"
    ]
  },
  "https://openreview.net/forum?id=H4OE7toXpa": {
    "title": "Inverse Kernel Decomposition",
    "volume": "main",
    "abstract": "The state-of-the-art dimensionality reduction approaches largely rely on complicated optimization procedures. On the other hand, closed-form approaches requiring merely eigen-decomposition do not have enough sophistication and nonlinearity. In this paper, we propose a novel nonlinear dimensionality reduction method---Inverse Kernel Decomposition (IKD)---based on an eigen-decomposition of the sample covariance matrix of data. The method is inspired by Gaussian process latent variable models (GPLVMs) and has comparable performance with GPLVMs. To deal with very noisy data with weak correlations, we propose two solutions---blockwise and geodesic---to make use of locally correlated data points and provide better and numerically more stable latent estimations. We use synthetic datasets and four real-world datasets to show that IKD is a better dimensionality reduction method than other eigen-decomposition-based methods, and achieves comparable performance against optimization-based methods with faster running speeds. Open-source IKD implementation in Python can be accessed at \\url{https://github.com/JerrySoybean/ikd}",
    "checked": true,
    "id": "34a61e9d173eb6ffa1d13d59c39732ece8e67c47",
    "semantic_title": "inverse kernel decomposition",
    "citation_count": 0,
    "authors": [
      "Chengrui Li",
      "Anqi Wu"
    ]
  },
  "https://openreview.net/forum?id=yUmJ483OB0": {
    "title": "Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding",
    "volume": "main",
    "abstract": "This paper presents Predictive Pipelined Decoding (PPD), an approach that speeds up decoding in Large Language Models (LLMs) while maintaining the exact same output as the original decoding. Unlike conventional strategies, PPD employs additional compute resources to parallelize the initiation of subsequent token decoding during the current token decoding. This method reduces decoding latency and reshapes the understanding of trade-offs in LLM decoding strategies. We have developed a theoretical framework that allows us to analyze the trade-off between computation and latency. Using this framework, we can analytically estimate the potential reduction in latency associated with our proposed method, achieved through the assessment of the match rate, represented as $p_\\text{correct}$. The results demonstrate that the use of extra computational resources has the potential to accelerate LLM decoding. Additionally, we implement PPD and conduct preliminary experiments to empirically validate its efficacy, addressing potential practical overheads not covered by theoretical analysis",
    "checked": true,
    "id": "d988eb48e8b4e471f5df9d081bfc32db0781e6bf",
    "semantic_title": "predictive pipelined decoding: a compute-latency trade-off for exact llm decoding",
    "citation_count": 9,
    "authors": [
      "Seongjun Yang",
      "Gibbeum Lee",
      "Jaewoong Cho",
      "Dimitris Papailiopoulos",
      "Kangwook Lee"
    ]
  },
  "https://openreview.net/forum?id=8GI1SXqJBk": {
    "title": "Maximizing Global Model Appeal in Federated Learning",
    "volume": "main",
    "abstract": "Federated learning (FL) aims to collaboratively train a global model using local data from a network of clients. To warrant collaborative training, each federated client may expect the resulting global model to satisfy some individual requirement, such as achieving a certain loss threshold on their local data. However, in real FL scenarios, the global model may not satisfy the requirements of all clients in the network due to the data heterogeneity across clients. In this work, we explore the problem of global model appeal in FL, which we define as the total number of clients that find that the global model satisfies their individual requirements. We discover that global models trained using traditional FL approaches can result in a significant number of clients unsatisfied with the model based on their local requirements. As a consequence, we show that global model appeal can directly impact how clients participate in training and how the model performs on new clients at inference time. Our work proposes MaxFL, which maximizes the number of clients that find the global model appealing. MaxFL achieves a $22$-$40\\%$ and $18$-$50\\%$ improvement in the test accuracy of training clients and (unseen) test clients respectively, compared to a wide range of FL approaches that tackle data heterogeneity, aim to incentivize clients, and learn personalized/fair models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yae Jee Cho",
      "Divyansh Jhunjhunwala",
      "Tian Li",
      "Virginia Smith",
      "Gauri Joshi"
    ]
  },
  "https://openreview.net/forum?id=f9l4eiPKpV": {
    "title": "Learning Sparse Graphs for Functional Regression using Graph-induced Operator-valued Kernels",
    "volume": "main",
    "abstract": "A functional regression problem aims to learn a map $\\mathfrak{F}:\\mathcal{Z}\\mapsto\\mathcal{Y}$, where $\\mathcal{Z}$ is an appropriate input space and $\\mathcal{Y}$ is a space of output functions. When $\\mathcal{Z}$ is also a space of functions, the learning problem is known as function-to-function regression. In this work, we consider the problem of learning a map of the form ${F}:{\\mathcal{Z}}^p\\mapsto\\mathcal{Y}$, a many-to-one function-to-function regression problem, where the aim is to learn a suitable $F$ which maps $p$ input functions to an output function. In order to solve this regression problem with $p$ input functions and a corresponding output function, we propose a graph-induced operator-valued kernel (OVK) obtained by imposing a graphical structure describing the inter-relationships among the $p$ input functions. When the underlying graphical structure is unknown, we propose to learn an appropriate Laplacian matrix characterizing the graphical structure, which would also aid in learning the map $F$. We formulate a learning problem using the proposed graph-induced OVK, and devise an alternating minimization framework to solve the learning problem. To learn $F$ along with meaningful and important interactions in the graphical structure, a minimax concave penalty (MCP) is used as a sparsity-inducing regularization on the Laplacian matrix. We further extend the alternating minimization framework to learn $F$, where each of the $p$ constituent input functions as well as the output function are multi-dimensional. To scale the proposed algorithm to large datasets, we design an efficient sample-based approximation algorithm. Further, we provide bounds on generalization error for the map obtained by solving the proposed learning problem. An extensive empirical evaluation on both synthetic and real data demonstrates the utility of the proposed learning framework. Our experiments show that simultaneous learning of $F$ along with sparse graphical structure helps in discovering significant relationships among the input functions, and motivates interpretability of such relationships driving the regression problem",
    "checked": false,
    "id": "9e9ed5b08218f24ca6e11afaf9a1dce823eba163",
    "semantic_title": "scalable gaussian process regression enables accurate prediction of protein and small molecule properties with uncertainty quantitation",
    "citation_count": 1,
    "authors": [
      "Akash Saha",
      "Balamurugan Palaniappan"
    ]
  },
  "https://openreview.net/forum?id=JYbnJ92TJf": {
    "title": "Addressing Attribute Bias with Adversarial Support-Matching",
    "volume": "main",
    "abstract": "When trained on diverse labelled data, machine learning models have proven themselves to be a powerful tool in all facets of society. However, due to budget limitations, deliberate or non-deliberate censorship, and other problems during data collection, certain groups may be under-represented in the labelled training set. We investigate a scenario in which the absence of certain data is linked to the second level of a two-level hierarchy in the data. Inspired by the idea of protected attributes from algorithmic fairness, we consider generalised secondary \"attributes\" which subdivide the classes into smaller partitions. We refer to the partitions defined by the combination of an attribute and a class label, or leaf nodes in aforementioned hierarchy, as groups. To characterise the problem, we introduce the concept of classes with incomplete attribute support. The representational bias in the training set can give rise to spurious correlations between the classes and the attributes which cause standard classification models to generalise poorly to unseen groups. To overcome this bias, we make use of an additional, diverse but unlabelled dataset, called the deployment set, to learn a representation that is invariant to the attributes. This is done by adversarially matching the support of the training and deployment sets in representation space using a set discriminator operating on sets, or bags, of samples. In order to learn the desired invariance, it is paramount that the bags are balanced by class; this is easily achieved for the training set, but requires using semi-supervised clustering for the deployment set. We demonstrate the effectiveness of our method on several datasets and realisations of the problem",
    "checked": true,
    "id": "ed71c8ed6b386950c004157f83ee933bf2a3c0eb",
    "semantic_title": "addressing attribute bias with adversarial support-matching",
    "citation_count": 0,
    "authors": [
      "Thomas Kehrenberg",
      "Myles Bartlett",
      "Viktoriia Sharmanska",
      "Novi Quadrianto"
    ]
  },
  "https://openreview.net/forum?id=ZSWKdRi2cU": {
    "title": "Hyper-parameter Tuning for Fair Classification without Sensitive Attribute Access",
    "volume": "main",
    "abstract": "Fair machine learning methods seek to train models that balance model performance across demographic subgroups defined over sensitive attributes like race and gender. Although sensitive attributes are typically assumed to be known during training, they may not be available in practice due to privacy and other logistical concerns. Recent work has sough to train fair models without sensitive attributes on training data. However, these methods need extensive hyper-parameter tuning to achieve good results, and hence assume that sensitive attributes are known on validation data. However, this assumption too might not be practical. Here, we propose Antigone, a framework to train fair classifiers without access to sensitive attributes on either training or validation data. Instead, we generate pseudo sensitive attributes on the validation data by training a ERM model and using the classifier's incorrectly (correctly) classified examples as proxies for disadvantaged (advantaged) groups. Since fairness metrics like demographic parity, equal opportunity and subgroup accuracy can be estimated to within a proportionality constant even with noisy sensitive attribute information, we show theoretically and empirically that these proxy labels can be used to maximize fairness under average accuracy constraints. Key to our results is a principled approach to select the hyper-parameters of the ERM model in a completely unsupervised fashion (meaning without access to ground truth sensitive attributes) that minimizes the gap between fairness estimated using noisy versus ground-truth sensitive labels. We demonstrate that Antigone outperforms existing methods on CelebA, Waterbirds, and UCI datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshaj Kumar Veldanda",
      "Ivan Brugere",
      "Sanghamitra Dutta",
      "Alan Mishler",
      "Siddharth Garg"
    ]
  },
  "https://openreview.net/forum?id=7PNJzAxkij": {
    "title": "E(n)-equivariant Graph Neural Cellular Automata",
    "volume": "main",
    "abstract": "Cellular automata (CAs) are notable computational models exhibiting rich dynamics emerging from the local interaction of cells arranged in a regular lattice. Graph CAs (GCAs) generalise standard CAs by allowing for arbitrary graphs rather than regular lattices, similar to how Graph Neural Networks (GNNs) generalise Convolutional NNs. Recently, Graph Neural CAs (GNCAs) have been proposed as models built on top of standard GNNs that can be trained to approximate the transition rule of any arbitrary GCA. We note that existing GNCAs can violate the locality principle of CAs by leveraging global information and, furthermore, are anisotropic in the sense that their transition rules are not equivariant to isometries of the nodes' spatial locations. However, it is desirable for instances related by such transformations to be treated identically by the model. By replacing standard graph convolutions with E(n)-equivariant ones, we avoid anisotropy by design and propose a class of isotropic automata that we call E(n)-GNCAs. These models are lightweight, but can nevertheless handle large graphs, capture complex dynamics and exhibit emergent self-organising behaviours. We showcase the broad and successful applicability of E(n)-GNCAs on three different tasks: (i) isotropic pattern formation, (ii) graph auto-encoding, and (iii) simulation of E(n)-equivariant dynamical systems",
    "checked": true,
    "id": "a17a7abe9c4341748b4fb8da03e98370af184dd2",
    "semantic_title": "e(n)-equivariant graph neural cellular automata",
    "citation_count": 2,
    "authors": [
      "Gennaro Gala",
      "Daniele Grattarola",
      "Erik Quaeghebeur"
    ]
  },
  "https://openreview.net/forum?id=WOrdoKbxh6": {
    "title": "Layer-diverse Negative Sampling for Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) are a powerful solution for various structure learning applications due to their strong representation capabilities for graph data. However, traditional GNNs, relying on message-passing mechanisms that gather information exclusively from first-order neighbours (known as positive samples), can lead to issues such as over-smoothing and over-squashing. To mitigate these issues, we propose a layer-diverse negative sampling method for message-passing propagation. This method employs a sampling matrix within a determinantal point process, which transforms the candidate set into a space and selectively samples from this space to generate negative samples. To further enhance the diversity of the negative samples during each forward pass, we develop a space-squeezing method to achieve layer-wise diversity in multi-layer GNNs. Experiments on various real-world graph datasets demonstrate the effectiveness of our approach in improving the diversity of negative samples and overall learning performance. Moreover, adding negative samples dynamically changes the graph's topology, thus with the strong potential to improve the expressiveness of GNNs and reduce the risk of over-squashing",
    "checked": true,
    "id": "fd8806e41d8c6885f0bf4a47fc70c5f9dbeb3545",
    "semantic_title": "layer-diverse negative sampling for graph neural networks",
    "citation_count": 2,
    "authors": [
      "Wei Duan",
      "Jie Lu",
      "Yu Guang Wang",
      "Junyu Xuan"
    ]
  },
  "https://openreview.net/forum?id=N2wx9UVHkH": {
    "title": "Personalized Federated Learning with Spurious Features: An Adversarial Approach",
    "volume": "main",
    "abstract": "One of the common approaches for personalizing federated learning is fine-tuning the global model for each local client. While this addresses some issues of statistical heterogeneity, we find that such personalization methods are vulnerable to spurious features at local agents, leading to reduced generalization performance. This work considers a setup where spurious features correlate with the label in each client's training environment, and the mixture of multiple training environments (i.e., the global environment) diminishes the spurious correlations. In other words, while the global federated learning model trained over the global environment suffers less from spurious features, the local fine-tuning step may lead to personalized models vulnerable to spurious correlations. In light of this practical and pressing challenge, we propose a novel strategy to mitigate the effect of spurious features during personalization by maintaining the adversarial transferability between the global and personalized models. Empirical results on object and action recognition tasks show that our proposed approach bounds personalized models from further exploiting spurious features while preserving the benefit of enhanced accuracy from fine-tuning",
    "checked": true,
    "id": "d1aedc04df8c16fdc06f4aea0aafecc29b3cdfda",
    "semantic_title": "personalized federated learning with spurious features: an adversarial approach",
    "citation_count": 0,
    "authors": [
      "Xiaoyang Wang",
      "Han Zhao",
      "Klara Nahrstedt",
      "Sanmi Koyejo"
    ]
  },
  "https://openreview.net/forum?id=QySD5r7PeE": {
    "title": "A Pseudo-Metric between Probability Distributions based on Depth-Trimmed Regions",
    "volume": "main",
    "abstract": "The design of a metric between probability distributions is a longstanding problem motivated by numerous applications in machine learning. Focusing on continuous probability distributions in the Euclidean space $\\mathbb{R}^d$, we introduce a novel pseudo-metric between probability distributions by leveraging the extension of univariate quantiles to multivariate spaces. Data depth is a nonparametric statistical tool that measures the centrality of any element $x\\in\\mathbb{R}^d$ with respect to (w.r.t.) a probability distribution or a dataset. It is a natural median-oriented extension of the cumulative distribution function (cdf) to the multivariate case. Thus, its upper-level sets---the depth-trimmed regions---give rise to a definition of multivariate quantiles. The new pseudo-metric relies on the average of the Hausdorff distance between the depth-based quantile regions for each distribution. Its good behavior regarding major transformation groups, as well as its ability to factor out translations, are depicted. Robustness, an appealing feature of this pseudo-metric, is studied through the finite sample breakdown point. Moreover, we propose an efficient approximation method with linear time complexity w.r.t. the size of the dataset and its dimension. The quality of this approximation and the performance of the proposed approach are illustrated in numerical experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guillaume Staerman",
      "Pavlo Mozharovskyi",
      "Pierre Colombo",
      "Stephan Clémençon",
      "Florence d'Alché-Buc"
    ]
  },
  "https://openreview.net/forum?id=9CcgO0LhKG": {
    "title": "World Models via Policy-Guided Trajectory Diffusion",
    "volume": "main",
    "abstract": "World models are a powerful tool for developing intelligent agents. By predicting the outcome of a sequence of actions, world models enable policies to be optimised via on-policy reinforcement learning (RL) using synthetic data, i.e. in \"in imagination\". Existing world models are autoregressive in that they interleave predicting the next state with sampling the next action from the policy. Prediction error inevitably compounds as the trajectory length grows. In this work, we propose a novel world modelling approach that is not autoregressive and generates entire on-policy trajectories in a single pass through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion (PolyGRAD), leverages a denoising model in addition to the gradient of the action distribution of the policy to diffuse a trajectory of initially random states and actions into an on-policy synthetic trajectory. We analyse the connections between PolyGRAD, score-based generative models, and classifier-guided diffusion models. Our results demonstrate that PolyGRAD outperforms state-of-the-art baselines in terms of trajectory prediction error for short trajectories, with the exception of autoregressive diffusion. For short trajectories, PolyGRAD obtains similar errors to autoregressive diffusion, but with lower computational requirements. For long trajectories, PolyGRAD obtains comparable performance to baselines. Our experiments demonstrate that PolyGRAD enables performant policies to be trained via on-policy RL in imagination for MuJoCo continuous control domains. Thus, PolyGRAD introduces a new paradigm for accurate on-policy world modelling without autoregressive sampling",
    "checked": true,
    "id": "bebcb0832cdafb102c75c96ca18207d05034ab0c",
    "semantic_title": "world models via policy-guided trajectory diffusion",
    "citation_count": 3,
    "authors": [
      "Marc Rigter",
      "Jun Yamada",
      "Ingmar Posner"
    ]
  },
  "https://openreview.net/forum?id=YCgX7sJRF1": {
    "title": "Adapting Contrastive Language-Image Pretrained (CLIP) Models for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "We present a comprehensive experimental study on pre-trained feature extractors for visual out-of-distribution (OOD) detection, focusing on leveraging contrastive language-image pre-trained (CLIP) models. Without fine-tuning on the training data, we are able to establish a positive correlation ($R^2\\geq0.92$) between in-distribution classification and unsupervised OOD detection for CLIP models in $4$ benchmarks. We further propose a new simple and scalable method called \\textit{pseudo-label probing} (PLP) that adapts vision-language models for OOD detection. Given a set of label names of the training set, PLP trains a linear layer using the pseudo-labels derived from the text encoder of CLIP. Intriguingly, we show that without modifying the weights of CLIP or training additional image/text encoders (i) PLP outperforms the previous state-of-the-art on all $5$ large-scale benchmarks based on ImageNet, specifically by an average AUROC gain of 3.4\\% using the largest CLIP model (ViT-G), (ii) linear probing outperforms fine-tuning by large margins for CLIP architectures (i.e. CLIP ViT-H achieves a mean gain of 7.3\\% AUROC on average on all ImageNet-based benchmarks), and (iii) billion-parameter CLIP models still fail at detecting feature-based adversarially manipulated OOD images. The code is available at https://github.com/HHU-MMBS/plp-official-tmlr2024",
    "checked": true,
    "id": "eb1dbaf8e7a92a95050fd8f6e025ff4fbebadb5e",
    "semantic_title": "adapting contrastive language-image pretrained (clip) models for out-of-distribution detection",
    "citation_count": 0,
    "authors": [
      "Nikolas Adaloglou",
      "Felix Michels",
      "Tim Kaiser",
      "Markus Kollmann"
    ]
  },
  "https://openreview.net/forum?id=5VotySkajV": {
    "title": "Multi-conditioned Graph Diffusion for Neural Architecture Search",
    "volume": "main",
    "abstract": "Neural architecture search automates the design of neural network architectures usually by exploring a large and thus complex architecture search space. To advance the architecture search, we present a graph diffusion-based NAS approach that uses discrete conditional graph diffusion processes to generate high-performing neural network architectures. We then propose a multi-conditioned classifier-free guidance approach applied to graph diffusion networks to jointly impose constraints such as high accuracy and low hardware latency. Unlike the related work, our method is completely differentiable and requires only a single model training. In our evaluations, we show promising results on six standard benchmarks, yielding novel and unique architectures at a fast speed, i.e. less than 0.2 seconds per architecture. Furthermore, we demonstrate the generalisability and efficiency of our method through experiments on ImageNet dataset",
    "checked": true,
    "id": "989552c9131953d7b5f2d8cb6f06e75ea05a415f",
    "semantic_title": "multi-conditioned graph diffusion for neural architecture search",
    "citation_count": 0,
    "authors": [
      "Rohan Asthana",
      "Joschua Conrad",
      "Youssef Dawoud",
      "Maurits Ortmanns",
      "Vasileios Belagiannis"
    ]
  },
  "https://openreview.net/forum?id=KJRoQvRWNs": {
    "title": "How does over-squashing affect the power of GNNs?",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) are the state-of-the-art model for machine learning on graph-structured data. The most popular class of GNNs operate by exchanging information between adjacent nodes, and are known as Message Passing Neural Networks (MPNNs). While understanding the expressive power of MPNNs is a key question, existing results typically consider settings with uninformative node features. In this paper, we provide a rigorous analysis to determine which function classes of node features can be learned by an MPNN of a given capacity. We do so by measuring the level of *pairwise interactions* between nodes that MPNNs allow for. This measure provides a novel quantitative characterization of the so-called over-squashing effect, which is observed to occur when a large volume of messages is aggregated into fixed-size vectors. Using our measure, we prove that, to guarantee sufficient communication between pairs of nodes, the capacity of the MPNN must be large enough, depending on properties of the input graph structure, such as commute times. For many relevant scenarios, our analysis results in impossibility statements in practice, showing that *over-squashing hinders the expressive power of MPNNs*. Our theory also holds for geometric graphs and hence extends to equivariant MPNNs on point clouds. We validate our analysis through extensive controlled experiments and ablation studies",
    "checked": true,
    "id": "dc8f407e9eae9ff2383c4d6d8325bbf9d5c6a5b0",
    "semantic_title": "how does over-squashing affect the power of gnns?",
    "citation_count": 11,
    "authors": [
      "Francesco Di Giovanni",
      "T. Konstantin Rusch",
      "Michael Bronstein",
      "Andreea Deac",
      "Marc Lackenby",
      "Siddhartha Mishra",
      "Petar Veličković"
    ]
  },
  "https://openreview.net/forum?id=ue9igTDLN2": {
    "title": "Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models",
    "volume": "main",
    "abstract": "As general purpose vision models get increasingly effective at a wide set of tasks, it is imperative that they be consistent across the tasks they support. Inconsistent AI models are considered brittle and untrustworthy by human users and are more challenging to incorporate into larger systems that take dependencies on their outputs. Measuring consistency between very heterogeneous tasks that might include outputs in different modalities is challenging since it is difficult to determine if the predictions are consistent with one another. As a solution, we introduce a benchmark dataset, CocoCON, where we create contrast sets by modifying test instances for multiple tasks in small but semantically meaningful ways to change the gold label and outline metrics for measuring if a model is consistent by ranking the original and perturbed instances across tasks. We find that state-of-the-art vision-language models suffer from a surprisingly high degree of inconsistent behavior across tasks, especially for more heterogeneous tasks. To alleviate this issue, we propose a rank correlation-based auxiliary training objective, computed over large automatically created cross-task contrast sets, that improves the multi-task consistency of large unified models while retaining their original accuracy on downstream tasks",
    "checked": true,
    "id": "7ac75cb733140d9ec7829b751b974f56dfe3b38f",
    "semantic_title": "exposing and addressing cross-task inconsistency in unified vision-language models",
    "citation_count": 2,
    "authors": [
      "Adyasha Maharana",
      "Amita Kamath",
      "Christopher Clark",
      "Mohit Bansal",
      "Aniruddha Kembhavi"
    ]
  },
  "https://openreview.net/forum?id=aD0ExytnEK": {
    "title": "Controlling the Inductive Bias of Wide Neural Networks by Modifying the Kernel's Spectrum",
    "volume": "main",
    "abstract": "Wide neural networks are biased towards learning certain functions, influencing both the rate of convergence of gradient descent (GD) and the functions that are reachable with GD in finite training time. As such, there is a great need for methods that can modify this bias according to the task at hand. To that end, we introduce Modified Spectrum Kernels (MSKs), a novel family of constructed kernels that can be used to approximate kernels with desired eigenvalues for which no closed form is known. We leverage the duality between wide neural networks and Neural Tangent Kernels and propose a preconditioned gradient descent method, which alters the trajectory of GD. As a result, this allows for a polynomial and, in some cases, exponential training speedup without changing the final solution. Our method is both computationally efficient and simple to implement",
    "checked": true,
    "id": "3c5dafeab83eaeee3c31db8525ce3f37a82bf3ca",
    "semantic_title": "controlling the inductive bias of wide neural networks by modifying the kernel's spectrum",
    "citation_count": 1,
    "authors": [
      "Amnon Geifman",
      "Daniel Barzilai",
      "Ronen Basri",
      "Meirav Galun"
    ]
  },
  "https://openreview.net/forum?id=HhbqHBBrfZ": {
    "title": "Attending to Graph Transformers",
    "volume": "main",
    "abstract": "Recently, transformer architectures for graphs emerged as an alternative to established techniques for machine learning with graphs, such as (message-passing) graph neural networks. So far, they have shown promising empirical results, e.g., on molecular prediction datasets, often attributed to their ability to circumvent graph neural networks' shortcomings, such as over-smoothing and over-squashing. Here, we derive a taxonomy of graph transformer architectures, bringing some order to this emerging field. We overview their theoretical properties, survey structural and positional encodings, and discuss extensions for important graph classes, e.g., 3D molecular graphs. Empirically, we probe how well graph transformers can recover various graph properties, how well they can deal with heterophilic graphs, and to what extent they prevent over-squashing. Further, we outline open challenges and research direction to stimulate future work",
    "checked": true,
    "id": "30258c205060af5ce958dc6c9e184c9498ee48ed",
    "semantic_title": "attending to graph transformers",
    "citation_count": 43,
    "authors": [
      "Luis Müller",
      "Mikhail Galkin",
      "Christopher Morris",
      "Ladislav Rampášek"
    ]
  },
  "https://openreview.net/forum?id=2iOOvQmJBK": {
    "title": "Discovering Model Structure of Dynamical Systems with Combinatorial Bayesian Optimization",
    "volume": "main",
    "abstract": "Deciding on a model structure is a fundamental problem in machine learning. In this paper we consider the problem of building a data-based model for dynamical systems from a library of discrete components. In addition to optimizing performance, we consider crash and inequality constraints that arise from additional requirements, such as real-time capability and model complexity. We address this task of model structure selection with a focus on dynamical systems and propose to search over potential model structures efficiently using a constrained combinatorial Bayesian Optimization (BO) algorithm. We propose expressive surrogate models suited for combinatorial domains and an acquisition function that can handle inequality and crash constraints. We provide simulated benchmark problems within the domain of equation discovery of nonlinear dynamical systems. Our method outperforms the state-of-the-art in constrained combinatorial optimization of black-box functions and has a favorable computational overhead compared to other BO methods. As a real-world application example, we apply our method to optimize the configuration of an electric vehicle's digital twin while ensuring its real-time capability for the use in one of the world's largest driving simulators",
    "checked": true,
    "id": "00bb3148c2df49953a9f3682627e5bf45c404551",
    "semantic_title": "discovering model structure of dynamical systems with combinatorial bayesian optimization",
    "citation_count": 0,
    "authors": [
      "Lucas Rath",
      "Alexander von Rohr",
      "Andreas Schultze",
      "Sebastian Trimpe",
      "Burkhard Corves"
    ]
  },
  "https://openreview.net/forum?id=1ZGA5mSkoB": {
    "title": "An Improved Federated Clustering Algorithm with Model-based Clustering",
    "volume": "main",
    "abstract": "Federated learning (FL) is a distributed learning paradigm that allows multiple clients to collaboratively train a shared model via communications to a central server. However, optimal models of different clients often differ due to heterogeneity of data across clients. In this paper, we address the dichotomy between heterogeneous models and simultaneous training in FL via a clustering structure among the clients. The clustering framework is one way to allow for high heterogeneity level between clients, while clients with similar data can still train a shared model. We define a new clustering framework for FL based on the (optimal) local models of the clients: two clients belong to the same cluster if their local models are close. We propose an algorithm, \\emph{Successive Refine Federated Clustering Algorithm} (\\texttt{SR-FCA}), that treats each client as a singleton cluster as an initialization, and then successively refine the cluster estimation via exploiting similarity with other clients. In any intermediate step, \\texttt{SR-FCA} uses an {\\em error-tolerant} federated learning algorithm within each cluster to exploit simultaneous training and to correct clustering errors. Unlike some prominent prior works \\texttt{SR-FCA} does not require any \\emph{good} initialization (or warm start), both in theory and practice. We show that with proper choice of learning rate, \\texttt{SR-FCA} incurs arbitrarily small clustering error. Additionally, \\texttt{SR-FCA} does not require the knowledge of the number of clusters apriori like some prior works. We validate the performance of \\texttt{SR-FCA} on real-world FL datasets including FEMNIST and Shakespeare in non-convex problems and show the benefits of \\texttt{SR-FCA} over several baselines",
    "checked": false,
    "id": "7da6ea094a3cc855b6febc19b0bdea3540d1a157",
    "semantic_title": "an efficient client clustering algorithm for clustered federated learning",
    "citation_count": 0,
    "authors": [
      "Harsh Vardhan",
      "Avishek Ghosh",
      "Arya Mazumdar"
    ]
  },
  "https://openreview.net/forum?id=qBZeQBEDIW": {
    "title": "Series of Hessian-Vector Products for Tractable Saddle-Free Newton Optimisation of Neural Networks",
    "volume": "main",
    "abstract": "Despite their popularity in the field of continuous optimisation, second-order quasi-Newton methods are challenging to apply in machine learning, as the Hessian matrix is intractably large. This computational burden is exacerbated by the need to address non-convexity, for instance by modifying the Hessian's eigenvalues as in Saddle-Free Newton methods. We propose an optimisation algorithm which addresses both of these concerns – to our knowledge, the first efficiently-scalable optimisation algorithm to asymptotically use the exact inverse Hessian with absolute-value eigenvalues. Our method frames the problem as a series which principally square-roots and inverts the squared Hessian, then uses it to precondition a gradient vector, all without explicitly computing or eigendecomposing the Hessian. A truncation of this infinite series provides a new optimisation algorithm which is scalable and comparable to other first- and second-order optimisation methods in both runtime and optimisation performance. We demonstrate this in a variety of settings, including a ResNet-18 trained on CIFAR-10",
    "checked": true,
    "id": "606f7aeb3e02a120d829cb22760de9ac34e44e39",
    "semantic_title": "series of hessian-vector products for tractable saddle-free newton optimisation of neural networks",
    "citation_count": 0,
    "authors": [
      "Elre Talea Oldewage",
      "Ross M Clarke",
      "José Miguel Hernández-Lobato"
    ]
  },
  "https://openreview.net/forum?id=8W6IDyFZgC": {
    "title": "Demographically-Informed Prediction Discrepancy Index: Early Warnings of Demographic Biases for Unlabeled Populations",
    "volume": "main",
    "abstract": "An ever-growing body of work has shown that machine learning systems can be systematically biased against certain sub-populations defined by attributes like race or gender. Data imbalance and under-representation of certain populations in the training datasets have been identified as potential causes behind this phenomenon. However, understanding whether data imbalance with respect to a specific demographic group may result in biases for a given task and model class is not simple. An approach to answering this question is to perform controlled experiments, where several models are trained with different imbalance ratios and then their performance is evaluated on the target population. However, in the absence of ground-truth annotations at deployment for an unseen population, most fairness metrics cannot be computed. In this work, we explore an alternative method to study potential bias issues based on the output discrepancy of pools of models trained on different demographic groups. Models within a pool are otherwise identical in terms of architecture, hyper-parameters, and training scheme. Our hypothesis is that the output consistency between models may serve as a proxy to anticipate biases concerning demographic groups. In other words, if models tailored to different demographic groups produce inconsistent predictions, then biases are more prone to appear in the task under analysis. We formulate the Demographically-Informed Prediction Discrepancy Index (DIPDI) and validate our hypothesis in numerical experiments using both synthetic and real-world datasets. Our work sheds light on the relationship between model output discrepancy and demographic biases and provides a means to anticipate potential bias issues in the absence of ground-truth annotations. Indeed, we show how DIPDI could provide early warnings about potential demographic biases when deploying machine learning models on new and unlabeled populations that exhibit demographic shifts",
    "checked": true,
    "id": "b210ab52cd35a1a80a1d5e3480a3436d73dcea63",
    "semantic_title": "demographically-informed prediction discrepancy index: early warnings of demographic biases for unlabeled populations",
    "citation_count": 0,
    "authors": [
      "Lucas Mansilla",
      "Estanislao Claucich",
      "Rodrigo Echeveste",
      "Diego H Milone",
      "Enzo Ferrante"
    ]
  },
  "https://openreview.net/forum?id=vTBjBtGioE": {
    "title": "Fast Training of Diffusion Models with Masked Transformers",
    "volume": "main",
    "abstract": "We propose an efficient approach to train large diffusion models with masked transformers. While masked transformers have been extensively explored for representation learning, their application to generative learning is less explored in the vision domain. Our work is the first to exploit masked training to reduce the training cost of diffusion models significantly. Specifically, we randomly mask out a high proportion (e.g., 50\\%) of patches in diffused input images during training. For masked training, we introduce an asymmetric encoder-decoder architecture consisting of a transformer encoder that operates only on unmasked patches and a lightweight transformer decoder on full patches. To promote a long-range understanding of full patches, we add an auxiliary task of reconstructing masked patches to the denoising score matching objective that learns the score of unmasked patches. Experiments on ImageNet-256x256 and ImageNet-512x512 show that our approach achieves competitive and even better generative performance than the state-of-the-art Diffusion Transformer (DiT) model, using only around 30\\% of its original training time. Thus, our method shows a promising way of efficiently training large transformer-based diffusion models without sacrificing the generative performance. Our code is available at https://github.com/Anima-Lab/MaskDiT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongkai Zheng",
      "Weili Nie",
      "Arash Vahdat",
      "Anima Anandkumar"
    ]
  },
  "https://openreview.net/forum?id=ZOqJCP4eMk": {
    "title": "Functional Linear Regression of Cumulative Distribution Functions",
    "volume": "main",
    "abstract": "The estimation of cumulative distribution functions (CDF) is an important learning task with a great variety of downstream applications, such as risk assessments in predictions and decision making. In this paper, we study functional regression of contextual CDF{}s where each data point is sampled from a linear combination of context dependent CDF basis functions. We propose functional ridge-regression-based estimation methods that estimate CDF{}s accurately everywhere. In particular, given $n$ samples with $d$ basis functions, we show estimation error upper bounds of $\\widetilde O(\\sqrt{d/n})$ for fixed design, random design, and adversarial context cases. We also derive matching information theoretic lower bounds, establishing minimax optimality for CDF functional regression. Furthermore, we remove the burn-in time in the random design setting using an alternative penalized estimator. Then, we consider agnostic settings where there is a mismatch in the data generation process. We characterize the error of the proposed estimators in terms of the mismatched error, and show that the estimators are well-behaved under model mismatch. Moreover, to complete our study, we formalize infinite dimensional models where the parameter space is an infinite dimensional Hilbert space, and establish a self-normalized estimation error upper bound for this setting. Notably, the upper bound reduces to the $\\widetilde O(\\sqrt{d/n})$ bound when the parameter space is constrained to be $d$-dimensional. Our comprehensive numerical experiments validate the efficacy of our estimation methods in both synthetic and practical settings",
    "checked": true,
    "id": "1b813d1535a222072517aa3173b77bb572252be9",
    "semantic_title": "functional linear regression of cumulative distribution functions",
    "citation_count": 1,
    "authors": [
      "Qian Zhang",
      "Anuran Makur",
      "Kamyar Azizzadenesheli"
    ]
  },
  "https://openreview.net/forum?id=4TnFbv16hK": {
    "title": "Bias/Variance is not the same as Approximation/Estimation",
    "volume": "main",
    "abstract": "We study the relation between two classical results: the bias-variance decomposition, and the approximation-estimation decomposition. Both are important conceptual tools in Machine Learning, helping us describe the nature of model fitting. It is commonly stated that they are \"closely related\", or \"similar in spirit\". However, sometimes it is said they are equivalent. In fact they are different, but have subtle connections cutting across learning theory, classical statistics, and information geometry, that (very surprisingly) have not been previously observed. We present several results for losses expressible as Bregman divergences: a broad family with a known bias-variance decomposition. Discussion and future directions are presented for more general losses, including the 0/1 classification loss",
    "checked": true,
    "id": "1657ac7717b92926efb3eb1a5cf544c51e55a002",
    "semantic_title": "bias/variance is not the same as approximation/estimation",
    "citation_count": 0,
    "authors": [
      "Gavin Brown",
      "Riccardo Ali"
    ]
  },
  "https://openreview.net/forum?id=HhjSalvWVe": {
    "title": "Incorporating Prior Knowledge into Neural Networks through an Implicit Composite Kernel",
    "volume": "main",
    "abstract": "It is challenging to guide neural network (NN) learning with prior knowledge. In contrast, many known properties, such as spatial smoothness or seasonality, are straightforward to model by choosing an appropriate kernel in a Gaussian process (GP). Many deep learning applications could be enhanced by modeling such known properties. For example, convolutional neural networks (CNNs) are frequently used in remote sensing, which is subject to strong seasonal effects. We propose to blend the strengths of NNs and the clear modeling capabilities of GPs by using a composite kernel that combines a kernel implicitly defined by a neural network with a second kernel function chosen to model known properties (e.g., seasonality). We implement this idea by combining a deep network and an efficient mapping function based on either Nystrom approximation or random Fourier features, which we call Implicit Composite Kernel (ICK). We then adopt a sample-then-optimize approach to approximate the full GP posterior distribution. We demonstrate that ICK has superior performance and flexibility on both synthetic and real-world datasets including a remote sensing dataset. The ICK framework can be used to include prior information into neural networks in many applications",
    "checked": true,
    "id": "35fc6239a0658eeca805572ffea1296a8379f2ef",
    "semantic_title": "incorporating prior knowledge into neural networks through an implicit composite kernel",
    "citation_count": 3,
    "authors": [
      "Ziyang Jiang",
      "Tongshu Zheng",
      "Yiling Liu",
      "David Carlson"
    ]
  },
  "https://openreview.net/forum?id=Ai9XpjGxjl": {
    "title": "Using Sum-Product Networks to Assess Uncertainty in Deep Active Learning",
    "volume": "main",
    "abstract": "The success of deep active learning hinges on the choice of an effective acquisition function, which ranks not yet labeled data points according to their expected informativeness. Many acquisition functions are (partly) based on the uncertainty that the current model has about the class label of a point, yet there is no generally agreed upon strategy for computing such uncertainty. This paper proposes a new and very simple approach to computing uncertainty in deep active learning with a Convolutional Neural Network (CNN). The main idea is to use the feature representation extracted by the CNN as data for training a Sum-Product Network (SPN). Since SPNs are typically used for estimating the distribution of a dataset, they are well suited to the task of estimating class probabilities that can be used directly by standard acquisition functions such as max entropy and variational ratio. The effectiveness of our method is demonstrated in an experimental study on several standard benchmark datasets for image classification, where we compare it to various state-of-the-art methods for assessing uncertainty in deep active learning",
    "checked": true,
    "id": "46282bb5a0e4066e243e35b72a992ca704af96b7",
    "semantic_title": "using sum-product networks to assess uncertainty in deep active learning",
    "citation_count": 0,
    "authors": [
      "Mohamadsadegh Khosravani",
      "Sandra Zilles"
    ]
  },
  "https://openreview.net/forum?id=UVE7LllpXe": {
    "title": "How Much Pre-training Is Enough to Discover a Good Subnetwork?",
    "volume": "main",
    "abstract": "Neural network pruning helps discover efficient, high-performing subnetworks within pre-trained, dense network architectures. More often than not, it involves a three-step process—pre-training, pruning, and re-training—that is computationally expensive, as the dense model must be fully pre-trained. While previous work has revealed through experiments the relationship between the amount of pre-training and the performance of the pruned network, a theoretical characterization of such dependency is still missing. Aiming to mathematically analyze the amount of dense network pre-training needed for a pruned network to perform well, we discover a simple theoretical bound in the number of gradient descent pre-training iterations on a two-layer fully connected network in the NTK regime, beyond which pruning via greedy forward selection \\citep{provable_subnetworks} yields a subnetwork that achieves good training error. Interestingly, this threshold is logarithmically dependent upon the size of the dataset, meaning that experiments with larger datasets require more pre-training for subnetworks obtained via pruning to perform well. Lastly, we empirically validate our theoretical results on multi-layer perceptions and residual-based convolutional networks trained on MNIST, CIFAR, and ImageNet datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cameron R. Wolfe",
      "Fangshuo Liao",
      "Qihan Wang",
      "Junhyung Lyle Kim",
      "Anastasios Kyrillidis"
    ]
  },
  "https://openreview.net/forum?id=7yswRA8zzw": {
    "title": "Pull-back Geometry of Persistent Homology Encodings",
    "volume": "main",
    "abstract": "Persistent homology (PH) is a method for generating topology-inspired representations of data. Empirical studies that investigate the properties of PH, such as its sensitivity to perturbations or ability to detect a feature of interest, commonly rely on training and testing an additional model on the basis of the PH representation. To gain more intrinsic insights about PH, independently of the choice of such a model, we propose a novel methodology based on the pull-back geometry that a PH encoding induces on the data manifold. The spectrum and eigenvectors of the induced metric help to identify the most and least significant information captured by PH. Furthermore, the pull-back norm of tangent vectors provides insights about the sensitivity of PH to a given perturbation, or its potential to detect a given feature of interest, and in turn its ability to solve a given classification or regression problem. Experimentally, the insights gained through our methodology align well with the existing knowledge about PH. Moreover, we show that the pull-back norm correlates with the performance on downstream tasks, and can therefore guide the choice of a suitable PH encoding",
    "checked": true,
    "id": "4c7ee2d082ac78d4a8df1501e63c3acacc701f77",
    "semantic_title": "pull-back geometry of persistent homology encodings",
    "citation_count": 0,
    "authors": [
      "Shuang Liang",
      "Renata Turkes",
      "Jiayi Li",
      "Nina Otter",
      "Guido Montufar"
    ]
  },
  "https://openreview.net/forum?id=bG3ICt3E0C": {
    "title": "MC Layer Normalization for calibrated uncertainty in Deep Learning",
    "volume": "main",
    "abstract": "Efficiently estimating the uncertainty of neural network predictions has become an increasingly important challenge as machine learning models are adopted for high-stakes industrial applications where shifts in data distribution may occur. Thus, calibrated prediction uncertainty is crucial to determine when to trust a model's output and when to discard them as implausible. We propose a novel deep learning module - MC Layer Normalization - that acts as a drop-in replacement for Layer Normalization blocks and endows a neural network with uncertainty estimation capabilities. Our method is motivated from an approximate Bayesian perspective, but it is simple to deploy with no significant computational overhead thanks to an efficient one-shot approximation of Monte Carlo integration at prediction time. To evaluate the effectiveness of our module, we conduct experiments in two distinct settings. First, we investigate its potential to replace existing methods such as MC-Dropout and Prediction-Time Batch Normalization. Second, we explore its suitability for use cases where such conventional modules are either unsuitable or sub-optimal for certain tasks (as is the case with modules based on Batch Normalization, which is incompatible for instance with transformers). We empirically demonstrate the competitiveness of our module in terms of prediction accuracy and uncertainty calibration on established out-of-distribution image classification benchmarks, as well as its flexibility by applying it on tasks and architectures where previous methods are unsuitable",
    "checked": true,
    "id": "70f64d7538ed49b64a577d71bda7bc2ef8360304",
    "semantic_title": "mc layer normalization for calibrated uncertainty in deep learning",
    "citation_count": 0,
    "authors": [
      "Thomas Frick",
      "Diego Antognini",
      "Ioana Giurgiu",
      "Benjamin F Grewe",
      "Cristiano Malossi",
      "Rong J.B. Zhu",
      "Mattia Rigotti"
    ]
  },
  "https://openreview.net/forum?id=Uv3XVAEgG6": {
    "title": "Kernel Normalized Convolutional Networks",
    "volume": "main",
    "abstract": "Existing convolutional neural network architectures frequently rely upon batch normalization (BatchNorm) to effectively train the model. BatchNorm, however, performs poorly with small batch sizes, and is inapplicable to differential privacy. To address these limitations, we propose the kernel normalization (KernelNorm) and kernel normalized convolutional layers, and incorporate them into kernel normalized convolutional networks (KNConvNets) as the main building blocks. We implement KNConvNets corresponding to the state-of-the-art ResNets while forgoing the BatchNorm layers. Through extensive experiments, we illustrate that KNConvNets achieve higher or competitive performance compared to the BatchNorm counterparts in image classification and semantic segmentation. They also significantly outperform their batch-independent competitors including those based on layer and group normalization in non-private and differentially private training. Given that, KernelNorm combines the batch-independence property of layer and group normalization with the performance advantage of BatchNorm",
    "checked": true,
    "id": "fff40fa88682803540aeb057e0f6ce1646273f86",
    "semantic_title": "kernel normalized convolutional networks",
    "citation_count": 1,
    "authors": [
      "Reza Nasirigerdeh",
      "Reihaneh Torkzadehmahani",
      "Daniel Rueckert",
      "Georgios Kaissis"
    ]
  },
  "https://openreview.net/forum?id=kNCZ95mw7N": {
    "title": "A VAE-based Framework for Learning Multi-Level Neural Granger-Causal Connectivity",
    "volume": "main",
    "abstract": "Granger causality has been widely used in various application domains to capture lead-lag relationships amongst the components of complex dynamical systems, and the focus in extant literature has been on a single dynamical system. In certain applications in macroeconomics and neuroscience, one has access to data from a collection of related such systems, wherein the modeling task of interest is to extract the shared common structure that is embedded across them, as well as to identify the idiosyncrasies within individual ones. This paper introduces a Variational Autoencoder (VAE) based framework that jointly learns Granger-causal relationships amongst components in a collection of related-yet-heterogeneous dynamical systems, and handles the aforementioned task in a principled way. The performance of the proposed framework is evaluated on several synthetic data settings and benchmarked against existing approaches designed for individual system learning. The method is further illustrated on a real dataset involving time series data from a neurophysiological experiment and produces interpretable results",
    "checked": true,
    "id": "798b2f3ea41127ba5698a3f08af96f8450a7773b",
    "semantic_title": "a vae-based framework for learning multi-level neural granger-causal connectivity",
    "citation_count": 0,
    "authors": [
      "Jiahe Lin",
      "Huitian Lei",
      "George Michailidis"
    ]
  },
  "https://openreview.net/forum?id=KutEe24Yai": {
    "title": "Exploit CAM by itself: Complementary Learning System for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has long been suffering from fragmentary object regions led by Class Activation Map (CAM), which is incapable of generating fine-grained masks for semantic segmentation. To guide CAM to find more non-discriminating object patterns, this paper turns to an interesting working mechanism in agent learning named Complementary Learning System (CLS). CLS holds that the neocortex builds a sensation of general knowledge, while the hippocampus specially learns specific details, completing the learned patterns. Motivated by this simple but effective learning pattern, we propose a General-Specific Learning Mechanism (GSLM) to explicitly drive a coarse-grained CAM to a fine-grained pseudo mask. Specifically, GSLM develops a General Learning Module (GLM) and a Specific Learning Module (SLM). The GLM is trained with image-level supervision to extract coarse and general localization representations from CAM. Based on the general knowledge in the GLM, the SLM progressively exploits the specific spatial knowledge from the localization representations, expanding the CAM in an explicit way. To this end, we propose the Seed Reactivation to help SLM reactivate non-discriminating regions by setting a boundary for activation values, which successively identifies more regions of CAM. Without extra refinement processes, our method is able to achieve improvements for CAM of over 20.0% mIoU on PASCAL VOC 2012 and 10.0% mIoU on MS COCO 2014 datasets, representing a new state-of-the-art among existing WSSS methods. The code is publicly available at: https://github.com/tmlr-group/GSLM",
    "checked": true,
    "id": "9ce6fc2d6392853ffbc41f2c2d6da2405bf13d3f",
    "semantic_title": "exploit cam by itself: complementary learning system for weakly supervised semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Wankou Yang",
      "Jiren Mai",
      "Fei Zhang",
      "Tongliang Liu",
      "Bo Han"
    ]
  },
  "https://openreview.net/forum?id=xo3hI5MwvU": {
    "title": "Learning from Natural Language Feedback",
    "volume": "main",
    "abstract": "The potential for pre-trained large language models (LLMs) to use natural language feedback at inference time has been an exciting recent development. We build upon this observation by formalizing an algorithm for learning from natural language feedback at training time instead, which we call Imitation learning from Language Feedback (ILF). ILF requires only a small amount of human-written feedback during training and does not require the same feedback at test time, making it both user-friendly and sample-efficient. We further show that ILF can be seen as a form of minimizing the KL divergence to the target distribution and demonstrate proof-of-concepts on text summarization and program synthesis tasks. For code generation, ILF improves a Codegen-Mono 6.1B model's pass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic Python Problems (MBPP) benchmark, outperforming both fine-tuning on MBPP and fine-tuning on repaired programs written by humans. For summarization, we show that ILF can be combined with learning from human preferences to improve a GPT-3 model's summarization performance to be comparable to human quality, outperforming fine-tuning on human-written summaries. Overall, our results suggest that learning from human-written natural language feedback is both more effective and sample-efficient than training exclusively on demonstrations for improving an LLM's performance on a variety of tasks",
    "checked": true,
    "id": "c43a6f12b062a50617244611af180a8146e792de",
    "semantic_title": "learning from natural language feedback",
    "citation_count": 3,
    "authors": [
      "Angelica Chen",
      "Jérémy Scheurer",
      "Jon Ander Campos",
      "Tomasz Korbak",
      "Jun Shern Chan",
      "Samuel R. Bowman",
      "Kyunghyun Cho",
      "Ethan Perez"
    ]
  },
  "https://openreview.net/forum?id=lVE1VeGQwg": {
    "title": "Manifold Contrastive Learning with Variational Lie Group Operators",
    "volume": "main",
    "abstract": "Self-supervised learning of deep neural networks has become a prevalent paradigm for learning representations that transfer to a variety of downstream tasks. Similar to proposed models of the ventral stream of biological vision, it is observed that these networks lead to a separation of category manifolds in the representations of the penultimate layer. Although this observation matches the manifold hypothesis of representation learning, current self-supervised approaches are limited in their ability to explicitly model this manifold. Indeed, current approaches often only apply a pre-specified set of augmentations for \"positive pairs\" during learning. In this work, we propose a contrastive learning approach that directly models the latent manifold using Lie group operators parameterized by coefficients with a sparsity-promoting prior. A variational distribution over these coefficients provides a generative model of the manifold, with samples which provide feature augmentations applicable both during contrastive training and downstream tasks. Additionally, learned coefficient distributions provide a quantification of which transformations are most likely at each point on the manifold while preserving identity. We demonstrate benefits in self-supervised benchmarks for image datasets, as well as a downstream semi-supervised task. In the former case, we demonstrate that the proposed methods can effectively apply manifold feature augmentations and improve learning both with and without a projection head. In the latter case, we demonstrate that feature augmentations sampled from learned Lie group operators can improve classification performance when using few labels",
    "checked": true,
    "id": "5861aeb9659b9449f7482e0d5543933216577727",
    "semantic_title": "manifold contrastive learning with variational lie group operators",
    "citation_count": 0,
    "authors": [
      "Kion Fallah",
      "Alec Helbling",
      "Kyle A. Johnsen",
      "Christopher John Rozell"
    ]
  },
  "https://openreview.net/forum?id=805jKZ0Gqf": {
    "title": "Pseudo-Differential Neural Operator: Generalize Fourier Neural operator for Learning Solution Operators of Partial Differential Equations",
    "volume": "main",
    "abstract": "Learning mapping between two function spaces has attracted considerable research attention. However, learning the solution operator of partial differential equations (PDEs) remains a challenge in scientific computing. Fourier neural operator (FNO) is recently proposed to learn the solution operators with an excellent performance. In this study, we propose a novel pseudo-differential integral operator (PDIO) to analyze and generalize the Fourier integral operator in FNO. PDIO is inspired by a pseudo-differential operator, which is a generalization of a differential operator and characterized by a certain symbol. We parameterize the symbol by using a neural network and show that the neural-network-based symbol is contained in a smooth symbol class. Subsequently, we prove that the PDIO is a bounded linear operator, and thus is continuous in the Sobolev space. We combine the PDIO with the neural operator to develop a pseudo-differential neural operator (PDNO) to learn the nonlinear solution operator of PDEs. We experimentally validate the effectiveness of the proposed model by using Darcy flow and the Navier-Stokes equation. The results reveal that the proposed PDNO outperforms the existing neural operator approaches in most experiments",
    "checked": false,
    "id": "72551363d7f8e457239bb7852ee64b6e06d437d6",
    "semantic_title": "pseudo-differential neural operator: generalized fourier neural operator for learning solution operators of partial differential equations",
    "citation_count": 2,
    "authors": [
      "Jin Young Shin",
      "Jae Yong Lee",
      "Hyung Ju Hwang"
    ]
  },
  "https://openreview.net/forum?id=TTRDCVnbjI": {
    "title": "Are Population Graphs Really as Powerful as Believed?",
    "volume": "main",
    "abstract": "Population graphs and their use in combination with graph neural networks (GNNs) have demonstrated promising results for multi-modal medical data integration and improving disease diagnosis and prognosis. Several different methods for constructing these graphs and advanced graph learning techniques have been established to maximise the predictive power of GNNs on population graphs. However, in this work, we raise the question of whether existing methods are really strong enough by showing that simple baseline methods --such as random forests or linear regressions--, perform on par with advanced graph learning models on several population graph datasets for a variety of different clinical applications. We use the commonly used public population graph datasets TADPOLE and ABIDE, a brain age estimation and a cardiac dataset from the UK Biobank, and a real-world in-house COVID dataset. We (a) investigate the impact of different graph construction methods, graph convolutions, and dataset size and complexity on GNN performance and (b) discuss the utility of GNNs for multi-modal data integration in the context of population graphs. Based on our results, we argue towards the need for \"better\" graph construction methods or innovative applications for population graphs to render them beneficial",
    "checked": true,
    "id": "5b636a833df6bd16462f508e7d00458f8ee639cd",
    "semantic_title": "are population graphs really as powerful as believed?",
    "citation_count": 0,
    "authors": [
      "Tamara T. Müller",
      "Sophie Starck",
      "Kyriaki-Margarita Bintsi",
      "Alexander Ziller",
      "Rickmer Braren",
      "Georgios Kaissis",
      "Daniel Rueckert"
    ]
  },
  "https://openreview.net/forum?id=sPlhAIp6mk": {
    "title": "Multitask Learning Can Improve Worst-Group Outcomes",
    "volume": "main",
    "abstract": "In order to create machine learning systems that serve a variety of users well, it is vital to not only achieve high average performance but also ensure equitable outcomes across diverse groups. However, most machine learning methods are designed to improve a model's average performance on a chosen end task without consideration for their impact on worst group error. Multitask learning (MTL) is one such widely used technique. In this paper, we seek not only to understand the impact of MTL on worst-group accuracy but also to explore its potential as a tool to address the challenge of group-wise fairness. We primarily consider the standard setting of fine-tuning a pre-trained model, where, following recent work \\citep{gururangan2020don, dery2023aang}, we multitask the end task with the pre-training objective constructed from the end task data itself. In settings with few or no group annotations, we find that multitasking often, but not consistently, achieves better worst-group accuracy than Just-Train-Twice (JTT; \\citet{pmlr-v139-liu21f}) -- a representative distributionally robust optimization (DRO) method. Leveraging insights from synthetic data experiments, we propose to modify standard MTL by regularizing the joint multitask representation space. We run a large number of fine-tuning experiments across computer vision and natural language processing datasets and find that our regularized MTL approach \\emph{consistently} outperforms JTT on both average and worst-group outcomes. Our official code can be found here: \\href{https://github.com/atharvajk98/MTL-group-robustness.git}{\\url{https://github.com/atharvajk98/MTL-group-robustness}}",
    "checked": true,
    "id": "e36dbc1ae9b5116b798d519948de097c3bf19e5b",
    "semantic_title": "multitask learning can improve worst-group outcomes",
    "citation_count": 0,
    "authors": [
      "Atharva Kulkarni",
      "Lucio M. Dery",
      "Amrith Setlur",
      "Aditi Raghunathan",
      "Ameet Talwalkar",
      "Graham Neubig"
    ]
  },
  "https://openreview.net/forum?id=3nprbNR3HB": {
    "title": "ASPEST: Bridging the Gap Between Active Learning and Selective Prediction",
    "volume": "main",
    "abstract": "Selective prediction aims to learn a reliable model that abstains from making predictions when uncertain. These predictions can then be deferred to humans for further evaluation. As an everlasting challenge for machine learning, in many real-world scenarios, the distribution of test data is different from the training data. This results in more inaccurate predictions, and often increased dependence on humans, which can be difficult and expensive. Active learning aims to lower the overall labeling effort, and hence human dependence, by querying the most informative examples. Selective prediction and active learning have been approached from different angles, with the connection between them missing. In this work, we introduce a new learning paradigm, active selective prediction, which aims to query more informative samples from the shifted target domain while increasing accuracy and coverage. For this new paradigm, we propose a simple yet effective approach, ASPEST, that utilizes ensembles of model snapshots with self-training with their aggregated outputs as pseudo labels. Extensive experiments on numerous image, text and structured datasets, which suffer from domain shifts, demonstrate that ASPEST can significantly outperform prior work on selective prediction and active learning (e.g. on the MNIST$\\to$SVHN benchmark with the labeling budget of 100, ASPEST improves the AUACC metric from 79.36% to 88.84%) and achieves more optimal utilization of humans in the loop",
    "checked": true,
    "id": "217d5e0c5ed75f45256e14e122035eeb9af1722b",
    "semantic_title": "aspest: bridging the gap between active learning and selective prediction",
    "citation_count": 1,
    "authors": [
      "Jiefeng Chen",
      "Jinsung Yoon",
      "Sayna Ebrahimi",
      "Sercan O Arik",
      "Somesh Jha",
      "Tomas Pfister"
    ]
  },
  "https://openreview.net/forum?id=OUWG6O4yo9": {
    "title": "Statistical Component Separation for Targeted Signal Recovery in Noisy Mixtures",
    "volume": "main",
    "abstract": "Separating signals from an additive mixture may be an unnecessarily hard problem when one is only interested in specific properties of a given signal. In this work, we tackle simpler \"statistical component separation\" problems that focus on recovering a predefined set of statistical descriptors of a target signal from a noisy mixture. Assuming access to samples of the noise process, we investigate a method devised to match the statistics of the solution candidate corrupted by noise samples with those of the observed mixture. We first analyze the behavior of this method using simple examples with analytically tractable calculations. Then, we apply it in an image denoising context employing 1) wavelet-based descriptors, 2) ConvNet-based descriptors on astrophysics and ImageNet data. In the case of 1), we show that our method better recovers the descriptors of the target data than a standard denoising method in most situations. Additionally, despite not constructed for this purpose, it performs surprisingly well in terms of peak signal-to-noise ratio on full signal reconstruction. In comparison, representation 2) appears less suitable for image denoising. Finally, we extend this method by introducing a diffusive stepwise algorithm which gives a new perspective to the initial method and leads to promising results for image denoising under specific circumstances",
    "checked": true,
    "id": "63a2bd0261a3f28f52327c8f43f4c8c44e2f1fed",
    "semantic_title": "statistical component separation for targeted signal recovery in noisy mixtures",
    "citation_count": 0,
    "authors": [
      "Bruno Régaldo-Saint Blancard",
      "Michael Eickenberg"
    ]
  },
  "https://openreview.net/forum?id=WeiRR8h87X": {
    "title": "Budgeted Online Model Selection and Fine-Tuning via Federated Learning",
    "volume": "main",
    "abstract": "Online model selection involves selecting a model from a set of candidate models `on the fly' to perform prediction on a stream of data. The choice of candidate models henceforth has a crucial impact on the performance. Although employing a larger set of candidate models naturally leads to more flexibility in model selection, this may be infeasible in cases where prediction tasks are performed on edge devices with limited memory. Faced with this challenge, the present paper proposes an online federated model selection framework where a group of learners (clients) interacts with a server with sufficient memory such that the server stores all candidate models. However, each client only chooses to store a subset of models that can be fit into its memory and performs its own prediction task using one of the stored models. Furthermore, employing the proposed algorithm, clients and the server collaborate to fine-tune models to adapt them to a non-stationary environment. Theoretical analysis proves that the proposed algorithm enjoys sub-linear regret with respect to the best model in hindsight. Experiments on real datasets demonstrate the effectiveness of the proposed algorithm",
    "checked": true,
    "id": "fa74e4a136cc81ffe79e7b936de451d947310a99",
    "semantic_title": "budgeted online model selection and fine-tuning via federated learning",
    "citation_count": 0,
    "authors": [
      "Pouya M. Ghari",
      "Yanning Shen"
    ]
  },
  "https://openreview.net/forum?id=icoP08mrQJ": {
    "title": "Leveraging Endo- and Exo-Temporal Regularization for Black-box Video Domain Adaptation",
    "volume": "main",
    "abstract": "To enable video models to be applied seamlessly across video tasks in different environments, various Video Unsupervised Domain Adaptation (VUDA) methods have been proposed to improve the robustness and transferability of video models. Despite improvements made in model robustness, these VUDA methods require access to both source data and source model parameters for adaptation, raising serious data privacy and model portability issues. To cope with the above concerns, this paper firstly formulates Black-box Video Domain Adaptation (BVDA) as a more realistic yet challenging scenario where the source video model is provided only as a black-box predictor. While a few methods for Black-box Domain Adaptation (BDA) are proposed in the image domain, these methods cannot apply to the video domain since video modality has more complicated temporal features that are harder to align. To address BVDA, we propose a novel Endo and eXo-TEmporal Regularized Network (EXTERN) by applying mask-to-mix strategies and video-tailored regularizations. They are the endo-temporal regularization and exo-temporal regularization, which are performed across both clip and temporal features, while distilling knowledge from the predictions obtained from the black-box predictor. Empirical results demonstrate the state-of-the-art performance of EXTERN across various cross-domain closed-set and partial-set action recognition benchmarks, which even surpasses most existing video domain adaptation methods with source data accessibility. Code will be available at https://xuyu0010.github.io/b2vda.html",
    "checked": true,
    "id": "5c22f7d443a98ff02dd7c15c4473979205b42098",
    "semantic_title": "leveraging endo- and exo-temporal regularization for black-box video domain adaptation",
    "citation_count": 1,
    "authors": [
      "Yuecong Xu",
      "Jianfei Yang",
      "Haozhi Cao",
      "Min Wu",
      "Xiaoli Li",
      "Lihua Xie",
      "Zhenghua Chen"
    ]
  },
  "https://openreview.net/forum?id=Ryf1TVCjBz": {
    "title": "Correlation Clustering with Active Learning of Pairwise Similarities",
    "volume": "main",
    "abstract": "Correlation clustering is a well-known unsupervised learning setting that deals with positive and negative pairwise similarities. In this paper, we study the case where the pairwise similarities are not given in advance and must be queried in a cost-efficient way. Thereby, we develop a generic active learning framework for this task that benefits from several advantages, e.g., flexibility in the type of feedback that a user/annotator can provide, adaptation to any correlation clustering algorithm and query strategy, and robustness to noise. In addition, we propose and analyze a number of novel query strategies suited to this setting. We demonstrate the effectiveness of our framework and the proposed query strategies via several experimental studies",
    "checked": true,
    "id": "f672dec6afd24e63652363a58b69614cf0bd11de",
    "semantic_title": "correlation clustering with active learning of pairwise similarities",
    "citation_count": 1,
    "authors": [
      "Linus Aronsson",
      "Morteza Haghir Chehreghani"
    ]
  },
  "https://openreview.net/forum?id=uxNfN2PU1W": {
    "title": "Effective Latent Differential Equation Models via Attention and Multiple Shooting",
    "volume": "main",
    "abstract": "Scientific Machine Learning (SciML) is a burgeoning field that synergistically combines domain-aware and interpretable models with agnostic machine learning techniques. In this work, we introduce GOKU-UI, an evolution of the SciML generative model GOKU-nets. GOKU-UI not only broadens the original model's spectrum to incorporate other classes of differential equations, such as Stochastic Differential Equations (SDEs), but also integrates attention mechanisms and a novel multiple shooting training strategy in the latent space. These modifications have led to a significant increase in its performance in both reconstruction and forecast tasks, as demonstrated by our evaluation on simulated and empirical data. Specifically, GOKU-UI outperformed all baseline models on synthetic datasets even with a training set 16-fold smaller, underscoring its remarkable data efficiency. Furthermore, when applied to empirical human brain data, while incorporating stochastic Stuart-Landau oscillators into its dynamical core, our proposed enhancements markedly increased the model's effectiveness in capturing complex brain dynamics. GOKU-UI demonstrated a reconstruction error five times lower than other baselines, and the multiple shooting method reduced the GOKU-nets prediction error for future brain activity up to 15 seconds ahead. By training GOKU-UI on resting state fMRI data, we encoded whole-brain dynamics into a latent representation, learning a low-dimensional dynamical system model that could offer insights into brain functionality and open avenues for practical applications such as the classification of mental states or psychiatric conditions. Ultimately, our research provides further impetus for the field of Scientific Machine Learning, showcasing the potential for advancements when established scientific insights are interwoven with modern machine learning",
    "checked": true,
    "id": "95ce5e9ab22aacfdfae412dbcd80d8b246364e15",
    "semantic_title": "effective latent differential equation models via attention and multiple shooting",
    "citation_count": 0,
    "authors": [
      "Germán Abrevaya",
      "Mahta Ramezanian-Panahi",
      "Jean-Christophe Gagnon-Audet",
      "Pablo Polosecki",
      "Irina Rish",
      "Silvina Ponce Dawson",
      "Guillermo Cecchi",
      "Guillaume Dumas"
    ]
  },
  "https://openreview.net/forum?id=9TqAUYB6tC": {
    "title": "Global Convergence of SGD For Logistic Loss on Two Layer Neural Nets",
    "volume": "main",
    "abstract": "In this note, we demonstrate a first-of-its-kind provable convergence of SGD to the global minima of appropriately regularized logistic empirical risk of depth $2$ nets -- for arbitrary data with any number of gates with adequately smooth and bounded activations, like sigmoid and tanh, and for a class of distributions from which the initial weight is sampled. We also prove an exponentially fast convergence rate for continuous time SGD that also applies to smooth unbounded activations like SoftPlus. Our key idea is to show that the logistic loss function on any size neural net can be Frobenius norm regularized by a width-independent parameter such that the regularized loss is a ``Villani function'' -- and thus be able to build on recent progress with analyzing SGD on such objectives",
    "checked": true,
    "id": "2b6040d20a0bf41d1d20caf886de57371ed464b0",
    "semantic_title": "global convergence of sgd for logistic loss on two layer neural nets",
    "citation_count": 0,
    "authors": [
      "Pulkit Gopalani",
      "Samyak Jha",
      "Anirbit Mukherjee"
    ]
  },
  "https://openreview.net/forum?id=uGVFtjvI3v": {
    "title": "Why should autoencoders work?",
    "volume": "main",
    "abstract": "Deep neural network autoencoders are routinely used computationally for model reduction. They allow recognizing the intrinsic dimension of data that lie in a $k$-dimensional subset $K$ of an input Euclidean space $\\mathbb{R}^n$. The underlying idea is to obtain both an encoding layer that maps $\\mathbb{R}^n$ into $\\mathbb{R}^k$ (called the bottleneck layer or the space of latent variables) and a decoding layer that maps $\\mathbb{R}^k$ back into $\\mathbb{R}^n$, in such a way that the input data from the set $K$ is recovered when composing the two maps. This is achieved by adjusting parameters (weights) in the network to minimize the discrepancy between the input and the reconstructed output. Since neural networks (with continuous activation functions) compute continuous maps, the existence of a network that achieves perfect reconstruction would imply that $K$ is homeomorphic to a $k$-dimensional subset of $\\mathbb{R}^k$, so clearly there are topological obstructions to finding such a network. On the other hand, in practice the technique is found to ``work'' well, which leads one to ask if there is a way to explain this effectiveness. We show that, up to small errors, indeed the method is guaranteed to work. This is done by appealing to certain facts from differential topology. A computational example is also included to illustrate the ideas",
    "checked": true,
    "id": "31e6d857ac83fe4dd2d9084d5c0485cb6b42504b",
    "semantic_title": "why should autoencoders work?",
    "citation_count": 0,
    "authors": [
      "Matthew Kvalheim",
      "Eduardo Sontag"
    ]
  },
  "https://openreview.net/forum?id=n2gAD8Fdzk": {
    "title": "Enhancing Robustness to Class-Conditional Distribution Shift in Long-Tailed Recognition",
    "volume": "main",
    "abstract": "For long-tailed recognition problem, beyond imbalanced label distribution, unreliable empirical data distribution due to instance scarcity has recently emerged as a concern. It inevitably causes Class-Conditional Distribution (CCD) shift between training and test. Data augmentation and head-to-tail information transfer methods indirectly alleviate the problem by synthesizing novel examples but may remain biased. In this paper, we conduct a thorough study on the impact of CCD shift and propose Distributionally Robust Augmentation (DRA) to directly train models robust to the shift. DRA admits a novel generalization bound reflecting the benefit of distributional robustness to CCD shift for long-tailed recognition. Extensive experiments show DRA greatly improves existing re-balancing and data augmentation methods when cooperating with them. It also alleviates the recently discovered saddle-point issue, verifying its ability to achieve enhanced robustness",
    "checked": true,
    "id": "4a4752eba69c7ce66ba4c0edb7c94a94f6ea4b8f",
    "semantic_title": "enhancing robustness to class-conditional distribution shift in long-tailed recognition",
    "citation_count": 0,
    "authors": [
      "Keliang Li",
      "Hong Chang",
      "Shiguang Shan",
      "Xilin CHEN"
    ]
  },
  "https://openreview.net/forum?id=Eg8Rnb0Hdd": {
    "title": "Expected Pinball Loss For Quantile Regression And Inverse CDF Estimation",
    "volume": "main",
    "abstract": "We analyze and improve a recent strategy to train a quantile regression model by minimizing an expected pinball loss over all quantiles. Through an asymptotic convergence analysis, we show that minimizing the expected pinball loss can be more efficient at estimating single quantiles than training with the standard pinball loss for that quantile, an insight that generalizes the known deficiencies of the sample quantile in the unconditioned setting. Then, to guarantee a legitimate inverse CDF, we propose using flexible deep lattice networks with a monotonicity constraint on the quantile input to guarantee non-crossing quantiles, and show lattice models can be regularized to the same location-scale family. Our analysis and experiments on simulated and real datasets show that the proposed method produces state-of-the-art legitimate inverse CDF estimates that are likely to be as good or better for specific target quantiles",
    "checked": true,
    "id": "559c57ec361e8086588a299b50dfc95083493018",
    "semantic_title": "expected pinball loss for quantile regression and inverse cdf estimation",
    "citation_count": 2,
    "authors": [
      "Taman Narayan",
      "Serena Lutong Wang",
      "Kevin Robert Canini",
      "Maya Gupta"
    ]
  },
  "https://openreview.net/forum?id=OZbn8ULouY": {
    "title": "The Slingshot Effect: A Late-Stage Optimization Anomaly in Adaptive Gradient Methods",
    "volume": "main",
    "abstract": "Adaptive gradient methods, notably Adam ~\\citep{kingma2014adam, loshchilov2017decoupled}, have become indispensable for optimizing neural networks, particularly in conjunction with Transformers ~\\citep{vaswani2017attention, dosovitskiy2020an}. In this paper, we present a novel optimization anomaly called the \\emph{Slingshot Effect}, which manifests during extremely late stages of training. We identify a distinctive characteristic of this phenomenon through cyclic phase transitions between stable and unstable training regimes, as evidenced by the cyclic behavior of the norm of the last layer's weights. Although the Slingshot Effect can be easily reproduced in more general settings, it does not align with any known optimization theories, emphasizing the need for in-depth examination. Moreover, we make a noteworthy observation that Grokking, as reported by ~\\citet{power2021grokking}, occurs predominantly during the onset of the Slingshot Effects and is absent without it, even in the absence of explicit regularization. This finding suggests a surprising inductive bias of adaptive gradient optimizers at late training stages, urging a revised theoretical analysis of their origin. Our study sheds light on an intriguing optimization behavior that has significant implications for understanding the inner workings of adaptive gradient methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vimal Thilak",
      "Etai Littwin",
      "Shuangfei Zhai",
      "Omid Saremi",
      "Roni Paiss",
      "Joshua M. Susskind"
    ]
  },
  "https://openreview.net/forum?id=mqMzerrVOB": {
    "title": "Mixed Nash for Robust Federated Learning",
    "volume": "main",
    "abstract": "We study robust federated learning (FL) within a game theoretic framework to alleviate the server vulnerabilities to even an informed adversary who can tailor training-time attacks. Specifically, we introduce RobustTailor, a simulation-based framework that prevents the adversary from being omniscient and derives its convergence guarantees. RobustTailor improves robustness to training-time attacks significantly while preserving almost the same privacy guarantees as standard robust aggregation schemes in FL. Empirical results under challenging attacks show that RobustTailor performs close to an upper bound with perfect knowledge of honest clients",
    "checked": true,
    "id": "aea10c5336e7f321af5800aafbf6faac3374b2dd",
    "semantic_title": "mixed nash for robust federated learning",
    "citation_count": 1,
    "authors": [
      "Wanyun Xie",
      "Thomas Pethick",
      "Ali Ramezani-Kebrya",
      "Volkan Cevher"
    ]
  },
  "https://openreview.net/forum?id=WFI9xhJrxF": {
    "title": "Policy Gradient with Kernel Quadrature",
    "volume": "main",
    "abstract": "Reward evaluation of episodes becomes a bottleneck in a broad range of reinforcement learning tasks. Our aim in this paper is to select a small but representative subset of a large batch of episodes, only on which we actually compute rewards for more efficient policy gradient iterations. We build a Gaussian process modeling of discounted returns or rewards to derive a positive definite kernel on the space of episodes, run an ``episodic\" kernel quadrature method to compress the information of sample episodes, and pass the reduced episodes to the policy network for gradient updates. We present the theoretical background of this procedure as well as its numerical illustrations in MuJoCo tasks",
    "checked": true,
    "id": "ffbf87817e86739040ef7e80169d55db707ea947",
    "semantic_title": "policy gradient with kernel quadrature",
    "citation_count": 0,
    "authors": [
      "Satoshi Hayakawa",
      "Tetsuro Morimura"
    ]
  },
  "https://openreview.net/forum?id=oCBsxCov2g": {
    "title": "PNeRV: A Polynomial Neural Representation for Videos",
    "volume": "main",
    "abstract": "Extracting Implicit Neural Representations (INRs) on video data poses unique challenges due to the additional temporal dimension. In the context of videos, INRs have predominantly relied on a frame-only parameterization, which sacrifices the spatiotemporal continuity observed in pixel-level (spatial) representations. To mitigate this, we introduce Polynomial Neural Representation for Videos (PNeRV), a parameter-wise efficient, patch-wise INR for videos that preserves spatiotemporal continuity. PNeRV leverages the modeling capabilities of Polynomial Neural Networks to perform the modulation of a continuous spatial (patch) signal with a continuous time (frame) signal. We further propose a custom Hierarchical Patch-wise Spatial Sampling Scheme that ensures spatial continuity while retaining parameter efficiency. We also employ a carefully designed Positional Embedding methodology to further enhance PNeRV's performance. Our extensive experimentation demonstrates that PNeRV outperforms the baselines in conventional Implicit Neural Representation tasks like compression along with downstream applications that require spatiotemporal continuity in the underlying representation. PNeRV not only addresses the challenges posed by video data in the realm of INRs but also opens new avenues for advanced video processing and analysis",
    "checked": true,
    "id": "c216d8d79503107bf68897f9206d81cf153823b8",
    "semantic_title": "pnerv: a polynomial neural representation for videos",
    "citation_count": 0,
    "authors": [
      "Sonam Gupta",
      "Snehal Singh Tomar",
      "Grigorios Chrysos",
      "Sukhendu Das",
      "Rajagopalan N Ambasamduram"
    ]
  },
  "https://openreview.net/forum?id=daX2UkLMS0": {
    "title": "Exploring Simple, High Quality Out-of-Distribution Detection with L2 Normalization",
    "volume": "main",
    "abstract": "We demonstrate that L2 normalization over feature space can produce capable performance for Out-of-Distribution (OoD) detection for some models and datasets. Although it does not demonstrate outright state-of-the-art performance, this method is notable for its extreme simplicity: it requires only two addition lines of code, and does not need specialized loss functions, image augmentations, outlier exposure or extra parameter tuning. We also observe that training may be more efficient for some datasets and architectures. Notably, only 60 epochs with ResNet18 on CIFAR10 (or 100 epochs with ResNet50) can produce performance within two percentage points (AUROC) of several state-of-the-art methods for some near and far OoD datasets. We provide theoretical and empirical support for this method, and demonstrate viability across five architectures and three In-Distribution (ID) datasets",
    "checked": true,
    "id": "9b952ed671fa82de294ae53c275c443f393896bd",
    "semantic_title": "exploring simple, high quality out-of-distribution detection with l2 normalization",
    "citation_count": 0,
    "authors": [
      "Jarrod Haas",
      "William Yolland",
      "Bernhard T Rabus"
    ]
  },
  "https://openreview.net/forum?id=TySx8fsSSU": {
    "title": "On the Out-of-Distribution Coverage of Combining Split Conformal Prediction and Bayesian Deep Learning",
    "volume": "main",
    "abstract": "Bayesian deep learning and conformal prediction are two methods that have been used to convey uncertainty and increase safety in machine learning systems. We focus on combining Bayesian deep learning with split conformal prediction and how the addition of conformal prediction affects out-of-distribution coverage that we would otherwise see; particularly in the case of multiclass image classification. We suggest that if the model is generally underconfident on the calibration set, then the resultant conformal sets may exhibit worse out-of-distribution coverage compared to simple predictive credible sets (i.e. not using conformal prediction). Conversely, if the model is overconfident on the calibration set, the use of conformal prediction may improve out-of-distribution coverage. In particular, we study the extent to which the addition of conformal prediction increases or decreases out-of-distribution coverage for a variety of inference techniques. In particular, (i) stochastic gradient descent, (ii) deep ensembles, (iii) mean-field variational inference, (iv) stochastic gradient Hamiltonian Monte Carlo, and (v) Laplace approximation. Our results suggest that the application of conformal prediction to different predictive deep learning methods can have significantly different consequences",
    "checked": true,
    "id": "5e7707e2133d965ff48718ecf9ce61b19c65b570",
    "semantic_title": "on the out-of-distribution coverage of combining split conformal prediction and bayesian deep learning",
    "citation_count": 0,
    "authors": [
      "Paul Scemama",
      "Ariel Kapusta"
    ]
  },
  "https://openreview.net/forum?id=RUNiIDU8P7": {
    "title": "Estimating Optimal Policy Value in Linear Contextual Bandits Beyond Gaussianity",
    "volume": "main",
    "abstract": "In many bandit problems, the maximal reward achievable by a policy is often unknown in advance. We consider the problem of estimating the optimal policy value in the sublinear data regime before the optimal policy is even learnable. We refer to this as $V^*$ estimation. It was previously shown that fast $V^*$ estimation is possible but only in disjoint linear bandits with Gaussian covariates. Whether this is possible for more realistic context distributions has remained an open and important question for tasks such as model selection. In this paper, we first provide lower bounds showing that this general problem is hard. However, under stronger assumptions, we give an algorithm and analysis proving that $\\widetilde{\\mathcal{O}}(\\sqrt{d})$ sublinear estimation of $V^*$ is indeed information-theoretically possible, where $d$ is the dimension. We subsequently introduce a practical and computationally efficient algorithm that estimates a problem-specific upper bound on $V^*$, valid for general distributions and tight for Gaussian context distributions. We prove our algorithm requires only $\\widetilde{\\mathcal{O}}(\\sqrt{d})$ samples to estimate the upper bound. We use this upper bound in conjunction with the estimator to derive novel and improved guarantees for several applications in bandit model selection and testing for treatment effects. We present promising experimental benefits on a semi-synthetic simulation using historical data on warfarin treatment dosage outcomes",
    "checked": false,
    "id": "50115a89c089fd3914633f67c1e8418ef4cd92cf",
    "semantic_title": "estimating optimal policy value in general linear contextual bandits",
    "citation_count": 0,
    "authors": [
      "Jonathan Lee",
      "Weihao Kong",
      "Aldo Pacchiano",
      "Vidya Muthukumar",
      "Emma Brunskill"
    ]
  },
  "https://openreview.net/forum?id=48pHFcg0YO": {
    "title": "DynaConF: Dynamic Forecasting of Non-Stationary Time Series",
    "volume": "main",
    "abstract": "Deep learning has shown impressive results in a variety of time series forecasting tasks, where modeling the conditional distribution of the future given the past is the essence. However, when this conditional distribution is non-stationary, it poses challenges for these models to learn consistently and to predict accurately. In this work, we propose a new method to model non-stationary conditional distributions over time by clearly decoupling stationary conditional distribution modeling from non-stationary dynamics modeling. Our method is based on a Bayesian dynamic model that can adapt to conditional distribution changes and a deep conditional distribution model that handles multivariate time series using a factorized output space. Our experimental results on synthetic and real-world datasets show that our model can adapt to non-stationary time series better than state-of-the-art deep learning solutions",
    "checked": false,
    "id": "d111d372e1f0ae5982c7a52726b1bf9c502466e9",
    "semantic_title": "dynaconf: dynamic forecasting of non-stationary time-series",
    "citation_count": 2,
    "authors": [
      "Siqi Liu",
      "Andreas Lehrmann"
    ]
  },
  "https://openreview.net/forum?id=uXGUSX8GoY": {
    "title": "QDC: Quantum Diffusion Convolution Kernels on Graphs",
    "volume": "main",
    "abstract": "Graph convolutional neural networks (GCNs) operate by aggregating messages over local neighborhoods given the prediction task under interest. Many GCNs can be understood as a form of generalized diffusion of input features on the graph, and significant work has been dedicated to improving predictive accuracy by altering the ways of message passing. In this work, we propose a new convolution kernel that effectively rewires the graph according to the occupation correlations of the vertices by trading on the generalized diffusion paradigm for the propagation of a quantum particle over the graph. We term this new convolution kernel the Quantum Diffusion Convolution (QDC) operator. In addition, we introduce a multiscale variant that combines messages from the QDC operator and the traditional combinatorial Laplacian. To understand our method, we explore the spectral dependence of homophily and the importance of quantum dynamics in the construction of a bandpass filter. Through these studies, as well as experiments on a range of datasets, we observe that QDC improves predictive performance on the widely used benchmark datasets when compared to similar methods",
    "checked": true,
    "id": "82aa9b8b8ccf2f879665397d023c4ca1173e6b34",
    "semantic_title": "qdc: quantum diffusion convolution kernels on graphs",
    "citation_count": 2,
    "authors": [
      "Thomas Markovich"
    ]
  },
  "https://openreview.net/forum?id=torWsEui9N": {
    "title": "Image Reconstruction via Deep Image Prior Subspaces",
    "volume": "main",
    "abstract": "Deep learning has been widely used for solving image reconstruction tasks but its deployability has been held back due to the shortage of high-quality paired training data. Unsupervised learning methods, e.g., deep image prior (DIP), naturally fill this gap, but bring a host of new issues: the susceptibility to overfitting due to a lack of robust early stopping strategies and unstable convergence. We present a novel approach to tackle these issues by restricting DIP optimisation to a sparse linear subspace of its parameters, employing a synergy of dimensionality reduction techniques and second order optimisation methods. The low-dimensionality of the subspace reduces DIP's tendency to fit noise and allows the use of stable second order optimisation methods, e.g., natural gradient descent or L-BFGS. Experiments across both image restoration and tomographic tasks of different geometry and ill-posedness show that second order optimisation within a low-dimensional subspace is favourable in terms of optimisation stability to reconstruction fidelity trade-off",
    "checked": true,
    "id": "174910a4357196a37632efa3022afb5bd254e676",
    "semantic_title": "image reconstruction via deep image prior subspaces",
    "citation_count": 0,
    "authors": [
      "Riccardo Barbano",
      "Javier Antoran",
      "Johannes Leuschner",
      "José Miguel Hernández-Lobato",
      "Bangti Jin",
      "Zeljko Kereta"
    ]
  },
  "https://openreview.net/forum?id=0yMuNezwJ1": {
    "title": "On the Dual Problem of Convexified Convolutional Neural Networks",
    "volume": "main",
    "abstract": "We study the dual problem of convexified convolutional neural networks (DCCNNs). First, we introduce a primal learning problem motivated by convexified convolutional neural networks (CCNNs), and then construct the dual convex training program through careful analysis of the Karush-Kuhn-Tucker (KKT) conditions and Fenchel conjugates. Our approach reduces the computational overhead of constructing a large kernel matrix and more importantly, eliminates the ambiguity of factorizing the matrix. Due to the low-rank structure in CCNNs and the related subdifferential of nuclear norms, there is no closed-form expression to recover the primal solution from the dual solution. To overcome this, we propose a highly novel weight recovery algorithm, which takes the dual solution and the kernel information as the input, and recovers the linear weight and the output of convolutional layer, instead of weight parameter. Furthermore, our recovery algorithm exploits the low-rank structure and imposes a small number of filters indirectly, which reduces the parameter size. As a result, DCCNNs inherit all the statistical benefits of CCNNs, while enjoying a more formal and efficient workflow",
    "checked": false,
    "id": "1b533197353c25814dde96bf5b0c7cfcb5fedc18",
    "semantic_title": "on the dual problem of convexiﬁed convolutional neural networks",
    "citation_count": 0,
    "authors": [
      "Site Bai",
      "Chuyang Ke",
      "Jean Honorio"
    ]
  },
  "https://openreview.net/forum?id=xkiflfKCw3": {
    "title": "Evaluating Spatial Understanding of Large Language Models",
    "volume": "main",
    "abstract": "Large language models (LLMs) show remarkable capabilities across a variety of tasks. Despite the models only seeing text in training, several recent studies suggest that LLM representations implicitly capture aspects of the underlying grounded concepts. Here, we explore LLM representations of a particularly salient kind of grounded knowledge --- spatial relationships. We design natural-language navigation tasks and evaluate the ability of LLMs, in particular GPT-3.5-turbo, GPT-4, and Llama2 series models, to represent and reason about spatial structures. We also compare these abilities to human performance on the same tasks. These tasks reveal substantial variability in LLM performance across different spatial structures, including square, hexagonal, and triangular grids, rings, and trees. In extensive error analysis, we find that LLMs' mistakes reflect both spatial and non-spatial factors. These findings suggest that LLMs appear to capture certain aspects of spatial structure implicitly, but room for improvement remains",
    "checked": true,
    "id": "a00cb75a2bf2ee20b778ec5587f802ea2db013e3",
    "semantic_title": "evaluating spatial understanding of large language models",
    "citation_count": 1,
    "authors": [
      "Yutaro Yamada",
      "Yihan Bao",
      "Andrew Kyle Lampinen",
      "Jungo Kasai",
      "Ilker Yildirim"
    ]
  },
  "https://openreview.net/forum?id=3PbxuMNQkp": {
    "title": "Robust Learning Rate Selection for Stochastic Optimization via Splitting Diagnostic",
    "volume": "main",
    "abstract": "This paper proposes SplitSGD, a new dynamic learning rate schedule for stochastic optimization. This method decreases the learning rate for better adaptation to the local geometry of the objective function whenever a stationary phase is detected, that is, the iterates are likely to bounce at around a vicinity of a local minimum. The detection is performed by splitting the single thread into two and using the inner product of the gradients from the two threads as a measure of stationarity. Owing to this simple yet provably valid stationarity detection, SplitSGD is easy-to-implement and essentially does not incur additional computational cost than standard SGD. Through a series of extensive experiments, we show that this method is appropriate for both convex problems and training (non-convex) neural networks, with performance compared favorably to other stochastic optimization methods. Importantly, this method is observed to be very robust with a set of default parameters for a wide range of problems and, moreover, can yield better generalization performance than other adaptive gradient methods such as Adam",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matteo Sordello",
      "Niccolo Dalmasso",
      "Hangfeng He",
      "Weijie J Su"
    ]
  },
  "https://openreview.net/forum?id=d3xwrfAG4V": {
    "title": "Transfer Learning for High-dimensional Quantile Regression with Statistical Guarantee",
    "volume": "main",
    "abstract": "The task of transfer learning is to improve estimation/inference of a target model by migrating data from closely related source populations. In this article, we propose transfer learning algorithms for high-dimensional Quantile Regression (QR) models with the technique of convolution-type smoothing. Given the transferable source populations, we derive $\\ell_1/\\ell_2$-estimation error bounds for the estimators of the target regression coefficients under mild conditions. Theoretical analysis shows that the upper bounds are improved over those of the classical penalized QR estimator with only the target data, as long as the target and the sources are sufficiently similar to each other. When the set of informative sources is unknown, a transferable source detection algorithm is proposed to detect informative sources from all available sources. Thorough simulation studies justify our theoretical analysis",
    "checked": true,
    "id": "e5b434cb641b572f60bec85d335227273ca76f52",
    "semantic_title": "transfer learning for high-dimensional quantile regression with statistical guarantee",
    "citation_count": 0,
    "authors": [
      "Sheng Qiao",
      "Yong He",
      "Wenxin Zhou"
    ]
  },
  "https://openreview.net/forum?id=JdXzKSyqbH": {
    "title": "Recovering Exact Support in Federated lasso without Optimization",
    "volume": "main",
    "abstract": "Federated learning provides a framework to address the challenges of distributed computing, data ownership, and privacy over a large number of distributed clients with low computational and communication capabilities. In this paper, we study the problem of learning the exact support of sparse linear regression in the federated learning setup. We provide a simple communication efficient algorithm that only needs one-shot communication with the centralized server to compute the exact support by majority voting. Our method does not require the clients to solve any optimization problem and thus, can be run on devices with low computational capabilities. Our method is naturally robust to the problems of client failure, model poisoning, and straggling clients. We formally prove that our method requires a number of samples per client that is polynomial with respect to the support size, but independent of the dimension of the problem. We require the number of distributed clients to be logarithmic in the dimension of the problem. For certain classes of predictor variables (e.g. mutually independent, correlated Gaussian, etc.), the overall sample complexity matches the optimal sample complexity of the non-federated centralized setting. Furthermore, our method is easy to implement and has an overall polynomial time complexity",
    "checked": true,
    "id": "559e73fc2e9caacb4e8fa86103198150d479d884",
    "semantic_title": "recovering exact support in federated lasso without optimization",
    "citation_count": 0,
    "authors": [
      "Adarsh Barik",
      "Jean Honorio"
    ]
  },
  "https://openreview.net/forum?id=ebiAFpQ0Lw": {
    "title": "NorMatch: Matching Normalizing Flows with Discriminative Classifiers for Semi-Supervised Learning",
    "volume": "main",
    "abstract": "Semi-Supervised Learning (SSL) aims to learn a model using a tiny labeled set and massive amounts of unlabeled data. To better exploit the unlabeled data the latest SSL methods use pseudo-labels predicted from \\emph{a single discriminative classifier}. However, the generated pseudo-labels are inevitably linked to inherent confirmation bias and noise which greatly affects the model performance. In this work, we introduce a new framework for SSL named NorMatch. Firstly, we introduce a new uncertainty estimation scheme based on normalizing flows, as an auxiliary classifier, to enforce highly certain pseudo-labels yielding a boost of the discriminative classifiers. Secondly, we introduce a threshold-free sample weighting strategy to exploit better both high and low confidence pseudo-labels. Furthermore, we utilize normalizing flows to model, in an unsupervised fashion, the distribution of unlabeled data. This modelling assumption can further improve the performance of generative classifiers via unlabeled data, and thus, implicitly contributing to training a better discriminative classifier. We demonstrate, through numerical and visual results, that NorMatch achieves state-of-the-art performance on several datasets",
    "checked": true,
    "id": "383a44d071b8125ff864ffb3c5129e1d03cdee10",
    "semantic_title": "normatch: matching normalizing flows with discriminative classifiers for semi-supervised learning",
    "citation_count": 0,
    "authors": [
      "Zhongying Deng",
      "Rihuan Ke",
      "Carola-Bibiane Schönlieb",
      "Angelica I Aviles-Rivero"
    ]
  },
  "https://openreview.net/forum?id=CyjG4ZKCtE": {
    "title": "Mitigating Off-Policy Bias in Actor-Critic Methods with One-Step Q-learning: A Novel Correction Approach",
    "volume": "main",
    "abstract": "Compared to on-policy counterparts, off-policy model-free deep reinforcement learning can improve data efficiency by repeatedly using the previously gathered data. However, off-policy learning becomes challenging when the discrepancy between the underlying distributions of the agent's policy and collected data increases. Although the well-studied importance sampling and off-policy policy gradient techniques were proposed to compensate for this discrepancy, they usually require a collection of long trajectories and induce additional problems such as vanishing/exploding gradients or discarding many useful experiences, which eventually increases the computational complexity. Moreover, their generalization to either continuous action domains or policies approximated by deterministic deep neural networks is strictly limited. To overcome these limitations, we introduce a novel policy similarity measure to mitigate the effects of such discrepancy in continuous control. Our method offers an adequate single-step off-policy correction that is applicable to deterministic policy networks. Theoretical and empirical studies demonstrate that it can achieve a \"safe\" off-policy learning and substantially improve the state-of-the-art by attaining higher returns in fewer steps than the competing methods through an effective schedule of the learning rate in Q-learning and policy optimization",
    "checked": true,
    "id": "1dd9c9eb635fe8f77b1c4d5ed958f26612f90c2b",
    "semantic_title": "mitigating off-policy bias in actor-critic methods with one-step q-learning: a novel correction approach",
    "citation_count": 0,
    "authors": [
      "Baturay Saglam",
      "Doğan Can Çiçek",
      "Furkan Burak Mutlu",
      "Suleyman Kozat"
    ]
  },
  "https://openreview.net/forum?id=SSqOqAwpN7": {
    "title": "Provable Guarantees for Sparsity Recovery with Deterministic Missing Data Patterns",
    "volume": "main",
    "abstract": "We study the problem of consistently recovering the sparsity pattern of a regression parameter vector from correlated observations governed by deterministic missing data patterns using Lasso. We consider the case in which the observed dataset is censored by a deterministic, non-uniform filter. Recovering the sparsity pattern in datasets with deterministic missing structure can be arguably more challenging than recovering in a uniformly-at-random scenario. In this paper, we propose an efficient algorithm for missing value imputation by utilizing the topological property of the censorship filter. We then provide novel theoretical results for exact recovery of the sparsity pattern using the proposed imputation strategy. Our analysis shows that, under certain statistical and topological conditions, the hidden sparsity pattern can be recovered consistently with high probability in polynomial time and logarithmic sample complexity",
    "checked": true,
    "id": "6f13e295b6d50e025a35784d954b5d44ba0b67c7",
    "semantic_title": "provable guarantees for sparsity recovery with deterministic missing data patterns",
    "citation_count": 0,
    "authors": [
      "Chuyang Ke",
      "Jean Honorio"
    ]
  },
  "https://openreview.net/forum?id=dUVejidXO7": {
    "title": "Visual Prompt Based Personalized Federated Learning",
    "volume": "main",
    "abstract": "As a popular paradigm of distributed learning, personalized federated learning (PFL) allows personalized models to improve generalization ability and robustness by utilizing knowledge from all distributed clients. Most existing PFL algorithms tackle personalization in a model-centric way, such as personalized layer partition, model regularization, and model interpolation, which all fail to take into account the data characteristics of distributed clients. In this paper, we propose a novel PFL framework for image classification tasks, dubbed pFedPT, that leverages personalized visual prompts to implicitly represent local data distribution information of clients and provides that information to the aggregation model to help with classification tasks. Specifically, in each round of pFedPT training, each client generates a local personalized prompt related to local data distribution. Then, the local model is trained on the input composed of raw data and a visual prompt to learn the distribution information contained in the prompt. During model testing, the aggregated model obtains client-specific knowledge of the data distributions based on the prompts, which can be seen as an adaptive fine-tuning of the aggregation model to improve model performances on different clients. Furthermore, the visual prompt can be added as an orthogonal method to implement personalization on the client for existing FL methods to boost their performance. Experiments on the CIFAR10 and CIFAR100 datasets show that pFedPT outperforms several state-of-the-art (SOTA) PFL algorithms by a large margin in various settings. The code is available at: https://github.com/hkgdifyu/pFedPT",
    "checked": true,
    "id": "92c2c090ad911db57166821f8494de60fafe7d1d",
    "semantic_title": "visual prompt based personalized federated learning",
    "citation_count": 8,
    "authors": [
      "Guanghao Li",
      "Wansen Wu",
      "Yan Sun",
      "Li Shen",
      "Baoyuan Wu",
      "Dacheng Tao"
    ]
  },
  "https://openreview.net/forum?id=qKIvn9xL1R": {
    "title": "CR-MoE: Consistent Routed Mixture-of-Experts for Scaling Contrastive Learning",
    "volume": "main",
    "abstract": "While Contrastive Learning (CL) achieves great success in many downstream tasks, its good performance heavily relies on a large model capacity. As previous methods focus on scaling dense models, training and inference costs increase rapidly with model sizes, leading to large resource consumption. In this paper, we explore CL with an efficient scaling method, Mixture of Experts (MoE), to obtain a large but sparse model. We start by plugging in the state-of-the-art CL method to MoE. However, this naive combination fails to visibly improve performance despite a much larger capacity. A closer look reveals that the naive MoE+CL model has a strong tendency to route two augmented views of the same image token to different subsets of experts: such ``cross-view instability\" breaks the weight-sharing nature in CL and misleads the invariant feature learning. To address this issue, we introduce a new regularization mechanism, by enforcing expert-routing similarity between different views of the same image (or its overlapped patch tokens), while promoting expert-routing diversity of patches from different images. The resultant method, called CR-MoE, improves by 1.7 points in terms of 1\\% semi-supervised learning accuracy on ImageNet, compared to the naive combination baseline. It further surpasses the state-of-the-art CL methods on ImageNet pre-training of Vision Transformer (ViT) by 2.8 points, at the same computational cost. Our findings validate CR-MoE as an effective and efficient image representation learner. Code is available at https://github.com/VITA-Group/CRMoE",
    "checked": true,
    "id": "40891680185dd23b1270cb814ea555811c3b4618",
    "semantic_title": "cr-moe: consistent routed mixture-of-experts for scaling contrastive learning",
    "citation_count": 0,
    "authors": [
      "Ziyu Jiang",
      "Guoqing Zheng",
      "Yu Cheng",
      "Ahmed Hassan Awadallah",
      "Zhangyang Wang"
    ]
  },
  "https://openreview.net/forum?id=uqQPyWFDhY": {
    "title": "Error Bounds for Flow Matching Methods",
    "volume": "main",
    "abstract": "Score-based generative models are a popular class of generative modelling techniques relying on stochastic differential equations (SDEs). From their inception, it was realized that it was also possible to perform generation using ordinary differential equations (ODEs) rather than SDEs. This led to the introduction of the probability flow ODE approach and denoising diffusion implicit models. Flow matching methods have recently further extended these ODE-based approaches and approximate a flow between two arbitrary probability distributions. Previous work derived bounds on the approximation error of diffusion models under the stochastic sampling regime, given assumptions on the $L^2$ loss. We present error bounds for the flow matching procedure using fully deterministic sampling, assuming an $L^2$ bound on the approximation error and a certain regularity condition on the data distributions",
    "checked": true,
    "id": "465ea43827d145fef68e100bb367208ad85678bb",
    "semantic_title": "error bounds for flow matching methods",
    "citation_count": 12,
    "authors": [
      "Joe Benton",
      "George Deligiannidis",
      "Arnaud Doucet"
    ]
  },
  "https://openreview.net/forum?id=17ESEjETbP": {
    "title": "Non-Uniform Smoothness for Gradient Descent",
    "volume": "main",
    "abstract": "The analysis of gradient descent-type methods typically relies on the Lipschitz continuity of the objective gradient. This generally requires an expensive hyperparameter tuning process to appropriately calibrate a stepsize for a given problem. In this work we introduce a local first-order smoothness oracle (LFSO) which generalizes the Lipschitz continuous gradients smoothness condition and is applicable to any twice-differentiable function. We show that this oracle can encode all relevant problem information for tuning stepsizes for a suitably modified gradient descent method and give global and local convergence results. We also show that LFSOs in this modified first-order method can yield global linear convergence rates for non-strongly convex problems with extremely flat minima, and thus improve over the lower bound on rates achievable by general (accelerated) first-order methods",
    "checked": true,
    "id": "ab21e7ee715a02b76c209e20d90a58f758cbcad8",
    "semantic_title": "non-uniform smoothness for gradient descent",
    "citation_count": 1,
    "authors": [
      "Albert S. Berahas",
      "Lindon Roberts",
      "Fred Roosta"
    ]
  },
  "https://openreview.net/forum?id=Jy2IgzjoFH": {
    "title": "Hierarchical Neural Simulation-Based Inference Over Event Ensembles",
    "volume": "main",
    "abstract": "When analyzing real-world data it is common to work with event ensembles, which comprise sets of observations that collectively constrain the parameters of an underlying model of interest. Such models often have a hierarchical structure, where ``local'' parameters impact individual events and ``global'' parameters influence the entire dataset. We introduce practical approaches for frequentist and Bayesian dataset-wide probabilistic inference in cases where the likelihood is intractable, but simulations can be realized via a hierarchical forward model. We construct neural estimators for the likelihood(-ratio) or posterior and show that explicitly accounting for the model's hierarchical structure can lead to significantly tighter parameter constraints. We ground our discussion using case studies from the physical sciences, focusing on examples from particle physics and cosmology",
    "checked": true,
    "id": "69667fd30ec42d34e78345bb6e19071e6e5a9b5b",
    "semantic_title": "hierarchical neural simulation-based inference over event ensembles",
    "citation_count": 1,
    "authors": [
      "Lukas Heinrich",
      "Siddharth Mishra-Sharma",
      "Chris Pollard",
      "Philipp Windischhofer"
    ]
  },
  "https://openreview.net/forum?id=HyqSwNhM3x": {
    "title": "What is the Solution for State-Adversarial Multi-Agent Reinforcement Learning?",
    "volume": "main",
    "abstract": "Various methods for Multi-Agent Reinforcement Learning (MARL) have been developed with the assumption that agents' policies are based on accurate state information. However, policies learned through Deep Reinforcement Learning (DRL) are susceptible to adversarial state perturbation attacks. In this work, we propose a State-Adversarial Markov Game (SAMG) and make the first attempt to investigate different solution concepts of MARL under state uncertainties. Our analysis shows that the commonly used solution concepts of optimal agent policy and robust Nash equilibrium do not always exist in SAMGs. To circumvent this difficulty, we consider a new solution concept called robust agent policy, where agents aim to maximize the worst-case expected state value. We prove the existence of robust agent policy for finite state and finite action SAMGs. Additionally, we propose a Robust Multi-Agent Adversarial Actor-Critic (RMA3C) algorithm to learn robust policies for MARL agents under state uncertainties. Our experiments demonstrate that our algorithm outperforms existing methods when faced with state perturbations and greatly improves the robustness of MARL policies. Our code is public on https://songyanghan.github.io/what_is_solution/",
    "checked": true,
    "id": "cfd2c668504c0a97e73fe6e40fe7fc869aa5a40a",
    "semantic_title": "what is the solution for state-adversarial multi-agent reinforcement learning?",
    "citation_count": 18,
    "authors": [
      "Songyang Han",
      "Sanbao Su",
      "Sihong He",
      "Shuo Han",
      "Haizhao Yang",
      "Shaofeng Zou",
      "Fei Miao"
    ]
  },
  "https://openreview.net/forum?id=Y2ru0LuQeS": {
    "title": "MESSY Estimation: Maximum-Entropy based Stochastic and Symbolic densitY Estimation",
    "volume": "main",
    "abstract": "We introduce MESSY estimation, a Maximum-Entropy based Stochastic and Symbolic densitY estimation method. The proposed approach recovers probability density functions symbolically from samples using moments of a Gradient flow in which the ansatz serves as the driving force. In particular, we construct a gradient-based drift-diffusion process that connects samples of the unknown distribution function to a guess symbolic expression. We then show that when the guess distribution has the maximum entropy form, the parameters of this distribution can be found efficiently by solving a linear system of equations constructed using the moments of the provided samples. Furthermore, we use Symbolic regression to explore the space of smooth functions and find optimal basis functions for the exponent of the maximum entropy functional leading to good conditioning. The cost of the proposed method for each set of selected basis functions is linear with the number of samples and quadratic with the number of basis functions. However, the underlying acceptance/rejection procedure for finding optimal and well-conditioned bases adds to the computational cost. We validate the proposed MESSY estimation method against other benchmark methods for the case of a bi-modal and a discontinuous density, as well as a density at the limit of physical realizability. We find that the addition of a symbolic search for basis functions improves the accuracy of the estimation at a reasonable additional computational cost. Our results suggest that the proposed method outperforms existing density recovery methods in the limit of a small to moderate number of samples by providing a low-bias and tractable symbolic description of the unknown density at a reasonable computational cost",
    "checked": true,
    "id": "f248be199edf5dce636f958814d50f9b9e52bc34",
    "semantic_title": "messy estimation: maximum-entropy based stochastic and symbolic density estimation",
    "citation_count": 3,
    "authors": [
      "Tony Tohme",
      "Mohsen Sadr",
      "KAMAL YOUCEF-TOUMI",
      "Nicolas Hadjiconstantinou"
    ]
  },
  "https://openreview.net/forum?id=ynG5Ak7n7Q": {
    "title": "The Fair Value of Data Under Heterogeneous Privacy Constraints in Federated Learning",
    "volume": "main",
    "abstract": "Modern data aggregation often involves a platform collecting data from a network of users with various privacy options. Platforms must solve the problem of how to allocate incentives to users to convince them to share their data. This paper puts forth an idea for a fair amount to compensate users for their data at a given privacy level based on an axiomatic definition of fairness, along the lines of the celebrated Shapley value. To the best of our knowledge, these are the first fairness concepts for data that explicitly consider privacy constraints. We also formulate a heterogeneous federated learning problem for the platform with privacy level options for users. By studying this problem, we investigate the amount of compensation users receive under fair allocations with different privacy levels, amounts of data, and degrees of heterogeneity. We also discuss what happens when the platform is forced to design fair incentives. Under certain conditions we find that when privacy sensitivity is low, the platform will set incentives to ensure that it collects all the data with the lowest privacy options. When the privacy sensitivity is above a given threshold, the platform will provide no incentives to users. Between these two extremes, the platform will set the incentives so some fraction of the users chooses the higher privacy option and the others chooses the lower privacy option",
    "checked": true,
    "id": "3fa0bf2a4f563f25514bafe79098d2ea18f0f56d",
    "semantic_title": "the fair value of data under heterogeneous privacy constraints in federated learning",
    "citation_count": 3,
    "authors": [
      "Justin Singh Kang",
      "Ramtin Pedarsani",
      "Kannan Ramchandran"
    ]
  },
  "https://openreview.net/forum?id=pWsfWDnJDa": {
    "title": "Out-of-Distribution Optimality of Invariant Risk Minimization",
    "volume": "main",
    "abstract": "Deep Neural Networks often inherit spurious correlations embedded in training data and hence may fail to generalize to unseen domains, which have different distributions from the domain to provide training data. M. Arjovsky et al. (2019) introduced the concept out-of-distribution (o.o.d.) risk, which is the maximum risk among all domains, and formulated the issue caused by spurious correlations as a minimization problem of the o.o.d. risk. Invariant Risk Minimization (IRM) is considered to be a promising approach to minimize the o.o.d. risk: IRM estimates a minimum of the o.o.d. risk by solving a bi-level optimization problem. While IRM has attracted considerable attention with empirical success, it comes with few theoretical guarantees. Especially, a solid theoretical guarantee that the bi-level optimization problem gives the minimum of the o.o.d. risk has not yet been established. Aiming at providing a theoretical justification for IRM, this paper rigorously proves that a solution to the bi-level optimization problem minimizes the o.o.d. risk under certain conditions. The result also provides sufficient conditions on distributions providing training data and on a dimension of feature space for the bi-leveled optimization problem to minimize the o.o.d. risk",
    "checked": true,
    "id": "1b326b04bfe326295e588e18ebfd94b26a878c85",
    "semantic_title": "out-of-distribution optimality of invariant risk minimization",
    "citation_count": 0,
    "authors": [
      "Shoji Toyota",
      "Kenji Fukumizu"
    ]
  },
  "https://openreview.net/forum?id=ZLVbQEu4Ab": {
    "title": "When is Momentum Extragradient Optimal? A Polynomial-Based Analysis",
    "volume": "main",
    "abstract": "The extragradient method has gained popularity due to its robust convergence properties for differentiable games. Unlike single-objective optimization, game dynamics involve complex interactions reflected by the eigenvalues of the game vector field's Jacobian scattered across the complex plane. This complexity can cause the simple gradient method to diverge, even for bilinear games, while the extragradient method achieves convergence. Building on the recently proven accelerated convergence of the momentum extragradient method for bilinear games \\citep{azizian2020accelerating}, we use a polynomial-based analysis to identify three distinct scenarios where this method exhibits further accelerated convergence. These scenarios encompass situations where the eigenvalues reside on the (positive) real line, lie on the real line alongside complex conjugates, or exist solely as complex conjugates. Furthermore, we derive the hyperparameters for each scenario that achieve the fastest convergence rate",
    "checked": true,
    "id": "1412997ab05fabe40a9dec6e1d8622c8480a8c71",
    "semantic_title": "when is momentum extragradient optimal? a polynomial-based analysis",
    "citation_count": 0,
    "authors": [
      "Junhyung Lyle Kim",
      "Gauthier Gidel",
      "Anastasios Kyrillidis",
      "Fabian Pedregosa"
    ]
  },
  "https://openreview.net/forum?id=Wqn8zirthg": {
    "title": "DDLP: Unsupervised Object-centric Video Prediction with Deep Dynamic Latent Particles",
    "volume": "main",
    "abstract": "We propose a new object-centric video prediction algorithm based on the deep latent particle (DLP) representation of Daniel and Tamar (2022). In comparison to existing slot- or patch-based representations, DLPs model the scene using a set of keypoints with learned parameters for properties such as position and size, and are both efficient and interpretable. Our method, \\textit{deep dynamic latent particles} (DDLP), yields state-of-the-art object-centric video prediction results on several challenging datasets. The interpretable nature of DDLP allows us to perform ``what-if'' generation -- predict the consequence of changing properties of objects in the initial frames, and DLP's compact structure enables efficient diffusion-based unconditional video generation. Videos, code and pre-trained models are available: https://taldatech.github.io/ddlp-web",
    "checked": true,
    "id": "7d22e6d110a2be18ef7236fb2238fd85463de4b2",
    "semantic_title": "ddlp: unsupervised object-centric video prediction with deep dynamic latent particles",
    "citation_count": 3,
    "authors": [
      "Tal Daniel",
      "Aviv Tamar"
    ]
  },
  "https://openreview.net/forum?id=vWTZO1RXZR": {
    "title": "Introspective Experience Replay: Look Back When Surprised",
    "volume": "main",
    "abstract": "In reinforcement learning (RL), experience replay-based sampling techniques are crucial in promoting convergence by eliminating spurious correlations. However, widely used methods such as uniform experience replay (UER) and prioritized experience replay (PER) have been shown to have sub-optimal convergence and high seed sensitivity, respectively. To address these issues, we propose a novel approach called Introspective Experience Replay (IER) that selectively samples batches of data points prior to surprising events. Our method is inspired from the reverse experience replay (RER) technique, which has been shown to reduce bias in the output of Q-learning-type algorithms with linear function approximation. However, RER is not always practically reliable when using neural function approximation. Through empirical evaluations, we demonstrate that IER with neural function approximation yields reliable and superior performance compared to UER, PER, and hindsight experience replay (HER) across most tasks",
    "checked": true,
    "id": "e6bd20a796c48f9c17ef55c6a821ec678a341b3b",
    "semantic_title": "introspective experience replay: look back when surprised",
    "citation_count": 0,
    "authors": [
      "Ramnath Kumar",
      "Dheeraj Mysore Nagaraj"
    ]
  },
  "https://openreview.net/forum?id=O9RUANpPmb": {
    "title": "Domain-Generalizable Multiple-Domain Clustering",
    "volume": "main",
    "abstract": "This work generalizes the problem of unsupervised domain generalization to the case in which no labeled samples are available (completely unsupervised). We are given unlabeled samples from multiple source domains, and we aim to learn a shared predictor that assigns examples to semantically related clusters. Evaluation is done by predicting cluster assignments in previously unseen domains. Towards this goal, we propose a two-stage training framework: (1) self-supervised pre-training for extracting domain invariant semantic features. (2) multi-head cluster prediction with pseudo labels, which rely on both the feature space and cluster head prediction, further leveraging a novel prediction-based label smoothing scheme. We demonstrate empirically that our model is more accurate than baselines that require fine-tuning using samples from the target domain or some level of supervision. Our code is available at \\url{https://github.com/AmitRozner/domain-generalizable-multiple-domain-clustering}",
    "checked": true,
    "id": "dfe2fffe974ec43dff94f02228d545270e3e63fb",
    "semantic_title": "domain-generalizable multiple-domain clustering",
    "citation_count": 4,
    "authors": [
      "Amit Rozner",
      "Barak Battash",
      "Lior Wolf",
      "Ofir Lindenbaum"
    ]
  },
  "https://openreview.net/forum?id=vsCpILiWHu": {
    "title": "RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation",
    "volume": "main",
    "abstract": "The ability to leverage heterogeneous robotic experience from different robots and tasks to quickly master novel skills and embodiments has the potential to transform robot learning. Inspired by recent advances in foundation models for vision and language, we propose a multi-embodiment, multi-task generalist agent for robotic manipulation. This agent, named RoboCat, is a visual goal-conditioned decision transformer capable of consuming action-labelled visual experience. This data spans a large repertoire of motor control skills from simulated and real robotic arms with varying sets of observations and actions. With RoboCat, we demonstrate the ability to generalise to new tasks and robots, both zero-shot as well as through adaptation using only 100–1000 examples for the target task. We also show how a trained model itself can be used to generate data for subsequent training iterations, thus providing a basic building block for an autonomous improvement loop. We investigate the agent's capabilities, with large-scale evaluations both in simulation and on three different real robot embodiments. We find that as we grow and diversify its training data, RoboCat not only shows signs of cross-task transfer, but also becomes more efficient at adapting to new tasks",
    "checked": true,
    "id": "2562fe379554d201aad312f786903f4c60b68acf",
    "semantic_title": "robocat: a self-improving generalist agent for robotic manipulation",
    "citation_count": 22,
    "authors": [
      "Konstantinos Bousmalis",
      "Giulia Vezzani",
      "Dushyant Rao",
      "Coline Manon Devin",
      "Alex X. Lee",
      "Maria Bauza Villalonga",
      "Todor Davchev",
      "Yuxiang Zhou",
      "Agrim Gupta",
      "Akhil Raju",
      "Antoine Laurens",
      "Claudio Fantacci",
      "Valentin Dalibard",
      "Martina Zambelli",
      "Murilo Fernandes Martins",
      "Rugile Pevceviciute",
      "Michiel Blokzijl",
      "Misha Denil",
      "Nathan Batchelor",
      "Thomas Lampe",
      "Emilio Parisotto",
      "Konrad Zolna",
      "Scott Reed",
      "Sergio Gómez Colmenarejo",
      "Jonathan Scholz",
      "Abbas Abdolmaleki",
      "Oliver Groth",
      "Jean-Baptiste Regli",
      "Oleg Sushkov",
      "Thomas Rothörl",
      "Jose Enrique Chen",
      "Yusuf Aytar",
      "David Barker",
      "Joy Ortiz",
      "Martin Riedmiller",
      "Jost Tobias Springenberg",
      "Raia Hadsell",
      "Francesco Nori",
      "Nicolas Heess"
    ]
  },
  "https://openreview.net/forum?id=Igxp7FC8uf": {
    "title": "Fixed-Budget Best-Arm Identification in Sparse Linear Bandits",
    "volume": "main",
    "abstract": "We study the best-arm identification problem in sparse linear bandits under the fixed-budget setting. In sparse linear bandits, the unknown feature vector $\\theta^*$ may be of large dimension $d$, but only a few, say $s \\ll d$ of these features have non-zero values. We design a two-phase algorithm, Lasso and Optimal-Design- (Lasso-OD) based linear best-arm identification. The first phase of Lasso-OD leverages the sparsity of the feature vector by applying the thresholded Lasso introduced by Zhou (2009), which estimates the support of $\\theta^*$ correctly with high probability using rewards from the selected arms and a judicious choice of the design matrix. The second phase of Lasso-OD applies the OD-LinBAI algorithm by Yang and Tan (2022) on that estimated support. We derive a non-asymptotic upper bound on the error probability of Lasso-OD by carefully choosing hyperparameters (such as Lasso's regularization parameter) and balancing the error probabilities of both phases. For fixed sparsity $s$ and budget $T$, the exponent in the error probability of Lasso-OD depends on $s$ but not on the dimension $d$, yielding a significant performance improvement for sparse and high-dimensional linear bandits. Furthermore, we show that Lasso-OD is almost minimax optimal in the exponent. Finally, we provide numerical examples to demonstrate the significant performance improvement over the existing algorithms for non-sparse linear bandits such as OD-LinBAI, BayesGap, Peace, LinearExploration, and GSE",
    "checked": true,
    "id": "e59f0a8bceffaefb7f646723e53afcb3ff3bc9e0",
    "semantic_title": "fixed-budget best-arm identification in sparse linear bandits",
    "citation_count": 1,
    "authors": [
      "Recep Can Yavas",
      "Vincent Y. F. Tan"
    ]
  },
  "https://openreview.net/forum?id=6BDHUkSPna": {
    "title": "Understanding the Role of Layer Normalization in Label-Skewed Federated Learning",
    "volume": "main",
    "abstract": "Layer normalization (LN) is a widely adopted deep learning technique especially in the era of foundation models. Recently, LN has been shown to be surprisingly effective in federated learning (FL) with non-i.i.d. data. However, exactly why and how it works remains mysterious. In this work, we reveal the profound connection between layer normalization and the label shift problem in federated learning. To understand layer normalization better in FL, we identify the key contributing mechanism of normalization methods in FL, called feature normalization (FN), which applies normalization to the latent feature representation before the classifier head. Although LN and FN do not improve expressive power, they control feature collapse and local overfitting to heavily skewed datasets, and thus accelerates global training. Empirically, we show that normalization leads to drastic improvements on standard benchmarks under extreme label shift. Moreover, we conduct extensive ablation studies to understand the critical factors of layer normalization in FL. Our results verify that FN is an essential ingredient inside LN to significantly improve the convergence of FL while remaining robust to learning rate choices, especially under extreme label shift where each client has access to few classes",
    "checked": true,
    "id": "dcf939f84b63e38a0235868ac3e7b9f99a196765",
    "semantic_title": "understanding the role of layer normalization in label-skewed federated learning",
    "citation_count": 1,
    "authors": [
      "Guojun Zhang",
      "Mahdi Beitollahi",
      "Alex Bie",
      "Xi Chen"
    ]
  },
  "https://openreview.net/forum?id=KKARKoPcEA": {
    "title": "Learning to Abstain From Uninformative Data",
    "volume": "main",
    "abstract": "Learning and decision-making in domains with naturally high noise-to-signal ratios – such as Finance or Healthcare – is often challenging, while the stakes are very high. In this paper, we study the problem of learning and acting under a general noisy generative process. In this problem, the data distribution has a significant proportion of uninformative samples with high noise in the label, while part of the data contains useful information represented by low label noise. This dichotomy is present during both training and inference, which requires the proper handling of uninformative data during both training and testing. We propose a novel approach to learning under these conditions via a loss inspired by the selective learning theory. By minimizing this loss, the model is guaranteed to make a near-optimal decision by distinguishing informative data from uninformative data and making predictions. We build upon the strength of our theoretical guarantees by describing an iterative algorithm, which jointly optimizes both a predictor and a selector, and evaluates its empirical performance in a variety of settings",
    "checked": true,
    "id": "6df82e80441fc2a67c6ad81879001313fab36568",
    "semantic_title": "learning to abstain from uninformative data",
    "citation_count": 0,
    "authors": [
      "Yikai Zhang",
      "Songzhu Zheng",
      "Mina Dalirrooyfard",
      "Pengxiang Wu",
      "Anderson Schneider",
      "Anant Raj",
      "Yuriy Nevmyvaka",
      "Chao Chen"
    ]
  },
  "https://openreview.net/forum?id=6wpInwnzs8": {
    "title": "WaveBench: Benchmarking Data-driven Solvers for Linear Wave Propagation PDEs",
    "volume": "main",
    "abstract": "Wave-based imaging techniques play a critical role in diverse scientific, medical, and industrial endeavors, from discovering hidden structures beneath the Earth's surface to ultrasound diagnostics. They rely on accurate solutions to the forward and inverse problems for partial differential equations (PDEs) that govern wave propagation. Surrogate PDE solvers based on machine learning emerged as an effective approach to computing the solutions more efficiently than via classical numerical schemes. However, existing datasets for PDE surrogates offer only limited coverage of the wave propagation phenomenon. In this paper, we present WaveBench, a comprehensive collection of benchmark datasets for wave propagation PDEs. WaveBench (1) contains 24 datasets that cover a wide range of forward and inverse problems for time-harmonic and time-varying wave phenomena; (2) includes a user-friendly PyTorch environment for comparing learning-based methods; and (3) comprises reference performance and model checkpoints of popular PDE surrogates such as Fourier neural operators and U-Nets. Our evaluation on WaveBench demonstrates the impressive performance of PDE surrogates on in-distribution samples, while simultaneously unveiling their limitations on out-of-distribution samples, indicating room for future improvements. We anticipate that WaveBench will stimulate the development of accurate wave-based imaging techniques through machine learning",
    "checked": true,
    "id": "ad319a06082af904a1c1b4ee82a96e9b159d2eff",
    "semantic_title": "wavebench: benchmarking data-driven solvers for linear wave propagation pdes",
    "citation_count": 0,
    "authors": [
      "Tianlin Liu",
      "Jose Antonio Lara Benitez",
      "Florian Faucher",
      "AmirEhsan Khorashadizadeh",
      "Maarten V. de Hoop",
      "Ivan Dokmanić"
    ]
  },
  "https://openreview.net/forum?id=fUhOb14sQv": {
    "title": "Using Motion Cues to Supervise Single-frame Body Pose & Shape Estimation in Low Data Regimes",
    "volume": "main",
    "abstract": "When enough annotated training data is available, supervised deep-learning algorithms excel at estimating human body pose and shape using a single camera. The effects of too little such data being available can be mitigated by using other information sources, such as databases of body shapes, to learn priors. Unfortunately, such sources are not always available either. We show that, in such cases, easy to-obtain unannotated videos can be used instead to provide the required supervisory signals. Given a trained model using too little annotated data, we compute poses in consecutive frames along with the optical flow between them. We then enforce consistency between the image optical flow and the one that can be inferred from the change in pose from one frame to the next. This provides enough additional supervision to effectively refine the network weights and to perform on par with methods trained using far more annotated data",
    "checked": false,
    "id": "04ebb3d96065b4296c85dcd030ac1ea9654c9412",
    "semantic_title": "using motion cues to supervise single-frame body pose and shape estimation in low data regimes",
    "citation_count": 0,
    "authors": [
      "Andrey Davydov",
      "Alexey Sidnev",
      "Artsiom Sanakoyeu",
      "Yuhua Chen",
      "Mathieu Salzmann",
      "Pascal Fua"
    ]
  },
  "https://openreview.net/forum?id=E8m8oySvPJ": {
    "title": "LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations",
    "volume": "main",
    "abstract": "Can a Large Language Model (LLM) solve simple abstract reasoning problems? We explore this broad question through a systematic analysis of GPT on the Abstraction and Reasoning Corpus (ARC), a representative benchmark of abstract reasoning ability from limited examples in which solutions require some \"core knowledge\" of concepts such as objects, goal states, counting, and basic geometry. GPT-4 solves only 13/50 of the most straightforward ARC tasks when using textual encodings for their two-dimensional input-output grids. Our failure analysis reveals that GPT-4's capacity to identify objects and reason about them is significantly influenced by the sequential nature of the text that represents an object within a text encoding of a task. To test this hypothesis, we design a new benchmark, the 1D-ARC, which consists of one-dimensional (array-like) tasks that are more conducive to GPT-based reasoning, and where it indeed performs better than on the (2D) ARC. To alleviate this issue, we propose an object-based representation that is obtained through an external tool, resulting in nearly doubling the performance on solved ARC tasks and near-perfect scores on the easier 1D-ARC. Although the state-of-the-art GPT-4 is unable to \"reason\" perfectly within non-language domains such as the 1D-ARC or a simple ARC subset, our study reveals that the use of object-based representations can significantly improve its reasoning ability. Visualizations, GPT logs, and data are available at https://khalil-research.github.io/LLM4ARC",
    "checked": true,
    "id": "8826311d922135dbf0cfdb4a661ebab347e3b826",
    "semantic_title": "llms and the abstraction and reasoning corpus: successes, failures, and the importance of object-based representations",
    "citation_count": 14,
    "authors": [
      "Yudong Xu",
      "Wenhao Li",
      "Pashootan Vaezipoor",
      "Scott Sanner",
      "Elias Boutros Khalil"
    ]
  },
  "https://openreview.net/forum?id=fJAwemcvpL": {
    "title": "Candidate Set Re-ranking for Composed Image Retrieval with Dual Multi-modal Encoder",
    "volume": "main",
    "abstract": "Composed image retrieval aims to find an image that best matches a given multi-modal user query consisting of a reference image and text pair. Existing methods commonly pre-compute image embeddings over the entire corpus and compare these to a reference image embedding modified by the query text at test time. Such a pipeline is very efficient at test time since fast vector distances can be used to evaluate candidates, but modifying the reference image embedding guided only by a short textual description can be difficult, especially independent of potential candidates. An alternative approach is to allow interactions between the query and every possible candidate, i.e., reference-text-candidate triplets, and pick the best from the entire set. Though this approach is more discriminative, for large-scale datasets the computational cost is prohibitive since pre-computation of candidate embeddings is no longer possible. We propose to combine the merits of both schemes using a two-stage model. Our first stage adopts the conventional vector distancing metric and performs a fast pruning among candidates. Meanwhile, our second stage employs a dual-encoder architecture, which effectively attends to the input triplet of reference-text-candidate and re-ranks the candidates. Both stages utilize a vision-and-language pre-trained network, which has proven beneficial for various downstream tasks. Our method consistently outperforms state-of-the-art approaches on standard benchmarks for the task. Our implementation is available at https://github.com/Cuberick-Orion/Candidate-Reranking-CIR",
    "checked": true,
    "id": "5eceaeac5d45d49e1d5698947ed8292ff3fccd81",
    "semantic_title": "candidate set re-ranking for composed image retrieval with dual multi-modal encoder",
    "citation_count": 4,
    "authors": [
      "Zheyuan Liu",
      "Weixuan Sun",
      "Damien Teney",
      "Stephen Gould"
    ]
  },
  "https://openreview.net/forum?id=jesKcQxQ7j": {
    "title": "A Review of the Applications of Deep Learning-Based Emergent Communication",
    "volume": "main",
    "abstract": "Emergent communication, or emergent language, is the field of research which studies how human language-like communication systems emerge de novo in deep multi-agent reinforcement learning environments. The possibilities of replicating the emergence of a complex behavior like language have strong intuitive appeal, yet it is necessary to complement this with clear notions of how such research can be applicable to other fields of science, technology, and engineering. This paper comprehensively reviews the applications of emergent communication research across machine learning, natural language processing, linguistics, and cognitive science. Each application is illustrated with a description of its scope, an explication of emergent communication's unique role in addressing it, a summary of the extant literature working towards the application, and brief recommendations for near-term research directions",
    "checked": true,
    "id": "5b19fb4c441856490cbfe8c3eab05187e4063d33",
    "semantic_title": "a review of the applications of deep learning-based emergent communication",
    "citation_count": 0,
    "authors": [
      "Brendon Boldt",
      "David R Mortensen"
    ]
  },
  "https://openreview.net/forum?id=mH6TelHVKD": {
    "title": "Data-Dependent Generalization Bounds for Neural Networks with ReLU",
    "volume": "main",
    "abstract": "We try to establish that one of the correct data-dependent quantities to look at while trying to prove generalization bounds, even for overparameterized neural networks, are the gradients encountered by stochastic gradient descent while training the model. If these are small, then the model generalizes. To make this conclusion rigorous, we weaken the notion of uniform stability of a learning algorithm in a probabilistic way by positing the notion of almost sure (a.s.) support stability and showing that algorithms that have this form of stability have generalization error tending to 0 as the training set size increases. Further, we show that for Stochastic Gradient Descent to be a.s. support stable we only need the loss function to be a.s. locally Lipschitz and locally Smooth at the training points, thereby showing low generalization error with weaker conditions than have been used in the literature. We then show that Neural Networks with ReLU activation and a doubly differentiable loss function possess these properties. Our notion of stability is the first data-dependent notion to be able to show good generalization bounds for non-convex functions with learning rates strictly slower than $1/t$ at the $t$-th step. Finally, we present experimental evidence to validate our theoretical results",
    "checked": true,
    "id": "26363cbad28c93b60485a21ee9cbf7da4c293ad3",
    "semantic_title": "data-dependent generalization bounds for neural networks with relu",
    "citation_count": 0,
    "authors": [
      "Harsh Pandey",
      "Amitabha Bagchi",
      "Srikanta J. Bedathur",
      "Arindam Bhattacharya"
    ]
  },
  "https://openreview.net/forum?id=5nBqY1y96B": {
    "title": "Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs",
    "volume": "main",
    "abstract": "Large language models (LLMs) have achieved widespread success on a variety of in-context few-shot tasks, but this success is typically evaluated via correctness rather than consistency. We argue that self-consistency is an important criteria for valid multi-step reasoning in tasks where the solution is composed of the answers to multiple sub-steps. We propose two types of self-consistency that are particularly important for multi-step reasoning -- hypothetical consistency (a model's ability to predict what its output would be in a hypothetical other context) and compositional consistency (consistency of a model's final outputs when intermediate sub-steps are replaced with the model's outputs for those steps). We demonstrate that multiple variants of the GPT-3/-4 models exhibit poor consistency rates across both types of consistency on a variety of tasks",
    "checked": true,
    "id": "3200a0d6fef7164f0341cf1938f584da6057ffd6",
    "semantic_title": "two failures of self-consistency in the multi-step reasoning of llms",
    "citation_count": 9,
    "authors": [
      "Angelica Chen",
      "Jason Phang",
      "Alicia Parrish",
      "Vishakh Padmakumar",
      "Chen Zhao",
      "Samuel R. Bowman",
      "Kyunghyun Cho"
    ]
  },
  "https://openreview.net/forum?id=OdDsCaacZ0": {
    "title": "MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments",
    "volume": "main",
    "abstract": "Self-supervised learning can be used for mitigating the greedy needs of Vision Transformer networks for very large fully-annotated datasets. Different classes of self-supervised learning offer representations with either good contextual reasoning properties, e.g., using masked image modeling strategies, or invariance to image perturbations, e.g., with contrastive methods. In this work, we propose a single-stage and standalone method, MOCA, which unifies both desired properties using novel mask-and-predict objectives defined with high-level features (instead of pixel-level details). Moreover, we show how to effectively employ both learning paradigms in a synergistic and computation-efficient way. Doing so, we achieve new state-of-the-art results on low-shot settings and strong experimental results in various evaluation protocols with a training that is at least 3 times faster than prior methods. We provide the implementation code at https://github.com/valeoai/MOCA",
    "checked": true,
    "id": "39ffd546e3b64b6313035164ed899339456307a2",
    "semantic_title": "moca: self-supervised representation learning by predicting masked online codebook assignments",
    "citation_count": 1,
    "authors": [
      "Spyros Gidaris",
      "Andrei Bursuc",
      "Oriane Siméoni",
      "Antonín Vobecký",
      "Nikos Komodakis",
      "Matthieu Cord",
      "Patrick Perez"
    ]
  },
  "https://openreview.net/forum?id=emXh4M7TyH": {
    "title": "Transfer Learning for Bayesian Optimization on Heterogeneous Search Spaces",
    "volume": "main",
    "abstract": "Bayesian optimization (BO) is a popular black-box function optimization method, which makes sequential decisions based on a Bayesian model, typically a Gaussian process (GP), of the function. To ensure the quality of the model, transfer learning approaches have been developed to automatically design GP priors by learning from observations on \"training\" functions. These training functions are typically required to have the same domain as the \"test\" function (black-box function to be optimized). In this paper, we introduce MPHD, a model pre-training method on heterogeneous domains, which uses a neural net mapping from domain-specific contexts to specifications of hierarchical GPs. MPHD can be seamlessly integrated with BO to transfer knowledge across heterogeneous search spaces. Our theoretical and empirical results demonstrate the validity of MPHD and its superior performance on challenging black-box function optimization tasks",
    "checked": true,
    "id": "7889cf92ff5120430db0371ff5c70aeb151ac71d",
    "semantic_title": "transfer learning for bayesian optimization on heterogeneous search spaces",
    "citation_count": 2,
    "authors": [
      "Zhou Fan",
      "Xinran Han",
      "Zi Wang"
    ]
  },
  "https://openreview.net/forum?id=zc0Y0cAuTV": {
    "title": "A Multilinear Least-Squares Formulation for Sparse Tensor Canonical Correlation Analysis",
    "volume": "main",
    "abstract": "Tensor data are becoming important recently in various applications, e.g., image and video recognition, which pose new challenges for data modeling and analysis approaches, such as high-order relations of large complexity, varying data scale and gross noise. In this paper, we consider the problem of sparse canonical correlation analysis for arbitrary tensor data. Although several methods have been proposed for this task, there are still limitations hindering its practical applications. To this end, we present a general Sparse Tensor Canonical Correlation Analysis (gSTCCA) method from a multilinear least-squares perspective. Specifically, we formulate the problem as a constrained multilinear least-squares problem with tensor-structured sparsity regularization based on CANDECOMP/PARAFAC (CP) decomposition. Then we present a divide-and-conquer deflation approach to tackle the problem by successive rank-one tensor estimation of the residual tensors, where the overall model is broken up into a set of unconstrained linear least-squares problems that can be efficiently solved. Through extensive experiments conducted on five different datasets for recognition tasks, we demonstrate that the proposed method achieves promising performance compared to the SOTA vector- and tensor-based canonical correlation analysis methods in terms of classification accuracy, model sparsity, and robustness to missing and noisy data. The code is publicly available at https://github.com/junfish/gSTCCA",
    "checked": true,
    "id": "c3435b396472eb8231e3dc153858e12b1db47ed6",
    "semantic_title": "a multilinear least-squares formulation for sparse tensor canonical correlation analysis",
    "citation_count": 0,
    "authors": [
      "Jun Yu",
      "Zhaoming Kong",
      "Kun Chen",
      "Xin Zhang",
      "Yong Chen",
      "Lifang He"
    ]
  },
  "https://openreview.net/forum?id=xLg8ljlEba": {
    "title": "Generalizing Neural Additive Models via Statistical Multimodal Analysis",
    "volume": "main",
    "abstract": "Interpretable models are gaining increasing attention in the machine learning community, and significant progress is being made to develop simple, interpretable, yet powerful deep learning approaches. Generalized Additive Models (GAM) and Neural Additive Models (NAM) are prime examples. Despite these methods' great potential and popularity in critical applications, e.g., medical applications, they fail to generalize to distributions with more than one mode (multimodal\\footnote{In this paper, multimodal refers to the context of distributions, wherein a distribution possesses more than one mode.}). The main reason behind this limitation is that these \"all-fit-one\" models collapse multiple relationships by being forced to fit the data unimodally. We address this critical limitation by proposing interpretable multimodal network frameworks capable of learning a Mixture of Neural Additive Models (MNAM). The proposed MNAM learns relationships between input features and outputs in a multimodal fashion and assigns a probability to each mode. The proposed method shares similarities with Mixture Density Networks (MDN) while keeping the interpretability that characterizes GAM and NAM. We demonstrate how the proposed MNAM balances between rich representations and interpretability with numerous empirical observations and pedagogical studies. We present and discuss different training alternatives and provided extensive practical evaluation to assess the proposed framework. The code is available at \\href{https://github.com/youngkyungkim93/MNAM}{https://github.com/youngkyungkim93/MNAM}",
    "checked": false,
    "id": "2511437a4198b6e3c972382aff50b78a852438f0",
    "semantic_title": "generalizing neural additive models via statistical multi-modal analysis",
    "citation_count": 0,
    "authors": [
      "Young Kyung Kim",
      "Juan Matias Di Martino",
      "Guillermo Sapiro"
    ]
  },
  "https://openreview.net/forum?id=5G3PI1hEdw": {
    "title": "A Joint Study of Phrase Grounding and Task Performance in Vision and Language Models",
    "volume": "main",
    "abstract": "Key to tasks that require reasoning about natural language in visual contexts is grounding words and phrases to image regions. However, observing this grounding in contemporary models is complex, even if it is generally expected to take place if the task is addressed in a way that is conductive to generalization. We propose a framework to jointly study task performance and phrase grounding, and propose three benchmarks to study the relation between the two. Our results show that contemporary models demonstrate inconsistency between their ability to ground phrases and solve tasks. We show how this can be addressed through brute-force training on ground phrasing annotations, and analyze the dynamics it creates. Code and data are available at https://github.com/lil-lab/phrase_grounding",
    "checked": true,
    "id": "b38a634a2902a333cce1f97789df03ae3189ed76",
    "semantic_title": "a joint study of phrase grounding and task performance in vision and language models",
    "citation_count": 1,
    "authors": [
      "Noriyuki Kojima",
      "Hadar Averbuch-Elor",
      "Yoav Artzi"
    ]
  },
  "https://openreview.net/forum?id=RwmWODTNFE": {
    "title": "Size Lowerbounds for Deep Operator Networks",
    "volume": "main",
    "abstract": "Deep Operator Networks are an increasingly popular paradigm for solving regression in infinite dimensions and hence solve families of PDEs in one shot. In this work, we aim to establish a first-of-its-kind data-dependent lowerbound on the size of DeepONets required for them to be able to reduce empirical error on noisy data. In particular, we show that for low training errors to be obtained on $n$ data points it is necessary that the common output dimension of the branch and the trunk net be scaling as $\\Omega \\left ( \\sqrt[\\leftroot{-1}\\uproot{-1}4]{n} \\right )$. This inspires our experiments with DeepONets solving the advection-diffusion-reaction PDE, where we demonstrate the possibility that at a fixed model size, to leverage increase in this common output dimension and get monotonic lowering of training error, the size of the training data might necessarily need to scale at least quadratically with it",
    "checked": true,
    "id": "cf80dd08c9a3783c62585203a71f50e6e1a238f8",
    "semantic_title": "size lowerbounds for deep operator networks",
    "citation_count": 2,
    "authors": [
      "Anirbit Mukherjee",
      "Amartya Roy"
    ]
  },
  "https://openreview.net/forum?id=0T2OTVCCC1": {
    "title": "Extending Path-Dependent NJ-ODEs to Noisy Observations and a Dependent Observation Framework",
    "volume": "main",
    "abstract": "The \\emph{Path-Dependent Neural Jump Ordinary Differential Equation (PD-NJ-ODE)} is a model for predicting continuous-time stochastic processes with irregular and incomplete observations. In particular, the method learns optimal forecasts given irregularly sampled time series of incomplete past observations. So far the process itself and the coordinate-wise observation times were assumed to be independent and observations were assumed to be noiseless. In this work we discuss two extensions to lift these restrictions and provide theoretical guarantees as well as empirical examples for them. In particular, we can lift the assumption of independence by extending the theory to much more realistic settings of conditional independence without any need to change the algorithm. Moreover, we introduce a new loss function, which allows us to deal with noisy observations and explain why the previously used loss function did not lead to a consistent estimator",
    "checked": true,
    "id": "4c384b8a6b5ff1550926eeddb8f32f7f77771469",
    "semantic_title": "extending path-dependent nj-odes to noisy observations and a dependent observation framework",
    "citation_count": 1,
    "authors": [
      "William Andersson",
      "Jakob Heiss",
      "Florian Krach",
      "Josef Teichmann"
    ]
  },
  "https://openreview.net/forum?id=3s7ior0WZ5": {
    "title": "Blind Biological Sequence Denoising with Self-Supervised Set Learning",
    "volume": "main",
    "abstract": "Biological sequence analysis relies on the ability to denoise the imprecise output of sequencing platforms. We consider a common setting where a short sequence is read out repeatedly using a high-throughput long-read platform to generate multiple subreads, or noisy obser- vations of the same sequence. Denoising these subreads with alignment-based approaches often fails when too few subreads are available or error rates are too high. In this paper, we propose a novel method for blindly denoising sets of sequences without directly observing clean source sequence labels. Our method, Self-Supervised Set Learning (SSSL), gathers subreads together in an embedding space and estimates a single set embedding as the mid- point of the subreads in both the latent and sequence spaces. This set embedding represents the \"average\" of the subreads and can be decoded into a prediction of the clean sequence. In experiments on simulated long-read DNA data, SSSL methods denoise small reads of ≤ 6 subreads with 17% fewer errors and large reads of > 6 subreads with 8% fewer errors compared to the best baseline. On a real dataset of antibody sequences, SSSL improves over baselines on two self-supervised metrics, with a significant improvement on difficult small reads that comprise over 60% of the test set. By accurately denoising these reads, SSSL promises to better realize the potential of high-throughput DNA sequencing data for downstream scientific applications",
    "checked": true,
    "id": "512d4baaacb28578464db7697a49ee5dcd69f915",
    "semantic_title": "blind biological sequence denoising with self-supervised set learning",
    "citation_count": 0,
    "authors": [
      "Nathan Hoyen Ng",
      "Ji Won Park",
      "Jae Hyeon Lee",
      "Ryan Lewis Kelly",
      "Stephen Ra",
      "Kyunghyun Cho"
    ]
  },
  "https://openreview.net/forum?id=wyU3Q4gahM": {
    "title": "Unsupervised Discovery of Steerable Factors When Graph Deep Generative Models Are Entangled",
    "volume": "main",
    "abstract": "Deep generative models (DGMs) have been widely developed for graph data. However, much less investigation has been carried out on understanding the latent space of such pretrained graph DGMs. These understandings possess the potential to provide constructive guidelines for crucial tasks, such as graph controllable generation. Thus in this work, we are interested in studying this problem and propose GraphCG, a method for the unsupervised discovery of steerable factors in the latent space of pretrained graph DGMs. We first examine the representation space of three pretrained graph DGMs with six disentanglement metrics, and we observe that the pretrained representation space is entangled. Motivated by this observation, GraphCG learns the steerable factors via maximizing the mutual information between semantic-rich directions, where the controlled graph moving along the same direction will share the same steerable factors. We quantitatively verify that GraphCG outperforms four competitive baselines on two graph DGMs pretrained on two molecule datasets. Additionally, we qualitatively illustrate seven steerable factors learned by GraphCG on five pretrained DGMs over five graph datasets, including two for molecules and three for point clouds",
    "checked": true,
    "id": "7856380962bcdd4a98ca7c8381dc7c80f963623e",
    "semantic_title": "unsupervised discovery of steerable factors when graph deep generative models are entangled",
    "citation_count": 0,
    "authors": [
      "Shengchao Liu",
      "Chengpeng Wang",
      "Jiarui Lu",
      "Weili Nie",
      "Hanchen Wang",
      "Zhuoxinran Li",
      "Bolei Zhou",
      "Jian Tang"
    ]
  },
  "https://openreview.net/forum?id=n2YifD4Dxo": {
    "title": "Are you using test log-likelihood correctly?",
    "volume": "main",
    "abstract": "Test log-likelihood is commonly used to compare different models of the same data or different approximate inference algorithms for fitting the same probabilistic model. We present simple examples demonstrating how comparisons based on test log-likelihood can contradict comparisons according to other objectives. Specifically, our examples show that (i) approximate Bayesian inference algorithms that attain higher test log-likelihoods need not also yield more accurate posterior approximations and (ii) conclusions about forecast accuracy based on test log-likelihood comparisons may not agree with conclusions based on root mean squared error",
    "checked": true,
    "id": "b6e363f617667014aea2d609136816fad22affc7",
    "semantic_title": "are you using test log-likelihood correctly?",
    "citation_count": 2,
    "authors": [
      "Sameer Deshpande",
      "Soumya Ghosh",
      "Tin D. Nguyen",
      "Tamara Broderick"
    ]
  },
  "https://openreview.net/forum?id=M2m618iIPk": {
    "title": "Blockwise Self-Supervised Learning at Scale",
    "volume": "main",
    "abstract": "Current state-of-the-art deep networks are all powered by backpropagation. However, long backpropagation paths as found in end-to-end training are biologically implausible, as well as inefficient in terms of energy consumption. In this paper, we explore alternatives to full backpropagation in the form of blockwise learning rules, leveraging the latest developments in self-supervised learning. We show that a blockwise pretraining procedure consisting of training independently the 4 main blocks of layers of a ResNet-50 with Barlow Twins' loss function at each block performs almost as well as end-to-end backpropagation on ImageNet: a linear probe trained on top of our blockwise pretrained model obtains a top-1 classification accuracy of 70.48\\%, only 1.1\\% below the accuracy of an end-to-end pretrained network (71.57\\% accuracy). We perform extensive experiments to understand the impact of different components within our method and explore a variety of adaptations of self-supervised learning to the blockwise paradigm, building an exhaustive understanding of the critical avenues for scaling local learning rules to large networks, with implications ranging from hardware design to neuroscience",
    "checked": true,
    "id": "a09a197325be3fb2e865692b164e8827042201d1",
    "semantic_title": "blockwise self-supervised learning at scale",
    "citation_count": 9,
    "authors": [
      "Shoaib Siddiqui",
      "David Krueger",
      "Yann LeCun",
      "Stephane Deny"
    ]
  },
  "https://openreview.net/forum?id=zSeoG5dRHK": {
    "title": "Temporally Rich Deep Learning Models for Magnetoencephalography",
    "volume": "main",
    "abstract": "Deep learning has been used in a wide range of applications, but it has only very recently been applied to Magnetoencephalography (MEG). MEG is a neurophysiological technique used to investigate a variety of cognitive processes such as language and learning, and an emerging technology in the quest to identify neural correlates of cognitive impairments such as those occurring in dementia. Recent work has shown that it is possible to apply deep learning to MEG to categorise induced responses to stimuli across subjects. While novel in the application of deep learning, such work has generally used relatively simple neural network (NN) models compared to those being used in domains such as computer vision and natural language processing. In these other domains, there is a long history in developing complex NN models that combine spatial and temporal information. We propose more complex NN models that focus on modelling temporal relationships in the data, and apply them to the challenges of MEG data. We apply these models to an extended range of MEG-based tasks, and find that they substantially outperform existing work on a range of tasks, particularly but not exclusively temporally-oriented ones. We also show that an autoencoder-based preprocessing component that focuses on the temporal aspect of the data can improve the performance of existing models. Our source code is available at https://github.com/tim-chard/DeepLearningForMEG",
    "checked": false,
    "id": "56b36b5339450db4ab8f4dafeb54a0313690e40e",
    "semantic_title": "temporally rich deep learning models for magnetoen-cephalography",
    "citation_count": 0,
    "authors": [
      "Tim Chard",
      "Mark Dras",
      "Paul Sowman",
      "Steve Cassidy",
      "Jia Wu"
    ]
  },
  "https://openreview.net/forum?id=KhMLfEIoUm": {
    "title": "Disciplined Saddle Programming",
    "volume": "main",
    "abstract": "We consider convex-concave saddle point problems, and more generally convex optimization problems we refer to as saddle problems, which include the partial supremum or infimum of convex-concave saddle functions. Saddle problems arise in a wide range of applications, including game theory, machine learning, and finance. It is well known that a saddle problem can be reduced to a single convex optimization problem by dualizing either the convex (min) or concave (max) objectives, reducing a min-max problem into a min-min (or max-max) problem. Carrying out this conversion by hand can be tedious and error prone. In this paper we introduce disciplined saddle programming (DSP), a domain specific language (DSL) for specifying saddle problems, for which the dualizing trick can be automated. The language and methods are based on recent work by Juditsky and Nemirovski, who developed the idea of conic-representable saddle point programs, and showed how to carry out the required dualization automatically using conic duality. Juditsky and Nemirovski's conic representation of saddle problems extends Nesterov and Nemirovski's earlier development of conic representable convex problems; DSP can be thought of as extending disciplined convex programming (DCP) to saddle problems. Just as DCP makes it easy for users to formulate and solve complex convex problems, DSP allows users to easily formulate and solve saddle problems. Our method is implemented in an open-source package, also called DSP",
    "checked": true,
    "id": "3e544c28b964c434885dee829b7c83ab3814d0f0",
    "semantic_title": "disciplined saddle programming",
    "citation_count": 5,
    "authors": [
      "Philipp Schiele",
      "Eric Sager Luxenberg",
      "Stephen P. Boyd"
    ]
  },
  "https://openreview.net/forum?id=Sj7bFPeR6W": {
    "title": "Federated Sampling with Langevin Algorithm under Isoperimetry",
    "volume": "main",
    "abstract": "Federated learning uses a set of techniques to efficiently distribute the training of a machine learning algorithm across several devices, who own the training data. These techniques critically rely on reducing the communication cost---the main bottleneck---between the devices and a central server. Federated learning algorithms usually take an optimization approach: they are algorithms for minimizing the training loss subject to communication (and other) constraints. In this work, we instead take a Bayesian approach for the training task, and propose a communication-efficient variant of the Langevin algorithm to sample \\textit{a posteriori}. The latter approach is more robust and provides more knowledge of the \\textit{a posteriori} distribution than its optimization counterpart. We analyze our algorithm without assuming that the target distribution is strongly log-concave. Instead, we assume the weaker log Sobolev inequality, which allows for nonconvexity",
    "checked": true,
    "id": "c76ac22ce1f27098be1ec07e50948c5ec465d259",
    "semantic_title": "federated sampling with langevin algorithm under isoperimetry",
    "citation_count": 0,
    "authors": [
      "Lukang Sun",
      "Adil Salim",
      "Peter Richtárik"
    ]
  },
  "https://openreview.net/forum?id=rQqzt4gYcc": {
    "title": "TensorVAE: a simple and efficient generative model for conditional molecular conformation generation",
    "volume": "main",
    "abstract": "Efficient generation of 3D conformations of a molecule from its 2D graph is a key challenge in in-silico drug discovery. Deep learning (DL) based generative modelling has recently become a potent tool to tackling this challenge. However, many existing DL-based methods are either indirect–leveraging inter-atomic distances or direct–but requiring numerous sampling steps to generate conformations. In this work, we propose a simple model abbreviated TensorVAE capable of generating conformations directly from a 2D molecular graph in a single step. The main novelty of the proposed method is focused on feature engineering. We develop a novel encoding and feature extraction mechanism relying solely on standard convolution operation to generate token-like feature vector for each atom. These feature vectors are then transformed through standard transformer encoders under a conditional Variational Autoencoder framework for generating conformations directly. We show through experiments on two benchmark datasets that with intuitive feature engineering, a relatively simple and standard model can provide promising generative capability outperforming more than a dozen state-of-the-art models employing more sophisticated and specialized generative architecture",
    "checked": true,
    "id": "3e16332485dba71d2ee6f0a2b2f1d76225601cb4",
    "semantic_title": "tensorvae: a simple and efficient generative model for conditional molecular conformation generation",
    "citation_count": 0,
    "authors": [
      "Hongyang Yu",
      "Hongjiang Yu"
    ]
  },
  "https://openreview.net/forum?id=qyfz0QrkqP": {
    "title": "PixMIM: Rethinking Pixel Reconstruction in Masked Image Modeling",
    "volume": "main",
    "abstract": "Masked Image Modeling (MIM) has achieved promising progress with the advent of Masked Autoencoders (MAE) and BEiT. However, subsequent works have complicated the framework with new auxiliary tasks or extra pre-trained models, inevitably increasing computational overhead. This paper undertakes a fundamental analysis of MIM from the perspective of pixel reconstruction, which examines the input image patches and reconstruction target, and highlights two critical but previously overlooked bottlenecks. Based on this analysis, we propose a remarkably simple and effective method, PixMIM, that entails two strategies: 1) filtering the high-frequency components from the reconstruction target to de-emphasize the network's focus on texture-rich details and 2) adopting a conservative data transform strategy to alleviate the problem of missing foreground in MIM training. PixMIM can be easily integrated into most existing pixel-based MIM approaches (i.e., using raw images as reconstruction target) with negligible additional computation. Without bells and whistles, our method consistently improves four MIM approaches, MAE, MFF, ConvMAE, and LSMAE, across various downstream tasks. We believe this effective plug-and-play method will serve as a strong baseline for self-supervised learning and provide insights for future improvements of the MIM framework. Code and models will be available",
    "checked": true,
    "id": "0a84577447a81d34aa9363a6790a65eb600f8384",
    "semantic_title": "pixmim: rethinking pixel reconstruction in masked image modeling",
    "citation_count": 11,
    "authors": [
      "Yuan Liu",
      "Songyang Zhang",
      "Jiacheng Chen",
      "Kai Chen",
      "Dahua Lin"
    ]
  },
  "https://openreview.net/forum?id=eN9CjU3h1b": {
    "title": "MMD-Regularized Unbalanced Optimal Transport",
    "volume": "main",
    "abstract": "We study the unbalanced optimal transport (UOT) problem, where the marginal constraints are enforced using Maximum Mean Discrepancy (MMD) regularization. Our work is motivated by the observation that the literature on UOT is focused on regularization based on $\\phi$-divergence (e.g., KL divergence). Despite the popularity of MMD, its role as a regularizer in the context of UOT seems less understood. We begin by deriving a specific dual of MMD-regularized UOT (MMD-UOT), which helps us prove several useful properties. One interesting outcome of this duality result is that MMD-UOT induces novel metrics, which not only lift the ground metric like the Wasserstein but are also sample-wise efficient to estimate like the MMD. Further, for real-world applications involving non-discrete measures, we present an estimator for the transport plan that is supported only on the given ($m$) samples. Under certain conditions, we prove that the estimation error with this finitely-supported transport plan is also $\\mathcal{O}(1/\\sqrt{m})$. As far as we know, such error bounds that are free from the curse of dimensionality are not known for $\\phi$-divergence regularized UOT. Finally, we discuss how the proposed estimator can be computed efficiently using accelerated gradient descent. Our experiments show that MMD-UOT consistently outperforms popular baselines, including KL-regularized UOT and MMD, in diverse machine learning applications",
    "checked": false,
    "id": "4a3d67894e699f7dcba0b54a5b0684702126e8bf",
    "semantic_title": "unbalanced low-rank optimal transport solvers",
    "citation_count": 1,
    "authors": [
      "Piyushi Manupriya",
      "SakethaNath Jagarlapudi",
      "Pratik Jawanpuria"
    ]
  },
  "https://openreview.net/forum?id=oGIR0ic3jU": {
    "title": "Bandits Corrupted by Nature: Lower Bounds on Regret and Robust Optimistic Algorithms",
    "volume": "main",
    "abstract": "We study the corrupted bandit problem, i.e. a stochastic multi-armed bandit problem with $k$ unknown reward distributions, which are heavy-tailed and corrupted by a history-independent adversary or Nature. To be specific, the reward obtained by playing an arm comes from corresponding heavy-tailed reward distribution with probability $1-\\varepsilon \\in (0.5,1]$ and an arbitrary corruption distribution of unbounded support with probability $\\varepsilon \\in [0,0.5)$. First, we provide \\textit{a problem-dependent lower bound on the regret} of any corrupted bandit algorithm. The lower bounds indicate that the corrupted bandit problem is harder than the classical stochastic bandit problem with subGaussian or heavy-tail rewards. Following that, we propose a novel UCB-type algorithm for corrupted bandits, namely \\texttt{HubUCB}, that builds on Huber's estimator for robust mean estimation. Leveraging a novel concentration inequality of Huber's estimator, we prove that \\texttt{HubUCB} achieves a near-optimal regret upper bound. Since computing Huber's estimator has quadratic complexity, we further introduce a sequential version of Huber's estimator that exhibits linear complexity. We leverage this sequential estimator to design \\texttt{SeqHubUCB} that enjoys similar regret guarantees while reducing the computational burden. Finally, we experimentally illustrate the efficiency of \\texttt{HubUCB} and \\texttt{SeqHubUCB} in solving corrupted bandits for different reward distributions and different levels of corruptions",
    "checked": false,
    "id": "71be523a35f799fb7d5bf9278f386d6cdb65b34d",
    "semantic_title": "bandits corrupted by nature: lower bounds on regret and robust optimistic algorithm",
    "citation_count": 4,
    "authors": [
      "Timothée Mathieu",
      "Debabrota Basu",
      "Odalric-Ambrym Maillard"
    ]
  },
  "https://openreview.net/forum?id=eTgxr7gPuU": {
    "title": "High-dimensional Bayesian Optimization via Covariance Matrix Adaptation Strategy",
    "volume": "main",
    "abstract": "Bayesian Optimization (BO) is an effective method for finding the global optimum of expensive black-box functions. However, it is well known that applying BO to high-dimensional optimization problems is challenging. To address this issue, a promising solution is to use a local search strategy that partitions the search domain into local regions with high likelihood of containing the global optimum, and then use BO to optimize the objective function within these regions. In this paper, we propose a novel technique for defining the local regions using the Covariance Matrix Adaptation (CMA) strategy. Specifically, we use CMA to learn a search distribution that can estimate the probabilities of data points being the global optimum of the objective function. Based on this search distribution, we then define the local regions consisting of data points with high probabilities of being the global optimum. Our approach serves as a meta-algorithm as it can incorporate existing black-box BO optimizers, such as BO, TuRBO, and BAxUS, to find the global optimum of the objective function within our derived local regions. We evaluate our proposed method on various benchmark synthetic and real-world problems. The results demonstrate that our method outperforms existing state-of-the-art techniques",
    "checked": true,
    "id": "7c1ee8b8c4065b0ae6a81946696b6bc72e1514d7",
    "semantic_title": "high-dimensional bayesian optimization via covariance matrix adaptation strategy",
    "citation_count": 0,
    "authors": [
      "Lam Ngo",
      "Huong Ha",
      "Jeffrey Chan",
      "Vu Nguyen",
      "Hongyu Zhang"
    ]
  },
  "https://openreview.net/forum?id=cvOpIhQQMN": {
    "title": "A general framework for formulating structured variable selection",
    "volume": "main",
    "abstract": "In variable selection, a selection rule that prescribes the permissible sets of selected variables (called a \"selection dictionary\") is desirable due to the inherent structural constraints among the candidate variables. Such selection rules can be complex in real-world data analyses, and failing to incorporate such restrictions could not only compromise the interpretability of the model but also lead to decreased prediction accuracy. However, no general framework has been proposed to formalize selection rules and their applications, which poses a significant challenge for practitioners seeking to integrate these rules into their analyses. In this work, we establish a framework for structured variable selection that can incorporate universal structural constraints. We develop a mathematical language for constructing arbitrary selection rules, where the selection dictionary is formally defined. We demonstrate that all selection rules can be expressed as combinations of operations on constructs, facilitating the identification of the corresponding selection dictionary. We use a detailed and complex example to illustrate the developed framework. Once this selection dictionary is derived, practitioners can apply their own user-defined criteria to select the optimal model. Additionally, our framework enhances existing penalized regression methods for variable selection by providing guidance on how to appropriately group variables to achieve the desired selection rule. Furthermore, our innovative framework opens the door to establishing new $\\ell_0$-based penalized regression techniques that can be tailored to respect arbitrary selection rules, thereby expanding the possibilities for more robust and tailored model development",
    "checked": false,
    "id": "300a0c3e03c00714f292a29708ac5a9e46956b25",
    "semantic_title": "a machine learning framework for neighbor generation in metaheuristic search",
    "citation_count": 1,
    "authors": [
      "GUANBO WANG",
      "Mireille Schnitzer",
      "Tom Chen",
      "Rui Wang",
      "Robert W Platt"
    ]
  },
  "https://openreview.net/forum?id=gllUnpYuXg": {
    "title": "Towards fully covariant machine learning",
    "volume": "main",
    "abstract": "Any representation of data involves arbitrary investigator choices. Because those choices are external to the data-generating process, each choice leads to an exact symmetry, corresponding to the group of transformations that takes one possible representation to another. These are the passive symmetries; they include coordinate freedom, gauge symmetry, and units covariance, all of which have led to important results in physics. In machine learning, the most visible passive symmetry is the relabeling or permutation symmetry of graphs. The active symmetries are those that must be established by observation and experiment. They include, for instance, translations invariances or rotation invariances of physical law. These symmetries are the subject of most of the equivariant machine learning literature. Our goal, in this conceptual contribution, is to understand the implications for machine learning of the many passive and active symmetries in play. We discuss dos and don'ts for machine learning practice if passive symmetries are to be respected. We discuss links to causal modeling and argue that the implementation of passive symmetries is particularly valuable when the goal of the learning problem is to generalize out of sample. We conjecture that the implementation of passive symmetries might help machine learning in the same ways that it transformed physics in the twentieth century",
    "checked": true,
    "id": "f6fe532c25cd480a66f097ac1f1f9e53a7a55d26",
    "semantic_title": "towards fully covariant machine learning",
    "citation_count": 1,
    "authors": [
      "Soledad Villar",
      "David W Hogg",
      "Weichi Yao",
      "George A Kevrekidis",
      "Bernhard Schölkopf"
    ]
  },
  "https://openreview.net/forum?id=VDy6LgErFM": {
    "title": "Empowering GNNs via Edge-Aware Weisfeiler-Leman Algorithm",
    "volume": "main",
    "abstract": "Message passing graph neural networks (GNNs) are known to have their expressiveness upper-bounded by 1-dimensional Weisfeiler-Leman (1-WL) algorithm. To achieve more powerful GNNs, existing attempts either require \\emph{ad hoc} features, or involve operations that incur high time and space complexities. In this work, we propose a \\textit{general} and \\textit{provably powerful} GNN framework that preserves the \\textit{scalability} of the message passing scheme. In particular, we first propose to empower 1-WL for graph isomorphism test by considering edges among neighbors, giving rise to NC-1-WL. The expressiveness of NC-1-WL is shown to be strictly above 1-WL and below 3-WL theoretically. Further, we propose the NC-GNN framework as a differentiable neural version of NC-1-WL. Our simple implementation of NC-GNN is provably as powerful as NC-1-WL. Experiments demonstrate that our NC-GNN performs effectively and efficiently on various benchmarks",
    "checked": true,
    "id": "5036b6ddd2e19e601cc391b516daab0ba7e63761",
    "semantic_title": "empowering gnns via edge-aware weisfeiler-leman algorithm",
    "citation_count": 0,
    "authors": [
      "Meng Liu",
      "Haiyang Yu",
      "Shuiwang Ji"
    ]
  },
  "https://openreview.net/forum?id=BNP4MxzDEI": {
    "title": "To Transfer or Not to Transfer: Suppressing Concepts from Source Representations",
    "volume": "main",
    "abstract": "With the proliferation of large pre-trained models in various domains, transfer learning has gained prominence where intermediate representations from these models can be leveraged to train better (target) task-specific models, with possibly limited labeled data. Although transfer learning can be beneficial in many applications, it can transfer undesirable information to target tasks that may severely curtail its performance in the target domain or raise ethical concerns related to privacy and/or fairness. In this paper, we propose a novel approach for suppressing the transfer of user-determined semantic concepts (viz. color, glasses, etc.) in intermediate source representations to target tasks without retraining the source model which can otherwise be expensive or even infeasible. Notably, we tackle a bigger challenge in the input data as a given intermediate source representation is biased towards the source task, thus possibly further entangling the desired concepts. We evaluate our approach qualitatively and quantitatively in the visual domain showcasing its efficacy for classification and generative source models. Finally, we provide a concept selection approach that automatically suppresses the undesirable concepts",
    "checked": true,
    "id": "6206021e2c5838be293f31f40dde408433420e0c",
    "semantic_title": "to transfer or not to transfer: suppressing concepts from source representations",
    "citation_count": 0,
    "authors": [
      "Vijay Sadashivaiah",
      "Keerthiram Murugesan",
      "Ronny Luss",
      "Pin-Yu Chen",
      "Chris Sims",
      "James Hendler",
      "Amit Dhurandhar"
    ]
  },
  "https://openreview.net/forum?id=DPvwr4HJdt": {
    "title": "On the Choice of Learning Rate for Local SGD",
    "volume": "main",
    "abstract": "Distributed data-parallel optimization accelerates the training of neural networks, but requires constant synchronization of gradients between the workers, which can become a bottleneck. One way to reduce communication overhead is to use Local SGD, where each worker asynchronously takes multiple local gradient steps, after which the model weights are averaged. In this work, we discuss the choice of learning rate for Local SGD, showing that it faces an intricate trade-off. Unlike in the synchronous case, its gradient estimate is biased, with the bias dependent on the learning rate itself. Thus using learning rate scaling techniques designed for faster convergence in the synchronous case with Local SGD results in a performance degradation as previously observed. To analyze the manifestation of this bias, we study convergence behaviour of Local SGD and synchronous data-parallel SGD when using their optimal learning rates. Our experiments show that the optimal learning rate for Local SGD differs substantially from that of SGD, and when using it the performance of Local SGD matches that of SGD. However, this performance comes at the cost of added training iterations, rendering Local SGD faster than SGD only when communication is much more time-consuming than computation. This suggests that Local SGD may be of limited practical utility",
    "checked": true,
    "id": "0b15ec181efd73af6d73160c6ccddce306dfd7fa",
    "semantic_title": "on the choice of learning rate for local sgd",
    "citation_count": 0,
    "authors": [
      "Lukas Balles",
      "Prabhu Teja S",
      "Cedric Archambeau"
    ]
  },
  "https://openreview.net/forum?id=bfsNmgN5je": {
    "title": "Semantic similarity prediction is better than other semantic similarity measures",
    "volume": "main",
    "abstract": "Semantic similarity between natural language texts is typically measured either by looking at the overlap between subsequences (e.g., BLEU) or by using embeddings (e.g., BERTScore, S-BERT). Within this paper, we argue that when we are only interested in measuring the semantic similarity, it is better to directly predict the similarity using a fine-tuned model for such a task. Using a fine-tuned model for the Semantic Textual Similarity Benchmark tasks (STS-B) from the GLUE benchmark, we define the STSScore approach and show that the resulting similarity is better aligned with our expectations on a robust semantic similarity measure than other approaches",
    "checked": true,
    "id": "96bfa287ace006d6b9fd9a73a84571e6c8cb7908",
    "semantic_title": "semantic similarity prediction is better than other semantic similarity measures",
    "citation_count": 1,
    "authors": [
      "Steffen Herbold"
    ]
  },
  "https://openreview.net/forum?id=R7H43YD6Lo": {
    "title": "Prismer: A Vision-Language Model with Multi-Task Experts",
    "volume": "main",
    "abstract": "Recent vision-language models have shown impressive multi-modal generation capabilities. However, typically they require training huge models on massive datasets. As a more scalable alternative, we introduce Prismer, a data- and parameter-efficient vision-language model that leverages an ensemble of task-specific experts. Prismer only requires training of a small number of components, with the majority of network weights inherited from multiple readily-available, pre-trained experts, and kept frozen during training. By leveraging experts from a wide range of domains, we show Prismer can efficiently pool this expert knowledge and adapt it to various vision-language reasoning tasks. In our experiments, we show that Prismer achieves fine-tuned and few-shot learning performance which is competitive with current state-of-the-arts, whilst requiring up to two orders of magnitude less training data. Code is available at https://github.com/NVlabs/prismer",
    "checked": true,
    "id": "f02d56e630986997e0aea3d92bf53e0f363ce401",
    "semantic_title": "prismer: a vision-language model with multi-task experts",
    "citation_count": 17,
    "authors": [
      "Shikun Liu",
      "Linxi Fan",
      "Edward Johns",
      "Zhiding Yu",
      "Chaowei Xiao",
      "Anima Anandkumar"
    ]
  },
  "https://openreview.net/forum?id=4i1MXH8Sle": {
    "title": "CAREER: A Foundation Model for Labor Sequence Data",
    "volume": "main",
    "abstract": "Labor economists regularly analyze employment data by fitting predictive models to small, carefully constructed longitudinal survey datasets. Although machine learning methods offer promise for such problems, these survey datasets are too small to take advantage of them. In recent years large datasets of online resumes have also become available, providing data about the career trajectories of millions of individuals. However, standard econometric models cannot take advantage of their scale or incorporate them into the analysis of survey data. To this end we develop CAREER, a foundation model for job sequences. CAREER is first fit to large, passively-collected resume data and then fine-tuned to smaller, better-curated datasets for economic inferences. We fit CAREER to a dataset of 24 million job sequences from resumes, and adjust it on small longitudinal survey datasets. We find that CAREER forms accurate predictions of job sequences, outperforming econometric baselines on three widely-used economics datasets. We further find that CAREER can be used to form good predictions of other downstream variables. For example, incorporating CAREER into a wage model provides better predictions than the econometric models currently in use",
    "checked": true,
    "id": "6325d9a0bb710687e12241a47d4df0eadc6a6d9d",
    "semantic_title": "career: a foundation model for labor sequence data",
    "citation_count": 0,
    "authors": [
      "Keyon Vafa",
      "Emil Palikot",
      "Tianyu Du",
      "Ayush Kanodia",
      "Susan Athey",
      "David Blei"
    ]
  },
  "https://openreview.net/forum?id=z3ZlnaOM0d": {
    "title": "Hyperspherical Prototype Node Clustering",
    "volume": "main",
    "abstract": "The general workflow of deep node clustering is to encode the nodes into node embeddings via graph neural networks and uncover clustering decisions from them, so clustering performance is heavily affected by the embeddings. However, existing works only consider preserving the semantics of the graph but ignore the inter-cluster separability of the nodes, so there's no guarantee that the embeddings can present a clear clustering structure. To remedy this deficiency, we propose Hyperspherical Prototype Node Clustering (HPNC), an end-to-end clustering paradigm that explicitly enhances the inter-cluster separability of learned node embeddings. Concretely, we constrain the embedding space to a unit-hypersphere, enabling us to scatter the cluster prototypes over the space with maximized pairwise distances. Then, we employ a graph autoencoder to map nodes onto the same hypersphere manifold. Consequently, cluster affinities can be directly retrieved from cosine similarities between node embeddings and prototypes. A clustering-oriented loss is imposed to sharpen the affinity distribution so that the learned node embeddings are encouraged to have small intra-cluster distances and large inter-cluster distances. Based on the proposed HPNC paradigm, we devise two schemes (HPNC-IM and HPNC-DEC) with distinct clustering backbones. Empirical results on popular benchmark datasets demonstrate the superiority of our method compared to other state-of-the-art clustering methods, and visualization results illustrate improved separability of the learned embeddings",
    "checked": true,
    "id": "b6dccff36f94daca36c7455517eff1b1fc317f84",
    "semantic_title": "hyperspherical prototype node clustering",
    "citation_count": 0,
    "authors": [
      "Jitao Lu",
      "Danyang Wu",
      "Feiping Nie",
      "Rong Wang",
      "Xuelong Li"
    ]
  },
  "https://openreview.net/forum?id=rFecyFpFUp": {
    "title": "AdaFed: Fair Federated Learning via Adaptive Common Descent Direction",
    "volume": "main",
    "abstract": "Federated learning (FL) is a promising technology via which some edge devices/clients collaboratively train a machine learning model orchestrated by a server. Learning an unfair model is known as a critical problem in federated learning, where the trained model may unfairly advantage or disadvantage some of the devices. To tackle this problem, in this work, we propose AdaFed. The goal of AdaFed is to find an updating direction for the server along which (i) all the clients' loss functions are decreasing; and (ii) more importantly, the loss functions for the clients with larger values decrease with a higher rate. AdaFed adaptively tunes this common direction based on the values of local gradients and loss functions. We validate the effectiveness of AdaFed on a suite of federated datasets, and demonstrate that AdaFed outperforms state-of-the-art fair FL methods",
    "checked": true,
    "id": "f87c32ce9af0da40a49382810f9f79e679f17b2f",
    "semantic_title": "adafed: fair federated learning via adaptive common descent direction",
    "citation_count": 3,
    "authors": [
      "Shayan Mohajer Hamidi",
      "EN-HUI YANG"
    ]
  },
  "https://openreview.net/forum?id=nzG9KGssSe": {
    "title": "Variational autoencoder with weighted samples for high-dimensional non-parametric adaptive importance sampling",
    "volume": "main",
    "abstract": "Adaptive importance sampling is a well-known family of algorithms for density approximation, generation and Monte Carlo integration including rare event estimation. The main common denominator of this family of algorithms is to perform density estimation with weighted samples at each iteration. However, the classical existing methods to do so, such as kernel smoothing or approximation by a Gaussian distribution, suffer from the curse of dimensionality and/or a lack of flexibility. Both are limitations in high dimension and when we do not have any prior knowledge on the form of the target distribution, such as its number of modes. Variational autoencoders are probabilistic tools able to represent with fidelity high-dimensional data in a lower dimensional space. They constitute a parametric family of distributions robust faced to the dimension and since they are based on deep neural networks, they are flexible enough to be considered as non-parametric models. In this paper, we propose to use a variational autoencoder as the auxiliary importance sampling distribution by extending the existing framework to weighted samples. We integrate the proposed procedure in existing adaptive importance sampling algorithms and we illustrate its practical interest on diverse examples",
    "checked": true,
    "id": "e1a08b6a72895499c5609cee6de40ea4bf9cf281",
    "semantic_title": "variational autoencoder with weighted samples for high-dimensional non-parametric adaptive importance sampling",
    "citation_count": 1,
    "authors": [
      "Julien Demange-Chryst",
      "Francois Bachoc",
      "Jérôme Morio",
      "Timothé Krauth"
    ]
  },
  "https://openreview.net/forum?id=ALRWXT1RLZ": {
    "title": "Separability Analysis for Causal Discovery in Mixture of DAGs",
    "volume": "main",
    "abstract": "Directed acyclic graphs (DAGs) are effective for compactly representing causal systems and specifying the causal relationships among the system's constituents. Specifying such causal relationships in some systems requires a mixture of multiple DAGs -- a single DAG is insufficient. Some examples include time-varying causal systems or aggregated subgroups of a population. Recovering the causal structure of the systems represented by single DAGs is investigated extensively, but it remains mainly open for the systems represented by a mixture of DAGs. A major difference between single- versus mixture-DAG recovery is the existence of node pairs that are separable in the individual DAGs but become inseparable in their mixture. This paper provides the theoretical foundations for analyzing such inseparable node pairs. Specifically, the notion of \\emph{emergent edges} is introduced to represent such inseparable pairs that do not exist in the single DAGs but emerge in their mixtures. Necessary conditions for identifying the emergent edges are established. Operationally, these conditions serve as sufficient conditions for separating a pair of nodes in the mixture of DAGs. These results are further extended, and matching necessary and sufficient conditions for identifying the emergent edges in tree-structured DAGs are established. Finally, a novel graphical representation is formalized to specify these conditions, and an algorithm is provided for inferring the learnable causal relations",
    "checked": true,
    "id": "61523d0574d1cb6e8148edfc9c436a5bdc526ba7",
    "semantic_title": "separability analysis for causal discovery in mixture of dags",
    "citation_count": 0,
    "authors": [
      "Burak Varici",
      "Dmitriy Katz",
      "Dennis Wei",
      "Prasanna Sattigeri",
      "Ali Tajer"
    ]
  },
  "https://openreview.net/forum?id=0CM7Hfsy61": {
    "title": "Unleashing the Potential of Acquisition Functions in High-Dimensional Bayesian Optimization",
    "volume": "main",
    "abstract": "Bayesian optimization (BO) is widely used to optimize expensive-to-evaluate black-box functions. It first builds a surrogate for the objective and quantifies its uncertainty. It then decides where to sample by maximizing an acquisition function (AF) defined by the surrogate model. However, when dealing with high-dimensional problems, finding the global maximum of the AF becomes increasingly challenging. In such cases, the manner in which the AF maximizer is initialized plays a pivotal role. An inappropriate initialization can severely limit the potential of AF. This paper investigates a largely understudied problem concerning the impact of AF maximizer initialization on exploiting AFs' capability. Our large-scale empirical study shows that the widely used random initialization strategy may fail to harness the potential of an AF. Based on this observation, we propose a better initialization approach by employing multiple heuristic optimizers to leverage the historical data of black-box optimization to generate initial points for an AF maximizer. We evaluate our approach with a variety of heavily studied synthetic test functions and real-world applications. Experimental results show that our techniques, while simple, can significantly enhance the standard BO and outperform state-of-the-art methods by a large margin in most test cases",
    "checked": true,
    "id": "d6600d43daaaeaad75e343a1a4bc8de16e9b0a79",
    "semantic_title": "unleashing the potential of acquisition functions in high-dimensional bayesian optimization",
    "citation_count": 1,
    "authors": [
      "Jiayu Zhao",
      "Renyu Yang",
      "SHENGHAO QIU",
      "Zheng Wang"
    ]
  },
  "https://openreview.net/forum?id=6SofFlwhEv": {
    "title": "On the Adversarial Robustness of Camera-based 3D Object Detection",
    "volume": "main",
    "abstract": "In recent years, camera-based 3D object detection has gained widespread attention for its ability to achieve high performance with low computational cost. However, the robustness of these methods to adversarial attacks has not been thoroughly examined, especially when considering their deployment in safety-critical domains like autonomous driving. In this study, we conduct the first comprehensive investigation of the robustness of leading camera-based 3D object detection approaches under various adversarial conditions. We systematically analyze the resilience of these models under two attack settings: white-box and black-box; focusing on two primary objectives: classification and localization. Additionally, we delve into two types of adversarial attack techniques: pixel-based and patch-based. Our experiments yield four interesting findings: (a) bird's-eye-view-based representations exhibit stronger robustness against localization attacks; (b) depth-estimation-free approaches have the potential to show stronger robustness; (c) accurate depth estimation effectively improves robustness for depth-estimation-based methods; (d) incorporating multi-frame benign inputs can effectively mitigate adversarial attacks. We hope our findings can steer the development of future camera-based object detection models with enhanced adversarial robustness. The code is available at: https://github.com/Daniel-xsy/BEV-Attack",
    "checked": true,
    "id": "c60c9928b4c1dfd259d190a24c338eadb8b4b5fd",
    "semantic_title": "on the adversarial robustness of camera-based 3d object detection",
    "citation_count": 10,
    "authors": [
      "Shaoyuan Xie",
      "Zichao Li",
      "Zeyu Wang",
      "Cihang Xie"
    ]
  },
  "https://openreview.net/forum?id=txpYITR8oa": {
    "title": "AmbientFlow: Invertible generative models from incomplete, noisy measurements",
    "volume": "main",
    "abstract": "Generative models have gained popularity for their potential applications in imaging science, such as image reconstruction, posterior sampling and data sharing. Flow-based generative models are particularly attractive due to their ability to tractably provide exact density estimates along with fast, inexpensive and diverse samples. Training such models, however, requires a large, high quality dataset of objects. In applications such as computed imaging, it is often difficult to acquire such data due to requirements such as long acquisition time or high radiation dose, while acquiring noisy or partially observed measurements of these objects is more feasible. In this work, we propose AmbientFlow, a framework for learning flow-based generative models directly from noisy and incomplete data. Using variational Bayesian methods, a novel framework for establishing flow-based generative models from noisy, incomplete data is proposed. Extensive numerical studies demonstrate the effectiveness of AmbientFlow in learning the object distribution. The utility of AmbientFlow in a downstream inference task of image reconstruction is demonstrated",
    "checked": true,
    "id": "1356a03fcbcbfa9b5a60ccdfbd6310dcbe3ac4c1",
    "semantic_title": "ambientflow: invertible generative models from incomplete, noisy measurements",
    "citation_count": 2,
    "authors": [
      "Varun A. Kelkar",
      "Rucha Deshpande",
      "Arindam Banerjee",
      "Mark Anastasio"
    ]
  },
  "https://openreview.net/forum?id=fovUNTilp9": {
    "title": "Multi-Horizon Representations with Hierarchical Forward Models for Reinforcement Learning",
    "volume": "main",
    "abstract": "Learning control from pixels is difficult for reinforcement learning (RL) agents because representation learning and policy learning are intertwined. Previous approaches remedy this issue with auxiliary representation learning tasks, but they either do not consider the temporal aspect of the problem or only consider single-step transitions, which may cause learning inefficiencies if important environmental changes take many steps to manifest. We propose Hierarchical $k$-Step Latent (HKSL), an auxiliary task that learns multiple representations via a hierarchy of forward models that learn to communicate and an ensemble of $n$-step critics that all operate at varying magnitudes of step skipping. We evaluate HKSL in a suite of 30 robotic control tasks with and without distractors and a task of our creation. We find that HKSL either converges to higher or optimal episodic returns more quickly than several alternative representation learning approaches. Furthermore, we find that HKSL's representations capture task-relevant details accurately across timescales (even in the presence of distractors) and that communication channels between hierarchy levels organize information based on both sides of the communication process, both of which improve sample efficiency",
    "checked": true,
    "id": "50dc64ab218420b121da5c956371b2d35d1a2ae2",
    "semantic_title": "multi-horizon representations with hierarchical forward models for reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Trevor McInroe",
      "Lukas Schäfer",
      "Stefano V Albrecht"
    ]
  },
  "https://openreview.net/forum?id=aYkYajcJDN": {
    "title": "Neural Task Synthesis for Visual Programming",
    "volume": "main",
    "abstract": "Generative neural models hold great promise in enhancing programming education by synthesizing new content. We seek to design neural models that can automatically generate programming tasks for a given specification in the context of visual programming domains. Despite the recent successes of large generative models like GPT-4, our initial results show that these models are ineffective in synthesizing visual programming tasks and struggle with logical and spatial reasoning. We propose a novel neuro-symbolic technique, NeurTaskSyn, that can synthesize programming tasks for a specification given in the form of desired programming concepts exercised by its solution code and constraints on the visual task. NeurTaskSyn has two components: the first component is trained via imitation learning procedure to generate possible solution codes, and the second component is trained via reinforcement learning procedure to guide an underlying symbolic execution engine that generates visual tasks for these codes. We demonstrate the effectiveness of NeurTaskSyn through an extensive empirical evaluation and a qualitative study on reference tasks taken from the Hour of Code: Classic Maze challenge by Code.org and the Intro to Programming with Karel course by CodeHS.com",
    "checked": true,
    "id": "59a4a5db0a913b99b7afe1c6b2bf6e24f0d31857",
    "semantic_title": "neural task synthesis for visual programming",
    "citation_count": 9,
    "authors": [
      "Victor-Alexandru Pădurean",
      "Georgios Tzannetos",
      "Adish Singla"
    ]
  },
  "https://openreview.net/forum?id=umggDfMHha": {
    "title": "Federated Classification in Hyperbolic Spaces via Secure Aggregation of Convex Hulls",
    "volume": "main",
    "abstract": "Hierarchical and tree-like data sets arise in many relevant applications, including language processing, graph data mining, phylogeny and genomics. It is known that tree-like data cannot be embedded into Euclidean spaces of finite dimension with small distortion, and that this problem can be mitigated through the use of hyperbolic spaces. When such data also has to be processed in a distributed and privatized setting, it becomes necessary to work with new federated learning methods tailored to hyperbolic spaces. As an initial step towards the development of the field of federated learning in hyperbolic spaces, we propose the first known approach to federated classification in hyperbolic spaces. Our contributions are as follows. First, we develop distributed versions of convex SVM classifiers for Poincar\\'e discs. In this setting, the information conveyed from clients to the global classifier are convex hulls of clusters present in individual client data. Second, to avoid label switching issues, we introduce a number-theoretic approach for label recovery based on the so-called integer $B_h$ sequences. Third, we compute the complexity of the convex hulls in hyperbolic spaces to assess the extent of data leakage; at the same time, in order to limit the communication cost for the hulls, we propose a new quantization method for the Poincar\\'e disc coupled with Reed-Solomon-like encoding. Fourth, at the server level, we introduce a new approach for aggregating convex hulls of the clients based on balanced graph partitioning. We test our method on a collection of diverse data sets, including hierarchical single-cell RNA-seq data from different patients distributed across different repositories that have stringent privacy constraints. The classification accuracy of our method is up to $\\sim11\\%$ better than its Euclidean counterpart, demonstrating the importance of privacy-preserving learning in hyperbolic spaces. Our implementation for the proposed method is available at \\url{https://github.com/sauravpr/hyperbolic_federated_classification}",
    "checked": true,
    "id": "460a12b01476b681528bb693498157795bc9c929",
    "semantic_title": "federated classification in hyperbolic spaces via secure aggregation of convex hulls",
    "citation_count": 0,
    "authors": [
      "Saurav Prakash",
      "Jin Sima",
      "Chao Pan",
      "Eli Chien",
      "Olgica Milenkovic"
    ]
  },
  "https://openreview.net/forum?id=8sg2I9zXgO": {
    "title": "Personalized Algorithmic Recourse with Preference Elicitation",
    "volume": "main",
    "abstract": "Algorithmic Recourse (AR) is the problem of computing a sequence of actions that -- once performed by a user -- overturns an undesirable machine decision. It is paramount that the sequence of actions does not require too much effort for users to implement. Yet, most approaches to AR assume that actions cost the same for all users, and thus may recommend unfairly expensive recourse plans to certain users. Prompted by this observation, we introduce PEAR, the first human-in-the-loop approach capable of providing personalized algorithmic recourse tailored to the needs of any end-user. PEAR builds on insights from Bayesian Preference Elicitation to iteratively refine an estimate of the costs of actions by asking choice set queries to the target user. The queries themselves are computed by maximizing the Expected Utility of Selection, a principled measure of information gain accounting for uncertainty on both the cost estimate and the user's responses. PEAR integrates elicitation into a Reinforcement Learning agent coupled with Monte Carlo Tree Search to quickly identify promising recourse plans. Our empirical evaluation on real-world datasets highlights how PEAR produces high-quality personalized recourse in only a handful of iterations",
    "checked": true,
    "id": "2ae21771e4d66a4835e02bdca0836a4ec5d8e7d4",
    "semantic_title": "personalized algorithmic recourse with preference elicitation",
    "citation_count": 3,
    "authors": [
      "Giovanni De Toni",
      "Paolo Viappiani",
      "Stefano Teso",
      "Bruno Lepri",
      "Andrea Passerini"
    ]
  },
  "https://openreview.net/forum?id=MppUW90uU2": {
    "title": "A Fully Decentralized Surrogate for Multi-Agent Policy Optimization",
    "volume": "main",
    "abstract": "The study of fully decentralized learning or independent learning in cooperative multi-agent reinforcement learning has a history of decades. Recent empirical studies have shown that independent PPO (IPPO) can achieve good performance, comparable to or even better than the methods of centralized training with decentralized execution, in several benchmarks. However, a decentralized actor-critic algorithm with a convergence guarantee is still an open problem. In this paper, we propose decentralized policy optimization (DPO), a decentralized actor-critic algorithm with monotonic improvement and convergence guarantee. We derive a novel decentralized surrogate for policy optimization such that the monotonic improvement of joint policy can be guaranteed by each agent independently optimizing the surrogate. For practical implementation, this decentralized surrogate can be realized by two adaptive coefficients for policy optimization at each agent. Empirically, we evaluate DPO, IPPO, and independent Q-learning (IQL) in a variety of cooperative multi-agent tasks, covering discrete and continuous action spaces, as well as fully and partially observable environments. The results show DPO outperforms both IPPO and IQL in most tasks, which serves as evidence for our theoretical results. The code is available at https://github.com/PKU-RL/DPO",
    "checked": true,
    "id": "7e7fc9a94dbced6707b3679d2f7665c5fa346347",
    "semantic_title": "a fully decentralized surrogate for multi-agent policy optimization",
    "citation_count": 0,
    "authors": [
      "Kefan Su",
      "Zongqing Lu"
    ]
  },
  "https://openreview.net/forum?id=EDqCY6ihbr": {
    "title": "A Globally Convergent Algorithm for Neural Network Parameter Optimization Based on Difference-of-Convex Functions",
    "volume": "main",
    "abstract": "We propose an algorithm for optimizing the parameters of single hidden layer neural networks. Specifically, we derive a blockwise difference-of-convex (DC) functions representation of the objective function. Based on the latter, we propose a block coordinate descent (BCD) approach that we combine with a tailored difference-of-convex functions algorithm (DCA). We prove global convergence of the proposed algorithm. Furthermore, we mathematically analyze the convergence rate of parameters and the convergence rate in value (i.e., the training loss). We give conditions under which our algorithm converges linearly or even faster depending on the local shape of the loss function. We confirm our theoretical derivations numerically and compare our algorithm against state-of-the-art gradient-based solvers in terms of both training loss and test loss",
    "checked": true,
    "id": "484108d2f939be95808d2f5c8a38f1d057b16f4e",
    "semantic_title": "a globally convergent algorithm for neural network parameter optimization based on difference-of-convex functions",
    "citation_count": 0,
    "authors": [
      "Daniel Tschernutter",
      "Mathias Kraus",
      "Stefan Feuerriegel"
    ]
  },
  "https://openreview.net/forum?id=pfbVayaUMc": {
    "title": "Online Reference Tracking For Linear Systems with Unknown Dynamics and Unknown Disturbances",
    "volume": "main",
    "abstract": "This paper presents an online learning mechanism to address the challenge of state tracking for unknown linear systems under general adversarial disturbances. The reference trajectory is assumed to be generated by unknown exosystem dynamics, which relaxes the common assumption of known dynamics for exosystems. Learning a tracking control policy for unknown systems with unknown exosystem dynamics under general disturbances is challenging and surprisingly unsettled. To face this challenge, the presented online learning algorithm has two stages: In the first stage, an algorithm identifies the dynamics of the uncertain system, and in the second stage, an online parametrized memory-augmented controller accounts for the identification error, unknown exosystem dynamics as well as disturbances. The controller's parameters are learned to optimize a general convex cost function, and learning the control parameters is formulated as an online convex optimization problem. This approach uses the memory of previous disturbances and reference values to capture their effects on performance over time. Besides, it implicitly learns the dynamics of the exosystems. The algorithm enables online tuning of controller parameters to achieve state tracking and disturbance rejection while minimizing general convex costs. It is shown that the algorithm achieves a policy regret of $\\mathcal{O}({T}^{\\frac{2}{3}})$. In the simulation results, the performance of the presented tracking algorithm was compared with the certainty equivalent $H_{\\infty}$-control and linear quadratic regulator",
    "checked": false,
    "id": "9a58ccd4f6933f73b43aa2a4c6e4780dc66f97cb",
    "semantic_title": "online reference tracking for linear systems with un-known dynamics and unknown disturbances",
    "citation_count": 1,
    "authors": [
      "Nariman Niknejad",
      "Farnaz Adib Yaghmaie",
      "Hamidreza Modares"
    ]
  },
  "https://openreview.net/forum?id=a68SUt6zFt": {
    "title": "DINOv2: Learning Robust Visual Features without Supervision",
    "volume": "main",
    "abstract": "The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP on most of the benchmarks at image and pixel levels",
    "checked": true,
    "id": "5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891",
    "semantic_title": "dinov2: learning robust visual features without supervision",
    "citation_count": 834,
    "authors": [
      "Maxime Oquab",
      "Timothée Darcet",
      "Théo Moutakanni",
      "Huy V. Vo",
      "Marc Szafraniec",
      "Vasil Khalidov",
      "Pierre Fernandez",
      "Daniel HAZIZA",
      "Francisco Massa",
      "Alaaeldin El-Nouby",
      "Mido Assran",
      "Nicolas Ballas",
      "Wojciech Galuba",
      "Russell Howes",
      "Po-Yao Huang",
      "Shang-Wen Li",
      "Ishan Misra",
      "Michael Rabbat",
      "Vasu Sharma",
      "Gabriel Synnaeve",
      "Hu Xu",
      "Herve Jegou",
      "Julien Mairal",
      "Patrick Labatut",
      "Armand Joulin",
      "Piotr Bojanowski"
    ]
  },
  "https://openreview.net/forum?id=NYdThkjNW1": {
    "title": "Boomerang: Local sampling on image manifolds using diffusion models",
    "volume": "main",
    "abstract": "The inference stage of diffusion models can be seen as running a reverse-time diffusion stochastic differential equation, where samples from a Gaussian latent distribution are transformed into samples from a target distribution that usually reside on a low-dimensional manifold, e.g., an image manifold. The intermediate values between the initial latent space and the image manifold can be interpreted as noisy images, with the amount of noise determined by the forward diffusion process noise schedule. We utilize this interpretation to present Boomerang, an approach for local sampling of image manifolds exploiting the reverse diffusion process dynamics. As implied by its name, Boomerang local sampling involves adding noise to an input image, moving it closer to the latent space, and then mapping it back to the image manifold through a partial reverse diffusion process. Thus, Boomerang generates images on the manifold that are ``similar,'' but nonidentical, to the original input image. We can control the proximity of the generated images to the original by adjusting the amount of noise added. Furthermore, due to the stochastic nature of the partial reverse diffusion process in Boomerang, the generated images display a certain degree of stochasticity, allowing us to obtain ample local samples from the manifold without encountering any duplicates. Boomerang offers the flexibility to work seamlessly with any pretrained diffusion model, such as Stable Diffusion, without necessitating any adjustments to the reverse diffusion process. We present three applications for local sampling using Boomerang. First, we provide a framework for constructing privacy-preserving datasets having controllable degrees of anonymity. Second, we show that using Boomerang for data augmentation increases generalization performance and outperforms state-of-the-art synthetic data augmentation. Lastly, we introduce a perceptual image enhancement framework powered by Boomerang, which enables resolution enhancement",
    "checked": true,
    "id": "e607c4516c7f2dda67b326ddda8c5afd1b2ff2a9",
    "semantic_title": "boomerang: local sampling on image manifolds using diffusion models",
    "citation_count": 12,
    "authors": [
      "Lorenzo Luzi",
      "Paul M Mayer",
      "Josue Casco-Rodriguez",
      "Ali Siahkoohi",
      "Richard Baraniuk"
    ]
  },
  "https://openreview.net/forum?id=KcmWZSk53y": {
    "title": "Improved Regret Bounds for Linear Adversarial MDPs via Linear Optimization",
    "volume": "main",
    "abstract": "Learning Markov decision processes (MDP) in an adversarial environment has been a challenging problem. The problem becomes even more challenging with function approximation since the underlying structure of the loss function and transition kernel are especially hard to estimate in a varying environment. In fact, the state-of-the-art results for linear adversarial MDP achieve a regret of $\\tilde{\\mathcal{O}}({K^{6/7}})$ ($K$ denotes the number of episodes), which admits a large room for improvement. In this paper, we propose a novel explore-exploit algorithm framework and investigate the problem with a new view, which reduces linear MDP into linear optimization by subtly setting the feature maps of the bandit arms of linear optimization. This new technique, under an exploratory assumption, yields an improved bound of $\\tilde{\\mathcal{O}}({K^{4/5}})$ for linear adversarial MDP without access to a transition simulator. The new view could be of independent interest for solving other MDP problems that possess a linear structure",
    "checked": true,
    "id": "64c44e4d7f405ae8df334980642fbb21942a889f",
    "semantic_title": "improved regret bounds for linear adversarial mdps via linear optimization",
    "citation_count": 9,
    "authors": [
      "Fang Kong",
      "XiangCheng Zhang",
      "Baoxiang Wang",
      "Shuai Li"
    ]
  },
  "https://openreview.net/forum?id=RyZB4qXEgt": {
    "title": "Neural Circuit Diagrams: Robust Diagrams for the Communication, Implementation, and Analysis of Deep Learning Architectures",
    "volume": "main",
    "abstract": "Diagrams matter. Unfortunately, the deep learning community has no standard method for diagramming architectures. The current combination of linear algebra notation and ad-hoc diagrams fails to offer the necessary precision to understand architectures in all their detail. However, this detail is critical for faithful implementation, mathematical analysis, further innovation, and ethical assurances. I present neural circuit diagrams, a graphical language tailored to the needs of communicating deep learning architectures. Neural circuit diagrams naturally keep track of the changing arrangement of data, precisely show how operations are broadcast over axes, and display the critical parallel behavior of linear operations. A lingering issue with existing diagramming methods is the inability to simultaneously express the detail of axes and the free arrangement of data, which neural circuit diagrams solve. Their compositional structure is analogous to code, creating a close correspondence between diagrams and implementation. In this work, I introduce neural circuit diagrams for an audience of machine learning researchers. After introducing neural circuit diagrams, I cover a host of architectures to show their utility and breed familiarity. This includes the transformer architecture, convolution (and its difficult-to-explain extensions), residual networks, the U-Net, and the vision transformer. I include a Jupyter notebook that provides evidence for the close correspondence between diagrams and code. Finally, I examine backpropagation using neural circuit diagrams. I show their utility in providing mathematical insight and analyzing algorithms' time and space complexities",
    "checked": true,
    "id": "3be78b45d1cc14a496a7eafdc0e5cfec34cbb030",
    "semantic_title": "neural circuit diagrams: robust diagrams for the communication, implementation, and analysis of deep learning architectures",
    "citation_count": 2,
    "authors": [
      "Vincent Abbott"
    ]
  },
  "https://openreview.net/forum?id=YRKS2J0x36": {
    "title": "DyG2Vec: Efficient Representation Learning for Dynamic Graphs",
    "volume": "main",
    "abstract": "Temporal graph neural networks have shown promising results in learning inductive representations by automatically extracting temporal patterns. However, previous works often rely on complex memory modules or inefficient random walk methods to construct temporal representations. To address these limitations, we present an efficient yet effective attention-based encoder that leverages temporal edge encodings and window-based subgraph sampling to generate task-agnostic embeddings. Moreover, we propose a joint-embedding architecture using non-contrastive SSL to learn rich temporal embeddings without labels. Experimental results on 7 benchmark datasets indicate that on average, our model outperforms SoTA baselines on the future link prediction task by 4.23% for the transductive setting and 3.30% for the inductive setting while only requiring 5-10x less training/inference time. Lastly, different aspects of the proposed framework are investigated through experimental analysis and ablation studies. The code is publicly available at https://github.com/huawei-noah/noah-research/tree/master/graph_atlas",
    "checked": true,
    "id": "07fdc2b9c97b371b83a7596d488b0735c78400a0",
    "semantic_title": "dyg2vec: efficient representation learning for dynamic graphs",
    "citation_count": 0,
    "authors": [
      "Mohammad Alomrani",
      "Mahdi Biparva",
      "Yingxue Zhang",
      "Mark Coates"
    ]
  },
  "https://openreview.net/forum?id=nYjSkOy8ij": {
    "title": "A Survey on Out-of-Distribution Detection in NLP",
    "volume": "main",
    "abstract": "Out-of-distribution (OOD) detection is essential for the reliable and safe deployment of machine learning systems in the real world. Great progress has been made over the past years. This paper presents the first review of recent advances in OOD detection with a particular focus on natural language processing approaches. First, we provide a formal definition of OOD detection and discuss several related fields. We then categorize recent algorithms into three classes according to the data they used: (1) OOD data available, (2) OOD data unavailable + in-distribution (ID) label available, and (3) OOD data unavailable + ID label unavailable. Third, we introduce datasets, applications, and metrics. Finally, we summarize existing work and present potential future research topics",
    "checked": true,
    "id": "dcfca93185c49811ec6cf7c995eea58cf88c7bb3",
    "semantic_title": "a survey on out-of-distribution detection in nlp",
    "citation_count": 11,
    "authors": [
      "Hao Lang",
      "Yinhe Zheng",
      "Yixuan Li",
      "Jian SUN",
      "Fei Huang",
      "Yongbin Li"
    ]
  },
  "https://openreview.net/forum?id=vyRBsqj5iG": {
    "title": "Proximal Mean Field Learning in Shallow Neural Networks",
    "volume": "main",
    "abstract": "We propose a custom learning algorithm for shallow over-parameterized neural networks, i.e., networks with single hidden layer having infinite width. The infinite width of the hidden layer serves as an abstraction for the over-parameterization. Building on the recent mean field interpretations of learning dynamics in shallow neural networks, we realize mean field learning as a computational algorithm, rather than as an analytical tool. Specifically, we design a Sinkhorn regularized proximal algorithm to approximate the distributional flow for the learning dynamics over weighted point clouds. In this setting, a contractive fixed point recursion computes the time-varying weights, numerically realizing the interacting Wasserstein gradient flow of the parameter distribution supported over the neuronal ensemble. An appealing aspect of the proposed algorithm is that the measure-valued recursions allow meshless computation. We demonstrate the proposed computational framework of interacting weighted particle evolution on binary and multi-class classification. Our algorithm performs gradient descent of the free energy associated with the risk functional",
    "checked": true,
    "id": "be4c0c6d3590401ea41a0eca481beb9ea20b22c1",
    "semantic_title": "proximal mean field learning in shallow neural networks",
    "citation_count": 0,
    "authors": [
      "Alexis Teter",
      "Iman Nodozi",
      "Abhishek Halder"
    ]
  },
  "https://openreview.net/forum?id=42BKnT2qW3": {
    "title": "Synaptic Interaction Penalty: Appropriate Penalty Term for Energy-Efficient Spiking Neural Networks",
    "volume": "main",
    "abstract": "Spiking neural networks (SNNs) are energy-efficient neural networks because of their spiking nature. However, as the spike firing rate of SNNs increases, the energy consumption does as well, and thus, the advantage of SNNs diminishes. Here, we tackle this problem by introducing a novel penalty term for the spiking activity into the objective function in the training phase. Our method is designed so as to optimize the energy consumption metric directly without modifying the network architecture. Therefore, the proposed method can reduce the energy consumption more than other methods while maintaining the accuracy. We conducted experiments for image classification tasks, and the results indicate the effectiveness of the proposed method, which mitigates the dilemma of the energy--accuracy trade-off",
    "checked": true,
    "id": "94a680ceb5f106b41b3d0f10369ae06c44bcf6be",
    "semantic_title": "synaptic interaction penalty: appropriate penalty term for energy-efficient spiking neural networks",
    "citation_count": 0,
    "authors": [
      "Kazuma Suetake",
      "Takuya Ushimaru",
      "Ryuji Saiin",
      "Yoshihide Sawada"
    ]
  },
  "https://openreview.net/forum?id=n8fZ6mY6PB": {
    "title": "Exploring Format Consistency for Instruction Tuning",
    "volume": "main",
    "abstract": "Instruction tuning has emerged as a promising approach to enhancing large language models in following human instructions. It is shown that increasing the diversity and number of instructions in the training data can consistently enhance generalization performance, which facilitates a recent endeavor to collect various instructions and integrate existing instruction tuning datasets into larger collections. However, different users have their unique ways of expressing instructions, and there often exist variations across different datasets in the instruction styles and formats, i.e., format inconsistency. In this work, a framework named Unified Instruction Tuning (UIT) is proposed, which calls OpenAI APIs for automatic format transfer among different instruction tuning datasets such as PromptSource, FLAN and CrossFit. With the framework, we (1) demonstrate the necessity of maintaining format consistency in instruction tuning; (2) improve the generalization performance on unseen instructions on T5-LM-xl; (3) provide a novel perplexity-based denoising method to reduce the noise of automatic format transfer to make the UIT framework more practical and a smaller offline model based on GPT-J that achieves comparable format transfer capability to OpenAI APIs to reduce costs in practice. Further analysis regarding variations of targeted formats and other effects is intended. The code and trained models will soon be available",
    "checked": true,
    "id": "46ac88bb0acbf736840ff8a392cec2bf43d917e1",
    "semantic_title": "exploring format consistency for instruction tuning",
    "citation_count": 7,
    "authors": [
      "Shihao Liang",
      "Runchu Tian",
      "Kunlun Zhu",
      "Yujia Qin",
      "Huadong Wang",
      "Xin Cong",
      "Zhiyuan Liu",
      "Xiaojiang Liu",
      "Maosong Sun"
    ]
  },
  "https://openreview.net/forum?id=EWv9XGOpB3": {
    "title": "Variational Classification: A Probabilistic Generalization of the Softmax Classifier",
    "volume": "main",
    "abstract": "We present a latent variable model for classification that provides a novel probabilistic interpretation of neural network softmax classifiers. We derive a variational objective to train the model, analogous to the evidence lower bound (ELBO) used to train variational auto-encoders, that generalises the cross-entropy loss used to train classification models. Treating inputs to the softmax layer as samples of a latent variable, our abstracted perspective reveals a potential inconsistency between their anticipated distribution, required for accurate label predictions to be output, and the empirical distribution found in practice. We augment the variational objective to mitigate such inconsistency and encourage a chosen latent distribution, instead of the implicit assumption in off-the-shelf softmax classifiers. Overall, we provide new theoretical insight into the inner workings of widely-used softmax classification. Empirical evaluation on image and text classification datasets demonstrates that our proposed approach, variational classification, maintains classification accuracy while the reshaped latent space improves other desirable properties of a classifier, such as calibration, adversarial robustness, robustness to distribution shift and sample efficiency useful in low data settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shehzaad Zuzar Dhuliawala",
      "Mrinmaya Sachan",
      "Carl Allen"
    ]
  },
  "https://openreview.net/forum?id=mqoxLkX210": {
    "title": "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emre Kiciman",
      "Robert Ness",
      "Amit Sharma",
      "Chenhao Tan"
    ]
  },
  "https://openreview.net/forum?id=aloEru2qCG": {
    "title": "LoRA Learns Less and Forgets Less",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Biderman",
      "Jacob Portes",
      "Jose Javier Gonzalez Ortiz",
      "Mansheej Paul",
      "Philip Greengard",
      "Connor Jennings",
      "Daniel King",
      "Sam Havens",
      "Vitaliy Chiley",
      "Jonathan Frankle",
      "Cody Blakeney",
      "John Patrick Cunningham"
    ]
  },
  "https://openreview.net/forum?id=QaCCuDfBk2": {
    "title": "Revisiting Feature Prediction for Learning Visual Representations from Video",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrien Bardes",
      "Quentin Garrido",
      "Jean Ponce",
      "Xinlei Chen",
      "Michael Rabbat",
      "Yann LeCun",
      "Mido Assran",
      "Nicolas Ballas"
    ]
  },
  "https://openreview.net/forum?id=cT8oOJ6Q6F": {
    "title": "Grid Cell-Inspired Fragmentation and Recall for Efficient Map Building",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaedong Hwang",
      "Zhang-Wei Hong",
      "Eric R Chen",
      "Akhilan Boopathy",
      "Pulkit Agrawal",
      "Ila R Fiete"
    ]
  },
  "https://openreview.net/forum?id=kfhoeZCeW7": {
    "title": "Fine-tuning can cripple your foundation model; preserving features may be the solution",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jishnu Mukhoti",
      "Yarin Gal",
      "Philip Torr",
      "Puneet K. Dokania"
    ]
  },
  "https://openreview.net/forum?id=lQBsLfAWhj": {
    "title": "Layerwise complexity-matched learning yields an improved model of cortical area V2",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikhil Parthasarathy",
      "Olivier J Henaff",
      "Eero P Simoncelli"
    ]
  },
  "https://openreview.net/forum?id=10YJTIsVYq": {
    "title": "Gradient Scarcity in Graph Learning with Bilevel Optimization",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hashem Ghanem",
      "Samuel Vaiter",
      "Nicolas Keriven"
    ]
  },
  "https://openreview.net/forum?id=1Yp6xpTV55": {
    "title": "Q-Learning for Stochastic Control under General Information Structures and Non-Markovian Environments",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Devran Kara",
      "Serdar Yuksel"
    ]
  },
  "https://openreview.net/forum?id=agT8ojoH0X": {
    "title": "Self-Improvement for Neural Combinatorial Optimization: Sample Without Replacement, but Improvement",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Pirnay",
      "Dominik G. Grimm"
    ]
  },
  "https://openreview.net/forum?id=CrpDwMFgxr": {
    "title": "Linear Bandits with Memory",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giulia Clerici",
      "Pierre Laforgue",
      "Nicolò Cesa-Bianchi"
    ]
  },
  "https://openreview.net/forum?id=wczqrpOrIc": {
    "title": "LeanVec: Searching vectors faster by making them fit",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mariano Tepper",
      "Ishwar Singh Bhati",
      "Cecilia Aguerrebere",
      "Mark Hildebrand",
      "Theodore L. Willke"
    ]
  },
  "https://openreview.net/forum?id=7I199lc54z": {
    "title": "Soft Merging of Experts with Adaptive Routing",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammed Muqeeth",
      "Haokun Liu",
      "Colin Raffel"
    ]
  },
  "https://openreview.net/forum?id=H7gLN5nqVF": {
    "title": "Deep Generalized Prediction Set Classifier and Its Theoretical Guarantees",
    "volume": "featured",
    "abstract": "A standard classification rule returns a single-valued prediction for any observation without a confidence guarantee, which may result in severe consequences in many critical applications when the uncertainty is high. In contrast, set-valued classification is a new paradigm to handle the uncertainty in classification by reporting a set of plausible labels to observations in highly ambiguous regions. In this article, we propose the Deep Generalized Prediction Set (DeepGPS) method, a network-based set-valued classifier induced by acceptance region learning. DeepGPS is capable of identifying ambiguous observations and detecting out-of-distribution (OOD) observations. It is the first set-valued classification of this kind with a theoretical guarantee and scalable to large datasets. Our nontrivial proof shows that the risk of DeepGPS, defined as the expected size of the prediction set, attains the optimality within a neural network hypothesis class while simultaneously achieving the user-prescribed class-specific accuracy. Additionally, by using a weighted loss, DeepGPS returns tighter acceptance regions, leading to informative predictions and improved OOD detection performance. Empirically, our method outperforms the baselines on several benchmark datasets",
    "checked": false,
    "id": "a3ebed9b1e75a528f8e943275bdb99ffe515ab2c",
    "semantic_title": "optimal mass transport meets thermodynamics: on power and efficiency of finite-time thermodynamic engines",
    "citation_count": 0,
    "authors": [
      "Zhou Wang",
      "Xingye Qiao"
    ]
  },
  "https://openreview.net/forum?id=xRy1YRcHWj": {
    "title": "As large as it gets – Studying Infinitely Large Convolutions via Neural Implicit Frequency Filters",
    "volume": "featured",
    "abstract": "Recent work in neural networks for image classification has seen a strong tendency towards increasing the spatial context during encoding. Whether achieved through large convolution kernels or self-attention, models scale poorly with the increased spatial context, such that the improved model accuracy often comes at significant costs. In this paper, we propose a module for studying the effective filter size of convolutional neural networks (CNNs). To facilitate such a study, several challenges need to be addressed: (i) we need an effective means to train models with large filters (potentially as large as the input data) without increasing the number of learnable parameters, (ii) the employed convolution operation should be a plug-and-play module that can replace conventional convolutions in a CNN and allow for an efficient implementation in current frameworks, (iii) the study of filter sizes has to be decoupled from other aspects such as the network width or the number of learnable parameters, and (iv) the cost of the convolution operation itself has to remain manageable i.e.~we can not naïvely increase the size of the convolution kernel. To address these challenges, we propose to learn the frequency representations of filter weights as neural implicit functions, such that the better scalability of the convolution in the frequency domain can be leveraged. Additionally, due to the implementation of the proposed neural implicit function, even large and expressive spatial filters can be parameterized by only a few learnable weights. Interestingly, our analysis shows that, although the proposed networks could learn very large convolution kernels, the learned filters are well localized and relatively small in practice when transformed from the frequency to the spatial domain. We anticipate that our analysis of individually optimized filter sizes will allow for more efficient, yet effective, models in the future. Our code is available at https://github.com/GeJulia/NIFF",
    "checked": false,
    "id": "78fbdad00b2864e8dfd37abfae6e2cd3d811d050",
    "semantic_title": "as large as it gets: learning infinitely large filters via neural implicit functions in the fourier domain",
    "citation_count": 0,
    "authors": [
      "Julia Grabinski",
      "Janis Keuper",
      "Margret Keuper"
    ]
  },
  "https://openreview.net/forum?id=iulMde3dP1": {
    "title": "What Has Been Overlooked in Contrastive Source-Free Domain Adaptation: Leveraging Source-Informed Latent Augmentation within Neighborhood Context",
    "volume": "featured",
    "abstract": "Source-free domain adaptation (SFDA) involves adapting a model originally trained using a labeled dataset (source domain) to perform effectively on an unlabeled dataset (target domain) without relying on any source data during adaptation. This adaptation is especially crucial when significant disparities in data distributions exist between the two domains and when there are privacy concerns regarding the source model's training data. The absence of access to source data during adaptation makes it challenging to analytically estimate the domain gap. To tackle this issue, various techniques have been proposed, such as unsupervised clustering, contrastive learning, and continual learning. In this paper, we first conduct an extensive theoretical analysis of SFDA based on contrastive learning, primarily because it has demonstrated superior performance compared to other techniques. Motivated by the obtained insights, we then introduce a straightforward yet highly effective latent augmentation method tailored for contrastive SFDA. This augmentation method leverages the dispersion of latent features within the neighborhood of the query sample, guided by the source pre-trained model, to enhance the informativeness of positive keys. Our approach, based on a single InfoNCE-based contrastive loss, outperforms state-of-the-art SFDA methods on widely recognized benchmark datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Wang",
      "Wonho Bae",
      "Jiahong Chen",
      "Kuangen Zhang",
      "Leonid Sigal",
      "Clarence W. de Silva"
    ]
  },
  "https://openreview.net/forum?id=Gh0cxhbz3c": {
    "title": "On the Convergence of Adaptive Gradient Methods for Nonconvex Optimization",
    "volume": "featured",
    "abstract": "Adaptive gradient methods are workhorses in deep learning. However, the convergence guarantees of adaptive gradient methods for nonconvex optimization have not been thoroughly studied. In this paper, we provide a fine-grained convergence analysis for a general class of adaptive gradient methods including AMSGrad, RMSProp and AdaGrad. For smooth nonconvex functions, we prove that adaptive gradient methods in expectation converge to a first-order stationary point. Our convergence rate is better than existing results for adaptive gradient methods in terms of dimension. In addition, we also prove high probability bounds on the convergence rates of AMSGrad, RMSProp as well as AdaGrad, which have not been established before. Our analyses shed light on better understanding the mechanism behind adaptive gradient methods in optimizing nonconvex objectives",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongruo Zhou",
      "Jinghui Chen",
      "Yuan Cao",
      "Ziyan Yang",
      "Quanquan Gu"
    ]
  },
  "https://openreview.net/forum?id=hFALpTb4fR": {
    "title": "LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models",
    "volume": "featured",
    "abstract": "Recent advancements in text-to-image diffusion models have yielded impressive results in generating realistic and diverse images. However, these models still struggle with complex prompts, such as those that involve numeracy and spatial reasoning. This work proposes to enhance prompt understanding capabilities in diffusion models. Our method leverages a pretrained large language model (LLM) for grounded generation in a novel two-stage process. In the first stage, the LLM generates a scene layout that comprises captioned bounding boxes from a given prompt describing the desired image. In the second stage, a novel controller guides an off-the-shelf diffusion model for layout-grounded image generation. Both stages utilize existing pretrained models without additional model parameter optimization. Our method significantly outperforms the base diffusion model and several strong baselines in accurately generating images according to prompts that require various capabilities, doubling the generation accuracy across four tasks on average. Furthermore, our method enables instruction-based multi-round scene specification and can handle prompts in languages not supported by the underlying diffusion model. We anticipate that our method will unleash users' creativity by accurately following more complex prompts. Our code, demo, and benchmark are available at: https://llm-grounded-diffusion.github.io",
    "checked": true,
    "id": "e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0",
    "semantic_title": "llm-grounded diffusion: enhancing prompt understanding of text-to-image diffusion models with large language models",
    "citation_count": 56,
    "authors": [
      "Long Lian",
      "Boyi Li",
      "Adam Yala",
      "Trevor Darrell"
    ]
  },
  "https://openreview.net/forum?id=r9p9CV52MV": {
    "title": "ModuLoRA: Finetuning 2-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers",
    "volume": "featured",
    "abstract": "We propose a memory-efficient finetuning algorithm for large language models (LLMs) that supports finetuning LLMs with 65B parameters in 2/3/4-bit precision on as little as one 24GB GPU. Our method, modular low-rank adaptation (ModuLoRA), integrates any user-specified weight quantizer with finetuning via low-rank adapters (LoRAs). Our approach relies on a simple quantization-agnostic backward pass that adaptively materializes low-precision LLM weights from a custom black-box quantization module. This approach enables finetuning 2-bit and 3-bit LLMs for the first time---leveraging state-of-the-art 2-bit QuIP# quantization and 3-bit OPTQ quantization---outperforming finetuning that relies on less sophisticated 4-bit and 8-bit methods. In our experiments, ModuLoRA attains competitive performance on text classification, natural language infernece, and instruction following tasks using significantly less memory than existing approaches, and we also surpass the state-of-the-art ROUGE score on a popular summarization task. We release ModuLoRA together with a series of low-precision models as part of LLMTOOLS, a user-friendly library for quantizing, running, and finetuning LLMs on consumer GPUs",
    "checked": true,
    "id": "e3314e18032898b33a2e2b067a45ae4cb02e1a95",
    "semantic_title": "modulora: finetuning 2-bit llms on consumer gpus by integrating with modular quantizers",
    "citation_count": 1,
    "authors": [
      "Junjie Yin",
      "Jiahao Dong",
      "Yingheng Wang",
      "Christopher De Sa",
      "Volodymyr Kuleshov"
    ]
  },
  "https://openreview.net/forum?id=NmpjDHWIvg": {
    "title": "Wasserstein Distributionally Robust Policy Evaluation and Learning for Contextual Bandits",
    "volume": "featured",
    "abstract": "Off-policy evaluation and learning are concerned with assessing a given policy and learning an optimal policy from offline data without direct interaction with the environment. Often, the environment in which the data are collected differs from the environment in which the learned policy is applied. To account for the effect of different environments during learning and execution, distributionally robust optimization (DRO) methods have been developed that compute worst-case bounds on the policy values assuming that the distribution of the new environment lies within an uncertainty set. Typically, this uncertainty set is defined based on the KL divergence around the empirical distribution computed from the logging dataset. However, the KL uncertainty set fails to encompass distributions with varying support and lacks awareness of the geometry of the distribution support. As a result, KL approaches fall short in addressing practical environment mismatches and lead to over-fitting to worst-case scenarios. To overcome these limitations, we propose a novel DRO approach that employs the Wasserstein distance instead. While Wasserstein DRO is generally computationally more expensive compared to KL DRO, we present a regularized method and a practical (biased) stochastic gradient descent method to optimize the policy efficiently. We also provide a theoretical analysis of the finite sample complexity and iteration complexity for our proposed method. We further validate our approach using a public dataset that was recorded in a randomized stoke trial",
    "checked": true,
    "id": "96c77909fd7ee5bb27213ddf4309475873f8a958",
    "semantic_title": "wasserstein distributionally robust policy evaluation and learning for contextual bandits",
    "citation_count": 1,
    "authors": [
      "Yi Shen",
      "Pan Xu",
      "Michael Zavlanos"
    ]
  },
  "https://openreview.net/forum?id=TQfQUksaC8": {
    "title": "Pathologies of Predictive Diversity in Deep Ensembles",
    "volume": "featured",
    "abstract": "Classic results establish that encouraging predictive diversity improves performance in ensembles of low-capacity models, e.g. through bagging or boosting. Here we demonstrate that these intuitions do not apply to high-capacity neural network ensembles (deep ensembles), and in fact the opposite is often true. In a large scale study of nearly 600 neural network classification ensembles, we examine a variety of interventions that trade off component model performance for predictive diversity. While such interventions can improve the performance of small neural network ensembles (in line with standard intuitions), they harm the performance of the large neural network ensembles most often used in practice. Surprisingly, we also find that discouraging predictive diversity is often benign in large-network ensembles, fully inverting standard intuitions. Even when diversity-promoting interventions do not sacrifice component model performance (e.g. using heterogeneous architectures and training paradigms), we observe an opportunity cost associated with pursuing increased predictive diversity. Examining over 1000 ensembles, we observe that the performance benefits of diverse architectures/training procedures are easily dwarfed by the benefits of simply using higher-capacity models, despite the fact that such higher capacity models often yield significantly less predictive diversity. Overall, our findings demonstrate that standard intuitions around predictive diversity, originally developed for low-capacity ensembles, do not directly apply to modern high-capacity deep ensembles. This work clarifies fundamental challenges to the goal of improving deep ensembles by making them more diverse, while suggesting an alternative path: simply forming ensembles from ever more powerful (and less diverse) component models",
    "checked": true,
    "id": "20113225d61a160bdab4ad2228a2918e41232e29",
    "semantic_title": "pathologies of predictive diversity in deep ensembles",
    "citation_count": 4,
    "authors": [
      "Taiga Abe",
      "E. Kelly Buchanan",
      "Geoff Pleiss",
      "John Patrick Cunningham"
    ]
  },
  "https://openreview.net/forum?id=FDt2UGM1Nz": {
    "title": "DIG In: Evaluating Disparities in Image Generations with Indicators for Geographic Diversity",
    "volume": "featured",
    "abstract": "The unprecedented photorealistic results achieved by recent text-to-image generative systems and their increasing use as plug-and-play content creation solutions make it crucial to understand their potential biases. In this work, we introduce three indicators to evaluate the realism, diversity and prompt-generation consistency of text-to-image generative systems when prompted to generate objects from across the world. Our indicators complement qualitative analysis of the broader impact of such systems by enabling automatic and efficient benchmarking of geographic disparities, an important step towards building responsible visual content creation systems. We use our proposed indicators to analyze potential geographic biases in state-of-the-art visual content creation systems and find that: (1) models have less realism and diversity of generations when prompting for Africa and West Asia than Europe, (2) prompting with geographic information comes at a cost to prompt-consistency and diversity of generated images, and (3) models exhibit more region-level disparities for some objects than others. Perhaps most interestingly, our indicators suggest that progress in image generation quality has come at the cost of real-world geographic representation. Our comprehensive evaluation constitutes a crucial step towards ensuring a positive experience of visual content creation for everyone. Code is available at https://github.com/facebookresearch/DIG-In/",
    "checked": true,
    "id": "a00537a398ce1cacf3b6835a9817ade535d2dae2",
    "semantic_title": "dig in: evaluating disparities in image generations with indicators for geographic diversity",
    "citation_count": 2,
    "authors": [
      "Melissa Hall",
      "Candace Ross",
      "Adina Williams",
      "Nicolas Carion",
      "Michal Drozdzal",
      "Adriana Romero-Soriano"
    ]
  },
  "https://openreview.net/forum?id=DOWSP7y2cu": {
    "title": "Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kamalika Chaudhuri",
      "Chuan Guo",
      "Laurens van der Maaten",
      "Saeed Mahloujifar",
      "Mark Tygert"
    ]
  },
  "https://openreview.net/forum?id=y0b0H1ndGQ": {
    "title": "GCondNet: A Novel Method for Improving Neural Networks on Small High-Dimensional Tabular Data",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrei Margeloiu",
      "Nikola Simidjievski",
      "Pietro Lio",
      "Mateja Jamnik"
    ]
  },
  "https://openreview.net/forum?id=C6wj17VBnu": {
    "title": "Attacking Bayes: On the Adversarial Robustness of Bayesian Neural Networks",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunzhen Feng",
      "Tim G. J. Rudner",
      "Nikolaos Tsilivis",
      "Julia Kempe"
    ]
  },
  "https://openreview.net/forum?id=Yj8fUQGXXL": {
    "title": "Reproducibility Study: Equal Improvability: A New Fairness Notion Considering the Long-Term Impact",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Berkay Chakar",
      "Amina Izbassar",
      "Mina Janićijević",
      "Jakub Tomaszewski"
    ]
  },
  "https://openreview.net/forum?id=ydcrP55u2e": {
    "title": "Chain-of-Thought Unfaithfulness as Disguised Accuracy",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oliver Bentham",
      "Nathan Stringham",
      "Ana Marasovic"
    ]
  },
  "https://openreview.net/forum?id=srFEYJkqD7": {
    "title": "[Re] Classwise-Shapley values for data valuation",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Markus Semmler",
      "Miguel de Benito Delgado"
    ]
  },
  "https://openreview.net/forum?id=nPZgtpfgIx": {
    "title": "On the Reproducibility of: \"Learning Perturbations to Explain Time Series Predictions",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wouter Bant",
      "Ádám Divák",
      "Jasper Eppink",
      "Floris Six Dijkstra"
    ]
  },
  "https://openreview.net/forum?id=Xu1sEPhjqH": {
    "title": "Reproducibility study of \"Robust Fair Clustering: A Novel Fairness Attack and Defense Framework",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Ponticelli",
      "Vincent Loos",
      "Eren Kocadag",
      "Kacper Bartosik"
    ]
  },
  "https://openreview.net/forum?id=Mf1H8X5DVb": {
    "title": "Reproducibility study of \"LICO: Explainable Models with Language-Image Consistency",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luan Fletcher",
      "Robert van der Klis",
      "Martin Sedláček",
      "Stefan Vasilev",
      "Christos Athanasiadis"
    ]
  },
  "https://openreview.net/forum?id=QdeBbK5CSh": {
    "title": "Explaining RL Decisions with Trajectories': A Reproducibility Study",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karim Ahmed Abdel Sadek",
      "Matteo Nulli",
      "Joan Velja",
      "Jort Vincenti"
    ]
  },
  "https://openreview.net/forum?id=FEEKR0Vl9s": {
    "title": "Reproducibility Study on Adversarial Attacks Against Robust Transformer Trackers",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fatemeh Nourilenjan Nokabadi",
      "Jean-Francois Lalonde",
      "Christian Gagné"
    ]
  },
  "https://openreview.net/forum?id=BbvSU02jLg": {
    "title": "Transfer Learning with Informative Priors: Simple Baselines Better than Previously Reported",
    "volume": "reprod",
    "abstract": "We pursue transfer learning to improve classifier accuracy on a target task with few labeled examples available for training. Recent work suggests that using a source task to learn a prior distribution over neural net weights, not just an initialization, can boost target task performance. In this study, we carefully compare transfer learning with and without source task informed priors across 5 datasets. We find that standard transfer learning informed by an initialization only performs far better than reported in previous comparisons. The relative gains of methods using informative priors over standard transfer learning vary in magnitude across datasets. For the scenario of 5-300 examples per class, we find negative or negligible gains on 2 datasets, modest gains (between 1.5-3 points of accuracy) on 2 other datasets, and substantial gains (>8 points) on one dataset. Among methods using informative priors, we find that an isotropic covariance appears competitive with learned low-rank covariance matrix while being substantially simpler to understand and tune. Further analysis suggests that the mechanistic justification for informed priors -- hypothesized improved alignment between train and test loss landscapes -- is not consistently supported due to high variability in empirical landscapes. We release code to allow independent reproduction of all experiments",
    "checked": false,
    "id": "7553c009437b828ffb9063da8d4d281d4bb35674",
    "semantic_title": "an exploration of effective patient education with an emphasis on concussion",
    "citation_count": 0,
    "authors": [
      "Ethan Harvey",
      "Mikhail Petrov",
      "Michael C Hughes"
    ]
  },
  "https://openreview.net/forum?id=fCNqD2IuoD": {
    "title": "Reproducibility Study of \"Learning Perturbations to Explain Time Series Predictions",
    "volume": "reprod",
    "abstract": "In this work, we attempt to reproduce the results of Enguehard (2023), which introduced ExtremalMask, a mask-based perturbation method for explaining time series data. We investigated the key claims of this paper, namely that (1) the model outperformed other models in several key metrics on both synthetic and real data, and (2) the model performed better when using the loss function of the preservation game relative to that of the deletion game. Although discrepancies exist, our results generally support the core of the original paper's conclusions. Next, we interpret ExtremalMask's outputs using new visualizations and metrics and discuss the insights each interpretation provides. Finally, we test whether ExtremalMask create out of distribution samples, and found the model does not exhibit this flaw on our tested synthetic dataset. Overall, our results support and add nuance to the original paper's findings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiapeng Fan",
      "Luke Cadigan",
      "Paulius Skaisgiris",
      "Sebastian Uriel Arias"
    ]
  },
  "https://openreview.net/forum?id=VzKXbCzNoU": {
    "title": "Efficient Parallelized Simulation of Cyber-Physical Systems",
    "volume": "reprod",
    "abstract": "Advancements in accelerated physics simulations have greatly reduced training times for reinforcement learning policies, yet the conventional step-by-step agent-simulator interaction undermines simulation accuracy. In the real-world, interactions are asynchronous, with sensing, acting and processing happening simultaneously. Failing to capture this widens the sim2real gap and results in suboptimal real-world performance. In this paper, we address the challenges of simulating realistic asynchronicity and delays within parallelized simulations, crucial to bridging the sim2real gap in complex cyber-physical systems. Our approach efficiently parallelizes cyber-physical system simulations on accelerator hardware, including physics, sensors, actuators, processing components and their asynchronous interactions. We extend existing accelerated physics simulations with latency simulation capabilities by constructing a `supergraph' that encodes all data dependencies across parallelized simulation steps, ensuring accurate simulation. By finding the smallest supergraph, we minimize redundant computation. We validate our approach on two real-world systems and perform an extensive ablation, demonstrating superior performance compared to baseline methods",
    "checked": false,
    "id": "1980638d973ed7ab0684b148fbba8a13889bbb50",
    "semantic_title": "optimizing highly-parallel simulation-based verification of cyber-physical systems",
    "citation_count": 0,
    "authors": [
      "Bas van der Heijden",
      "Laura Ferranti",
      "Jens Kober",
      "Robert Babuska"
    ]
  },
  "https://openreview.net/forum?id=H1hLNjwrGy": {
    "title": "Reproducibility Study of \"Robust Fair Clustering: A Novel Fairness Attack and Defense Framework",
    "volume": "reprod",
    "abstract": "Clustering algorithms play a pivotal role in various societal applications, where fairness is paramount to prevent adverse impacts on individuals. In this study, we revisit the robustness of fair clustering algorithms against adversarial attacks, affirming previous research findings that highlighted their susceptibility and the resilience of the Consensus Fair Clustering (CFC) model. Beyond reproducing these critical results, our work extends the original analysis by refining the codebase for enhanced experimentation, introducing additional metrics and datasets to deepen the evaluation of fairness and clustering quality, and exploring novel attack strategies, including targeted attacks on new metrics and a combined approach for balance and entropy as well as an ablation study. These contributions validate the original claims about the vulnerability and resilience of fair clustering algorithms and broaden the research landscape by offering a more comprehensive toolkit for assessing adversarial robustness in fair clustering",
    "checked": false,
    "id": "6545eebb1162c70751119e173f8d51f5c112fcc1",
    "semantic_title": "robust fair clustering: a novel fairness attack and defense framework",
    "citation_count": 10,
    "authors": [
      "Iason Skylitsis",
      "Zheng Feng",
      "Idries Nasim",
      "Camille Niessink"
    ]
  },
  "https://openreview.net/forum?id=sHSKFYyINO": {
    "title": "InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers",
    "volume": "reprod",
    "abstract": "We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which—as we found out—produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact, on three out of five datasets, DeBERTA slightly outperformed monoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used by Bonifacio et al. (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available. https://github.com/searchivarius/inpars_light/",
    "checked": true,
    "id": "3a30217c4115777fb30c182c97cc77d34d065556",
    "semantic_title": "inpars-light: cost-effective unsupervised training of efficient rankers",
    "citation_count": 13,
    "authors": [
      "Leonid Boytsov",
      "Preksha Patel",
      "Vivek Sourabh",
      "Riddhi Nisar",
      "Sayani Kundu",
      "Ramya Ramanathan",
      "Eric Nyberg"
    ]
  },
  "https://openreview.net/forum?id=10R6iX6JHm": {
    "title": "We're Not Using Videos Effectively: An Updated Domain Adaptive Video Segmentation Baseline",
    "volume": "reprod",
    "abstract": "There has been abundant work in unsupervised domain adaptation for semantic segmentation (DAS) seeking to adapt a model trained on images from a labeled source domain to an unlabeled target domain. While the vast majority of prior work has studied this as a frame-level Image-DAS problem, a few Video-DAS works have sought to additionally leverage the temporal signal present in adjacent frames. However, Video-DAS works have historically studied a distinct set of benchmarks from Image-DAS, with minimal cross-benchmarking. In this work, we address this gap. Surprisingly, we find that (1) even after carefully controlling for data and model architecture, state-of-the-art Image-DAS methods (HRDA and HRDA+MIC)} outperform Video-DAS methods on established Video-DAS benchmarks (+14.5 mIoU on Viper$\\rightarrow$CityscapesSeq, +19.0 mIoU on Synthia$\\rightarrow$CityscapesSeq), and (2) naive combinations of Image-DAS and Video-DAS techniques only lead to marginal improvements across datasets. To avoid siloed progress between Image-DAS and Video-DAS, we open-source our codebase with support for a comprehensive set of Video-DAS and Image-DAS methods on a common benchmark. Code available at https://github.com/SimarKareer/UnifiedVideoDA",
    "checked": true,
    "id": "e96e71694d02a631e48f39874c42c6480ed7078e",
    "semantic_title": "we're not using videos effectively: an updated domain adaptive video segmentation baseline",
    "citation_count": 0,
    "authors": [
      "Simar Kareer",
      "Vivek Vijaykumar",
      "Harsh Maheshwari",
      "Judy Hoffman",
      "Prithvijit Chattopadhyay",
      "Viraj Uday Prabhu"
    ]
  },
  "https://openreview.net/forum?id=3Wg1oErMcJ": {
    "title": "Self-Supervised Visual Representation Learning for Medical Image Analysis: A Comprehensive Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siladittya Manna",
      "Saumik Bhattacharya",
      "Umapada Pal"
    ]
  },
  "https://openreview.net/forum?id=ul2tbUPtIQ": {
    "title": "Vision-Language Instruction Tuning: A Review and Analysis",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Li",
      "Yixiao Ge",
      "Dian Li",
      "Ying Shan"
    ]
  },
  "https://openreview.net/forum?id=XfHWcNTSHp": {
    "title": "A Survey on Data Selection for Language Models",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alon Albalak",
      "Yanai Elazar",
      "Sang Michael Xie",
      "Shayne Longpre",
      "Nathan Lambert",
      "Xinyi Wang",
      "Niklas Muennighoff",
      "Bairu Hou",
      "Liangming Pan",
      "Haewon Jeong",
      "Colin Raffel",
      "Shiyu Chang",
      "Tatsunori Hashimoto",
      "William Yang Wang"
    ]
  },
  "https://openreview.net/forum?id=bsCCJHbO8A": {
    "title": "Efficient Large Language Models: A Survey",
    "volume": "survey",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in important tasks such as natural language understanding and language generation, and thus have the potential to make a substantial impact on our society. Such capabilities, however, come with the considerable resources they demand, highlighting the strong need to develop effective techniques for addressing their efficiency challenges. In this survey, we provide a systematic and comprehensive review of efficient LLMs research. We organize the literature in a taxonomy consisting of three main categories, covering distinct yet interconnected efficient LLMs topics from model-centric, data-centric, and framework-centric perspective, respectively. We have also created a GitHub repository where we organize the papers featured in this survey at https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey. We will actively maintain the repository and incorporate new research as it emerges. We hope our survey can serve as a valuable resource to help researchers and practitioners gain a systematic understanding of efficient LLMs research and inspire them to contribute to this important and exciting field",
    "checked": true,
    "id": "d13adc935c02fa1ab7a714b6b92433a931679b60",
    "semantic_title": "efficient large language models: a survey",
    "citation_count": 1,
    "authors": [
      "Zhongwei Wan",
      "Xin Wang",
      "Che Liu",
      "Samiul Alam",
      "Yu Zheng",
      "Jiachen Liu",
      "Zhongnan Qu",
      "Shen Yan",
      "Yi Zhu",
      "Quanlu Zhang",
      "Mosharaf Chowdhury",
      "Mi Zhang"
    ]
  },
  "https://openreview.net/forum?id=IhXM3g2gxg": {
    "title": "A Short Survey on Importance Weighting for Machine Learning",
    "volume": "survey",
    "abstract": "Importance weighting is a fundamental procedure in statistics and machine learning that weights the objective function or probability distribution based on the importance of the instance in some sense. The simplicity and usefulness of the idea has led to many applications of importance weighting. For example, it is known that supervised learning under an assumption about the difference between the training and test distributions, called distribution shift, can guarantee statistically desirable properties through importance weighting by their density ratio. This survey summarizes the broad applications of importance weighting in machine learning and related research",
    "checked": true,
    "id": "8feca4f15b7fe6bd61e8d634ff4ef869378d4485",
    "semantic_title": "a short survey on importance weighting for machine learning",
    "citation_count": 2,
    "authors": [
      "Masanari Kimura",
      "Hideitsu Hino"
    ]
  },
  "https://openreview.net/forum?id=bNtr6SLgZf": {
    "title": "A Survey of Temporal Credit Assignment in Deep Reinforcement Learning",
    "volume": "survey",
    "abstract": "The Credit Assignment Problem (CAP) refers to the longstanding challenge of Reinforcement Learning agents to associate actions with their long-term consequences. Solving the CAP is a crucial step towards the successful deployment of RL in the real world since most decision problems provide feedback that is noisy, delayed, and with little or no information about the causes. These conditions make it hard to distinguish serendipitous outcomes from those caused by informed decision-making. However, the mathematical nature of credit and the CAP remains poorly understood and defined. In this survey, we review the state of the art of Temporal Credit Assignment (CA) in deep RL. We propose a unifying formalism for credit that enables equitable comparisons of state-of-the-art algorithms and improves our understanding of the trade-offs between the various methods. We cast the CAP as the problem of learning the influence of an action over an outcome from a finite amount of experience. We discuss the challenges posed by delayed effects, dilution, and a lack of action influence, and analyse how existing methods aim to address them. Finally, we survey the protocols to evaluate a credit assignment method and suggest ways to diagnose the sources of struggle for different methods. Overall, this survey provides an overview of the field for new-entry practitioners and researchers, it offers a coherent perspective for scholars looking to expedite the starting stages of a new study on the CAP, and it suggests potential directions for future research",
    "checked": true,
    "id": "c03e530e7d91b6d74f4ec5a2088c90e5db7810f7",
    "semantic_title": "a survey of temporal credit assignment in deep reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Eduardo Pignatelli",
      "Johan Ferret",
      "Matthieu Geist",
      "Thomas Mesnard",
      "Hado van Hasselt",
      "Laura Toni"
    ]
  },
  "https://openreview.net/forum?id=YgmBD2c9qX": {
    "title": "A Unified View of Differentially Private Deep Generative Modeling",
    "volume": "survey",
    "abstract": "The availability of rich and vast data sources has greatly advanced machine learning applications in various domains. However, data with privacy concerns comes with stringent regulations that frequently prohibit data access and data sharing. Overcoming these obstacles in compliance with privacy considerations is key for technological progress in many real-world application scenarios that involve sensitive data. Differentially private (DP) data publishing provides a compelling solution, where only a sanitized form of the data is publicly released, enabling privacy-preserving downstream analysis and reproducible research in sensitive domains. In recent years, various approaches have been proposed for achieving privacy-preserving high-dimensional data generation by private training on top of deep neural networks. In this paper, we present a novel unified view that systematizes these approaches. Our view provides a joint design space for systematically deriving methods that cater to different use cases. We then discuss the strengths, limitations, and inherent correlations between different approaches, aiming to shed light on crucial aspects and inspire future research. We conclude by presenting potential paths forward for the field of DP data generation, with the aim of steering the community toward making the next important steps in advancing privacy-preserving learning",
    "checked": true,
    "id": "724b9cd48f1a524aa889a76fdb5babdb0a24fdba",
    "semantic_title": "a unified view of differentially private deep generative modeling",
    "citation_count": 3,
    "authors": [
      "Dingfan Chen",
      "Raouf Kerkouche",
      "Mario Fritz"
    ]
  },
  "https://openreview.net/forum?id=1i6ZCvflQJ": {
    "title": "Cognitive Architectures for Language Agents",
    "volume": "survey",
    "abstract": "Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA). CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decision-making process to choose actions. We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, CoALA contextualizes today's language agents within the broader history of AI and outlines a path towards language-based general intelligence",
    "checked": true,
    "id": "e4bb1b1f97711a7634bf4bff72c56891be2222e6",
    "semantic_title": "cognitive architectures for language agents",
    "citation_count": 83,
    "authors": [
      "Theodore Sumers",
      "Shunyu Yao",
      "Karthik Narasimhan",
      "Thomas Griffiths"
    ]
  },
  "https://openreview.net/forum?id=fPQSxjqa2o": {
    "title": "From Continuous Dynamics to Graph Neural Networks: Neural Diffusion and Beyond",
    "volume": "survey",
    "abstract": "Graph neural networks (GNNs) have demonstrated significant promise in modelling relational data and have been widely applied in various fields of interest. The key mechanism behind GNNs is the so-called message passing where information is being iteratively aggregated to central nodes from their neighbourhood. Such a scheme has been found to be intrinsically linked to a physical process known as heat diffusion, where the propagation of GNNs naturally corresponds to the evolution of heat density. Analogizing the process of message passing to the heat dynamics allows to fundamentally understand the power and pitfalls of GNNs and consequently informs better model design. Recently, there emerges a plethora of works that proposes GNNs inspired from the continuous dynamics formulation, in an attempt to mitigate the known limitations of GNNs, such as oversmoothing and oversquashing. In this survey, we provide the first systematic and comprehensive review of studies that leverage the continuous perspective of GNNs. To this end, we introduce foundational ingredients for adapting continuous dynamics to GNNs, along with a general framework for the design of graph neural dynamics. We then review and categorize existing works based on their driven mechanisms and underlying dynamics. We also summarize how the limitations of classic GNNs can be addressed under the continuous framework. We conclude by identifying multiple open research directions",
    "checked": true,
    "id": "008ff48d90ea1776419ae0d2dbf528572dc2c0e0",
    "semantic_title": "from continuous dynamics to graph neural networks: neural diffusion and beyond",
    "citation_count": 10,
    "authors": [
      "Andi Han",
      "Dai Shi",
      "Lequan Lin",
      "Junbin Gao"
    ]
  },
  "https://openreview.net/forum?id=qhtHsvF5zj": {
    "title": "Automated Design of Metaheuristic Algorithms: A Survey",
    "volume": "survey",
    "abstract": "Metaheuristics have gained great success in academia and practice because their search logic can be applied to any problem with available solution representation, solution quality evaluation, and certain notions of locality. Manually designing metaheuristic algorithms for solving a target problem is criticized for being laborious, error-prone, and requiring intensive specialized knowledge. This gives rise to increasing interest in automated design of metaheuristic algorithms. With computing power to fully explore potential design choices, the automated design could reach and even surpass human-level design and could make high-performance algorithms accessible to a much wider range of researchers and practitioners. This paper presents a broad picture of automated design of metaheuristic algorithms, by conducting a survey on the common grounds and representative techniques in terms of design space, design strategies, performance evaluation strategies, and target problems in this field",
    "checked": true,
    "id": "29c96f5358556930fb86ba2242a4d8a074d9df95",
    "semantic_title": "automated design of metaheuristic algorithms: a survey",
    "citation_count": 5,
    "authors": [
      "Qi Zhao",
      "Qiqi Duan",
      "Bai Yan",
      "Shi Cheng",
      "Yuhui Shi"
    ]
  },
  "https://openreview.net/forum?id=kQmz1BMIYi": {
    "title": "Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation",
    "volume": "survey",
    "abstract": "The recent success of ChatGPT and GPT-4 has drawn widespread attention to multimodal dialogue systems. However, there is a lack of datasets in the academic community that can effectively evaluate the multimodal generation capabilities of Visual Language Models (VLMs) in textual-visual chat tasks. In this paper, we address this gap by introducing two novel multimodal datasets: the synthetic CLEVR-ATVC dataset (620K) and the manually pictured Fruit-ATVC dataset (50K). These datasets incorporate both visual and text-based inputs and outputs. Furthermore, to facilitate the accountability of multimodal systems in rejecting human requests, similar to language-based ChatGPT conversations, we introduce specific rules as supervisory signals within the datasets. This allows the trained VLM to provide a yes or no answer after engaging in visual and textual reasoning, accompanied by a language explanation to clarify the reasons behind the inability to execute the given human instruction. Our proposed method involves a two-stage training procedure, which includes training the image auto-encoder and the auto-regressive transformer from scratch. The first stage employs a discrete variational autoencoder (dVAE) to compress each image into concise tokens, which are then combined with text tokens into a single data stream. This stream is subsequently fed into the decoder-based transformer to generate visual re-creations and textual feedback in the second stage. We conduct comprehensive analyses of experimental results, focusing on re-created image quality, answer accuracy, and the model's behavior when faced with uncertainty and imperfect user queries. Through our explorations and findings, we aim to contribute valuable insights into the accountability of textual-visual generative models",
    "checked": true,
    "id": "53df959bcf6499c45e316086a96a624389a39a52",
    "semantic_title": "accountable textual-visual chat learns to reject human instructions in image re-creation",
    "citation_count": 0,
    "authors": [
      "Zhiwei Zhang",
      "Yuliang Liu"
    ]
  },
  "https://openreview.net/forum?id=tQVZgvXhZb": {
    "title": "A Unified View on Solving Objective Mismatch in Model-Based Reinforcement Learning",
    "volume": "survey",
    "abstract": "Model-based Reinforcement Learning (MBRL) aims to make agents more sample-efficient, adaptive, and explainable by learning an explicit model of the environment. While the capabilities of MBRL agents have significantly improved in recent years, how to best learn the model is still an unresolved question. The majority of MBRL algorithms aim at training the model to make accurate predictions about the environment and subsequently using the model to determine the most rewarding actions. However, recent research has shown that model predictive accuracy is often not correlated with action quality, tracing the root cause to the objective mismatch between accurate dynamics model learning and policy optimization of rewards. A number of interrelated solution categories to the objective mismatch problem have emerged as MBRL continues to mature as a research area. In this work, we provide an in-depth survey of these solution categories and propose a taxonomy to foster future research",
    "checked": true,
    "id": "690fe81e99b1486ff49c9fc4da9bcd1a7e674668",
    "semantic_title": "a unified view on solving objective mismatch in model-based reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Ran Wei",
      "Nathan Lambert",
      "Anthony D McDonald",
      "Alfredo Garcia",
      "Roberto Calandra"
    ]
  },
  "https://openreview.net/forum?id=sWlHhfijcS": {
    "title": "A Survey on Graph Construction for Geometric Deep Learning in Medicine: Methods and Recommendations",
    "volume": "survey",
    "abstract": "Graph neural networks are powerful tools that enable deep learning on non-Euclidean data structures like graphs, point clouds, and meshes. They leverage the connectivity of data points and can even benefit learning tasks on data, which is not naturally graph-structured -like point clouds. In these cases, the graph structure needs to be determined from the dataset, which adds a significant challenge to the learning process. This opens up a multitude of design choices for creating suitable graph structures, which have a substantial impact on the success of the graph learning task. However, so far no concrete guidance for choosing the most appropriate graph construction is available, not only due to the large variety of methods out there but also because of its strong connection to the dataset at hand. In medicine, for example, a large variety of different data types complicates the selection of graph construction methods even more. We therefore summarise the current state-of-the-art graph construction methods, especially for medical data. In this work, we introduce a categorisation scheme for graph types and graph construction methods. We identify two main strands of graph construction: static and adaptive methods, discuss their advantages and disadvantages, and formulate recommendations for choosing a suitable graph construction method. We furthermore discuss how a created graph structure can be assessed and to what degree it supports graph learning. We hope to support medical research with graph deep learning with this work by elucidating the wide variety of graph construction methods",
    "checked": true,
    "id": "21d8c97d588f6dfed0df00d8e2609da1bba8f07b",
    "semantic_title": "a survey on graph construction for geometric deep learning in medicine: methods and recommendations",
    "citation_count": 1,
    "authors": [
      "Tamara T. Müller",
      "Sophie Starck",
      "Alina Dima",
      "Stephan Wunderlich",
      "Kyriaki-Margarita Bintsi",
      "Kamilia Zaripova",
      "Rickmer Braren",
      "Daniel Rueckert",
      "Anees Kazi",
      "Georgios Kaissis"
    ]
  },
  "https://openreview.net/forum?id=3FsVtsISHW": {
    "title": "Deconfounding Imitation Learning with Variational Inference",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Risto Vuorio",
      "Pim De Haan",
      "Johann Brehmer",
      "Hanno Ackermann",
      "Daniel Dijkman",
      "Taco Cohen"
    ]
  },
  "https://openreview.net/forum?id=Aoj9H6jl6F": {
    "title": "Improving Predictor Reliability with Selective Recalibration",
    "volume": "expert",
    "abstract": "A reliable deep learning system should be able to accurately express its confidence with respect to its predictions, a quality known as calibration. One of the most effective ways to produce reliable confidence estimates with a pre-trained model is by applying a post-hoc recalibration method. Popular recalibration methods like temperature scaling are typically fit on a small amount of data and work in the model's output space, as opposed to the more expressive feature embedding space, and thus usually have only one or a handful of parameters. However, the target distribution to which they are applied is often complex and difficult to fit well with such a function. To this end we propose selective recalibration, where a selection model learns to reject some user-chosen proportion of the data in order to allow the recalibrator to focus on regions of the input space that can be well-captured by such a model. We provide theoretical analysis to motivate our algorithm, and test our method through comprehensive experiments on difficult medical imaging and zero-shot classification tasks. Our results show that selective recalibration consistently leads to significantly lower calibration error than a wide range of selection and recalibration baselines",
    "checked": true,
    "id": "db0b23b1611276cfcd423699155c72175a1ba14f",
    "semantic_title": "improving predictor reliability with selective recalibration",
    "citation_count": 0,
    "authors": [
      "Thomas P Zollo",
      "Zhun Deng",
      "Jake Snell",
      "Toniann Pitassi",
      "Richard Zemel"
    ]
  },
  "https://openreview.net/forum?id=dLaazW9zuF": {
    "title": "Multi-Fidelity Active Learning with GFlowNets",
    "volume": "expert",
    "abstract": "In the last decades, the capacity to generate large amounts of data in science and engineering applications has been growing steadily. Meanwhile, machine learning has progressed to become a suitable tool to process and utilise the available data. Nonetheless, many relevant scientific and engineering problems present challenges where current machine learning methods cannot yet efficiently leverage the available data and resources. For example, in scientific discovery, we are often faced with the problem of exploring very large, structured and high-dimensional spaces. Moreover, the high fidelity, black-box objective function is often very expensive to evaluate. Progress in machine learning methods that can efficiently tackle such challenges would help accelerate currently crucial areas such as drug and materials discovery. In this paper, we propose a multi-fidelity active learning algorithm with GFlowNets as a sampler, to efficiently discover diverse, high-scoring candidates where multiple approximations of the black-box function are available at lower fidelity and cost. Our evaluation on molecular discovery tasks shows that multi-fidelity active learning with GFlowNets can discover high-scoring candidates at a fraction of the budget of its single-fidelity counterpart while maintaining diversity, unlike RL-based alternatives. These results open new avenues for multi-fidelity active learning to accelerate scientific discovery and engineering design",
    "checked": true,
    "id": "2e065651fe8851238544cecb4185811744af5300",
    "semantic_title": "multi-fidelity active learning with gflownets",
    "citation_count": 9,
    "authors": [
      "Alex Hernández-García",
      "Nikita Saxena",
      "Moksh Jain",
      "Cheng-Hao Liu",
      "Yoshua Bengio"
    ]
  },
  "https://openreview.net/forum?id=q2AbLOwmHm": {
    "title": "Incorporating Unlabelled Data into Bayesian Neural Networks",
    "volume": "expert",
    "abstract": "Conventional Bayesian Neural Networks (BNNs) are unable to leverage unlabelled data to improve their predictions. To overcome this limitation, we introduce Self-Supervised Bayesian Neural Networks, which use unlabelled data to learn models with suitable prior predictive distributions. This is achieved by leveraging contrastive pretraining techniques and optimising a variational lower bound. We then show that the prior predictive distributions of self-supervised BNNs capture problem semantics better than conventional BNN priors. In turn, our approach offers improved predictive performance over conventional BNNs, especially in low-budget regimes",
    "checked": true,
    "id": "958e9c67dbd4da171c334db14a325cdfcc4f9215",
    "semantic_title": "incorporating unlabelled data into bayesian neural networks",
    "citation_count": 8,
    "authors": [
      "Mrinank Sharma",
      "Tom Rainforth",
      "Yee Whye Teh",
      "Vincent Fortuin"
    ]
  },
  "https://openreview.net/forum?id=mKtlzW0bWc": {
    "title": "CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion",
    "volume": "expert",
    "abstract": "This paper proposes a novel diffusion-based model, CompoDiff, for solving zero-shot Composed Image Retrieval (ZS-CIR) with latent diffusion. This paper also introduces a new synthetic dataset, named SynthTriplets18M, with 18.8 million reference images, conditions, and corresponding target image triplets to train CIR models. CompoDiff and SynthTriplets18M tackle the shortages of the previous CIR approaches, such as poor generalizability due to the small dataset scale and the limited types of conditions. CompoDiff not only achieves a new state-of-the-art on four ZS-CIR benchmarks, including FashionIQ, CIRR, CIRCO, and GeneCIS, but also enables a more versatile and controllable CIR by accepting various conditions, such as negative text, and image mask conditions. CompoDiff also shows the controllability of the condition strength between text and image queries and the trade-off between inference speed and performance, which are unavailable with existing CIR methods. The code and dataset samples are available at https://github.com/navervision/CompoDiff",
    "checked": true,
    "id": "b6eb82fc94cacdf4575fe509ef437634bf090ac2",
    "semantic_title": "compodiff: versatile composed image retrieval with latent diffusion",
    "citation_count": 23,
    "authors": [
      "Geonmo Gu",
      "Sanghyuk Chun",
      "Wonjae Kim",
      "HeeJae Jun",
      "Yoohoon Kang",
      "Sangdoo Yun"
    ]
  },
  "https://openreview.net/forum?id=iRDwUXYsSJ": {
    "title": "Differential Equation Scaling Limits of Shaped and Unshaped Neural Networks",
    "volume": "expert",
    "abstract": "Recent analyses of neural networks with shaped activations (i.e. the activation function is scaled as the network size grows) have led to scaling limits described by differential equations. However, these results do not a priori tell us anything about ``ordinary'' unshaped networks, where the activation is unchanged as the network size grows. In this article, we find similar differential equation based asymptotic characterization for two types of unshaped networks. Firstly, we show that the following two architectures converge to the same infinite-depth-and-width limit at initialization: (i) a fully connected ResNet with a $d^{-1/2}$ factor on the residual branch, where $d$ is the network depth. (ii) a multilayer perceptron (MLP) with depth $d \\ll$ width $n$ and shaped ReLU activation at rate $d^{-1/2}$. Secondly, for an unshaped MLP at initialization, we derive the first order asymptotic correction to the layerwise correlation. In particular, if $\\rho_\\ell$ is the correlation at layer $\\ell$, then $q_t = \\ell^2 (1 - \\rho_\\ell)$ with $t = \\frac{\\ell}{n}$ converges to an SDE with a singularity at $t=0$. These results together provide a connection between shaped and unshaped network architectures, and opens up the possibility of studying the effect of normalization methods and how it connects with shaping activation functions",
    "checked": true,
    "id": "196803e4e33df1c35f1f05ba988c9508cc80c989",
    "semantic_title": "differential equation scaling limits of shaped and unshaped neural networks",
    "citation_count": 1,
    "authors": [
      "Mufan Bill Li",
      "Mihai Nica"
    ]
  },
  "https://openreview.net/forum?id=lNAyUngGFK": {
    "title": "Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models",
    "volume": "expert",
    "abstract": "Fine-tuning language models~(LMs) on human-generated data remains a prevalent practice. However, the performance of such models is often limited by the quantity and diversity of high-quality human data. In this paper, we explore whether we can go beyond human data on tasks where we have access to scalar feedback, for example, on math problems where one can verify correctness. To do so, we investigate a simple self-training method based on expectation-maximization, which we call \\method, where we (1) generate samples from the model and filter them using binary feedback, (2) fine-tune the model on these samples, and (3) repeat this process a few times. Testing on advanced MATH reasoning and APPS coding benchmarks using PaLM-2 models, we find that \\method{} scales favorably with model size and significantly surpasses fine-tuning only on human data. Overall, our findings suggest self-training with feedback can reduce dependence on human-generated data",
    "checked": true,
    "id": "48362b169a235ca650918c489c8cea4c597da645",
    "semantic_title": "beyond human data: scaling self-training for problem-solving with language models",
    "citation_count": 59,
    "authors": [
      "Avi Singh",
      "John D Co-Reyes",
      "Rishabh Agarwal",
      "Ankesh Anand",
      "Piyush Patil",
      "Xavier Garcia",
      "Peter J Liu",
      "James Harrison",
      "Jaehoon Lee",
      "Kelvin Xu",
      "Aaron T Parisi",
      "Abhishek Kumar",
      "Alexander A Alemi",
      "Alex Rizkowsky",
      "Azade Nova",
      "Ben Adlam",
      "Bernd Bohnet",
      "Gamaleldin Fathy Elsayed",
      "Hanie Sedghi",
      "Igor Mordatch",
      "Isabelle Simpson",
      "Izzeddin Gur",
      "Jasper Snoek",
      "Jeffrey Pennington",
      "Jiri Hron",
      "Kathleen Kenealy",
      "Kevin Swersky",
      "Kshiteej Mahajan",
      "Laura A Culp",
      "Lechao Xiao",
      "Maxwell Bileschi",
      "Noah Constant",
      "Roman Novak",
      "Rosanne Liu",
      "Tris Warkentin",
      "Yamini Bansal",
      "Ethan Dyer",
      "Behnam Neyshabur",
      "Jascha Sohl-Dickstein",
      "Noah Fiedel"
    ]
  },
  "https://openreview.net/forum?id=CD9Snc73AW": {
    "title": "Improving and generalizing flow-based generative models with minibatch optimal transport",
    "volume": "expert",
    "abstract": "Continuous normalizing flows (CNFs) are an attractive generative modeling technique, but they have been held back by limitations in their simulation-based maximum likelihood training. We introduce the generalized conditional flow matching (CFM) technique, a family of simulation-free training objectives for CNFs. CFM features a stable regression objective like that used to train the stochastic flow in diffusion models but enjoys the efficient inference of deterministic flow models. In contrast to both diffusion models and prior CNF training algorithms, CFM does not require the source distribution to be Gaussian or require evaluation of its density. A variant of our objective is optimal transport CFM (OT-CFM), which creates simpler flows that are more stable to train and lead to faster inference, as evaluated in our experiments. Furthermore, we show that when the true OT plan is available, our OT-CFM method approximates dynamic OT. Training CNFs with CFM improves results on a variety of conditional and unconditional generation tasks, such as inferring single cell dynamics, unsupervised image translation, and Schrödinger bridge inference. The Python code is available at https://github.com/atong01/conditional-flow-matching",
    "checked": true,
    "id": "5396c55bee2a2abf2207e1cc5e5ae72c9edef9fa",
    "semantic_title": "improving and generalizing flow-based generative models with minibatch optimal transport",
    "citation_count": 91,
    "authors": [
      "Alexander Tong",
      "Kilian FATRAS",
      "Nikolay Malkin",
      "Guillaume Huguet",
      "Yanlei Zhang",
      "Jarrid Rector-Brooks",
      "Guy Wolf",
      "Yoshua Bengio"
    ]
  },
  "https://openreview.net/forum?id=YH3oERVYjF": {
    "title": "A density estimation perspective on learning from pairwise human preferences",
    "volume": "expert",
    "abstract": "Learning from human feedback (LHF)—and in particular learning from pairwise preferences—has recently become a crucial ingredient in training large language models (LLMs), and has been the subject of much research. Most recent works frame it as a reinforcement learning problem, where a reward function is learned from pairwise preference data and the LLM is treated as a policy which is adapted to maximize the rewards, often under additional regularization constraints. We propose an alternative interpretation which centers on the generative process for pairwise preferences and treats LHF as a density estimation problem. We provide theoretical and empirical results showing that for a family of generative processes defined via preference behavior distribution equations, training a reward function on pairwise preferences effectively models an annotator's implicit preference distribution. Finally, we discuss and present findings on \"annotator misspecification\"—failure cases where wrong modeling assumptions are made about annotator behavior, resulting in poorly-adapted models—suggesting that approaches that learn from pairwise human preferences could have trouble learning from a population of annotators with diverse viewpoints",
    "checked": true,
    "id": "12cf8be0865649b5e878051182b69f51e67fb8d4",
    "semantic_title": "a density estimation perspective on learning from pairwise human preferences",
    "citation_count": 5,
    "authors": [
      "Vincent Dumoulin",
      "Daniel D. Johnson",
      "Pablo Samuel Castro",
      "Hugo Larochelle",
      "Yann Dauphin"
    ]
  },
  "https://openreview.net/forum?id=vFSsRYGpjW": {
    "title": "Distributional GFlowNets with Quantile Flows",
    "volume": "expert",
    "abstract": "Generative Flow Networks (GFlowNets) are a new family of probabilistic samplers where an agent learns a stochastic policy for generating complex combinatorial structure through a series of decision-making steps. There have been recent successes in applying GFlowNets to a number of practical domains where diversity of the solutions is crucial, while reinforcement learning aims to learn an optimal solution based on the given reward function only and fails to discover diverse and high-quality solutions. However, the current GFlowNet framework is relatively limited in its applicability and cannot handle stochasticity in the reward function. In this work, we adopt a distributional paradigm for GFlowNets, turning each flow function into a distribution, thus providing more informative learning signals during training. By parameterizing each edge flow through their quantile functions, our proposed \\textit{quantile matching} GFlowNet learning algorithm is able to learn a risk-sensitive policy, an essential component for handling scenarios with risk uncertainty. Moreover, we find that the distributional approach can achieve substantial improvement on existing benchmarks compared to prior methods due to our enhanced training algorithm, even in settings with deterministic rewards",
    "checked": true,
    "id": "8031bfd2408b9fa41e85b346b9452ebd63073076",
    "semantic_title": "distributional gflownets with quantile flows",
    "citation_count": 19,
    "authors": [
      "Dinghuai Zhang",
      "Ling Pan",
      "Ricky T. Q. Chen",
      "Aaron Courville",
      "Yoshua Bengio"
    ]
  },
  "https://openreview.net/forum?id=Ytp9KFKZfZ": {
    "title": "Leveraging Function Space Aggregation for Federated Learning at Scale",
    "volume": "expert",
    "abstract": "The federated learning paradigm has motivated the development of methods for aggregating multiple client updates into a global server model, without sharing client data. Many federated learning algorithms, including the canonical Federated Averaging (FedAvg), take a direct (possibly weighted) average of the client parameter updates, motivated by results in distributed optimization. In this work, we adopt a function space perspective and propose a new algorithm, FedFish, that aggregates local approximations to the functions learned by clients, using an estimate based on their Fisher information. We evaluate FedFish on realistic, large-scale cross-device benchmarks. While the performance of FedAvg can suffer as client models drift further apart, we demonstrate that FedFish is more robust to longer local training. Our evaluation across several settings in image and language benchmarks shows that FedFish outperforms FedAvg as local training epochs increase. Further, FedFish results in global networks that are more amenable to efficient personalization via local fine-tuning on the same or shifted data distributions. For instance, federated pretraining on the C4 dataset, followed by few-shot personalization on Stack Overflow, results in a 7% improvement in next-token prediction by FedFish over FedAvg",
    "checked": true,
    "id": "ea88649be9fbc54d49833edcf8903df21afb5f6f",
    "semantic_title": "leveraging function space aggregation for federated learning at scale",
    "citation_count": 1,
    "authors": [
      "Nikita Dhawan",
      "Nicole Elyse Mitchell",
      "Zachary Charles",
      "Zachary Garrett",
      "Gintare Karolina Dziugaite"
    ]
  },
  "https://openreview.net/forum?id=UAT4j3Y7HP": {
    "title": "Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks",
    "volume": "expert",
    "abstract": "Real-world natural language processing systems need to be robust to human adversaries. Collecting examples of human adversaries for training is an effective but expensive solution. On the other hand, training on synthetic attacks with small perturbations---such as word-substitution---does not actually improve robustness to human adversaries. In this paper, we propose an adversarial training framework that uses limited human adversarial examples to generate more useful adversarial examples at scale. We demonstrate the advantages of this system on the ANLI and hate speech detection benchmark datasets---both collected via an iterative, adversarial human-and-model-in-the-loop procedure. Compared to training only on observed human attacks, also training on our synthetic adversarial examples improves model robustness to future rounds. In ANLI, we see accuracy gains on the current set of attacks (44.1\\%$\\,\\to\\,$50.1\\%) and on two future unseen rounds of human generated attacks (32.5\\%$\\,\\to\\,$43.4\\%, and 29.4\\%$\\,\\to\\,$40.2\\%). In hate speech detection, we see AUC gains on current attacks (0.76 $\\to$ 0.84) and a future round (0.77 $\\to$ 0.79). Attacks from methods that do not learn the distribution of existing human adversaries, meanwhile, degrade robustness",
    "checked": true,
    "id": "9262e201ecaf678429bf86481b1c1d1ae6332b44",
    "semantic_title": "break it, imitate it, fix it: robustness by generating human-like attacks",
    "citation_count": 3,
    "authors": [
      "Aradhana Sinha",
      "Ananth Balashankar",
      "Ahmad Beirami",
      "Thi Avrahami",
      "Jilin Chen",
      "Alex Beutel"
    ]
  },
  "https://openreview.net/forum?id=ga5SNulYet": {
    "title": "Wavelet Networks: Scale-Translation Equivariant Learning From Raw Time-Series",
    "volume": "expert",
    "abstract": "Leveraging the symmetries inherent to specific data domains for the construction of equivariant neural networks has lead to remarkable improvements in terms of data efficiency and generalization. However, most existing research focuses on symmetries arising from planar and volumetric data, leaving a crucial data source largely underexplored: *time-series*. In this work, we fill this gap by leveraging the symmetries inherent to time-series for the construction of equivariant neural network. We identify two core symmetries: *scale and translation*, and construct scale-translation equivariant neural networks for time-series learning. Intriguingly, we find that scale-translation equivariant mappings share strong resemblance with the *wavelet transform*. Inspired by this resemblance, we term our networks *Wavelet Networks*, and show that they perform nested non-linear wavelet-like time-frequency transforms. Empirical results show that Wavelet Networks outperform conventional CNNs on raw waveforms, and match strongly engineered spectrogram techniques across several tasks and time-series types, including audio, environmental sounds, and electrical signals. Our code is publicly available at https://github.com/dwromero/wavelet_networks",
    "checked": false,
    "id": "1ad1d4691c74f25a6d3548214bcd1b2c10611bce",
    "semantic_title": "sossf: landsat-8 image synthesis on the blending of sentinel-1 and modis data",
    "citation_count": 1,
    "authors": [
      "David W. Romero",
      "Erik J Bekkers",
      "Jakub M. Tomczak",
      "Mark Hoogendoorn"
    ]
  },
  "https://openreview.net/forum?id=lTOku838Zv": {
    "title": "Neural Implicit Manifold Learning for Topology-Aware Density Estimation",
    "volume": "expert",
    "abstract": "Natural data observed in $\\mathbb{R}^n$ is often constrained to an $m$-dimensional manifold $\\mathcal{M}$, where $m < n$. This work focuses on the task of building theoretically principled generative models for such data. Current generative models learn $\\mathcal{M}$ by mapping an $m$-dimensional latent variable through a neural network $f_\\theta: \\mathbb{R}^m \\to \\mathbb{R}^n$. These procedures, which we call pushforward models, incur a straightforward limitation: manifolds cannot in general be represented with a single parameterization, meaning that attempts to do so will incur either computational instability or the inability to learn probability densities within the manifold. To remedy this problem, we propose to model $\\mathcal{M}$ as a neural implicit manifold: the set of zeros of a neural network. We then learn the probability density within $\\mathcal{M}$ with a constrained energy-based model, which employs a constrained variant of Langevin dynamics to train and sample from the learned manifold. In experiments on synthetic and natural data, we show that our model can learn manifold-supported distributions with complex topologies more accurately than pushforward models",
    "checked": true,
    "id": "914b2aed4d0938d7dce4695d1c25069ea68a1d37",
    "semantic_title": "neural implicit manifold learning for topology-aware density estimation",
    "citation_count": 1,
    "authors": [
      "Brendan Leigh Ross",
      "Gabriel Loaiza-Ganem",
      "Anthony L. Caterini",
      "Jesse C. Cresswell"
    ]
  },
  "https://openreview.net/forum?id=dltUedmUVT": {
    "title": "Temporal Difference Learning with Compressed Updates: Error-Feedback meets Reinforcement Learning",
    "volume": "main",
    "abstract": "In large-scale distributed machine learning, recent works have studied the effects of compressing gradients in stochastic optimization to alleviate the communication bottleneck. These works have collectively revealed that stochastic gradient descent (SGD) is robust to structured perturbations such as quantization, sparsification, and delays. Perhaps surprisingly, despite the surge of interest in multi-agent reinforcement learning, almost nothing is known about the analogous question: \\textit{Are common reinforcement learning (RL) algorithms also robust to similar perturbations?} We investigate this question by studying a variant of the classical temporal difference (TD) learning algorithm with a perturbed update direction, where a general compression operator is used to model the perturbation. Our work makes three important technical contributions. First, we prove that compressed TD algorithms, coupled with an error-feedback mechanism used widely in optimization, exhibit the same non-asymptotic theoretical guarantees as their SGD counterparts. Second, we show that our analysis framework extends seamlessly to nonlinear stochastic approximation schemes that subsume Q-learning. Third, we prove that for multi-agent TD learning, one can achieve linear convergence speedups with respect to the number of agents while communicating just $\\tilde{O}(1)$ bits per iteration. Notably, these are the first finite-time results in RL that account for general compression operators and error-feedback in tandem with linear function approximation and Markovian sampling. Our proofs hinge on the construction of novel Lyapunov functions that capture the dynamics of a memory variable introduced by error-feedback",
    "checked": null,
    "id": "a58ea65258e4c3569cd02d2038246b2b8720bf1d",
    "semantic_title": "temporal difference learning with compressed updates: error-feedback meets reinforcement learning",
    "citation_count": 8,
    "authors": [
      "Aritra Mitra",
      "George J. Pappas",
      "Hamed Hassani"
    ]
  },
  "https://openreview.net/forum?id=iA2KQyoun1": {
    "title": "Granger Causal Interaction Skill Chains",
    "volume": "main",
    "abstract": "Reinforcement Learning (RL) has demonstrated promising results in learning policies for complex tasks, but it often suffers from low sample efficiency and limited transferability. Hierarchical RL (HRL) methods aim to address the difficulty of learning long-horizon tasks by decomposing policies into skills, abstracting states, and reusing skills in new tasks. However, many HRL methods require some initial task success to discover useful skills, which paradoxically may be very unlikely without access to useful skills. On the other hand, reward-free HRL methods often need to learn far too many skills to achieve proper coverage in high-dimensional domains. In contrast, we introduce the Chain of Interaction Skills (COInS) algorithm, which focuses on \\textit{controllability} in factored domains to identify a small number of task-agnostic skills that allow for a high degree of control of the factored state. COInS uses learned detectors to identify interactions between state factors and then trains a chain of skills to control each of these factors successively. We evaluate COInS on a robotic pushing task with obstacles—a challenging domain where other RL and HRL methods fall short. We also demonstrate the transferability of skills learned by COInS, using variants of Breakout, a common RL benchmark, and show 2-3x improvement in both sample efficiency and final performance compared to standard RL baselines",
    "checked": null,
    "id": "d324e77123e9d5cfdeeb6f9172a76be2713910b4",
    "semantic_title": "granger causal interaction skill chains",
    "citation_count": 0,
    "authors": [
      "Caleb Chuck",
      "Kevin Black",
      "Aditya Arjun",
      "Yuke Zhu",
      "Scott Niekum"
    ]
  },
  "https://openreview.net/forum?id=hpKJkVoThY": {
    "title": "Models of human preference for learning reward functions",
    "volume": "main",
    "abstract": "The utility of reinforcement learning is limited by the alignment of reward functions with the interests of human stakeholders. One promising method for alignment is to learn the reward function from human-generated preferences between pairs of trajectory segments, a type of reinforcement learning from human feedback (RLHF). These human preferences are typically assumed to be informed solely by partial return, the sum of rewards along each segment. We find this assumption to be flawed and propose modeling human preferences instead as informed by each segment's regret, a measure of a segment's deviation from optimal decision-making. Given infinitely many preferences generated according to regret, we prove that we can identify a reward function equivalent to the reward function that generated those preferences, and we prove that the previous partial return model lacks this identifiability property in multiple contexts. We empirically show that our proposed regret preference model outperforms the partial return preference model with finite training data in otherwise the same setting. Additionally, we find that our proposed regret preference model better predicts real human preferences and also learns reward functions from these preferences that lead to policies that are better human-aligned. Overall, this work establishes that the choice of preference model is impactful, and our proposed regret preference model provides an improvement upon a core assumption of recent research. We have open sourced our experimental code, the human preferences dataset we gathered, and our training and preference elicitation interfaces for gathering such a dataset",
    "checked": null,
    "id": "9f9b61e429e85e37d6df0e3c478a074f7e6cb9fc",
    "semantic_title": "models of human preference for learning reward functions",
    "citation_count": 25,
    "authors": [
      "W. Bradley Knox",
      "Stephane Hatgis-Kessell",
      "Serena Booth",
      "Scott Niekum",
      "Peter Stone",
      "Alessandro G Allievi"
    ]
  },
  "https://openreview.net/forum?id=axBIMcGZn9": {
    "title": "Continual Learning: Applications and the Road Forward",
    "volume": "main",
    "abstract": "Continual learning is a subfield of machine learning, which aims to allow machine learning models to continuously learn on new data, by accumulating knowledge without forgetting what was learned in the past. In this work, we take a step back, and ask: \"Why should one care about continual learning in the first place?\". We set the stage by examining recent continual learning papers published at four major machine learning conferences, and show that memory-constrained settings dominate the field. Then, we discuss five open problems in machine learning, and even though they might seem unrelated to continual learning at first sight, we show that continual learning will inevitably be part of their solution. These problems are model editing, personalization and specialization, on-device learning, faster (re-)training and reinforcement learning. Finally, by comparing the desiderata from these unsolved problems and the current assumptions in continual learning, we highlight and discuss four future directions for continual learning research. We hope that this work offers an interesting perspective on the future of continual learning, while displaying its potential value and the paths we have to pursue in order to make it successful. This work is the result of the many discussions the authors had at the Dagstuhl seminar on Deep Continual Learning, in March 2023",
    "checked": null,
    "id": "3d7e5485fae2965ddf081dc64be6ab52f5834cf8",
    "semantic_title": "continual learning: applications and the road forward",
    "citation_count": 12,
    "authors": [
      "Eli Verwimp",
      "Rahaf Aljundi",
      "Shai Ben-David",
      "Matthias Bethge",
      "Andrea Cossu",
      "Alexander Gepperth",
      "Tyler L. Hayes",
      "Eyke Hüllermeier",
      "Christopher Kanan",
      "Dhireesha Kudithipudi",
      "Christoph H. Lampert",
      "Martin Mundt",
      "Razvan Pascanu",
      "Adrian Popescu",
      "Andreas S. Tolias",
      "Joost van de Weijer",
      "Bing Liu",
      "Vincenzo Lomonaco",
      "Tinne Tuytelaars",
      "Gido M van de Ven"
    ]
  },
  "https://openreview.net/forum?id=cAthubStyG": {
    "title": "AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks",
    "volume": "main",
    "abstract": "The fields of both Natural Language Processing (NLP) and Automated Machine Learning (AutoML) have achieved remarkable results over the past years. In NLP, especially Large Language Models (LLMs) have experienced a rapid series of breakthroughs very recently. We envision that the two fields can radically push the boundaries of each other through tight integration. To showcase this vision, we explore the potential of a symbiotic relationship between AutoML and LLMs, shedding light on how they can benefit each other. In particular, we investigate both the opportunities to enhance AutoML approaches with LLMs from different perspectives and the challenges of leveraging AutoML to further improve LLMs. To this end, we survey existing work, and we critically assess risks. We strongly believe that the integration of the two fields has the potential to disrupt both fields, NLP and AutoML. By highlighting conceivable synergies, but also risks, we aim to foster further exploration at the intersection of AutoML and LLMs",
    "checked": null,
    "id": "9afa0c3227fd0ec3a76928784e59c4205cbace24",
    "semantic_title": "automl in the age of large language models: current challenges, future opportunities and risks",
    "citation_count": 14,
    "authors": [
      "Alexander Tornede",
      "Difan Deng",
      "Theresa Eimer",
      "Joseph Giovanelli",
      "Aditya Mohan",
      "Tim Ruhkopf",
      "Sarah Segel",
      "Daphne Theodorakopoulos",
      "Tanja Tornede",
      "Henning Wachsmuth",
      "Marius Lindauer"
    ]
  },
  "https://openreview.net/forum?id=g01OVahtN9": {
    "title": "Controlling Federated Learning for Covertness",
    "volume": "main",
    "abstract": "A learner aims to minimize a function $f$ by repeatedly querying a distributed oracle that provides noisy gradient evaluations. At the same time, the learner seeks to hide $\\arg\\min f$ from a malicious eavesdropper that observes the learner's queries. This paper considers the problem of \\textit{covert} or \\textit{learner-private} optimization, where the learner has to dynamically choose between learning and obfuscation by exploiting the stochasticity. The problem of controlling the stochastic gradient algorithm for covert optimization is modeled as a Markov decision process, and we show that the dynamic programming operator has a supermodular structure implying that the optimal policy has a monotone threshold structure. A computationally efficient policy gradient algorithm is proposed to search for the optimal querying policy without knowledge of the transition probabilities. As a practical application, our methods are demonstrated on a hate speech classification task in a federated setting where an eavesdropper can use the optimal weights to generate toxic content, which is more easily misclassified. Numerical results show that when the learner uses the optimal policy, an eavesdropper can only achieve a validation accuracy of $52\\%$ with no information and $69\\%$ when it has a public dataset with $10\\%$ positive samples compared to $83\\%$ when the learner employs a greedy policy",
    "checked": null,
    "id": "2a864f9dbd52e8965818cce75b2b51fecc6d47f4",
    "semantic_title": "controlling federated learning for covertness",
    "citation_count": 4,
    "authors": [
      "Adit Jain",
      "Vikram Krishnamurthy"
    ]
  },
  "https://openreview.net/forum?id=TYYApLzjaQ": {
    "title": "Unmasking the Veil: An Investigation into Concept Ablation for Privacy and Copyright Protection in Images",
    "volume": "main",
    "abstract": "In this paper, we extend the study of concept ablation within pre-trained models as introduced in 'Ablating Concepts in Text-to-Image Diffusion Models' by $\\citep{Kumari2022}$. Our work focuses on reproducing the results achieved by the different variants of concept ablation proposed through predefined metrics. We also introduce a novel variant of concept ablation—trademark ablation. This variant combines the principles of memorization and instance ablation to tackle the nuanced influence of proprietary or branded elements in model outputs. Further, our research contributions include an observational analysis of the model's limitations. Moreover, we investigate the model's behavior in response to ablation leakage-inducing prompts, which aim to indirectly ablate concepts, revealing insights into the model's resilience and adaptability. We also observe the model's performance degradation on images generated by concepts far from its target ablation concept, which is documented in the appendix",
    "checked": null,
    "id": "f81a1ef3154346be192588637105974766464060",
    "semantic_title": "unmasking the veil: an investigation into concept ablation for privacy and copyright protection in images",
    "citation_count": 0,
    "authors": [
      "Shivank Garg",
      "Manyana Tiwari"
    ]
  },
  "https://openreview.net/forum?id=d3Vj360Wi2": {
    "title": "Reproducibility Study of \"ITI-GEN: Inclusive Text-to-Image Generation",
    "volume": "main",
    "abstract": "Text-to-image generative models often present issues regarding fairness with respect to certain sensitive attributes, such as gender or skin tone. This study aims to reproduce the results presented in \"ITI-GEN: Inclusive Text-to-Image Generation\" by Zhang et al. (2023), which introduces a model to improve inclusiveness in these kinds of models. We show that most of the claims made by the authors about ITI-GEN hold: it improves the diversity and quality of generated images, it is scalable to different domains, it has plug-and-play capabilities, and it is efficient from a computational point of view. However, ITI-GEN sometimes uses undesired attributes as proxy features and it is unable to disentangle some pairs of (correlated) attributes such as gender and baldness. In addition, when the number of considered attributes increases, the training time grows exponentially and ITI-GEN struggles to generate inclusive images for all elements in the joint distribution. To solve these issues, we propose using Hard Prompt Search with negative prompting, a method that does not require training and that handles negation better than vanilla Hard Prompt Search. Nonetheless, Hard Prompt Search (with or without negative prompting) cannot be used for continuous attributes that are hard to express in natural language, an area where ITI-GEN excels as it is guided by images during training. Finally, we propose combining ITI-GEN and Hard Prompt Search with negative prompting",
    "checked": null,
    "id": "54cfe67fb02467fbf45b24f8ce1caa34728cde30",
    "semantic_title": "reproducibility study of \"iti-gen: inclusive text-to-image generation",
    "citation_count": 0,
    "authors": [
      "Daniel Gallo Fernández",
      "Răzvan-Andrei Matișan",
      "Alejandro Monroy Muñoz",
      "Janusz Partyka"
    ]
  },
  "https://openreview.net/forum?id=ccDi5jtSF7": {
    "title": "Reproducibility study of FairAC",
    "volume": "main",
    "abstract": "This work aims to reproduce the findings of the paper \"Fair Attribute Completion on Graph with Missing Attributes\" written by Guo et al. (2023) by investigating the claims made in the paper. This paper suggests that the results of the original paper are reproducible and thus, the claims hold. However, the claim that FairAC is a generic framework for many downstream tasks is very broad and could therefore only be partially tested. Moreover, we show that FairAC is generalizable to various datasets and sensitive attributes and show evidence that the improvement in group fairness of the FairAC framework does not come at the expense of individual fairness. Lastly, the codebase of FairAC has been refactored and is now easily applicable for various datasets and models",
    "checked": null,
    "id": "24a348dfaf029a87e10bb46eb8c3ab6dd54197a3",
    "semantic_title": "reproducibility study of fairac",
    "citation_count": 0,
    "authors": [
      "Gijs de Jong",
      "Macha J. Meijer",
      "Derck W. E. Prinzhorn",
      "Harold Ruiter"
    ]
  },
  "https://openreview.net/forum?id=Wm6d44I8St": {
    "title": "[Re] CUDA: Curriculum of Data Augmentation for Long‐tailed Recognition",
    "volume": "main",
    "abstract": "In this reproducibility study, we present our results and experience during replicating the paper, titled CUDA: Curriculum of Data Augmentation for Long-Tailed Recognition(Ahn et al., 2023).Traditional datasets used in image recognition, such as ImageNet, are often synthetically balanced, meaning each class has an equal number of samples. In practical scenarios, datasets frequently exhibit significant class imbalances, with certain classes having a disproportionately larger number of samples compared to others. This discrepancy poses a challenge for traditional image recognition models, as they tend to favor classes with larger sample sizes, leading to poor performance on minority classes. CUDA proposes a class-wise data augmentation technique which can be used over any existing model to improve the accuracy for LTR: Long Tailed Recognition. We successfully replicated all of the results pertaining to the long-tailed CIFAR-100-LT dataset and extended our analysis to provide deeper insights into how CUDA efficiently tackles class imbalance. The code and the readings are available in https://anonymous.4open.science/r/CUDA-org--C2FD/README.md",
    "checked": null,
    "id": "1e6c5ebf8d03eacc501fd344531e9a25b139e504",
    "semantic_title": "[re] cuda: curriculum of data augmentation for long-tailed recognition",
    "citation_count": 2,
    "authors": [
      "Barath Chandran.C"
    ]
  },
  "https://openreview.net/forum?id=9M2XqvH2SB": {
    "title": "[Re] Reproducibility Study of \"Explaining Temporal Graph Models Through an Explorer-Navigator Framework",
    "volume": "main",
    "abstract": "This paper seeks to reproduce and extend the results of the paper \"Explaining Temporal Graph Models Through an Explorer-Navigator Framework\" by (Xia et al., 2023). The main contribution of the original authors is a novel explainer for temporal graph networks, the Temporal GNN Explainer (T-GNNExplainer), which finds a subset of preceding events that \"explain\" a prediction made by a temporal graph model. The explorer is tested on two temporal graph models that are trained on two real-world and two synthetic datasets. The explorer is evaluated using a newly proposed metric for explanatory graph models. The authors compare the performance of their explorer to three baseline explainer methods, either adapted from a GNN explainer or developed by the authors. The authors claim that T-GNNExplainer achieves superior performance compared to the baselines when evaluated with their proposed metric. This work reproduces the original experiments by using the code (with minor adjustments), model specifications, and hyperparameters provided by the original authors. To evaluate the robustness of these claims, the method was extended to one new dataset (MOOC). Results show that the T-GNNexplainer performs best on some, but not all metrics as reported in the original findings. We conclude that the main lines of this paper hold up even though all results are less pronounced than claimed. Results show that the T-GNNExplainer does not perform similarly across different T-GNN models, precise dataset specifications are needed to obtain high performance, and there are simpler, less computationally costly explainer methods (like PBONE) that could offer competitive results",
    "checked": null,
    "id": "5b58f532809ef51513f5e8c27e349abc5710a7f6",
    "semantic_title": "[re] reproducibility study of \"explaining temporal graph models through an explorer-navigator framework",
    "citation_count": 0,
    "authors": [
      "Helia Ghasemi",
      "Christina Isaicu",
      "Jesse Wonnink",
      "Andreas Berentzen"
    ]
  },
  "https://openreview.net/forum?id=8cYcR23WUo": {
    "title": "[Re] GNNInterpreter: A probabilistic generative model-level explanation for Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph Neural Networks have recently gained recognition for their performance on graph machine learning tasks. The increasing attention on these models' trustworthiness and decision-making mechanisms has instilled interest in the exploration of explainability tech- niques, including the model proposed in \"GNNInterpreter: A probabilistic generative model- level explanation for Graph Neural Networks.\" (Wang & Shen (2022)). This work aims to reproduce the findings of the original paper, by investigation the main claims made by its authors, namely that GNNInterpreter (i) generates faithful and realistic explanations with- out requiring domain-specific knowledge, (ii) has the ability to work with various node and edge features, (iii) produces explanations that are representative for the target class and (iv) has a much lower training time compared to XGNN, the current state-of-the-art model- level GNN explanation technique. To reproduce the results, we make use of the open-source implementation and we test the interpreter on the same datasets and GNN models as in the original paper. We conduct an enhanced quantitative and qualitative evaluation, and additionally we extend the original experiments to include another real-world dataset. Our results show that we are not able to validate the first claim, due to significant hyperpa- rameter and seed variation, as well as due to training instability. Furthermore, we partially validate the second claim by testing on datasets with different node and edge features, but we reject the third claim due to GNNInterpreter's failure to outperform XGNN in producing dataset aligned explanations. Lastly, we are able to confirm the last claim",
    "checked": null,
    "id": "f0f69efbd5dd3ea955f8fa62086640c9387b1a57",
    "semantic_title": "[re] gnninterpreter: a probabilistic generative model-level explanation for graph neural networks",
    "citation_count": 0,
    "authors": [
      "Ana Vasilcoiu",
      "T.H.F. Stessen",
      "Thies Kersten",
      "Batu Helvacioglu"
    ]
  },
  "https://openreview.net/forum?id=FI1XvwpchC": {
    "title": "[Re] Explaining Temporal Graph Models through an Explorer-Navigator Framework",
    "volume": "main",
    "abstract": "Temporal graphs model complex dynamic relations that change over time, and are being used in a growing number of applications. In recent years, several graph neural networks (GNNs) were proposed, designed specifically for this temporal setting (Xu et al., 2020; Rossi et al., 2020). However, these models are notoriously hard to interpret. For this reason, the original authors (Xia et al., 2023) propose the Temporal GNN Explainer (T-GNNExplainer) – an explorer-navigator framework to efficiently compute sparse explanations of target Temporal GNNs. We reproduce the main findings of the original paper, extend their work by proposing a different type of navigator method, and examine in detail the explanation capabilities and efficiency of the provided framework within various model and hyperparameter settings. We confirm that their explainer outperforms the other baselines across nearly all datasets and metrics. Our findings suggest the navigator helps bias the search process, as well as that T-GNNExplainer can find an exact influential event set. Moreover, we examine the effect of different navigator methods and quantify the runtime-fidelity tradeoff controlled by two hyper-parameters",
    "checked": null,
    "id": "6cdbd0c38baca989d257d803b85bf487bb05b7b8",
    "semantic_title": "[re] explaining temporal graph models through an explorer-navigator framework",
    "citation_count": 0,
    "authors": [
      "Miklós Hamar",
      "Matey Krastev",
      "Kristiyan Danielov Hristov",
      "David Beglou"
    ]
  },
  "https://openreview.net/forum?id=8UfhCZjOV7": {
    "title": "[Re] On the Reproducibility of Post-Hoc Concept Bottleneck Models",
    "volume": "main",
    "abstract": "To obtain state-of-the-art performance, many deeper artificial intelligence models sacrifice human explainability in their decision-making. One solution proposed for achieving top performance and retaining explainability is the Post-Hoc Concept Bottleneck Model (PCBM) (Yuksekgonul et al., 2023), which can convert the embeddings of any deep neural network into a set of human-interpretable concept weights. In this work, we reproduce and expand upon the findings of Yuksekgonul et al. (2023). Our results show that while most of the authors' claims and results hold, some of the results they obtained could not be sufficiently replicated. Specifically, the claims relating to PCBM performance preservation and its non-requirement of labeled concept datasets were generally reproduced, whereas the one claiming its model editing capabilities was not. Beyond these results, our contributions to their work include evidence that PCBMs may work for audio classification problems, verification of the interpretability of their methods, and updates to their code for missing implementations. The code for our implementations can be found at https://github.com/dgcnz/FACT",
    "checked": null,
    "id": "9be1a1ad73aa42bef615507bc49617893cbb4346",
    "semantic_title": "[re] on the reproducibility of post-hoc concept bottleneck models",
    "citation_count": 0,
    "authors": [
      "Nesta Midavaine",
      "Gregory Hok Tjoan Go",
      "Diego Canez",
      "Ioana Simion",
      "Satchit Chatterji"
    ]
  },
  "https://openreview.net/forum?id=JQoWmeNaC2": {
    "title": "Reproducibility Study of \"Explaining RL Decisions with Trajectories",
    "volume": "main",
    "abstract": "This paper reports on the reproducibility study on the paper `Explaining RL Decisions with Trajectories' by Deshmukh et al. (2023). The authors proposed a method to elucidate the decisions of an offline RL agent by attributing them to clusters of trajectories encountered during training. The original paper explored various environments and conducted a human study to gauge real-world performance. Our objective is to validate the effectiveness of their proposed approach. This paper conducted quantitative and qualitative experiments across three environments: a Grid-world, an Atari video game (Seaquest), and a continuous control task from MuJoCo (HalfCheetah). While the authors provided the code for the Grid-world environment, we re-implemented it for the Seaquest and HalfCheetah environments. This work extends the original paper by including trajectory rankings within a cluster, experimenting with alternative trajectory clustering, and expanding the human study. The results affirm the effectiveness of the method, both in its reproduction and in the additional experiments. However, the results of the human study suggest that the method's explanations are more challenging to interpret for humans in more complex environments. Our implementations can be found on GitHub",
    "checked": null,
    "id": "b444f3471e4027eac47211b77437a97af067820c",
    "semantic_title": "reproducibility study of \"explaining rl decisions with trajectories",
    "citation_count": 0,
    "authors": [
      "Clio Feng",
      "Colin Bot",
      "Bart den Boef",
      "Bart Aaldering"
    ]
  }
}