{
  "https://openreview.net/forum?id=A4RLpHPXCu": {
    "title": "Offset Unlearning for Large Language Models",
    "volume": "main",
    "abstract": "Despite the strong capabilities of Large Language Models (LLMs) to acquire knowledge from their training corpora, the memorization of sensitive information in the corpora such as copyrighted, biased, and private content has led to ethical and legal concerns. In response to these challenges, unlearning has emerged as a potential remedy for LLMs affected by problematic training data. However, previous unlearning techniques are either not applicable to black-box LLMs due to required access to model internal weights, or violate data protection principles by retaining sensitive data for inference-time correction. We propose $\\delta$-unlearning, an offset unlearning framework for black-box LLMs. Instead of tuning the black-box LLM itself, $\\delta$-unlearning learns the logit offset needed for unlearning by contrasting the logits from a pair of smaller models. Experiments demonstrate that $\\delta$-unlearning can effectively unlearn target data while maintaining similar or even stronger performance on general out-of-forget-scope tasks. $\\delta$-unlearning also effectively incorporates different unlearning algorithms, making our approach a versatile solution to adapting various existing unlearning algorithms to black-box LLMs",
    "checked": true,
    "id": "72010b7fe48301a38e8063109b8ef8fcfd573e05",
    "semantic_title": "offset unlearning for large language models",
    "citation_count": 14,
    "authors": [
      "James Y. Huang",
      "Wenxuan Zhou",
      "Fei Wang",
      "Fred Morstatter",
      "Sheng Zhang",
      "Hoifung Poon",
      "Muhao Chen"
    ]
  },
  "https://openreview.net/forum?id=oYmRiWCQ1W": {
    "title": "Rethinking MUSHRA: Addressing Modern Challenges in Text-to-Speech Evaluation",
    "volume": "main",
    "abstract": "Despite rapid advancements in TTS models, a consistent and robust human evaluation framework is still lacking. For example, MOS tests fail to differentiate between similar models, and CMOS's pairwise comparisons are time-intensive. The MUSHRA test is a promising alternative for evaluating multiple TTS systems simultaneously, but in this work we show that its reliance on matching human reference speech unduly penalises the scores of modern TTS systems that can exceed human speech quality. More specifically, we conduct a comprehensive assessment of the MUSHRA test, focusing on its sensitivity to factors such as rater variability, listener fatigue, and reference bias. Based on our extensive evaluation involving 492 human listeners across Hindi and Tamil we identify two primary shortcomings: (i) reference-matching bias, where raters are unduly influenced by the human reference, and (ii) judgement ambiguity, arising from a lack of clear fine-grained guidelines. To address these issues, we propose two refined variants of the MUSHRA test. The first variant enables fairer ratings for synthesized samples that surpass human reference quality. The second variant reduces ambiguity, as indicated by the relatively lower variance across raters. By combining these approaches, we achieve both more reliable and more fine-grained assessments. We also release MANGO, a massive dataset of 246,000 human ratings, the first-of-its-kind collection for Indian languages, aiding in analyzing human preferences and developing automatic metrics for evaluating TTS systems",
    "checked": true,
    "id": "28939a8f7fbc7a557b55b117b55759be9409f48c",
    "semantic_title": "rethinking mushra: addressing modern challenges in text-to-speech evaluation",
    "citation_count": 1,
    "authors": [
      "Praveen Srinivasa Varadhan",
      "amogh gulati",
      "Ashwin Sankar",
      "Srija Anand",
      "Anirudh Gupta",
      "Anirudh Mukherjee",
      "Shiva Kumar Marepally",
      "Ankur Bhatia",
      "Saloni Jaju",
      "Suvrat Bhooshan",
      "Mitesh M Khapra"
    ]
  },
  "https://openreview.net/forum?id=cCQKwd5MFP": {
    "title": "Part-aware Prompted Segment Anything Model for Adaptive Segmentation",
    "volume": "main",
    "abstract": "Precision medicine, such as patient-adaptive treatments assisted by medical image analysis, poses new challenges for image segmentation algorithms due to the large variability across different patients and the limited availability of annotated data for each patient. In this work, we propose a data-efficient segmentation method to address these challenges, namely $\\textit{\\textbf{P}art-aware}$ $\\textit{\\textbf{P}rompted}$ $\\textit{\\textbf{S}egment}$ $\\textit{\\textbf{A}nything}$ $\\textit{\\textbf{M}odel}$ ($\\mathbf{{P}^{2}SAM}$). Without any model fine-tuning, $\\text{P}^2\\text{SAM}$ enables seamless adaptation to any new patients relying only on one-shot patient-specific data. We introduce a novel part-aware prompt mechanism to select multiple-point prompts based on part-level features of the one-shot data, which can be extensively integrated into different promptable segmentation models, such as SAM and SAM 2. To further promote the robustness of the part-aware prompt mechanism, we propose a distribution-guided retrieval approach to determine the optimal number of part-level features for a specific case. $\\text{P}^2\\text{SAM}$ improves the performance by $\\texttt{+} 8.0\\%$ and $\\texttt{+} 2.0\\%$ mean Dice score for two different patient-adaptive segmentation applications, respectively. In addition, $\\text{P}^2\\text{SAM}$ also exhibits impressive generalizability in other adaptive segmentation tasks in the natural image domain, $\\textit{e.g.}$, $\\texttt{+} 6.4\\%$ mIoU within personalized object segmentation task. Code will be released upon acceptance",
    "checked": false,
    "id": "acab79684723aee3480dcc22b9d8c9882d44cdc3",
    "semantic_title": "part-aware personalized segment anything model for patient-specific segmentation",
    "citation_count": 3,
    "authors": [
      "Chenhui Zhao",
      "Liyue Shen"
    ]
  },
  "https://openreview.net/forum?id=kY2fKLOGkI": {
    "title": "On the Utility of Existing Fine-Tuned Models on Data-Scarce Domains",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have been observed to perform well on a wide range of downstream tasks when fine-tuned on domain-specific data. However, such data may not be readily available in many applications, motivating zero-shot or few-shot approaches using existing domain or task adjacent (fine-tuned) models, which we call DAFT. While several fine-tuned models for various tasks are available, finding one appropriate DAFT model for a given task is often not straight forward. In this paper, we explore different utilization techniques of these existing DAFT models for data-scarce problems, i.e., tasks for which data is not available or limited. We observe that for zero-shot problems, ensembling of DAFT models provides an accuracy performance close to that of the single best model. With few-shot problems (few data from target domain available), this performance can be improved further by picking or putting more weights to the DAFT models that are expected to perform better on the target task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Ibrahim Ibne Alam",
      "Parikshit Ram",
      "Soham Dan",
      "Horst Samulowitz",
      "Koushik Kar"
    ]
  },
  "https://openreview.net/forum?id=k8x44wVIs1": {
    "title": "Group Fair Federated Learning via Stochastic Kernel Regularization",
    "volume": "main",
    "abstract": "Ensuring \\textbf{group fairness} in federated learning (FL) presents unique challenges due to data heterogeneity and communication constraints. We propose Kernel Fair Federated Learning (\\texttt{KFFL}), a novel framework that incorporates group fairness into FL models using the Kernel Hilbert-Schmidt Independence Criterion (KHSIC) as a fairness regularizer. To address scalability, \\texttt{KFFL} approximates KHSIC with Random Feature Maps (RFMs), significantly reducing computational and communication overhead while achieving \\textit{group fairness}. To address the resulting non-convex optimization problem, we propose \\texttt{FedProxGrad}, a federated proximal gradient algorithm that guarantees convergence. Through experiments on standard benchmark datasets across both IID and Non-IID settings for regression and classification tasks, \\texttt{KFFL} demonstrates its ability to balance accuracy and fairness effectively, outperforming existing methods by comprehensively exploring the Pareto Frontier. Furthermore, we introduce \\texttt{KFFL-TD}, a time-delayed variant that further reduces communication rounds, enhancing efficiency in decentralized environments",
    "checked": false,
    "id": "062934e018b815b33d8186fe0a8257609d88e8f2",
    "semantic_title": "global group fairness in federated learning via function tracking",
    "citation_count": 0,
    "authors": [
      "Huzaifa Arif",
      "Pin-Yu Chen",
      "Keerthiram Murugesan",
      "Alex Gittens"
    ]
  },
  "https://openreview.net/forum?id=quE8gDDegf": {
    "title": "Exploring Weak-to-Strong Generalization for CLIP-based Classification",
    "volume": "main",
    "abstract": "Aligning large-scale commercial models with user intent is crucial to preventing harmful outputs. Current methods rely on human supervision but become impractical as model complexity increases. When models surpass human knowledge, providing accurate feedback becomes challenging and inefficient. A novel solution proposed recently is using a weaker model to supervise a stronger model. This concept leverages the ability of weaker models to perform evaluations, thereby reducing the workload on human supervisors. Previous work has shown the effectiveness of weak-to-strong generalization in the context of language-only models. Extending this concept to vision-language models leverages these insights, adapting the proven benefits to a multi-modal context. In our study, we explore weak-to-strong generalization for CLIP-based classification. We propose a method, \\emph{class prototype learning} (CPL), which aims to enhance the classification capabilities of the CLIP model, by learning more representative prototypes for each category. Our findings indicate that, despite using a simple loss function under weak supervision, CPL yields robust improvements in targeted scenarios, particularly when pretraining is limited. Extensive experiments demonstrate that our approach is effective under these settings, achieving a 3.67\\% improvement over strong baseline methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhao Li",
      "Sarah Monazam Erfani",
      "Lei Feng",
      "James Bailey",
      "Feng Liu"
    ]
  },
  "https://openreview.net/forum?id=OjWB2671AR": {
    "title": "Deep Augmentation: Dropout as Augmentation for Self-Supervised Learning",
    "volume": "main",
    "abstract": "Despite dropout's ubiquity in machine learning, its effectiveness as a form of data augmentation remains under-explored. We address two key questions: (i) When is dropout effective as an augmentation strategy? (ii) Is dropout uniquely effective under these conditions? To explore these questions, we propose Deep Augmentation, a network- and modality-agnostic method that applies dropout or PCA transformations to targeted layers in neural networks. Through extensive experiments on contrastive learning tasks in NLP, computer vision, and graph learning, we find that uniformly applying dropout across layers does not consistently improve performance. Instead, dropout proves most beneficial in deeper layers and can be matched by alternative augmentations (e.g., PCA). We also show that a stop-gradient operation is critical for ensuring dropout functions effectively as an augmentation, and that performance trends invert when moving from contrastive tasks to supervised tasks. Our analysis suggests that Deep Augmentation helps mitigate inter-layer co-adaptation---a notable issue in self-supervised learning due to the absence of labeled data. Drawing on these insights, we outline a procedure for selecting the optimal augmentation layer and demonstrate that Deep Augmentation can outperform traditional input-level augmentations. This simple yet powerful approach can be seamlessly integrated into a wide range of architectures and modalities, yielding notable gains in both performance and generalization",
    "checked": true,
    "id": "1869e4b3ec0a248c6ba360bf17706621207d310e",
    "semantic_title": "deep augmentation: dropout as augmentation for self-supervised learning",
    "citation_count": 0,
    "authors": [
      "Rickard Brüel Gabrielsson",
      "Tongzhou Wang",
      "Manel Baradad",
      "Justin Solomon"
    ]
  },
  "https://openreview.net/forum?id=haf78jerSt": {
    "title": "Forecasting Company Fundamentals",
    "volume": "main",
    "abstract": "Company fundamentals are key to assessing companies' financial and overall success and stability. Forecasting them is important in multiple fields, including investing and econometrics. While statistical and contemporary machine learning methods have been applied to many time series tasks, there is a lack of comparison of these approaches on this particularly challenging data regime. To this end, we try to bridge this gap and thoroughly evaluate the theoretical properties and practical performance of 24 deterministic and probabilistic company fundamentals forecasting models on real company data. We observe that deep learning models provide superior forecasting performance to classical models, in particular when considering uncertainty estimation. To validate the findings, we compare them to human analyst expectations and find that their accuracy is comparable to the automatic forecasts. We further show how these high-quality forecasts can benefit automated stock allocation. We close by presenting possible ways of integrating domain experts to further improve performance and increase reliability",
    "checked": true,
    "id": "cef84fc1987e0e6fa3f675c2eefb68061445ba60",
    "semantic_title": "forecasting company fundamentals",
    "citation_count": 0,
    "authors": [
      "Felix Divo",
      "Eric Endress",
      "Kevin Endler",
      "Kristian Kersting",
      "Devendra Singh Dhami"
    ]
  },
  "https://openreview.net/forum?id=6g1WJ55N51": {
    "title": "ETGL-DDPG: A Deep Deterministic Policy Gradient Algorithm for Sparse Reward Continuous Control",
    "volume": "main",
    "abstract": "We consider deep deterministic policy gradient (DDPG) in the context of reinforcement learning with sparse rewards. To enhance exploration, we introduce a search procedure, \\emph{${\\epsilon}{t}$-greedy}, which generates exploratory options for exploring less-visited states. We prove that search using $\\epsilon t$-greedy has polynomial sample complexity under mild MDP assumptions. To more efficiently use the information provided by rewarded transitions, we develop a new dual experience replay buffer framework, \\emph{GDRB}, and implement \\emph{longest n-step returns}. The resulting algorithm, \\emph{ETGL-DDPG}, integrates all three techniques: \\bm{$\\epsilon t$}-greedy, \\textbf{G}DRB, and \\textbf{L}ongest $n$-step, into DDPG. We evaluate ETGL-DDPG on standard benchmarks and demonstrate that it outperforms DDPG, as well as other state-of-the-art methods, across all tested sparse-reward continuous environments. Ablation studies further highlight how each strategy individually enhances the performance of DDPG in this setting",
    "checked": true,
    "id": "331586853f0b2f45d66fa8bde7bdc5dde07725cf",
    "semantic_title": "etgl-ddpg: a deep deterministic policy gradient algorithm for sparse reward continuous control",
    "citation_count": 1,
    "authors": [
      "Ehsan Futuhi",
      "Shayan Karimi",
      "Chao Gao",
      "Martin Müller"
    ]
  },
  "https://openreview.net/forum?id=kHl4JzyNzF": {
    "title": "Music Foundation Model as Generic Booster for Music Downstream Tasks",
    "volume": "main",
    "abstract": "We demonstrate the efficacy of using intermediate representations from a single foundation model to enhance various music downstream tasks. We introduce SoniDo, a music foundation model (MFM) designed to extract hierarchical features from target music samples. By leveraging hierarchical intermediate features, SoniDo constrains the information granularity, leading to improved performance across various downstream tasks including both understanding and generative tasks. We specifically evaluated this approach on representative tasks such as music tagging, music transcription, music source separation, and music mixing. Our results reveal that the features extracted from foundation models provide valuable enhancements in training downstream task models. This highlights the capability of using features extracted from music foundation models as a booster for downstream tasks. Our approach not only benefits existing task-specific models but also supports music downstream tasks constrained by data scarcity. This paves the way for more effective and accessible music processing solutions",
    "checked": true,
    "id": "fa689259129b052f5f97934128d476f7c82d4a23",
    "semantic_title": "music foundation model as generic booster for music downstream tasks",
    "citation_count": 1,
    "authors": [
      "Wei-Hsiang Liao",
      "Yuhta Takida",
      "Yukara Ikemiya",
      "Zhi Zhong",
      "Chieh-Hsin Lai",
      "Giorgio Fabbro",
      "Kazuki Shimada",
      "Keisuke Toyama",
      "Kin Wai Cheuk",
      "Marco A. Martínez-Ramírez",
      "Shusuke Takahashi",
      "Stefan Uhlich",
      "Taketo Akama",
      "Woosung Choi",
      "Yuichiro Koyama",
      "Yuki Mitsufuji"
    ]
  },
  "https://openreview.net/forum?id=pdC092Nn8N": {
    "title": "Studying Exploration in RL: An Optimal Transport Analysis of Occupancy Measure Trajectories",
    "volume": "main",
    "abstract": "The rising successes of RL are propelled by combining smart algorithmic strategies and deep architectures to optimize the distribution of returns and visitations over the state-action space. A quantitative framework to compare the learning processes of these eclectic RL algorithms is currently absent but desired in practice. We address this gap by representing the learning process of an RL algorithm as a sequence of policies generated during training, and then studying the policy trajectory induced in the manifold of state-action occupancy measures. Using an optimal transport-based metric, we measure the length of the paths induced by the policy sequence yielded by an RL algorithm between an initial policy and a final optimal policy. Hence, we first define the Effort of Sequential Learning (ESL). ESL quantifies the relative distance that an RL algorithm travels compared to the shortest path from the initial to the optimal policy. Furthermore, we connect the dynamics of policies in the occupancy measure space and regret (another metric to understand the suboptimality of an RL algorithm), by defining the Optimal Movement Ratio (OMR). OMR assesses the fraction of movements in the occupancy measure space that effectively reduce an analogue of regret. Finally, we derive approximation guarantees to estimate ESL and OMR with a finite number of samples and without access to an optimal policy. Through empirical analyses across various environments and algorithms, we demonstrate that ESL and OMR provide insights into the exploration processes of RL algorithms and the hardness of different tasks in discrete and continuous MDPs",
    "checked": false,
    "id": "5d95543ad4f9d8a9901302d133a91d8bd6d629fa",
    "semantic_title": "how does your rl agent explore? an optimal transport analysis of occupancy measure trajectories",
    "citation_count": 1,
    "authors": [
      "Reabetswe M. Nkhumise",
      "Debabrota Basu",
      "Tony J. Prescott",
      "Aditya Gilra"
    ]
  },
  "https://openreview.net/forum?id=gG8sQUUtN7": {
    "title": "LASP: Linear Attention Sequence Parallelism",
    "volume": "main",
    "abstract": "Sequence parallelism (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single device. However, for linear sequence modeling methods like linear attention, existing SP approaches do not take advantage of their right-product-first feature, resulting in sub-optimal communication efficiency and usability. In this paper, we introduce Linear Attention Sequence Parallelism (LASP), an efficient SP approach designed for linear attention-based transformer models. Specifically, we design an efficient point-to-point ring-style communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead, comparing with existing SP methods. We enhance the computation efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPUs. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with very-long sequences. We also discuss the generalization of LASP on other linear sequence modeling methods. Extensive experiments on linear attention-based models are conducted with varying sequence lengths from 2K to 4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$ longer than existing SP methods. Code is available at: \\url{https://github.com/OpenNLPLab/LASP}",
    "checked": false,
    "id": "660d80773f55c5dc889de3ce93b71672133a91bc",
    "semantic_title": "linear attention sequence parallelism",
    "citation_count": 2,
    "authors": [
      "Weigao Sun",
      "Zhen Qin",
      "Dong Li",
      "Xuyang Shen",
      "Yu Qiao",
      "Yiran Zhong"
    ]
  },
  "https://openreview.net/forum?id=PHsfZnF2FC": {
    "title": "MOORL: A Framework for Integrating Offline-Online Reinforcement Learning",
    "volume": "main",
    "abstract": "Sample efficiency and exploration remain critical challenges in Deep Reinforcement Learning (DRL), particularly in complex domains. Offline RL, which enables agents to learn optimal policies from static, pre-collected datasets, has emerged as a promising alternative. However, offline RL is constrained by issues such as out-of-distribution (OOD) actions that limit policy performance and generalization. To overcome these limitations, we propose Meta Offline-Online Reinforcement Learning (MOORL), a hybrid framework that unifies offline and online RL for efficient and scalable learning. While previous hybrid methods rely on extensive design choices and added complexity to utilize offline data effectively, MOORL introduces a meta-policy that seamlessly adapts across offline and online trajectories. This enables the agent to leverage offline data for robust initialization while utilizing online interactions to drive efficient exploration. Importantly, MOORL addresses the key challenges of hybrid RL in terms of being design-free. Our theoretical analysis demonstrates that the hybrid approach enhances exploration by effectively combining the complementary strengths of offline and online data. Furthermore, we demonstrate that MOORL learns a stable Q-function without relying on extensive design choices. Extensive experiments on 28 tasks from the D4RL and V-D4RL benchmarks validate its effectiveness, showing consistent improvements over state-of-the-art offline and hybrid RL baselines. With minimal computational overhead, MOORL achieves strong performance, underscoring its potential for practical applications in real-world scenarios",
    "checked": false,
    "id": "6e82b1838165f80d7a2d945046e583c3c1e48f39",
    "semantic_title": "energy-efficient online path planning for internet of drones using reinforcement learning",
    "citation_count": 3,
    "authors": [
      "Gaurav Chaudhary",
      "Washim Uddin Mondal",
      "Laxmidhar Behera"
    ]
  },
  "https://openreview.net/forum?id=2NSb3cJE03": {
    "title": "Time-Uniform Confidence Spheres for Means of Random Vectors",
    "volume": "main",
    "abstract": "We study sequential mean estimation in $\\mathbb{R}^d$. In particular, we derive time-uniform confidence spheres---\\emph{confidence sphere sequences} (CSSs)---which contain the mean of random vectors with high probability simultaneously across all sample sizes. Our results include a dimension-free CSS for log-concave random vectors, a dimension-free CSS for sub-Gaussian random vectors, and CSSs for sub-$\\psi$ random vectors (which includes sub-gamma, and sub-exponential distributions). Many of our results are optimal. For sub-Gaussian distributions we also provide a CSS which tracks a time-varying mean, generalizing Robbins' mixture approach to the multivariate setting. Finally, we provide several CSSs for heavy-tailed random vectors (two moments only). Our bounds hold under a martingale assumption on the mean and do not require that the observations be iid. Our work is based on PAC-Bayesian theory and inspired by an approach of Catoni and Giulini",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Chugg",
      "Hongjian Wang",
      "Aaditya Ramdas"
    ]
  },
  "https://openreview.net/forum?id=6BlOCx5c5T": {
    "title": "How far away are truly hyperparameter-free learning algorithms?",
    "volume": "main",
    "abstract": "Despite major advances in methodology, hyperparameter tuning remains a crucial (and expensive) part of the development of machine learning systems. Even ignoring architectural choices, deep neural networks have a large number of optimization and regularization hyperparameters that need to be tuned carefully per workload in order to obtain the best results. In a perfect world, training algorithms would not require workload-specific hyperparameter tuning, but would instead have default settings that performed well across many workloads. Recently, there has been a growing literature on optimization methods which attempt to reduce the number of hyperparameters---particularly the learning rate and its accompanying schedule. Given these developments, how far away is the dream of neural network training algorithms that completely obviate the need for painful tuning? In this paper, we evaluate the potential of learning-rate-free methods as components of hyperparameter-free methods. We freeze their (non-learning rate) hyperparameters to default values, and score their performance using the recently-proposed AlgoPerf: Training Algorithms benchmark. We found that literature-supplied default settings performed poorly on the benchmark, so we performed a search for hyperparameter configurations that performed well across all workloads simultaneously. The best \"algoperf-calibrated\" learning-rate-free methods had much improved performance but still lagged slightly behind a similarly calibrated NadamW baseline in overall benchmark score. Our results suggest that there is still much room for improvement for learning-rate-free methods, and that testing against a strong, workload-agnostic baseline is important to improve hyperparameter reduction techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Priya Kasimbeg",
      "Vincent Roulet",
      "Naman Agarwal",
      "Sourabh Medapati",
      "Fabian Pedregosa",
      "Atish Agarwala",
      "George E. Dahl"
    ]
  },
  "https://openreview.net/forum?id=VTgixSbrJI": {
    "title": "Hitchhiker's guide on the relation of Energy-Based Models with other generative models, sampling and statistical physics: a comprehensive review",
    "volume": "main",
    "abstract": "Energy-Based Models (EBMs) have emerged as a powerful framework in the realm of generative modeling, offering a unique perspective that aligns closely with principles of statistical mechanics. This review aims to provide physicists with a comprehensive understanding of EBMs, delineating their connection to other generative models such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Normalizing Flows. We explore the sampling techniques crucial for EBMs, including Markov Chain Monte Carlo (MCMC) methods, and draw parallels between EBM concepts and statistical mechanics, highlighting the significance of energy functions and partition functions. Furthermore, we delve into state-of-the-art training methodologies for EBMs, covering recent advancements and their implications for enhanced model performance and efficiency. This review is designed to clarify the often complex interconnections between these models, which can be challenging due to the diverse communities working on the topic",
    "checked": true,
    "id": "6d9a3d849928c1e569a5fc10ff72241edfe42b15",
    "semantic_title": "hitchhiker's guide on the relation of energy-based models with other generative models, sampling and statistical physics: a comprehensive review",
    "citation_count": 1,
    "authors": [
      "Davide Carbone"
    ]
  },
  "https://openreview.net/forum?id=VPl3T43Hxb": {
    "title": "A Local Polyak-Łojasiewicz and Descent Lemma of Gradient Descent For Overparametrized Linear Models",
    "volume": "main",
    "abstract": "Most prior work on the convergence of gradient descent (GD) for overparameterized neural networks relies on strong assumptions on the step size (infinitesimal), the hidden-layer width (infinite), or the initialization (large, spectral, balanced). Recent efforts to relax these assumptions focus on two-layer linear networks trained with the squared loss. In this work, we derive a linear convergence rate for training two-layer linear neural networks with GD for general losses and under relaxed assumptions on the step size, width, and initialization. A key challenge in deriving this result is that classical ingredients for deriving convergence rates for nonconvex problems, such as the Polyak-Łojasiewicz (PL) condition and Descent Lemma, do not hold globally for overparameterized neural networks. Here, we prove that these two conditions hold locally with local constants that depend on the weights. Then, we provide bounds on these local constants, which depend on the initialization of the weights, the current loss, and the global PL and smoothness constants of the non-overparameterized model. Based on these bounds, we derive a linear convergence rate for GD. Our convergence analysis not only improves upon prior results but also suggests a better choice for the step size, as verified through our numerical experiments",
    "checked": false,
    "id": "ad848a24708cf438340f97023c091908789c94db",
    "semantic_title": "a local polyak-lojasiewicz and descent lemma of gradient descent for overparametrized linear models",
    "citation_count": 0,
    "authors": [
      "Ziqing Xu",
      "Hancheng Min",
      "Salma Tarmoun",
      "Enrique Mallada",
      "Rene Vidal"
    ]
  },
  "https://openreview.net/forum?id=p499xXaclC": {
    "title": "Pruning Feature Extractor Stacking for Cross-domain Few-shot Learning",
    "volume": "main",
    "abstract": "Combining knowledge from source domains to learn efficiently from a few labelled instances in a target domain is a transfer learning problem known as cross-domain few-shot learning (CDFSL). Feature extractor stacking (FES) is a state-of-the-art CDFSL method that maintains a collection of source domain feature extractors instead of a single universal extractor. FES uses stacked generalisation to build an ensemble from extractor snapshots saved during target domain fine-tuning. It outperforms several contemporary universal model-based CDFSL methods in the Meta-Dataset benchmark. However, it incurs higher storage cost because it saves a snapshot for every fine-tuning iteration for every extractor. In this work, we propose a bidirectional snapshot selection strategy for FES, leveraging its cross-validation process and the ordered nature of its snapshots, and demonstrate that a 95% snapshot reduction can be achieved while retaining the same level of accuracy",
    "checked": false,
    "id": "3201efc4b51f9ff94c04789daa07d3951d6a948d",
    "semantic_title": "soft weight pruning for cross-domain few-shot learning with unlabeled target data",
    "citation_count": 2,
    "authors": [
      "Hongyu Wang",
      "Eibe Frank",
      "Bernhard Pfahringer",
      "Geoff Holmes"
    ]
  },
  "https://openreview.net/forum?id=nuN1mRrrjX": {
    "title": "Cometh: A continuous-time discrete-state graph diffusion model",
    "volume": "main",
    "abstract": "Discrete-state denoising diffusion models led to state-of-the-art performance in graph generation, especially in the molecular domain. Recently, they have been transposed to continuous time, allowing more flexibility in the reverse process and a better trade-off between sampling efficiency and quality. Here, to leverage the benefits of both approaches, we propose Cometh, a continuous-time discrete-state graph diffusion model, tailored to the specificities of graph data. In addition, we also successfully replaced the set of structural encodings previously used in the discrete graph diffusion model with a single random-walk-based encoding, providing a simple and principled way to boost the model's expressive power. Empirically, we show that integrating continuous time leads to significant improvements across various metrics over state-of-the-art discrete-state diffusion models on a large set of molecular and non-molecular benchmark datasets. In terms of VUN samples, Cometh obtains a near-perfect performance of 99.5% on the planar graph dataset and outperforms DiGress by 12.6% on the large GuacaMol dataset",
    "checked": true,
    "id": "4f31bcb88a7e6f50c308bb5cf2ea808a115af93e",
    "semantic_title": "cometh: a continuous-time discrete-state graph diffusion model",
    "citation_count": 3,
    "authors": [
      "Antoine Siraudin",
      "Fragkiskos D. Malliaros",
      "Christopher Morris"
    ]
  },
  "https://openreview.net/forum?id=LDBjgS5Ez7": {
    "title": "Uniform Noise Distribution and Compact Clusters: Unveiling the Success of Self-Supervised Learning in Label Noise",
    "volume": "main",
    "abstract": "Label noise is ubiquitous in real-world datasets, posing significant challenges to machine learning models. While self-supervised learning (SSL) algorithms have empirically demonstrated effectiveness in learning noisy labels, the theoretical understanding of their effectiveness remains underexplored. In this paper, we present a theoretical framework to understand how SSL methods enhance learning with noisy labels, especially for the instance-dependent label noise. We reveal that the uniform and compact cluster structures induced by contrastive SSL play a crucial role in mitigating the adverse effects of label noise. Specifically, we theoretically show that a classifier trained on SSL-learned representations significantly outperforms one trained using traditional supervised learning methods. This results from two key merits of SSL representations over label noise: 1. Uniform Noise Distribution: Label noise becomes uniformly distributed over SSL representations with respect to the true class labels, rather than the noisy ones, leading to an easier learning task. 2. Enhanced Cluster Structure: SSL enhances the formation of well-separated and compact categorical clusters, increasing inter-class distances while tightening intra-class clusters. We further theoretically justify the benefits of training a classifier on such structured representations, demonstrating that it encourages the classifier trained on noisy data to be aligned with the optimal classifier. Extensive experiments validate the robustness of SSL representations in combating label noise, confirming the practical values of our theoretical findings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengcheng Xu",
      "Li Yi",
      "Gezheng Xu",
      "Xi Chen",
      "Ian McLeod",
      "Charles Ling",
      "Boyu Wang"
    ]
  },
  "https://openreview.net/forum?id=S6JpSsYBDZ": {
    "title": "RefinedFields: Radiance Fields Refinement for Planar Scene Representations",
    "volume": "main",
    "abstract": "Planar scene representations have recently witnessed increased interests for modeling scenes from images, as their lightweight planar structure enables compatibility with image-based models. Notably, K-Planes have gained particular attention as they extend planar scene representations to support in-the-wild scenes, in addition to object-level scenes. However, their visual quality has recently lagged behind that of state-of-the-art techniques. To reduce this gap, we propose RefinedFields, a method that leverages pre-trained networks to refine K-Planes scene representations via optimization guidance using an alternating training procedure. We carry out extensive experiments and verify the merit of our method on synthetic data and real tourism photo collections. RefinedFields enhances rendered scenes with richer details and improves upon its base representation on the task of novel view synthesis. Our project page can be found at https://refinedfields.github.io",
    "checked": false,
    "id": "dae11349e5059c02a702a161d31d8c2358d61bdb",
    "semantic_title": "refinedfields: radiance fields refinement for unconstrained scenes",
    "citation_count": 7,
    "authors": [
      "Karim Kassab",
      "Antoine Schnepf",
      "Jean-Yves Franceschi",
      "Laurent Caraffa",
      "Jeremie Mary",
      "Valerie Gouet-Brunet"
    ]
  },
  "https://openreview.net/forum?id=NeQYi56MFj": {
    "title": "M3CoL: Harnessing Shared Relations via Multimodal Mixup Contrastive Learning for Multimodal Classification",
    "volume": "main",
    "abstract": "Deep multimodal learning has shown remarkable success by leveraging contrastive learning to capture explicit one-to-one relations across modalities. However, real-world data often exhibits shared relations beyond simple pairwise associations. We propose M3CoL, a Multimodal Mixup Contrastive Learning approach to capture nuanced shared relations inherent in multimodal data. Our key contribution is a Mixup-based contrastive loss that learns robust representations by aligning mixed samples from one modality with their corresponding samples from other modalities thereby capturing shared relations between them. For multimodal classification tasks, we introduce a framework that integrates a fusion module with unimodal prediction modules for auxiliary supervision during training, complemented by our proposed Mixup-based contrastive loss. Through extensive experiments on diverse datasets (N24News, ROSMAP, BRCA, and Food-101), we demonstrate that M3CoL effectively captures shared multimodal relations and generalizes across domains. It outperforms state-of-the-art methods on N24News, ROSMAP, and BRCA, while achieving comparable performance on Food-101. Our work highlights the significance of learning shared relations for robust multimodal learning, opening up promising avenues for future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raja Kumar",
      "Raghav Singhal",
      "Pranamya Prashant Kulkarni",
      "Deval Mehta",
      "Kshitij Sharad Jadhav"
    ]
  },
  "https://openreview.net/forum?id=N2rWhTgits": {
    "title": "Guided Discrete Diffusion for Electronic Health Record Generation",
    "volume": "main",
    "abstract": "Electronic health records (EHRs) are a pivotal data source that enables numerous applications in computational medicine, e.g., disease progression prediction, clinical trial design, and health economics and outcomes research. Despite wide usability, their sensitive nature raises privacy and confidentially concerns, which limit potential use cases. To tackle these challenges, we explore the use of generative models to synthesize artificial, yet realistic EHRs. While diffusion-based methods have recently demonstrated state-of-the-art performance in generating other data modalities and overcome the training instability and mode collapse issues that plague previous GAN-based approaches, their applications in EHR generation remain underexplored. The discrete nature of tabular medical code data in EHRs poses challenges for high-quality data generation, especially for continuous diffusion models. To this end, we introduce a novel tabular EHR generation method, EHR-D3PM, which enables both unconditional and conditional generation using the discrete diffusion model. Our experiments demonstrate that EHR-D3PM significantly outperforms existing generative baselines on comprehensive fidelity and utility metrics while maintaining less attribute and membership vulnerability risks. Furthermore, we show EHR-D3PM is effective as a data augmentation method and enhances performance on downstream tasks when combined with real data",
    "checked": true,
    "id": "3643812e72325a7d92e57b21ad2cb24faf563f30",
    "semantic_title": "guided discrete diffusion for electronic health record generation",
    "citation_count": 6,
    "authors": [
      "Jun Han",
      "Zixiang Chen",
      "Yongqian Li",
      "Yiwen Kou",
      "Eran Halperin",
      "Robert E. Tillman",
      "Quanquan Gu"
    ]
  },
  "https://openreview.net/forum?id=sq5AJvVuha": {
    "title": "DyGMamba: Efficiently Modeling Long-Term Temporal Dependency on Continuous-Time Dynamic Graphs with State Space Models",
    "volume": "main",
    "abstract": "Learning useful representations for continuous-time dynamic graphs (CTDGs) is challenging, due to the concurrent need to span long node interaction histories and grasp nuanced temporal details. In particular, two problems emerge: (1) Encoding longer histories requires more computational resources, making it crucial for CTDG models to maintain low computational complexity to ensure efficiency; (2) Meanwhile, more powerful models are needed to identify and select the most critical temporal information within the extended context provided by longer histories. To address these problems, we propose a CTDG representation learning model named DyGMamba, originating from the popular Mamba state space model (SSM). DyGMamba first leverages a node-level SSM to encode the sequence of historical node interactions. Another time-level SSM is then employed to exploit the temporal patterns hidden in the historical graph, where its output is used to dynamically select the critical information from the interaction history. We validate DyGMamba experimentally on the dynamic link prediction task. The results show that our model achieves state-of-the-art in most cases. DyGMamba also maintains high efficiency in terms of computational resources, making it possible to capture long temporal dependencies with a limited computation budget",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zifeng Ding",
      "Yifeng Li",
      "Yuan He",
      "Antonio Norelli",
      "Jingcheng Wu",
      "Volker Tresp",
      "Michael M. Bronstein",
      "Yunpu Ma"
    ]
  },
  "https://openreview.net/forum?id=5f7YlSKG1l": {
    "title": "Towards identifiability of micro total effects in summary causal graphs with latent confounding: extension of the front-door criterion",
    "volume": "main",
    "abstract": "Conducting experiments to estimate total effects can be challenging due to cost, ethical concerns, or practical limitations. As an alternative, researchers often rely on causal graphs to determine whether these effects can be identified from observational data. Identifying total effects in fully specified causal graphs has received considerable attention, with Pearl's front-door criterion enabling the identification of total effects in the presence of latent confounding even when no variable set is sufficient for adjustment. However, specifying a complete causal graph is challenging in many domains. Extending these identifiability results to partially specified graphs is crucial, particularly in dynamic systems where causal relationships evolve over time. This paper addresses the challenge of identifying total effects using a specific and well-known partially specified graph in dynamic systems called a summary causal graph, which does not specify the temporal lag between causal relations and can contain cycles. In particular, this paper presents sufficient graphical conditions for identifying total effects from observational data, even in the presence of cycles and latent confounding, and when no variable set is sufficient for adjustment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charles K. Assaad"
    ]
  },
  "https://openreview.net/forum?id=ZrqLpXbXvA": {
    "title": "Explaining the Behavior of Black-Box Prediction Algorithms with Causal Learning",
    "volume": "main",
    "abstract": "Causal approaches to post-hoc explainability for black-box prediction models (e.g., deep neural networks trained on image pixel data) have become increasingly popular. However, existing approaches have two important shortcomings: (i) the \"explanatory units\" are micro-level inputs into the relevant prediction model, e.g., image pixels, rather than interpretable macro-level features that are more useful for understanding how to possibly change the algorithm's behavior, and (ii) existing approaches assume there exists no unmeasured confounding between features and target model predictions, which fails to hold when the explanatory units are macro-level variables. Our focus is on the important setting where the analyst has no access to the inner workings of the target prediction algorithm, rather only the ability to query the output of the model in response to a particular input. To provide causal explanations in such a setting, we propose to learn causal graphical representations that allow for arbitrary unmeasured confounding among features. We demonstrate the resulting graph can differentiate between interpretable features that causally influence model predictions versus those that are merely associated with model predictions due to confounding. Our approach is motivated by a counterfactual theory of causal explanation wherein good explanations point to factors that are \"difference-makers\" in an interventionist sense",
    "checked": false,
    "id": "6e2836fc572ef2f9d2965e64ef5ba17d7eb48d03",
    "semantic_title": "petrophysical prediction of oil reservoirs using explainable artificial intelligence",
    "citation_count": 0,
    "authors": [
      "Numair Sani",
      "Daniel Malinsky",
      "Ilya Shpitser"
    ]
  },
  "https://openreview.net/forum?id=z3JZzu9EA3": {
    "title": "A Survey on Large Language Model Acceleration based on KV Cache Management",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, model-level, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications",
    "checked": true,
    "id": "6bcd708d2e49b34f34f157daa6bf1c3e062f57c5",
    "semantic_title": "a survey on large language model acceleration based on kv cache management",
    "citation_count": 15,
    "authors": [
      "Haoyang LI",
      "Yiming Li",
      "Anxin Tian",
      "Tianhao Tang",
      "Zhanchao Xu",
      "Xuejia Chen",
      "Nicole HU",
      "Wei Dong",
      "Li Qing",
      "Lei Chen"
    ]
  },
  "https://openreview.net/forum?id=CkVlt2Qgdb": {
    "title": "Investigating the Effects of Fairness Interventions Using Pointwise Representational Similarity",
    "volume": "main",
    "abstract": "Machine learning (ML) algorithms can often exhibit discriminatory behavior, negatively affecting certain populations across protected groups. To address this, numerous debiasing methods, and consequently evaluation measures, have been proposed. Current evaluation measures for debiasing methods suffer from two main limitations: (1) they primarily provide a global estimate of unfairness, failing to provide a more fine-grained analysis, and (2) they predominantly analyze the model output on a specific task, failing to generalize the findings to other tasks. In this work, we introduce Pointwise Normalized Kernel Alignment (PNKA), a pointwise representational similarity measure that addresses these limitations by measuring how debiasing measures affect the intermediate representations of individuals. On tabular data, the use of PNKA reveals previously unknown insights: while group fairness predominantly influences a small subset of the population, maintaining high representational similarity for the majority, individual fairness constraints uniformly impact representations across the entire population, altering nearly every data point. We show that by evaluating representations using PNKA, we can reliably predict the behavior of ML models trained on these representations. Moreover, applying PNKA to language embeddings shows that existing debiasing methods may not perform as intended, failing to remove biases from stereotypical words and sentences. Our findings suggest that current evaluation measures for debiasing methods are insufficient, highlighting the need for a deeper understanding of the effects of debiasing methods, and show how pointwise representational similarity metrics can help with fairness audits",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Camila Kolling",
      "Till Speicher",
      "Vedant Nanda",
      "Mariya Toneva",
      "Krishna P. Gummadi"
    ]
  },
  "https://openreview.net/forum?id=ZgjhykPSdU": {
    "title": "Statistical Error Bounds for GANs with Nonlinear Objective Functionals",
    "volume": "main",
    "abstract": "Generative adversarial networks (GANs) are unsupervised learning methods for training a generator distribution to produce samples that approximate those drawn from a target distribution. Many such methods can be formulated as minimization of a metric or divergence between probability distributions. Recent works have derived statistical error bounds for GANs that are based on integral probability metrics (IPMs), e.g., WGAN which is based on the 1-Wasserstein metric. In general, IPMs are defined by optimizing a linear functional (difference of expectations) over a space of discriminators. A much larger class of GANs, which we here call $(f,\\Gamma)$-GANs, can be constructed using $f$-divergences (e.g., Jensen-Shannon, KL, or $\\alpha$-divergences) together with a regularizing discriminator space $\\Gamma$ (e.g., $1$-Lipschitz functions). These GANs have nonlinear objective functions, depending on the choice of $f$, and have been shown to exhibit improved performance in a number of applications. In this work we derive statistical error bounds for $(f,\\Gamma)$-GANs for general classes of $f$ and $\\Gamma$ in the form of finite-sample concentration inequalities. These results prove the statistical consistency of $(f,\\Gamma)$-GANs and reduce to the known results for IPM-GANs in the appropriate limit. Our results use novel Rademacher complexity bounds which provide new insight into the performance of IPM-GANs for distributions with unbounded support and have application to statistical learning tasks beyond GANs",
    "checked": true,
    "id": "53dae7b4af66074490adcc15f27a9444673faad4",
    "semantic_title": "statistical error bounds for gans with nonlinear objective functionals",
    "citation_count": 0,
    "authors": [
      "Jeremiah Birrell"
    ]
  },
  "https://openreview.net/forum?id=JkMifr17wc": {
    "title": "Closed-Form Diffusion Models",
    "volume": "main",
    "abstract": "Score-based generative models (SGMs) sample from a target distribution by iteratively transforming noise using the score function of the perturbed target. For any finite training set, this score function can be evaluated in closed form, but the resulting SGM memorizes its training data and does not generate novel samples. In practice, one approximates the score by training a neural network via score-matching. The error in this approximation promotes generalization, but neural SGMs are costly to train and sample, and the effective regularization this error provides is not well-understood theoretically. In this work, we instead explicitly smooth the closed-form score to obtain an SGM that generates novel samples without training. We analyze our model and propose an efficient nearest-neighbor-based estimator of its score function. Using this estimator, our method achieves competitive sampling times while running on consumer-grade CPUs",
    "checked": true,
    "id": "1242e11e676fe238157127ef2825b89c50d99d10",
    "semantic_title": "closed-form diffusion models",
    "citation_count": 9,
    "authors": [
      "Christopher Scarvelis",
      "Haitz Sáez de Ocáriz Borde",
      "Justin Solomon"
    ]
  },
  "https://openreview.net/forum?id=Oyueig10Ed": {
    "title": "Policy Optimization via Adv2: Adversarial Learning on Advantage Functions",
    "volume": "main",
    "abstract": "We revisit the reduction of learning in adversarial Markov decision processes [MDPs] to adversarial learning based on $Q$--values; this reduction has been considered in a number of recent articles as one building block to perform policy optimization. Namely, we first consider and extend this reduction in an ideal setting where an oracle provides value functions: it may involve any adversarial learning strategy (not just exponential weights) and it may be based indifferently on $Q$--values or on advantage functions. We then present two extensions: on the one hand, convergence of the last iterate for a vast class of adversarial learning strategies (again, not just exponential weights), satisfying a property called monotonicity of weights; on the other hand, stronger regret criteria for learning in MDPs, inherited from the stronger regret criteria of adversarial learning called strongly adaptive regret and tracking regret. Third, we demonstrate how adversarial learning, also referred to as aggregation of experts, relates to aggregation (orchestration) of expert policies: we obtain stronger forms of performance guarantees in this setting than existing ones, via yet another, simple reduction. Finally, we discuss the impact of the reduction of learning in adversarial MDPs to adversarial learning in the practical scenarios where transition kernels are unknown and value functions must be learned. In particular, we review the literature and note that many strategies for policy optimization feature a policy-improvement step based on exponential weights with estimated $Q$--values. Our main message is that this step may be replaced by the application of any adversarial learning strategy on estimated $Q$--values or on estimated advantage functions. We leave the empirical evaluation of these twists for future research",
    "checked": true,
    "id": "2c860df4131cb10fb94821cec993699fd2d46f50",
    "semantic_title": "policy optimization via adv2: adversarial learning on advantage functions",
    "citation_count": 2,
    "authors": [
      "Matthieu Jonckheere",
      "Chiara Mignacco",
      "Gilles Stoltz"
    ]
  },
  "https://openreview.net/forum?id=4xXJDO8Bvu": {
    "title": "Node Classification With Reject Option",
    "volume": "main",
    "abstract": "One of the key tasks in graph learning is node classification. While Graph neural networks have been used for various applications, their adaptivity to reject option settings has not been previously explored. In this paper, we propose NCwR, a novel approach to node classification in Graph Neural Networks (GNNs) with an integrated reject option. This allows the model to abstain from making predictions for samples with high uncertainty. We propose cost-based and coverage-based methods for classification with abstention in node classification settings using GNNs. We perform experiments using our method on standard citation network datasets Cora, CiteSeer, PubMed and ogbn-arxiv. We also model the Legal judgment prediction problem on the ILDC dataset as a node classification problem, where nodes represent legal cases and edges represent citations. We further interpret the model by analyzing the cases in which it abstains from predicting and visualizing which part of the input features influenced this decision",
    "checked": false,
    "id": "b4d975f1fedab4532ce01cc473a91e50ccb8c938",
    "semantic_title": "node classification with integrated reject option",
    "citation_count": 0,
    "authors": [
      "Uday Bhaskar Kuchipudi",
      "Jayadratha Gayen",
      "Charu Sharma",
      "Naresh Manwani"
    ]
  },
  "https://openreview.net/forum?id=xT8BEgXmVc": {
    "title": "Decentralized Transformers with Centralized Aggregation are Sample-Efficient Multi-Agent World Models",
    "volume": "main",
    "abstract": "Learning a world model for model-free Reinforcement Learning (RL) agents can significantly improve the sample efficiency by learning policies in imagination. However, building a world model for Multi-Agent RL (MARL) can be particularly challenging due to the scalability issue across different number of agents in a centralized architecture, and also the non-stationarity issue in a decentralized architecture stemming from the inter-dependency among agents. To address both challenges, we propose a novel world model for MARL that learns decentralized local dynamics for scalability, combined with a centralized representation aggregation from all agents. We cast the dynamics learning as an auto-regressive sequence modeling problem over discrete tokens by leveraging the expressive Transformer architecture, in order to model complex local dynamics across different agents and provide accurate and consistent long-term imaginations. As the first pioneering Transformer-based world model for multi-agent systems, we introduce a Perceiver Transformer as an effective solution to enable centralized representation aggregation within this context. Extensive results on Starcraft Multi-Agent Challenge (SMAC) and MAMujoco demonstrate superior sample efficiency and overall performance compared to strong model-free approaches and existing model-based methods",
    "checked": true,
    "id": "ba5420997282c19fa04db9cbe8d9e877ef4f73c4",
    "semantic_title": "decentralized transformers with centralized aggregation are sample-efficient multi-agent world models",
    "citation_count": 1,
    "authors": [
      "Yang Zhang",
      "Chenjia Bai",
      "Bin Zhao",
      "Junchi Yan",
      "Xiu Li",
      "Xuelong Li"
    ]
  },
  "https://openreview.net/forum?id=3cnpZ5SIjU": {
    "title": "Hard-Negative Sampling for Contrastive Learning: Optimal Representation Geometry and Neural- vs Dimensional-Collapse",
    "volume": "main",
    "abstract": "For a widely-studied data model and general loss and sample-hardening functions we prove that the losses of Supervised Contrastive Learning (SCL), Hard-SCL (HSCL), and Unsupervised Contrastive Learning (UCL) are minimized by representations that exhibit Neural-Collapse (NC), i.e., the class means form an Equiangular Tight Frame (ETF) and data from the same class are mapped to the same representation. We also prove that for any representation mapping, the HSCL and Hard-UCL (HUCL) losses are lower bounded by the corresponding SCL and UCL losses. In contrast to existing literature, our theoretical results for SCL do not require class-conditional independence of augmented views and work for a general loss function class that includes the widely used InfoNCE loss function. Moreover, our proofs are simpler, compact, and transparent. Similar to existing literature, our theoretical claims also hold for the practical scenario where batching is used for optimization. We empirically demonstrate, for the first time, that Adam optimization (with batching) of HSCL and HUCL losses with random initialization and suitable hardness levels can indeed converge to the NC-geometry if we incorporate unit-ball or unit-sphere feature normalization. Without incorporating hard-negatives or feature normalization, however, the representations learned via Adam suffer from Dimensional-Collapse (DC) and fail to attain the NC-geometry. These results exemplify the role of hard-negative sampling in contrastive representation learning and we conclude with several open theoretical problems for future work. The code can be found at https://github.com/rjiang03/HCL/tree/main",
    "checked": true,
    "id": "d97fda96d01250760a192547e2dd1357f1c40d3a",
    "semantic_title": "hard-negative sampling for contrastive learning: optimal representation geometry and neural- vs dimensional-collapse",
    "citation_count": 2,
    "authors": [
      "Ruijie Jiang",
      "Thuan Nguyen",
      "Shuchin Aeron",
      "Prakash Ishwar"
    ]
  },
  "https://openreview.net/forum?id=Ckh17xN2R2": {
    "title": "Infrastructure for AI Agents",
    "volume": "main",
    "abstract": "\\textbf{AI agents} plan and execute interactions in open-ended environments. For example, OpenAI's Operator can use a web browser to do product comparisons and buy online goods. To facilitate beneficial interactions and mitigate harmful ones, much research focuses on directly modifying agent behaviour. For example, developers can train agents to follow user instructions. This focus on direct modifications is useful, but insufficient. We will also need external protocols and systems that shape how agents interact with institutions and other actors. For instance, agents will need more efficient protocols to communicate with each other and form agreements. In addition, attributing an agent's actions to a particular human or other legal entity can help to establish trust, and also disincentivize misuse. Given this motivation, we propose the concept of \\textbf{agent infrastructure}: technical systems and shared protocols external to agents that are designed to mediate and influence their interactions with and impacts on their environments. Just as the Internet relies on protocols like HTTPS, our work argues that agent infrastructure will be similarly indispensable to ecosystems of agents. We identify three functions for agent infrastructure: 1) attributing actions, properties, and other information to specific agents, their users, or other actors; 2) shaping agents' interactions; and 3) detecting and remedying harmful actions from agents. We provide an incomplete catalog of research directions for such functions. For each direction, we include analysis of use cases, infrastructure adoption, relationships to existing (internet) infrastructure, limitations, and open questions. Making progress on agent infrastructure can prepare society for the adoption of more advanced agents",
    "checked": true,
    "id": "9504e2f28fd2316124d45bdb216f58781e1b81b6",
    "semantic_title": "infrastructure for ai agents",
    "citation_count": 3,
    "authors": [
      "Alan Chan",
      "Kevin Wei",
      "Sihao Huang",
      "Nitarshan Rajkumar",
      "Elija Perrier",
      "Seth Lazar",
      "Gillian K Hadfield",
      "Markus Anderljung"
    ]
  },
  "https://openreview.net/forum?id=WADLPccB6o": {
    "title": "Conformal Bounds on Full-Reference Image Quality for Imaging Inverse Problems",
    "volume": "main",
    "abstract": "In imaging inverse problems, we would like to know how close the recovered image is to the true image in terms of full-reference image quality (FRIQ) metrics like PSNR, SSIM, LPIPS, etc. This is especially important in safety-critical applications like medical imaging, where knowing that, say, the SSIM was poor could potentially avoid a costly misdiagnosis. But since we don't know the true image, computing FRIQ is non-trivial. In this work, we combine conformal prediction with approximate posterior sampling to construct bounds on FRIQ that are guaranteed to hold up to a user-specified error probability. We demonstrate our approach on image denoising and accelerated magnetic resonance imaging (MRI) problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeffrey Wen",
      "Rizwan Ahmad",
      "Philip Schniter"
    ]
  },
  "https://openreview.net/forum?id=k1eYngOvf0": {
    "title": "G-RepsNet: A Lightweight Construction of Equivariant Networks for Arbitrary Matrix Groups",
    "volume": "main",
    "abstract": "Group equivariance is a strong inductive bias useful in a wide range of deep learning tasks. However, constructing efficient equivariant networks for general groups and domains is difficult. Recent work by Finzi et al. directly solves the equivariance constraint for arbitrary matrix groups to obtain equivariant MLPs (EMLPs). But this method does not scale well and scaling is crucial in deep learning. Here, we introduce Group Representation Networks (G-RepsNets), a lightweight equivariant network for arbitrary matrix groups with features represented using tensor polynomials. The key insight in our design is that using tensor representations in the hidden layers of a neural network along with simple inexpensive tensor operations leads to scalable equivariant networks. Further, these networks are universal approximators of functions equivariant to orthogonal groups. We find G-RepsNet to be competitive to EMLP on several tasks with group symmetries such as $O(5)$, $O(1, 3)$, and $O(3)$ with scalars, vectors, and second-order tensors as data types. On image classification tasks, we find that G-RepsNet using second-order representations is competitive and often even outperforms sophisticated state-of-the-art equivariant models such as GCNNs and $E(2)$-CNNs. To further illustrate the generality of our approach, we show that G-RepsNet is competitive to G-FNO and EGNN on N-body predictions and solving PDEs respectively, while being efficient",
    "checked": true,
    "id": "9f24687521c534aaa822970176d6ea7e3cd2b164",
    "semantic_title": "g-repsnet: a lightweight construction of equivariant networks for arbitrary matrix groups",
    "citation_count": 0,
    "authors": [
      "Sourya Basu",
      "Suhas Lohit",
      "Matthew Brand"
    ]
  },
  "https://openreview.net/forum?id=4uPJN6yfY1": {
    "title": "Retrieve, Merge, Predict: Augmenting Tables with Data Lakes",
    "volume": "main",
    "abstract": "Machine-learning from a disparate set of tables, a data lake, requires assembling features by merging and aggregating tables. Data discovery can extend autoML to data tables by automating these steps. We present an in-depth analysis of such automated table augmentation for machine learning tasks, analyzing different methods for the three main steps: retrieving joinable tables, merging information, and predicting with the resultant table. We use two data lakes: Open Data US, a well-referenced real data lake, and a novel semi-synthetic dataset, YADL (Yet Another Data Lake), which we developed as a tool for benchmarking this data discovery task. Systematic exploration on both lakes outlines 1) the importance of accurately retrieving join candidates, 2) the efficiency of simple merging methods, and 3) the resilience of tree-based learners to noisy conditions. Our experimental environment is easily reproducible and based on open data, to foster more research on feature engineering, autoML, and learning in data lakes",
    "checked": true,
    "id": "33c4cea9159564a7bc4239f183c7b0bf6ecc4fff",
    "semantic_title": "retrieve, merge, predict: augmenting tables with data lakes",
    "citation_count": 2,
    "authors": [
      "Riccardo Cappuzzo",
      "Aimee Coelho",
      "Félix Lefebvre",
      "Paolo Papotti",
      "Gaël Varoquaux"
    ]
  },
  "https://openreview.net/forum?id=55593xywWG": {
    "title": "Foundation Models Meet Federated Learning: A One-shot Feature-sharing Method with Privacy and Performance Guarantees",
    "volume": "main",
    "abstract": "Adapting foundation models for downstream tasks via Federated Learning (FL) is a promising strategy for protecting privacy while leveraging the capability of foundation models. However, FL's iterative training and model transmission result in high communication costs and GPU memory demands, making large foundation models impractical for FL. This paper introduces a one-shot FL method with a server-side performance bound to enable foundation models by reducing communication costs and GPU memory requirements. Our approach, FedPFT (FL with Parametric Feature Transfer), involves clients learning and transferring parametric models for features extracted from frozen foundation models in a single round. Parametric models are then used to generate synthetic features at the server to train a classifier head. We evaluate FedPFT across eight vision datasets using three vision foundation models. Our findings demonstrate that FedPFT is agnostic to data heterogeneity and network topology and it enhances the communication-accuracy frontier up to 7.8\\%. Finally, we show FedPFT's compatibility with differential privacy and its resilience against reconstruction attacks. Our work highlights the capability of private, feature-sharing methods for one-shot knowledge transfer using foundation models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahdi Beitollahi",
      "Alex Bie",
      "Sobhan Hemati",
      "Leo Maxime Brunswic",
      "Xu Li",
      "Xi Chen",
      "Guojun Zhang"
    ]
  },
  "https://openreview.net/forum?id=XHXAvACdgv": {
    "title": "NITO: Neural Implicit Fields for Resolution-free and Domain-Adaptable Topology Optimization",
    "volume": "main",
    "abstract": "Structural topology optimization plays a crucial role in engineering by determining the optimal material layout within a design space to maximize performance under given constraints. We introduce Neural Implicit Topology Optimization (NITO), a deep learning regression approach to accelerate topology optimization tasks. We demonstrate that, compared to state-of-the-art diffusion models, NITO generates structures that are under 15% as structurally sub-optimal and does so ten times faster. Furthermore, we show that NITO is entirely resolution-free and domain-agnostic, offering a more scalable solution than the current fixed-resolution and domain-specific diffusion models. To achieve this state-of-the-art performance, NITO combines three key innovations. First, we introduce the Boundary Point Order-Invariant MLP (BPOM), which represents loads and supports in a sparse and domain-agnostic manner, allowing NITO to train on variable conditioning, domain shapes, and mesh resolutions. Second, we adopt a neural implicit field representation, which allows NITO to synthesize topologies of any shape or resolution. Finally, we propose an inference-time refinement step using a few steps of gradient-based optimization to enable NITO to achieve results comparable to direct optimization methods. These three innovations empower NITO with a precision and versatility that is currently unparalleled among competing deep learning approaches for topology optimization. Code & Data: https://github.com/ahnobari/NITO_Public",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amin Heyrani Nobari",
      "Lyle Regenwetter",
      "Giorgio Giannone",
      "Faez Ahmed"
    ]
  },
  "https://openreview.net/forum?id=tYjoHjShxF": {
    "title": "An Empirical Study of Pre-trained Model Selection for Out-of-Distribution Generalization and Calibration",
    "volume": "main",
    "abstract": "In out-of-distribution (OOD) generalization tasks, fine-tuning pre-trained models has become a prevalent strategy. Different from most prior work that has focused on advancing learning algorithms, we systematically examined how pre-trained model size, pre-training dataset size, and training strategies impact generalization and uncertainty calibration on downstream tasks. We evaluated 100 models across diverse pre-trained model sizes, five pre-training datasets, and five data augmentations through extensive experiments on four distribution shift datasets totaling over 120,000 GPU hours. Our results demonstrate the significant impact of pre-trained model selection, with optimal choices substantially improving OOD accuracy over algorithm improvement alone. Additionally, we find that larger models and bigger pre-training datasets not only enhance OOD performance but also improve calibration, helping to mitigate overconfidence, contrary to some prior studies that found modern deep networks to calibrate worse than classical shallow models. Our work underscores the overlooked importance of pre-trained model selection for out-of-distribution generalization and calibration",
    "checked": true,
    "id": "37f729f59495f001cee0eff1637afdb92f1ab3e7",
    "semantic_title": "an empirical study of pre-trained model selection for out-of-distribution generalization and calibration",
    "citation_count": 2,
    "authors": [
      "Hiroki Naganuma",
      "Ryuichiro Hataya",
      "Kotaro Yoshida",
      "Ioannis Mitliagkas"
    ]
  },
  "https://openreview.net/forum?id=Hy2KAldqAo": {
    "title": "Robust Offline Imitation Learning from Diverse Auxiliary Data",
    "volume": "main",
    "abstract": "Offline imitation learning enables learning a policy solely from a set of expert demonstrations, without any environment interaction. To alleviate the issue of distribution shift arising due to the small amount of expert data, recent works incorporate large numbers of auxiliary demonstrations alongside the expert data. However, the performance of these approaches rely on assumptions about the quality and composition of the auxiliary data, and they are rarely successful when those assumptions do not hold. To address this limitation, we propose Robust Offline Imitation from Diverse Auxiliary Data (ROIDA). ROIDA first identifies high-quality transitions from the entire auxiliary dataset using a learned reward function. These high-reward samples are combined with the expert demonstrations for weighted behavioral cloning. For lower-quality samples, ROIDA applies temporal difference learning to steer the policy towards high-reward states, improving long-term returns. This two-pronged approach enables our framework to effectively leverage both high and low-quality data without any assumptions. Extensive experiments validate that ROIDA achieves robust and consistent performance across multiple auxiliary datasets with diverse ratios of expert and non-expert demonstrations. ROIDA effectively leverages unlabeled auxiliary data, outperforming prior methods reliant on specific data assumptions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Udita Ghosh",
      "Dripta S. Raychaudhuri",
      "Jiachen Li",
      "Konstantinos Karydis",
      "Amit Roy-Chowdhury"
    ]
  },
  "https://openreview.net/forum?id=7KkytYYhMv": {
    "title": "Rethinking the Value of Training-Free Structured Pruning of LLMs",
    "volume": "main",
    "abstract": "This paper investigates the effectiveness of training-free structured pruning techniques for Large Language Models (LLMs), with a particular focus on depth and width pruning strategies. Through an extensive empirical evaluation across a diverse range of tasks, datasets and modalities, we reveal critical limitations in current pruning methods. While some tasks exhibit minimal performance degradation, others face significant deterioration, even at low pruning rates, contradicting prior findings that often rely on selective benchmarks. Our analysis also finds that depth pruning, despite its simplicity, usually outperforms the more granular width pruning approaches in maintaining downstream task performance. Our findings highlight that existing evaluations of pruned LLMs often overstate their effectiveness due to incomplete or limited evaluation tasks, necessitating a critical reassessment of the true value of pruning and emphasizing the need to explore more robust pruning algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nahush Lele",
      "Arnav Chavan",
      "Aryamaan Thakur",
      "Deepak Gupta"
    ]
  },
  "https://openreview.net/forum?id=Qhfw5CUVd7": {
    "title": "FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback",
    "volume": "main",
    "abstract": "Large Vision-Language Models (LVLMs) have demonstrated proficiency in tackling a variety of visual-language tasks. However, current LVLMs suffer from misalignment between text and image modalities which causes three kinds of hallucination problems, i.e., object existence, object attribute, and object relationship. To tackle this issue, existing methods mainly utilize Reinforcement Learning (RL) to align modalities in LVLMs. However, they still suffer from three main limitations: (1) General feedback can not indicate the hallucination type contained in the response; (2) Sparse rewards only give the sequence-level reward for the whole response; and (3)Annotation cost is time-consuming and labor-intensive. To handle these limitations, we propose an innovative method to align modalities in LVLMs through \\textbf{F}ine-\\textbf{G}rained \\textbf{A}rtificial \\textbf{I}ntelligence \\textbf{F}eedback (\\textbf{\\ours}), which mainly consists of three steps: AI-based Feedback Collection, Fine-grained Reward Model Training, and Reinforcement Learning with Fine-grained Reward. Finally, a novel fine-grained feedback module is integrated into the Proximal Policy Optimization (PPO) algorithm. Extensive experiments are conducted on hallucination and general benchmarks, demonstrating the superior performance of our proposed method. Notably, compared with previous models trained with the RL-based aligning method, our proposed method is effective even with fewer parameters",
    "checked": true,
    "id": "fa84ef486184eb1c3d63949b700342bbcaf7b0c7",
    "semantic_title": "fgaif: aligning large vision-language models with fine-grained ai feedback",
    "citation_count": 17,
    "authors": [
      "Liqiang Jing",
      "Xinya Du"
    ]
  },
  "https://openreview.net/forum?id=OE4P1tW8iQ": {
    "title": "Noise-free Loss Gradients: A Surprisingly Effective Baseline for Coreset Selection",
    "volume": "main",
    "abstract": "The exponential rise in size and complexity of deep learning models and datasets have resulted in a considerable demand for computational resources. Coreset selection is one of the methods to alleviate this rising demand. The goal is to select a subset from a large dataset to train a model that performs almost at par with the one trained on the large dataset while reducing computational time and resource requirements. Existing approaches either attempt to identify remarkable samples (e.g., Forgetting, Adversarial Deepfool, EL2N, etc.) that stand out from the rest or solve complex optimization (e.g., submodular maximization, OMP) problems to compose the coresets. This paper proposes a novel and intuitive approach to efficiently select a coreset based on the similarity of loss gradients. Our method works on the hypothesis that gradients of samples belonging to a given class will point in similar directions during the early training phase. Samples with most neighbours that produce similar gradient directions, in other words, that produce noise-free gradients, will represent that class. Through extensive experimentation, we have demonstrated the effectiveness of our approach in out-performing state-of-the-art coreset selection algorithms on a range of benchmark datasets from CIFAR-10 to ImageNet with architectures of varied complexity (ResNet-18, ResNet-50, VGG-16, ViT).We have also demonstrated the effectiveness of our approach in Generative Modelling by implementing coreset selection to reduce training time for various GAN models (DCGAN, MSGAN, SAGAN, SNGAN) for different datasets (CIFAR-10, CIFAR-100, Tiny ImageNet) while not impacting the performance metrics significantly. Source code is provided at URL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saumyaranjan Mohanty",
      "Chimata Anudeep",
      "Konda Reddy Mopuri"
    ]
  },
  "https://openreview.net/forum?id=BDPvuD5FTg": {
    "title": "Graph-based Confidence Calibration for Large Language Models",
    "volume": "main",
    "abstract": "Reliable confidence estimation is essential for enhancing the trustworthiness of large language models (LLMs), especially in high-stakes scenarios. Despite its importance, accurately estimating confidence in LLM responses remains a significant challenge. In this work, we propose using an auxiliary learning model to assess response correctness based on the self-consistency of multiple outputs generated by the LLM. Our method builds a consistency graph to represent the agreement among multiple responses and uses a graph neural network (GNN) to estimate the likelihood that each response is correct. Experiments demonstrate that this method has strong calibration performance on various benchmark datasets and generalizes well to out-of-domain cases",
    "checked": true,
    "id": "e1536547084406d9f9864cc2dc08ca46add4a30b",
    "semantic_title": "graph-based confidence calibration for large language models",
    "citation_count": 1,
    "authors": [
      "Yukun Li",
      "Sijia Wang",
      "Lifu Huang",
      "Liping Liu"
    ]
  },
  "https://openreview.net/forum?id=mjsoESaWDH": {
    "title": "Preferential Multi-Objective Bayesian Optimization",
    "volume": "main",
    "abstract": "Preferential Bayesian optimization (PBO) is a framework for optimizing a decision-maker's latent preferences over available design choices. While real-world problems often involve multiple conflicting objectives, existing PBO methods assume that preferences can be encoded by a single objective function. For instance, in the customization of robotic assistive devices, technicians aim to maximize user comfort while minimizing energy consumption to extend battery life. Likewise, in autonomous driving policy design, stakeholders must evaluate safety and performance trade-offs before committing to a policy. To bridge this gap, we introduce the first framework for PBO with multiple objectives. Within this framework, we propose dueling scalarized Thompson sampling (DSTS), a multi-objective generalization of the popular dueling Thompson sampling algorithm, which may also be of independent interest beyond our setting. We evaluate DSTS across four synthetic test functions and two simulated tasks—exoskeleton personalization and driving policy design—demonstrating that it outperforms several benchmarks. Finally, we prove that DSTS is asymptotically consistent. Along the way, we provide, to our knowledge, the first convergence guarantee for dueling Thompson sampling in single-objective PBO",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raul Astudillo",
      "Kejun Li",
      "Maegan Tucker",
      "Chu Xin Cheng",
      "Aaron Ames",
      "Yisong Yue"
    ]
  },
  "https://openreview.net/forum?id=xVEHiAZ7uR": {
    "title": "Adam-family Methods with Decoupled Weight Decay in Deep Learning",
    "volume": "main",
    "abstract": "In this paper, we investigate the convergence properties of a wide class of Adam-family methods for minimizing quadratically regularized nonsmooth nonconvex optimization problems, especially in the context of training nonsmooth neural networks with weight decay. Motivated by AdamW, we propose a novel framework for Adam-family methods with decoupled weight decay. Within our framework, the estimators for the first-order and second-order moments of stochastic subgradients are updated independently of the weight decay term. Under mild assumptions and with non-diminishing stepsizes for updating the primary optimization variables, we establish the convergence properties of our proposed framework. In addition, we show that our proposed framework encompasses a wide variety of well-known Adam-family methods, hence offering convergence guarantees for these methods in the training of nonsmooth neural networks. More importantly, compared to the existing results on the choices of the parameters for the moment terms in Adam, we show that our proposed framework provides more flexibility for these parameters. As a practical application of our proposed framework, we propose a novel Adam-family method named Adam with Decoupled Weight Decay (AdamD), and establish its convergence properties under mild conditions. Numerical experiments demonstrate that AdamD outperforms Adam and is comparable to AdamW, in the aspects of both generalization performance and efficiency",
    "checked": true,
    "id": "b4e7f304bfc61a4679961de80e37d1c6f53be90d",
    "semantic_title": "adam-family methods with decoupled weight decay in deep learning",
    "citation_count": 3,
    "authors": [
      "Kuangyu Ding",
      "Nachuan Xiao",
      "Kim-chuan Toh"
    ]
  },
  "https://openreview.net/forum?id=lmHh4FmPWZ": {
    "title": "Generalized Compressed Sensing for Image Reconstruction with Diffusion Probabilistic Models",
    "volume": "main",
    "abstract": "We examine the problem of selecting a small set of linear measurements for reconstructing high-dimensional signals. Well-established methods for optimizing such measurements include principal component analysis (PCA), independent component analysis (ICA) and compressed sensing (CS) based on random projections, all of which rely on axis- or subspace-aligned statistical characterization of the signal source. However, many naturally occurring signals, including photographic images, contain richer statistical structure. To exploit such structure, we introduce a general method for obtaining an optimized set of linear measurements for efficient image reconstruction, where the signal statistics are expressed by the prior implicit in a neural network trained to perform denoising (known as a ``diffusion model''). We demonstrate that the optimal measurements derived for two natural image datasets differ from those of PCA, ICA, or CS, and result in substantially lower mean squared reconstruction error. Interestingly, the marginal distributions of the measurement values are asymmetrical (skewed), substantially more so than those of previous methods. We also find that optimizing with respect to perceptual loss, as quantified by structural similarity (SSIM), leads to measurements different from those obtained when optimizing for MSE. Our results highlight the importance of incorporating the specific statistical regularities of natural signals when designing effective linear measurements",
    "checked": true,
    "id": "7ebf1eb74d4dae3e9bf91e5a34e1bef1f68c0645",
    "semantic_title": "generalized compressed sensing for image reconstruction with diffusion probabilistic models",
    "citation_count": 0,
    "authors": [
      "Ling-Qi Zhang",
      "Zahra Kadkhodaie",
      "Eero P Simoncelli",
      "David H. Brainard"
    ]
  },
  "https://openreview.net/forum?id=0c6iG28rRl": {
    "title": "Towards Better Understanding of In-Context Learning Ability from In-Context Uncertainty Quantification",
    "volume": "main",
    "abstract": "Predicting simple function classes has been widely used as a testbed for developing theory and understanding of the trained Transformer's in-context learning (ICL) ability. In this paper, we revisit the training of Transformers on linear regression tasks, and different from all the existing literature, we consider a bi-objective prediction task of predicting both the conditional expectation $\\mathbb{E}[Y|X]$ and the conditional variance Var$(Y|X)$. This additional uncertainty quantification objective provides a handle to (i) better design out-of-distribution experiments to distinguish ICL from in-weight learning (IWL) and (ii) make a better separation between the algorithms with and without using the prior information of the training distribution. Theoretically, we show that the trained Transformer reaches near Bayes optimum, suggesting the usage of the information of the training distribution. Our method can be extended to other cases. Specifically, with the Transformer's context window $S$, we prove a generalization bound of $\\tilde{\\mathcal{O}}(\\sqrt{\\min\\{S, T\\}/(n T)})$ on $n$ tasks with sequences of length $T$, providing sharper analysis compared to previous results of $\\tilde{\\mathcal{O}}(\\sqrt{1/n})$. Empirically, we illustrate that while the trained Transformer behaves as the Bayes-optimal solution as a natural consequence of supervised training in distribution, it does not necessarily perform a Bayesian inference when facing task shifts, in contrast to the \\textit{equivalence} between these two proposed in many existing literature. We also demonstrate the trained Transformer's ICL ability over covariate shift and prompt-length shift and interpret them as a generalization over a meta distribution",
    "checked": true,
    "id": "97417c75b2c8c54780d25c46ca06de1b2a320ddf",
    "semantic_title": "towards better understanding of in-context learning ability from in-context uncertainty quantification",
    "citation_count": 1,
    "authors": [
      "Shang Liu",
      "Zhongze Cai",
      "Guanting Chen",
      "Xiaocheng Li"
    ]
  },
  "https://openreview.net/forum?id=qRAjZuf48S": {
    "title": "A Theoretical Study of Neural Network Expressive Power via Manifold Topology",
    "volume": "main",
    "abstract": "A prevalent assumption regarding real-world data is that it lies on or close to a low-dimensional manifold. When deploying a neural network on data manifolds, the required size, i.e., the number of neurons of the network, heavily depends on the intricacy of the underlying latent manifold. While significant advancements have been made in understanding the geometric attributes of manifolds, it's essential to recognize that topology, too, is a fundamental characteristic of manifolds. In this study, we investigate network expressive power in terms of the latent data manifold. Integrating both topological and geometric facets of the data manifold, we present a size upper bound of ReLU neural networks",
    "checked": true,
    "id": "87fcdc4c168ace7d72b64c4e7448ceed3e806ee1",
    "semantic_title": "a theoretical study of neural network expressive power via manifold topology",
    "citation_count": 0,
    "authors": [
      "Jiachen Yao",
      "Lingjie Yi",
      "Mayank Goswami",
      "Chao Chen"
    ]
  },
  "https://openreview.net/forum?id=Gl6dF9soQo": {
    "title": "UniZero: Generalized and Efficient Planning with Scalable Latent World Models",
    "volume": "main",
    "abstract": "Learning predictive world models is crucial for enhancing the planning capabilities of reinforcement learning (RL) agents. Recently, MuZero-style algorithms, leveraging the value equivalence principle and Monte Carlo Tree Search (MCTS), have achieved superhuman performance in various domains. However, these methods struggle to scale in heterogeneous scenarios with diverse dependencies and task variability. To overcome these limitations, we introduce UniZero, a novel approach that employs a transformer-based world model to effectively learn a shared latent space. By concurrently predicting latent dynamics and decision-oriented quantities conditioned on the learned latent history, UniZero enables joint optimization of the long-horizon world model and policy, facilitating broader and more efficient planning in the latent space. We show that UniZero significantly outperforms existing baselines in benchmarks that require long-term memory. Additionally, UniZero demonstrates superior scalability in multitask learning experiments conducted on Atari benchmarks. In standard single-task RL settings, such as Atari and DMControl, UniZero matches or even surpasses the performance of current state-of-the-art methods. Finally, extensive ablation studies and visual analyses validate the effectiveness and scalability of UniZero's design choices. Our code is available at \\textcolor{magenta}{https://github.com/opendilab/LightZero}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Pu",
      "Yazhe Niu",
      "Zhenjie Yang",
      "Jiyuan Ren",
      "Hongsheng Li",
      "Yu Liu"
    ]
  },
  "https://openreview.net/forum?id=WvgoxpGpuU": {
    "title": "T2L: Efficient Zero-Shot Action Recognition with Temporal Token Learning",
    "volume": "main",
    "abstract": "Recent advancements in large-scale pre-training of visual-language models on paired image-text data have demonstrated impressive generalization capabilities for zero-shot tasks. Building on this success, efforts have been made to adapt these image-based visual-language models, such as CLIP, for videos extending their zero-shot capabilities to the video domain. While these adaptations have shown promising results, they come at a significant computational cost and struggle with effectively modeling the temporal aspects inherent to the video domain. In this study, we present Efficient Zero-Shot Action Recognition with Temporal Token Learning(T2L), a simple and efficient adaptation of CLIP that addresses these challenges. T2L leverages Temporal Token Learning (TTL) for seamless temporal adaptation, requiring no fundamental changes to the core CLIP architecture while preserving its remarkable generalization abilities. TTL relies on temporal feature diversity (TFD), a novel learning objective, which guides TTL to focus on capturing motion, thereby enhancing its learning capabilities from videos. We perform extensive experiments on nine different benchmark datasets, thoroughly evaluating T2L for zero-shot learning and base-to-novel video action recognition, and also demonstrating its potential for few-shot generalization. Impressively, with merely 5.2 million learnable parameters, T2L can be efficiently trained on a single GPU (with 25x less learnable parameters, 3x reduction in GFLOPs, and 4x improvement in throughput when compared with prior best model), outperforming existing approaches in several evaluations",
    "checked": false,
    "id": "04b50a42dac98f56b3bc89b61bc4336bb85aded7",
    "semantic_title": "rethinking image-to-video adaptation: an object-centric perspective",
    "citation_count": 1,
    "authors": [
      "Shahzad Ahmad",
      "Sukalpa Chanda",
      "Yogesh S Rawat"
    ]
  },
  "https://openreview.net/forum?id=BMGikHBjlx": {
    "title": "Ctrl-V: Higher Fidelity Autonomous Vehicle Video Generation with Bounding-Box Controlled Object Motion",
    "volume": "main",
    "abstract": "Controllable video generation has attracted significant attention, largely due to advances in video diffusion models. In domains such as autonomous driving, developing highly accurate predictions for object motions is essential. This paper addresses the key challenge of enabling fine-grained control over object motion in the context of driving video synthesis. To accomplish this, we 1) employ a distinct, specialized model to forecast the trajectories of object bounding boxes, 2) adapt and enhance a separate video diffusion network to create video content conditioned on these high-quality trajectory forecasts, and 3) we are able to exert precise control over object position/movements using bounding boxes in both 2D and 3D spaces. Our method, Ctrl-V, leverages modified and fine-tuned Stable Video Diffusion (SVD) models to solve both trajectory and video generation. Extensive experiments conducted on the KITTI, Virtual-KITTI 2, BDD100k, and nuScenes datasets validate the effectiveness of our approach in producing realistic and controllable video generation. Project page: \\url{https://oooolga.github.io/ctrl-v.github.io/}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ge Ya Luo",
      "ZhiHao Luo",
      "Anthony Gosselin",
      "Alexia Jolicoeur-Martineau",
      "Christopher Pal"
    ]
  },
  "https://openreview.net/forum?id=Teu1Blr2YJ": {
    "title": "Node Feature Forecasting in Temporal Graphs: an Interpretable Online Algorithm",
    "volume": "main",
    "abstract": "In this paper, we propose an online algorithm mspace for forecasting node features in temporal graphs, which captures spatial cross-correlation among different nodes as well as the temporal auto-correlation within a node. The algorithm can be used for both probabilistic and deterministic multi-step forecasting, making it applicable for estimation and generation tasks. Evaluations against various baselines, including temporal graph neural network (TGNN) models and classical Kalman filters, demonstrate that mspace performs comparably to the state-of-the-art and even surpasses them on some datasets. Importantly, mspace demonstrates consistent performance across datasets with varying training sizes, a notable advantage over TGNN models that require abundant training samples to effectively learn the spatiotemporal trends in the data. Therefore, employing mspace is advantageous in scenarios where the training sample availability is limited. Additionally, we establish theoretical bounds on multi-step forecasting error of mspace and show that it scales linearly with the number of forecast steps $q$ as $\\mathcal{O}(q)$. For an asymptotically large number of nodes $n$, and timesteps $T$, the computational complexity of mspace grows linearly with both \\$n\\$ and \\$T\\$, i.e., $\\mathcal{O}(nT)$, while its space complexity remains constant $\\mathcal{O}(1)$. We compare the performance of various mspace variants against ten recent TGNN baselines and two classical baselines, ARIMA and the Kalman filter, across ten real-world datasets. Lastly, we have investigated the interpretability of different mspace variants by analyzing model parameters alongside dataset characteristics to jointly derive model-centric and data-centric insights",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aniq Ur Rahman",
      "Justin Coon"
    ]
  },
  "https://openreview.net/forum?id=Reh1S8rxfh": {
    "title": "Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach",
    "volume": "main",
    "abstract": "In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is important for reasonable causal models reflecting the broad knowledge of domain experts, despite the challenges in the systematic acquisition of background knowledge. To overcome these challenges, this paper proposes a novel method for causal inference, in which SCD and knowledge-based causal inference (KBCI) with a large language model (LLM) are synthesized through ``statistical causal prompting (SCP)'' for LLMs and prior knowledge augmentation for SCD. The experiments in this work have revealed that the results of LLM-KBCI and SCD augmented with LLM-KBCI approach the ground truths, more than the SCD result without prior knowledge. These experiments have also revealed that the SCD result can be further improved if the LLM undergoes SCP. Furthermore, with an unpublished real-world dataset, we have demonstrated that the background knowledge provided by the LLM can improve the SCD on this dataset, even if this dataset has never been included in the training data of the LLM. For future practical application of this proposed method across important domains such as healthcare, we also thoroughly discuss the limitations, risks of critical errors, expected improvement of techniques around LLMs, and realistic integration of expert checks of the results into this automatic process, with SCP simulations under various conditions both in successful and failure scenarios. The careful and appropriate application of the proposed approach in this work, with improvement and customization for each domain, can thus address challenges such as dataset biases and limitations, illustrating the potential of LLMs to improve data-driven causal inference across diverse scientific domains. The code used in this work is publicly available at: https://github.com/mas-takayama/LLM-and-SCD",
    "checked": true,
    "id": "c213737923f58ad6c4cd18a8c17bca6522d7f4c6",
    "semantic_title": "integrating large language models in causal discovery: a statistical causal approach",
    "citation_count": 17,
    "authors": [
      "MASAYUKI TAKAYAMA",
      "Tadahisa OKUDA",
      "Thong Pham",
      "Tatsuyoshi Ikenoue",
      "Shingo Fukuma",
      "Shohei Shimizu",
      "Akiyoshi Sannai"
    ]
  },
  "https://openreview.net/forum?id=72YVabBErN": {
    "title": "Efficient Open Set Single Image Test Time Adaptation of Vision Language Models",
    "volume": "main",
    "abstract": "Adapting models to dynamic, real-world environments characterized by shifting data distributions and unseen test scenarios is a critical challenge in deep learning. In this paper, we consider a realistic and challenging Test-Time Adaptation setting, where a model must continuously adapt to test samples that arrive sequentially, one at a time, while distinguishing between known and unknown classes. Current Test-Time Adaptation methods operate under closed-set assumptions or batch processing, differing from the real-world open-set scenarios. We address this limitation by establishing a comprehensive benchmark for Open-set Single-image Test-Time Adaptation using Vision-Language Models. Furthermore, we propose ROSITA, a novel framework that leverages dynamically updated feature banks to identify reliable test samples and employs a contrastive learning objective to improve the separation between known and unknown classes. Our approach effectively adapts models to domain shifts for known classes while rejecting unfamiliar samples. Extensive experiments across diverse real-world benchmarks demonstrate that ROSITA sets a new state-of-the-art in open-set TTA, achieving both strong performance and computational efficiency for real-time deployment. The code is released at https://github.com/manogna-s/ROSITA.git",
    "checked": true,
    "id": "f21359cb435f678b5474f9f7ad287b29c8942509",
    "semantic_title": "efficient open set single image test time adaptation of vision language models",
    "citation_count": 0,
    "authors": [
      "Manogna Sreenivas",
      "Soma Biswas"
    ]
  },
  "https://openreview.net/forum?id=eQeYyup1tm": {
    "title": "Bridging Lottery Ticket and Grokking: Understanding Grokking from Inner Structure of Networks",
    "volume": "main",
    "abstract": "Grokking is an intriguing phenomenon of delayed generalization, where neural networks initially memorize training data with perfect accuracy but exhibit poor generalization, subsequently transitioning to a generalizing solution with continued training. While factors such as weight norms and sparsity have been proposed to explain this delayed generalization, the influence of network structure remains underexplored. In this work, we link the grokking phenomenon to the lottery ticket hypothesis to investigate the impact of internal network structures. We demonstrate that utilizing lottery tickets obtained during the generalizing phase (termed grokked tickets) significantly reduces delayed generalization across various tasks, including multiple modular arithmetic operations, polynomial regression, sparse parity, and MNIST classification. Through controlled experiments, we show that the mitigation of delayed generalization is not due solely to reduced weight norms or increased sparsity, but rather to the discovery of good subnetworks. Furthermore, we find that grokked tickets exhibit periodic weight patterns and undergo rapid structural changes that coincide with improvements in generalization. Additionally, pruning techniques like the edge-popup algorithm can identify these effective structures without modifying the weights, thereby transforming memorizing networks into generalizing ones. These results underscore the novel insight that structural exploration plays a pivotal role in understanding grokking",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gouki Minegishi",
      "Yusuke Iwasawa",
      "Yutaka Matsuo"
    ]
  },
  "https://openreview.net/forum?id=6o3vVBWYis": {
    "title": "Language Models Are Good Tabular Learners",
    "volume": "main",
    "abstract": "Transformer-based language models have become the de facto standard in natural language processing. However, they underperform in the tabular data domain compared to traditional tree-based methods. We posit that current models fail to achieve the full potential of language models due to (i) heterogeneity of tabular data; and (ii) challenges faced by the model in interpreting numerical values. Based on this hypothesis, we propose the Tabular Domain Transformer (TDTransformer) framework. TDTransformer has distinct embedding processes for different types of columns. The alignment layers for different column-types transform these embeddings to a common space. Besides, TDTransformer adapts piece-wise linear encoding for numerical values for better performance. We test the proposed method on 76 real-world tabular classification datasets from the OpenML benchmark. Extensive experiments indicate that TDTransformer significantly improves the state-of-the-art methods",
    "checked": false,
    "id": "7d87fbdfbf5038a4e0ff09801b6d3b8a2e0c613a",
    "semantic_title": "language models are weak learners",
    "citation_count": 16,
    "authors": [
      "Zhenhan Huang",
      "Kavitha Srinivas",
      "Horst Samulowitz",
      "Niharika S. D'Souza",
      "Charu C. Aggarwal",
      "Pin-Yu Chen",
      "Jianxi Gao"
    ]
  },
  "https://openreview.net/forum?id=vc7poEYOFK": {
    "title": "Learning Energy-Based Generative Models via Potential Flow: A Variational Principle Approach to Probability Density Homotopy Matching",
    "volume": "main",
    "abstract": "Energy-based models (EBMs) are a powerful class of probabilistic generative models due to their flexibility and interpretability. However, relationships between potential flows and explicit EBMs remain underexplored, while contrastive divergence training via implicit Markov chain Monte Carlo (MCMC) sampling is often unstable and expensive in high-dimensional settings. In this paper, we propose Variational Potential (VAPO) Flow Bayes, a new energy-based generative framework that eliminates the need for implicit MCMC sampling and does not rely on auxiliary networks or cooperative training. VAPO learns an energy-parameterized potential flow by constructing a flow-driven density homotopy that is matched to the data distribution through a variational loss minimizing the Kullback-Leibler divergence between the flow-driven and marginal homotopies. This principled formulation enables robust and efficient generative modeling while preserving the interpretability of EBMs. Experimental results on image generation, interpolation, out-of-distribution detection, and compositional generation confirm the effectiveness of VAPO, showing that our method performs competitively with existing approaches in terms of sample quality and versatility across diverse generative modeling tasks",
    "checked": true,
    "id": "ab8f30257748b2830dca3930d6a613c054de7ad9",
    "semantic_title": "learning energy-based generative models via potential flow: a variational principle approach to probability density homotopy matching",
    "citation_count": 0,
    "authors": [
      "Junn Yong Loo",
      "Leong Fang Yu",
      "Michelle Adeline",
      "Julia K. Lau",
      "Hwa Hui Tew",
      "Arghya Pal",
      "VISHNU MONN BASKARAN",
      "Chee-Ming Ting",
      "Raphael CW Phan"
    ]
  },
  "https://openreview.net/forum?id=lyxRBPmmnV": {
    "title": "Neural Slot Interpreters: Grounding Object Semantics in Emergent Slot Representations",
    "volume": "main",
    "abstract": "Several accounts of human cognition posit that our intelligence is rooted in our ability to form abstract composable concepts, ground them in our environment, and reason over these grounded entities. This trifecta of human thought has remained elusive in modern intelligent machines. In this work, we investigate whether slot representations extracted from visual scenes serve as appropriate compositional abstractions for grounding and reasoning. We present the Neural Slot Interpreter (NSI), which learns to ground object semantics in slots. At the core of NSI is a nested schema that uses simple syntax rules to organize the object semantics of a scene into object-centric schema primitives. Then, the NSI metric learns to ground primitives into slots through a structured contrastive learning objective that reasons over the intermodal alignment. Experiments with a bi-modal object-property and scene retrieval task demonstrate the grounding efficacy and interpretability of correspondences learned by NSI. From a scene representation standpoint, we find that emergent NSI slots that move beyond the image grid by binding to spatial objects facilitate improved visual grounding compared to conventional bounding-box-based approaches. From a data efficiency standpoint, we empirically validate that NSI learns more generalizable representations from a fixed amount of annotation data than the traditional approach. We also show that the grounded slots surpass unsupervised slots in real-world object discovery and scale with scene complexity. Finally, we investigate the downstream efficacy of the grounded slots. Vision Transformers trained on grounding-aware NSI tokenizers using as few as ten tokens outperform patch-based tokens on challenging few-shot classification tasks",
    "checked": true,
    "id": "934152abcb942dadffb0c34aea1e55d947efc91d",
    "semantic_title": "neural slot interpreters: grounding object semantics in emergent slot representations",
    "citation_count": 1,
    "authors": [
      "Bhishma Dedhia",
      "Niraj Jha"
    ]
  },
  "https://openreview.net/forum?id=FEo55EIvGI": {
    "title": "Cross Entropy versus Label Smoothing: A Neural Collapse Perspective",
    "volume": "main",
    "abstract": "Label smoothing loss is a widely adopted technique to mitigate overfitting in deep neural networks. This paper studies label smoothing from the perspective of Neural Collapse (NC), a powerful empirical and theoretical framework which characterizes model behavior during the terminal phase of training. We first show empirically that models trained with label smoothing converge faster to neural collapse solutions and attain a stronger level of neural collapse compared to those trained with cross-entropy loss. Furthermore, we show that at the same level of NC1, models under label smoothing loss exhibit intensified NC2. These findings provide valuable insights into the impact of label smoothing on model performance and calibration. Then, leveraging the unconstrained feature model, we derive closed-form solutions for the global minimizers under both label smoothing and cross-entropy losses. We show that models trained with label smoothing have a lower conditioning number and, therefore, theoretically converge faster. Our study, combining empirical evidence and theoretical results, not only provides nuanced insights into the differences between label smoothing and cross-entropy losses, but also serves as an example of how the powerful neural collapse framework can be used to improve our understanding of DNNs",
    "checked": true,
    "id": "08901f2f5b363f28660f401afafccc2e6c9373d5",
    "semantic_title": "cross entropy versus label smoothing: a neural collapse perspective",
    "citation_count": 9,
    "authors": [
      "Li Guo",
      "George Andriopoulos",
      "Zifan Zhao",
      "Zixuan Dong",
      "Shuyang Ling",
      "Keith W. Ross"
    ]
  },
  "https://openreview.net/forum?id=3Jm4dbrKGZ": {
    "title": "Lurie Networks with Robust Convergent Dynamics",
    "volume": "main",
    "abstract": "The Lurie network is a novel and unifying time-invariant neural ODE. Many existing continuous-time models, including recurrent neural networks and neural oscillators, are special cases of the Lurie network in this context. Mild constraints on the weights and biases of the Lurie network are derived to ensure a generalised concept of stability is guaranteed. This generalised stability measure is that of k-contraction which permits global convergence to a point, line or plane in the neural state-space. This includes global convergence to one of multiple equilibrium points or limit cycles as observed in many dynamical systems including associative and working memory. Weights and biases of the Lurie network, which satisfy the k-contraction constraints, are encoded through unconstrained parametrisations. The novel stability results and parametrisations provide a toolset for training over the space of k-contracting Lurie network's using standard optimisation algorithms. These results are also leveraged to construct and train a graph Lurie network satisfying the same convergence properties. Empirical results show the improvement in prediction accuracy, generalisation and robustness on a range of simulated dynamical systems, when the graph structure and k-contraction conditions are introduced. These results also compare favourably against other well known stability-constrained models and an unconstrained neural ODE",
    "checked": false,
    "id": "3b1a5d6b9e415a236f73b0b108905062f9589f50",
    "semantic_title": "l urie networks with k - contracting dynamics",
    "citation_count": 0,
    "authors": [
      "Carl R Richardson",
      "Matthew C. Turner",
      "Steve R. Gunn"
    ]
  },
  "https://openreview.net/forum?id=hMPzJ3qKpf": {
    "title": "LocalFormer: Mitigating Over-Globalising in Transformers on Graphs with Localised Training",
    "volume": "main",
    "abstract": "As Transformers become more popular for graph machine learning, a significant issue has recently been observed. Their global attention mechanisms tend to overemphasize distant vertices, leading to the phenomenon of ``over-globalising.'' This phenomenon often results in the dilution of essential local information, particularly in graphs where local neighbourhoods carry significant predictive power. Existing methods often struggle with rigidity in their local processing, where tightly coupled operations limit flexibility and adaptability in diverse graph structures. Additionally, these methods can overlook critical structural nuances, resulting in an incomplete integration of local and global contexts. This paper addresses these issues by proposing LocalFormer, a novel framework, to effectively localise a transformer model by integrating a distinct local module and a complementary module that integrates global information. The local module focuses on capturing and preserving fine-grained, neighbourhood-specific patterns, ensuring that the model maintains sensitivity to critical local structures. In contrast, the complementary module dynamically integrates broader context without overshadowing the localised information, offering a balanced approach to feature aggregation across different scales of the graph. Through collaborative and warm-up training strategies, these modules work synergistically to mitigate the adverse effects of over-globalising, leading to improved empirical performance. Our experimental results demonstrate the effectiveness of LocalFormer compared to state-of-the-art baselines on vertex-classification tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naganand Yadati"
    ]
  },
  "https://openreview.net/forum?id=UcrVnXBdZI": {
    "title": "On the effectiveness of Rotation-Equivariance in U-Net: A Benchmark for Image Segmentation",
    "volume": "main",
    "abstract": "Numerous studies have recently focused on incorporating different variations of equivariance in Convolutional Neural Networks (CNNs). In particular, rotation-equivariance has gathered significant attention due to its relevance in many applications related to medical imaging, microscopic imaging, satellite imaging, industrial tasks, etc. While prior research has primarily focused on enhancing classification tasks with rotation equivariant CNNs, their impact on more complex architectures, such as U-Net for image segmentation, remains scarcely explored. Indeed, previous work interested in integrating rotation-equivariance into U-Net architecture have focused on solving specific applications with a limited scope. In contrast, this paper aims to provide a more exhaustive evaluation of rotation equivariant U-Net for image segmentation across a broader range of tasks. We benchmark their effectiveness against standard U-Net architectures, assessing improvements in terms of performance and sustainability (i.e., computational cost). Our evaluation focuses on datasets whose orientation of objects of interest is arbitrary in the image (e.g., Kvasir-SEG), but also on more standard segmentation datasets (such as COCO-Stuff) as to explore the wider applicability of rotation equivariance beyond tasks undoubtedly concerned by rotation equivariance. The main contribution of this work is to provide insights into the trade-offs and advantages of integrating rotation equivariance for segmentation tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Robin Ghyselinck",
      "Valentin Delchevalerie",
      "Bruno Dumas",
      "Benoit Frenay"
    ]
  },
  "https://openreview.net/forum?id=J6oxTJPOyN": {
    "title": "LEGO-Learn: Label-Efficient Graph Open-Set Learning",
    "volume": "main",
    "abstract": "How can we train graph-based models to recognize unseen classes while keeping labeling costs low? Graph open-set learning (GOL) and out-of-distribution (OOD) detection aim to address this challenge by training models that can accurately classify known, in-distribution (ID) classes while identifying and handling previously unseen classes during inference. It is critical for high-stakes, real-world applications where models frequently encounter unexpected data, including finance, security, and healthcare. However, current GOL methods assume access to a large number of labeled ID samples, which is unrealistic for large-scale graphs due to high annotation costs. In this paper, we propose LEGO-Learn (Label-Efficient Graph Open-set Learning), a novel framework that addresses open-set node classification on graphs within a given label budget by selecting the most informative ID nodes. LEGO-Learn employs a GNN-based filter to identify and exclude potential OOD nodes and then selects highly informative ID nodes for labeling using the K-Medoids algorithm. To prevent the filter from discarding valuable ID examples, we introduce a classifier that differentiates between the $C$ known ID classes and an additional class representing OOD nodes (hence, a $C+1$ classifier). This classifier utilizes a weighted cross-entropy loss to balance the removal of OOD nodes while retaining informative ID nodes. Experimental results on four real-world datasets demonstrate that LEGO-Learn significantly outperforms leading methods, achieving up to a $6.62\\%$ improvement in ID classification accuracy and a $7.49\\%$ increase in AUROC for OOD detection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyan Xu",
      "Kay Liu",
      "Zhengtao Yao",
      "Philip S. Yu",
      "Mengyuan Li",
      "Kaize Ding",
      "Yue Zhao"
    ]
  },
  "https://openreview.net/forum?id=VlwqIz41Hp": {
    "title": "Generalized Prediction Set with Bandit Feedback",
    "volume": "main",
    "abstract": "In high-stakes environments where uncertainties abound, set-valued prediction offers a cautious and robust mechanism by presenting multiple potential labels as the prediction for each test instance to mitigate the potential risk associated with prediction errors. Yet, integrating this paradigm with out-of-distribution (OOD) detection remains scarcely explored in such settings as online learning with bandit feedback. The bandit feedback mechanism informs the learner about the correctness of the pulled arm/action instead of the explicit ground truth label, leaving the true class label unknown when an incorrect action is taken. To address this challenge, we introduce BanditGPS which conducts set-valued prediction with OOD detection in the bandit feedback setting, using an estimation to the ground truth of class labels. BanditGPS achieves three objectives: render small/informative prediction sets, enhance the OOD detection performance, and control the recall for all normal classes to meet prescribed requirements. Our approach is characterized by the loss function, which trades off between high OOD detection and small prediction sets. Theoretically, we prove that the convergence rate of the regret is $\\tilde{\\mathcal{O}}(T^{-1/2})$. The empirical results further show that BanditGPS effectively controls the recalls with promising performances on OOD detection and informative prediction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhou Wang",
      "Xingye Qiao"
    ]
  },
  "https://openreview.net/forum?id=mk1YIkVvTQ": {
    "title": "Is What You Ask For What You Get? Investigating Concept Associations in Text-to-Image Models",
    "volume": "main",
    "abstract": "Text-to-image (T2I) models are increasingly used in impactful real-life applications. As such, there is a growing need to audit these models to ensure that they generate desirable, task-appropriate images. However, systematically inspecting the associations between prompts and generated content in a human-understandable way remains challenging. To address this, we propose \\emph{Concept2Concept}, a framework where we characterize conditional distributions of vision language models using interpretable concepts and metrics that can be defined in terms of these concepts. This characterization allows us to use our framework to audit models and prompt-datasets. To demonstrate, we investigate several case studies of conditional distributions of prompts, such as user-defined distributions or empirical, real-world distributions. Lastly, we implement Concept2Concept as an open-source interactive visualization tool to facilitate use by non-technical end-users. A demo is available at https://tinyurl.com/Concept2ConceptDemo. Warning: This paper contains discussions of harmful content, including CSAM and NSFW material, which may be disturbing to some readers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Salma Abdel Magid",
      "Weiwei Pan",
      "Simon Warchol",
      "Grace Guo",
      "Junsik Kim",
      "Mahia Rahman",
      "Hanspeter Pfister"
    ]
  },
  "https://openreview.net/forum?id=VuLEOyTiPO": {
    "title": "GeNIe: Generative Hard Negative Images Through Diffusion",
    "volume": "main",
    "abstract": "Data augmentation is crucial in training deep models, preventing them from overfitting to limited data. Recent advances in generative AI, e.g., diffusion models, have enabled more sophisticated augmentation techniques that produce data resembling natural images. We introduce $\\texttt{GeNIe}$ a novel augmentation method which leverages a latent diffusion model conditioned on a text prompt to combine two contrasting data points (an image from the source category and a text prompt from the target category) to generate challenging augmentations. To achieve this, we adjust the noise level (equivalently, number of diffusion iterations) to ensure the generated image retains low-level and background features from the source image while representing the target category, resulting in a hard negative sample for the source category. We further automate and enhance $\\texttt{GeNIe}$ by adaptively adjusting the noise level selection on a per image basis (coined as $\\texttt{GeNIe-Ada}$), leading to further performance improvements. Our extensive experiments, in both few-shot and long-tail distribution settings, demonstrate the effectiveness of our novel augmentation method and its superior performance over the prior art. Our code is available at https://github.com/UCDvision/GeNIe",
    "checked": true,
    "id": "7f0e374f1920d962359d7e9ccb7a7d7e2d18e6f4",
    "semantic_title": "genie: generative hard negative images through diffusion",
    "citation_count": 4,
    "authors": [
      "Soroush Abbasi Koohpayegani",
      "Anuj Singh",
      "Navaneet K L",
      "Hamed Pirsiavash",
      "Hadi J. Rad"
    ]
  },
  "https://openreview.net/forum?id=FFnRLvWefK": {
    "title": "System-Aware Neural ODE Processes for Few-Shot Bayesian Optimization",
    "volume": "main",
    "abstract": "We consider the problem of optimizing initial conditions and termination time in dynamical systems governed by unknown ordinary differential equations (ODEs), where evaluating different initial conditions is costly and the state's value can not be measured in real-time but only with a delay while the measuring device processes the sample. To identify the optimal conditions in limited trials, we introduce a few-shot Bayesian Optimization (BO) framework based on the system's prior information. At the core of our approach is the System-Aware Neural ODE Processes (SANODEP), an extension of Neural ODE Processes (NODEP) designed to meta-learn ODE systems from multiple trajectories using a novel context embedding block. We further develop a two-stage BO framework to effectively incorporate search space constraints, enabling efficient optimization of both initial conditions and observation timings. We conduct extensive experiments showcasing SANODEP's potential for few-shot BO within dynamical systems. We also explore SANODEP's adaptability to varying levels of prior information, highlighting the trade-off between prior flexibility and model fitting accuracy",
    "checked": true,
    "id": "fcc3aee3d63369b46495f52c938226e7f6a37977",
    "semantic_title": "system-aware neural ode processes for few-shot bayesian optimization",
    "citation_count": 1,
    "authors": [
      "Jixiang Qing",
      "Rebecca D. Langdon",
      "Robert Matthew Lee",
      "Behrang Shafei",
      "Mark van der Wilk",
      "Calvin Tsay",
      "Ruth Misener"
    ]
  },
  "https://openreview.net/forum?id=x9VQFjtOPS": {
    "title": "A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling, Search Algorithms, and Relevant Frameworks",
    "volume": "main",
    "abstract": "LLM test-time compute (or LLM inference) via search has emerged as a promising research area with rapid developments. However, current frameworks often adopt distinct perspectives on three key aspects—task definition, LLM profiling, and search procedures—making direct comparisons challenging. Moreover, the search algorithms employed often diverge from standard implementations, and their specific characteristics are not thoroughly specified. This survey aims to provide a comprehensive but integrated technical review on existing LIS frameworks. Specifically, we unify task definitions under Markov Decision Process (MDP) and provides modular definitions of LLM profiling and search procedures. The definitions enable precise comparisons of various LLM inference frameworks while highlighting their departures from conventional search algorithms. We also discuss the applicability, performance, and efficiency of these methods. For ongoing paper updates, please refer to our GitHub repository: https://github.com/xinzhel/LLM-Search",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinzhe Li"
    ]
  },
  "https://openreview.net/forum?id=bCmEP1Ltwq": {
    "title": "Neural Deconstruction Search for Vehicle Routing Problems",
    "volume": "main",
    "abstract": "Autoregressive construction approaches generate solutions to vehicle routing problems in a step-by-step fashion, leading to high-quality solutions that are nearing the performance achieved by handcrafted operations research techniques. In this work, we challenge the conventional paradigm of sequential solution construction and introduce an iterative search framework where solutions are instead deconstructed by a neural policy. Throughout the search, the neural policy collaborates with a simple greedy insertion algorithm to rebuild the deconstructed solutions. Our approach matches or surpasses the performance of state-of-the-art operations research methods across three challenging vehicle routing problems of various problem sizes",
    "checked": true,
    "id": "f0a50c531fff4a7fdd7715fe20d2ade10f438fb4",
    "semantic_title": "neural deconstruction search for vehicle routing problems",
    "citation_count": 1,
    "authors": [
      "André Hottung",
      "Paula Wong-Chung",
      "Kevin Tierney"
    ]
  },
  "https://openreview.net/forum?id=IbQTE24aZw": {
    "title": "Deflated Dynamics Value Iteration",
    "volume": "main",
    "abstract": "The Value Iteration (VI) algorithm is an iterative procedure to compute the value function of a Markov decision process, and is the basis of many reinforcement learning (RL) algorithms as well. As the error convergence rate of VI as a function of iteration $k$ is $O(\\gamma^k)$, it is slow when the discount factor $\\gamma$ is close to $1$. To accelerate the computation of the value function, we propose Deflated Dynamics Value Iteration (DDVI). DDVI uses matrix splitting and matrix deflation techniques to effectively remove (deflate) the top $s$ dominant eigen-structure of the transition matrix $\\mathcal{P}^\\pi$. We prove that this leads to a $\\tilde{O}(\\gamma^k |\\lambda_{s+1}|^k)$ convergence rate, where $\\lambda_{s+1}$ is the $(s+1)$-th largest eigenvalue of the dynamics matrix. We also extend DDVI to the RL setting and present Deflated Dynamics Temporal Difference (DDTD) algorithm. We empirically show the effectiveness of the proposed algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongmin Lee",
      "Amin Rakhsha",
      "Ernest K. Ryu",
      "Amir-massoud Farahmand"
    ]
  },
  "https://openreview.net/forum?id=zVo6PfBa0K": {
    "title": "Generating Symbolic World Models via Test-time Scaling of Large Language Models",
    "volume": "main",
    "abstract": "Solving complex planning problems requires Large Language Models (LLMs) to explicitly model the state transition to avoid rule violations, comply with constraints, and ensure optimality—a task hindered by the inherent ambiguity of natural language. To overcome such ambiguity, Planning Domain Definition Language (PDDL) is leveraged as a planning abstraction that enables precise and formal state descriptions. With PDDL, we can generate a symbolic world model where classic searching algorithms, such as A*, can be seamlessly applied to find optimal plans. However, directly generating PDDL domains with current LLMs remains an open challenge due to the lack of PDDL training data. To address this challenge, we propose to scale up the test-time computation of LLMs to enhance their PDDL reasoning capabilities, thereby enabling the generation of high-quality PDDL domains. Specifically, we introduce a simple yet effective algorithm, which first employs a Best-of-N sampling approach to improve the quality of the initial solution and then refines the solution in a fine-grained manner with verbalized machine learning. Our method outperforms o1-mini by a considerable margin in the generation of PDDL domains, achieving over 50% success rate on two tasks (i.e., generating PDDL domains from natural language description or PDDL problems). This is done without requiring additional training. By taking advantage of PDDL as state abstraction, our method is able to outperform the current state-of-the-art methods on almost all competition-level planning tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhouliang Yu",
      "Yuhuan Yuan",
      "Tim Z. Xiao",
      "Fuxiang Frank Xia",
      "Jie Fu",
      "Ge Zhang",
      "Ge lin",
      "Weiyang Liu"
    ]
  },
  "https://openreview.net/forum?id=JyjTJAG9yZ": {
    "title": "Personalized Layer Selection for Graph Neural Networks",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) combine node attributes over a fixed granularity of the local graph structure around a node to predict its label. However, different nodes may relate to a node-level property with a different granularity of its local neighborhood, and using the same level of smoothing for all nodes can be detrimental to their classification. In this work, we challenge the common fact that a single GNN layer can classify all nodes of a graph by training GNNs with a distinct personalized layer for each node. Inspired by metric learning, we propose a novel algorithm, MetSelect, to select the optimal representation layer to classify each node. In particular, we identify a prototype representation of each class in a transformed GNN layer and then, classify using the layer where the distance is smallest to a class prototype after normalizing with that layer's variance. Results on 10 datasets and 3 different GNNs show that we significantly improve the node classification accuracy of GNNs in a plug-and-play manner. We also find that using variable layers for prediction enables GNNs to be deeper and more robust to poisoning attacks. We hope this work can inspire future works to learn more adaptive and personalized graph representations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kartik Sharma",
      "Vineeth Rakesh",
      "Yingtong Dou",
      "Srijan Kumar",
      "Mahashweta Das"
    ]
  },
  "https://openreview.net/forum?id=WgJgIULL9Q": {
    "title": "HyperVQ: MLR-based Vector Quantization in Hyperbolic Space",
    "volume": "main",
    "abstract": "The success of models operating on tokenized data has heightened the need for effective tokenization methods, particularly in vision and auditory tasks where inputs are naturally continuous. A common solution is to employ Vector Quantization (VQ) within VQ Variational Autoencoders (VQVAEs), transforming inputs into discrete tokens by clustering embeddings in Euclidean space. However, Euclidean embeddings not only suffer from inefficient packing and limited separation—due to their polynomial volume growth—but are also prone to codebook collapse, where only a small subset of codebook vectors are effectively utilized. To address these limitations, we introduce HyperVQ, a novel approach that formulates VQ as a hyperbolic Multinomial Logistic Regression (MLR) problem, leveraging the exponential volume growth in hyperbolic space to mitigate collapse and improve cluster separability. Additionally, HyperVQ represents codebook vectors as geometric representatives of hyperbolic decision hyperplanes, encouraging disentangled and robust latent representations. Our experiments demonstrate that HyperVQ matches traditional VQ in generative and reconstruction tasks, while surpassing it in discriminative performance and yielding a more efficient and disentangled codebook",
    "checked": true,
    "id": "61613785fe09423e8df112ea726bb1195bc2575c",
    "semantic_title": "hypervq: mlr-based vector quantization in hyperbolic space",
    "citation_count": 4,
    "authors": [
      "Nabarun Goswami",
      "Yusuke Mukuta",
      "Tatsuya Harada"
    ]
  },
  "https://openreview.net/forum?id=1weZ9Wsajk": {
    "title": "Optimizing Cycle Life Prediction of Lithium-ion Batteries via a Physics-Informed Model",
    "volume": "main",
    "abstract": "Accurately measuring the cycle lifetime of commercial lithium-ion batteries is crucial for performance and technology development. We introduce a novel hybrid approach combining a physics-based equation with a self-attention model to predict the cycle lifetimes of commercial lithium iron phosphate graphite cells via early-cycle data. After fitting capacity loss curves to this physics-based equation, we then use a self-attention layer to reconstruct entire battery capacity loss curves. Our model exhibits comparable performances to existing models while predicting more information: the entire capacity loss curve instead of cycle life. This provides more robustness and interpretability: our model does not need to be retrained for a different notion of end-of-life and is backed by physical intuition",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathan Sun",
      "Daniel Nicolae",
      "Sara Sameer",
      "Karena Yan"
    ]
  },
  "https://openreview.net/forum?id=WOwQKguWT0": {
    "title": "When SNN meets ANN: Error-Free ANN-to-SNN Conversion for Extreme Edge Efficiency",
    "volume": "main",
    "abstract": "Spiking Neural Networks (SNN) are now demonstrating comparable accuracy to convolutional neural networks (CNN), thanks to advanced ANN-to-SNN conversion techniques, all while delivering remarkable energy and latency efficiency when deployed on neuromorphic hardware. However, these conversion techniques incur a large number of time steps, and consequently, high spiking activity. In this paper, we propose a novel ANN-to-SNN conversion framework, that incurs an exponentially lower number of time steps compared to that required in the existing conversion approaches. Our framework modifies the standard integrate-and-fire (IF) neuron model used in SNNs with no change in computational complexity and shifts the bias term of each batch normalization (BN) layer in the trained ANN. To reduce spiking activity, we propose training the source ANN with a fine-grained $\\ell_1$ regularizer with surrogate gradients that encourages high spike sparsity in the converted SNN. Our proposed framework thus yields lossless SNNs with low latency, low compute energy, thanks to the low time steps and high spike sparsity, and high test accuracy, for example, $75.12$% with only $4$ time steps on the ImageNet dataset. Codes will be made available. Code is available at https://github.com/godatta/SNN_meets_ANN",
    "checked": true,
    "id": "1138d51f4658754ebf0a2f07388a4d9dd406c344",
    "semantic_title": "when snn meets ann: error-free ann-to-snn conversion for extreme edge efficiency",
    "citation_count": 0,
    "authors": [
      "Gourav Datta",
      "Zeyu Liu",
      "James Diffenderfer",
      "Bhavya Kailkhura",
      "Peter Anthony Beerel"
    ]
  },
  "https://openreview.net/forum?id=fqSVqPcaVi": {
    "title": "ASTRA: A Scene-aware Transformer-based Model for Trajectory Prediction",
    "volume": "main",
    "abstract": "We present ASTRA (A Scene-aware Transformer-based model for trajectory prediction), a light-weight pedestrian trajectory forecasting model that integrates the scene context, spatial dynamics, social inter-agent interactions and temporal progressions for precise forecasting. We utilised a U-Net-based feature extractor, via its latent vector representation, to capture scene representations and a graph-aware transformer encoder for capturing social interactions. These components are integrated to learn an agent-scene aware embedding, enabling the model to learn spatial dynamics and forecast the future trajectory of pedestrians. The model is designed to produce both deterministic and stochastic outcomes, with the stochastic predictions being generated by incorporating a Conditional Variational Auto-Encoder (CVAE). ASTRA also proposes a simple yet effective weighted penalty loss function, which helps to yield predictions that outperform a wide array of state-of-the-art deterministic and generative models. ASTRA demonstrates an average improvement of 27%/10% in deterministic/stochastic settings on the ETH-UCY dataset, and 26% improvement on the PIE dataset, respectively, along with seven times fewer parameters than the existing state-of-the-art model (see Figure 1). Additionally, the model's versatility allows it to generalize across different perspectives, such as Bird's Eye View (BEV) and Ego-Vehicle View (EVV)",
    "checked": true,
    "id": "9f53e2f70504462f90676d3ec83e3cfade71a9a8",
    "semantic_title": "astra: a scene-aware transformer-based model for trajectory prediction",
    "citation_count": 0,
    "authors": [
      "Izzeddin Teeti",
      "Aniket Thomas",
      "Munish Monga",
      "Sachin Kumar Giroh",
      "Uddeshya Singh",
      "Andrew Bradley",
      "Biplab Banerjee",
      "Fabio Cuzzolin"
    ]
  },
  "https://openreview.net/forum?id=78N9tCL6Ly": {
    "title": "Leveraging Unlabeled Data Sharing through Kernel Function Approximation in Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "Offline reinforcement learning (RL) learns policies from a fixed dataset, but often requires large amounts of data. The challenge arises when labeled datasets are expensive, especially when rewards have to be provided by human labelers for large datasets. In contrast, unlabelled data tends to be less expensive. This situation highlights the importance of finding effective ways to use unlabelled data in offline RL, especially when labelled data is limited or expensive to obtain. In this paper, we present the algorithm to utilize the unlabeled data in the offline RL method with kernel function approximation and give the theoretical guarantee. We present various eigenvalue decay conditions of $\\mathcal{H}_k$ which determine the complexity of the algorithm. In summary, our work provides a promising approach for exploiting the advantages offered by unlabeled data in offline RL, whilst maintaining theoretical assurances",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yen Ru Lai",
      "Fu-Chieh Chang",
      "Pei-Yuan Wu"
    ]
  },
  "https://openreview.net/forum?id=xu4ATNjcdy": {
    "title": "Variational Stochastic Gradient Descent for Deep Neural Networks",
    "volume": "main",
    "abstract": "Optimizing deep neural networks is one of the main tasks in successful deep learning. Current state-of-the-art optimizers are adaptive gradient-based optimization methods such as Adam. Recently, there has been an increasing interest in formulating gradient-based optimizers in a probabilistic framework for better modeling the uncertainty of the gradients. Here, we propose to combine both approaches, resulting in the Variational Stochastic Gradient Descent (VSGD) optimizer. We model gradient updates as a probabilistic model and utilize stochastic variational inference (SVI) to derive an efficient and effective update rule. Further, we show how our VSGD method relates to other adaptive gradient-based optimizers like Adam. Lastly, we carry out experiments on two image classification datasets and four deep neural network architectures, where we show that VSGD outperforms Adam and SGD",
    "checked": true,
    "id": "9501deace2f45582a3a10a7700ada2a0918a2eb4",
    "semantic_title": "variational stochastic gradient descent for deep neural networks",
    "citation_count": 0,
    "authors": [
      "Anna Kuzina",
      "Haotian Chen",
      "Babak Esmaeili",
      "Jakub M. Tomczak"
    ]
  },
  "https://openreview.net/forum?id=2e1aZZd88C": {
    "title": "Non-Myopic Multi-Objective Bayesian Optimization",
    "volume": "main",
    "abstract": "We consider the problem of finite-horizon sequential experimental design to solve multi-objective optimization (MOO) of expensive black-box objective functions. This problem arises in many real-world applications, including materials design, where we have a small resource budget to make and evaluate candidate materials in the lab. We solve this problem using the framework of Bayesian optimization (BO) and propose the first set of non-myopic methods for MOO problems. Prior work on non-myopic BO for single-objective problems relies on the Bellman optimality principle to handle the lookahead reasoning process. However, this principle does not hold for most MOO problems because the reward function needs to satisfy some conditions: scalar variable, monotonicity, and additivity. We address this challenge by using hypervolume improvement (HVI) as our scalarization approach, which allows us to use a lower-bound on the Bellman equation to approximate the finite-horizon using a batch expected hypervolume improvement (EHVI) acquisition function (AF) for MOO. Our formulation naturally allows us to use other improvement-based scalarizations and compare their efficacy to HVI. We derive three non-myopic AFs for MOBO: 1) the Nested AF, which is based on the exact computation of the lower bound, 2) the Joint AF, which is a lower bound on the nested AF, and 3) the BINOM AF, which is a fast and approximate variant based on batch multi-objective acquisition functions. Our experiments on multiple diverse real-world MO problems demonstrate that our non-myopic AFs substantially improve performance over the existing myopic AFs for MOBO",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Syrine Belakaria",
      "Alaleh Ahmadian",
      "Barbara E Engelhardt",
      "Stefano Ermon",
      "Jana Doppa"
    ]
  },
  "https://openreview.net/forum?id=aKjJoEVKgO": {
    "title": "Investigating Continual Pretraining in Large Language Models: Insights and Implications",
    "volume": "main",
    "abstract": "Continual learning (CL) in large language models (LLMs) is an evolving domain that focuses on developing efficient and sustainable training strategies to adapt models to emerging knowledge and achieve robustness in dynamic environments. Our primary emphasis is on continual domain-adaptive pretraining, a process designed to equip LLMs with the ability to integrate new information from various domains while retaining previously learned knowledge. Since existing works concentrate mostly on continual fine-tuning for a limited selection of downstream tasks or training domains, we introduce a new benchmark designed to measure the adaptability of LLMs to changing pretraining data landscapes. We further examine the impact of model size on learning efficacy and forgetting, as well as how the progression and similarity of emerging domains affect the knowledge transfer within these models. Our findings uncover several key insights: (i) continual pretraining consistently improves <1.5B models studied in this work and is also superior to domain adaptation, (ii) larger models always achieve better perplexity than smaller ones when continually pretrained on the same corpus, (iii) smaller models are particularly sensitive to continual pretraining, showing the most significant rates of both learning and forgetting, (iv) continual pretraining boosts downstream task performance of GPT-2 family, (v) continual pretraining enables LLMs to specialize better when the sequence of domains shows semantic similarity while randomizing training domains leads to better transfer and final performance otherwise. We posit that our research establishes a new benchmark for CL in LLMs, providing a more realistic evaluation of knowledge retention and transfer across diverse domains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Çağatay Yıldız",
      "Nishaanth Kanna Ravichandran",
      "Nitin Sharma",
      "Matthias Bethge",
      "Beyza Ermis"
    ]
  },
  "https://openreview.net/forum?id=XofMHO5yVY": {
    "title": "A Gold Standard Dataset for the Reviewer Assignment Problem",
    "volume": "main",
    "abstract": "Many peer-review venues are using algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the \"similarity score'' — a numerical estimate of the expertise of a reviewer in reviewing a paper — and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key challenge in comparing existing algorithms and developing better algorithms is the lack of the publicly available gold-standard data that would be needed to perform reproducible research. We address this challenge by collecting a novel dataset of similarity scores that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously. We use this data to compare several popular algorithms currently employed in computer science conferences and come up with recommendations for stakeholders. Our four main findings are: - All algorithms make a non-trivial amount of error. For the task of ordering two papers in terms of their relevance for a reviewer, the error rates range from 12%-30% in easy cases to 36%-43% in hard cases, thereby highlighting the vital need for more research on the similarity-computation problem. - Most specialized algorithms are designed to work with titles and abstracts of papers, and in this regime the Specter2 algorithm performs best. - The classical TF-IDF algorithm which can use full texts of papers is on par with Specter2 that uses only titles and abstracts. - The performance of off-the-shelf LLMs is worse than the specialized algorithms. We encourage researchers to participate in our survey and contribute their data to the dataset here: https://forms.gle/SP1Rh8eivGz54xR37",
    "checked": true,
    "id": "5ea8eedcb31859c5730dd1da3804e1be529ffabb",
    "semantic_title": "a gold standard dataset for the reviewer assignment problem",
    "citation_count": 15,
    "authors": [
      "Ivan Stelmakh",
      "John Frederick Wieting",
      "Yang Xi",
      "Graham Neubig",
      "Nihar B Shah"
    ]
  },
  "https://openreview.net/forum?id=NUV8THrLZC": {
    "title": "Efficient Exploration in Multi-Agent Reinforcement Learning via Farsighted Self-Direction",
    "volume": "main",
    "abstract": "Multi-agent reinforcement learning faces greater challenges with efficient exploration compared to single-agent counterparts, primarily due to the exponential growth in state and action spaces. Methods based on intrinsic rewards have been proven to enhance exploration efficiency in multi-agent scenarios effectively. However, these methods are plagued by instability during training and biases in exploration direction. To address these challenges, we propose Farsighted Self-Direction (FSD), a novel model-free method that utilizes a long-term exploration bonus to achieve coordinated exploration. Since prediction error against individual Q-values indicates a potential bonus for committed exploration, it is taken into account in action selection to directly guide the coordinated exploration. Further, we also use clipped double Q-learning to reduce noise in prediction error. We validate the method on didactic examples and demonstrate the outperformance of our method on challenging StarCraft II micromanagement tasks",
    "checked": true,
    "id": "f8564d05a3bf9a178849ed60ae523e3b07f50363",
    "semantic_title": "efficient exploration in multi-agent reinforcement learning via farsighted self-direction",
    "citation_count": 0,
    "authors": [
      "Tiancheng Lao",
      "Xudong Guo",
      "Mengge Liu",
      "Junjie Yu",
      "Yi Liu",
      "Wenhui Fan"
    ]
  },
  "https://openreview.net/forum?id=8otbGorZK2": {
    "title": "Semantic-Syntactic Discrepancy in Images (SSDI): Learning Meaning and Order of Features from Natural Images",
    "volume": "main",
    "abstract": "Despite considerable progress in image classification tasks, classification models seem unaffected by the images that significantly deviate from those that appear natural to human eyes. Specifically, while human perception can easily identify abnormal appearances or compositions in images, classification models overlook any alterations in the arrangement of object parts as long as they are present in any order, even if unnatural. Hence, this work exposes the vulnerability of having semantic and syntactic discrepancy in images (SSDI) in the form of corruptions that remove or shuffle image patches or present images in the form of puzzles. To address this vulnerability, we propose the concept of \"image grammar\", comprising \"image semantics\" and \"image syntax\". Image semantics pertains to the interpretation of parts or patches within an image, whereas image syntax refers to the arrangement of these parts to form a coherent object. We present a semi-supervised two-stage method for learning the image grammar of visual elements and environments solely from natural images. While the first stage learns the semantic meaning of individual object parts, the second stage learns how their relative arrangement constitutes an entire object. The efficacy of the proposed approach is then demonstrated by achieving SSDI detection rates ranging from 70% to 90% on corruptions generated from CelebA and SUN-RGBD datasets. Code is publicly available at: https://github.com/ChunTao1999/SSDI/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun Tao",
      "Timur Ibrayev",
      "Kaushik Roy"
    ]
  },
  "https://openreview.net/forum?id=sSAp8ITBpC": {
    "title": "Operationalizing a Threat Model for Red-Teaming Large Language Models (LLMs)",
    "volume": "main",
    "abstract": "Creating secure and resilient applications with large language models (LLM) requires anticipating, adjusting to, and countering unforeseen threats. Red-teaming has emerged as a critical technique for identifying vulnerabilities in real-world LLM implementations. This paper presents a detailed threat model and provides a systematization of knowledge (SoK) of red-teaming attacks on LLMs. We develop a taxonomy of attacks based on the stages of the LLM development and deployment process and extract various insights from previous research. In addition, we compile methods for defense and practical red-teaming strategies for practitioners. By delineating prominent attack motifs and shedding light on various entry points, this paper provides a framework for improving the security and robustness of LLM-based systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Apurv Verma",
      "Satyapriya Krishna",
      "Sebastian Gehrmann",
      "Madhavan Seshadri",
      "Anu Pradhan",
      "John A. Doucette",
      "David Rabinowitz",
      "Leslie Barrett",
      "Tom Ault",
      "Hai Phan"
    ]
  },
  "https://openreview.net/forum?id=bXUipBbZDA": {
    "title": "Reinforcement Learning from Bagged Reward",
    "volume": "main",
    "abstract": "In Reinforcement Learning (RL), it is commonly assumed that an immediate reward signal is generated for each action taken by the agent, helping the agent maximize cumulative rewards to obtain the optimal policy. However, in many real-world scenarios, designing immediate reward signals is difficult; instead, agents receive a single reward that is contingent upon a partial sequence or a complete trajectory. In this work, we define this challenging problem as RL from Bagged Reward (RLBR), where sequences of data are treated as bags with non-Markovian bagged rewards, leading to the formulation of Bagged Reward Markov Decision Processes (BRMDPs). Theoretically, we demonstrate that RLBR can be addressed by solving a standard MDP with properly redistributed bagged rewards allocated to each instance within a bag. Empirically, we find that reward redistribution becomes more challenging as the bag length increases, due to reduced informational granularity. Existing reward redistribution methods are insufficient to address these challenges. Therefore, we propose a novel reward redistribution method equipped with a bidirectional attention mechanism, enabling the accurate interpretation of contextual nuances and temporal dependencies within each bag. We experimentally demonstrate that the proposed method consistently outperforms existing approaches",
    "checked": false,
    "id": "6f9dbae279fa0c3a90d12f3b0f271dc8e6274817",
    "semantic_title": "a survey of reinforcement learning from human feedback",
    "citation_count": 126,
    "authors": [
      "Yuting Tang",
      "Xin-Qiang Cai",
      "Yao-Xiang Ding",
      "Qiyu Wu",
      "Guoqing Liu",
      "Masashi Sugiyama"
    ]
  },
  "https://openreview.net/forum?id=AcLlg4J52H": {
    "title": "RS-Reg: Probabilistic and Robust Certified Regression through Randomized Smoothing",
    "volume": "main",
    "abstract": "Randomized smoothing has shown promising certified robustness against adversaries in classification tasks. Despite such success with only zeroth-order access to base models, randomized smoothing has not been extended to a general form of regression. By defining robustness in regression tasks flexibly through probabilities, we demonstrate how to establish upper bounds on input data point perturbation (using the $\\ell_2$ norm) for a user-specified probability of observing valid outputs. Furthermore, we showcase the asymptotic property of a basic averaging function in scenarios where the regression model operates without any constraint. We then derive a certified upper bound of the input perturbations when dealing with a family of regression models where the outputs are bounded. Our simulations verify the validity of the theoretical results and reveal the advantages and limitations of simple smoothing functions, i.e., averaging, in regression tasks. The code is publicly available at \\url{https://github.com/arekavandi/Certified_Robust_Regression}",
    "checked": true,
    "id": "66925c3184e9e17980df8395fd8c5f293028e60b",
    "semantic_title": "rs-reg: probabilistic and robust certified regression through randomized smoothing",
    "citation_count": 0,
    "authors": [
      "Aref Miri Rekavandi",
      "Olga Ohrimenko",
      "Benjamin I. P. Rubinstein"
    ]
  },
  "https://openreview.net/forum?id=qahoztvThX": {
    "title": "A functional framework for nonsmooth autodiff with {\\it maxpooling} functions",
    "volume": "main",
    "abstract": "We make a comment on the recent work by Boustany, by showing that the Murat-TrombettiTheorem provides a simple and efficient mathematical framework for nonsmooth automatic differentiation of {\\it maxpooling} functions. In particular it gives a the chain rule formula which correctly defines the composition of Lipschitz-continuous functions which are piecewise $C^1$. The formalism is applied to four basic examples, with some tests in PyTorch. A self contained proof of an important Stampacchia formula is in the appendix",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bruno Després"
    ]
  },
  "https://openreview.net/forum?id=ubrOSWyTS8": {
    "title": "∇QDARTS: Quantization as an Elastic Dimension to Differentiable NAS",
    "volume": "main",
    "abstract": "Differentiable Neural Architecture Search methods efficiently find high-accuracy architectures using gradient-based optimization in a continuous domain, saving computational resources. Mixed-precision search helps optimize precision within a fixed architecture. However, applying it to a NAS-generated network does not assure optimal performance as the optimized quantized architecture may not emerge from a standalone NAS method. In light of these considerations, this paper introduces ∇QDARTS, a novel approach that combines differentiable NAS with mixed-precision search for both weight and activation. ∇QDARTS aims to identify the optimal mixed-precision neural architecture capable of achieving remarkable accuracy while operating with minimal computational requirements in a single-shot, end-to-end differentiable framework, obviating the need for pretraining and proxy methods. Compared to fp32, ∇QDARTS shows impressive performance on CIFAR10 with (2,4) bit precision, reducing bit operations by 160× with a slight 1.57% accuracy drop. Increasing the capacity enables ∇QDARTS to match fp32 accuracy while reducing bit operations by 18×. For the ImageNet dataset, with just (2,4) bit precision, ∇QDARTS outperforms state-of-the-art methods such as APQ, SPOS, OQA, and MNAS by 2.3%, 2.9%, 0.3%, and 2.7% in terms of accuracy. By incorporating (2,4,8) bit precision, ∇QDARTS further minimizes the accuracy drop to 1% compared to fp32, alongside a substantial reduction of 17× in required bit operations and 2.6× in memory footprint. In terms of bit-operation (memory footprint) ∇QDARTS excels over APQ, SPOS, OQA, and MNAS with similar accuracy by 2.3× (12×), 2.4× (3×), 13% (6.2×), 3.4× (37%), for bit-operation (memory footprint), respectively. ∇QDARTS enhances the overall search and training efficiency, achieving a 3.1× and 1.54× improvement over APQ and OQA, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Payman Behnam",
      "Uday Kamal",
      "Sanjana Vijay Ganesh",
      "Zhaoyi Li",
      "Michael Andrew Jurado",
      "Alind Khare",
      "Igor Fedorov",
      "Gaowen Liu",
      "Alexey Tumanov"
    ]
  },
  "https://openreview.net/forum?id=sTdVnDW0HX": {
    "title": "Piecewise Constant Spectral Graph Neural Network",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have achieved significant success across various domains by leveraging graph structures in data. Existing spectral GNNs, which use low-degree polynomial filters to capture graph spectral properties, may not fully identify the graph's spectral characteristics because of the polynomial's small degree. However, increasing the polynomial degree is computationally expensive and beyond certain thresholds leads to performance plateaus or degradation. In this paper, we introduce the Piecewise Constant Spectral Graph Neural Network(PieCoN) to address these challenges. PieCoN combines constant spectral filters with polynomial filters to provide a more flexible way to leverage the graph structure. By adaptively partitioning the spectrum into intervals, our approach increases the range of spectral properties that can be effectively learned. Experiments on nine benchmark datasets, including both homophilic and heterophilic graphs, demonstrate that PieCoN is particularly effective on heterophilic datasets, highlighting its potential for a wide range of applications",
    "checked": true,
    "id": "ac91583c081a69ab96702b35d72fc90d39b4b7ae",
    "semantic_title": "piecewise constant spectral graph neural network",
    "citation_count": 0,
    "authors": [
      "Vahan Martirosyan",
      "Jhony H. Giraldo",
      "Fragkiskos D. Malliaros"
    ]
  },
  "https://openreview.net/forum?id=KQzJYI6eo0": {
    "title": "Global Graph Counterfactual Explanation: A Subgraph Mapping Approach",
    "volume": "main",
    "abstract": "Graph Neural Networks (GNNs) have been widely deployed in various real-world applications. However, most GNNs are black-box models that lack explanations. One strategy to explain GNNs is through counterfactual explanation, which aims to find minimum perturbations on input graphs that change the GNN predictions. Existing works on GNN counterfactual explanations primarily concentrate on the local-level perspective (i.e., generating counterfactuals for each individual graph), which suffers from information overload and lacks insights into the broader cross-graph relationships. To address such issues, we propose GlobalGCE, a novel global-level graph counterfactual explanation method. GlobalGCE aims to identify a collection of subgraph mapping rules as counterfactual explanations for the target GNN. According to these rules, substituting certain significant subgraphs with their counterfactual subgraphs will change the GNN prediction to the desired class for most graphs (i.e., maximum coverage). Methodologically, we design a significant subgraph generator and a counterfactual subgraph autoencoder in our GlobalGCE, where the subgraphs and the rules can be effectively generated. Extensive experiments demonstrate the superiority of our GlobalGCE compared to existing baselines. Our code can be found at \\url{https://github.com/YinhanHe123/GlobalGCE}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinhan He",
      "Wendy Zheng",
      "Yaochen Zhu",
      "Jing Ma",
      "Saumitra Mishra",
      "Natraj Raman",
      "Ninghao Liu",
      "Jundong Li"
    ]
  },
  "https://openreview.net/forum?id=F6l3BBPElY": {
    "title": "Speech Synthesis By Unrolling Diffusion Process using Neural Network Layers",
    "volume": "main",
    "abstract": "This work proposes a novel setup where a neural network is trained to predict multiple steps of the reverse diffusion process in an unrolled manner, with successive layers corresponding to equally spaced steps in the diffusion schedule. Each layer progressively denoises the input during the reverse process until the final layer estimates the original input, $x_0$. Additionally, we introduce a new learning target by using latent variables, rather than the conventional approach of predicting the original input $x_0$ or source error $\\epsilon_0$. In speech synthesis, using $x_0$ or $\\epsilon_0$ often leads to large prediction errors in the early stages of the denoising process, causing distortion in the recovered speech. Our method mitigates this issue and, through extensive evaluation, demonstrates the generation of high-fidelity speech in competitive time, outperforming current state-of-the-art techniques. Moreover, the proposed approach generalizes well to unseen speech. Sample audio is available at \\url{https://onexpeters.github.io/UDPNet/}",
    "checked": true,
    "id": "47df6a7118baa441598e69e1cb5a3bd41cec97f6",
    "semantic_title": "speech synthesis by unrolling diffusion process using neural network layers",
    "citation_count": 0,
    "authors": [
      "Peter Ochieng"
    ]
  },
  "https://openreview.net/forum?id=goe6fv6iSh": {
    "title": "Gaussian Pre-Activations in Neural Networks: Myth or Reality?",
    "volume": "main",
    "abstract": "The study of feature propagation at initialization in neural networks lies at the root of numerous initialization designs. A very common assumption is that the pre-activations are Gaussian. Although this convenient *Gaussian hypothesis* can be justified when the number of neurons per layer tends to infinity, it is challenged by both theoretical and experimental work for finite-width neural networks. Our main contribution is to construct a family of pairs of activation functions and initialization distributions that ensure that the pre-activations remain Gaussian throughout the network depth, even in narrow neural networks, under the assumption that the pre-activations are independent. In the process, we discover a set of constraints that a neural network should satisfy to ensure Gaussian pre-activations. In addition, we provide a critical review of the claims of the Edge of Chaos line of work and construct a non-asymptotic Edge of Chaos analysis. We also propose a unified view on the propagation of pre-activations, encompassing the framework of several well-known initialization procedures. More generally, our work provides a principled framework for addressing the much-debated question: is it desirable to initialize the training of a neural network whose pre-activations are guaranteed to be Gaussian? Our code is available on GitHub: https://github.com/p-wol/gaussian-preact/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre Wolinski",
      "Julyan Arbel"
    ]
  },
  "https://openreview.net/forum?id=akumIxQjNN": {
    "title": "ReDistill: Residual Encoded Distillation for Peak Memory Reduction of CNNs",
    "volume": "main",
    "abstract": "The expansion of neural network sizes and the enhanced resolution of modern image sensors result in heightened memory and power demands to process modern computer vision models. In order to deploy these models in extremely resource-constrained edge devices, it is crucial to reduce their peak memory, which is the maximum memory consumed during the execution of a model. A naive approach to reducing peak memory is aggressive down-sampling of feature maps via pooling with large stride, which often results in unacceptable degradation in network performance. To mitigate this problem, we propose residual encoded distillation (ReDistill) for peak memory reduction in a teacher-student framework, in which a student network with less memory is derived from the teacher network using aggressive pooling. We apply our distillation method to multiple problems in computer vision, including image classification and diffusion-based image generation. For image classification, our method yields 4x-5x theoretical peak memory reduction with less degradation in accuracy for most CNN-based architectures. For diffusion-based image generation, our proposed distillation method yields a denoising network with 4x lower theoretical peak memory while maintaining decent diversity and fidelity for image generation. Experiments demonstrate our method's superior performance compared to other feature-based and response-based distillation methods when applied to the same student network. The code is available at https://github.com/mengtang-lab/ReDistill",
    "checked": false,
    "id": "7012a749533b2c30c3c633844008134fe73968cf",
    "semantic_title": "redistill: residual encoded distillation for peak memory reduction",
    "citation_count": 1,
    "authors": [
      "Fang Chen",
      "Gourav Datta",
      "Mujahid Al Rafi",
      "Hyeran Jeon",
      "Meng Tang"
    ]
  },
  "https://openreview.net/forum?id=9fPinz1iH2": {
    "title": "Heterophily-informed Message Passing",
    "volume": "main",
    "abstract": "Graph neural networks (GNNs) are known to be vulnerable to oversmoothing due to their implicit homophily assumption. We mitigate this problem with a novel scheme that regulates the aggregation of messages, modulating the type and extent of message passing locally thereby preserving both the low and high-frequency components of information. Our approach relies solely on learnt embeddings, obviating the need for auxiliary labels, thus extending the benefits of heterophily-aware embeddings to broader applications, e.g. generative modelling. Our experiments, conducted across various data sets and GNN architectures, demonstrate performance enhancements and reveal heterophily patterns across standard classification benchmarks. Furthermore, application to molecular generation showcases notable performance improvements on chemoinformatics benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haishan Wang",
      "Arno Solin",
      "Vikas K Garg"
    ]
  },
  "https://openreview.net/forum?id=AIby9MQXbu": {
    "title": "Robust Model Selection of Gaussian Graphical Models",
    "volume": "main",
    "abstract": "In Gaussian graphical model selection, noise-corrupted samples present significant challenges. It is known that even minimal amounts of noise can obscure the underlying structure, leading to fundamental identifiability issues. A recent line of work addressing this \"robust model selection\" problem narrows its focus to tree-structured graphical models. Even within this specific class of models, exact structure recovery is shown to be impossible. However, several algorithms have been developed that are known to provably recover the underlying tree-structure up to an (unavoidable) equivalence class. In this paper, we extend these results beyond tree-structured graphs. We first characterize the equivalence class up to which general graphs can be recovered in the presence of noise. Despite the inherent ambiguity (which we prove is unavoidable), the structure that can be recovered reveals local clustering information and global connectivity patterns in the underlying model. Such information is useful in a range of real-world problems, including power grids, social networks, protein-protein interactions, and neural structures. We then propose an algorithm which provably recovers the underlying graph up to the identified ambiguity. We further provide finite sample guarantees in the high-dimensional regime for our algorithm and validate our results through numerical simulations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abrar Zahin",
      "Rajasekhar Anguluri",
      "Lalitha Sankar",
      "Oliver Kosut",
      "Gautam Dasarathy"
    ]
  },
  "https://openreview.net/forum?id=h434zx5SX0": {
    "title": "Sample, estimate, aggregate: A recipe for causal discovery foundation models",
    "volume": "main",
    "abstract": "Causal discovery, the task of inferring causal structure from data, has the potential to uncover mechanistic insights from biological experiments, especially those involving perturbations. However, causal discovery algorithms over larger sets of variables tend to be brittle against misspecification or when data are limited. For example, single-cell transcriptomics measures thousands of genes, but the nature of their relationships is not known, and there may be as few as tens of cells per intervention setting. To mitigate these challenges, we propose a foundation model-inspired approach: a supervised model trained on large-scale, synthetic data to predict causal graphs from summary statistics — like the outputs of classical causal discovery algorithms run over subsets of variables and other statistical hints like inverse covariance. Our approach is enabled by the observation that typical errors in the outputs of a discovery algorithm remain comparable across datasets. Theoretically, we show that the model architecture is well-specified, in the sense that it can recover a causal graph consistent with graphs over subsets. Empirically, we train the model to be robust to misspecification and distribution shift using diverse datasets. Experiments on biological and synthetic data confirm that this model generalizes well beyond its training set, runs on graphs with hundreds of variables in seconds, and can be easily adapted to different underlying data assumptions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Menghua Wu",
      "Yujia Bao",
      "Regina Barzilay",
      "Tommi Jaakkola"
    ]
  },
  "https://openreview.net/forum?id=IizmQoF86Y": {
    "title": "A Learning-Based Framework for Fair and Scalable Solution Generation in Kidney Exchange Problems",
    "volume": "main",
    "abstract": "Reinforcement learning and Generative Flow Networks, known as GFlowNets, present an exciting possibility for neural networks to model distributions across various data structures. In this paper, we broaden their applicability to data structures consisting of optimal solutions for a combinatorial problem. Concretely, we propose using Q-learning and various policy gradient methods, as well as GFlowNets to learn the distribution of optimal solutions for kidney exchange problems (KEPs). This could provide a useful tool for decision-making authorities, policymakers and clinicians, as it offers them multiple optimal or near-optimal solutions, and provides a complementary landscape to their traditional integer programming-based toolbox for promoting fairness and societal benefits. Our reinforcement learning-based framework trained on KEP instances provides an effective addition to computationally expensive exact approaches, notably mixed-integer programming. Our experiments thoroughly evaluate the quality of the solution sets sampled from the trained neural networks in terms of optimality, their scalability when dealing with real-sized KEP instances, and their capability to generate a diverse pool of solutions. We also cover the use of their efficient solution generation capabilities to improve fairness and simulate the evolution of the KEP pool in a dynamic setting. Our contribution is thus: 1) methodological, as it introduces a novel setting for reinforcement learning in addition to GFlowNets, 2) implementational, as it delves beyond the theory and details how to use conditional information, and 3) of practical significance, as it considers a specific combinatorial problem in the healthcare domain",
    "checked": true,
    "id": "0214f08c7764551ff4afb3a4e46b5c407bb899cc",
    "semantic_title": "a learning-based framework for fair and scalable solution generation in kidney exchange problems",
    "citation_count": 0,
    "authors": [
      "William St-Arnaud",
      "Margarida Carvalho",
      "Golnoosh Farnadi"
    ]
  },
  "https://openreview.net/forum?id=HRvHCd03HM": {
    "title": "Double Horizon Model-Based Policy Optimization",
    "volume": "main",
    "abstract": "Model-based reinforcement learning (MBRL) reduces the cost of real-environment sampling by generating synthetic trajectories (called rollouts) from a learned dynamics model. However, choosing the length of the rollouts poses two dilemmas: (1) Longer rollouts better preserve on-policy training but amplify model bias, indicating the need for an intermediate horizon to mitigate distribution shift (i.e., the gap between on-policy and past off-policy samples). (2) Moreover, a longer model rollout may reduce value estimation bias but raise the variance of policy gradients due to backpropagation through multiple steps, implying another intermediate horizon for stable gradient estimates. However, these two optimal horizons may differ. To resolve this conflict, we propose Double Horizon Model-Based Policy Optimization (DHMBPO), which divides the rollout procedure into a long ``distribution rollout'' (DR) and a short ``training rollout'' (TR). The DR generates on-policy state samples for mitigating distribution shift. In contrast, the short TR leverages differentiable transitions to offer accurate value gradient estimation with stable gradient updates, thereby requiring fewer updates and reducing overall runtime. We demonstrate that the double-horizon approach effectively balances distribution shift, model bias, and gradient instability, and surpasses existing MBRL methods on continuous-control benchmarks in terms of both sample efficiency and runtime",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akihiro Kubo",
      "Paavo Parmas",
      "Shin Ishii"
    ]
  },
  "https://openreview.net/forum?id=vPVqQmjCy8": {
    "title": "LLM-TS Integrator: Integrating LLM for Enhanced Time Series Modeling",
    "volume": "main",
    "abstract": "Time series~(TS) modeling is essential in dynamic systems like weather prediction and anomaly detection. Recent studies utilize Large Language Models (LLMs) for TS modeling, leveraging their powerful pattern recognition capabilities. These methods primarily position LLMs as the predictive backbone, often omitting the mathematical modeling within traditional TS models, such as periodicity. However, disregarding the potential of LLMs also overlooks their pattern recognition capabilities. To address this gap, we introduce \\textit{LLM-TS Integrator}, a novel framework that effectively integrates the capabilities of LLMs into traditional TS modeling. Central to this integration is our \\textit{mutual information} module. The core of this \\textit{mutual information} module is a traditional TS model enhanced with LLM-derived insights for improved predictive abilities. This enhancement is achieved by maximizing the mutual information between traditional model's TS representations and LLM's textual representation counterparts, bridging the two modalities. Moreover, we recognize that samples vary in importance for two losses: traditional prediction and mutual information maximization. To address this variability, we introduce the \\textit{sample reweighting} module to improve information utilization. This module assigns dual weights to each sample: one for prediction loss and another for mutual information loss, dynamically optimizing these weights via bi-level optimization. Our method achieves state-of-the-art or comparable performance across five mainstream TS tasks, including short-term and long-term forecasting, imputation, classification, and anomaly detection. Our code is available at: \\url{https://anonymous.4open.science/r/llm_ts_anonymous-F07D/README.MD}",
    "checked": true,
    "id": "013fefa7c76d4f07cd5b40bb12553cb17be9e98c",
    "semantic_title": "llm-ts integrator: integrating llm for enhanced time series modeling",
    "citation_count": 1,
    "authors": [
      "Can Chen",
      "Gabriel L. Oliveira",
      "Hossein Sharifi-Noghabi",
      "Tristan Sylvain"
    ]
  },
  "https://openreview.net/forum?id=beqSqPgE33": {
    "title": "Covariate-dependent Graphical Model Estimation via Neural Networks with Statistical Guarantees",
    "volume": "main",
    "abstract": "Graphical models are widely used in diverse application domains to model the conditional dependencies amongst a collection of random variables. In this paper, we consider settings where the graph structure is covariate-dependent, and investigate a deep neural network-based approach to estimate it. The method allows for flexible functional dependency on the covariate, and fits the data reasonably well in the absence of a Gaussianity assumption. Theoretical results with PAC guarantees are established for the method, under assumptions commonly used in an Empirical Risk Minimization framework. The performance of the proposed method is evaluated on several synthetic data settings and benchmarked against existing approaches. The method is further illustrated on real datasets involving data from neuroscience and finance, respectively, and produces interpretable results",
    "checked": true,
    "id": "f94e7f3658b928895c03be91b72d7b0e0a24f87b",
    "semantic_title": "covariate-dependent graphical model estimation via neural networks with statistical guarantees",
    "citation_count": 0,
    "authors": [
      "Jiahe Lin",
      "Yikai Zhang",
      "George Michailidis"
    ]
  },
  "https://openreview.net/forum?id=X6IY04Akw1": {
    "title": "Generalizable and Robust Spectral Method for Multi-view Representation Learning",
    "volume": "main",
    "abstract": "Multi-view representation learning (MvRL) has garnered substantial attention in recent years, driven by the increasing demand for applications that can effectively process and analyze data from multiple sources. In this context, graph Laplacian-based MvRL methods have demonstrated remarkable success in representing multi-view data. However, these methods often struggle with generalization to new data and face challenges with scalability. Moreover, in many practical scenarios, multi-view data is contaminated by noise or outliers. In such cases, modern deep-learning-based MvRL approaches that rely on alignment or contrastive objectives present degraded performance in downstream tasks, as they may impose incorrect consistency between clear and corrupted data sources. We introduce *SpecRaGE*, a novel fusion-based framework that integrates the strengths of graph Laplacian methods with the power of deep learning to overcome these challenges. SpecRage uses neural networks to learn parametric mapping that approximates a joint diagonalization of graph Laplacians. This solution bypasses the need for alignment while enabling generalizable and scalable learning of informative and meaningful representations. Moreover, it incorporates a meta-learning fusion module that dynamically adapts to data quality, ensuring robustness against outliers and noisy views. Our extensive experiments demonstrate that SpecRaGE outperforms state-of-the-art methods, particularly in scenarios with data contamination, paving the way for more reliable and efficient multi-view learning. Our code will be made publicly available upon acceptance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amitai Yacobi",
      "Ofir Lindenbaum",
      "Uri Shaham"
    ]
  },
  "https://openreview.net/forum?id=5qo8MF3QU1": {
    "title": "Out-of-Distribution Learning with Human Feedback",
    "volume": "main",
    "abstract": "Out-of-distribution (OOD) learning often relies on strong statistical assumptions or predefined OOD data distributions, limiting its effectiveness in real-world deployment for both OOD generalization and detection, especially when human inspection is minimal. This paper introduces a novel framework for OOD learning that integrates human feedback to enhance model adaptation and reliability. Our approach leverages freely available unlabeled data in the wild, which naturally captures environmental test-time OOD distributions under both covariate and semantic shifts. To effectively utilize such data, we propose selectively acquiring human feedback to label a small subset of informative samples. These labeled samples are then used to train both a multi-class classifier and an OOD detector. By incorporating human feedback, our method significantly improves model robustness and precision in handling OOD scenarios. We provide theoretical insights by establishing generalization error bounds for our algorithm. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods by a significant margin. Code is publicly available at https://github.com/HaoyueBaiZJU/ood-hf",
    "checked": true,
    "id": "04bf3cb0a104edd715d7ba639822174a86af7c11",
    "semantic_title": "out-of-distribution learning with human feedback",
    "citation_count": 3,
    "authors": [
      "Haoyue Bai",
      "Xuefeng Du",
      "Katie Rainey",
      "Shibin Parameswaran",
      "Yixuan Li"
    ]
  },
  "https://openreview.net/forum?id=B9BHjTN4z6": {
    "title": "RLeXplore: Accelerating Research in Intrinsically-Motivated Reinforcement Learning",
    "volume": "main",
    "abstract": "Extrinsic rewards can effectively guide reinforcement learning (RL) agents in specific tasks. However, extrinsic rewards frequently fall short in complex environments due to the significant human effort needed for their design and annotation. This limitation underscores the necessity for intrinsic rewards, which offer auxiliary and dense signals and can enable agents to learn in an unsupervised manner. Although various intrinsic reward formulations have been proposed, their implementation and optimization details are insufficiently explored and lack standardization, thereby hindering research progress. To address this gap, we introduce RLeXplore, a unified, highly modularized, and plug-and-play framework offering reliable implementations of eight state-of-the-art intrinsic reward methods. Furthermore, we conduct an in-depth study that identifies critical implementation details and establishes well-justified standard practices in intrinsically-motivated RL. Our documentation, examples, and source code are available at [https://github.com/RLE-Foundation/RLeXplore](https://github.com/RLE-Foundation/RLeXplore)",
    "checked": true,
    "id": "769d8fdf6520c52e8767ee6d54f6417cf6e7904e",
    "semantic_title": "rlexplore: accelerating research in intrinsically-motivated reinforcement learning",
    "citation_count": 0,
    "authors": [
      "Mingqi Yuan",
      "Roger Creus Castanyer",
      "Bo Li",
      "Xin Jin",
      "Wenjun Zeng",
      "Glen Berseth"
    ]
  },
  "https://openreview.net/forum?id=hiiRCXmbAz": {
    "title": "Hyperparameters in Continual Learning: A Reality Check",
    "volume": "main",
    "abstract": "Continual learning (CL) aims to train a model on a sequence of tasks (i.e., a CL scenario) while balancing the trade-off between plasticity (learning new tasks) and stability (retaining prior knowledge). The dominantly adopted conventional evaluation protocol for CL algorithms selects the best hyperparameters (e.g., learning rate, mini-batch size, regularization strengths, etc.) within a given scenario and then evaluates the algorithms using these hyperparameters in the same scenario. However, this protocol has significant shortcomings: it overestimates the CL capacity of algorithms and relies on unrealistic hyperparameter tuning, which is not feasible for real-world applications. From the fundamental principles of evaluation in machine learning, we argue that the evaluation of CL algorithms should focus on assessing the generalizability of their CL capacity to unseen scenarios. Based on this, we propose the Generalizable Two-phase Evaluation Protocol (GTEP) consisting of hyperparameter tuning and evaluation phases. Both phases share the same scenario configuration (e.g., number of tasks) but are generated from different datasets. Hyperparameters of CL algorithms are tuned in the first phase and applied in the second phase to evaluate the algorithms. We apply this protocol to class-incremental learning, both with and without pretrained models. Across more than 8,000 experiments, our results show that most state-of-the-art algorithms fail to replicate their reported performance, highlighting that their CL capacity has been significantly overestimated in the conventional evaluation protocol",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungmin Cha",
      "Kyunghyun Cho"
    ]
  },
  "https://openreview.net/forum?id=Ucpfdn66k2": {
    "title": "When Are Bias-Free ReLU Networks Effectively Linear Networks?",
    "volume": "main",
    "abstract": "We investigate the implications of removing bias in ReLU networks regarding their expressivity and learning dynamics. We first show that two-layer bias-free ReLU networks have limited expressivity: the only odd function two-layer bias-free ReLU networks can express is a linear one. We then show that, under symmetry conditions on the data, these networks have the same learning dynamics as linear networks. This enables us to give analytical time-course solutions to certain two-layer bias-free (leaky) ReLU networks outside the lazy learning regime. While deep bias-free ReLU networks are more expressive than their two-layer counterparts, they still share a number of similarities with deep linear networks. These similarities enable us to leverage insights from linear networks to understand certain ReLU networks. Overall, our results show that some properties previously established for bias-free ReLU networks arise due to equivalence to linear networks",
    "checked": true,
    "id": "929e9a89b82c95f52eb23c7b932c1969487a5238",
    "semantic_title": "when are bias-free relu networks effectively linear networks?",
    "citation_count": 0,
    "authors": [
      "Yedi Zhang",
      "Andrew M Saxe",
      "Peter E. Latham"
    ]
  },
  "https://openreview.net/forum?id=nannw4SGfS": {
    "title": "Accelerating Learned Image Compression Through Modeling Neural Training Dynamics",
    "volume": "main",
    "abstract": "As learned image compression (LIC) methods become increasingly computationally demanding, enhancing their training efficiency is crucial. This paper takes a step forward in accelerating the training of LIC methods by modeling the neural training dynamics. We first propose a Sensitivity-aware True and Dummy Embedding Training mechanism (STDET) that clusters LIC model parameters into few separate modes where parameters are expressed as affine transformations of reference parameters within the same mode. By further utilizing the stable intra-mode correlations throughout training and parameter sensitivities, we gradually embed non-reference parameters, reducing the number of trainable parameters. Additionally, we incorporate a Sampling-then-Moving Average (SMA) technique, interpolating sampled weights from stochastic gradient descent (SGD) training to obtain the moving average weights, ensuring smooth temporal behavior and minimizing training state variances. Overall, our method significantly reduces training space dimensions and the number of trainable parameters without sacrificing model performance, thus accelerating model convergence. We also provide a theoretical analysis on the Noisy quadratic model, showing that the proposed method achieves a lower training variance than standard SGD. Our approach offers valuable insights for further developing efficient training methods for LICs",
    "checked": true,
    "id": "6abeb9b5b6ad6c3f6ea2b5fb6a0f0b2ddff59f49",
    "semantic_title": "accelerating learned image compression through modeling neural training dynamics",
    "citation_count": 0,
    "authors": [
      "Yichi Zhang",
      "Zhihao Duan",
      "Yuning Huang",
      "Fengqing Zhu"
    ]
  },
  "https://openreview.net/forum?id=BaRD2Nfj41": {
    "title": "Overcoming Knowledge Barriers: Online Imitation Learning from Visual Observation with Pretrained World Models",
    "volume": "main",
    "abstract": "Pretraining and finetuning models has become increasingly popular in decision-making. But there are still serious impediments in Imitation Learning from Observation (ILfO) with pretrained models. This study identifies two primary obstacles: the Embodiment Knowledge Barrier (EKB) and the Demonstration Knowledge Barrier (DKB). The EKB emerges due to the pretrained models' limitations in handling novel observations, which leads to inaccurate action inference. Conversely, the DKB stems from the reliance on limited demonstration datasets, restricting the model's adaptability across diverse scenarios. We propose separate solutions to overcome each barrier and apply them to Action Inference by Maximising Evidence (AIME), a state-of-the-art algorithm. This new algorithm, AIME-NoB, integrates online interactions and a data-driven regulariser to mitigate the EKB. Additionally, it uses a surrogate reward function to broaden the policy's supported states, addressing the DKB. Our experiments on vision-based control tasks from the DeepMind Control Suite and MetaWorld benchmarks show that AIME-NoB significantly improves sample efficiency and converged performance, presenting a robust framework for overcoming the challenges in ILfO with pretrained models. Code available at https://github.com/IcarusWizard/AIME-NoB",
    "checked": true,
    "id": "a8ad39fc162c238b5c126a2d350d00dd7ab1ba87",
    "semantic_title": "overcoming knowledge barriers: online imitation learning from visual observation with pretrained world models",
    "citation_count": 0,
    "authors": [
      "Xingyuan Zhang",
      "Philip Becker-Ehmck",
      "Patrick van der Smagt",
      "Maximilian Karl"
    ]
  },
  "https://openreview.net/forum?id=yGGoOVpBVP": {
    "title": "Connecting Parameter Magnitudes and Hessian Eigenspaces at Scale using Sketched Methods",
    "volume": "main",
    "abstract": "Recently, it has been observed that when training a deep neural net with SGD, the majority of the loss landscape's curvature quickly concentrates in a tiny *top* eigenspace of the loss Hessian, which remains largely stable thereafter. Independently, it has been shown that successful magnitude pruning masks for deep neural nets emerge early in training and remain stable thereafter. In this work, we study these two phenomena jointly and show that they are connected: We develop a methodology to measure the similarity between arbitrary parameter masks and Hessian eigenspaces via Grassmannian metrics. We identify *overlap* as the most useful such metric due to its interpretability and stability. To compute *overlap*, we develop a matrix-free algorithm based on sketched SVDs that allows us to compute over 1000 Hessian eigenpairs for nets with over 10M parameters --an unprecedented scale by several orders of magnitude. Our experiments reveal an *overlap* between magnitude parameter masks and top Hessian eigenspaces consistently higher than chance-level, and that this effect gets accentuated for larger network sizes. This result indicates that *top Hessian eigenvectors tend to be concentrated around larger parameters*, or equivalently, that *larger parameters tend to align with directions of larger loss curvature*. Our work provides a methodology to approximate and analyze deep learning Hessians at scale, as well as a novel insight on the structure of their eigenspace",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andres Fernandez",
      "Frank Schneider",
      "Maren Mahsereci",
      "Philipp Hennig"
    ]
  },
  "https://openreview.net/forum?id=mvbZBaqSXo": {
    "title": "Dimension reduction via score ratio matching",
    "volume": "main",
    "abstract": "Gradient-based dimension reduction decreases the cost of Bayesian inference and probabilistic modeling by identifying maximally informative (and informed) low-dimensional projections of the data and parameters, allowing high-dimensional problems to be reformulated as cheaper low-dimensional problems. A broad family of such techniques identify these projections and provide error bounds on the resulting posterior approximations, via eigendecompositions of certain diagnostic matrices. Yet these matrices require gradients or even Hessians of the log-likelihood, excluding the purely data-driven setting and many problems of simulation-based inference. We propose a framework, derived from score-matching, to extend gradient-based dimension reduction to problems where gradients are unavailable. Specifically, we formulate an objective function to directly learn the score ratio function needed to compute the diagnostic matrices, propose a tailored parameterization for the score ratio network, and introduce regularization methods that capitalize on the hypothesized low-dimensional structure. We also introduce a novel algorithm to iteratively identify the low-dimensional reduced basis vectors more accurately with limited data based on eigenvalue deflation methods. We show that our approach outperforms standard score-matching for problems with low-dimensional structure, and demonstrate its effectiveness for PDE-constrained Bayesian inverse problems and conditional generative modeling",
    "checked": true,
    "id": "b9018ae0afe7f92707f5cdcb2b95e1b831d343a7",
    "semantic_title": "dimension reduction via score ratio matching",
    "citation_count": 1,
    "authors": [
      "Ricardo Baptista",
      "Michael Brennan",
      "Youssef Marzouk"
    ]
  },
  "https://openreview.net/forum?id=eIPwJgadfZ": {
    "title": "Convex Relaxation for Solving Large-Margin Classifiers in Hyperbolic Space",
    "volume": "main",
    "abstract": "Hyperbolic spaces have increasingly been recognized for their outstanding performance in handling data with inherent hierarchical structures compared to their Euclidean counterparts. However, learning in hyperbolic spaces poses significant challenges. In particular, extending support vector machines to hyperbolic spaces is in general a constrained non-convex optimization problem. Previous and popular attempts to solve hyperbolic SVMs, primarily using projected gradient descent, are generally sensitive to hyperparameters and initializations, often leading to suboptimal solutions. In this work, by first rewriting the problem into a polynomial optimization, we apply semidefinite relaxation and sparse moment-sum-of-squares relaxation to effectively approximate the optima. From extensive empirical experiments, these methods are shown to achieve better classification accuracies than the projected gradient descent approach in most of the synthetic and real two-dimensional hyperbolic embedding dataset under the one-vs-rest multiclass-classification scheme",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Yang",
      "Peihan Liu",
      "Cengiz Pehlevan"
    ]
  },
  "https://openreview.net/forum?id=MbF1gYfIlY": {
    "title": "Can Kernel Methods Explain How the Data Affects Neural Collapse?",
    "volume": "main",
    "abstract": "A vast amount of literature has recently focused on the \"Neural Collapse\" (NC) phenomenon, which emerges when training neural network (NN) classifiers beyond the zero training error point. The core component of NC is the decrease in the within-class variability of the network's deepest features, dubbed as NC1. The theoretical works that study NC are typically based on simplified unconstrained features models (UFMs) that mask any effect of the data on the extent of collapse. To address this limitation of UFMs, this paper explores the possibility of analyzing NC1 using kernels associated with shallow NNs. We begin by formulating an NC1 metric as a function of the kernel. Then, we specialize it to the NN Gaussian Process kernel (NNGP) and the Neural Tangent Kernel (NTK), associated with wide networks at initialization and during gradient-based training with a small learning rate, respectively. As a key result, we show that the NTK does not represent more collapsed features than the NNGP for Gaussian data of arbitrary dimensions. This showcases the limitations of data-independent kernels such as NTK in approximating the NC behavior of NNs. As an alternative to NTK, we then empirically explore a recently proposed data-aware Gaussian Process kernel, which generalizes NNGP to model feature learning. We show that this kernel yields lower NC1 than NNGP but may not follow the trends of the shallow NN. Our study demonstrates that adaptivity to data may allow kernel-based analysis of NC, though further advancements in this area are still needed. A nice byproduct of our study is showing both theoretically and empirically that the choice of nonlinear activation function affects NC1 (with ERF yielding lower values than ReLU)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vignesh Kothapalli",
      "Tom Tirer"
    ]
  },
  "https://openreview.net/forum?id=CovLQwu611": {
    "title": "ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization",
    "volume": "main",
    "abstract": "Parameter-efficient fine-tuning (PEFT) enables creation of specialized language models for diverse tasks, resulting in numerous expert modules. In many practical use cases, these expert PEFT modules are integrated into a single model that answers arbitrary queries by routing queries to different experts. However, only a few experts can be kept in GPU memory due to memory constraints. Consequently, expert modules are frequently loaded and offloaded between CPU/GPU memory or disk storage. This frequent swapping dramatically increases communication overhead, leading unacceptable latency and degrading user experience. The large size of modern PEFT modules further exacerbates this latency. For example, QLoRA experts for 65B LLaMA are 3.2GB, making swapping a major communication bottleneck, particularly in memory-constrained environments. To address these issues, we present ComPEFT (compressed PEFT), a novel method for compressing fine-tuning residuals (task vectors) of PEFT models. Reducing expert PEFT module size effectively addresses both memory and communication limitations, facilitating faster swapping and enabling a higher density of experts within a given memory footprint. ComPEFT employs sparsification and ternary quantization to reduce PEFT module size without any additional training while preserving or enhancing model performance. Extensive evaluation across T5, T0, and LLaMA-based models with 200M − 65B parameters, ComPEFT achieves compression ratios of 8x − 50x. Specifically, we show that ComPEFT improves with scale – stronger models exhibit higher compressibility and better performance. We show ComPEFT applied to LLaMA − 65B outperforms QLoRA by 4.16% on MMLU with a 26x storage size reduction. Additionally, compressed experts produced by ComPEFT maintain few-shot compositional generalization capabilities, facilitate efficient communication and computation, and exhibit enhanced performance when merged. Lastly, we provide an analysis of different method components, compare ComPEFT with other PEFT methods, and test its efficacy for compressing full finetuning residual",
    "checked": true,
    "id": "dff9b29918369f2ce7c06d13258ffad5c644788a",
    "semantic_title": "compeft: compression for communicating parameter efficient updates via sparsification and quantization",
    "citation_count": 13,
    "authors": [
      "Prateek Yadav",
      "Leshem Choshen",
      "Colin Raffel",
      "Mohit Bansal"
    ]
  },
  "https://openreview.net/forum?id=322PpCGAX8": {
    "title": "MaxCutBench: Revisiting and Benchmarking Graph Neural Networks for Maximum Cut",
    "volume": "main",
    "abstract": "Recently, there has been much work on designing general heuristics for graph-based, combinatorial optimization problems via the incorporation of Graph Neural Networks (GNNs) to learn distribution-specific solution structures. However, there is a lack of consistency in evaluating these heuristics in terms of the baselines and instances chosen, making it difficult to assess the relative performance of the algorithms. In this paper, we introduce \\textbf{MaxCutBench}—an open-source benchmark suite dedicated to the NP-hard Maximum Cut problem. The suite offers a unified interface for $16$ algorithms, both traditional and machine-learning-based. Using our benchmark, we conduct an in-depth analysis of the implemented algorithms on a carefully selected set of hard instances from diverse graph datasets. Our main finding is that classical local search heuristics can outperform several highly cited learning-based approaches, including S2V-DQN (Khalil et al., 2017), ECO-DQN (Barrett et al., 2020), among others, in terms of objective value, generalization, inference time, and scalability. Additionally, we find that the performance of ECO-DQN either remains the same or improves when the GNN is replaced by simple linear regression. We hope our benchmark will contribute to the efforts of the community to standardize the evaluation of learned heuristics for combinatorial optimization. Code, data, and pre-trained models are available at: \\url{https://github.com/ankurnath/MaxCut-Bench}",
    "checked": true,
    "id": "e315b73b783eed4b628d6aa7692f13db5c201e54",
    "semantic_title": "maxcutbench: revisiting and benchmarking graph neural networks for maximum cut",
    "citation_count": 0,
    "authors": [
      "Ankur Nath",
      "Alan Kuhnle"
    ]
  },
  "https://openreview.net/forum?id=YBPbMKJbLd": {
    "title": "Future-aware Safe Active Learning of Time Varying Systems using Gaussian Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": "fdc69c043851bacf57ed840d930609bdfdfe09f5",
    "semantic_title": "future aware safe active learning of time varying systems using gaussian processes",
    "citation_count": 0,
    "authors": [
      "Markus Lange-Hegermann",
      "Christoph Zimmer"
    ]
  },
  "https://openreview.net/forum?id=CgWkVb2lHB": {
    "title": "VLM's Eye Examination: Instruct and Inspect Visual Competency of Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "698bdd195324d52b3998541015bd0fe3db3ffef7",
    "semantic_title": "vlm's eye examination: instruct and inspect visual competency of vision language models",
    "citation_count": 3,
    "authors": [
      "Nam Hyeon-Woo",
      "Moon Ye-Bin",
      "Wonseok Choi",
      "Lee Hyun",
      "Tae-Hyun Oh"
    ]
  },
  "https://openreview.net/forum?id=jdvnaki7ZY": {
    "title": "Jet: A Modern Transformer-Based Normalizing Flow",
    "volume": "main",
    "abstract": "",
    "checked": true,
    "id": "0dc3ce7fa1ad45e39367da7118ce0947e69c5a0d",
    "semantic_title": "jet: a modern transformer-based normalizing flow",
    "citation_count": 2,
    "authors": [
      "Alexander Kolesnikov",
      "André Susano Pinto",
      "Michael Tschannen"
    ]
  },
  "https://openreview.net/forum?id=j6Rm6T2lFU": {
    "title": "Deep Koopman Learning using Noisy Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjian Hao",
      "Devesh Upadhyay",
      "Shaoshuai Mou"
    ]
  },
  "https://openreview.net/forum?id=DqPCWMiMU0": {
    "title": "CoDe: Blockwise Control for Denoising Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anuj Singh",
      "Sayak Mukherjee",
      "Ahmad Beirami",
      "Hadi J. Rad"
    ]
  },
  "https://openreview.net/forum?id=7rqV7Cb67L": {
    "title": "Fairness-Aware Dense Subgraph Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emmanouil Kariotakis",
      "Nicholas D Sidiropoulos",
      "Aritra Konar"
    ]
  },
  "https://openreview.net/forum?id=jXcx2oAIbw": {
    "title": "LLM-Guided Self-Supervised Tabular Learning With Task-Specific Pre-text Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungwon Han",
      "Seungeon Lee",
      "Meeyoung Cha",
      "Sercan O Arik",
      "Jinsung Yoon"
    ]
  },
  "https://openreview.net/forum?id=9aiuB3kIjd": {
    "title": "FragFormer: A Fragment-based Representation Learning Framework for Molecular Property Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxi Wang",
      "Yaosen Min",
      "Miao Li",
      "Ji Wu"
    ]
  },
  "https://openreview.net/forum?id=spqbyeGyLR": {
    "title": "When resampling/reweighting improves feature learning in imbalanced classification? A toy-model study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomoyuki Obuchi",
      "Toshiyuki Tanaka"
    ]
  },
  "https://openreview.net/forum?id=Gdf4P7sEzE": {
    "title": "HyperMagNet: A Magnetic Laplacian based Hypergraph Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tatyana Benko",
      "Martin Buck",
      "Ilya Amburg",
      "Stephen J. Young",
      "Sinan Guven Aksoy"
    ]
  },
  "https://openreview.net/forum?id=TWOTKhwU5n": {
    "title": "ODEStream: A Buffer-Free Online Learning Framework with ODE-based Adaptor for Streaming Time Series Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Futoon M. Abushaqra",
      "Hao Xue",
      "Yongli Ren",
      "Flora D. Salim"
    ]
  },
  "https://openreview.net/forum?id=n4AaKOBWbB": {
    "title": "Amphibian: A Meta-Learning Framework for Rehearsal-Free, Fast Online Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gobinda Saha",
      "Kaushik Roy"
    ]
  },
  "https://openreview.net/forum?id=qvJraN50DT": {
    "title": "Sample-efficient decoding of visual stimuli from fMRI through inter-individual functional alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexis Thual",
      "Yohann Benchetrit",
      "Felix Geilert",
      "Jérémy Rapin",
      "Iurii Makarov",
      "Stanislas Dehaene",
      "Bertrand Thirion",
      "Hubert Banville",
      "Jean-Remi King"
    ]
  },
  "https://openreview.net/forum?id=16f7ea1N3p": {
    "title": "LLM-Select: Feature Selection with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel P Jeong",
      "Zachary Chase Lipton",
      "Pradeep Kumar Ravikumar"
    ]
  },
  "https://openreview.net/forum?id=MTrhFmkC45": {
    "title": "Reproducibility Study of \"Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jose L. Garcia",
      "Karolina Hajkova",
      "Maria Marchenko",
      "Carlos Miguel Patiño"
    ]
  },
  "https://openreview.net/forum?id=DVeFqV56Iz": {
    "title": "Change Point Detection in Dynamic Graphs with Decoder-only Latent Space Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yik Lun Kei",
      "Jialiang Li",
      "Hangjian Li",
      "Yanzhen Chen",
      "OSCAR HERNAN MADRID PADILLA"
    ]
  },
  "https://openreview.net/forum?id=OPFnpl7KiF": {
    "title": "Design Editing for Offline Model-based Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Yuan",
      "Youyuan Zhang",
      "Can Chen",
      "Haolun Wu",
      "Melody Zixuan Li",
      "Jianmo Li",
      "James J. Clark",
      "Xue Liu"
    ]
  },
  "https://openreview.net/forum?id=8L3khbpUJL": {
    "title": "Referential communication in heterogeneous communities of pre-trained visual deep networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matéo Mahaut",
      "Roberto Dessi",
      "Francesca Franzon",
      "Marco Baroni"
    ]
  },
  "https://openreview.net/forum?id=dghM7sOudh": {
    "title": "MemLLM: Finetuning LLMs to Use Explicit Read-Write Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Modarressi",
      "Abdullatif Köksal",
      "Ayyoob Imani",
      "Mohsen Fayyaz",
      "Hinrich Schuetze"
    ]
  },
  "https://openreview.net/forum?id=UdcF3JbSKb": {
    "title": "Accelerating Non-Conjugate Gaussian Processes By Trading Off Computation For Uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lukas Tatzel",
      "Jonathan Wenger",
      "Frank Schneider",
      "Philipp Hennig"
    ]
  },
  "https://openreview.net/forum?id=B4SyciDyIh": {
    "title": "Optimal Embedding Guided Negative Sample Generation for Knowledge Graph Link Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Makoto Takamoto",
      "Daniel Onoro Rubio",
      "Wiem Ben Rim",
      "Takashi Maruyama",
      "Bhushan Kotnis"
    ]
  },
  "https://openreview.net/forum?id=muWEt1TOyo": {
    "title": "SE3Set: Harnessing Equivariant Hypergraph Neural Networks for Molecular Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongfei Wu",
      "Lijun Wu",
      "Guoqing Liu",
      "Zhirong Liu",
      "Bin Shao",
      "Zun Wang"
    ]
  },
  "https://openreview.net/forum?id=l4Qnj4tHBx": {
    "title": "Oblique Bayesian Additive Regression Trees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul-Hieu V. Nguyen",
      "Ryan Yee",
      "Sameer Deshpande"
    ]
  },
  "https://openreview.net/forum?id=FIWHRSuoos": {
    "title": "Leveraging Gradients for Unsupervised Accuracy Estimation under Distribution Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "RENCHUNZI XIE",
      "Ambroise Odonnat",
      "Vasilii Feofanov",
      "Ievgen Redko",
      "Jianfeng Zhang",
      "Bo An"
    ]
  },
  "https://openreview.net/forum?id=PJUbMDkQVY": {
    "title": "Scaling Laws for Predicting Downstream Performance in LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangyi Chen",
      "Binxuan Huang",
      "Yifan Gao",
      "Zhengyang Wang",
      "Jingfeng Yang",
      "Heng Ji"
    ]
  },
  "https://openreview.net/forum?id=c7vkDg558Z": {
    "title": "EDM-TTS: Efficient Dual-Stage Masked Modeling for Alignment-Free Text-to-Speech Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nabarun Goswami",
      "Hanqin Wang",
      "Tatsuya Harada"
    ]
  },
  "https://openreview.net/forum?id=B6y12Ot0cP": {
    "title": "Formal Verification of Graph Convolutional Networks with Uncertain Node Features and Uncertain Graph Structure",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Ladner",
      "Michael Eichelbeck",
      "Matthias Althoff"
    ]
  },
  "https://openreview.net/forum?id=J5IRyTKZ9s": {
    "title": "An Adversarial Perspective on Machine Unlearning for AI Safety",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jakub Łucki",
      "Boyi Wei",
      "Yangsibo Huang",
      "Peter Henderson",
      "Florian Tramèr",
      "Javier Rando"
    ]
  },
  "https://openreview.net/forum?id=GaUtrgXMHe": {
    "title": "Bayesian Transferability Assessment for Spiking Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiqing Hao",
      "Wenhui Wang"
    ]
  },
  "https://openreview.net/forum?id=vttqWoSJIW": {
    "title": "Relative Phase Equivariant Deep Neural Systems for Physical Layer Communications",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arwin Gansekoele",
      "Sandjai Bhulai",
      "Mark Hoogendoorn",
      "Rob van der Mei"
    ]
  },
  "https://openreview.net/forum?id=D2PjEPGXgh": {
    "title": "Multi-Bellman operator for convergence of $Q$-learning with linear function approximation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diogo S. Carvalho",
      "Pedro A. Santos",
      "Francisco S. Melo"
    ]
  },
  "https://openreview.net/forum?id=laPAh2hRFC": {
    "title": "SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Robey",
      "Eric Wong",
      "Hamed Hassani",
      "George J. Pappas"
    ]
  },
  "https://openreview.net/forum?id=haP586YomL": {
    "title": "Reward Distance Comparisons Under Transition Sparsity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clement Nyanhongo",
      "Bruno Miranda Henrique",
      "Eugene Santos"
    ]
  },
  "https://openreview.net/forum?id=sNzBi8rZTy": {
    "title": "Reinforcement Learning for Causal Discovery without Acyclicity Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bao Duong",
      "Hung Le",
      "Biwei Huang",
      "Thin Nguyen"
    ]
  },
  "https://openreview.net/forum?id=TR6iUG8i6Z": {
    "title": "Federated Spectral Graph Transformers Meet Neural Ordinary Differential Equations for Non-IID Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kishan Gurumurthy",
      "Himanshu Pal",
      "Charu Sharma"
    ]
  },
  "https://openreview.net/forum?id=z37LCgSIzI": {
    "title": "ResiDual Transformer Alignment with Spectral Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Basile",
      "Valentino Maiorca",
      "Luca Bortolussi",
      "Emanuele Rodolà",
      "Francesco Locatello"
    ]
  },
  "https://openreview.net/forum?id=uKZ0R4IQaO": {
    "title": "Dynamic Pricing in the Linear Valuation Model using Shape Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniele Bracale",
      "Moulinath Banerjee",
      "Yuekai Sun",
      "Salam Turki",
      "Kevin Stoll"
    ]
  },
  "https://openreview.net/forum?id=9Xj5w4DX0t": {
    "title": "Rank Suggestion in Non-negative Matrix Factorization: Residual Sensitivity to Initial Conditions (RSIC)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marc A. Tunnell",
      "Zachary DeBruine",
      "Erin Carrier"
    ]
  },
  "https://openreview.net/forum?id=cFmmaxkD5A": {
    "title": "Downstream Task Guided Masking Learning in Masked Autoencoders Using Multi-Level Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Guo",
      "Ramtin Hosseini",
      "Ruiyi Zhang",
      "Sai Ashish Somayajula",
      "Ranak Roy Chowdhury",
      "Rajesh K. Gupta",
      "Pengtao Xie"
    ]
  },
  "https://openreview.net/forum?id=ntGPYNUF3t": {
    "title": "Latte: Latent Diffusion Transformer for Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Ma",
      "Yaohui Wang",
      "Xinyuan Chen",
      "Gengyun Jia",
      "Ziwei Liu",
      "Yuan-Fang Li",
      "Cunjian Chen",
      "Yu Qiao"
    ]
  },
  "https://openreview.net/forum?id=LJHVPWNnV6": {
    "title": "Graph Potential Field Neural Network for Massive Agents Group-wise Path Planning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yueming Lyu",
      "Xiaowei Zhou",
      "Xingrui Yu",
      "Ivor Tsang"
    ]
  },
  "https://openreview.net/forum?id=JT2KMuo2BV": {
    "title": "Rethinking Patch Dependence for Masked Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Letian Fu",
      "Long Lian",
      "Renhao Wang",
      "Baifeng Shi",
      "XuDong Wang",
      "Adam Yala",
      "Trevor Darrell",
      "Alexei A Efros",
      "Ken Goldberg"
    ]
  },
  "https://openreview.net/forum?id=FkKBxp0FhR": {
    "title": "A Systematic Evaluation of the Planning and Scheduling Abilities of the Reasoning Model o1",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karthik Valmeekam",
      "Kaya Stechly",
      "Atharva Gundawar",
      "Subbarao Kambhampati"
    ]
  },
  "https://openreview.net/forum?id=7bIfe2I7bK": {
    "title": "Evaluating Compositional Scene Understanding in Multimodal Generative Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuhao Fu",
      "Andrew Jun Lee",
      "Yixin Anna Wang",
      "Ida Momennejad",
      "Trevor Bihl",
      "Hongjing Lu",
      "Taylor Whittington Webb"
    ]
  },
  "https://openreview.net/forum?id=3jdI0aEW3k": {
    "title": "Distributed and Secure Kernel-Based Quantum Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arjhun Swaminathan",
      "Mete Akgün"
    ]
  },
  "https://openreview.net/forum?id=X3gSvQjShh": {
    "title": "An Embedding is Worth a Thousand Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Di Salvo",
      "Sebastian Doerrich",
      "Ines Rieger",
      "Christian Ledig"
    ]
  },
  "https://openreview.net/forum?id=gxUp2d4JTw": {
    "title": "LTL-Constrained Policy Optimization with Cycle Experience Replay",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ameesh Shah",
      "Cameron Voloshin",
      "Chenxi Yang",
      "Abhinav Verma",
      "Swarat Chaudhuri",
      "Sanjit A. Seshia"
    ]
  },
  "https://openreview.net/forum?id=I1gALvbRxj": {
    "title": "Bézier Flow: a Surface-wise Gradient Descent Method for Multi-objective Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akiyoshi Sannai",
      "Yasunari Hikima",
      "Ken Kobayashi",
      "Akinori Tanaka",
      "Naoki Hamada"
    ]
  },
  "https://openreview.net/forum?id=SBM9yeNZz5": {
    "title": "Maximising the Utility of Validation Sets for Imbalanced Noisy-label Meta-learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoang Anh Dung",
      "Cuong C. Nguyen",
      "Vasileios Belagiannis",
      "Thanh-Toan Do",
      "Gustavo Carneiro"
    ]
  },
  "https://openreview.net/forum?id=sSOxuUjE2o": {
    "title": "Controlled Training Data Generation with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Teresa Yeo",
      "Andrei Atanov",
      "Harold Luc Benoit",
      "Aleksandr Alekseev",
      "Ruchira Ray",
      "Pooya Esmaeil Akhoondi",
      "Amir Zamir"
    ]
  },
  "https://openreview.net/forum?id=Okxp1W8If0": {
    "title": "(Accelerated) Noise-adaptive Stochastic Heavy-Ball Momentum",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anh Quang Dang",
      "Reza Babanezhad Harikandeh",
      "Sharan Vaswani"
    ]
  },
  "https://openreview.net/forum?id=nWk5OtZ7ze": {
    "title": "Quantile Activation: Correcting a failure mode of traditional ML models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Challa",
      "Sravan Danda",
      "Laurent Najman",
      "Snehanshu Saha"
    ]
  },
  "https://openreview.net/forum?id=hCyT4RsF27": {
    "title": "GOTHAM: Graph Class Incremental Learning Framework under Weak Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Hemant Shahane",
      "Prathosh AP",
      "Sandeep Kumar"
    ]
  },
  "https://openreview.net/forum?id=dNWaTuKV9M": {
    "title": "Bayesian Learning-driven Prototypical Contrastive Loss for Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nisha L. Raichur",
      "Lucas Heublein",
      "Tobias Feigl",
      "Alexander Rügamer",
      "Christopher Mutschler",
      "Felix Ott"
    ]
  },
  "https://openreview.net/forum?id=s1zfBJysbI": {
    "title": "Enhancing Temporal Consistency in Video Editing by Reconstructing Videos with 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inkyu Shin",
      "Qihang Yu",
      "Xiaohui Shen",
      "In So Kweon",
      "Kuk-Jin Yoon",
      "Liang-Chieh Chen"
    ]
  },
  "https://openreview.net/forum?id=GXlsrvOGIK": {
    "title": "On Learning Representations for Tabular Data Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inwon Kang",
      "Parikshit Ram",
      "Yi Zhou",
      "Horst Samulowitz",
      "Oshani Seneviratne"
    ]
  },
  "https://openreview.net/forum?id=baZLwdphqw": {
    "title": "Stabilizing the Kumaraswamy Distribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Max Wasserman",
      "Gonzalo Mateos"
    ]
  },
  "https://openreview.net/forum?id=AHTz2mTlKk": {
    "title": "Empirical Bayes Trend Filtering Through a Variational Inference Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyue Xie"
    ]
  },
  "https://openreview.net/forum?id=MJOKrHqiV1": {
    "title": "Multi-Output Distributional Fairness via Post-Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gang Li",
      "Qihang Lin",
      "Ayush Ghosh",
      "Tianbao Yang"
    ]
  },
  "https://openreview.net/forum?id=uJELgNGiMW": {
    "title": "Meta-Learning to Teach Semantic Prompts for Open Domain Generalization in Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shirsha Bose",
      "Mainak Singha",
      "Ankit Jha",
      "Souradeep Mukhopadhyay",
      "Biplab Banerjee"
    ]
  },
  "https://openreview.net/forum?id=nay3Kvw8BD": {
    "title": "An Efficient Training Algorithm for Models with Block-wise Sparsity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ding Zhu",
      "Zhiqun Zuo",
      "Mohammad Mahdi Khalili"
    ]
  },
  "https://openreview.net/forum?id=OTwnNBxZFB": {
    "title": "Almost Sure Convergence of Stochastic Gradient Methods under Gradient Domination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Weissmann",
      "Sara Klein",
      "Waïss Azizian",
      "Leif Döring"
    ]
  },
  "https://openreview.net/forum?id=WfAvMdwiE8": {
    "title": "Consistency-Guided Asynchronous Contrastive Tuning for Few-Shot Class-Incremental Tuning of Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuvendu Roy",
      "Elham Dolatabadi",
      "Arash Afkanpour",
      "Ali Etemad"
    ]
  },
  "https://openreview.net/forum?id=heeJqQXKg7": {
    "title": "LitLLMs, LLMs for Literature Review: Are we there yet?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shubham Agarwal",
      "Gaurav Sahu",
      "Abhay Puri",
      "Issam H. Laradji",
      "Krishnamurthy Dj Dvijotham",
      "Jason Stanley",
      "Laurent Charlin",
      "Christopher Pal"
    ]
  },
  "https://openreview.net/forum?id=M62P7iOT7d": {
    "title": "DeformTime: capturing variable dependencies with deformable attention for time series forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Shu",
      "Vasileios Lampos"
    ]
  },
  "https://openreview.net/forum?id=9kFlOyLwyf": {
    "title": "Latent Covariate Shift: Unlocking Partial Identifiability for Multi-Source Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Liu",
      "Zhen Zhang",
      "Dong Gong",
      "Mingming Gong",
      "Biwei Huang",
      "Anton van den Hengel",
      "Kun Zhang",
      "Javen Qinfeng Shi"
    ]
  },
  "https://openreview.net/forum?id=Wj8yFjIpom": {
    "title": "$f$-Divergence Policy Optimization in Fully Decentralized Cooperative MARL",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kefan Su",
      "Zongqing Lu"
    ]
  },
  "https://openreview.net/forum?id=vQDKYYuqWA": {
    "title": "Vision-Language Models Provide Promptable Representations for Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Chen",
      "Oier Mees",
      "Aviral Kumar",
      "Sergey Levine"
    ]
  },
  "https://openreview.net/forum?id=pKilnjQsb0": {
    "title": "Implicit Bias and Fast Convergence Rates for Self-attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bhavya Vasudeva",
      "Puneesh Deora",
      "Christos Thrampoulidis"
    ]
  },
  "https://openreview.net/forum?id=sXq3Wb3vef": {
    "title": "Decomposing The Dark Matter of Sparse Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua Engels",
      "Logan Riggs Smith",
      "Max Tegmark"
    ]
  },
  "https://openreview.net/forum?id=Mae23iEqPS": {
    "title": "Predicting sub-population specific viral evolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxian Shi",
      "Menghua Wu",
      "Regina Barzilay"
    ]
  },
  "https://openreview.net/forum?id=SB7JzhDG45": {
    "title": "Simulation-based Bayesian Inference from Privacy Protected Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Xiong",
      "Nianqiao Ju",
      "Sanguo Zhang"
    ]
  },
  "https://openreview.net/forum?id=0AOUWC4ss8": {
    "title": "Illustrated Landmark Graphs for Long-horizon Policy Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Watson",
      "Arjun Krishna",
      "Rajeev Alur",
      "Dinesh Jayaraman"
    ]
  },
  "https://openreview.net/forum?id=Rwf31BYTAU": {
    "title": "Adaptive Incentive Design for Markov Decision Processes with Unknown Rewards",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxiang Ma",
      "Shuo Han",
      "Ahmed Hemida",
      "Charles A kamhoua",
      "Jie Fu"
    ]
  },
  "https://openreview.net/forum?id=tUnyInYbjK": {
    "title": "Influence Learning in Complex Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elena Congeduti",
      "Roberto Rocchetta",
      "Frans A Oliehoek"
    ]
  },
  "https://openreview.net/forum?id=t1utIThKHD": {
    "title": "An Information Theoretic Approach to Machine Unlearning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jack Foster",
      "Kyle Fogarty",
      "Stefan Schoepf",
      "Zack Dugue",
      "Cengiz Oztireli",
      "Alexandra Brintrup"
    ]
  },
  "https://openreview.net/forum?id=JhYbGiFn3Y": {
    "title": "Emergent representations in networks trained with the Forward-Forward algorithm",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niccolo Tosato",
      "Lorenzo Basile",
      "Emanuele Ballarin",
      "Giuseppe De Alteriis",
      "Alberto Cazzaniga",
      "Alessio ansuini"
    ]
  },
  "https://openreview.net/forum?id=ZfPbCFZQbx": {
    "title": "Robust Symbolic Regression for Dynamical System Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ramzi Dakhmouche",
      "Ivan Lunati",
      "Hossein Gorji"
    ]
  },
  "https://openreview.net/forum?id=0yPWtbR3MC": {
    "title": "Show or Tell? Effectively prompting Vision-Language Models for semantic segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niccolò Avogaro",
      "Thomas Frick",
      "Mattia Rigotti",
      "Andrea Bartezzaghi",
      "Filip Janicki",
      "A. Cristiano I. Malossi",
      "Konrad Schindler",
      "Roy Assaf"
    ]
  },
  "https://openreview.net/forum?id=gangoPXSRw": {
    "title": "Probabilistic neural operators for functional uncertainty quantification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Bülte",
      "Philipp Scholl",
      "Gitta Kutyniok"
    ]
  },
  "https://openreview.net/forum?id=A6tOXkkE4Z": {
    "title": "Decision-Focused Surrogate Modeling for Mixed-Integer Linear Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shivi Dixit",
      "Rishabh Gupta",
      "Qi Zhang"
    ]
  },
  "https://openreview.net/forum?id=4ZJjr9YbBw": {
    "title": "A Vector Bernstein Inequality for Self-Normalized Martingales",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ingvar Ziemann"
    ]
  },
  "https://openreview.net/forum?id=Cw2xlg0e46": {
    "title": "Long-context LLMs Struggle with Long In-context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianle Li",
      "Ge Zhang",
      "Quy Duc Do",
      "Xiang Yue",
      "Wenhu Chen"
    ]
  },
  "https://openreview.net/forum?id=d9htascfP8": {
    "title": "Meta-learning Population-based Methods for Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johannes Hog",
      "Raghu Rajan",
      "André Biedenkapp",
      "Noor Awad",
      "Frank Hutter",
      "Vu Nguyen"
    ]
  },
  "https://openreview.net/forum?id=kd6CfmdPfX": {
    "title": "Posterior Sampling for Reinforcement Learning on Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arnaud Robert",
      "Aldo A. Faisal",
      "Ciara Pike-Burke"
    ]
  },
  "https://openreview.net/forum?id=wPHVijYksq": {
    "title": "A limitation on black-box dynamics approaches to Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brieuc Pinon",
      "Raphael Jungers",
      "Jean-Charles Delvenne"
    ]
  },
  "https://openreview.net/forum?id=w4nd5695sq": {
    "title": "Salsa Fresca: Angular Embeddings and Pre-Training for ML Attacks on Learning With Errors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Stevens",
      "Emily Wenger",
      "Cathy Yuanchen Li",
      "Niklas Nolte",
      "Eshika Saxena",
      "Francois Charton",
      "Kristin E. Lauter"
    ]
  },
  "https://openreview.net/forum?id=xBbj46Y2fN": {
    "title": "What's Left After Distillation? How Knowledge Transfer Impacts Fairness and Bias",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aida Mohammadshahi",
      "Yani Ioannou"
    ]
  },
  "https://openreview.net/forum?id=CeNNIQ8GJf": {
    "title": "Efficient Multi-Agent Cooperation Learning through Teammate Lookahead",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Chen",
      "Xinwei Chen",
      "Rong-Jun Qin",
      "Cong Guan",
      "Lei Yuan",
      "Zongzhang Zhang",
      "Yang Yu"
    ]
  },
  "https://openreview.net/forum?id=DcIW0idrg8": {
    "title": "Memory-Modular Classification: Learning to Generalize with Memory Replacement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dahyun Kang",
      "Ahmet Iscen",
      "Eunchan Jo",
      "Sua Choi",
      "Minsu Cho",
      "Cordelia Schmid"
    ]
  },
  "https://openreview.net/forum?id=HJbcwRbMQQ": {
    "title": "Efficient Training of Multi-task Neural Solver for Combinatorial Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenguang Wang",
      "Zhang-Hua Fu",
      "Pinyan Lu",
      "Tianshu Yu"
    ]
  },
  "https://openreview.net/forum?id=B1q9po4LPl": {
    "title": "Uncovering Strong Lottery Tickets in Graph Transformers: A Path to Memory Efficient and Robust Graph Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroaki Ito",
      "Jiale Yan",
      "Hikari Otsuka",
      "Kazushi Kawamura",
      "Masato Motomura",
      "Thiem Van Chu",
      "Daichi Fujiki"
    ]
  },
  "https://openreview.net/forum?id=MKCwO34oIq": {
    "title": "FRAP: Faithful and Realistic Text-to-Image Generation with Adaptive Prompt Weighting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyao Jiang",
      "Negar Hassanpour",
      "Mohammad Salameh",
      "Mohan Sai Singamsetti",
      "Fengyu Sun",
      "Wei Lu",
      "Di Niu"
    ]
  },
  "https://openreview.net/forum?id=4zGPT0ZwnU": {
    "title": "Theoretical Insights into Overparameterized Models in Multi-Task and Replay-Based Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammadamin Banayeeanzade",
      "Mahdi Soltanolkotabi",
      "Mohammad Rostami"
    ]
  },
  "https://openreview.net/forum?id=a6WthNFhL2": {
    "title": "FedDr+: Stabilizing Dot-regression with Global Feature Distillation for Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seongyoon Kim",
      "Minchan Jeong",
      "Sungnyun Kim",
      "Sungwoo Cho",
      "Sumyeong Ahn",
      "Se-Young Yun"
    ]
  },
  "https://openreview.net/forum?id=G1p0YwrX8X": {
    "title": "Sparsified State-Space Models are Efficient Highway Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Woomin Song",
      "Jihoon Tack",
      "Sangwoo Mo",
      "Seunghyuk Oh",
      "Jinwoo Shin"
    ]
  },
  "https://openreview.net/forum?id=BvKYsaOVEn": {
    "title": "Removing Structured Noise using Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tristan Stevens",
      "Hans van Gorp",
      "Faik C Meral",
      "Junseob Shin",
      "Jason Yu",
      "Jean-luc Robert",
      "Ruud Van Sloun"
    ]
  },
  "https://openreview.net/forum?id=UaaT2fI9DC": {
    "title": "On Using Certified Training towards Empirical Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alessandro De Palma",
      "Serge Durand",
      "Zakaria Chihani",
      "François Terrier",
      "Caterina Urban"
    ]
  },
  "https://openreview.net/forum?id=Nu6N69i8SB": {
    "title": "Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weixin Liang",
      "LILI YU",
      "Liang Luo",
      "Srini Iyer",
      "Ning Dong",
      "Chunting Zhou",
      "Gargi Ghosh",
      "Mike Lewis",
      "Wen-tau Yih",
      "Luke Zettlemoyer",
      "Xi Victoria Lin"
    ]
  },
  "https://openreview.net/forum?id=nMCJ8bFq4B": {
    "title": "Multiplayer Information Asymmetric Contextual Bandits",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "William Chang",
      "Yuanhao Lu"
    ]
  },
  "https://openreview.net/forum?id=pqZ6nOm3WF": {
    "title": "Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo-Line-Search Learning Rate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuki Tsukada",
      "Hideaki Iiduka"
    ]
  },
  "https://openreview.net/forum?id=OGCuDFab4b": {
    "title": "Daphne: Multi-Pass Compilation of Probabilistic Programs into Graphical Models and Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christian Dietrich Weilbach",
      "Frank Wood"
    ]
  },
  "https://openreview.net/forum?id=ELtNtkGXoK": {
    "title": "Cluster Tree for Nearest Neighbor Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dan Kushnir",
      "Sandeep Silwal"
    ]
  },
  "https://openreview.net/forum?id=UWNa9Pv6qA": {
    "title": "Neuron-based explanations of neural networks sacrifice completeness and interpretability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nolan Simran Dey",
      "Eric Taylor",
      "Alexander Wong",
      "Bryan P. Tripp",
      "Graham W. Taylor"
    ]
  },
  "https://openreview.net/forum?id=vRYt8QLKqK": {
    "title": "Building Blocks for Robust and Effective Semi-Supervised Real-World Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moussa Kassem Sbeyti",
      "Nadja Klein",
      "Azarm Nowzad",
      "Fikret Sivrikaya",
      "Sahin Albayrak"
    ]
  },
  "https://openreview.net/forum?id=msI02LXVJX": {
    "title": "Compositionality in Time Series: A Proof of Concept using Symbolic Dynamics and Compositional Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michael Hagmann",
      "Michael Staniek",
      "Stefan Riezler"
    ]
  },
  "https://openreview.net/forum?id=oAzu0gzUUb": {
    "title": "Understanding and Robustifying Sub-domain Alignment for Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiling Liu",
      "Juncheng Dong",
      "Ziyang Jiang",
      "Ahmed Aloui",
      "Keyu Li",
      "Michael Hunter Klein",
      "Vahid Tarokh",
      "David Carlson"
    ]
  },
  "https://openreview.net/forum?id=hDywd5AbIM": {
    "title": "SAFE-NID: Self-Attention with Normalizing-Flow Encodings for Network Intrusion Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian Matejek",
      "Ashish Gehani",
      "Nathaniel D. Bastian",
      "Daniel J Clouse",
      "Bradford J Kline",
      "Susmit Jha"
    ]
  },
  "https://openreview.net/forum?id=aPyJilTiIb": {
    "title": "A Unified View of Double-Weighting for Marginal Distribution Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "José I. Segovia-Martín",
      "Santiago Mazuelas",
      "Anqi Liu"
    ]
  },
  "https://openreview.net/forum?id=qsipSdfWeV": {
    "title": "Distilling Datasets Into Less Than One Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Asaf Shul",
      "Eliahu Horwitz",
      "Yedid Hoshen"
    ]
  },
  "https://openreview.net/forum?id=uxyWlXPuIg": {
    "title": "On Using Secure Aggregation in Differentially Private Federated Learning with Multiple Local Steps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mikko A. Heikkilä"
    ]
  },
  "https://openreview.net/forum?id=XVSQnnf7QT": {
    "title": "Which Backbone to Use: A Resource-efficient Domain Specific Comparison for Computer Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pranav Jeevan P",
      "Amit Sethi"
    ]
  },
  "https://openreview.net/forum?id=tIfS6jyO9f": {
    "title": "Enhancing Maritime Trajectory Forecasting via H3 Index and Causal Language Modelling (CLM)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolas Drapier",
      "Aladine Chetouani",
      "Aurélien Chateigner"
    ]
  },
  "https://openreview.net/forum?id=EoiuRII7MQ": {
    "title": "Lower Ricci Curvature for Efficient Community Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yun Jin Park",
      "Didong Li"
    ]
  },
  "https://openreview.net/forum?id=uRbf9ANAns": {
    "title": "Meta-learning Optimizers for Communication-Efficient Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charles-Étienne Joseph",
      "Benjamin Thérien",
      "Abhinav Moudgil",
      "Boris Knyazev",
      "Eugene Belilovsky"
    ]
  },
  "https://openreview.net/forum?id=HTpMOl6xSI": {
    "title": "Towards Efficient Mixture of Experts: A Holistic Study of Compression Techniques",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shwai He",
      "Daize Dong",
      "Liang Ding",
      "Ang Li"
    ]
  },
  "https://openreview.net/forum?id=MGdydNfWzQ": {
    "title": "Ensemble and Mixture-of-Experts DeepONets For Operator Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ramansh Sharma",
      "Varun Shankar"
    ]
  },
  "https://openreview.net/forum?id=56EBglCFvx": {
    "title": "HARE: Human-in-the-Loop Algorithmic Recourse",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sai Srinivas Kancheti",
      "Rahul Vigneswaran",
      "Bamdev Mishra",
      "Vineeth N. Balasubramanian"
    ]
  },
  "https://openreview.net/forum?id=nNN1pPJRVL": {
    "title": "Domain Generalization for Time Series: Enhancing Drilling Regression Models for Stick-Slip Index Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hana YAHIA",
      "Bruno Figliuzzi",
      "Florent Di Meglio",
      "Gerbaud",
      "Stephane Menand",
      "Mohamed MAHJOUB"
    ]
  },
  "https://openreview.net/forum?id=VNM6V1gi3k": {
    "title": "Early Directional Convergence in Deep Homogeneous Neural Networks for Small Initializations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Kumar",
      "Jarvis Haupt"
    ]
  },
  "https://openreview.net/forum?id=HaAg9RN7Hi": {
    "title": "Unlabelled Compressive Sensing under Sparse Permutation and Prior Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Garweet Sresth",
      "Satish Mulleti",
      "Ajit Rajwade"
    ]
  },
  "https://openreview.net/forum?id=osesw2V10u": {
    "title": "A unifying framework for generalised Bayesian online learning in non-stationary environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gerardo Duran-Martin",
      "Leandro Sánchez-Betancourt",
      "Alex Shestopaloff",
      "Kevin Patrick Murphy"
    ]
  },
  "https://openreview.net/forum?id=GEilvtsFNV": {
    "title": "Variational Neural Stochastic Differential Equations with Change Points",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yousef El-Laham",
      "Zhongchang Sun",
      "Haibei Zhu",
      "Tucker Balch",
      "Svitlana Vyetrenko"
    ]
  },
  "https://openreview.net/forum?id=y5Hf0otJLk": {
    "title": "Respecting the limit: Bayesian optimization with a bound on the optimal value",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanyang Wang",
      "Juergen Branke",
      "Matthias Poloczek"
    ]
  },
  "https://openreview.net/forum?id=dg1tqNIWg3": {
    "title": "Rethinking Knowledge Transfer in Learning Using Privileged Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Danil Provodin",
      "Bram van den Akker",
      "Christina Katsimerou",
      "Maurits Clemens Kaptein",
      "Mykola Pechenizkiy"
    ]
  },
  "https://openreview.net/forum?id=FecsgPCOHk": {
    "title": "Causal Discovery over High-Dimensional Structured Hypothesis Spaces with Causal Graph Partitioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashka Shah",
      "Adela Frances DePavia",
      "Nathaniel C Hudson",
      "Ian Foster",
      "Rick Stevens"
    ]
  },
  "https://openreview.net/forum?id=uafxqhImPM": {
    "title": "On the Robustness of Kolmogorov-Arnold Networks: An Adversarial Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tal Alter",
      "Raz Lapid",
      "Moshe Sipper"
    ]
  },
  "https://openreview.net/forum?id=uA19Xo1o31": {
    "title": "CroissantLLM: A Truly Bilingual French-English Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manuel Faysse",
      "Patrick Fernandes",
      "Nuno M Guerreiro",
      "António Loison",
      "Duarte Miguel Alves",
      "Caio Corro",
      "Nicolas Boizard",
      "João Alves",
      "Ricardo Rei",
      "Pedro Henrique Martins",
      "Antoni Bigata Casademunt",
      "François Yvon",
      "Andre Martins",
      "Gautier Viaud",
      "CELINE HUDELOT",
      "Pierre Colombo"
    ]
  },
  "https://openreview.net/forum?id=uPCvfyr2KP": {
    "title": "Reheated Gradient-based Discrete Sampling for Combinatorial Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muheng Li",
      "Ruqi Zhang"
    ]
  },
  "https://openreview.net/forum?id=5zRs34Ls3C": {
    "title": "Enhancing Fairness in Unsupervised Graph Anomaly Detection through Disentanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjing Chang",
      "Kay Liu",
      "Philip S. Yu",
      "Jianjun Yu"
    ]
  },
  "https://openreview.net/forum?id=Xv3ZrFayIO": {
    "title": "Attention Overlap Is Responsible for The Entity Missing Problem in Text-to-image Diffusion Models!",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arash Mari Oriyad",
      "Mohammadali Banayeeanzade",
      "Reza Abbasi",
      "Mohammad Hossein Rohban",
      "Mahdieh Soleymani Baghshah"
    ]
  },
  "https://openreview.net/forum?id=iHYCdTAOqF": {
    "title": "The Time-Energy Model: Selective Time-Series Forecasting Using Energy-Based Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonas Brusokas",
      "Seshu Tirupathi",
      "Dalin Zhang",
      "Torben Bach Pedersen"
    ]
  },
  "https://openreview.net/forum?id=Is9APiPg4V": {
    "title": "Characterizing the Convergence of Game Dynamics via Potentialness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martin Bichler",
      "Davide Legacci",
      "Panayotis Mertikopoulos",
      "Matthias Oberlechner",
      "Bary Pradelski"
    ]
  },
  "https://openreview.net/forum?id=OGifiton47": {
    "title": "Active Diffusion Subsampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oisín Nolan",
      "Tristan Stevens",
      "Wessel L. van Nierop",
      "Ruud Van Sloun"
    ]
  },
  "https://openreview.net/forum?id=XosdLS7KVE": {
    "title": "Mixed Sparsity Training: Achieving 4$\\times$ FLOP Reduction for Transformer Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pihe Hu",
      "Shaolong Li",
      "Xun Wang",
      "Longbo Huang"
    ]
  },
  "https://openreview.net/forum?id=LDzvZEVl5H": {
    "title": "Online Control-Informed Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Liang",
      "Tianyu Zhou",
      "Zehui Lu",
      "Shaoshuai Mou"
    ]
  },
  "https://openreview.net/forum?id=D3DA7pgpvn": {
    "title": "Visual Privacy Auditing with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kristian Schwethelm",
      "Johannes Kaiser",
      "Moritz Knolle",
      "Sarah Lockfisch",
      "Daniel Rueckert",
      "Alexander Ziller"
    ]
  },
  "https://openreview.net/forum?id=nuIUTHGlM5": {
    "title": "Calibrated Probabilistic Forecasts for Arbitrary Sequences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charles Marx",
      "Volodymyr Kuleshov",
      "Stefano Ermon"
    ]
  },
  "https://openreview.net/forum?id=QlBaDKb370": {
    "title": "State space models can express $n$-gram languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vinoth Nandakumar",
      "Qiang Qu",
      "Peng Mi",
      "Tongliang Liu"
    ]
  },
  "https://openreview.net/forum?id=VxC4PZ71Ym": {
    "title": "Unlearning Personal Data from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas De Min",
      "Massimiliano Mancini",
      "Stéphane Lathuilière",
      "Subhankar Roy",
      "Elisa Ricci"
    ]
  },
  "https://openreview.net/forum?id=pF2ukh7HxA": {
    "title": "FlashAttention on a Napkin: A Diagrammatic Approach to Deep Learning IO-Awareness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vincent Abbott",
      "Gioele Zardini"
    ]
  },
  "https://openreview.net/forum?id=EEeVYfXor5": {
    "title": "Out of Spuriousity: Improving Robustness to Spurious Correlations without Group Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Phuong Quynh Le",
      "Jörg Schlötterer",
      "Christin Seifert"
    ]
  },
  "https://openreview.net/forum?id=5PPbvCExZs": {
    "title": "No Need for Ad-hoc Substitutes: The Expected Cost is a Principled All-purpose Classification Metric",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luciana Ferrer"
    ]
  },
  "https://openreview.net/forum?id=HOnL5hjaIt": {
    "title": "Generalized Tangent Kernel: A Unified Geometric Foundation for Natural Gradient and Standard Gradient",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinxun Bai",
      "Steven Rosenberg",
      "Wei Xu"
    ]
  },
  "https://openreview.net/forum?id=Yk7GUlJwGa": {
    "title": "GeoMask3D: Geometrically Informed Mask Selection for Self-Supervised Point Cloud Learning in 3D",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Bahri",
      "Moslem Yazdanpanah",
      "Mehrdad Noori",
      "Milad Cheraghalikhani",
      "Gustavo Adolfo Vargas Hakim",
      "David OSOWIECHI",
      "Farzad Beizaee",
      "Ismail Ben Ayed",
      "Christian Desrosiers"
    ]
  },
  "https://openreview.net/forum?id=FoQK84nwY3": {
    "title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenao Zhang",
      "Donghan Yu",
      "Hiteshi Sharma",
      "Han Zhong",
      "Zhihan Liu",
      "Ziyi Yang",
      "Shuohang Wang",
      "Hany Hassan Awadalla",
      "Zhaoran Wang"
    ]
  },
  "https://openreview.net/forum?id=RXoSmiyObR": {
    "title": "Path-Specific Counterfactual Fairness via Dividend Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daisuke Hatano",
      "Satoshi Hara",
      "Hiromi Arai"
    ]
  },
  "https://openreview.net/forum?id=03UB1MCAMr": {
    "title": "KAGNNs: Kolmogorov-Arnold Networks meet Graph Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roman Bresson",
      "Giannis Nikolentzos",
      "George Panagopoulos",
      "Michail Chatzianastasis",
      "Jun Pang",
      "Michalis Vazirgiannis"
    ]
  },
  "https://openreview.net/forum?id=Xz5IcOizQ6": {
    "title": "Buffer-based Gradient Projection for Continual Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenghong Dai",
      "Jy-yong Sohn",
      "Yicong Chen",
      "S M Iftekharul Alam",
      "Ravikumar Balakrishnan",
      "Suman Banerjee",
      "Nageen Himayat",
      "Kangwook Lee"
    ]
  },
  "https://openreview.net/forum?id=38cwP8xVxD": {
    "title": "The 2024 Foundation Model Transparency Index",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishi Bommasani",
      "Kevin Klyman",
      "Sayash Kapoor",
      "Shayne Longpre",
      "Betty Xiong",
      "Nestor Maslej",
      "Percy Liang"
    ]
  },
  "https://openreview.net/forum?id=dczXe0S1oL": {
    "title": "How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giuseppe Serra",
      "Ben Werner",
      "Florian Buettner"
    ]
  },
  "https://openreview.net/forum?id=ZInwrlkQ3f": {
    "title": "An elementary concentration bound for Gibbs measures arising in statistical learning theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kelly Ramsay",
      "Aukosh Jagannath",
      "Shojaeddin Chenouri"
    ]
  },
  "https://openreview.net/forum?id=tSFpsfndE7": {
    "title": "Random Walk Diffusion for Efficient Large-Scale Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Bernecker",
      "Ghalia Rehawi",
      "Francesco Paolo Casale",
      "Janine Knauer-Arloth",
      "Annalisa Marsico"
    ]
  },
  "https://openreview.net/forum?id=N28FdYO2sH": {
    "title": "Learning Linear Polytree Structural Equation Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingmei Lou",
      "Yu Hu",
      "Xiaodong Li"
    ]
  },
  "https://openreview.net/forum?id=6nBIweDYzZ": {
    "title": "DP-2Stage: Adapting Language Models as Differentially Private Tabular Data Generators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tejumade Afonja",
      "Hui-Po Wang",
      "Raouf Kerkouche",
      "Mario Fritz"
    ]
  },
  "https://openreview.net/forum?id=BPDVZajOW5": {
    "title": "Optimizing Estimators of Squared Calibration Errors in Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Gregor Gruber",
      "Francis R. Bach"
    ]
  },
  "https://openreview.net/forum?id=ZdMIXltJzK": {
    "title": "Reset-free Reinforcement Learning with World Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhao Yang",
      "Thomas M. Moerland",
      "Mike Preuss",
      "Aske Plaat",
      "Edward S. Hu"
    ]
  },
  "https://openreview.net/forum?id=QIzRdjIWnS": {
    "title": "Convergence Guarantees for RMSProp and Adam in Generalized-smooth Non-convex Optimization with Affine Noise Variance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zhang",
      "Yi Zhou",
      "Shaofeng Zou"
    ]
  },
  "https://openreview.net/forum?id=UV58hNygne": {
    "title": "HoSNNs: Adversarially-Robust Homeostatic Spiking Neural Networks with Adaptive Firing Thresholds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hejia Geng",
      "Peng Li"
    ]
  },
  "https://openreview.net/forum?id=8Q4qxe9a9Z": {
    "title": "A Self-Explainable Heterogeneous GNN for Relational Deep Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francesco Ferrini",
      "Antonio Longa",
      "Andrea Passerini",
      "Manfred Jaeger"
    ]
  },
  "https://openreview.net/forum?id=9NVJ0ZgEfT": {
    "title": "Long Short-Term Imputer: Handling Consecutive Missing Values in Time Series",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacheng You",
      "Xinyang Chen",
      "Yu Sun",
      "Weili Guan",
      "Liqiang Nie"
    ]
  },
  "https://openreview.net/forum?id=58gPkcVbFL": {
    "title": "Evolution of Discriminator and Generator Gradients in GAN Training: From Fitting to Collapse",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiguo Gao",
      "Ming Li"
    ]
  },
  "https://openreview.net/forum?id=GkYOcbNLaW": {
    "title": "Cycle Conditioning for Robust Representation Learning from Categorical Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohsen Tabejamaat",
      "Farzaneh Etminani",
      "Mattias Ohlsson"
    ]
  },
  "https://openreview.net/forum?id=CrKMqRAhBo": {
    "title": "A Lean Dataset for International Math Olympiad: Small Steps towards Writing Math Proofs for Hard Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roozbeh Yousefzadeh",
      "Xuenan Cao"
    ]
  },
  "https://openreview.net/forum?id=HkmymFPODz": {
    "title": "Deep Active Learning in the Open World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Xie",
      "Jifan Zhang",
      "Haoyue Bai",
      "Robert D Nowak"
    ]
  },
  "https://openreview.net/forum?id=J7cY9Jr9WM": {
    "title": "A Fused Gromov-Wasserstein Approach to Subgraph Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amadou Siaka SANGARE",
      "Nicolas Dunou",
      "Jhony H. Giraldo",
      "Fragkiskos D. Malliaros"
    ]
  },
  "https://openreview.net/forum?id=bqMJToTkvT": {
    "title": "QPO: Query-dependent Prompt Optimization via Multi-Loop Offline Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilun Kong",
      "Hangyu Mao",
      "Zhao Qi",
      "Bin Zhang",
      "Jingqing Ruan",
      "Li Shen",
      "Yongzhe Chang",
      "Xueqian Wang",
      "Rui Zhao",
      "Dacheng Tao"
    ]
  },
  "https://openreview.net/forum?id=jRbKsQ3sYO": {
    "title": "Combating Inter-Task Confusion and Catastrophic Forgetting by Metric Learning and Re-Using a Past Trained Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayedmoslem Shokrolahi",
      "IL MIN KIM"
    ]
  },
  "https://openreview.net/forum?id=prVLANCshF": {
    "title": "AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijun Wang",
      "Haoqin Tu",
      "Jieru Mei",
      "Bingchen Zhao",
      "Yisen Wang",
      "Cihang Xie"
    ]
  },
  "https://openreview.net/forum?id=IaUh7CSD3k": {
    "title": "Metalearning Continual Learning Algorithms",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kazuki Irie",
      "Róbert Csordás",
      "Jürgen Schmidhuber"
    ]
  },
  "https://openreview.net/forum?id=CAkt3DsAZs": {
    "title": "Meta-Learning for Graphs with Heterogeneous Node Attribute Spaces for Few-Shot Edge Predictions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhong Chuang",
      "Yusuke Tanaka",
      "Tomoharu Iwata"
    ]
  },
  "https://openreview.net/forum?id=jJOVpnNrEp": {
    "title": "Video-Language Critic: Transferable Reward Functions for Language-Conditioned Robotics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minttu Alakuijala",
      "Reginald McLean",
      "Isaac Woungang",
      "Nariman Farsad",
      "Samuel Kaski",
      "Pekka Marttinen",
      "Kai Yuan"
    ]
  },
  "https://openreview.net/forum?id=2Zan4ATYsh": {
    "title": "DivIL: Unveiling and Addressing Over-Invariance for Out-of- Distribution Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi WANG",
      "Yuhang Zhou",
      "Zhixiong Zhang",
      "Qiguang Chen",
      "Yongqiang Chen",
      "James Cheng"
    ]
  },
  "https://openreview.net/forum?id=k4AxEwTaHq": {
    "title": "FaAlGrad: Fairness through Alignment of Gradients across Different Subpopulations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikita Malik",
      "Konda Reddy Mopuri"
    ]
  },
  "https://openreview.net/forum?id=xXs2GKXPnH": {
    "title": "Faster Diffusion Through Temporal Attention Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haozhe Liu",
      "Wentian Zhang",
      "Jinheng Xie",
      "Francesco Faccio",
      "Mengmeng Xu",
      "Tao Xiang",
      "Mike Zheng Shou",
      "Juan-Manuel Perez-Rua",
      "Jürgen Schmidhuber"
    ]
  },
  "https://openreview.net/forum?id=Za9Tm07fig": {
    "title": "TACO Vision Models Can Be Efficiently Specialized via Few-Shot Task-Aware Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Denis Kuznedelev",
      "Soroush Tabesh",
      "Kimia Noorbakhsh",
      "Elias Frantar",
      "Sara Beery",
      "Eldar Kurtic",
      "Dan Alistarh"
    ]
  },
  "https://openreview.net/forum?id=XPREcQlAM0": {
    "title": "Global Convergence Rate of Deep Equilibrium Models with General Activations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lan V. Truong"
    ]
  },
  "https://openreview.net/forum?id=ZckLMG00sO": {
    "title": "Stability-Aware Training of Machine Learning Force Fields with Differentiable Boltzmann Estimators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanjeev Raja",
      "Ishan Amin",
      "Fabian Pedregosa",
      "Aditi S. Krishnapriyan"
    ]
  },
  "https://openreview.net/forum?id=6jTQrr3APY": {
    "title": "Fair principal component analysis (PCA): minorization-maximization algorithms for Fair PCA, Fair Robust PCA and Fair Sparse PCA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prabhu babu",
      "Petre Stoica",
      "Astha Saini"
    ]
  },
  "https://openreview.net/forum?id=EWT4GxjGDS": {
    "title": "Producers Equilibria and Dynamics in Engagement-Driven Recommender Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Krishna Acharya",
      "Juba Ziani",
      "Jingyan Wang",
      "Varun Vangala"
    ]
  },
  "https://openreview.net/forum?id=dvRysCqmYQ": {
    "title": "Balanced Mixed-Type Tabular Data Synthesis with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Yang",
      "Han Yu",
      "Peikun Guo",
      "Khadija Zanna",
      "Xiaoxue Yang",
      "Akane Sano"
    ]
  },
  "https://openreview.net/forum?id=zSK81A2hxQ": {
    "title": "A Neural Material Point Method for Particle-based Emulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omer Rochman-Sharabi",
      "Sacha Lewin",
      "Gilles Louppe"
    ]
  },
  "https://openreview.net/forum?id=0RJvZY0h6O": {
    "title": "Lognormal Mutations and their Use in Detecting Surreptitious Fake Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Olivier Teytaud",
      "Mariia Zameshina",
      "Tom Sander",
      "Pierre Fernandez",
      "Furong Ye",
      "Laurent Najman",
      "Thomas Bäck",
      "Ismail Labiad"
    ]
  },
  "https://openreview.net/forum?id=k3Ab6RuJE9": {
    "title": "Verbalized Machine Learning: Revisiting Machine Learning with Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Z. Xiao",
      "Robert Bamler",
      "Bernhard Schölkopf",
      "Weiyang Liu"
    ]
  },
  "https://openreview.net/forum?id=fC4bh1PmZr": {
    "title": "Counterfactual Learning of Stochastic Policies with Continuous Actions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Houssam Zenati",
      "Alberto Bietti",
      "Matthieu Martin",
      "Eustache Diemert",
      "Pierre Gaillard",
      "Julien Mairal"
    ]
  },
  "https://openreview.net/forum?id=Vwgjk5ysWn": {
    "title": "Why is constrained neural language generation particularly challenging?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cristina Garbacea",
      "Qiaozhu Mei"
    ]
  },
  "https://openreview.net/forum?id=K6CvWPtF62": {
    "title": "Provable Quantum Algorithm Advantage for Gaussian Process Quadrature",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cristian A. Galvis-Florez",
      "Ahmad Farooq",
      "Simo Särkkä"
    ]
  },
  "https://openreview.net/forum?id=ojeCoOKwWp": {
    "title": "Differentially Private Source-Target Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shachar Schnapp",
      "Sivan Sabato"
    ]
  },
  "https://openreview.net/forum?id=WxHTSPS2pi": {
    "title": "Uncertainty-Based Experience Replay for Task-Agnostic Continual Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adrian Remonda",
      "Cole Corbitt Terrell",
      "Eduardo E. Veas",
      "Marc Masana"
    ]
  },
  "https://openreview.net/forum?id=E2zKNuwNDc": {
    "title": "Robust Preference Optimization through Reward Model Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adam Fisch",
      "Jacob Eisenstein",
      "Vicky Zayats",
      "Alekh Agarwal",
      "Ahmad Beirami",
      "Chirag Nagpal",
      "Peter Shaw",
      "Jonathan Berant"
    ]
  },
  "https://openreview.net/forum?id=6LO1y8ZE0F": {
    "title": "SimPLR: A Simple and Plain Transformer for Efficient Object Detection and Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duy Kien Nguyen",
      "Martin R. Oswald",
      "Cees G. M. Snoek"
    ]
  },
  "https://openreview.net/forum?id=TnT59yz7lc": {
    "title": "Exploiting Benford's Law for Weight Regularization of Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julius Ott",
      "Huawei Sun",
      "Enrico Rinaldi",
      "Gianfranco Mauro",
      "Lorenzo Servadei",
      "Robert Wille"
    ]
  },
  "https://openreview.net/forum?id=rWSiBknwQa": {
    "title": "Are Large Language Models Really Robust to Word-Level Perturbations?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Wang",
      "Guozheng Ma",
      "Cong Yu",
      "Ning Gui",
      "Linrui Zhang",
      "Zhiqi Huang",
      "Suwei Ma",
      "Yongzhe Chang",
      "Sen Zhang",
      "Li Shen",
      "Xueqian Wang",
      "Peilin Zhao",
      "Dacheng Tao"
    ]
  },
  "https://openreview.net/forum?id=42v6I5Ut9a": {
    "title": "Single-pass Detection of Jailbreaking Input in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leyla Naz Candogan",
      "Yongtao Wu",
      "Elias Abad Rocamora",
      "Grigorios Chrysos",
      "Volkan Cevher"
    ]
  },
  "https://openreview.net/forum?id=pSk5qyt1ob": {
    "title": "On Training-Conditional Conformal Prediction and Binomial Proportion Confidence Intervals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rudi Coppola",
      "Manuel Mazo Espinosa"
    ]
  },
  "https://openreview.net/forum?id=EcMVskXo1n": {
    "title": "Generative Risk Minimization for Out-of-Distribution Generalization on Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Song Wang",
      "Zhen Tan",
      "Yaochen Zhu",
      "Chuxu Zhang",
      "Jundong Li"
    ]
  },
  "https://openreview.net/forum?id=69RntSRF5K": {
    "title": "An Analytical Model for Overparameterized Learning Under Class Imbalance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eliav Mor",
      "Yair Carmon"
    ]
  },
  "https://openreview.net/forum?id=t5cy5v9wph": {
    "title": "Evaluating the Robustness of Analogical Reasoning in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Martha Lewis",
      "Melanie Mitchell"
    ]
  },
  "https://openreview.net/forum?id=adhsMqURI1": {
    "title": "Comparing the information content of probabilistic representation spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kieran A. Murphy",
      "Sam Dillavou",
      "Danielle Bassett"
    ]
  },
  "https://openreview.net/forum?id=jAHEBivObO": {
    "title": "Adapt then Unlearn: Exploring Parameter Space Semantics for Unlearning in Generative Adversarial Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Piyush Tiwary",
      "Atri Guha",
      "Subhodip Panda",
      "Prathosh AP"
    ]
  },
  "https://openreview.net/forum?id=D3DBqvSDbj": {
    "title": "On Memorization in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangming Gu",
      "Chao Du",
      "Tianyu Pang",
      "Chongxuan Li",
      "Min Lin",
      "Ye Wang"
    ]
  },
  "https://openreview.net/forum?id=dNJmJ8bh1M": {
    "title": "The Sparse Matrix-Based Random Projection: A Study of Binary and Ternary Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weizhi Lu",
      "Zhongzheng Li",
      "Mingrui Chen",
      "Weiyu Li"
    ]
  },
  "https://openreview.net/forum?id=Sx1khIIi95": {
    "title": "Over-parameterised Shallow Neural Networks with Asymmetrical Node Scaling: Global Convergence Guarantees and Feature Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Francois Caron",
      "Fadhel Ayed",
      "Paul Jung",
      "Hoil Lee",
      "Juho Lee",
      "Hongseok Yang"
    ]
  },
  "https://openreview.net/forum?id=rfPns0WJyg": {
    "title": "Uncertainty Representations in State-Space Layers for Deep Reinforcement Learning under Partial Observability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carlos E. Luis",
      "Alessandro Giacomo Bottero",
      "Julia Vinogradska",
      "Felix Berkenkamp",
      "Jan Peters"
    ]
  },
  "https://openreview.net/forum?id=hCxtlfvL22": {
    "title": "Latent Space Energy-based Neural ODEs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Cheng",
      "Deqian Kong",
      "Jianwen Xie",
      "Kookjin Lee",
      "Ying Nian Wu",
      "Yezhou Yang"
    ]
  },
  "https://openreview.net/forum?id=U8EMkndyq4": {
    "title": "Using representation balancing to learn conditional-average dose responses from clustered data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Bockel-Rickermann",
      "Toon Vanderschueren",
      "Jeroen Berrevoets",
      "Tim Verdonck",
      "Wouter Verbeke"
    ]
  },
  "https://openreview.net/forum?id=TRKwzPnXWQ": {
    "title": "ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sucheng Ren",
      "Hongru Zhu",
      "Chen Wei",
      "Yijiang Li",
      "Alan Yuille",
      "Cihang Xie"
    ]
  },
  "https://openreview.net/forum?id=GGHk5ukO6t": {
    "title": "Dynamics-inspired Structure Hallucination for Protein-protein Interaction Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fang Wu",
      "Stan Z. Li"
    ]
  },
  "https://openreview.net/forum?id=h751wl9xiR": {
    "title": "ALTA: Compiler-Based Analysis of Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Shaw",
      "James Cohan",
      "Jacob Eisenstein",
      "Kenton Lee",
      "Jonathan Berant",
      "Kristina Toutanova"
    ]
  },
  "https://openreview.net/forum?id=BLDtWlFKhn": {
    "title": "Density of states in neural networks: an in-depth exploration of learning in parameter space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Margherita Mele",
      "Roberto Menichetti",
      "Alessandro Ingrosso",
      "Raffaello Potestio"
    ]
  },
  "https://openreview.net/forum?id=HjpD5kpfa3": {
    "title": "Rethinking Spectral Augmentation for Contrast-based Graph Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangru Jian",
      "Xinjian Zhao",
      "Wei Pang",
      "Chaolong Ying",
      "Yimu Wang",
      "Yaoyao Xu",
      "Tianshu Yu"
    ]
  },
  "https://openreview.net/forum?id=JQ0agisXny": {
    "title": "A Strong Baseline for Molecular Few-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philippe Formont",
      "Hugo Jeannin",
      "Pablo Piantanida",
      "Ismail Ben Ayed"
    ]
  },
  "https://openreview.net/forum?id=u9EHndbiVw": {
    "title": "PROXI: Challenging the GNNs for Link Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Astrit Tola",
      "Jack Myrick",
      "Baris Coskunuzer"
    ]
  },
  "https://openreview.net/forum?id=RfFqBXLDQk": {
    "title": "On Space Folds of ReLU Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michal Lewandowski",
      "Hamid Eghbalzadeh",
      "Bernhard Heinzl",
      "Raphael Pisoni",
      "Bernhard A. Moser"
    ]
  },
  "https://openreview.net/forum?id=asiBW1bB9b": {
    "title": "Improving Consistency in Large Language Models through Chain of Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harsh Raj",
      "Vipul Gupta",
      "Domenic Rosati",
      "Subhabrata Majumdar"
    ]
  },
  "https://openreview.net/forum?id=H4S4ETc8c9": {
    "title": "Evaluation of Best-of-N Sampling Strategies for Language Model Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuki Ichihara",
      "Yuu Jinnai",
      "Tetsuro Morimura",
      "Kenshi Abe",
      "Kaito Ariu",
      "Mitsuki Sakamoto",
      "Eiji Uchibe"
    ]
  },
  "https://openreview.net/forum?id=ScEv13W2f1": {
    "title": "Unsupervised Discovery of Object-Centric Neural Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rundong Luo",
      "Hong-Xing Yu",
      "Jiajun Wu"
    ]
  },
  "https://openreview.net/forum?id=Wt6Iz5XNIO": {
    "title": "Understanding LLM Embeddings for Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Tang",
      "Bangding Yang",
      "Xingyou Song"
    ]
  },
  "https://openreview.net/forum?id=5qKI2dkrjL": {
    "title": "APR-CNN: Convolutional Neural Networks for the Adaptive Particle Representation of Large Microscopy Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joel Jonsson",
      "Bevan Leslie Cheeseman",
      "Ivo Sbalzarini"
    ]
  },
  "https://openreview.net/forum?id=zjxKrb4ehr": {
    "title": "On diffusion-based generative models and their error bounds: The log-concave case with full convergence estimates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefano Bruno",
      "Ying Zhang",
      "Dongyoung Lim",
      "Omer Deniz Akyildiz",
      "Sotirios Sabanis"
    ]
  },
  "https://openreview.net/forum?id=A1R1cQ93Cb": {
    "title": "Relax and penalize: a new bilevel approach to mixed-binary hyperparameter optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sara Venturini",
      "Marianna De Santis",
      "Jordan Patracone",
      "Martin Schmidt",
      "Francesco Rinaldi",
      "Saverio Salzo"
    ]
  },
  "https://openreview.net/forum?id=nmBleuFzaN": {
    "title": "Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Simon Dufort-Labbé",
      "Pierluca D'Oro",
      "Evgenii Nikishin",
      "Irina Rish",
      "Pierre-Luc Bacon",
      "Razvan Pascanu",
      "Aristide Baratin"
    ]
  },
  "https://openreview.net/forum?id=ZMliWjMCor": {
    "title": "Personalized Federated Learning of Probabilistic Models: A PAC-Bayesian Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahrokh Ghoddousi Boroujeni",
      "Andreas Krause",
      "Giancarlo Ferrari-Trecate"
    ]
  },
  "https://openreview.net/forum?id=DrMCDS88IL": {
    "title": "Wasserstein Coreset via Sinkhorn Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyun Yin",
      "Yixuan Qiu",
      "Xiao Wang"
    ]
  },
  "https://openreview.net/forum?id=tzW948kU6x": {
    "title": "Diffusion on Graph: Augmentation of Graph Structure for Node Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yancheng Wang",
      "Changyu Liu",
      "Yingzhen Yang"
    ]
  },
  "https://openreview.net/forum?id=8rxtL0kZnX": {
    "title": "Hypergraph Neural Networks through the Lens of Message Passing: A Common Perspective to Homophily and Architecture Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lev Telyatnikov",
      "Maria Sofia Bucarelli",
      "Guillermo Bernardez",
      "Olga Zaghen",
      "Simone Scardapane",
      "Pietro Lio"
    ]
  },
  "https://openreview.net/forum?id=sbmp55k6iE": {
    "title": "Increasing Both Batch Size and Learning Rate Accelerates Stochastic Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hikaru Umeda",
      "Hideaki Iiduka"
    ]
  },
  "https://openreview.net/forum?id=JEHIVfjmOf": {
    "title": "JoIN: Joint GANs Inversion for Intrinsic Image Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Viraj Shah",
      "Svetlana Lazebnik",
      "Julien Philip"
    ]
  },
  "https://openreview.net/forum?id=1QeI99nH9k": {
    "title": "Robust High-Dimensional Mean Estimation With Low Data Size, an Empirical Study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cullen Anderson",
      "Jeff M. Phillips"
    ]
  },
  "https://openreview.net/forum?id=7CUluLpLxV": {
    "title": "Explaining Explainability: Recommendations for Effective Use of Concept Activation Vectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Angus Nicolson",
      "Lisa Schut",
      "Alison Noble",
      "Yarin Gal"
    ]
  },
  "https://openreview.net/forum?id=IrBYuh9W3T": {
    "title": "What Makes ImageNet Look Unlike LAION",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Shirali",
      "Moritz Hardt"
    ]
  },
  "https://openreview.net/forum?id=Bmy82p2eez": {
    "title": "Continual Learning from Simulated Interactions via Multitask Prospective Rehearsal for Bionic Limb Behavior Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sharmita Dey",
      "Benjamin Paassen",
      "Sarath Ravindran Nair",
      "Sabri Boughorbel",
      "Arndt F. Schilling"
    ]
  },
  "https://openreview.net/forum?id=DYCSRf3vby": {
    "title": "Geometry-Aware visualization of high dimensional Symmetric Positive Definite matrices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thibault de Surrel",
      "Sylvain Chevallier",
      "Fabien Lotte",
      "Florian Yger"
    ]
  },
  "https://openreview.net/forum?id=VM8bNd5A09": {
    "title": "CNN Interpretability with Multivector Tucker Saliency Maps for Self-Supervised Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aymene Mohammed Bouayed",
      "Samuel Deslauriers-gauthier",
      "Adrian IACOVELLI",
      "David Naccache"
    ]
  },
  "https://openreview.net/forum?id=YxXyRSlZ4b": {
    "title": "Neural Lattice Reduction: A Self-Supervised Geometric Deep Learning Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giovanni Luca Marchetti",
      "Gabriele Cesa",
      "Kumar Pratik",
      "Arash Behboodi"
    ]
  },
  "https://openreview.net/forum?id=tzFjcVqmxw": {
    "title": "Enhancing Remaining Useful Life Prediction with Ensemble Multi-Term Fourier Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ya Song",
      "Laurens Bliek",
      "Yaoxin Wu",
      "Yingqian Zhang"
    ]
  },
  "https://openreview.net/forum?id=Wnd0XY0twh": {
    "title": "Data Augmentation Policy Search for Long-Term Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liran Nochumsohn",
      "Omri Azencot"
    ]
  },
  "https://openreview.net/forum?id=M3SkSMfWcP": {
    "title": "Adaptive Multi-step Refinement Network for Robust Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi Chen",
      "Yufan Ren",
      "Tong Zhang",
      "Zheng Dang",
      "Wenbing Tao",
      "Sabine Susstrunk",
      "Mathieu Salzmann"
    ]
  },
  "https://openreview.net/forum?id=zKv8qULV6n": {
    "title": "LLaVA-OneVision: Easy Visual Task Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Li",
      "Yuanhan Zhang",
      "Dong Guo",
      "Renrui Zhang",
      "Feng Li",
      "Hao Zhang",
      "Kaichen Zhang",
      "Peiyuan Zhang",
      "Yanwei Li",
      "Ziwei Liu",
      "Chunyuan Li"
    ]
  },
  "https://openreview.net/forum?id=F5ALCh3GWG": {
    "title": "On the Regularization of Learnable Embeddings for Time Series Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Butera",
      "Giovanni De Felice",
      "Andrea Cini",
      "Cesare Alippi"
    ]
  },
  "https://openreview.net/forum?id=JQGmbVK4Fr": {
    "title": "Towards context and domain-aware algorithms for scene analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ibrahim Serouis",
      "Florence Sèdes"
    ]
  },
  "https://openreview.net/forum?id=P5y82LKGbY": {
    "title": "DELTA: Dual Consistency Delving with Topological Uncertainty for Active Graph Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengyun Wang",
      "Yadi Cao",
      "Chris Russell",
      "Yanxin Shen",
      "Junyu Luo",
      "Ming Zhang",
      "Siyu Heng",
      "Xiao Luo"
    ]
  },
  "https://openreview.net/forum?id=gwXfZ3xkUq": {
    "title": "When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan Wang",
      "Qian Liu",
      "Chao Du",
      "Tongyao Zhu",
      "Cunxiao Du",
      "Kenji Kawaguchi",
      "Tianyu Pang"
    ]
  },
  "https://openreview.net/forum?id=UYXPt7HUdl": {
    "title": "Score-Based Denoising Diffusion Models for Photon-Starved Image Restoration Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Savvas Melidonis",
      "Yiming Xi",
      "Konstantinos C. Zygalakis",
      "Yoann Altmann",
      "Marcelo Pereyra"
    ]
  },
  "https://openreview.net/forum?id=W50i7r3DHE": {
    "title": "Instance-Aware Graph Prompt Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiazheng Li",
      "Jundong Li",
      "Chuxu Zhang"
    ]
  },
  "https://openreview.net/forum?id=xpnPYfufhz": {
    "title": "Partially Frozen Random Networks Contain Compact Strong Lottery Tickets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hikari Otsuka",
      "Daiki Chijiwa",
      "Ángel López García-Arias",
      "Yasuyuki Okoshi",
      "Kazushi Kawamura",
      "Thiem Van Chu",
      "Daichi Fujiki",
      "Susumu Takeuchi",
      "Masato Motomura"
    ]
  },
  "https://openreview.net/forum?id=c7AAHdEYz5": {
    "title": "Label Distribution Shift-Aware Prediction Refinement for Test-Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minguk Jang",
      "Hye Won Chung"
    ]
  },
  "https://openreview.net/forum?id=Q7aXOnEGgU": {
    "title": "On the Sample Complexity of One Hidden Layer Networks with Equivariance, Locality and Weight Sharing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arash Behboodi",
      "Gabriele Cesa"
    ]
  },
  "https://openreview.net/forum?id=2wgnepQjyF": {
    "title": "Selective Prediction via Training Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stephan Rabanser",
      "Anvith Thudi",
      "Kimia Hamidieh",
      "Adam Dziedzic",
      "Israfil Bahceci",
      "Akram Bin Sediq",
      "HAMZA SOKUN",
      "Nicolas Papernot"
    ]
  },
  "https://openreview.net/forum?id=pxxmUKKgel": {
    "title": "How Does Code Pretraining Affect Language Model Task Performance?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jackson Petty",
      "Sjoerd van Steenkiste",
      "Tal Linzen"
    ]
  },
  "https://openreview.net/forum?id=uDRzORdPT7": {
    "title": "DeepRRTime: Robust Time-series Forecasting with a Regularized INR Basis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chandramouli Shama Sastry",
      "Mahdi Gilany",
      "Kry Yik-Chau Lui",
      "Martin Magill",
      "Alexander Pashevich"
    ]
  },
  "https://openreview.net/forum?id=Lt2H8Bd8jF": {
    "title": "Iterated $Q$-Network: Beyond One-Step Bellman Updates in Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Théo Vincent",
      "Daniel Palenicek",
      "Boris Belousov",
      "Jan Peters",
      "Carlo D'Eramo"
    ]
  },
  "https://openreview.net/forum?id=jZBAVFGUUo": {
    "title": "Towards Measuring Predictability: To which extent data-driven approaches can extract deterministic relations from data exemplified with time series prediction and classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saleh GHOLAM ZADEH",
      "Vaisakh Shaj",
      "Patrick Jahnke",
      "Gerhard Neumann",
      "Tim Breitenbach"
    ]
  },
  "https://openreview.net/forum?id=wIgRV336hC": {
    "title": "Minimax Lower Bounds for Estimating Distributions on Low-dimensional Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saptarshi Chakraborty"
    ]
  },
  "https://openreview.net/forum?id=XWAXcxNg4n": {
    "title": "Test-Time Adaptation with Source Based Auxiliary Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Motasem Alfarra",
      "Alvaro Correia",
      "Bernard Ghanem",
      "Christos Louizos"
    ]
  },
  "https://openreview.net/forum?id=HlzjI2fn2T": {
    "title": "On the stability of gradient descent with second order dynamics for time-varying cost functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Travis E Gibson",
      "Sawal Acharya",
      "Anjali Parashar",
      "Joseph Emilio Gaudio",
      "Anuradha Annaswamy"
    ]
  },
  "https://openreview.net/forum?id=O4CQ5AM5yP": {
    "title": "REX: GPU-Accelerated Sim2Real Framework with Delay and Dynamics Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bas van der Heijden",
      "Jens Kober",
      "Robert Babuska",
      "Laura Ferranti"
    ]
  },
  "https://openreview.net/forum?id=udVkqIDYSM": {
    "title": "Wonderful Team: Zero-Shot Physical Task Planning with Visual LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zidan Wang",
      "Rui Shen",
      "Bradly C. Stadie"
    ]
  },
  "https://openreview.net/forum?id=v47f4DwYZb": {
    "title": "Graph-level Representation Learning with Joint-Embedding Predictive Architectures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geri Skenderi",
      "Hang Li",
      "Jiliang Tang",
      "Marco Cristani"
    ]
  },
  "https://openreview.net/forum?id=L7sQ8CW2FY": {
    "title": "Conformalized Credal Regions for Classification with Ambiguous Ground Truth",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michele Caprio",
      "David Stutz",
      "Shuo Li",
      "Arnaud Doucet"
    ]
  },
  "https://openreview.net/forum?id=2nRcWy3RLM": {
    "title": "Bridging Causality, Individual Fairness, and Adversarial Robustness in the Absence of Structural Causal Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmad Reza Ehyaei",
      "Golnoosh Farnadi",
      "Samira Samadi"
    ]
  },
  "https://openreview.net/forum?id=8tMMCf4YYn": {
    "title": "Partially Personalized Federated Learning: Breaking the Curse of Data Heterogeneity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantin Mishchenko",
      "Rustem Islamov",
      "Eduard Gorbunov",
      "Samuel Horváth"
    ]
  },
  "https://openreview.net/forum?id=gLQ801ewwp": {
    "title": "Identifying Axiomatic Mathematical Transformation Steps using Tree-Structured Pointer Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Wankerl",
      "Jan Pfister",
      "Andrzej Dulny",
      "Gerhard Götz",
      "Andreas Hotho"
    ]
  },
  "https://openreview.net/forum?id=p9KSFrTLx0": {
    "title": "Mixture Degree-Corrected Stochastic Block Model for Multi-Group Community Detection in Multiplex Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noureddine Henka",
      "Mohamad Assaad",
      "Sami Tazi"
    ]
  },
  "https://openreview.net/forum?id=x1dXvvElVd": {
    "title": "Interpreting Neurons in Deep Vision Networks with Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicholas Bai",
      "Rahul Ajay Iyer",
      "Tuomas Oikarinen",
      "Akshay R. Kulkarni",
      "Tsui-Wei Weng"
    ]
  },
  "https://openreview.net/forum?id=f6yMdmrD2g": {
    "title": "Cooperative Minibatching in Graph Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammed Fatih Balin",
      "Dominique LaSalle",
      "Umit Catalyurek"
    ]
  },
  "https://openreview.net/forum?id=Ss9MTTN7OL": {
    "title": "Preserving Privacy in Large Language Models: A Survey on Current Threats and Solutions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michele Miranda",
      "Elena Sofia Ruzzetti",
      "Andrea Santilli",
      "Fabio Massimo Zanzotto",
      "Sébastien Bratières",
      "Emanuele Rodolà"
    ]
  },
  "https://openreview.net/forum?id=OGaTF9iOxi": {
    "title": "Maximum Mean Discrepancy on Exponential Windows for Online Change Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florian Kalinke",
      "Marco Heyden",
      "Georg Gntuni",
      "Edouard Fouché",
      "Klemens Böhm"
    ]
  },
  "https://openreview.net/forum?id=aWRMvXTvPf": {
    "title": "Shapley Values of Structured Additive Regression Models and Application to RKHS Weightings of Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gabriel Dubé",
      "Mario Marchand"
    ]
  },
  "https://openreview.net/forum?id=WppTEs4Kkn": {
    "title": "On the effects of similarity metrics in decentralized deep learning under distribution shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edvin Listo Zec",
      "Tom Hagander",
      "Eric Ihre-Thomason",
      "Sarunas Girdzijauskas"
    ]
  },
  "https://openreview.net/forum?id=VmfWywWuYQ": {
    "title": "Interactive Task Planning with Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyi Li",
      "Philipp Wu",
      "Pieter Abbeel",
      "Jitendra Malik"
    ]
  },
  "https://openreview.net/forum?id=4o8lIFkpn2": {
    "title": "\\copyright Plug-in Authorization for Human Copyright Protection in Text-to-Image Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Zhou",
      "Huishuai Zhang",
      "Jiang Bian",
      "Weiming Zhang",
      "Nenghai Yu"
    ]
  },
  "https://openreview.net/forum?id=3Y3o0yFZfu": {
    "title": "Private Fine-tuning of Large Language Models with Zeroth-order Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Tang",
      "Ashwinee Panda",
      "Milad Nasr",
      "Saeed Mahloujifar",
      "Prateek Mittal"
    ]
  },
  "https://openreview.net/forum?id=bZzXgheUSD": {
    "title": "ADAPT to Robustify Prompt Tuning Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Masih Eskandar",
      "Tooba Imtiaz",
      "Zifeng Wang",
      "Jennifer Dy"
    ]
  },
  "https://openreview.net/forum?id=MO1slfU9xy": {
    "title": "Explanation Shift: How Did the Distribution Shift Impact the Model?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carlos Mougan",
      "Klaus Broelemann",
      "Gjergji Kasneci",
      "Thanassis Tiropanis",
      "Steffen Staab"
    ]
  },
  "https://openreview.net/forum?id=zg0hPlABfY": {
    "title": "Enhancing Parameter Efficiency and Generalization in Large Models: A Regularized and Masked Low-Rank Adaptation Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhu Mao",
      "Zihao Zhao",
      "Siqi Ping",
      "Yang Liu",
      "Wenbo Ding"
    ]
  },
  "https://openreview.net/forum?id=nu1SjVgSuy": {
    "title": "SPFormer: Enhancing Vision Transformer with Superpixel Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jieru Mei",
      "Liang-Chieh Chen",
      "Alan Yuille",
      "Cihang Xie"
    ]
  },
  "https://openreview.net/forum?id=pWSrm3oP8b": {
    "title": "Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated Tokens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhepeng Cen",
      "Yao Liu",
      "Siliang Zeng",
      "Pratik Chaudhari",
      "Huzefa Rangwala",
      "George Karypis",
      "Rasool Fakoor"
    ]
  },
  "https://openreview.net/forum?id=oeg2ncuSPz": {
    "title": "Approximation Rates and VC-Dimension Bounds for (P)ReLU MLP Mixture of Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anastasis Kratsios",
      "Haitz Sáez de Ocáriz Borde",
      "Takashi Furuya",
      "Marc T. Law"
    ]
  },
  "https://openreview.net/forum?id=hJHf7PCuVt": {
    "title": "Counterfactual Fairness on Graphs: Augmentations, Hidden Confounders, and Identifiability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyi Ling",
      "Zhimeng Jiang",
      "Na Zou",
      "Shuiwang Ji"
    ]
  },
  "https://openreview.net/forum?id=uF9ZdAwrCT": {
    "title": "In-distribution adversarial attacks on object recognition models using gradient-free search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Spandan Madan",
      "Tomotake Sasaki",
      "Hanspeter Pfister",
      "Tzu-Mao Li",
      "Xavier Boix"
    ]
  },
  "https://openreview.net/forum?id=KbteA50cni": {
    "title": "Distributed Quasi-Newton Method for Fair and Fast Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shayan Mohajer Hamidi",
      "Linfeng Ye"
    ]
  },
  "https://openreview.net/forum?id=hMO8sT9qaD": {
    "title": "Making Reliable and Flexible Decisions in Long-tailed Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bolian Li",
      "Ruqi Zhang"
    ]
  },
  "https://openreview.net/forum?id=dzQCRHKRdC": {
    "title": "Stochastic Variance-Reduced Newton: Accelerating Finite-Sum Minimization with Large Batches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michal Derezinski"
    ]
  },
  "https://openreview.net/forum?id=GDN5cFTNaL": {
    "title": "Adjacency Search Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meher Chaitanya",
      "Kshitijaa Jaglan",
      "Ulrik Brandes"
    ]
  },
  "https://openreview.net/forum?id=4Xz0WBAiX4": {
    "title": "ExCeL: Combined Extreme and Collective Logit Information for Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naveen Karunanayake",
      "Suranga Seneviratne",
      "Sanjay Chawla"
    ]
  },
  "https://openreview.net/forum?id=8C8LJIqF4y": {
    "title": "Time Series Domain Adaptation via Channel-Selective Representation Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nauman Ahad",
      "Mark A. Davenport",
      "Eva L Dyer"
    ]
  },
  "https://openreview.net/forum?id=OOgsAZdFOt": {
    "title": "Can AI-Generated Text be Reliably Detected? Stress Testing AI Text Detectors Under Various Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vinu Sankar Sadasivan",
      "Aounon Kumar",
      "Sriram Balasubramanian",
      "Wenxiao Wang",
      "Soheil Feizi"
    ]
  },
  "https://openreview.net/forum?id=aiOHc1LGpD": {
    "title": "Differentially Private Gradient Flow based on the Sliced Wasserstein Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ilana Sebag",
      "Muni Sreenivas Pydi",
      "Jean-Yves Franceschi",
      "Alain Rakotomamonjy",
      "Mike Gartrell",
      "Jamal Atif",
      "Alexandre Allauzen"
    ]
  },
  "https://openreview.net/forum?id=dbaGuiYsTl": {
    "title": "Wasserstein Modality Alignment Makes Your Multimodal Transformer More Robust",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "zhuo zhi",
      "Yuxuan Sun",
      "Qiangqiang Wu",
      "Ziquan Liu",
      "Miguel R. D. Rodrigues"
    ]
  },
  "https://openreview.net/forum?id=AFxEdJwQcp": {
    "title": "A thorough reproduction and evaluation of $\\mu$P",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgios Vlassis",
      "David Belius",
      "Volodymyr Fomichov"
    ]
  },
  "https://openreview.net/forum?id=avDr56QjSI": {
    "title": "Semantic Alignment for Prompt-Tuning in Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hari Chandana Kuchibhotla",
      "Sai Srinivas Kancheti",
      "Abbavaram Gowtham Reddy",
      "Vineeth N. Balasubramanian"
    ]
  },
  "https://openreview.net/forum?id=Utjw2z1ale": {
    "title": "Identifying Spurious Correlations using Counterfactual Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joseph Paul Cohen",
      "Louis Blankemeier",
      "Akshay S Chaudhari"
    ]
  },
  "https://openreview.net/forum?id=LVQ8BEL5n3": {
    "title": "Numerically Robust Fixed-Point Smoothing Without State Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicholas Krämer"
    ]
  },
  "https://openreview.net/forum?id=r8UFp9olQ0": {
    "title": "Explicitly Disentangled Representations in Object-Centric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Riccardo Majellaro",
      "Jonathan Collu",
      "Aske Plaat",
      "Thomas M. Moerland"
    ]
  },
  "https://openreview.net/forum?id=Gb4HBGG9re": {
    "title": "Enhanced Federated Optimization: Adaptive Unbiased Client Sampling with Reduced Variance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dun Zeng",
      "Zenglin Xu",
      "Yu Pan",
      "Xu Luo",
      "Qifan Wang",
      "Xiaoying Tang"
    ]
  },
  "https://openreview.net/forum?id=vmmgFW3ztz": {
    "title": "Leveraging a Simulator for Learning Causal Representations from Post-Treatment Covariates for CATE",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lokesh Nagalapatti",
      "Pranava Singhal",
      "Avishek Ghosh",
      "Sunita Sarawagi"
    ]
  },
  "https://openreview.net/forum?id=INijCSPtbQ": {
    "title": "Preventing Conflicting Gradients in Neural Marked Temporal Point Processes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanguy Bosser",
      "Souhaib Ben Taieb"
    ]
  },
  "https://openreview.net/forum?id=LZ9FmeFeLV": {
    "title": "Towards LifeSpan Cognitive Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Wang",
      "Chi Han",
      "Tongtong Wu",
      "Xiaoxin He",
      "Wangchunshu Zhou",
      "Nafis Sadeq",
      "Xiusi Chen",
      "Zexue He",
      "Wei Wang",
      "Gholamreza Haffari",
      "Heng Ji",
      "Julian McAuley"
    ]
  },
  "https://openreview.net/forum?id=IIVr4Hu3Oi": {
    "title": "Distributed Multi-Agent Lifelong Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prithviraj Tarale",
      "Edward Rietman",
      "Hava T Siegelmann"
    ]
  },
  "https://openreview.net/forum?id=T5OuTgPxHS": {
    "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyu Guo",
      "Hidetaka Kamigaito",
      "Taro Watanabe"
    ]
  },
  "https://openreview.net/forum?id=0mGho8wrv5": {
    "title": "SelfEval: Leveraging discriminative nature of generative models for evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sai Saketh Rambhatla",
      "Ishan Misra"
    ]
  },
  "https://openreview.net/forum?id=dHljjaNHh1": {
    "title": "Fairness Through Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunwoong Kim",
      "Insung Kong",
      "Jongjin Lee",
      "Minwoo Chae",
      "Sangchul Park",
      "Yongdai Kim"
    ]
  },
  "https://openreview.net/forum?id=V2SD2uVKEE": {
    "title": "Zero-shot CLIP Class Forgetting via Text-image Space Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexey Kravets",
      "Vinay P. Namboodiri"
    ]
  },
  "https://openreview.net/forum?id=lTX4bYREAZ": {
    "title": "A Scalable Approach for Mapper via Efficient Spatial Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Simi"
    ]
  },
  "https://openreview.net/forum?id=VIkycTWDWo": {
    "title": "Doubly Robust Conditional VAE via Decoder Calibration: An Implicit KL Annealing Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanhui Liu",
      "Xiao Wang"
    ]
  },
  "https://openreview.net/forum?id=IZrt6hB2sI": {
    "title": "Improving CLIP Counting Accuracy via Parameter-Efficient Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruisu Zhang",
      "Yicong Chen",
      "Kangwook Lee"
    ]
  },
  "https://openreview.net/forum?id=ccu0M3nmlF": {
    "title": "Transfer Learning in $\\ell_1$ Regularized Regression: Hyperparameter Selection Strategy based on Sharp Asymptotic Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Koki Okajima",
      "Tomoyuki Obuchi"
    ]
  },
  "https://openreview.net/forum?id=qbrE0LR7fF": {
    "title": "Evaluating Posterior Probabilities: Decision Theory, Proper Scoring Rules, and Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luciana Ferrer",
      "Daniel Ramos"
    ]
  },
  "https://openreview.net/forum?id=LdflD41Gn8": {
    "title": "On the Properties and Estimation of Pointwise Mutual Information Profiles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paweł Czyż",
      "Frederic Grabowski",
      "Julia E Vogt",
      "Niko Beerenwinkel",
      "Alexander Marx"
    ]
  },
  "https://openreview.net/forum?id=BlYIPa0Fx1": {
    "title": "An analysis of the noise schedule for score-based generative models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stanislas Strasman",
      "Antonio Ocello",
      "Claire Boyer",
      "Sylvain Le Corff",
      "Vincent Lemaire"
    ]
  },
  "https://openreview.net/forum?id=PtD2gVmb3J": {
    "title": "Global Safe Sequential Learning via Efficient Knowledge Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cen-You Li",
      "Olaf Dünnbier",
      "Marc Toussaint",
      "Barbara Rakitsch",
      "Christoph Zimmer"
    ]
  },
  "https://openreview.net/forum?id=QQE5j2OsLW": {
    "title": "Can Optimization Trajectories Explain Multi-Task Transfer?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Mueller",
      "Mark Dredze",
      "Nicholas Andrews"
    ]
  },
  "https://openreview.net/forum?id=yzbAFf8vd5": {
    "title": "A comparison between humans and AI at recognizing objects in unusual poses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Netta Ollikka",
      "Amro Kamal Mohamed Abbas",
      "Andrea Perin",
      "Markku Kilpeläinen",
      "Stephane Deny"
    ]
  },
  "https://openreview.net/forum?id=V6ia5hWIMD": {
    "title": "νSAM: Memory-Efficient Sharpness-Aware Minimization via Nuclear Norm Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Pethick",
      "Parameswaran Raman",
      "Lenon Minorics",
      "Mingyi Hong",
      "Shoham Sabach",
      "Volkan Cevher"
    ]
  },
  "https://openreview.net/forum?id=WEYMCLu8u7": {
    "title": "Event-Triggered Time-Varying Bayesian Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Brunzema",
      "Alexander von Rohr",
      "Friedrich Solowjow",
      "Sebastian Trimpe"
    ]
  },
  "https://openreview.net/forum?id=PTTa3U29NR": {
    "title": "Optimization Dynamics of Equivariant and Augmented Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oskar Nordenfors",
      "Fredrik Ohlsson",
      "Axel Flinth"
    ]
  },
  "https://openreview.net/forum?id=QplBL2pV4Z": {
    "title": "Federated Learning on Virtual Heterogeneous Data with Local-Global Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun-Yin Huang",
      "Ruinan Jin",
      "Can Zhao",
      "Daguang Xu",
      "Xiaoxiao Li"
    ]
  },
  "https://openreview.net/forum?id=XL1N6iLr0G": {
    "title": "An Attribute-based Method for Video Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tal Reiss",
      "Yedid Hoshen"
    ]
  },
  "https://openreview.net/forum?id=8mgX3Uw2Ea": {
    "title": "Making Self-supervised Learning Robust to Spurious Correlation via Learning-speed Aware Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weicheng Zhu",
      "Sheng Liu",
      "Carlos Fernandez-Granda",
      "Narges Razavian"
    ]
  },
  "https://openreview.net/forum?id=tRpWaK3pWh": {
    "title": "A Generalization Bound for Nearly-Linear Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eugene Golikov"
    ]
  },
  "https://openreview.net/forum?id=Qq4ge9Qe31": {
    "title": "Uncertainty-aware Evaluation of Auxiliary Anomalies with the Expected Anomaly Posterior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Perini",
      "Maja Rudolph",
      "Sabrina Schmedding",
      "Chen Qiu"
    ]
  },
  "https://openreview.net/forum?id=BhOJreYmur": {
    "title": "MIND: Modality-Informed Knowledge Distillation Framework for Multimodal Clinical Prediction Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alejandro Guerra-Manzanares",
      "Farah Shamout"
    ]
  },
  "https://openreview.net/forum?id=b68QOenPWy": {
    "title": "Active Learning via Classifier Impact and Greedy Selection for Interactive Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leah Bar",
      "Boaz Lerner",
      "Nir Darshan",
      "Rami Ben-Ari"
    ]
  },
  "https://openreview.net/forum?id=I4IAwVOZrM": {
    "title": "Lifelong Learning in StyleGAN through Latent Subspaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adarsh Kappiyath",
      "ANMOL GARG",
      "Ramya Hebbalaguppe",
      "Prathosh AP"
    ]
  },
  "https://openreview.net/forum?id=bwRxXiGO9A": {
    "title": "Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolas Boizard",
      "Kevin El Haddad",
      "CELINE HUDELOT",
      "Pierre Colombo"
    ]
  },
  "https://openreview.net/forum?id=CTkABQvnkm": {
    "title": "Decoupled Sequence and Structure Generation for Realistic Antibody Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nayoung Kim",
      "Minsu Kim",
      "Sungsoo Ahn",
      "Jinkyoo Park"
    ]
  },
  "https://openreview.net/forum?id=pxdSm7PW5Q": {
    "title": "Reviving Life on the Edge: Joint Score-Based Graph Generation of Rich Edge Attributes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nimrod Berman",
      "Eitan Kosman",
      "Dotan Di Castro",
      "Omri Azencot"
    ]
  },
  "https://openreview.net/forum?id=60Gi1w6hte": {
    "title": "Directed Graph Generation with Heat Kernels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marc T. Law",
      "Karsten Kreis",
      "Haggai Maron"
    ]
  },
  "https://openreview.net/forum?id=eakh1Edffd": {
    "title": "Reinforcement learning with non-ergodic reward increments: robustness via ergodicity transformations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Baumann",
      "Erfaun Noorani",
      "James Price",
      "Ole Peters",
      "Colm Connaughton",
      "Thomas B. Schön"
    ]
  },
  "https://openreview.net/forum?id=bHdEtW5E7O": {
    "title": "Federated Learning with Efficient Local Adaptation for Realized Volatility Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Zhao",
      "Lin Cai",
      "Wu-Sheng Lu"
    ]
  },
  "https://openreview.net/forum?id=MvYddudHuE": {
    "title": "Reweighting Improves Conditional Risk Bounds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yikai Zhang",
      "Jiahe Lin",
      "Fengpei Li",
      "Songzhu Zheng",
      "Anant Raj",
      "Anderson Schneider",
      "Yuriy Nevmyvaka"
    ]
  },
  "https://openreview.net/forum?id=vZGZIIgcG4": {
    "title": "Cost-Efficient Online Decision Making: A Combinatorial Multi-Armed Bandit Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arman Rahbar",
      "Niklas Åkerblom",
      "Morteza Haghir Chehreghani"
    ]
  },
  "https://openreview.net/forum?id=DqWvxSQ1TK": {
    "title": "From Promise to Practice: A Study of Common Pitfalls Behind the Generalization Gap in Machine Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saeideh Ghanbari Azar",
      "Lorenzo Tronchin",
      "Attila Simkó",
      "Tufve Nyholm",
      "Tommy Löfstedt"
    ]
  },
  "https://openreview.net/forum?id=3mJZfL77WM": {
    "title": "Highway Graph to Accelerate Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zidu Yin",
      "Zhen Zhang",
      "Dong Gong",
      "Stefano V Albrecht",
      "Javen Qinfeng Shi"
    ]
  },
  "https://openreview.net/forum?id=DCAeXwLenB": {
    "title": "Optimal Transport for Domain Adaptation through Gaussian Mixture Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eduardo Fernandes Montesuma",
      "Fred Maurice NGOLE MBOULA",
      "Antoine Souloumiac"
    ]
  },
  "https://openreview.net/forum?id=gpHOtQQPJG": {
    "title": "Optimization and Generalization Guarantees for Weight Normalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pedro Cisneros-Velarde",
      "Zhijie Chen",
      "Sanmi Koyejo",
      "Arindam Banerjee"
    ]
  },
  "https://openreview.net/forum?id=LzmsvRTqaJ": {
    "title": "Shared Stochastic Gaussian Process Latent Variable Models: A Multi-modal Generative model for Quasar spectra",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vidhi Lalchand",
      "Anna-Christina Eilers"
    ]
  },
  "https://openreview.net/forum?id=fqkq1MgONB": {
    "title": "BM$^2$: Coupled Schrödinger Bridge Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefano Peluchetti"
    ]
  },
  "https://openreview.net/forum?id=wS1fD0ofay": {
    "title": "Partial-Label Learning with a Reject Option",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Fuchs",
      "Florian Kalinke",
      "Klemens Böhm"
    ]
  },
  "https://openreview.net/forum?id=34vtRA3Nvu": {
    "title": "PRIMO: Private Regression in Multiple Outcomes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seth Neel"
    ]
  },
  "https://openreview.net/forum?id=ytKFKoCpyK": {
    "title": "ODNet: Opinion Dynamics-Inspired Neural Message Passing for Graphs and Hypergraphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingxin Zhou",
      "Outongyi Lv",
      "Jing Wang",
      "Xiang Xiao",
      "Weishu Zhao"
    ]
  },
  "https://openreview.net/forum?id=0u7pWfjri5": {
    "title": "Bigger is not Always Better: Scaling Properties of Latent Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangfu Mei",
      "Zhengzhong Tu",
      "Mauricio Delbracio",
      "Hossein Talebi",
      "Vishal M. Patel",
      "Peyman Milanfar"
    ]
  },
  "https://openreview.net/forum?id=UrSgGSTM2J": {
    "title": "Minimax Posterior Contraction Rates for Unconstrained Distribution Estimation on $[0,1]^d$ under Wasserstein Distance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Matthew Jacobs",
      "Lekha Patel",
      "Anirban Bhattacharya",
      "Debdeep Pati"
    ]
  },
  "https://openreview.net/forum?id=9CWU8Oi86d": {
    "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei He",
      "Yuzheng Hu",
      "Yong Lin",
      "Tong Zhang",
      "Han Zhao"
    ]
  },
  "https://openreview.net/forum?id=yawWz4qWkF": {
    "title": "Attention Mechanisms Don't Learn Additive Models: Rethinking Feature Importance for Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobias Leemann",
      "Alina Fastowski",
      "Felix Pfeiffer",
      "Gjergji Kasneci"
    ]
  },
  "https://openreview.net/forum?id=CNaiJRcX84": {
    "title": "S-TLLR: STDP-inspired Temporal Local Learning Rule for Spiking Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Paul E. Apolinario",
      "Kaushik Roy"
    ]
  },
  "https://openreview.net/forum?id=iVV7IzI55V": {
    "title": "On Inherent Adversarial Robustness of Active Vision Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amitangshu Mukherjee",
      "Timur Ibrayev",
      "Kaushik Roy"
    ]
  },
  "https://openreview.net/forum?id=AjJTg5M0r8": {
    "title": "Slicing Unbalanced Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Clément Bonet",
      "Kimia Nadjahi",
      "Thibault Sejourne",
      "Kilian FATRAS",
      "Nicolas Courty"
    ]
  },
  "https://openreview.net/forum?id=yBgTVWccIx": {
    "title": "DafnyBench: A Benchmark for Formal Software Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chloe R Loughridge",
      "Qinyi Sun",
      "Seth Ahrenbach",
      "Federico Cassano",
      "Chuyue Sun",
      "Ying Sheng",
      "Anish Mudide",
      "Md Rakib Hossain Misu",
      "Nada Amin",
      "Max Tegmark"
    ]
  },
  "https://openreview.net/forum?id=x8wscCAJ2m": {
    "title": "Sparse Neural Architectures via Deterministic Ramanujan Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suryam Arnav Kalra",
      "Arindam Biswas",
      "Pabitra Mitra",
      "BISWAJIT BASU"
    ]
  },
  "https://openreview.net/forum?id=JHxrh00W1j": {
    "title": "Masked Capsule Autoencoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miles Everett",
      "Mingjun Zhong",
      "Georgios Leontidis"
    ]
  },
  "https://openreview.net/forum?id=gqh0yzPYdo": {
    "title": "No Detail Left Behind: Revisiting Self-Retrieval for Fine-Grained Image Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manu Gaur",
      "Darshan Singh S",
      "Makarand Tapaswi"
    ]
  },
  "https://openreview.net/forum?id=Conma3qnaT": {
    "title": "Effective Backdoor Mitigation in Vision-Language Models Depends on the Pre-training Objective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sahil Verma",
      "Gantavya Bhatt",
      "Avi Schwarzschild",
      "Soumye Singhal",
      "Arnav Mohanty Das",
      "Chirag Shah",
      "John P Dickerson",
      "Pin-Yu Chen",
      "Jeff Bilmes"
    ]
  },
  "https://openreview.net/forum?id=XDbY3qhM42": {
    "title": "Improving GFlowNets for Text-to-Image Diffusion Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dinghuai Zhang",
      "Yizhe Zhang",
      "Jiatao Gu",
      "Ruixiang ZHANG",
      "Joshua M. Susskind",
      "Navdeep Jaitly",
      "Shuangfei Zhai"
    ]
  },
  "https://openreview.net/forum?id=oYP2Pd5aQt": {
    "title": "AlgoFormer: An Efficient Transformer Framework with Algorithmic Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihang Gao",
      "Chuanyang Zheng",
      "Enze Xie",
      "Han Shi",
      "Tianyang Hu",
      "Yu Li",
      "Michael Ng",
      "Zhenguo Li",
      "Zhaoqiang Liu"
    ]
  },
  "https://openreview.net/forum?id=Og3VxBFhwj": {
    "title": "Linear Convergence of Decentralized FedAvg for PL Objectives: The Interpolation Regime",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shruti P Maralappanavar",
      "Prashant Khanduri",
      "Bharath B N"
    ]
  },
  "https://openreview.net/forum?id=XxbQAsxrRC": {
    "title": "Maximally Expressive GNNs for Outerplanar Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Franka Bause",
      "Fabian Jogl",
      "Patrick Indri",
      "Tamara Drucks",
      "David Penz",
      "Nils Morten Kriege",
      "Thomas Gärtner",
      "Pascal Welke",
      "Maximilian Thiessen"
    ]
  },
  "https://openreview.net/forum?id=aV6dCg1VFV": {
    "title": "Investigating the impact of missing value handling on Boosted trees and Deep learning for Tabular data: A Claim Reserving case study",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Larionov",
      "Niall M. Adams",
      "Kevin N. Webster"
    ]
  },
  "https://openreview.net/forum?id=IK2cR89z45": {
    "title": "Personalized Privacy Amplification via Importance Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Fay",
      "Sebastian Mair",
      "Jens Sjölund"
    ]
  },
  "https://openreview.net/forum?id=bwyHf5eery": {
    "title": "A Note on Generalization in Variational Autoencoders: How Effective Is Synthetic Data and Overparameterization?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Z. Xiao",
      "Johannes Zenn",
      "Robert Bamler"
    ]
  },
  "https://openreview.net/forum?id=kzPNHQ8ByY": {
    "title": "Beyond Joint Demonstrations: Personalized Expert Guidance for Efficient Multi-Agent Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peihong Yu",
      "Manav Mishra",
      "Alec Koppel",
      "Carl Busart",
      "Priya Narayan",
      "Dinesh Manocha",
      "Amrit Singh Bedi",
      "Pratap Tokekar"
    ]
  },
  "https://openreview.net/forum?id=Vq0wMFBjo2": {
    "title": "Pre-trained Vision-Language Models Learn Discoverable Visual Concepts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Zang",
      "Tian Yun",
      "Hao Tan",
      "Trung Bui",
      "Chen Sun"
    ]
  },
  "https://openreview.net/forum?id=o58uy91V2V": {
    "title": "On the Detection of Reviewer-Author Collusion Rings From Paper Bidding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steven Jecmen",
      "Nihar B Shah",
      "Fei Fang",
      "Leman Akoglu"
    ]
  },
  "https://openreview.net/forum?id=ZA7D4nQuQF": {
    "title": "Transformers in Uniform TC$^0$",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Chiang"
    ]
  },
  "https://openreview.net/forum?id=SeGNvJJjbs": {
    "title": "Diff-Instruct++: Training One-step Text-to-image Generator Model to Align with Human Preferences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijian Luo"
    ]
  },
  "https://openreview.net/forum?id=KqRnsEMYLx": {
    "title": "Fourier PINNs: From Strong Boundary Conditions to Adaptive Fourier Bases",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Madison Cooley",
      "Varun Shankar",
      "Mike Kirby",
      "Shandian Zhe"
    ]
  },
  "https://openreview.net/forum?id=nxQtoHHcj9": {
    "title": "An Analysis of Model Robustness across Concurrent Distribution Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Myeongho Jeon",
      "Suhwan Choi",
      "Hyoje Lee",
      "Teresa Yeo"
    ]
  },
  "https://openreview.net/forum?id=ZnWqtPhHM7": {
    "title": "Prompt Tuning Vision Language Models with Margin Regularizer for Few-Shot Learning under Distribution Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debarshi Brahma",
      "Anuska Roy",
      "Soma Biswas"
    ]
  },
  "https://openreview.net/forum?id=JN7iNWaPTe": {
    "title": "Mental Modelling of Reinforcement Learning Agents by Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Lu",
      "Xufeng Zhao",
      "Josua Spisak",
      "Jae Hee Lee",
      "Stefan Wermter"
    ]
  },
  "https://openreview.net/forum?id=PzmaWLqK0e": {
    "title": "Reward-based Autonomous Online Learning Framework for Resilient Cooperative Target Monitoring using a Swarm of Robots",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shubhankar Gupta",
      "Saksham Sharma",
      "Suresh Sundaram"
    ]
  },
  "https://openreview.net/forum?id=edULLIVnoc": {
    "title": "Ask Your Distribution Shift if Pre-Training is Right for You",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Cohen-Wang",
      "Joshua Vendrow",
      "Aleksander Madry"
    ]
  },
  "https://openreview.net/forum?id=Y8EspxaksH": {
    "title": "Faithful Interpretation for Graph Neural Networks",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lijie Hu",
      "Tianhao Huang",
      "Lu Yu",
      "Wanyu Lin",
      "Tianhang Zheng",
      "Di Wang"
    ]
  },
  "https://openreview.net/forum?id=mAiMKnr9r5": {
    "title": "Random Policy Enables In-Context Reinforcement Learning within Trust Horizons",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiqin Chen",
      "Santiago Paternain"
    ]
  },
  "https://openreview.net/forum?id=1p9hQTbjgo": {
    "title": "MiniFold: Simple, Fast, and Accurate Protein Structure Prediction",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeremy Wohlwend",
      "Mateo Reveiz",
      "Matt McPartlon",
      "Axel Feldmann",
      "Wengong Jin",
      "Regina Barzilay"
    ]
  },
  "https://openreview.net/forum?id=ssXSrZ94sR": {
    "title": "Align and Distill: Unifying and Improving Domain Adaptive Object Detection",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justin Kay",
      "Timm Haucke",
      "Suzanne Stathatos",
      "Siqi Deng",
      "Erik Young",
      "Pietro Perona",
      "Sara Beery",
      "Grant Van Horn"
    ]
  },
  "https://openreview.net/forum?id=x6fXnsM9Ez": {
    "title": "The 2023 Foundation Model Transparency Index",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishi Bommasani",
      "Kevin Klyman",
      "Shayne Longpre",
      "Sayash Kapoor",
      "Nestor Maslej",
      "Betty Xiong",
      "Daniel Zhang",
      "Percy Liang"
    ]
  },
  "https://openreview.net/forum?id=1Avb4jYjLb": {
    "title": "Loss-to-Loss Prediction: Scaling Laws for All Datasets",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Brandfonbrener",
      "Nikhil Anand",
      "Nikhil Vyas",
      "Eran Malach",
      "Sham M. Kakade"
    ]
  },
  "https://openreview.net/forum?id=Y7dRmpGiHj": {
    "title": "What is the Relationship between Tensor Factorizations and Circuits (and How Can We Exploit it)?",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Loconte",
      "Antonio Mari",
      "Gennaro Gala",
      "Robert Peharz",
      "Cassio de Campos",
      "Erik Quaeghebeur",
      "Gennaro Vessio",
      "Antonio Vergari"
    ]
  },
  "https://openreview.net/forum?id=YCt8lsIDwA": {
    "title": "Diversify, Don't Fine-Tune: Scaling Up Visual Recognition Training with Synthetic Images",
    "volume": "featured",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoran Yu",
      "Chenchen Zhu",
      "Sean Culatana",
      "Raghuraman Krishnamoorthi",
      "Fanyi Xiao",
      "Yong Jae Lee"
    ]
  },
  "https://openreview.net/forum?id=H6DtMcZf5s": {
    "title": "Remembering to Be Fair Again: Reproducing Non-Markovian Fairness in Sequential Decision Making",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Domonkos Nagy",
      "Lohithsai Yadala Chanchu",
      "Krystof Bobek",
      "Xin Zhou",
      "Jacobus Smit"
    ]
  },
  "https://openreview.net/forum?id=zLfLTHOdZW": {
    "title": "[RE] GNNBoundary: Towards Explaining Graph Neural Networks through the Lens of Decision Boundaries",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tyme Chatupanyachotikul",
      "Leonard Horns",
      "Matei Nastase"
    ]
  },
  "https://openreview.net/forum?id=l9rATNBB8Y": {
    "title": "Privacy Awareness for Information-Sharing Assistants: A Case-study on Form-filling with Contextual Integrity",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sahra Ghalebikesabi",
      "Eugene Bagdasarian",
      "Ren Yi",
      "Itay Yona",
      "Ilia Shumailov",
      "Aneesh Pappu",
      "Chongyang Shi",
      "Laura Weidinger",
      "Robert Stanforth",
      "Leonard Berrada",
      "Pushmeet Kohli",
      "Po-Sen Huang",
      "Borja Balle"
    ]
  },
  "https://openreview.net/forum?id=sXr1fRjs1N": {
    "title": "Contextualized Messages Boost Graph Representations",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian Godwin Lim",
      "Galvin Brice Sy Lim",
      "Renzo Roel Tan",
      "Kazushi Ikeda"
    ]
  },
  "https://openreview.net/forum?id=yeITEuhv4Q": {
    "title": "Revisiting Deep Hybrid Models for Out-of-Distribution Detection",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul-Ruben Schlumbom",
      "Eibe Frank"
    ]
  },
  "https://openreview.net/forum?id=mSoDRZXsqj": {
    "title": "Towards Graph Foundation Models: A Study on the Generalization of Positional and Structural Encodings",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Billy Joe Franks",
      "Moshe Eliasof",
      "Semih Cantürk",
      "Guy Wolf",
      "Carola-Bibiane Schönlieb",
      "Sophie Fellenz",
      "Marius Kloft"
    ]
  },
  "https://openreview.net/forum?id=rKAkp1f3R7": {
    "title": "Shedding Light on Problems with Hyperbolic Graph Learning",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isay Katsman",
      "Anna Gilbert"
    ]
  },
  "https://openreview.net/forum?id=IPmzyQSiQE": {
    "title": "Nomic Embed: Training a Reproducible Long Context Text Embedder",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zach Nussbaum",
      "John Xavier Morris",
      "Andriy Mulyar",
      "Brandon Duderstadt"
    ]
  },
  "https://openreview.net/forum?id=wcxrJcJ7vq": {
    "title": "The Elusive Pursuit of Reproducing PATE-GAN: Benchmarking, Auditing, Debugging",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgi Ganev",
      "Meenatchi Sundaram Muthu Selva Annamalai",
      "Emiliano De Cristofaro"
    ]
  },
  "https://openreview.net/forum?id=wF3ZtSlOcT": {
    "title": "Multivariate Dense Retrieval: A Reproducibility Study under a Memory-limited Setup",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georgios Sidiropoulos",
      "Samarth Bhargav",
      "Panagiotis Eustratiadis",
      "Evangelos Kanoulas"
    ]
  },
  "https://openreview.net/forum?id=knv4lQFVoE": {
    "title": "A general framework of Riemannian adaptive optimization methods with a convergence analysis",
    "volume": "reprod",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hiroyuki Sakai",
      "Hideaki Iiduka"
    ]
  },
  "https://openreview.net/forum?id=ewwNKwh6SK": {
    "title": "Conditional Image Synthesis with Diffusion Models: A Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheyuan Zhan",
      "Defang Chen",
      "Jian-Ping Mei",
      "Zhenghe Zhao",
      "Jiawei Chen",
      "Chun Chen",
      "Siwei Lyu",
      "Can Wang"
    ]
  },
  "https://openreview.net/forum?id=ZiJYahyXLU": {
    "title": "Machine Learning with Physics Knowledge for Prediction: A Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joe Watson",
      "Chen Song",
      "Oliver Weeger",
      "Theo Gruner",
      "An Thai Le",
      "Kay Hansel",
      "Ahmed Hendawy",
      "Oleg Arenz",
      "Will Trojak",
      "Miles Cranmer",
      "Carlo D'Eramo",
      "Fabian Buelow",
      "Tanmay Goyal",
      "Jan Peters",
      "Martin W Hoffmann"
    ]
  },
  "https://openreview.net/forum?id=CsoSWpR5xC": {
    "title": "A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiachong Feng",
      "Longxu Dou",
      "Minzhi Li",
      "Qinghao Wang",
      "Yu Guo",
      "Haochuan Wang",
      "Chang Ma",
      "Lingpeng Kong"
    ]
  },
  "https://openreview.net/forum?id=D1PPuk8ZBI": {
    "title": "When Should Reinforcement Learning Use Causal Reasoning?",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oliver Schulte",
      "Pascal Poupart"
    ]
  },
  "https://openreview.net/forum?id=fHf4jbIfex": {
    "title": "Graph Theory-Based Deep Graph Similarity Learning: A Unified Survey of Pipeline, Techniques, and Challenges",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhouyang LIU",
      "Ning Liu",
      "Yixin Chen",
      "Ziqing Wen",
      "Jiezhong He",
      "Dongsheng Li"
    ]
  },
  "https://openreview.net/forum?id=u0azVc9Y0y": {
    "title": "A Survey on Model MoErging: Recycling and Routing Among Specialized Experts for Collaborative Learning",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prateek Yadav",
      "Colin Raffel",
      "Mohammed Muqeeth",
      "Lucas Caccia",
      "Haokun Liu",
      "Tianlong Chen",
      "Mohit Bansal",
      "Leshem Choshen",
      "Alessandro Sordoni"
    ]
  },
  "https://openreview.net/forum?id=1BqXkjNEGP": {
    "title": "Autoregressive Models in Vision: A Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Xiong",
      "Gongye Liu",
      "Lun Huang",
      "Chengyue Wu",
      "Taiqiang Wu",
      "Yao Mu",
      "Yuan Yao",
      "Hui Shen",
      "Zhongwei Wan",
      "Jinfa Huang",
      "Chaofan Tao",
      "Shen Yan",
      "Huaxiu Yao",
      "Lingpeng Kong",
      "Hongxia Yang",
      "Mi Zhang",
      "Guillermo Sapiro",
      "Jiebo Luo",
      "Ping Luo",
      "Ngai Wong"
    ]
  },
  "https://openreview.net/forum?id=M7Lhr2anjg": {
    "title": "Expressivity of Representation Learning on Continuous-Time Dynamic Graphs: An Information-Flow Centric Review",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sofiane ENNADIR",
      "Gabriela Zarzar Gandler",
      "Filip Cornell",
      "Lele Cao",
      "Oleg Smirnov",
      "Tianze Wang",
      "Levente Zólyomi",
      "Björn Brinne",
      "Sahar Asadi"
    ]
  },
  "https://openreview.net/forum?id=vz5P1Kbt6t": {
    "title": "Adaptive Physics-informed Neural Networks: A Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edgar Torres",
      "Mathias Niepert"
    ]
  },
  "https://openreview.net/forum?id=1nO4qFMiS0": {
    "title": "Open Problems in Technical AI Governance",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anka Reuel",
      "Benjamin Bucknall",
      "Stephen Casper",
      "Timothy Fist",
      "Lisa Soder",
      "Onni Aarne",
      "Lewis Hammond",
      "Lujain Ibrahim",
      "Alan Chan",
      "Peter Wills",
      "Markus Anderljung",
      "Ben Garfinkel",
      "Lennart Heim",
      "Andrew Trask",
      "Gabriel Mukobi",
      "Rylan Schaeffer",
      "Mauricio Baker",
      "Sara Hooker",
      "Irene Solaiman",
      "Sasha Luccioni",
      "Nitarshan Rajkumar",
      "Nicolas Moës",
      "Jeffrey Ladish",
      "David Bau",
      "Paul Bricman",
      "Neel Guha",
      "Jessica Newman",
      "Yoshua Bengio",
      "Tobin South",
      "Alex Pentland",
      "Sanmi Koyejo",
      "Mykel Kochenderfer",
      "Robert Trager"
    ]
  },
  "https://openreview.net/forum?id=FJgtVfUxLQ": {
    "title": "A Survey on the Honesty of Large Language Models",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siheng Li",
      "Cheng Yang",
      "Taiqiang Wu",
      "Chufan Shi",
      "Yuji Zhang",
      "Xinyu Zhu",
      "Zesen Cheng",
      "Deng Cai",
      "Mo Yu",
      "Lemao Liu",
      "Jie Zhou",
      "Yujiu Yang",
      "Ngai Wong",
      "Xixin Wu",
      "Wai Lam"
    ]
  },
  "https://openreview.net/forum?id=QTsJXSvAI2": {
    "title": "Where Do We Stand with Implicit Neural Representations? A Technical and Performance Survey",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amer Essakine",
      "Yanqi Cheng",
      "Chun-Wun Cheng",
      "Lipei Zhang",
      "Zhongying Deng",
      "Lei Zhu",
      "Carola-Bibiane Schönlieb",
      "Angelica I Aviles-Rivero"
    ]
  },
  "https://openreview.net/forum?id=sZdtTJInUg": {
    "title": "Class Incremental Learning from First Principles: A Review",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neil Ashtekar",
      "Jingxi Zhu",
      "Vasant G Honavar"
    ]
  },
  "https://openreview.net/forum?id=ukLxqA8zXj": {
    "title": "Evaluating Interpretable Methods via Geometric Alignment of Functional Distortions",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anna Hedström",
      "Philine Lou Bommer",
      "Thomas F Burns",
      "Sebastian Lapuschkin",
      "Wojciech Samek",
      "Marina MC Höhne"
    ]
  },
  "https://openreview.net/forum?id=RGsdAwWuu6": {
    "title": "Unified Risk Analysis for Weakly Supervised Learning",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao-Kai Chiang",
      "Masashi Sugiyama"
    ]
  },
  "https://openreview.net/forum?id=YxKJihRcby": {
    "title": "Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey)",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SUBBA REDDY OOTA",
      "Zijiao Chen",
      "Manish Gupta",
      "Bapi Raju Surampudi",
      "Gael Jobard",
      "Frederic Alexandre",
      "Xavier Hinaut"
    ]
  },
  "https://openreview.net/forum?id=WUQsBiJqyP": {
    "title": "A Comprehensive Survey on Inverse Constrained Reinforcement Learning: Definitions, Progress and Challenges",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guiliang Liu",
      "Sheng Xu",
      "Shicheng Liu",
      "Ashish Gaurav",
      "Sriram Ganapathi Subramanian",
      "Pascal Poupart"
    ]
  },
  "https://openreview.net/forum?id=wZLWuFHxt5": {
    "title": "A Survey of Recent Backdoor Attacks and Defenses in Large Language Models",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Zhao",
      "Meihuizi Jia",
      "Zhongliang Guo",
      "Leilei Gan",
      "XIAOYU XU",
      "Xiaobao Wu",
      "Jie Fu",
      "Feng Yichao",
      "Fengjun Pan",
      "Anh Tuan Luu"
    ]
  },
  "https://openreview.net/forum?id=RJT1baPhdV": {
    "title": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models",
    "volume": "survey",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulei Qin",
      "Yuncheng Yang",
      "Pengcheng Guo",
      "Gang Li",
      "Hang Shao",
      "Yuchen Shi",
      "Zihan Xu",
      "Yun Gu",
      "Ke Li",
      "Xing Sun"
    ]
  },
  "https://openreview.net/forum?id=7A96yteeF9": {
    "title": "Latent mixed-effect models for high-dimensional longitudinal data",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Priscilla Ong",
      "Manuel Haussmann",
      "Otto Lönnroth",
      "Harri Lähdesmäki"
    ]
  },
  "https://openreview.net/forum?id=9L0B5N5hUX": {
    "title": "Investigating Generalization Behaviours of Generative Flow Networks",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lazar Atanackovic",
      "Emmanuel Bengio"
    ]
  },
  "https://openreview.net/forum?id=xdWP1d8BxI": {
    "title": "Sparse Decomposition of Graph Neural Networks",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaochen Hu",
      "Mai Zeng",
      "Ge Zhang",
      "Pavel Rumiantsev",
      "Liheng Ma",
      "Yingxue Zhang",
      "Mark Coates"
    ]
  },
  "https://openreview.net/forum?id=5298fKGmv3": {
    "title": "The BrowserGym Ecosystem for Web Agent Research",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thibault Le Sellier de Chezelles",
      "Maxime Gasse",
      "Alexandre Lacoste",
      "Massimo Caccia",
      "Alexandre Drouin",
      "Léo Boisvert",
      "Megh Thakkar",
      "Tom Marty",
      "Rim Assouel",
      "Sahar Omidi Shayegan",
      "Lawrence Keunho Jang",
      "Xing Han Lù",
      "Ori Yoran",
      "Dehan Kong",
      "Frank F. Xu",
      "Siva Reddy",
      "Graham Neubig",
      "Quentin Cappart",
      "Russ Salakhutdinov",
      "Nicolas Chapados"
    ]
  },
  "https://openreview.net/forum?id=SbGt90dxdp": {
    "title": "Variation Matters: from Mitigating to Embracing Zero-Shot NAS Ranking Function Variation",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pavel Rumiantsev",
      "Mark Coates"
    ]
  },
  "https://openreview.net/forum?id=FcyHZ6Q4k0": {
    "title": "Necessary and Sufficient Watermark for Large Language Models",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuki Takezawa",
      "Ryoma Sato",
      "Han Bao",
      "Kenta Niwa",
      "Makoto Yamada"
    ]
  },
  "https://openreview.net/forum?id=jrUUk5Fskm": {
    "title": "Personalized Negative Reservoir for Incremental Learning in Recommender Systems",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antonios Valkanas",
      "Yuening Wang",
      "Yingxue Zhang",
      "Mark Coates"
    ]
  },
  "https://openreview.net/forum?id=hGaWq5Buj7": {
    "title": "The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers",
    "volume": "expert",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hussein Mozannar",
      "Valerie Chen",
      "Mohammed Alsobay",
      "Subhro Das",
      "Sebastian Zhao",
      "Dennis Wei",
      "Manish Nagireddy",
      "Prasanna Sattigeri",
      "Ameet Talwalkar",
      "David Sontag"
    ]
  }
}